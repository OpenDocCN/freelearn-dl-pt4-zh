["```py\n    import networkx as nx\n    import numpy as np\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    ```", "```py\n    def textrank(sentences, top_n=2):\n        tfidf = TfidfVectorizer().fit_transform(sentences)\n        similarity_matrix = cosine_similarity(tfidf)\n    ```", "```py\n        graph = nx.from_numpy_array(similarity_matrix)\n        scores = nx.pagerank(graph)\n    ```", "```py\n        ranked_sentences = sorted(((\n            scores[i], s) for i, s in enumerate(sentences)\n        ), reverse=True)\n        return [s for _, s in ranked_sentences[:top_n]]\n    ```", "```py\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. NLP is used in many applications, including machine translation, speech recognition, and chatbots.\n\"\"\"\nsentences = text.strip().split('.')\nsummary = textrank(sentences)\nprint(\"Summary:\", ' '.join(summary))\n```", "```py\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch_geometric.nn import GCNConv\n    from torch_geometric.data import Data\n    ```", "```py\n    class AMRToTextSummarizer(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim):\n            super().__init__()\n            self.graph_conv = GCNConv(input_dim, hidden_dim)\n            self.gru = nn.GRU(\n                hidden_dim, hidden_dim, batch_first=True)\n            self.fc = nn.Linear(hidden_dim, output_dim)\n    ```", "```py\n        def forward(self, data):\n            # Graph encoding\n            x, edge_index = data.x, data.edge_index\n            h = self.graph_conv(x, edge_index)\n            h = F.relu(h)\n            # Sequence decoding\n            h = h.unsqueeze(0)  # Add batch dimension\n            output, _ = self.gru(h)\n            output = self.fc(output)\n            return output\n    ```", "```py\n    input_dim = 100  # Dimension of input node features\n    hidden_dim = 256\n    output_dim = 10000  # Vocabulary size\n    edge_index = torch.tensor([\n        [0, 1, 2], [1, 2, 0]], dtype=torch.long)\n    x = torch.randn(3, input_dim)\n    data = Data(x=x, edge_index=edge_index)\n    ```", "```py\n    model = AMRToTextSummarizer(input_dim, hidden_dim, output_dim)\n    summary_logits = model(data)\n    print(\"Summary logits shape:\", summary_logits.shape)\n    ```"]