- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning for Time Series Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll delve into anomaly detection problems using time series
    data. This task involves detecting rare observations that are significantly different
    from most samples in a dataset. We’ll explore different approaches to tackle this
    problem, such as prediction-based methods or reconstruction-based methods. This
    includes using powerful methods such as **autoencoders** (**AEs**), **variational
    AEs** (**VAEs**), or **generative adversarial** **networks** (**GANs**).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to define time series anomaly detection
    problems using different approaches with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Time series anomaly detection with **Autoregressive Integrated Moving** **Average**
    (**ARIMA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction-based anomaly detection using **deep** **learning** (**DL**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection using a **long short-term memory** (**LSTM**) AE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an AE using PyOD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a VAE for time series anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GANs for time series anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The models developed in this chapter are based on different frameworks. First,
    we show how to develop prediction-based methods using the `statsforecast` and
    `neuralforecast` libraries. Other methods, such as an LSTM AE, will be explored
    using the PyTorch Lightning ecosystem. Finally, we’ll also use the PyOD library
    to create anomaly detection models based on approaches such as GANs or VAEs. Of
    course, we also rely on typical data manipulation libraries such as `pandas or
    NumPy.` The following list contains all the required libraries for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` (1.3.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` (2.1.3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy (1.26.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statsforecast` (1.6.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasetsforecast` (0.08)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0neuralforecast` (1.6.4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` (2.1.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Lightning (2.1.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Forecasting (1.0.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyOD (1.1.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code and datasets used in this chapter can be found at the following GitHub
    URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Time series anomaly detection with ARIMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series anomaly detection is an important task in application domains such
    as healthcare or manufacturing, among many others. Anomaly detection methods aim
    to identify observations that do not conform to the typical behavior of a dataset.
    In practice, anomalies can represent phenomena such as faults in machinery or
    fraudulent activity. Anomaly detection is a common task in **machine learning**
    (**ML**), and it has a few dedicated methods when it involves time series data.
    This type of dataset and the patterns therein can evolve over time, which complicates
    the modeling process and the effectiveness of the detectors. Statistical learning
    methods for time series anomaly detection problems usually follow a prediction-based
    approach or a reconstruction-based approach. In this recipe, we describe how to
    use an ARIMA method to create a prediction-based anomaly detection system for
    univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll focus on a univariate time series from the `M3` dataset, which is available
    in the `datasetsforecast` library. Here’s how to get this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we start by loading the `M3` dataset using the `load()`
    method. Then, we use the `query()` method to get the univariate time series with
    an identifier (`unique_id` column) equal to `Q1`. Now, let’s see how to detect
    anomalies in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll build a forecasting model and use the corresponding prediction intervals
    to detect anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating a forecasting model. While any model would work, in this
    recipe, we focus on ARIMA. Here’s how to define this model using the `statsforecast`
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to fit the model and get the forecasts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we use the `forecast()` method to get the predictions. In this example,
    we set the forecasting horizon to `8` (`h=8`). We also pass two additional parameters:
    `level=[99]`, which means that we also want the model to predict the intervals
    with a `99`% confidence level; `fitted=True`, which tells the model to compute
    the training forecasts. We use the `forecast_fitted_values()` method to get the
    forecasts from the training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we identify anomalies based on whether the point forecasts are within
    the prediction intervals made by the model. This is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code checks whether the training predictions (`insample_forecasts['y']`
    object) are within the `99`% prediction intervals. Any observation that does not
    pass this check is considered an anomaly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we use the `plot()` method from the `StatsForecast` class to plot
    the anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s what the plot looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.1: Example of an anomaly identified by ARIMA](img/B21145_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Example of an anomaly identified by ARIMA'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used the `AutoARIMA` implementation available in the `statsforecast` library
    to create the ARIMA model. This approach automatically selects the best parameters
    for the model. We set the seasonal length to `4` since the frequency of the data
    is quarterly. The fitting process is carried out by a `StatsForecast` class instance.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction-based methods work by comparing the forecasts of a given model with
    the actual values of the series. In this case, we use an ARIMA model, but other
    methods can also be used. Moreover, we consider an approach based on prediction
    intervals. Specifically, an observation is considered an anomaly if its value
    is outside of the predicted interval. In the code shown in the previous section,
    we considered a prediction interval with a 99% level, but you can test a different
    value for your problem.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we focus on ARIMA to get prediction intervals, but you can use
    any other model with such capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the following URL for more details about how to use the `statsforecast`
    library for prediction-based anomaly detection: [https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html](https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction-based anomaly detection using DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We continue to explore prediction-based methods in this recipe. This time, we’ll
    create a forecasting model based on DL. Besides, we’ll use the point forecasts’
    error as a reference for detecting anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use a time series dataset about the number of taxi trips in New York
    City. This dataset is considered a benchmark problem for time series anomaly detection
    tasks. You can check the source at the following link: [https://databank.illinois.edu/datasets/IDB-9610843](https://databank.illinois.edu/datasets/IDB-9610843).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading the time series using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code involves several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset and corresponding labels using the `pd.read_csv``()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Processing this dataset into a tabular format with three main pieces of information:
    the time series identifier (`unique_id`), the timestamp (`ds`), and the value
    of the observation (`y`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing the labels into a new Boolean column called `is_anomaly` that denotes
    whether the corresponding observation is an anomaly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s what the series looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2: New York City dataset with marked anomalies](img/B21145_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: New York City dataset with marked anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we use the taxi trips dataset to train a forecasting model. In this recipe,
    we’ll resort to the `neuralforecast` library, which contains implementation for
    several DL algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use an input size (`n_lags`) of `144`, which corresponds to 3 days of data
    as the time series is collected every `30` minutes (`freq='30T'`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After defining the model, we can train it using the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before the fitting process, we drop the `is_anomaly` variable that contains
    anomaly information. Now, the idea is to use the model to forecast the values
    of the time series. Any significant deviation from the actual value is considered
    an anomaly. Let’s look at the training predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can get the training (or insample) predictions by calling the `predict_insample()`
    method, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we get the training sample and remove the initial `n_lag`
    observations to align the predictions with the actual data. Then, we measure the
    absolute error of the model by taking the absolute difference between the predictions
    and actual values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the absolute error in the training data along with the marked anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the interest of conciseness, the plotting functions are not shown. You can
    check them out in the GitHub repository. The plot is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3: Absolute error by the Neural Hierarchical Implementation for
    Time Series (NHITS) model and marked anomalies](img/B21145_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Absolute error by the Neural Hierarchical Implementation for Time
    Series (NHITS) model and marked anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: Large errors occur during two of the anomalies, though the model also misses
    some anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen in the previous recipe, we use a forecasting model to identify anomalies
    in a time series. In this case, instead of using prediction intervals, we rely
    on the absolute error of the model. A large error indicates a potential anomaly
    in the time series.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `neuralforecast` framework to build a DL forecasting model based
    on the NHITS method. NHITS is a model that extends **Neural Basis Expansion Analysis**
    (**NBEATS**) and is based on a **multilayer perceptron** (**MLP**) type of architecture.
  prefs: []
  type: TYPE_NORMAL
- en: This involves transforming the data into an appropriate format and training
    the model using auto-regression.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we focus on a univariate time series dataset and a particular
    forecasting method (NHITS). Yet, it’s important to note that the prediction-based
    approach for anomaly detection can be applied to different settings (such as multivariate
    time series) and with other forecasting methods.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, we need to define an error threshold above which
    we flag an observation as an anomaly. We will explore several implementations
    with this feature in subsequent recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection using an LSTM AE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll build an AE to detect anomalies in time series. An AE
    is a type of **neural network** (**NN**) that tries to reconstruct the input data.
    The motivation to use this kind of model for anomaly detection is that the reconstruction
    process of anomalous data is more difficult than that of typical observations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll continue with the New York City taxi time series in this recipe. In terms
    of framework, we’ll show how to build an AE using PyTorch Lightning. This means
    that we’ll build a data module to handle the data preprocessing and another module
    for handling the training and inference of the NN.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe is split into three parts. First, we build the data module based
    on PyTorch. Then, we create an AE module. Finally, we combine the two parts to
    build an anomaly detection system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by building the data module. We create a class called `TaxiDataModule`
    that extends `pl.LightningDataModule`. Here’s the constructor of the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `TaxiDataModule` class takes two inputs besides the dataset: the number
    of lags (context length) and the batch size.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we code the `setup()` method, where the data is prepared for training
    and testing the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we start by splitting the data into training, validation,
    and testing sets. Each of these is transformed into a `TimeSeriesDataSet` class
    instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The data loaders are implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Essentially, the data loading process is similar to what we did before in the
    forecasting tasks. You can check, for example, the *Multi-step and multi-output
    forecasting with multivariate time series* recipe in [*Chapter 5*](B21145_05.xhtml#_idTextAnchor306).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we focus on the AE model, which is split into two parts: an encoder and
    a decoder. Here’s an implementation of the encoder in a class called `Encoder`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The decoder is implemented in a class called `Decoder` that also extends `nn.Module`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The two parts are combined in an `AutoencoderLSTM` class that extends `pl.LightningModule`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `forward()` method, the encoder part takes the original input (`self.encoder(x)`)
    and transforms it into a reduced dimension (`xh` object). Then, the decoder reconstructs
    the original input data based on `xh`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we implement the training, validation, and prediction steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train the NN using the `Trainer` class from PyTorch Lightning. We use `144`
    lags, which amounts to 3 days of data. We also apply early stopping to guide the
    training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After training, we can apply the model to the test data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code, we transform the `test` object from the data module
    into a data loader. We use a batch size of `1` without shuffling to process each
    instance sequentially. Then, we use the `trainer` object to get the predictions.
    The following figure shows the reconstructed error in the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4: Reconstruction error by the AE and marked anomalies](img/B21145_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Reconstruction error by the AE and marked anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, the peaks in reconstruction error coincide with the anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The workflow in the data module may be familiar because it follows the same
    ideas behind the forecasting models we’ve built in other chapters; for example,
    in the *Multi-step and multi-output forecasting with multivariate time series*
    recipe in [*Chapter 5*](B21145_05.xhtml#_idTextAnchor306). But, in this case,
    we’re not interested in predicting the future values of the series. Instead, at
    each time step, both the input and the output of the model are recent lags of
    the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'An AE is composed of two main parts: an encoder and a decoder. The encoder
    aims to compress the input data into a small dimension, which is referred to as
    the bottleneck. Turning the input data into a small dimension is important to
    make the NN focus on the most important patterns in the data, disregarding noise.
    Then, the decoder takes the data encoded in the reduced dimension and tries to
    reconstruct the original input data. Both the encoder and the decoder of the NN
    are based on a stacked LSTM AE. Yet, you can use different architectures for these
    components.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Encoder` class extends the `nn.Module` class from `torch`. This particular
    encoder consists of two LSTM layers. These layers stack on top of each other as
    detailed in the `forward()` method. The `Decoder` class also contains two stacked
    LSTM layers that are followed by a densely connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: In the training step of the AE, we pass a batch of the lagged time series (`x['encoder_cont']`)
    to the model. It produces an object called `y_pred`, which is the reconstructed
    input. Then, we compute the `F.mse_loss`), which compares the original input with
    the reconstructed one.
  prefs: []
  type: TYPE_NORMAL
- en: Building an AE using PyOD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyOD is a Python library that is devoted to anomaly detection. It contains several
    reconstruction-based algorithms such as AEs. In this recipe, we’ll build an AE
    using PyOD to detect anomalies in time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install PyOD using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use the same dataset as in the previous recipe. So, we start with the
    dataset object created in the *Prediction-based anomaly detection using DL* recipe.
    Let’s see how to transform this data to build an AE with PyOD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to build an AE and predict the probability of
    anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by transforming the time series using a sliding window with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We get the value column of the time series and store it in the series object.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we iterate over the dataset using a sliding window similar to an auto-regressive
    approach. This way, the time series is represented by its past number of lags(`N_LAGS`)
    at each time step.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We standardize the data using `StandardScaler` from `scikit-learn`, which is
    an important step for training NNs such as AEs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After preprocessing the data, we define the AE based on PyOD and fit it using
    the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the distribution of the scores:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.5: Histogram with the anomaly scores produced by the AE](img/B21145_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Histogram with the anomaly scores produced by the AE'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the inference step, we can use the `predict``()` and `predict_proba``()`
    methods. The `predict``()` method works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `predict_proba()`method works as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The probabilities represent the probability of each observation being an anomaly.
    You can plot the probabilities using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s what the probabilities look like along the training set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.6: Anomaly probability scores produced by the AE](img/B21145_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Anomaly probability scores produced by the AE'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the anomaly probability score peaks coincide with some anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PyOD library follows a design pattern similar to `scikit-learn`. So, each
    method, such as `AutoEncoder`, is trained using the `fit``()` method and produces
    predictions based on a `predict` or `predict_proba``()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `AutoEncoder` class instance from the `auto_encoder_torch` module.
    The library also contains the equivalent method but with a TensorFlow backend.
    We create an instance of the model and set up a few parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_neurons=[144, 2, 2, 144]`: These parameters detail the number of hidden
    units per layer. The input and output layers have a number of units equal to the
    input size, which is the number of lags. The hidden layers of the AE typically
    have a low number of units to compress the input data before reconstruction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_activation`: The activation function, which is set to the rectified
    linear function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_norm`: A Boolean input that represents whether batch normalization should
    be applied. You can learn more about this at the following link: [https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate, which is set to `0.001`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The batch size, which is set to `64`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout_rate`: A dropout rate between the layers for regularization, which
    is set to `0.3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we created another AE for anomaly detection. This involves transforming
    the time series using a sliding window, similar to what we did for building forecasting
    models for auto-regression. The model predicts whether each observation is an
    anomaly based on the outlier scores. The threshold is set automatically by the
    model, though you can pick your own as well.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deciding whether an observation is an anomaly involves analyzing the anomaly
    scores of the model. You can use different approaches, such as percentiles or
    standard deviations. For example, consider an observation an anomaly if the reconstruction
    error is above some percentile (such as 95) or if the reconstruction error is
    above two standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We used the prediction from the training data for illustration purposes. Working
    with a test follows a similar approach.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a VAE for time series anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the foundation laid in the previous recipe, we now turn our attention
    to VAEs, a more sophisticated and probabilistic approach to anomaly detection
    in time series data. Unlike traditional AEs, VAEs introduce a probabilistic interpretation,
    making them more adept at handling inherent uncertainties in real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This code in this recipe is based on PyOD. We also use the same dataset as
    in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how to create a VAE for time series anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by preparing our dataset, as in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is first transformed using a sliding window, a technique that helps
    the model understand temporal dependencies within the time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After transforming the dataset, we define and fit the VAE model using PyOD’s
    `VAE` class. The configuration of the `VAE` class includes specifying the architecture
    of the encoder and decoder networks and various training parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fitted VAE model is then used to generate anomaly scores. These scores
    reflect how well each data point conforms to the pattern learned by the model.
    Points with higher scores are more likely to be anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A VAE is a NN model that stands out for its ability to handle data’s latent
    or hidden aspects. Unlike traditional AEs, which map inputs to a fixed point in
    a latent space, VAEs transform inputs into a probability distribution, usually
    a normal distribution, characterized by mean and variance. This way, every input
    is associated with a region in the latent space rather than a single point, introducing
    an element of randomness and variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder network then samples points from these estimated distributions
    and attempts to reconstruct the original input data. The training process involves
    two key objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the reconstruction error ensures that the decoder can accurately
    recreate the input data from latent representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing latent space distributions to be close to a standard normal distribution.
    This is typically achieved by minimizing the Kullback-Leibler divergence. The
    regularization process prevents overfitting and ensures a well-structured and
    continuous latent space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once trained, the VAE can be employed for anomaly detection. The VAE should
    be able to reconstruct normal data (similar to what it was trained on) with relatively
    low error. Conversely, data points significantly different from the training set
    (potential anomalies) will likely be reconstructed with higher error. Therefore,
    the reconstruction error serves as an anomaly score.
  prefs: []
  type: TYPE_NORMAL
- en: 'A high reconstruction error suggests that the data point does not conform well
    to the learned data distribution, flagging it as an anomaly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: True values, true anomalies, and the probability of anomalies
    predicted by the VAE](img/B21145_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: True values, true anomalies, and the probability of anomalies predicted
    by the VAE'
  prefs: []
  type: TYPE_NORMAL
- en: This comparison helps us evaluate the performance of our VAE in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most interesting aspects of VAEs is their ability to generate new
    data points. By sampling from learned distributions in the latent space, we can
    generate new instances that are similar to the training data. This property can
    be particularly useful in scenarios where data augmentation is required.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the probabilistic nature of VAEs offers a natural way to quantify
    uncertainty. This can be particularly beneficial in settings where it’s relevant
    to assess the confidence of the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Using GANs for time series anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs have gained significant popularity in various fields of ML, particularly
    in image generation and modification. However, their application in time series
    data, especially for anomaly detection, is an emerging area of research and practice.
    In this recipe, we focus on utilizing GANs, specifically **Anomaly Detection with
    Generative Adversarial Networks** (**AnoGAN**), to detect time series data anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into the implementation, ensure that you have the PyOD library
    installed. We will continue using the taxi trip dataset for this recipe, which
    provides a real-world context for time series anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation involves several steps: data preprocessing, defining and
    training the AnoGAN model, and finally, performing anomaly detection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the dataset and preparing it for the AnoGAN model. The
    dataset is transformed in the same way as before using a sliding window approach:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'AnoGAN is then defined with specific hyperparameters and trained on the preprocessed
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model is trained, we use it to predict anomalies in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we visualize the results to compare the model’s predictions with actual
    anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.8: True values, true anomalies, and the probability of anomalies
    predicted by a GAN](img/B21145_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: True values, true anomalies, and the probability of anomalies predicted
    by a GAN'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AnoGAN is a model that employs the principles of GANs for the specific task
    of anomaly detection in time series data. The core idea behind AnoGAN is the interaction
    between two key components: the generator and the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: The Generator is tasked with creating synthetic data that resembles the true
    time series data it has been trained on. It learns to capture the underlying patterns
    and distributions of the input data, trying to generate outputs that are indistinguishable
    from the real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Discriminator, on the other hand, acts as a critic. Its role is to discern
    whether the data it reviews are genuine (actual data points from the dataset)
    or fabricated (outputs generated by the Generator). During training, these two
    components engage in a continuous game: the Generator improves its ability to
    produce realistic data, while the Discriminator becomes better at detecting fakes.'
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction error is once again used to identify anomalies. The Generator,
    being trained only on normal data, will struggle to reproduce outliers or anomalous
    instances. Thus, when the reconstructed version of a data point diverges significantly
    from the original, we find a potential anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the reconstruction error can be calculated using various methods,
    such as MSE or other distance metrics, depending on the nature of the data and
    the specific requirements of the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While AnoGAN provides a novel approach to time series anomaly detection, it
    is worth exploring variations and improvements. For instance, one might consider
    tuning the model’s architecture or experimenting with different types of GANs,
    such as **conditional GANs** (**CGANs**) or **Wasserstein** **GANs** (**WGANs**).
  prefs: []
  type: TYPE_NORMAL
