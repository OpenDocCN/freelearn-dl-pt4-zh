["```py\npip install -U pandas numpy scikit-learn sktime torch pytorch-forecasting pytorch-lightning gluonts\n```", "```py\nN_LAGS = 7\nHORIZON = 14\n```", "```py\ndatamodule = MultivariateSeriesDataModule(data=mvtseries,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    batch_size=32,\n    test_size=0.3)\nmodel = MultivariateLSTM(input_dim=n_vars,\n    hidden_dim=32,\n    num_layers=1,\n    output_dim=HORIZON)\n```", "```py\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\",\n    min_delta=1e-4,\n    patience=10,\n    verbose=False,\n    mode=\"min\")\ntrainer = Trainer(max_epochs=20, callbacks=[early_stop_callback])\ntrainer.fit(model, datamodule)\ntrainer.test(model=model, datamodule=datamodule)\n```", "```py\nN_LAGS = 14\nHORIZON = 7\nTARGET = ['Incoming Solar', 'Air Temp', 'Vapor Pressure']\nmvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',\n    parse_dates=['datetime'],\n    index_col='datetime')\n```", "```py\nclass MultivariateSeriesDataModule(pl.LightningDataModule):\n    def __init__(\n            self,\n            data: pd.DataFrame,\n            target_variables: List[str],\n            n_lags: int,\n            horizon: int,\n            test_size: float = 0.2,\n            batch_size: int = 16,\n    ):\n        super().__init__()\n        self.data = data\n        self.batch_size = batch_size\n        self.test_size = test_size\n        self.n_lags = n_lags\n        self.horizon = horizon\n        self.target_variables = target_variables\n        self.target_scaler = {k: MinMaxScaler() \n            for k in target_variables}\n        self.feature_names = [col for col in data.columns\n            if col not in self.target_variables]\n        self.training = None\n        self.validation = None\n        self.test = None\n        self.predict_set = None\n        self.setup()\n```", "```py\ndef setup(self, stage=None):\n    self.preprocess_data()\n    train_indices, val_indices, test_indices = self.split_data()\n    train_df = self.data.loc\n        [self.data[\"time_index\"].isin(train_indices)]\n    val_df = self.data.loc[self.data[\"time_index\"].isin(val_indices)]\n    test_df = self.data.loc\n        [self.data[\"time_index\"].isin(test_indices)]\n    for c in self.target_variables:\n        self.target_scaler[c].fit(train_df[[c]])\n    self.scale_target(train_df, train_df.index)\n    self.scale_target(val_df, val_df.index)\n    self.scale_target(test_df, test_df.index)\n    self.training = TimeSeriesDataSet(\n        train_df,\n        time_idx=\"time_index\",\n        target=self.target_variables,\n        group_ids=[\"group_id\"],\n        max_encoder_length=self.n_lags,\n        max_prediction_length=self.horizon,\n        time_varying_unknown_reals=self.feature_names + \n            self.target_variables,\n        scalers={name: MinMaxScaler() for name in self.feature_names},\n    )\n    self.validation = TimeSeriesDataSet.from_dataset\n        (self.training, val_df)\n    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)\n    self.predict_set = TimeSeriesDataSet.from_dataset(\n        self.training, self.data, predict=True\n    )\n```", "```py\nclass MultiOutputLSTM(LightningModule):\n    def __init__(self, input_dim, hidden_dim, num_layers, \n        horizon, n_output):\n        super().__init__()\n        self.n_output = n_output\n        self.horizon = horizon\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        self.output_dim = int(self.n_output * self.horizon)\n        self.lstm = nn.LSTM(input_dim, hidden_dim,\n            num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, self.output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.lstm.num_layers, x.size(0),\n            self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.lstm.num_layers, x.size(0),\n            self.hidden_dim).to(self.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n```", "```py\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        y_pred = y_pred.unsqueeze(-1).view(-1, self.horizon, \n            self.n_output)\n        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]\n        loss = [F.mse_loss(y_pred[i], \n            y[0][i]) for i in range(self.n_output)]\n        loss = torch.mean(torch.stack(loss))\n        self.log('train_loss', loss)\n        return loss\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        y_pred = y_pred.unsqueeze(-1).view(-1, self.horizon, \n            self.n_output)\n        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]\n        loss = [F.mse_loss(y_pred[i],\n            y[0][i]) for i in range(self.n_output)]\n        loss = torch.mean(torch.stack(loss))\n        self.log('test_loss', loss)\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        y_pred = y_pred.unsqueeze(-1).view(-1,\n            self.horizon, self.n_output)\n        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]\n        return y_pred\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n```", "```py\nmodel = MultiOutputLSTM(input_dim=n_vars,\n    hidden_dim=32,\n    num_layers=1,\n    horizon=HORIZON,\n    n_vars=len(TARGET))\ndatamodule = MultivariateSeriesDataModule(data=mvtseries,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    target_variables=TARGET)\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\",\n    min_delta=1e-4,\n    patience=10,\n    verbose=False,\n    mode=\"min\")\ntrainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])\ntrainer.fit(model, datamodule)\ntrainer.test(model=model, datamodule=datamodule)\nforecasts = trainer.predict(model=model, datamodule=datamodule)\n```", "```py\nN_LAGS = 7\nHORIZON = 7\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset('nn5_daily_without_missing', regenerate=False)\n```", "```py\nimport lightning.pytorch as pl\nclass GlobalDataModule(pl.LightningDataModule):\n    def __init__(self,\n                 data,\n                 n_lags: int,\n                 horizon: int,\n                 test_size: float,\n                 batch_size: int):\n        super().__init__()\n        self.data = data\n        self.batch_size = batch_size\n        self.test_size = test_size\n        self.n_lags = n_lags\n        self.horizon = horizon\n        self.training = None\n        self.validation = None\n        self.test = None\n        self.predict_set = None\n        self.target_scaler = LocalScaler()\n```", "```py\ndef transform(self, df: pd.DataFrame):\n    df = df.copy()\n    df[\"value\"] = LogTransformation.transform(df[\"value\"])\n    df_g = df.groupby(\"group_id\")\n    scaled_df_l = []\n    for g, df_ in df_g:\n        df_[[\"value\"]] = self.scalers[g].transform(df_[[\"value\"]])\n        scaled_df_l.append(df_)\n    scaled_df = pd.concat(scaled_df_l)\n    scaled_df = scaled_df.sort_index()\n    return scaled_df\n```", "```py\ndef setup(self, stage=None):\n    data_list = list(self.data.train)\n    data_list = [pd.Series(ts['target'],\n        index=pd.date_range(start=ts['start'].to_timestamp(),\n        freq=ts['start'].freq,\n        periods=len(ts['target'])))\n        for ts in data_list]\n    tseries_df = pd.concat(data_list, axis=1)\n    tseries_df['time_index'] = np.arange(tseries_df.shape[0])\n    ts_df = tseries_df.melt('time_index')\n    ts_df = ts_df.rename(columns={'variable': 'group_id'})\n    unique_times = ts_df['time_index'].sort_values().unique()\n    tr_ind, ts_ind = \\\n        train_test_split(unique_times,\n            test_size=self.test_size,\n            shuffle=False)\n    tr_ind, vl_ind = \\\n        train_test_split(tr_ind,\n            test_size=0.1,\n            shuffle=False)\n    training_df = ts_df.loc[ts_df['time_index'].isin(tr_ind), :]\n    validation_df = ts_df.loc[ts_df['time_index'].isin(vl_ind), :]\n    test_df = ts_df.loc[ts_df['time_index'].isin(ts_ind), :]\n    self.target_scaler.fit(training_df)\n    training_df = self.target_scaler.transform(training_df)\n    validation_df = self.target_scaler.transform(validation_df)\n    test_df = self.target_scaler.transform(test_df)\n    self.training = TimeSeriesDataSet(\n        data=training_df,\n        time_idx='time_index',\n        target='value',\n        group_ids=['group_id'],\n        max_encoder_length=self.n_lags,\n        max_prediction_length=self.horizon,\n        time_varying_unknown_reals=['value'],\n    )\n    self.validation = TimeSeriesDataSet.from_dataset\n        (self.training, validation_df)\n    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)\n    self.predict_set = TimeSeriesDataSet.from_dataset\n        (self.training, ts_df, predict=True)\n```", "```py\n    def train_dataloader(self):\n        return self.training.to_dataloader(batch_size=self.batch_size,\n            shuffle=False)\n    def val_dataloader(self):\n        return self.validation.to_dataloader\n            (batch_size=self.batch_size, shuffle=False)\n    def test_dataloader(self):\n        return self.test.to_dataloader(batch_size=self.batch_size,\n            shuffle=False)\n    def predict_dataloader(self):\n        return self.predict_set.to_dataloader(batch_size=1,\n            shuffle=False)\n```", "```py\ndatamodule = GlobalDataModule(data=dataset,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    test_size=0.2,\n    batch_size=1)\n```", "```py\nN_LAGS = 7\nHORIZON = 7\nfrom gluonts.dataset.repository.datasets import get_dataset, dataset_names\ndataset = get_dataset('nn5_daily_without_missing', regenerate=False)\ndatamodule = GlobalDataModule(data=dataset,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    batch_size=32,\n    test_size=0.3)\n```", "```py\nclass GlobalLSTM(pl.LightningModule):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n            batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.lstm.num_layers, x.size(0), \n            self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.lstm.num_layers, x.size(0), \n            self.hidden_dim).to(self.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n```", "```py\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        loss = F.mse_loss(y_pred, y[0])\n        self.log('train_loss', loss)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        loss = F.mse_loss(y_pred, y[0])\n        self.log('val_loss', loss)\n        return loss\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        loss = F.mse_loss(y_pred, y[0])\n        self.log('test_loss', loss)\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        return y_pred\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.01)\n```", "```py\nmodel = GlobalLSTM(input_dim=1,\n    hidden_dim=32,\n    num_layers=1,\n    output_dim=HORIZON)\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\",\n    min_delta=1e-4,\n    patience=10,\n    verbose=False,\n    mode=\"min\")\ntrainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])\ntrainer.fit(model, datamodule)\ntrainer.test(model=model, datamodule=datamodule)\nforecasts = trainer.predict(model=model, datamodule=datamodule)\n```", "```py\nN_LAGS = 7\nHORIZON = 7\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset('nn5_daily_without_missing', regenerate=False)\n```", "```py\nfrom sktime.transformations.series.fourier import FourierFeatures\ndef setup(self, stage=None):\n    […]\n    fourier = FourierFeatures(sp_list=[7],\n        fourier_terms_list=[2],\n        keep_original_columns=False)\n    fourier_features = fourier.fit_transform(ts_df['index'])\n    ts_df = pd.concat\n        ([ts_df, fourier_features], axis=1).drop('index', axis=1)\n    […]\n    self.training = TimeSeriesDataSet(\n        data=training_df,\n        time_idx='time_index',\n        target='value',\n        group_ids=['group_id'],\n        max_encoder_length=self.n_lags,\n        max_prediction_length=self.horizon,\n        time_varying_unknown_reals=['value'],\n        time_varying_known_reals=['sin_7_1', 'cos_7_1',\n            'sin_7_2', 'cos_7_2']\n    )\n```", "```py\nmodel = GlobalLSTM(input_dim=5,\n    hidden_dim=32,\n    num_layers=1,\n    output_dim=HORIZON)\ndatamodule = GlobalDataModuleSeas(data=dataset,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    batch_size=128,\n    test_size=0.3)\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\",\n    min_delta=1e-4,\n    patience=10,\n    verbose=False,\n    mode=\"min\")\ntrainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])\ntrainer.fit(model, datamodule)\ntrainer.test(model=model, datamodule=datamodule)\nforecasts = trainer.predict(model=model, datamodule=datamodule)\n```", "```py\npip install -U 'ray[data,train,tune,serve]'\n```", "```py\nclass GlobalDataModule(pl.LightningDataModule):\n    ...\nclass GlobalLSTM(pl.LightningModule):\n    ...\nfrom ray.train.lightning import RayTrainReportCallback\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.train import RunConfig, ScalingConfig, CheckpointConfig\nfrom ray.train.torch import TorchTrainer\n```", "```py\nsearch_space = {\n    \"hidden_dim\": tune.choice([8, 16, 32]),\n    \"num_layers\": tune.choice([1, 2]),\n}\n```", "```py\ndef train_tune(config_hyper):\n    hidden_dim = config_hyper[\"hidden_dim\"]\n    num_layers = config_hyper[\"num_layers\"]\n    model = GlobalLSTM(input_dim=1,\n        hidden_dim=hidden_dim,\n        output_dim=HORIZON,\n        num_layers=num_layers)\n    data_module = GlobalDataModule(dataset,\n        n_lags=N_LAGS,\n        horizon=HORIZON,\n        batch_size=128,\n        test_size=0.3)\n    trainer = Trainer(callbacks=[RayTrainReportCallback()])\n    trainer.fit(model, data_module)\n```", "```py\nscaling_config = ScalingConfig(\n    num_workers=2, use_gpu=False, \n        resources_per_worker={\"CPU\": 1, \"GPU\": 0}\n)\nrun_config = RunConfig(\n    checkpoint_config=CheckpointConfig(\n        num_to_keep=1,\n        checkpoint_score_attribute=\"val_loss\",\n        checkpoint_score_order=\"min\",\n    ),\n)\nray_trainer = TorchTrainer(\n    train_tune,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n```", "```py\nscheduler = ASHAScheduler(max_t=30, grace_period=1, reduction_factor=2)\ntuner = tune.Tuner(\n    ray_trainer,\n    param_space={\"train_loop_config\": search_space},\n    tune_config=tune.TuneConfig(\n        metric=\"val_loss\",\n        mode=\"min\",\n        num_samples=10,\n        scheduler=scheduler,\n    ),\n)\n```", "```py\nresults = tuner.fit()\nbest_model_conf = \\\n    results.get_best_result(metric='val_loss', mode='min')\n```", "```py\npath = best_model_conf.get_best_checkpoint(metric='val_loss',\n    mode='min').path\nconfig = best_model_conf.config['train_loop_config']\nbest_model = \\\n    GlobalLSTM.load_from_checkpoint(checkpoint_path=f'{path}/\n        checkpoint.ckpt',\n        **config)\ndata_module = GlobalDataModule(dataset, n_lags=7, horizon=3)\ntrainer = Trainer(max_epochs=30)\ntrainer.test(best_model, datamodule=data_module)\n```"]