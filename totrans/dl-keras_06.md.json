["```py\nfrom __future__ import print_function\nfrom keras.layers import Dense, Activation\nfrom keras.layers.recurrent import SimpleRNN\nfrom keras.models import Sequential\nfrom keras.utils.visualize_util import plot\nimport numpy as np\n\n```", "```py\nfin = open(\"../data/alice_in_wonderland.txt\", 'rb')\nlines = []\nfor line in fin:\n    line = line.strip().lower()\n    line = line.decode(\"ascii\", \"ignore\")\n    if len(line) == 0:\n        continue\n    lines.append(line)\nfin.close()\ntext = \" \".join(lines)\n\n```", "```py\nchars = set([c for c in text])\nnb_chars = len(chars)\nchar2index = dict((c, i) for i, c in enumerate(chars))\nindex2char = dict((i, c) for i, c in enumerate(chars))\n\n```", "```py\nSEQLEN = 10\nSTEP = 1\n\ninput_chars = []\nlabel_chars = []\nfor i in range(0, len(text) - SEQLEN, STEP):\n    input_chars.append(text[i:i + SEQLEN])\n    label_chars.append(text[i + SEQLEN])\n\n```", "```py\nit turned -> i\n t turned i -> n\n turned in -> t\nturned int -> o\nurned into ->\nrned into -> a\nned into a ->\ned into a -> p\nd into a p -> i\n into a pi -> g\n\n```", "```py\nX = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\ny = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\nfor i, input_char in enumerate(input_chars):\n    for j, ch in enumerate(input_char):\n        X[i, j, char2index[ch]] = 1\n    y[i, char2index[label_chars[i]]] = 1\n\n```", "```py\nHIDDEN_SIZE = 128\nBATCH_SIZE = 128\nNUM_ITERATIONS = 25\nNUM_EPOCHS_PER_ITERATION = 1\nNUM_PREDS_PER_EPOCH = 100\n\nmodel = Sequential()\nmodel.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n    input_shape=(SEQLEN, nb_chars),\n    unroll=True))\nmodel.add(Dense(nb_chars))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n\n```", "```py\nfor iteration in range(NUM_ITERATIONS):\n    print(\"=\" * 50)\n    print(\"Iteration #: %d\" % (iteration))\n    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n\n    test_idx = np.random.randint(len(input_chars))\n    test_chars = input_chars[test_idx]\n    print(\"Generating from seed: %s\" % (test_chars))\n    print(test_chars, end=\"\")\n    for i in range(NUM_PREDS_PER_EPOCH):\n        Xtest = np.zeros((1, SEQLEN, nb_chars))\n        for i, ch in enumerate(test_chars):\n            Xtest[0, i, char2index[ch]] = 1\n        pred = model.predict(Xtest, verbose=0)[0]\n        ypred = index2char[np.argmax(pred)]\n        print(ypred, end=\"\")\n        # move forward with test_chars + ypred\n        test_chars = test_chars[1:] + ypred\nprint()\n\n```", "```py\nfrom keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport os\n\n```", "```py\nmaxlen = 0\nword_freqs = collections.Counter()\nnum_recs = 0\nftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'rb')\nfor line in ftrain:\n    label, sentence = line.strip().split(\"t\")\n    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n    if len(words) > maxlen:\n        maxlen = len(words)\n    for word in words:\n        word_freqs[word] += 1\n    num_recs += 1\nftrain.close()\n\n```", "```py\nmaxlen : 42\nlen(word_freqs) : 2313\n\n```", "```py\nDATA_DIR = \"../data\"\n\nMAX_FEATURES = 2000\nMAX_SENTENCE_LENGTH = 40\n\n```", "```py\nvocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\nword2index = {x[0]: i+2 for i, x in\nenumerate(word_freqs.most_common(MAX_FEATURES))}\nword2index[\"PAD\"] = 0\nword2index[\"UNK\"] = 1\nindex2word = {v:k for k, v in word2index.items()}\n\n```", "```py\nX = np.empty((num_recs, ), dtype=list)\ny = np.zeros((num_recs, ))\ni = 0\nftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'rb')\nfor line in ftrain:\n    label, sentence = line.strip().split(\"t\")\n    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n    seqs = []\n    for word in words:\n        if word2index.has_key(word):\n            seqs.append(word2index[word])\n        else:\n            seqs.append(word2index[\"UNK\"])\n    X[i] = seqs\n    y[i] = int(label)\n    i += 1\nftrain.close()\nX = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n\n```", "```py\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n```", "```py\nEMBEDDING_SIZE = 128\nHIDDEN_LAYER_SIZE = 64\nBATCH_SIZE = 32\nNUM_EPOCHS = 10\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, EMBEDDING_SIZE,\ninput_length=MAX_SENTENCE_LENGTH))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1))\nmodel.add(Activation(\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n    metrics=[\"accuracy\"])\n\n```", "```py\nhistory = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n    validation_data=(Xtest, ytest))\n\n```", "```py\nplt.subplot(211)\nplt.title(\"Accuracy\")\nplt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\nplt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\nplt.legend(loc=\"best\")\n\nplt.subplot(212)\nplt.title(\"Loss\")\nplt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\nplt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\nplt.legend(loc=\"best\")\n\nplt.tight_layout()\nplt.show()\n\n```", "```py\nscore, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\nprint(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n\nfor i in range(5):\n    idx = np.random.randint(len(Xtest))\n    xtest = Xtest[idx].reshape(1,40)\n    ylabel = ytest[idx]\n    ypred = model.predict(xtest)[0][0]\n    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n    print(\"%.0ft%dt%s\" % (ypred, ylabel, sent))\n\n```", "```py\nfrom keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import GRU\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport nltk\nimport numpy as np\nimport os\n\n```", "```py\nDATA_DIR = \"../data\"\n\nfedata = open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"wb\")\nffdata = open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"wb\")\n\nsents = nltk.corpus.treebank.tagged_sents()\nfor sent in sents:\n    words, poss = [], []\n    for word, pos in sent:\n        if pos == \"-NONE-\":\n            continue\n        words.append(word)\n        poss.append(pos)\n    fedata.write(\"{:s}n\".format(\" \".join(words)))\n    ffdata.write(\"{:s}n\".format(\" \".join(poss)))\n\nfedata.close()\nffdata.close()\n\n```", "```py\ndef parse_sentences(filename):\n    word_freqs = collections.Counter()\n    num_recs, maxlen = 0, 0\n    fin = open(filename, \"rb\")\n    for line in fin:\n        words = line.strip().lower().split()\n        for word in words:\n            word_freqs[word] += 1\n        if len(words) > maxlen:\n            maxlen = len(words)\n        num_recs += 1\n    fin.close()\n    return word_freqs, maxlen, num_recs\n\n    s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(\n    os.path.join(DATA_DIR, \"treebank_sents.txt\"))\n    t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(\n    os.path.join(DATA_DIR, \"treebank_poss.txt\"))\nprint(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs)\n\n```", "```py\nMAX_SEQLEN = 250\nS_MAX_FEATURES = 5000\nT_MAX_FEATURES = 45\n\n```", "```py\ns_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2\ns_word2index = {x[0]:i+2 for i, x in\nenumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\ns_word2index[\"PAD\"] = 0\ns_word2index[\"UNK\"] = 1\ns_index2word = {v:k for k, v in s_word2index.items()}\n\nt_vocabsize = len(t_wordfreqs) + 1\nt_word2index = {x[0]:i for i, x in\nenumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\nt_word2index[\"PAD\"] = 0\nt_index2word = {v:k for k, v in t_word2index.items()}\n\n```", "```py\ndef build_tensor(filename, numrecs, word2index, maxlen,\n        make_categorical=False, num_classes=0):\n    data = np.empty((numrecs, ), dtype=list)\n    fin = open(filename, \"rb\")\n    i = 0\n    for line in fin:\n        wids = []\n        for word in line.strip().lower().split():\n            if word2index.has_key(word):\n                wids.append(word2index[word])\n            else:\n                wids.append(word2index[\"UNK\"])\n        if make_categorical:\n            data[i] = np_utils.to_categorical(wids, \n                num_classes=num_classes)\n        else:\n            data[i] = wids\n        i += 1\n    fin.close()\n    pdata = sequence.pad_sequences(data, maxlen=maxlen)\n    return pdata\n\nX = build_tensor(os.path.join(DATA_DIR, \"treebank_sents.txt\"),\n    s_numrecs, s_word2index, MAX_SEQLEN)\nY = build_tensor(os.path.join(DATA_DIR, \"treebank_poss.txt\"),\n    t_numrecs, t_word2index, MAX_SEQLEN, True, t_vocabsize)\n\n```", "```py\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n```", "```py\nEMBED_SIZE = 128\nHIDDEN_SIZE = 64\nBATCH_SIZE = 32\nNUM_EPOCHS = 1\n\nmodel = Sequential()\nmodel.add(Embedding(s_vocabsize, EMBED_SIZE,\ninput_length=MAX_SEQLEN))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(RepeatVector(MAX_SEQLEN))\nmodel.add(GRU(HIDDEN_SIZE, return_sequences=True))\nmodel.add(TimeDistributed(Dense(t_vocabsize)))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n    metrics=[\"accuracy\"])\n\n```", "```py\nmodel.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n    validation_data=[Xtest, Ytest])\n\nscore, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\nprint(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n\n```", "```py\nfrom keras.layers.recurrent import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(s_vocabsize, EMBED_SIZE,\ninput_length=MAX_SEQLEN))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(RepeatVector(MAX_SEQLEN))\nmodel.add(GRU(HIDDEN_SIZE, return_sequences=True))\nmodel.add(TimeDistributed(Dense(t_vocabsize)))\nmodel.add(Activation(\"softmax\"))\n\n```", "```py\nfrom keras.layers.wrappers import Bidirectional\n\nmodel = Sequential()\nmodel.add(Embedding(s_vocabsize, EMBED_SIZE,\ninput_length=MAX_SEQLEN))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(Bidirectional(LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(RepeatVector(MAX_SEQLEN))\nmodel.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\nmodel.add(TimeDistributed(Dense(t_vocabsize)))\nmodel.add(Activation(\"softmax\"))\n\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport re\n\nDATA_DIR = \"../data\"\n\nfld = open(os.path.join(DATA_DIR, \"LD2011_2014.txt\"), \"rb\")\ndata = []\ncid = 250\nfor line in fld:\n    if line.startswith(\"\"\";\"):\n        continue\n    cols = [float(re.sub(\",\", \".\", x)) for x in \n                line.strip().split(\";\")[1:]]\n    data.append(cols[cid])\nfld.close()\n\nNUM_ENTRIES = 1000\nplt.plot(range(NUM_ENTRIES), data[0:NUM_ENTRIES])\nplt.ylabel(\"electricity consumption\")\nplt.xlabel(\"time (1pt = 15 mins)\")\nplt.show()\n\nnp.save(os.path.join(DATA_DIR, \"LD_250.npy\"), np.array(data))\n\n```", "```py\nfrom keras.layers.core import Dense\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport math\nimport os\n\n```", "```py\nDATA_DIR = \"../data\"\n\ndata = np.load(os.path.join(DATA_DIR, \"LD_250.npy\"))\ndata = data.reshape(-1, 1)\nscaler = MinMaxScaler(feature_range=(0, 1), copy=False)\ndata = scaler.fit_transform(data)\n\n```", "```py\nX = np.zeros((data.shape[0], NUM_TIMESTEPS))\nY = np.zeros((data.shape[0], 1))\nfor i in range(len(data) - NUM_TIMESTEPS - 1):\n    X[i] = data[i:i + NUM_TIMESTEPS].T\n    Y[i] = data[i + NUM_TIMESTEPS + 1]\n\n# reshape X to three dimensions (samples, timesteps, features)\nX = np.expand_dims(X, axis=2)\n\n```", "```py\nsp = int(0.7 * len(data))\nXtrain, Xtest, Ytrain, Ytest = X[0:sp], X[sp:], Y[0:sp], Y[sp:]\nprint(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n\n```", "```py\nNUM_TIMESTEPS = 20\nHIDDEN_SIZE = 10\nBATCH_SIZE = 96 # 24 hours (15 min intervals)\n\n# stateless\nmodel = Sequential()\nmodel.add(LSTM(HIDDEN_SIZE, input_shape=(NUM_TIMESTEPS, 1), \n    return_sequences=False))\nmodel.add(Dense(1))\n\n```", "```py\n# stateful\nmodel = Sequential()\nmodel.add(LSTM(HIDDEN_SIZE, stateful=True,\n    batch_input_shape=(BATCH_SIZE, NUM_TIMESTEPS, 1), \n    return_sequences=False))\nmodel.add(Dense(1))\n\n```", "```py\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\",\n    metrics=[\"mean_squared_error\"])\n\n```", "```py\nBATCH_SIZE = 96 # 24 hours (15 min intervals)\n\n# stateless\nmodel.fit(Xtrain, Ytrain, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n    validation_data=(Xtest, Ytest),\n    shuffle=False)\n\n```", "```py\nBATCH_SIZE = 96 # 24 hours (15 min intervals)\n\n# stateful\n# need to make training and test data to multiple of BATCH_SIZE\ntrain_size = (Xtrain.shape[0] // BATCH_SIZE) * BATCH_SIZE\ntest_size = (Xtest.shape[0] // BATCH_SIZE) * BATCH_SIZE\nXtrain, Ytrain = Xtrain[0:train_size], Ytrain[0:train_size]\nXtest, Ytest = Xtest[0:test_size], Ytest[0:test_size]\nprint(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\nfor i in range(NUM_EPOCHS):\n    print(\"Epoch {:d}/{:d}\".format(i+1, NUM_EPOCHS))\n    model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=1,\n        validation_data=(Xtest, Ytest),\n        shuffle=False)\n    model.reset_states()\n\n```", "```py\nscore, _ = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\nrmse = math.sqrt(score)\nprint(\"MSE: {:.3f}, RMSE: {:.3f}\".format(score, rmse))\n\n```"]