["```py\nimport typing as tt \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \n\nHIDDEN_SIZE = 128 \nBATCH_SIZE = 16 \nPERCENTILE = 70\n```", "```py\nclass Net(nn.Module): \n    def __init__(self, obs_size: int, hidden_size: int, n_actions: int): \n        super(Net, self).__init__() \n        self.net = nn.Sequential( \n            nn.Linear(obs_size, hidden_size), \n            nn.ReLU(), \n            nn.Linear(hidden_size, n_actions) \n        ) \n\n    def forward(self, x: torch.Tensor): \n        return self.net(x)\n```", "```py\n@dataclass \nclass EpisodeStep: \n    observation: np.ndarray \n    action: int \n\n@dataclass \nclass Episode: \n    reward: float \n    steps: tt.List[EpisodeStep]\n```", "```py\ndef iterate_batches(env: gym.Env, net: Net, batch_size: int) -> tt.Generator[tt.List[Episode], None, None]: \n    batch = [] \n    episode_reward = 0.0 \n    episode_steps = [] \n    obs, _ = env.reset() \n    sm = nn.Softmax(dim=1)\n```", "```py\n while True: \n        obs_v = torch.tensor(obs, dtype=torch.float32) \n        act_probs_v = sm(net(obs_v.unsqueeze(0))) \n        act_probs = act_probs_v.data.numpy()[0]\n```", "```py\n action = np.random.choice(len(act_probs), p=act_probs) \n        next_obs, reward, is_done, is_trunc, _ = env.step(action)\n```", "```py\n episode_reward += float(reward) \n        step = EpisodeStep(observation=obs, action=action) \n        episode_steps.append(step)\n```", "```py\n if is_done or is_trunc: \n            e = Episode(reward=episode_reward, steps=episode_steps) \n            batch.append(e) \n            episode_reward = 0.0 \n            episode_steps = [] \n            next_obs, _ = env.reset() \n            if len(batch) == batch_size: \n                yield batch \n                batch = []\n```", "```py\n obs = next_obs\n```", "```py\ndef filter_batch(batch: tt.List[Episode], percentile: float) -> \\ \n        tt.Tuple[torch.FloatTensor, torch.LongTensor, float, float]: \n    rewards = list(map(lambda s: s.reward, batch)) \n    reward_bound = float(np.percentile(rewards, percentile)) \n    reward_mean = float(np.mean(rewards))\n```", "```py\n train_obs: tt.List[np.ndarray] = [] \n    train_act: tt.List[int] = [] \n    for episode in batch: \n        if episode.reward < reward_bound: \n            continue \n        train_obs.extend(map(lambda step: step.observation, episode.steps)) \n        train_act.extend(map(lambda step: step.action, episode.steps))\n```", "```py\n train_obs_v = torch.FloatTensor(np.vstack(train_obs)) \n    train_act_v = torch.LongTensor(train_act) \n    return train_obs_v, train_act_v, reward_bound, reward_mean\n```", "```py\nif __name__ == \"__main__\": \n    env = gym.make(\"CartPole-v1\") \n    assert env.observation_space.shape is not None \n    obs_size = env.observation_space.shape[0] \n    assert isinstance(env.action_space, gym.spaces.Discrete) \n    n_actions = int(env.action_space.n) \n\n    net = Net(obs_size, HIDDEN_SIZE, n_actions) \n    print(net) \n    objective = nn.CrossEntropyLoss() \n    optimizer = optim.Adam(params=net.parameters(), lr=0.01) \n    writer = SummaryWriter(comment=\"-cartpole\")\n```", "```py\n for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): \n        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE) \n        optimizer.zero_grad() \n        action_scores_v = net(obs_v) \n        loss_v = objective(action_scores_v, acts_v) \n        loss_v.backward() \n        optimizer.step()\n```", "```py\n print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % ( \n            iter_no, loss_v.item(), reward_m, reward_b)) \n        writer.add_scalar(\"loss\", loss_v.item(), iter_no) \n        writer.add_scalar(\"reward_bound\", reward_b, iter_no) \n        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n```", "```py\n if reward_m > 475: \n            print(\"Solved!\") \n            break \n    writer.close()\n```", "```py\nChapter04$ ./01_cartpole.py \nNet( \n  (net): Sequential( \n   (0): Linear(in_features=4, out_features=128, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=128, out_features=2, bias=True) \n  ) \n) \n0: loss=0.683, reward_mean=25.2, rw_bound=24.0 \n1: loss=0.669, reward_mean=34.3, rw_bound=39.0 \n2: loss=0.648, reward_mean=37.6, rw_bound=40.0 \n3: loss=0.647, reward_mean=41.9, rw_bound=43.0 \n4: loss=0.634, reward_mean=41.2, rw_bound=50.0 \n.... \n38: loss=0.537, reward_mean=431.8, rw_bound=500.0 \n39: loss=0.529, reward_mean=450.1, rw_bound=500.0 \n40: loss=0.533, reward_mean=456.4, rw_bound=500.0 \n41: loss=0.526, reward_mean=422.0, rw_bound=500.0 \n42: loss=0.531, reward_mean=436.8, rw_bound=500.0 \n43: loss=0.526, reward_mean=475.5, rw_bound=500.0 \nSolved!\n```", "```py\n env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\") \n    env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n```", "```py\nChapter04$ ./01_cartpole.py \nNet( \n  (net): Sequential( \n   (0): Linear(in_features=4, out_features=128, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=128, out_features=2, bias=True) \n  ) \n) \nMoviepy - Building video Chapter04/video/rl-video-episode-0.mp4\\. \nMoviepy - Writing video Chapter04/video/rl-video-episode-0.mp4 \nMoviepy - Done ! \nMoviepy - video ready Chapter04/video/rl-video-episode-0.mp4 \nMoviepy - Building video Chapter04/video/rl-video-episode-1.mp4\\. \nMoviepy - Writing video Chapter04/video/rl-video-episode-1.mp4 \n...\n```", "```py\n>>> e = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\") \n>>> e.observation_space \nDiscrete(16) \n>>> e.action_space \nDiscrete(4) \n>>> e.reset() \n(0, {’prob’: 1}) \n>>> print(e.render()) \n\nSFFF \nFHFH \nFFFH \nHFFG\n```", "```py\nclass DiscreteOneHotWrapper(gym.ObservationWrapper): \n    def __init__(self, env: gym.Env): \n        super(DiscreteOneHotWrapper, self).__init__(env) \n        assert isinstance(env.observation_space, gym.spaces.Discrete) \n        shape = (env.observation_space.n, ) \n        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32) \n\n    def observation(self, observation): \n        res = np.copy(self.observation_space.low) \n        res[observation] = 1.0 \n        return res\n```", "```py\ndef filter_batch(batch: tt.List[Episode], percentile: float) -> \\ \n        tt.Tuple[tt.List[Episode], tt.List[np.ndarray], tt.List[int], float]: \n    reward_fun = lambda s: s.reward * (GAMMA ** len(s.steps)) \n    disc_rewards = list(map(reward_fun, batch)) \n    reward_bound = np.percentile(disc_rewards, percentile) \n\n    train_obs: tt.List[np.ndarray] = [] \n    train_act: tt.List[int] = [] \n    elite_batch: tt.List[Episode] = [] \n\n    for example, discounted_reward in zip(batch, disc_rewards): \n        if discounted_reward > reward_bound: \n            train_obs.extend(map(lambda step: step.observation, example.steps)) \n            train_act.extend(map(lambda step: step.action, example.steps)) \n            elite_batch.append(example) \n\n    return elite_batch, train_obs, train_act, reward_bound\n```", "```py\n full_batch = [] \n    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): \n        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) \n        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE) \n        if not full_batch: \n            continue \n        obs_v = torch.FloatTensor(obs) \n        acts_v = torch.LongTensor(acts) \n        full_batch = full_batch[-500:]\n```", "```py\n env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\", is_slippery=False))\n```", "```py\nChapter04$ ./04_frozenlake_nonslippery.py \n2: loss=1.436, rw_mean=0.010, rw_bound=0.000, batch=1 \n3: loss=1.410, rw_mean=0.010, rw_bound=0.000, batch=2 \n4: loss=1.391, rw_mean=0.050, rw_bound=0.000, batch=7 \n5: loss=1.379, rw_mean=0.020, rw_bound=0.000, batch=9 \n6: loss=1.375, rw_mean=0.010, rw_bound=0.000, batch=10 \n7: loss=1.367, rw_mean=0.040, rw_bound=0.000, batch=14 \n8: loss=1.361, rw_mean=0.000, rw_bound=0.000, batch=14 \n9: loss=1.356, rw_mean=0.010, rw_bound=0.000, batch=15 \n... \n134: loss=0.308, rw_mean=0.730, rw_bound=0.478, batch=93 \n136: loss=0.440, rw_mean=0.710, rw_bound=0.304, batch=70 \n137: loss=0.298, rw_mean=0.720, rw_bound=0.478, batch=106 \n139: loss=0.337, rw_mean=0.790, rw_bound=0.430, batch=65 \n140: loss=0.295, rw_mean=0.720, rw_bound=0.478, batch=99 \n142: loss=0.433, rw_mean=0.670, rw_bound=0.000, batch=67 \n143: loss=0.287, rw_mean=0.820, rw_bound=0.478, batch=114 \nSolved!\n```"]