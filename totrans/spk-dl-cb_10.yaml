- en: Face Recognition Using Deep Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and loading the MIT-CBCL dataset into the memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting and visualizing images from the directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model building, training, and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's world, the need to maintain the security of information is becoming
    increasingly important, as well as increasingly difficult. There are various methods
    by which this security can be enforced (passwords, fingerprint IDs, PIN numbers,
    and so on). However, when it comes to ease of use, accuracy, and low intrusiveness,
    face recognition algorithms have been doing very well. With the availability of
    high-speed computing and the evolution of deep convolutional networks, it has
    been made possible to further increase the robustness of these algorithms. They
    have gotten so advanced that they are now being used as the primary security feature
    in many electronic devices (for example, iPhoneX) and even banking applications.
    The goal of this chapter is to develop a robust, pose-invariant face recognition
    algorithm for use in security systems. For the purposes of this chapter, we will
    be using the openly available `MIT-CBCL` dataset of face images of 10 different
    subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and loading the MIT-CBCL dataset into the memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will understand how to download the MIT-CBCL dataset and
    load it into the memory.
  prefs: []
  type: TYPE_NORMAL
- en: With a predicted worth of $15 billion by 2025, the biometrics industry is poised
    to grow like never before. Some of the examples of physiological characteristics
    used for biometric authentication include fingerprints, DNA, face, retina or ear
    features, and voice. While technologies such as DNA authentication and fingerprints
    are quite advanced, face recognition brings its own advantages to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Ease of use and robustness due to recent developments in deep learning models
    are some of the driving factors behind face recognition algorithms gaining so
    much popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following key points need to be considered for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MIT-CBCL` dataset is composed of 3,240 images (324 images per subject).
    In our model, we will make arrangements to augment the data in order to increase
    model robustness. We will employ techniques such as shifting the subject, rotation,
    zooming, and shearing of the subject to obtain this augmented data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use 20% of the dataset to test our model (648 images) by randomly selecting
    these images from the dataset. Similarly, we randomly select 80% of the images
    in the dataset and use this as our training dataset (2,592 images).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest challenge is cropping the images to the exact same size so that
    they can be fed into the neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a known fact that it is much easier to design a network when all the input
    images are of the same size. However, since some of the subjects in these images
    have a side profile or rotated/tilted profiles, we have to adapt our network to
    take input images of different sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The steps are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `MIT-CBCL` dataset by visiting the FACE RECOGNITION HOMEPAGE,
    which contains a number of databases for face recognition experiments. The link,
    as well as a screenshot of the homepage, is provided as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.face-rec.org/databases/](http://www.face-rec.org/databases/):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/dcf4205e-a1b1-4f44-8376-2232d9d0f29a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Navigate down to the link that is named MIT-CBCL Face Recognition Database
    and click on it, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1519d67a-ca1b-4fde-9f75-b4ee218f11e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you have clicked on it, it will take you to a license page on which you
    are required to accept the license agreement and proceed to the download page.
    Once on the download page, click on `download now`. This downloads a zip file
    of about 116 MB. Go ahead and extract the contents into the working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The license agreement requires the appropriate citation for the use of the database
    in any projects. This database was developed by the research team from the Massachusetts
    Institute of Technology.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Credit is hereby given to the Massachusetts Institute of Technology and to the
    center for biological and computational learning for providing the database of
    facial images. The license also requires the mentioning of the paper titled *Component-based
    Face Recognition with 3D Morphable Models, First IEEE Workshop on Face Processing
    in Video,* Washington, D.C., 2004, B. Weyrauch, J. Huang, B. Heisele, and V. Blanz.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot describes the license agreement as well as the link
    to download the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9858e3c4-eb48-4590-92c2-bf365cc2a843.png)'
  prefs: []
  type: TYPE_IMG
- en: Face Recognition Database Homepage
  prefs: []
  type: TYPE_NORMAL
- en: Once the dataset is downloaded and extracted, you will see a folder titled MIT-CBCL-facerec-database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the purposes of this chapter, we will only be using the images in the **`training-synthetic`**
    folder, which contains all 3,240 images, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/138da0da-a780-4240-a60e-2ff5f944cbd9.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will require the following libraries to be imported by
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`os`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keras`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorFlow`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section of the chapter will deal with importing the necessary
    libraries and preprocessing the images before building the neural network model
    and loading them into it.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For complete information on the packages used in this chapter, visit the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://matplotlib.org/](https://matplotlib.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/2/library/os.html](https://docs.python.org/2/library/os.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy-1.9.1/reference/](https://docs.scipy.org/doc/numpy-1.9.1/reference/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting and visualizing images from the directory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will describe how to read and visualize the downloaded images before
    they are preprocessed and fed into the neural network for training. This is an
    important step in this chapter because the images need to be visualized to get
    a better understanding of the image sizes so they can be accurately cropped to
    omit the background and preserve only the necessary facial features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before beginning, complete the initial setup of importing the necessary libraries
    and functions as well as setting the path of the working directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the necessary libraries using the following lines of code. The output
    must result in a line that says `Using TensorFlow backend`, as shown in the screenshot
    that follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The importing of the libraries is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22238e64-6ac9-41c1-b103-7c46992e285e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print and set the current working directory as shown in the following screenshot.
    In our case, the desktop was set as the working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/06db13f8-4651-4e03-9bba-45a894469c1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Read all the images directly from the folder by using the commands illustrated
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e1bdd972-61a3-4a30-8463-afff8c8f6d86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Print a few random images from the dataset using the `plt.imshow (images[])`
    command, as shown in the following screenshots, to get a better idea of the face
    profiles in the images. This will also give an idea of the size of the image,
    which will be required at a later stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/22170372-8478-4b66-a6ba-646c59aed38b.png)'
  prefs: []
  type: TYPE_IMG
- en: Shown here are the images of different test subjects from the first image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7fd9236-5600-4f17-80c0-f1882d6d528f.png)![](img/4165425e-c59f-4eb3-bb12-457e8f5026ce.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/895e8032-8b1c-4c90-944e-f3b210adc0af.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `mypath` variable sets the path to read all the files from. The `training-synthetic`
    folder is specified in this step, as only the files in this folder are going to
    be used for this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `onlyfiles` variable is used in order to count all the files under the folder
    whose path is provided in the previous step by looping through all the files contained
    in the folder. This will be required in the next step for reading and storing
    the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `images` variable is used to create an empty array of size 3,240 in order
    to store the images, which are all 200 x 200-pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, by looping through all the files using the `onlyfiles` variable as an
    argument in the for loop, each image contained in the folder is read and stored
    into the previously defined `images` array using the `matplotlib.image` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, on printing randomly chosen images by specifying different indices
    of the images you will notice that each image is a 200 x 200-pixel array and each
    subject may either be facing forward or rotated between zero and fifteen degrees
    on either side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points are of note:'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting feature of this database is that the fourth digit of each filename
    describes which subject is in the respective image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names of the images are unique in the sense that the fourth digit represents
    the individual in the respective image. Two examples of image names are `0001_-4_0_0_60_45_1.pgm` and
    `0006_-24_0_0_0_75_15_1.pgm`. One can easily understand that the fourth digits
    represent the second and seventh individual respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will need to store this information for later use while making predictions.
    This will help the neural network during training by knowing what subject's facial
    features it is learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The filenames of each image can be read into an array, and each of the ten
    subjects can be segregated by using the following lines of code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will initialize an empty one-dimensional `numpy` array of
    size 3,240 (the number of images in the `training-synthetic` folder) and store
    the relevant subjects in different arrays by looping through the whole set of
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `if` statements are basically checking what the fourth digit is under each
    filename and storing that digit in the initialized `numpy` array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output in the iPython notebook for the same is shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4a2396a6-3cea-4144-a3f4-c3089ae3beb7.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following blog describes a method of cropping images in Python and can
    be used for image preprocessing which will be required in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/](https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More information about the Adam Optimizer and its use cases can be found by
    visiting the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ](https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you may have noticed how all the images are not a front
    view of the face profiles, and that there are also slightly rotated side profiles.
    You may also have noticed some unnecessary background areas in each image that
    needs to be omitted. This section will describe how to preprocess and handle the
    images so that they are ready to be fed into the network for training.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A lot of algorithms are devised to crop the significant part of an image; for
    example, SIFT, LBP, Haar-cascade filter, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will, however, tackle this problem with a very simplistic naïve code to
    crop the facial portion from the image. This is one of the novelties of this algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have found that the pixel intensity of the unnecessary background part is
    28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that each image is a three-channel matrix of 200 x 200-pixels. This
    means that every image contains three matrices or Tensors of red, green, and blue
    pixels with an intensity ranging from 0 to 255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, we will discard any row or column of the images that contain only
    28s as the pixel intensities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also make sure that all the images have the same pixel size after the
    cropping action to achieve the highest parallelizability of the convolutional
    neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `crop()` function to crop images to obtain only the significant
    part, as shown in the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following lines of code to loop through every image in the folder and
    crop it using the preceding defined function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, randomly choose an image and print it to check that it has been cropped
    from a 200 x 200 sized image to a different size. We have chosen image 23 in our
    case. This can be done using the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, split the data into a test and train set using `80%` of the images in
    the folder as the training set and the remaining `20% `as the test set. This can
    be done with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data has finished splitting, segregate the training and test images
    using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, reshape all the cropped images into sizes of 128 x 150, since this is
    the size that is to be fed into the neural network. This can be done using the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is done reshaping, convert it into `float32` type, which will
    make it easier to handle in the next step when it is normalized. Converting from
    int to float32 can be done using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After reshaping and converting the data into the float32 type, it has to be
    normalized in order to adjust all the values to a similar scale. This is an important
    step in preventing data redundancy. Perform normalization using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to convert the reshaped, normalized images into vectors,
    as this is the only form of input the neural network understands. Convert the
    images into vectors using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `crop()` function executes the following tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiplies all pixels with an intensity of 28 with a numpy array of 1s and stores
    in variable `a`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Checks for all instances where an entire column consists of only pixel intensities
    of 28 and stores in variable `b`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deletes all columns (or *Y* axes) where pixel intensities are 28 for the entire
    column.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plots the resulting image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transposes the image in order to perform the preceding set of operations on
    all the rows (or *X* axes) in a similar manner.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiplies all pixels with an intensity of 28 with a `numpy` array of 1s and
    stores in variable `d`.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: Checks for all instances where an entire column consists of only pixel intensities
    of 28 and stores in variable `e`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deletes all columns (from the transposed image) where pixel intensities are
    28 for the entire column.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transposes the image to get back the original image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Prints the shape of the image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Wherever a pixel intensity of less than 29 is found, replaces those pixel intensities
    with zeros, which will result in the cropping of all those pixels by making them
    white.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plots the resulting image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshapes the resulting image to a size of 150 x 128 pixels.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output for the `crop()` function, as seen on the Jupyter notebook during
    execution, is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7bfb81b-bb1a-4f8d-ace1-6a3350b8b618.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the defined `crop()` function is applied to all the files contained in
    the `training-synthetic` folder by looping through every file. This will result
    in an output as shown in the following screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/659f69f2-71dc-47c8-a41d-8abaff1fbbce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output continues as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33234eb4-b5c5-4825-a908-9b1a5f58455a.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that only the relevant facial features are preserved and the resulting
    shapes of all the cropped images are less than 200 x 200, which was the initial
    size.
  prefs: []
  type: TYPE_NORMAL
- en: On printing the image and shape of any random image, you will notice that every
    image is now resized to a 150 x 128-pixel array, and you will see the following
    output:![](img/82830a1d-393e-4620-9e03-fefd30f652fd.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting the images into test and train sets as well as segregating them into
    variables named `x_train`, `y1_train`, `x_test`, and `y1_test` will result in
    the output shown in the following screenshot:![](img/b6480e9c-aed7-44ed-aebf-d18dbfcfb3f2.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Segregating the data is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a8dbcce6-9e1e-495d-b135-e87c64fe5c58.png)'
  prefs: []
  type: TYPE_IMG
- en: Reshaping the training and test images and converting the data type to float32
    will result in the output seen in the following screenshot:![](img/3ee9a218-e25c-4f53-bcfc-631ea9dc78ea.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the images are done preprocessing they still need to be normalized and
    converted into vectors (in this case tensors) before being fed into the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization, in the simplest case, means adjusting values measured on different
    scales to a notionally common scale, often prior to averaging. It is always a
    good idea to normalize data in order to prevent gradients from exploding or vanishing
    as seen in the vanishing and exploding gradient problems during gradient descent.
    Normalization also ensures there is no data redundancy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of the data is done by dividing each pixel in each image by `255`
    since the pixel values range between 0 and `255`. This will result in the output
    shown in the following screenshot:![](img/cfeaed2d-a914-4816-aecf-05d58a71e66b.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the images are converted to input vectors with ten different classes
    using the `to_categorical()` function from `numpy_utils`, as shown in the following
    screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4dc71bb9-c7cb-4cba-bae7-f2c5d78246e9.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additional resources are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on data normalization, check the following link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.quora.com/What-is-normalization-in-machine-learning](https://www.quora.com/What-is-normalization-in-machine-learning)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For information on overfitting and why data is split into test and training
    sets, visit the following link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For more information on encoding variables and their importance, visit the
    following link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://pbpython.com/categorical-encoding.html](http://pbpython.com/categorical-encoding.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Model building, training, and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a standard sequential model from the `keras` library to build the
    CNN. The network will consist of three convolutional layers, two maxpooling layers,
    and four fully connected layers. The input layer and the subsequent hidden layers
    have 16 neurons, while the maxpooling layers contain a pool size of (2,2). The
    four fully connected layers consist of two dense layers and one flattened layer
    and one dropout layer. Dropout 0.25 was used to reduce the overfitting problem.
    Another novelty of this algorithm is the use of data augmentation to fight the
    overfitting phenomenon. Data augmentation is carried by rotating, shifting, shearing,
    and zooming the images to different extents to fit the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `relu` function is used as the activation function in both the input and
    hidden layers, while the `softmax` classifier is used in the output layer to classify
    the test images based on the predicted output.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The network which will be constructed can be visualized as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93d4ebf3-1c06-4776-b868-dc7838a9294f.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model using the `Sequential()` function in the Keras framework using
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Print the summary of the model to get a better understanding of how the model
    is built and to ensure that it is built as per the preceding specifications. This
    can be done by using the `model.summary()` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, compile the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to prevent overfitting and improve model accuracy further, implement
    some form of data augmentation. In this step, the images will be sheared, shifted
    on a horizontal as well as the vertical axis, zoomed in, and rotated. The ability
    of the model to learn and identify these anomalies will dictate how robust the
    model is. Augment the data using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, fit and evaluate the model after data augmentation using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the sequential function, a nine-layer convolutional neural network
    is defined with each layer performing the following functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first layer is a convolutional layer with 16 neurons and performs convolution
    on the input tensor/matrix. The size of the feature map is defined to be a 3 x
    3 matrix. The input shape needs to be specified for the first layer since the
    neural network needs to know what type of input to expect. Since all the images
    have been cropped to a size of 128 x 150 pixels, this will be the input shape
    defined for the first layer of the network as well. The activation function used
    in this layer is a **rectified linear unit** (**relu**).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The second layer of the network (first hidden layer) is another convolution
    layer with 16 neurons as well. Again, a `relu` will be used as the activation
    function for this layer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The third layer of the network (second hidden layer) is a max pooling layer
    with a pool size of 2 x 2\. The function of this layer is to extract all the valid
    features learned by performing convolution in the first two layers and reducing
    the size of the matrix with all the learned features. Convolution is nothing but
    a matrix multiplication between the feature map and the input matrix (in our case,
    an image). The resulting values, which form the convolution process, are stored
    by the network in a matrix. The maximum values from these stored values will define
    a certain feature in the input image. These maximum values are what will be preserved
    by the max pooling layer, which will omit the non-relevant features.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The fourth layer of the network (third hidden layer) is another convolutional
    layer with a feature map of 3 x 3 again. The activation function used in this
    layer will again be a `relu` function.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth layer of the network (fourth hidden layer) is a max pooling layer
    with a pool size of 2 x 2.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The sixth layer of the network (fifth hidden layer) is a flatten layer that
    will convert the matrix containing all the learned features (stored in the form
    of numbers) into a single row instead of a multi-dimensional matrix.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The seventh layer in the network (sixth hidden layer) is a dense layer with
    512 neurons and a `relu` activation. Each neuron will basically process a certain
    weight and bias, which is nothing but a representation of all the learned features
    from a particular image. This is done in order to easily classify the image by
    using a `softmax` classifier on the dense layer.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: The eighth layer in the network (seventh hidden layer) is a dropout layer with
    a dropout probability of 0.25 or 25%. This layer will randomly `dropout` 25% of
    the neurons during the training process and help prevent overfitting by encouraging
    the network to learn a given feature using many alternative paths.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The final layer in the network is a dense layer with just 10 neurons and the
    `softmax` classifier. This is the eighth hidden layer and will also serve as the
    output layer of the network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output after defining the model must look like the one in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/84106f1b-84f4-4055-8308-7482d1c56abc.png)'
  prefs: []
  type: TYPE_IMG
- en: On printing the `model.summary()` function, you must see an output like the
    one in the following screenshot:![](img/b68373e3-00ab-4a55-83cd-b797ac325262.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The model is compiled using categorical crossentropy, which is a function to
    measure and compute the loss from the network while transferring information from
    one layer to the subsequent layers. The model will make use of the `Adam()` optimizer
    function from the Keras framework, which will basically dictate how the network
    optimizes the weights and biases while learning the features. The output of the
    `model.compile()` function must look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f159955e-5e31-47ef-89bc-ee6d45165b31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the neural network is quite dense and the number of total images is only
    3,240, we devise a method to prevent overfitting. This is done by generating more
    images from the training set by performing data augmentation. In this step, the
    images are generated through the `ImageDataGenerator()` function. This function
    takes the training and test sets and augments images by:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rotating them
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shearing them
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting the width, which is basically widening the images
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting the images on a horizontal axis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting the images on a vertical axis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the preceding function must look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbfa63f1-cb81-4f83-a17b-4e7356ad096e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the model is fitted to the data and evaluated after training over
    5 epochs. The output we obtained is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/92f8e4d9-e9f4-4c41-bf72-f6ad2cb50118.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we obtained an accuracy of 98.46%, which resulted in an error
    rate of 1.54%. This is pretty good, but convolutional networks have advanced so
    much that we can improve this error rate by tuning a few hyperparameters or using
    a deeper network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using a deeper CNN with 12 layers (one extra convolution and one extra max
    pooling layer) resulted in an improvement of accuracy to 99.07%, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44198743-7e72-4c9d-a09f-48339f26cebf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using data normalization after every two layers during model building, we were
    further able to improve the accuracy to 99.85%, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72303ea6-b9b5-4037-978c-64b9e1812be1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You may obtain different results, but feel free to run the training step a
    few times. The following are some of the steps you can take to experiment with
    the network in the future to understand it better:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to tune hyperparameters better and implement a higher dropout percentage
    and see how the network responds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy greatly reduced when we tried using different activation functions
    or a smaller (less dense) network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, change the size of the feature maps and max pooling layer and see how
    this influences training time and model accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try including more neurons in a less dense CNN and tune it to improve accuracy.
    This may also result in a faster network that trains in less time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use more training data. Explore other online repositories and find larger databases
    to train the network. Convolutional neural networks usually perform better when
    the size of the training data is increased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following published papers are good resources to obtain a better understanding
    of convolutional neural networks. They may be used as further reading in order
    to gain more understanding of various applications of convolutional neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://dl.acm.org/citation.cfm?id=2807412](https://dl.acm.org/citation.cfm?id=2807412)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/abstract/document/6165309/](https://ieeexplore.ieee.org/abstract/document/6165309/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425](http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ieeexplore.ieee.org/abstract/document/6288864/](https://ieeexplore.ieee.org/abstract/document/6288864/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
