["```py\nChapter19$ ./01_a2c.py --dev cuda -n v0 --save save/v0 --db-path db-v0 \nA.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) \n[Powered by Stella] \nAtariA2C( \n  (conv): Sequential( \n   (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) \n   (1): ReLU() \n   (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) \n   (3): ReLU() \n   (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) \n   (5): ReLU() \n   (6): Flatten(start_dim=1, end_dim=-1) \n  ) \n  (policy): Sequential( \n   (0): Linear(in_features=3136, out_features=512, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=512, out_features=18, bias=True) \n  ) \n  (value): Sequential( \n   (0): Linear(in_features=3136, out_features=512, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=512, out_features=1, bias=True) \n  ) \n) \n0: Testing model... \nGot best reward 40.00 and steps 213.0 in 10 episodes \n1024: done 1 games, mean reward 0.000, steps 70, speed 312.22 f/s \n1056: done 2 games, mean reward 0.000, steps 72, speed 1188.69 f/s \n1104: done 3 games, mean reward 0.000, steps 75, speed 1216.18 f/s\n```", "```py\n# how many transitions to store in episode \nEPISODE_STEPS = 50 \n# probability to start episode recording \nSTART_PROB = 0.00005 \n\n@dataclass(frozen=True) \nclass EpisodeStep: \n    obs: np.ndarray \n    act: int \n\nclass EpisodeRecorderWrapper(gym.Wrapper): \n    def __init__(self, env: gym.Env, db_path: pathlib.Path, env_idx: int, \n                 start_prob: float = START_PROB, steps_count: int = EPISODE_STEPS): \n        super().__init__(env) \n        self._store_path = db_path / f\"{env_idx:02d}\" \n        self._store_path.mkdir(parents=True, exist_ok=True) \n        self._start_prob = start_prob \n        self._steps_count = steps_count \n        self._is_storing = False \n        self._steps: tt.List[EpisodeStep] = [] \n        self._prev_obs = None \n        self._step_idx = 0\n```", "```py\n def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None) \\ \n            -> tuple[WrapperObsType, dict[str, tt.Any]]: \n        self._step_idx += 1 \n        res = super().reset(seed=seed, options=options) \n        if self._is_storing: \n            self._prev_obs = deepcopy(res[0]) \n        return res\n```", "```py\n def step(self, action: WrapperActType) -> tuple[ \n        WrapperObsType, SupportsFloat, bool, bool, dict[str, tt.Any] \n    ]: \n        self._step_idx += 1 \n        obs, r, is_done, is_tr, extra = super().step(action) \n        if self._is_storing: \n            self._steps.append(EpisodeStep(self._prev_obs, int(action))) \n            self._prev_obs = deepcopy(obs) \n\n            if len(self._steps) >= self._steps_count: \n                store_segment(self._store_path, self._step_idx, self._steps) \n                self._is_storing = False \n                self._steps.clear() \n        elif random.random() <= self._start_prob: \n            # start recording \n            self._is_storing = True \n            self._prev_obs = deepcopy(obs) \n        return obs, r, is_done, is_tr, extra\n```", "```py\ndef store_segment(root_path: pathlib.Path, step_idx: int, steps: tt.List[EpisodeStep]): \n    out_path = root_path / f\"{step_idx:08d}.dat\" \n    dat = pickle.dumps(steps) \n    out_path.write_bytes(dat) \n    print(f\"Stored {out_path}\")\n```", "```py\n def make_env() -> gym.Env: \n        e = gym.make(\"SeaquestNoFrameskip-v4\") \n        if reward_path is not None: \n            p = pathlib.Path(reward_path) \n            e = rlhf.RewardModelWrapper(e, p, dev=dev, metrics_queue=metrics_queue) \n        if db_path is not None: \n            p = pathlib.Path(db_path) \n            p.mkdir(parents=True, exist_ok=True) \n            e = rlhf.EpisodeRecorderWrapper(e, p, env_idx=env_idx) \n        e = ptan.common.wrappers.wrap_dqn(e) \n        # add time limit after all wrappers \n        e = gym.wrappers.TimeLimit(e, TIME_LIMIT) \n        return e\n```", "```py\npip install nicegui==1.4.26\n```", "```py\nChapter19$ ./02_label_ui.py -d db-v0 \nNiceGUI ready to go on http://localhost:8080, http://172.17.0.1:8080, http://172.18.0.1:8080, and http://192.168.10.8:8080\n```", "```py\nChapter19$ head db-v0/labels.json \n{\"sample1\":\"14/00023925.dat\",\"sample2\":\"10/00606788.dat\",\"label\":0} \n{\"sample1\":\"02/01966114.dat\",\"sample2\":\"10/01667833.dat\",\"label\":2} \n{\"sample1\":\"00/02432057.dat\",\"sample2\":\"06/01410909.dat\",\"label\":1} \n{\"sample1\":\"01/02293138.dat\",\"sample2\":\"11/00997214.dat\",\"label\":0} \n{\"sample1\":\"10/00091149.dat\",\"sample2\":\"11/01262679.dat\",\"label\":2} \n{\"sample1\":\"12/01394239.dat\",\"sample2\":\"04/01792088.dat\",\"label\":2} \n{\"sample1\":\"10/01390371.dat\",\"sample2\":\"09/00077676.dat\",\"label\":0} \n{\"sample1\":\"10/01390371.dat\",\"sample2\":\"09/00077676.dat\",\"label\":1} \n{\"sample1\":\"12/02339611.dat\",\"sample2\":\"00/02755898.dat\",\"label\":2} \n{\"sample1\":\"06/00301623.dat\",\"sample2\":\"06/00112361.dat\",\"label\":2}\n```", "```py\nclass RewardModel(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): \n        super().__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 16, kernel_size=7, stride=3), \n            nn.BatchNorm2d(16), \n            nn.Dropout(p=0.5), \n            nn.LeakyReLU(), \n            nn.Conv2d(16, 16, kernel_size=5, stride=2), \n            nn.BatchNorm2d(16), \n            nn.Dropout(p=0.5), \n            nn.LeakyReLU(), \n            nn.Conv2d(16, 16, kernel_size=3, stride=1), \n            nn.BatchNorm2d(16), \n            nn.Dropout(p=0.5), \n            nn.LeakyReLU(), \n            nn.Conv2d(16, 16, kernel_size=3, stride=1), \n            nn.BatchNorm2d(16), \n            nn.Dropout(p=0.5), \n            nn.LeakyReLU(), \n            nn.Flatten(), \n        ) \n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.out = nn.Sequential( \n            nn.Linear(size + n_actions, 64), \n            nn.LeakyReLU(), \n            nn.Linear(64, 1), \n        ) \n\n    def forward(self, obs: torch.ByteTensor, acts: torch.Tensor) -> torch.Tensor: \n        conv_out = self.conv(obs / 255) \n        comb = torch.hstack((conv_out, acts)) \n        out = self.out(comb) \n        return out\n```", "```py\ndef calc_loss(model: rlhf.RewardModel, s1_obs: torch.ByteTensor, \n              s1_acts: torch.Tensor, s2_obs: torch.ByteTensor, \n              s2_acts: torch.Tensor, mu: torch.Tensor) -> torch.Tensor: \n    batch_size, steps = s1_obs.size()[:2] \n\n    s1_obs_flat = s1_obs.flatten(0, 1) \n    s1_acts_flat = s1_acts.flatten(0, 1) \n    r1_flat = model(s1_obs_flat, s1_acts_flat) \n    r1 = r1_flat.view((batch_size, steps)) \n    R1 = torch.sum(r1, 1) \n\n    s2_obs_flat = s2_obs.flatten(0, 1) \n    s2_acts_flat = s2_acts.flatten(0, 1) \n    r2_flat = model(s2_obs_flat, s2_acts_flat) \n    r2 = r2_flat.view((batch_size, steps)) \n    R2 = torch.sum(r2, 1) \n    R = torch.hstack((R1.unsqueeze(-1), R2.unsqueeze(-1))) \n    loss_t = F.binary_cross_entropy_with_logits(R, mu) \n    return loss_t\n```", "```py\nChapter19$ ./03_reward_train.py --dev cuda -n v0-rw -o rw db-v0 \nNamespace(dev=’cuda’, name=v0-rw’, out=’rw’, dbs=[’db-v0’]) \nLoaded DB from db-v0 with 149 labels and 2534 paths \nRewardModel( \n  (conv): Sequential( \n   (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(3, 3)) \n   (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n   (2): Dropout(p=0.5, inplace=False) \n   (3): LeakyReLU(negative_slope=0.01) \n   (4): Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 2)) \n   (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n   (6): Dropout(p=0.5, inplace=False) \n   (7): LeakyReLU(negative_slope=0.01) \n   (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n   (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n   (10): Dropout(p=0.5, inplace=False) \n   (11): LeakyReLU(negative_slope=0.01) \n   (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) \n   (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n   (14): Dropout(p=0.5, inplace=False) \n   (15): LeakyReLU(negative_slope=0.01) \n   (16): Flatten(start_dim=1, end_dim=-1) \n  ) \n  (out): Sequential( \n   (0): Linear(in_features=8978, out_features=64, bias=True) \n   (1): LeakyReLU(negative_slope=0.01) \n   (2): Linear(in_features=64, out_features=1, bias=True) \n  ) \n) \nEpoch 0 done, train loss 0.131852, test loss 0.132976 \nSave model for 0.13298 test loss \nEpoch 1 done, train loss 0.104426, test loss 0.354560 \nEpoch 2 done, train loss 0.159513, test loss 0.170160 \nEpoch 3 done, train loss 0.054362, test loss 0.066557 \nSave model for 0.06656 test loss \nEpoch 4 done, train loss 0.046695, test loss 0.121662 \nEpoch 5 done, train loss 0.055446, test loss 0.064895 \nSave model for 0.06490 test loss \nEpoch 6 done, train loss 0.024505, test loss 0.025308 \nSave model for 0.02531 test loss \nEpoch 7 done, train loss 0.015864, test loss 0.045814 \nEpoch 8 done, train loss 0.024745, test loss 0.054631 \nEpoch 9 done, train loss 0.027670, test loss 0.054107 \nEpoch 10 done, train loss 0.025979, test loss 0.048673 \nBest test loss was less than current for 4 epoches, stop\n```", "```py\nclass RewardModelWrapper(gym.Wrapper): \n    KEY_REAL_REWARD_SUM = \"real_reward_sum\" \n    KEY_REWARD_MU = \"reward_mu\" \n    KEY_REWARD_STD = \"reward_std\" \n\n    def __init__(self, env: gym.Env, model_path: pathlib.Path, dev: torch.device, \n                 reward_window: int = 100, metrics_queue: tt.Optional[queue.Queue] = None): \n        super().__init__(env) \n        self.device = dev \n        assert isinstance(env.action_space, gym.spaces.Discrete) \n        s = env.observation_space.shape \n        self.total_actions = env.action_space.n \n        self.model = RewardModel( \n            input_shape=(s[2], s[0], s[1]), n_actions=self.total_actions) \n        self.model.load_state_dict(torch.load(model_path, map_location=torch.device(’cpu’), \n                                              weights_only=True)) \n        self.model.eval() \n        self.model.to(dev) \n        self._prev_obs = None \n        self._reward_window = collections.deque(maxlen=reward_window) \n        self._real_reward_sum = 0.0 \n        self._metrics_queue = metrics_queue\n```", "```py\n def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None) \\ \n            -> tuple[WrapperObsType, dict[str, tt.Any]]: \n        res = super().reset(seed=seed, options=options) \n        self._prev_obs = deepcopy(res[0]) \n        self._real_reward_sum = 0.0 \n        return res\n```", "```py\n def step(self, action: WrapperActType) -> tuple[ \n        WrapperObsType, SupportsFloat, bool, bool, dict[str, tt.Any] \n    ]: \n        obs, r, is_done, is_tr, extra = super().step(action) \n        self._real_reward_sum += r \n        p_obs = np.moveaxis(self._prev_obs, (2, ), (0, )) \n        p_obs_t = torch.as_tensor(p_obs).to(self.device) \n        p_obs_t.unsqueeze_(0) \n        act = np.eye(self.total_actions)[[action]] \n        act_t = torch.as_tensor(act, dtype=torch.float32).to(self.device) \n        new_r_t = self.model(p_obs_t, act_t) \n        new_r = float(new_r_t.item()) \n\n        # track reward for normalization \n        self._reward_window.append(new_r) \n        if len(self._reward_window) == self._reward_window.maxlen: \n            mu = np.mean(self._reward_window) \n            std = np.std(self._reward_window) \n            new_r -= mu \n            new_r /= std \n            self._metrics_queue.put((self.KEY_REWARD_MU, mu)) \n            self._metrics_queue.put((self.KEY_REWARD_STD, std)) \n\n        if is_done or is_tr: \n            self._metrics_queue.put((self.KEY_REAL_REWARD_SUM, self._real_reward_sum)) \n        self._prev_obs = deepcopy(obs) \n        return obs, new_r, is_done, is_tr, extra\n```", "```py\n def make_env() -> gym.Env: \n        e = gym.make(\"SeaquestNoFrameskip-v4\") \n        if reward_path is not None: \n            p = pathlib.Path(reward_path) \n            e = rlhf.RewardModelWrapper(e, p, dev=dev, metrics_queue=metrics_queue) \n        if db_path is not None: \n            p = pathlib.Path(db_path) \n            p.mkdir(parents=True, exist_ok=True) \n            e = rlhf.EpisodeRecorderWrapper(e, p, env_idx=env_idx) \n        e = ptan.common.wrappers.wrap_dqn(e) \n        # add time limit after all wrappers \n        e = gym.wrappers.TimeLimit(e, TIME_LIMIT) \n        return e\n```"]