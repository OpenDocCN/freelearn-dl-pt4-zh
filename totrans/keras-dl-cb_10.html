<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Getting started with TensorFlow</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">TensorFlow is an open source deep learning library by Google. It provides primitives for defining functions on tensors and automatically computing their derivatives. A tensor can be represented as a multidimensional array of numbers. Scalar, Vector, and Matrix are types of tensors. TensorFlow is mainly used to design computational graphs, build, and train deep learning models. The TensorFlow library does numerical computations using data flow graphs, where the nodes represent mathematical operations and the edges represent the data points (usually multidimensional arrays or tensors that are transmitted between these edges).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Environment setup</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">It is best to use an IDE such as PyCharm to edit Python code; it provides faster development tools and coding assistance. Code completion and inspection makes coding and debugging faster and simpler, ensuring that you focus on the end goal of programming neural networks. </p>
<p class="calibre4">TensorFlow provides APIs for multiple languages: Python, C++, Java, Go, and so on. We will download a version of TensorFlow that will enable us to write the code for deep learning models in Python. On the TensorFlow installation website, we can find the most common ways and latest instructions to install TensorFlow using virtualenv, pip, and Docker.</p>
<p class="calibre4">The following steps describe how to set up a local development environment:</p>
<ol class="calibre23">
<li class="chapter">Download the Pycharm community edition.</li>
<li class="chapter"><span class="calibre5">Get the latest Python version on Pycharm.</span></li>
<li class="chapter">Go to <span class="calibre5">Preferences</span>, set up the python interpreter, and install the latest version of TensorFlow:</li>
</ol>
<div class="mce-root"><img src="Images/a0d7a22e-da9c-4477-827f-b3a6dcc80db6.png" width="2546" height="1524" class="calibre190"/></div>
<ol start="4" class="calibre23">
<li class="chapter"><span class="calibre5"><span class="calibre5">TensorFlow will now appear in the installed packages list. Click on <span class="calibre5">OK</span></span></span>. <span class="calibre5"><span class="calibre5">Now test your installation with a program such as hello world:</span></span></li>
</ol>
<pre class="calibre64">import TensorFlow  <span class="calibre5">as </span>tf<br class="calibre2"/>helloWorld = tf.constant(<span class="calibre5">"Hello World!"</span>)<br class="calibre2"/>sess = tf.Session()<br class="calibre2"/><span class="calibre5">print</span>(sess.run(helloWorld))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">TensorFlow comparison with Numpy</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">TensorFlow and Numpy are both N-dimensional array libraries. TensorFlow additionally allows us to create tensor functions and compute derivatives. TensorFlow has become one of the major libraries used for deep learning as it is incredibly efficient and can run on GPUs.</p>
<p class="calibre4">The following program describes how <kbd class="calibre18">TensorFlow</kbd> and <kbd class="calibre18">numpy</kbd> can be used to perform similar operations such as creating tensors of a <kbd class="calibre18">(3,3)</kbd> shape:</p>
<pre class="calibre191"><span class="calibre5">import </span><span class="calibre5">TensorFlow  </span><span class="calibre5">as </span><span class="calibre5">tf<br class="calibre2"/></span><span class="calibre5">import </span><span class="calibre5">numpy </span><span class="calibre5">as </span><span class="calibre5">np<br class="calibre2"/><br class="calibre2"/>tf.InteractiveSession()<br class="calibre2"/><br class="calibre2"/></span><span class="calibre5"># TensorFlow  operations<br class="calibre2"/></span><span class="calibre5">a = tf.zeros((</span><span class="calibre5">3</span><span class="calibre5">,</span><span class="calibre5">3</span><span class="calibre5">))<br class="calibre2"/>b = tf.ones((</span><span class="calibre5">3</span><span class="calibre5">,</span><span class="calibre5">3</span><span class="calibre5">))<br class="calibre2"/><br class="calibre2"/></span><span class="calibre5">print</span><span class="calibre5">(tf.reduce_sum(b, </span><span class="calibre5">reduction_indices</span><span class="calibre5">=</span><span class="calibre5">1</span><span class="calibre5">).eval())<br class="calibre2"/></span><span class="calibre5">print</span><span class="calibre5">(a.get_shape())<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/># numpy operations<br class="calibre2"/></span><span class="calibre5">a = np.zeros((</span><span class="calibre5">3</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">))<br class="calibre2"/>b = np.ones((</span><span class="calibre5">3</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">))<br class="calibre2"/></span><span class="calibre5">print</span><span class="calibre5">(np.sum(b, </span><span class="calibre5">axis</span><span class="calibre5">=</span><span class="calibre5">1</span><span class="calibre5">))
</span><span class="calibre5">print</span><span class="calibre5">(a.shape)</span></pre>
<p class="calibre4">The output of the preceding code is as follows:</p>
<pre class="calibre26">[ 3.  3.  3.]<br class="calibre2"/>(3, 3)<br class="calibre2"/>[ 3.  3.  3.]<br class="calibre2"/>(3, 3)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Computational graph</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">TensorFlow is based on building a computational graph. A computational graph is a network of nodes, where each node defines an operation running a function; this can be as plain as addition or subtraction, or as complicated as a multivariate equation. TensorFlow programs are structured in a construction phase that assembles a graph and an execution phase that utilizes a session object to execute operations in the graph.</p>
<p class="calibre4">An operation is referred to as the op and can return zero or more tensors, which can be used later in the graph. Each op can be given a constant, array, or n-dimensional matrix. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Graph</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The default graph gets instantiated when the TensorFlow library is imported. Constructing a graph object instead of using the default graph is useful when creating multiple models in one file that do not depend on each other. Constants and operations are added to the graph in TensorFlow.</p>
<p class="calibre4">Variables and operations applied outside of <kbd class="calibre18">newGraph.as_default()</kbd> will get added to the default graph, which is created when the library is imported:</p>
<pre class="calibre26">newGraph = tf.Graph()<br class="calibre2"/><span class="calibre5">with </span>newGraph.as_default():<br class="calibre2"/>    newGraphConst = tf.constant([<span class="calibre5">2.</span>, <span class="calibre5">3.</span>])<span class="calibre5"><br class="calibre2"/></span></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Session objects</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A session in TensorFlow encapsulates the environment in which tensor objects are evaluated. Sessions can have their private variables, queues, and readers that are designated. We should use the close method at the end of the session.</p>
<p class="calibre4">The session has three arguments, which are optional:</p>
<ul class="calibre20">
<li class="calibre21"><kbd class="calibre18">Target</kbd>: The execution engine to connect to</li>
<li class="calibre21"><kbd class="calibre18">graph</kbd>: The graph object to be started</li>
<li class="calibre21"><kbd class="calibre18">config</kbd>: This is a ConfigProto protocol buffer</li>
</ul>
<p class="calibre4">To run a single step of the TensorFlow computation, the step function is invoked and necessary dependencies of the graph are executed:</p>
<pre class="calibre26"><span class="calibre5"># session objects<br class="calibre2"/></span>a = tf.constant(<span class="calibre5">6.0</span>)<br class="calibre2"/>b = tf.constant(<span class="calibre5">7.0</span>)<br class="calibre2"/><br class="calibre2"/>c = a * b<br class="calibre2"/><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>   <span class="calibre5">print</span>(sess.run(c))<br class="calibre2"/>   <span class="calibre5">print</span>(c.eval())</pre>
<p class="calibre4"><kbd class="calibre18">sess.run(c)</kbd> in the currently active session!</p>
<p class="calibre4">The preceding code gives the following output:</p>
<pre class="calibre26">42.0, 42.0</pre>
<p class="calibre4">The <kbd class="calibre18">tf.InteractiveSession()</kbd> function is an easy way of keeping a default session open in <kbd class="calibre18">ipython</kbd>. The <kbd class="calibre18">sess.run(c)</kbd> is an example of a TensorFlow Fetch:</p>
<pre class="calibre26">session = tf.InteractiveSession()<br class="calibre2"/>cons1 = tf.constant(<span class="calibre5">1</span>)<br class="calibre2"/>cons2 = tf.constant(<span class="calibre5">2</span>)<br class="calibre2"/>cons3 = cons1 + cons2<br class="calibre2"/><span class="calibre5"># instead of sess.run(cons3)<br class="calibre2"/></span>cons3.eval()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Variables</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">When training a model, we use variables to hold and update the parameters. Variables are like in-memory buffers containing tensors. All tensors we used previously were constant tensors, not variables.</p>
<p class="calibre4">Variables are managed or maintained by the session object. Variables persist between sessions, which is useful because tensor and operation objects are immutable:</p>
<pre class="calibre26"># tensor variables<em class="calibre29"><br class="calibre2"/> </em>W1 = tf.ones((3,3))<br class="calibre2"/>W2 = tf.Variable(tf.zeros((3,3)), name=<strong class="calibre3">"weights"</strong>)<br class="calibre2"/> <br class="calibre2"/> <strong class="calibre3">with </strong>tf.Session() <strong class="calibre3">as </strong>sess:<br class="calibre2"/>   print(sess.run(W1))<br class="calibre2"/>   sess.run(tf.global_variables_initializer())<br class="calibre2"/>   print(sess.run(W2))</pre>
<p class="calibre4">The preceding code gives the following output: </p>
<pre class="calibre26">[[ 1.  1.  1.] [ 1.  1.  1.] [ 1.  1.  1.]]<br class="calibre2"/>[[ 0.  0.  0.] [ 0.  0.  0.] [ 0.  0.  0.]]</pre>
<p class="calibre4">TensorFlow variables must be initialized before they have values, which is in contrast with constant tensors:</p>
<pre class="calibre26"><span class="calibre5"># Variable objects can be initialized from constants or random values<br class="calibre2"/></span>W = tf.Variable(tf.zeros((<span class="calibre5">2</span>,<span class="calibre5">2</span>)), <span class="calibre5">name</span>=<span class="calibre5">"weights"</span>)<br class="calibre2"/>R = tf.Variable(tf.random_normal((<span class="calibre5">2</span>,<span class="calibre5">2</span>)), <span class="calibre5">name</span>=<span class="calibre5">"random_weights"</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>   <span class="calibre5"># Initializes all variables with specified values.<br class="calibre2"/></span><span class="calibre5">   </span>sess.run(tf.initialize_all_variables())<br class="calibre2"/>   <span class="calibre5">print</span>(sess.run(W))<br class="calibre2"/>   <span class="calibre5">print</span>(sess.run(R))</pre>
<p class="calibre4">The preceding code gives this output: </p>
<pre class="calibre26">[[ 0.  0.] [ 0.  0.]]<br class="calibre2"/>[[ 0.65469146 -0.97390586] [-2.39198709  0.76642162]]</pre>
<pre class="calibre26">state = tf.Variable(<span class="calibre5">0</span>, <span class="calibre5">name</span>=<span class="calibre5">"counter"</span>)<br class="calibre2"/>new_value = tf.add(state, tf.constant(<span class="calibre5">1</span>))<br class="calibre2"/>update = tf.assign(state, new_value)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>   sess.run(tf.initialize_all_variables())<br class="calibre2"/>   <span class="calibre5">print</span>(sess.run(state))<br class="calibre2"/>   <span class="calibre5">for </span>_ <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">3</span>):<br class="calibre2"/>      sess.run(update)<br class="calibre2"/>      <span class="calibre5">print</span>(sess.run(state))</pre>
<p class="calibre4">The preceding code gives the following output: </p>
<pre class="calibre26">0 1 2 3</pre>
<p class="calibre4">Fetching variable states:</p>
<pre class="calibre26">input1 = tf.constant(<span class="calibre5">5.0</span>)<br class="calibre2"/>input2 = tf.constant(<span class="calibre5">6.0</span>)<br class="calibre2"/>input3 = tf.constant(<span class="calibre5">7.0</span>)<br class="calibre2"/>intermed = tf.add(input2, input3)<br class="calibre2"/>mul = tf.multiply(input1, intermed)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Calling sess.run(var) on a tf.Session() object retrieves its value. Can retrieve multiple variables simultaneously with sess.run([var1, var2])<br class="calibre2"/></span><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>   result = sess.run([mul, intermed])<br class="calibre2"/>   <span class="calibre5">print</span>(result)</pre>
<p class="calibre4">The preceding code gives this output: </p>
<pre class="calibre26">[65.0, 13.0]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Scope</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">TensorFlow models may have hundreds of variables.<span class="calibre14"> </span><kbd class="calibre18">tf.variable_scope()</kbd><span class="calibre14"> </span>provides a simple name.</p>
<p class="calibre4">To manage the complexity of models and break down into unique pieces, TensorFlow has scopes. Scopes are extremely simple and help when using TensorBoard. Scopes can also be nested inside of other scopes:</p>
<pre class="calibre26"><strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"foo"</strong>):<br class="calibre2"/>     <strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"bar"</strong>):<br class="calibre2"/>         v = tf.get_variable(<strong class="calibre3">"v"</strong>, [1])<br class="calibre2"/> <strong class="calibre3">assert </strong>v.name == <strong class="calibre3">"foo/bar/v:0"<br class="calibre2"/> <br class="calibre2"/> </strong><strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"foo"</strong>):<br class="calibre2"/>     v = tf.get_variable(<strong class="calibre3">"v"</strong>, [1])<br class="calibre2"/>     tf.get_variable_scope().reuse_variables()<br class="calibre2"/>     v1 = tf.get_variable(<strong class="calibre3">"v"</strong>, [1])<br class="calibre2"/> <strong class="calibre3">assert </strong>v1 == v</pre>
<p class="calibre4">The following example shows how to use the reuse option to understand the behavior of <kbd class="calibre18">get_variable</kbd>:</p>
<pre class="calibre26">#reuse is false<em class="calibre29"><br class="calibre2"/> </em><strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"foo"</strong>):<br class="calibre2"/>     n = tf.get_variable(<strong class="calibre3">"n"</strong>, [1])<br class="calibre2"/> <strong class="calibre3">assert </strong>v.name == <strong class="calibre3">"foo/n:0"<br class="calibre2"/> <br class="calibre2"/> </strong><em class="calibre29">#Reuse is true<br class="calibre2"/> </em><strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"foo"</strong>):<br class="calibre2"/>     n = tf.get_variable(<strong class="calibre3">"n"</strong>, [1])<br class="calibre2"/> <strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"foo"</strong>, reuse=<strong class="calibre3">True</strong>):<br class="calibre2"/>     v1 = tf.get_variable(<strong class="calibre3">"n"</strong>, [1])<br class="calibre2"/> <strong class="calibre3">assert </strong>v1 == n</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Data input</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Input external data to TensorFlow objects:</p>
<pre class="calibre26">a = np.zeros((<span class="calibre5">3</span>,<span class="calibre5">3</span>))<br class="calibre2"/>ta = tf.convert_to_tensor(a)<br class="calibre2"/><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>   <span class="calibre5">print</span>(sess.run(ta))</pre>
<p class="calibre4">The preceding code gives the following output: </p>
<pre class="calibre26">[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Placeholders and feed dictionaries</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Using <kbd class="calibre18">tf.convert_to_tensor()</kbd> to input data is convenient but it doesn't scale. Use <kbd class="calibre18">tf.placeholder</kbd> variables (dummy nodes that provide entry points for data to a computational graph). A <kbd class="calibre18">feed_dict</kbd> is a Python dictionary mapping:</p>
<pre class="calibre26">input1 = tf.placeholder(tf.float32)<br class="calibre2"/> input2 = tf.placeholder(tf.float32)<br class="calibre2"/> output = tf.multiply(input1, input2)<br class="calibre2"/> <br class="calibre2"/> <strong class="calibre3">with </strong>tf.Session() <strong class="calibre3">as </strong>sess:<br class="calibre2"/>    print(sess.run([output], feed_dict={input1:[5.], input2:[6.]}))</pre>
<p class="calibre4">The preceding code gives this output: </p>
<pre class="calibre26">[array([ 30.], dtype=float32)]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Auto differentiation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Auto differentiation is also known as <strong class="calibre7">algorithmic differentiation</strong>, which is an automatic way of numerically computing the derivatives of a function. It is helpful for computing gradients, Jacobians, and Hessians for use in applications such as numerical optimization. Backpropagation algorithm is an implementation of the reverse mode of automatic differentiation for calculating the gradient.</p>
<p class="calibre4">In the following example, using the <kbd class="calibre18">mnist</kbd> dataset, we calculate the loss using one of the <kbd class="calibre18">loss</kbd> functions. The question is: how do we fit the model to the data?</p>
<p class="calibre4">We can use <kbd class="calibre18">tf.train.Optimizer</kbd> and create an optimizer. <kbd class="calibre18">tf.train.Optimizer.minimize(loss, var_list)</kbd> adds an optimization operation to the computational graph and automatic differentiation computes gradients without user input:</p>
<pre class="calibre26"><span class="calibre5">import </span>TensorFlow  <span class="calibre5">as </span>tf<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># get mnist dataset<br class="calibre2"/></span><span class="calibre5">from </span>TensorFlow .examples.tutorials.mnist <span class="calibre5">import </span>input_data<br class="calibre2"/>data = input_data.read_data_sets(<span class="calibre5">"MNIST_data/"</span>, <span class="calibre5">one_hot</span>=<span class="calibre5">True</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># x represents image with 784 values as columns (28*28), y represents output digit<br class="calibre2"/></span>x = tf.placeholder(tf.float32, [<span class="calibre5">None</span>, <span class="calibre5">784</span>])<br class="calibre2"/>y = tf.placeholder(tf.float32, [<span class="calibre5">None</span>, <span class="calibre5">10</span>])<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># initialize weights and biases [w1,b1][w2,b2]<br class="calibre2"/></span>numNeuronsInDeepLayer = <span class="calibre5">30<br class="calibre2"/></span>w1 = tf.Variable(tf.truncated_normal([<span class="calibre5">784</span>, numNeuronsInDeepLayer]))<br class="calibre2"/>b1 = tf.Variable(tf.truncated_normal([<span class="calibre5">1</span>, numNeuronsInDeepLayer]))<br class="calibre2"/>w2 = tf.Variable(tf.truncated_normal([numNeuronsInDeepLayer, <span class="calibre5">10</span>]))<br class="calibre2"/>b2 = tf.Variable(tf.truncated_normal([<span class="calibre5">1</span>, <span class="calibre5">10</span>]))<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># non-linear sigmoid function at each neuron<br class="calibre2"/></span><span class="calibre5">def </span>sigmoid(x):<br class="calibre2"/>    sigma = tf.div(tf.constant(<span class="calibre5">1.0</span>), tf.add(tf.constant(<span class="calibre5">1.0</span>), tf.exp(tf.negative(x))))<br class="calibre2"/>    <span class="calibre5">return </span>sigma<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># starting from first layer with wx+b, then apply sigmoid to add non-linearity<br class="calibre2"/></span>z1 = tf.add(tf.matmul(x, w1), b1)<br class="calibre2"/>a1 = sigmoid(z1)<br class="calibre2"/>z2 = tf.add(tf.matmul(a1, w2), b2)<br class="calibre2"/>a2 = sigmoid(z2)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># calculate the loss (delta)<br class="calibre2"/></span>loss = tf.subtract(a2, y)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># derivative of the sigmoid function der(sigmoid)=sigmoid*(1-sigmoid)<br class="calibre2"/></span><span class="calibre5">def </span>sigmaprime(x):<br class="calibre2"/>    <span class="calibre5">return </span>tf.multiply(sigmoid(x), tf.subtract(tf.constant(<span class="calibre5">1.0</span>), sigmoid(x)))<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># automatic differentiation<br class="calibre2"/></span>cost = tf.multiply(loss, loss)<br class="calibre2"/>step = tf.train.GradientDescentOptimizer(<span class="calibre5">0.1</span>).minimize(cost)<br class="calibre2"/><br class="calibre2"/>acct_mat = tf.equal(tf.argmax(a2, <span class="calibre5">1</span>), tf.argmax(y, <span class="calibre5">1</span>))<br class="calibre2"/>acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))<br class="calibre2"/><br class="calibre2"/>sess = tf.InteractiveSession()<br class="calibre2"/>sess.run(tf.global_variables_initializer())<br class="calibre2"/><br class="calibre2"/><span class="calibre5">for </span>i <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">10000</span>):<br class="calibre2"/>    batch_xs, batch_ys = data.train.next_batch(<span class="calibre5">10</span>)<br class="calibre2"/>    sess.run(step, <span class="calibre5">feed_dict</span>={x: batch_xs,<br class="calibre2"/>                              y: batch_ys})<br class="calibre2"/>    <span class="calibre5">if </span>i % <span class="calibre5">1000 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>        res = sess.run(acct_res, <span class="calibre5">feed_dict</span>=<br class="calibre2"/>        {x: data.test.images[:<span class="calibre5">1000</span>],<br class="calibre2"/>         y: data.test.labels[:<span class="calibre5">1000</span>]})<br class="calibre2"/>        <span class="calibre5">print</span>(res)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">TensorBoard</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">TensorFlow has a powerful built-in visualization tool called <strong class="calibre7">TensorBoard</strong>. It allows developers to interpret, visualize, and debug computational graphs. To visualize graph and metrics automatically in TensorBoard, TensorFlow writes events related to the execution of a computational graph to a particular folder.</p>
<p class="calibre4">This example shows a computational graph of the analysis done earlier:</p>
<div class="mce-root"><img src="Images/45fbcf50-72f4-49f2-b52f-04dee2a903b6.png" class="calibre192"/></div>
<p class="calibre4">To examine the graph, click on the graph tab on the top panel of TensorBoard. If the graph has several nodes, visualizing it in a single view can be hard. To make our visualization more accessible, we can organize the related operations into groups using <kbd class="calibre18">tf.name_scope</kbd> with specific names.</p>
<p class="calibre4"/>


            </article>

            
        </section>
    </div>



  </body></html>