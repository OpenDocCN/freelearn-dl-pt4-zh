- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Generating Graphs Using Graph Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图神经网络生成图
- en: 'Graph generation consists of finding methods to create new graphs. As a field
    of study, it provides insights into understanding how graphs work and evolve.
    It also has direct applications in data augmentation, anomaly detection, drug
    discovery, and so on. We can distinguish two types of generation: **realistic
    graph generation**, which imitates a given graph (for example, in data augmentation),
    and **goal-directed graph generation**, which creates graphs that optimize a specific
    metric (for instance, in molecule generation).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 图生成包括寻找创建新图的方法。作为一个研究领域，它为理解图的工作方式和演化过程提供了见解。它在数据增强、异常检测、药物发现等方面有直接应用。我们可以区分两种生成类型：**现实图生成**，它模仿给定的图（例如，在数据增强中），以及**目标导向图生成**，它创建优化特定指标的图（例如，在分子生成中）。
- en: 'In this chapter, we will explore traditional techniques to understand how graph
    generation works. We will focus on two popular algorithms: the **Erdős–Rényi**
    and the **small-world** models. They present interesting properties but also issues
    that motivate the need for GNN-based graph generation. In the second section,
    we will describe three families of solutions: **variational autoencoder** (**VAE**)-based,
    autoregressive, and **GAN**-based models. Finally, we will implement a GAN-based
    framework with **Reinforcement Learning** (**RL**) to generate new chemical compounds.
    Instead of PyTorch Geometric, we will use the **DeepChem** library with TensorFlow.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索传统技术，以了解图生成的工作原理。我们将重点介绍两种流行的算法：**埃尔德什–雷尼**模型和**小世界**模型。它们具有有趣的特性，但也存在一些问题，这些问题促使了基于GNN的图生成方法的需求。在第二部分中，我们将描述三种解决方案：**变分自编码器**（**VAE**）基础的、自动回归的和**GAN**基础的模型。最后，我们将实现一个基于GAN的框架，并结合**强化学习**（**RL**）生成新的化学化合物。我们将使用**DeepChem**库与TensorFlow，而不是PyTorch
    Geometric。
- en: By the end of this chapter, you will be able to generate graphs using traditional
    and GNN-based techniques. You will have a good overview of this field and the
    different applications you can build with it. You will know how to implement a
    hybrid architecture to guide the generation into generating valid molecules with
    your desired properties.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够使用传统方法和基于GNN的技术生成图。您将对这一领域以及您可以用它构建的不同应用有一个良好的概览。您将知道如何实现混合架构，以引导生成有效的分子，且具备您所期望的属性。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Generating graphs with traditional techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统技术生成图
- en: Generating graphs with graph neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图神经网络生成图
- en: Generating molecules with MolGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MolGAN生成分子
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到，网址是[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11)。
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* of this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*中可以找到在本地计算机上运行代码所需的安装步骤。
- en: Generating graphs with traditional techniques
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用传统技术生成图
- en: Traditional graph generation techniques have been studied for decades. This
    is why they are well understood and can be used as baselines in various applications.
    However, they are often limited in the type of graphs they can generate. Most
    of them are specialized to output certain topologies, which is why they cannot
    simply imitate a given network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的图生成技术已经研究了数十年。这就是它们被广泛理解并可以在各种应用中作为基准使用的原因。然而，它们在可以生成的图类型上通常有局限性。大多数技术专注于输出特定的拓扑结构，这也是它们无法简单模仿给定网络的原因。
- en: 'In this section, we will introduce two classical techniques: the Erdős–Rényi
    and the small-world models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种经典技术：埃尔德什–雷尼模型和小世界模型。
- en: The Erdős–Rényi model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 埃尔德什–雷尼模型
- en: 'The Erdős–Rényi model is the simplest and most popular random graph model.
    It was introduced by Hungarian mathematicians Paul Erdős and Alfréd Rényi in 1959
    [1] and was independently proposed by Edgar Gilbert the same year [2]. This model
    has two variants: ![](img/Formula_B19153_11_001.png) and ![](img/Formula_B19153_11_002.png).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The ![](img/Formula_B19153_11_003.png) model is straightforward: we are given
    ![](img/Formula_B19153_11_004.png) nodes and a probability ![](img/Formula_B19153_11_005.png)
    of connecting a pair of nodes. We try to randomly connect every node to each other
    to create the final graph. It means that there are ![](img/Formula_B19153_11_006.png)
    possible links. Another way of understanding the probability ![](img/Formula_B19153_11_007.png)
    is to consider it as a parameter to change the density of the network.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The `networkx` library has a direct implementation of the ![](img/Formula_B19153_11_008.png)
    model:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `networkx` library:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We generate a `G` graph using the `nx.erdos_renyi_graph()` function with `10`
    nodes (![](img/Formula_B19153_11_009.png)) and a probability for edge creation
    of `0.5` (![](img/Formula_B19153_11_010.png)):'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We position the resulting nodes using the `nx.circular_layout()` function.
    Other layouts can be used, but this one is handy for comparing different values
    of ![](img/Formula_B19153_11_011.png):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We draw the `G` graph with the `pos` layout using `nx.draw()`. Global heuristics
    are usually more accurate but require knowing the entirety of the graph. However,
    it is not the only way to predict links with this knowledge:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following graph:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5](img/B19153_11_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'We can repeat this process with a probability of **0.1** and **0.9** to obtain
    the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation](img/B19153_11_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: We can see that many nodes are isolated when ![](img/Formula_B19153_11_012.png)
    is low, while the graph is highly interconnected when ![](img/Formula_B19153_11_013.png)
    is high.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ![](img/Formula_B19153_11_014.png) model, we randomly choose a graph
    from all graphs with ![](img/Formula_B19153_11_015.png) nodes and ![](img/Formula_B19153_11_016.png)
    links. For instance, if ![](img/Formula_B19153_11_017.png) and ![](img/Formula_B19153_11_018.png),
    there are three possible graphs (see *Figure 11**.3*). The ![](img/Formula_B19153_11_019.png)
    model will just randomly select one of these graphs. This is a different approach
    to the same problem, but it is not as popular as the ![](img/Formula_B19153_11_020.png)
    model because it is more challenging to analyze in general:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – A set of graphs with three nodes and two links](img/B19153_11_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – A set of graphs with three nodes and two links
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also implement the ![](img/Formula_B19153_11_021.png) model in Python
    using the `nx.gnm_random_graph()` function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 11.4 – A graph randomly sampled from the set of graphs with three
    nodes and two links](img/B19153_11_004.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A graph randomly sampled from the set of graphs with three nodes
    and two links
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The strongest and most interesting assumption made by the ![](img/Formula_B19153_11_022.png)
    model is that links are independent (meaning that they do not interfere with each
    other). Unfortunately, it is not true for most real-world graphs, where we observe
    clusters and communities that contradict this rule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The small-world model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced in 1998 by Duncan Watts and Steven Strogatz [3], the small-world
    model tries to imitate the behavior of biological, technological, and social networks.
    The main concept is that real-world networks are not completely random (as in
    the Erdős–Rényi model) but not totally regular either (as in a grid). This kind
    of topology is somewhere in between, which is why we can interpolate it using
    a coefficient. The small-world model produces graphs that have both:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Short paths**: The average distance between any two nodes in the network
    is relatively small, which makes it easy for information to spread quickly throughout
    the network'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High clustering coefficients**: Nodes in the network tend to be closely connected
    to one another, creating dense clusters of nodes'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many algorithms display small-world properties. In the following, we will describe
    the original **Watts–Strogatz** model proposed in [3]. It can be implemented using
    the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: We initialize a graph with ![](img/Formula_B19153_11_023.png) nodes.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each node is connected to its ![](img/Formula_B19153_11_024.png) nearest neighbors
    (or ![](img/Formula_B19153_11_025.png) neighbors if ![](img/Formula_B19153_11_026.png)
    is odd).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each link between nodes ![](img/Formula_B19153_11_027.png) and ![](img/Formula_B19153_11_028.png)
    has a probability ![](img/Formula_B19153_11_029.png) of being rewired between
    ![](img/Formula_B19153_11_030.png) and ![](img/Formula_B19153_11_031.png), where
    ![](img/Formula_B19153_11_032.png) is another random node.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In Python, we can implement it by calling the `nx.watts_strogatz_graph()` function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This produces the following graph:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – A small-world network obtained with the Watts–Strogatz model](img/B19153_11_005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A small-world network obtained with the Watts–Strogatz model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the Erdős–Rényi model, we can repeat the same process with different
    probabilities ![](img/Formula_B19153_11_033.png) to obtain *Figure 11**.6*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – A small-world model with different probabilities for rewiring](img/B19153_11_006.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – A small-world model with different probabilities for rewiring
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: We can see that when ![](img/Formula_B19153_11_034.png), the graph is completely
    regular. On the opposite end, when ![](img/Formula_B19153_11_035.png), the graph
    is completely random as every link has been rewired. We obtain a balanced graph
    between these two extremes with hubs and local clustering.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the Watts–Strogatz model does not produce a realistic degree distribution.
    It also requires a fixed number of nodes, which means it cannot be used for network
    growth. In general, classical methods fail to capture real-world graphs’ full
    diversity and complexity. This motivated the creation of a new family of techniques,
    often referred to as deep graph generation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Generating graphs with graph neural networks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep graph generative models are GNN-based architectures that are more expressive
    than traditional techniques. However, it comes at a cost: they are often too complex
    to be analyzed and understood, like classical methods. We list three main families
    of architecture for deep graph generation: VAEs, GANs, and autoregressive models.
    Other techniques exist, such as normalizing flows or diffusion models, but they
    are less popular and mature than these three.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: This section will describe how to use VAEs, GANs, and autoregressive models
    to generate graphs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Graph variational autoencoders
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As seen in the last chapter, VAEs can be used to approximate an adjacency matrix.
    The **Graph Variational Autoencoder** (**GVAE**) model we saw has two components:
    an encoder and a decoder. The encoder uses two GCNs that share their first layer
    to learn the mean and the variance of each latent normal distribution. The decoder
    then samples the learned distributions to perform the inner product between latent
    variables ![](img/Formula_B19153_11_036.png). In the end, we obtained the approximated
    adjacency matrix ![](img/Formula_B19153_11_037.png).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we used ![](img/Formula_B19153_11_038.png) to predict
    links. However, it is not its only application: it directly gives us the adjacency
    matrix of a network that imitates graphs seen during training. Instead of predicting
    links, we can use this output to generate new graphs. Here is an example of the
    adjacency matrix created by the VGAE model from [*Chapter 10*](B19153_10.xhtml#_idTextAnchor116):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since 2016, this technique has been expanded beyond the GVAE model to also
    output node and edge features. A good example is one of the most popular VAE-based
    graph generative models: **GraphVAE** [4]. Introduced in 2018 by Simonovsky and
    Komodakis, it is designed to generate realistic molecules. This requires the ability
    to differentiate nodes (atoms) and edges (chemical bonds).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphVAE considers graphs ![](img/Formula_B19153_11_039.png), where ![](img/Formula_B19153_11_040.png)
    is the adjacency matrix, ![](img/Formula_B19153_11_041.png) is the edge attribute
    tensor, and ![](img/Formula_B19153_11_042.png) is the node attribute matrix. It
    learns a probabilistic version of the graph ![](img/Formula_B19153_11_043.png)
    with a predefined number of nodes. In this probabilistic version, ![](img/Formula_B19153_11_038.png)
    contains node (![](img/Formula_B19153_11_045.png)) and edge (![](img/Formula_B19153_11_046.png))
    probabilities, ![](img/Formula_B19153_11_047.png) indicates class probabilities
    for edges, and ![](img/Formula_B19153_11_048.png) contains class probabilities
    for nodes. Compared to GVAE, GraphVAE’s encoder is a feed forward network with
    **edge-conditional graph convolutions** (**ECC**), and its decoder is a **multilayer
    perceptron** (**MLP**) with three outputs. The entire architecture is summarized
    in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – GraphVAE’s inference process](img/B19153_11_007.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – GraphVAE’s inference process
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other VAE-based graph generative architectures. However, their
    role is not limited to imitating graphs: they can also embed constraints to guide
    the type of graphs they produce.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: A popular way of adding these constraints is to check them during the decoding
    phase, such as the **Constrained Graph Variational Autoencoder** (**CGVAE**) [5].
    In this architecture, the encoder is a **Gated Graph Convolutional Network** (**GGCN**),
    and the decoder is an autoregressive model. Autoregressive decoders are particularly
    suited for this task, as they can verify every constraint for each step of the
    process. Finally, another technique to add constraints consists of using Lagrangian-based
    regularizers that are faster to compute but less strict in terms of generation
    [6].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Autoregressive models can also be used on their own. The difference with other
    models is that past outputs become part of the current input. In this framework,
    graph generation becomes a sequential decision-making process that considers both
    data and past decisions. For instance, at each step, the autoregressive model
    can create a new node or a new link. Then, the resulting graph is fed to the model
    for the next generation step until we stop it. The following diagram illustrates
    this process:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – The autoregressive process for graph generation](img/B19153_11_008.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – The autoregressive process for graph generation
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we use **Recurrent Neural Networks** (**RNNs**) to implement this
    autoregressive ability. In this architecture, previous outputs are used as inputs
    to compute the current hidden state. In addition, they can process inputs of arbitrary
    length, which is crucial for generating graphs iteratively. However, this computation
    is slower than feedforward networks, as the entire sequence must be processed
    to obtain the final output. The two most popular types of RNNs are the **Gated
    Recurrent Unit** (**GRU**) and **Long Short-Term Memory** (**LSTM**) networks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduced in 2018 by You et al., **GraphRNN** [7] is a direct implementation
    of these techniques for deep graph generation. This architecture uses two RNNs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: A *graph-level RNN* to generate a sequence of nodes (including the initial state)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *edge-level RNN* to predict connections for each newly added node
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The edge-level RNN takes the hidden state of the graph-level RNN as input and
    then feeds it with its own output. This mechanism is illustrated in the following
    diagram at inference time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – GraphRNN’s architecture at inference time](img/B19153_11_009.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – GraphRNN’s architecture at inference time
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Both RNNs are actually completing an adjacency matrix: each new node created
    by the graph-level RNN adds a row and a column, which are filled with zeros and
    ones by the edge-level RNN. In summary, GraphRNN performs the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '*Add new node*: The graph-level RNN initializes the graph and its output if
    fed to the edge-level RNN.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Add new connections*: The edge-level RNN predicts if the new node is connected
    to each of the previous nodes.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Stop graph generation*: The two first steps are repeated until the edge-level
    RNN outputs an EOS token, marking the end of the process.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The GraphRNN can learn different types of graphs (grids, social networks, proteins,
    and so on) and completely outperform traditional techniques. It is an architecture
    of choice to imitate given graphs that should be preferred to GraphVAE.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like VAEs, GANs are a well-known generative model in **ML**. In this framework,
    two neural networks compete in a zero-sum game with two different goals. The first
    neural network is a generator that creates new data, and the second one is a discriminator
    that classifies each sample as real (from the training set) or fake (made by the
    generator).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, two main improvements to the original architecture have been
    proposed. The first one is called the **Wasserstein GAN** (**WGAN**). It improves
    learning stability by minimizing the Wasserstein distance (or Earth Mover’s distance)
    between two probability distributions. This variant is further refined by introducing
    a gradient penalty instead of the original gradient clipping scheme.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Multiple works applied this framework to deep graph generation. Like previous
    techniques, GANs can imitate graphs or generate networks that optimize certain
    constraints. The latter option is handy in applications such as finding new chemical
    compounds with specific properties. This problem is exceptionally vast (over ![](img/Formula_B19153_11_049.png)
    possible combinations) and complex due to its discrete nature.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Proposed by De Cao and Kipf in 2018 [8], the **molecular GAN** (**MolGAN**)
    is a popular solution to this problem. It combines a WGAN with a gradient penalty
    that directly processes graph-structured data and an RL objective to generate
    molecules with desired chemical properties. This RL objective is based on the
    **Deep Deterministic Policy Gradient** (**DDPG**) algorithm, an off-policy actor-critic
    model that uses deterministic policy gradients. MolGAN’s architecture is summarized
    in the following diagram:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – MolGAN’s architecture at inference time](img/B19153_11_010.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – MolGAN’s architecture at inference time
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'This framework is divided into three main components:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The **generator** is an MLP that outputs a node matrix ![](img/Formula_B19153_11_050.png)
    containing the atom types and an adjacency matrix ![](img/Formula_B19153_11_051.png),
    which is actually a tensor containing both the edges and bond types. The generator
    is trained using a linear combination of the WGAN and RL loss. We translate these
    dense representations into sparse objects (![](img/Formula_B19153_11_052.png)
    and ![](img/Formula_B19153_11_053.png)) via categorical sampling.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **discriminator** receives graphs from the generator and the dataset and
    learns to distinguish them. It is solely trained using the WGAN loss.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **reward network** scores each graph. It is trained using the MSE loss based
    on the real score provided by an external system (RDKit in this case).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The discriminator and the reward network use the GNN mode: the Relational-GCN,
    a GCN variant that supports multiple edge types. After several layers of graph
    convolutions, node embeddings are aggregated into a graph-level vector output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_11_054.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B19153_11_055.png) denotes the logistic sigmoid function,
    ![](img/Formula_B19153_11_056.png) and ![](img/Formula_B19153_11_057.png) are
    two MLPs with linear output, and ![](img/Formula_B19153_11_058.png) is the element-wise
    multiplication. A third MLP further processes this graph embedding to produce
    a value between 0 and 1 for the reward network and between ![](img/Formula_B19153_11_059.png)
    and ![](img/Formula_B19153_11_060.png) for the discriminator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: MolGAN produces valid chemical compounds that optimize properties such as drug
    likeliness, synthesizability, and solubility. We will implement this architecture
    in the next section to generate new molecules.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Generating molecules with MolGAN
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep graph generation is not well covered by PyTorch Geometric. Drug discovery
    is the main application of this subfield, which is why generative models can be
    found in specialized libraries. More specifically, there are two popular Python
    libraries for ML-based drug discovery: `DeepChem` and `torchdrug`. In this section,
    we will use DeepChem as it is more mature and directly implements MolGAN.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can use it with `DeepChem` and `tensorflow`. The following
    procedure is based on DeepChem’s example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'We install `DeepChem` ([https://deepchem.io](https://deepchem.io)), which requires
    the following libraries: `tensorflow`, `joblib`, `NumPy`, `pandas`, `scikit-learn`,
    `SciPy`, and `rdkit`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we import the required packages:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We download the `tox21` (*Toxicology in the 21st Century*) dataset, which comprises
    over 6,000 chemical compounds, to analyze their toxicity. We only need their **simplified
    molecular-input line-entry system** (**SMILES**) representations in this example:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is an output of these `smiles` strings:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We only consider molecules with a maximum number of 15 atoms. We filter our
    dataset and create a `featurizer` to convert the `smiles` strings into input features:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We manually loop through our dataset to convert the `smiles` strings:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We remove invalid molecules from the dataset:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we create the `MolGAN` model. It will be trained with a learning rate
    that has an exponential delay schedule:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create the dataset to feed to `MolGAN` in DeepChem’s format:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`MolGAN` uses batch training, which is why we need to define an iterable as
    follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We train the model for `25` epochs:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We generate `1000` molecules:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we check whether these molecules are valid or not:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We compare them to see how many molecules are unique:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We print the generated molecules in a grid:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 11.11 – Molecules generated with MolGAN](img/B19153_11_011.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Molecules generated with MolGAN
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Despite the GAN’s improvements, this training process is quite unstable and
    can fail to produce any meaningful result. The code we presented is sensitive
    to hyperparameter changes and does not generalize well to other datasets, including
    the `QM9` dataset used in the original paper.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, MolGAN’s concept of mixing RL and GANs can be employed beyond drug
    discovery to optimize any type of graph, such as computer networks, recommender
    systems, and so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we saw different techniques to generate graphs. First, we
    explored traditional methods based on probabilities with interesting mathematical
    properties. However, due to their lack of expressiveness, we switched to GNN-based
    techniques that are much more flexible. We covered three families of deep generative
    models: VAE-based, autoregressive, and GAN-based methods. We introduced a model
    from each family to understand how they work in real life.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a GAN-based model that combines a generator, a discriminator,
    and a reward network from RL. Instead of simply imitating graphs seen during training,
    this architecture can also optimize desired properties such as solubility. We
    used DeepChem and TensorFlow to create 24 unique and valid molecules. Nowadays,
    this pipeline is common in the drug discovery industry, where ML can drastically
    speed up drug development.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 12*](B19153_12.xhtml#_idTextAnchor144), *Handling Heterogeneous
    Graphs*, we will explore a new kind of graph that we previously encountered in
    recommender systems and molecules. These heterogeneous graphs contain multiple
    types of nodes and/or links, which requires specific processing. They are more
    general than the regular graphs we talked about and particularly useful in applications
    such as knowledge graphs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] P. Erdös and A. Rényi. *On random graphs I*, Publicationes Mathematicae
    Debrecen, vol. 6, p. 290, 1959\. Available at [https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf](https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] E. N. Gilbert, *Random Graphs*, The Annals of Mathematical Statistics,
    vol. 30, no. 4, pp. 1141–1144, 1959, DOI: 10.1214/aoms/1177706098\. Available
    at: [https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Duncan J. Watts and Steven H. Strogatz. *Collective dynamics of small-world
    networks*, Nature, 393, pp. 440–442, 1998\. Available at [http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf](http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Simonovsky and N. Komodakis. *GraphVAE: Towards Generation of Small
    Graphs Using Variational Autoencoders* CoRR, vol. abs/1802.03480, 2018, [Online].
    Available at [http://arxiv.org/abs/1802.03480](http://arxiv.org/abs/1802.03480).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt. *Constrained Graph
    Variational Autoencoders for Molecule Design*. arXiv, 2018\. DOI: 10.48550/ARXIV.1805.09076\.
    Available at [https://arxiv.org/abs/1805.09076](https://arxiv.org/abs/1805.09076).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Ma, J. Chen, and C. Xiao, Constrained Generation of Semantically Valid
    Graphs via Regularizing Variational Autoencoders. arXiv, 2018\. DOI: 10.48550/ARXIV.1809.02630\.
    Available at [https://arxiv.org/abs/1809.02630](https://arxiv.org/abs/1809.02630).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. *GraphRNN: Generating
    Realistic Graphs with Deep Auto-regressive Models*. arXiv, 2018\. DOI: 10.48550/ARXIV.1802.08773\.
    Available at [https://arxiv.org/abs/1802.08773](https://arxiv.org/abs/1802.08773).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. De Cao and T. Kipf. *MolGAN: An implicit generative model for small
    molecular graphs*. arXiv, 2018\. DOI: 10.48550/ARXIV.1805.11973\. Available at
    [https://arxiv.org/abs/1805.11973](https://arxiv.org/abs/1805.11973).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
