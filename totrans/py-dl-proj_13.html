<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Translation Using GANs for Style Transfer</h1>
                </header>
            
            <article>
                
<p>Welcome to the chapter on <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>). In this chapter, we will be building a neural network that fills in the missing part of a handwritten digit. Previously, we have built a digit classifier for the restaurant chain. But they have also noticed that <span>sometimes, </span>when customers write in their phone number, small sections/regions of the digits are missing. This may be a combination of the customer not having a smooth flow when writing on the iPad application, as well as issues with the iPad application not processing the complete user gesture on the screen. This makes it hard for the handwritten digit classifier to predict the correct digit corresponding to the handwritten number. Now, they want us to reconstruct (generate back) the missing parts of the handwritten numbers so that the classifier receives clear handwritten numbers for conversion into digits.<span> </span><span>With this, the classifier will be able to do a much more accurate job of classifying handwritten digits and the notice gets sent to the right hungry customer!</span></p>
<p>We will mostly focus on the generation/reconstruction of the missing sections of a digit and we will do this with the help of neural inpainting with GANs; see the following flowchart:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ad2ddbe3-8372-4f2f-a581-4fa73724f769.png" style="width:42.00em;height:18.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.1: GAN flowchart</div>
<p>What we'll learn in this chapter is the following:</p>
<ul>
<li>What is a GAN</li>
<li>What is a generator and a discriminator</li>
<li>Coding the model and defining hyperparameters</li>
<li>Building and understanding the training loop</li>
<li>Testing the model</li>
<li>Extending the model to new datasets</li>
</ul>
<p>In this chapter, you will implement the following:</p>
<ul>
<li>Build an MNIST digit classifier</li>
<li>Simulate a dataset of handwritten digits with sections of the handwritten numbers missing</li>
<li>Use the MNIST classifier to predict on noised/masked MNIST digits dataset (simulated dataset)</li>
<li>Implement GAN to generate back the missing regions of the digit</li>
<li>Use the MNIST classifier to predict on the generated digits from GAN</li>
<li>Compare performance between masked data and generated data</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>It would be better if you implement the code snippets as you go along in this chapter, either in a </span><span>Jupyter</span><span> Notebook or any source code editor</span><span>. This will make it easier for you to follow along, as well as understand what each part of the code does.</span></p>
<p>All of the Python files and Jupyter Notebook files for this chapter can be found here: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13</a>.<a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Let's code the implementation!</h1>
                </header>
            
            <article>
                
<p>In this exercise, we will be using the Keras deep learning library, which is a high-level neural network API, capable of running on top of Tensorflow, Theano, or Cognitive Toolkit (CNTK).</p>
<div class="packt_tip">Know the code! We will not spend time on understanding how Keras works but, if you are interested, refer to this easy-to-understand Keras official documentation at <a href="https://keras.io/">https://keras.io/</a>.<a href="https://keras.io/"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing all of the dependencies</h1>
                </header>
            
            <article>
                
<p>We will be using <kbd>numpy</kbd>, <kbd>matplotlib</kbd>, <kbd>keras</kbd>, <kbd>tensorflow</kbd>, and the <kbd>tqdm</kbd> package in this exercise. Here, TensorFlow is used as the backend for Keras. You can install these packages with <kbd>pip</kbd>. For the MNIST data, we will be using the dataset available in the <kbd>keras</kbd> module with a simple import:</p>
<pre>import numpy as np<br/>import random<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>from tqdm import tqdm<br/><br/>from keras.layers import Input, Conv2D<br/>from keras.layers import AveragePooling2D, BatchNormalization<br/>from keras.layers import UpSampling2D, Flatten, Activation<br/>from keras.models import Model, Sequential<br/>from keras.layers.core import Dense, Dropout<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.optimizers import Adam<br/>from keras import backend as k<br/><br/>from keras.datasets import mnist</pre>
<p>It is important that you set <kbd>seed</kbd> for reproducibility:</p>
<pre><strong># set seed for reproducibility</strong><br/>seed_val = 9000<br/>np.random.seed(seed_val)<br/>random.seed(seed_val)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>We will load the MNIST data into our session from the <kbd>keras</kbd> module with <kbd>mnist.load_data()</kbd>. After doing so, we will print the shape and the size of the dataset, as well as the number of classes and unique labels in the dataset:</p>
<pre>(X_train, y_train), (X_test, y_test) =  mnist.load_data()<br/><br/>print('Size of the training_set: ', X_train.shape)<br/>print('Size of the test_set: ', X_test.shape)<br/>print('Shape of each image: ', X_train[0].shape)<br/>print('Total number of classes: ', len(np.unique(y_train)))<br/>print('Unique class labels: ', np.unique(y_train))</pre>
<p><span>We have a dataset with 10 different classes and 60,000 images, with each image having a shape of 28*28 and each class having 6,000 images.</span></p>
<p>Let's plot and see what the handwritten images look like:</p>
<pre><strong># Plot of 9 random images</strong><br/>for i in range(0, 9):<br/>    plt.subplot(331+i) <strong># plot of 3 rows and 3 columns</strong><br/>    plt.axis('off') <strong># turn off axis</strong><br/>    plt.imshow(X_train[i], cmap='gray') <strong># gray scale</strong></pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1425 image-border" src="assets/c7df75a3-b0ef-4719-a96d-765558f6fb40.png" style="width:11.50em;height:8.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.2: Plot of nine MNIST digits from the training set</div>
<p>Let's plot a handwritten digit from each class:</p>
<pre><strong># plotting image from each class</strong><br/>fig=plt.figure(figsize=(8, 4))<br/>columns = 5<br/>rows = 2<br/>for i in range(0, rows*columns):<br/>    fig.add_subplot(rows, columns, i+1)<br/>    plt.title(str(i)) <strong># label</strong> <br/>    plt.axis('off') <strong># turn off axis</strong><br/>    plt.imshow(X_train[np.where(y_train==i)][0], cmap='gray') <strong># gray scale</strong><br/>plt.show()</pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1428 image-border" src="assets/a4f2888b-eb6b-4838-992f-2c0a702f5534.png" style="width:29.92em;height:15.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.3: Plot of an MNIST digit from each class</div>
<p>Look at the maximum and the minimum pixel value in the dataset:</p>
<pre>print('Maximum pixel value in the training_set: ', np.max(X_train))<br/>print('Minimum pixel value in the training_set: ', np.min(X_train))</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e7b3ce82-d1f9-42a7-a8b7-7753ab9b3e8e.png" style="width:35.08em;height:3.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 13.5: Plot of nine noised/masked MNIST digits </span></div>
<p>We see that the maximum pixel value in the dataset is 255 and the minimum is 0.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Type conversion, centering, scaling, and reshaping are some of the pre-processing we will implement in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Type conversion, centering, and scaling</h1>
                </header>
            
            <article>
                
<p>Set the type to <kbd>np.float32</kbd>.</p>
<div class="packt_tip"><strong>Important</strong>: One of the main reasons for doing this is that the weights will all be of the <kbd>float</kbd> type, and multiplication between floating numbers is much faster than between an integer and a float. So it's better to convert the input into the <kbd>float</kbd> type.</div>
<p>For centering, we subtract the dataset by 127.5. The values in the dataset will now range between -127.5 to 127.5.</p>
<p>For scaling, we divide the centered dataset by half of the maximum pixel value in the dataset, that is, <em>255/2</em>. This will result in a dataset with values ranging between -1 and 1:</p>
<pre><strong># Converting integer values to float types</strong> <br/>X_train = X_train.astype(np.float32)<br/>X_test = X_test.astype(np.float32)<br/><br/><strong># Scaling and centering</strong><br/>X_train = (X_train - 127.5) / 127.5<br/>X_test = (X_test - 127.5)/ 127.5<br/>print('Maximum pixel value in the training_set after Centering and Scaling: ', np.max(X_train))<br/>print('Minimum pixel value in the training_set after Centering and Scaling: ', np.min(X_train))</pre>
<p>Let's define a function to rescale the pixel values of the scaled image to range between 0 and 255:</p>
<pre><strong># Rescale the pixel values (</strong><strong>0 and 255)</strong><br/><strong>def upscale(image)</strong>:<br/>    return (image*127.5 + 127.5).astype(np.uint8)<br/><br/><strong># Lets see if this works</strong><br/>z = upscale(X_train[0])<br/>print('Maximum pixel value after upscaling scaled image: ',np.max(z))<br/>print('Maximum pixel value after upscaling scaled image: ',np.min(z))</pre>
<div class="packt_tip"><strong>Matplotlib tip</strong><span>: Rescaling needs to be done so that you avoid errors with </span><span>Matplotlib if you were to use the scaled image as is without upscaling.</span></div>
<p>A plot<span> of <kbd>9</kbd> centered and scaled images after upscaling:</span></p>
<pre>for i in range(0, 9):<br/>    plt.subplot(331+i) <strong># plot of 3 rows and 3 columns</strong><br/>    plt.axis('off') <strong># turn off axis</strong><br/>    plt.imshow(upscale(X_train[i]), cmap='gray') <strong># gray scale</strong></pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1430 image-border" src="assets/aa5cf429-bf29-4d84-87c3-ad1ecc57ebf8.png" style="width:16.50em;height:12.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.4: Plot of nine <span>centered and scaled </span>MNIST digits after upscaling </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Masking/inserting noise</h1>
                </header>
            
            <article>
                
<p>For the needs of this project, we need to simulate a dataset of incomplete digits. So, let's write a function to mask small regions in the original image to form the noised dataset.</p>
<p>The idea is to mask an 8*8 region of the image with the top-left corner of the mask falling between the 9<sup>th</sup> and 13<sup>th</sup> pixel (between index 8 and 12) along both the <em>x</em> and <em>y</em> axis of the image. This is to make sure that we are always masking around the center part of the image:</p>
<pre><strong>def noising(image)</strong>:<br/>    array = np.array(image)<br/>    i = random.choice(range(8,12)) <strong># x coordinate for the top left corner of the mask</strong><br/>    j = random.choice(range(8,12)) <strong># y coordinate for the top left corner of the mask</strong><br/>    array[i:i+8, j:j+8]=-1.0 <strong># setting the pixels in the masked region to -1</strong><br/>    return array<br/><br/>noised_train_data = np.array([*map(noising, X_train)])<br/>noised_test_data = np.array([*map(noising, X_test)])<br/>print('Noised train data Shape/Dimension : ', noised_train_data.shape)<br/>print('Noised test data Shape/Dimension : ', noised_train_data.shape)</pre>
<div class="packt_infobox"><span>The bigger the size of the mask, the harder it will be for the MNIST classifier to predict the right digit. </span></div>
<div>
<div class="packt_tip">Feel free to experiment with the size of the masked region, that is, try smaller/bigger, as well as the location of the mask on the image.</div>
<p>A plot of <kbd>9</kbd> scaled noised images after upscaling:</p>
</div>
<div>
<pre><strong># Plot of 9 scaled noised images after upscaling</strong><br/>for i in range(0, 9):<br/>    plt.subplot(331+i) # plot of 3 rows and 3 columns<br/>    plt.axis('off') # turn off axis<br/>    plt.imshow(upscale(noised_train_data[i]), cmap='gray') # gray scale</pre>
<p>The output is as follows:</p>
</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b4707253-1495-46c1-81b4-701aeb5e733d.png" style="width:16.75em;height:12.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.5: Plot of nine noised/masked MNIST digits </div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reshaping</h1>
                </header>
            
            <article>
                
<p>Reshape the original dataset and the noised dataset to a shape of 60000*28*28*1. This is important since the 2D convolutions expect to receive images of a shape of 28*28*1:</p>
<pre><strong># Reshaping the training data</strong><br/>X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)<br/>print('Size/Shape of the original training set: ', X_train.shape)<br/><br/><strong># Reshaping the noised training data</strong><br/>noised_train_data = noised_train_data.reshape(noised_train_data.shape[0],<br/>                                              noised_train_data.shape[1],<br/>                                              noised_train_data.shape[2], 1)<br/>print('Size/Shape of the noised training set: ', noised_train_data.shape)<br/><br/><strong># Reshaping the testing data</strong><br/>X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)<br/>print('Size/Shape of the original test set: ', X_test.shape)<br/><br/><strong># Reshaping the noised testing data</strong><br/>noised_test_data = noised_test_data.reshape(noised_test_data.shape[0],<br/>                                            noised_test_data.shape[1],<br/>                                            noised_test_data.shape[2], 1)<br/>print('Size/Shape of the noised test set: ', noised_test_data.shape)</pre>
<div class="packt_tip">If you are doing multiple training runs on the GPU, it is always a good idea to clear space on the GPU after each run so that your next run executes efficiently without errors related to <strong>resource exhaustion</strong><em>,</em> which is pretty common with GPUs. This can be done with the following code:<br/>
<kbd>from keras import backend as k</kbd><br/>
<kbd>k.clear_session()</kbd></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MNIST classifier</h1>
                </header>
            
            <article>
                
<div>
<div>
<p>To start off with modeling, let's build a simple <strong>convolutional neural network</strong> (<strong>CNN</strong>)<br/>
digit classifier.</p>
<p>The first layer is a convolution layer that has <kbd>32</kbd> filters of a shape of 3*3, with <kbd>relu</kbd> activation and <kbd>Dropout</kbd> as the regularizer. <span>The second layer is a convolution layer that has <kbd>64</kbd> filters of a shape of 3*3, with <kbd>relu</kbd> activation and <kbd>Dropout</kbd> as the regularizer. </span><span>The third layer is a convolution layer that has <kbd>128</kbd> filters of a shape of 3*3, with <kbd>relu</kbd> activation and <kbd>Dropout</kbd> as the regularizer, which is finally flattened. </span>The fourth layer is a <kbd>Dense</kbd> layer<span> </span>of <kbd>1024</kbd> neurons with <kbd>relu</kbd> activation. The final layer is a <kbd>Dense</kbd> layer with <kbd>10</kbd> neurons corresponding to the 10 classes in the MNIST dataset, and the activation used here is <kbd>softmax</kbd>, <kbd>batch_size</kbd> is set to <kbd>128</kbd>, the <span><kbd>optimizer</kbd> used is <kbd>adam</kbd>, and </span><kbd>validation_split</kbd> is set to <kbd>0.2</kbd>. This means that 20% of the training set will be used as the validation set:</p>
</div>
<pre><strong># input image shape</strong><br/>input_shape = (28,28,1)<br/><br/>def <strong>train_mnist(input_shape, X_train, y_train)</strong>:<br/>    model = Sequential()<br/>    model.add(Conv2D(32, (3, 3), strides=2, padding='same',<br/>                     input_shape=input_shape))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2))<br/><br/>    model.add(Conv2D(64, (3, 3), strides=2, padding='same'))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2)) <br/><br/>    model.add(Conv2D(128, (3, 3), padding='same'))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2))<br/>    model.add(Flatten())<br/><br/>    model.add(Dense(1024, activation = 'relu'))<br/>    model.add(Dense(10, activation='softmax'))<br/>    model.compile(loss = 'sparse_categorical_crossentropy',<br/>                  optimizer = 'adam', metrics = ['accuracy'])<br/>    model.fit(X_train, y_train, batch_size = 128,  <br/>              epochs = 3, validation_split=0.2, verbose = 1 )<br/>    return model<br/><br/>mnist_model = train_mnist(input_shape, X_train, y_train)</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65564be8-ebeb-46d3-8d82-ec7489f86a7c.png" style="text-align: center;font-size: 1em;"/></p>
</div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.6: MNIST CNN classifier training for three epochs</div>
<p>Use the built CNN digit classifier on the masked images to get a measure of its performance on digits that are missing small sections:</p>
<pre><strong># prediction on the masked images</strong><br/>pred_labels = mnist_model.predict_classes(noised_test_data)<br/>print('The model model accuracy on the masked images is:',np.mean(pred_labels==y_test)*100)</pre>
<p>On the masked images, the CNN digit classifier is 74.9% accurate. It might be slightly different when you run it, but it will still be very close.</p>
<div class="packt_infobox">We have not used maxpooling in the preceding classifier. Try building the same classifier with maxpooling or other pooling options.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining hyperparameters for GAN</h1>
                </header>
            
            <article>
                
<p>The following are some of the hyperparameters defined that we will be using throughout the code and are totally configurable:</p>
<pre><strong># Smoothing value</strong><br/>smooth_real = 0.9<br/><br/><strong># Number of epochs</strong><br/>epochs = 5<br/><br/><strong># Batchsize</strong><br/>batch_size = 128<br/><br/><strong># Optimizer for the generator</strong><br/>optimizer_g = Adam(lr=0.0002, beta_1=0.5)<br/><br/><strong># Optimizer for the discriminator</strong><br/>optimizer_d = Adam(lr=0.0004, beta_1=0.5)<br/><br/><strong># Shape of the input image</strong><br/>input_shape = (28,28,1)</pre>
<div class="packt_tip">Experiment with different learning rates, optimizers, batch sizes, and smoothing values to see how these factors affect the quality of your model and, if you get better results, show it to the deep learning community.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the GAN model components</h1>
                </header>
            
            <article>
                
<p>With the idea that the final GAN model will be able to fill in the part of the image that is missing (masked), let's define the generator. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the generator</h1>
                </header>
            
            <article>
                
<p>The generator that we are using here is a simple convolution autoencoder that is a combination of two parts—an encoder and a decoder. </p>
<p>In the encoder, we have the following:</p>
<ul>
<li>The first layer is a convolution 2D layer with <kbd>32</kbd> filters of a size of 3*3, followed by batch normalization, with activation as <kbd>relu</kbd>, followed by downsampling done with <kbd>AveragePooling2D</kbd> of size 2*2</li>
<li>The second layer is a convolution 2D layer with <kbd>64</kbd> filters of a size of 3*3, followed by batch normalization, with activation as <kbd>relu</kbd>, followed by downsampling with <kbd>AveragePooling2D</kbd> of a size of 2*2</li>
<li>The third layer or the final layer in this encoder part is again a <span>convolution 2D layer with <kbd>128</kbd> filters of a size of 3*3, batch normalization, with activation as </span><kbd>relu</kbd></li>
</ul>
<p>In the decoder, we have the following:</p>
<ul>
<li>The first layer is a convolution 2D layer with <kbd>128</kbd> filters of a size of 3*3 with activation as<span> </span><kbd>relu</kbd>, followed by upsampling done with <kbd>UpSampling2D</kbd></li>
<li>The second layer is a convolution 2D layer with <kbd>64</kbd> filters of a size of 3*3 with activation as<span> </span><kbd>relu</kbd>, followed by upsampling with <kbd>UpSampling2D</kbd></li>
<li>The third layer or the final layer in this decoder part is again a <span>convolution 2D layer with <kbd>1</kbd> filters of a size of 3*3 with activation as </span><kbd>tanh</kbd></li>
</ul>
<p>Remember, in the encoder, if you have <kbd>32</kbd>, <kbd>64</kbd>, <kbd>128</kbd> filters, it should be followed by <kbd>128</kbd>, <kbd>64</kbd>, <kbd>image_channels</kbd> filters in the decoder. <kbd>image_channels</kbd> is the number of channels in the input image, which is one in the MNIST dataset. If you have <kbd>64</kbd>, <kbd>128</kbd>, <kbd>256</kbd>, <kbd>512</kbd> filters in the first, second, third, and fourth layers of the encoder, the following filters in the decoder should be <kbd>256</kbd>, <kbd>128</kbd>, <kbd>64</kbd>, <span><kbd>image_channels</kbd>:</span></p>
<pre><strong>def img_generator(input_shape)</strong>:<br/>    generator = Sequential()<br/>    generator.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape)) <strong># 32 filters</strong><br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu'))<br/>    generator.add(AveragePooling2D(pool_size=(2, 2)))<br/>    <br/>    generator.add(Conv2D(64, (3, 3), padding='same')) <strong># 64 filters</strong><br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu'))<br/>    generator.add(AveragePooling2D(pool_size=(2, 2)))<br/>    <br/>    generator.add(Conv2D(128, (3, 3), padding='same')) <strong># 128 filters</strong><br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu')) <br/>    <br/>    generator.add(Conv2D(128, (3, 3), padding='same')) <strong># 128 filters</strong><br/>    generator.add(Activation('relu'))<br/>    generator.add(UpSampling2D((2,2)))<br/>    <br/>    generator.add(Conv2D(64, (3, 3), padding='same')) <strong># 64 filters</strong><br/>    generator.add(Activation('relu'))<br/>    generator.add(UpSampling2D((2,2)))<br/>    <br/>    generator.add(Conv2D(1, (3, 3), activation='tanh', padding='same')) <strong># 1 filter</strong><br/>    return generator</pre>
<p>Two important things to remember here about the final convolution layer in the generator. One is to use <kbd>tanh</kbd> as the activation function since the dataset range is between -1 and 1, and the other is, to use the same number of filter(s) as the number of channels in the input image. This is to make sure that the image being generated has the same number of channels as the input image.</p>
<div class="packt_infobox">If you decide to center and scale your data like we have done in this exercise, you need to use batch normalization in the generator during downsampling, otherwise, the loss will not converge. You can witness the effects of not using the batch normalization by training the generator without the <span>batch normalization layer.</span></div>
<p>In the following <kbd>summary</kbd> of the generator, if you refer to the output shape, you see the downscaling or compression of the image in the first half of the network and the upscaling of the images in the second half of the network:</p>
<pre><span><strong># print generator summary</strong><br/>img_generator(input_shape).summary()</span></pre>
<p>The output is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/f094db57-93b0-4f6e-8c45-8e36752266ac.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.7: Summary of the generator (autoencoder)</div>
<div>
<div class="packt_tip">Consider the following when you are not obtaining good results with the autoencoder. Use <kbd>AveragePooling2D</kbd> first and then check out <kbd>MaxPooling2D</kbd> for downsampling. Use <kbd>LeakyReLU</kbd> first and then <kbd>relu</kbd> next. For all of the convolution layers except the final one, use either <kbd>LeakyReLU</kbd> or <kbd>relu</kbd> activation. Try using a deeper autoencoder. Feel free to use more filters in the convolution layers, play with the filter sizes and the pooling sizes. </div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the discriminator</h1>
                </header>
            
            <article>
                
<p> The discriminator is a simple CNN binary classifier that takes in the image generated by the generator and tries to classify the image as original or fake.</p>
<p>The first layer is a convolution 2D layer with <kbd>64</kbd> filters of a size of 3*3  with the activation as <kbd>LeakyReLU</kbd> and <kbd>Dropout</kbd> as the regularizer. The second<sup> </sup><span>and third</span> layers are the same as the first layer except the second layer has <kbd>128</kbd> filters and the third layer has <kbd>256</kbd> filters. The final layer is a <kbd>Dense</kbd> layer with <kbd>sigmoid</kbd> activation since we are doing a binary classification:</p>
<pre>def <strong>img_discriminator(input_shape)</strong>:<br/>    discriminator = Sequential()<br/>    discriminator.add(Conv2D(64, (3, 3), strides=2, padding='same', input_shape=input_shape, activation = 'linear'))<br/>    discriminator.add(LeakyReLU(0.2))<br/>    discriminator.add(Dropout(0.2))<br/>    <br/>    discriminator.add(Conv2D(128, (3, 3), strides=2, padding='same', activation = 'linear'))<br/>    discriminator.add(LeakyReLU(0.2))<br/>    discriminator.add(Dropout(0.2))<br/>    <br/>    discriminator.add(Conv2D(256, (3, 3), padding='same', activation = 'linear'))<br/>    discriminator.add(LeakyReLU(0.2))<br/>    discriminator.add(Dropout(0.2))<br/>    <br/>    discriminator.add(Flatten())<br/>    discriminator.add(Dense(1, activation='sigmoid'))<br/><br/>    return discriminator<br/><br/><strong># print summary of the discriminator</strong><br/>img_discriminator(input_shape).summary()</pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fb119d52-9834-457c-a53d-e9c313a387f4.png" style="width:35.25em;height:31.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.8: Summary of the discriminator</div>
<div>
<div>
<div class="packt_tip">Play around with the parameters of the discriminator to suit the needs of the problem you are trying to solve. Include a <kbd>MaxPooling</kbd> layer in the model if needed.</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the DCGAN</h1>
                </header>
            
            <article>
                
<p><span>The following function pipes the input followed by the generator, which is then followed by the discriminator to form the </span>DCGAN<span> architecture:</span></p>
<pre>def <strong>dcgan(discriminator, generator, input_shape)</strong>:<br/>    <strong># Set discriminator as non trainable before compiling GAN</strong><br/>    discriminator.trainable = False<br/><br/>    <strong># Accepts the noised input</strong><br/>    gan_input = Input(shape=input_shape)<br/>    <br/>    <strong># Generates image by passing the above received input to the generator</strong><br/>    gen_img = generator(gan_input)<br/>    <br/>    <strong># Feeds the generated image to the discriminator</strong><br/>    gan_output = discriminator(gen_img)<br/>    <br/>    <strong># Compile everything as a model with binary crossentropy loss</strong><br/>    gan = Model(inputs=gan_input, outputs=gan_output)<br/>    return gan</pre>
<p>If you have not seen how to use the <kbd>Model</kbd> function API before, please visit the detailed documentation by Keras on using the <kbd>Model</kbd> function API and compiling it at <a href="https://keras.io/models/model/">https://keras.io/models/model/</a>.<a href="https://keras.io/models/model/"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training GAN</h1>
                </header>
            
            <article>
                
<p>We've built the components of the GAN.  Let's train the model in the next steps!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting the training – part 1</h1>
                </header>
            
            <article>
                
<p>During each epoch, the following function plots <kbd>9</kbd> generated images. For comparison, it will also plot the corresponding <kbd>9</kbd> original target images and <kbd>9</kbd> noised input images. We need to use the <kbd>upscale</kbd> function we've defined when plotting to make sure the images are scaled to range between 0 and 255, so that you do not encounter issues when plotting:</p>
<pre>def <strong>generated_images_plot(original, noised_data, generator)</strong>:<br/>    <br/>    print('NOISED')<br/>    for i in range(9):<br/>        plt.subplot(331 + i)<br/>        plt.axis('off')<br/>        plt.imshow(upscale(np.squeeze(noised_data[i])), cmap='gray') <strong># upscale for plotting</strong><br/>    plt.show()<br/>    <br/>    print('GENERATED')<br/>    for i in range(9):<br/>        pred = generator.predict(noised_data[i:i+1], verbose=0)<br/>        plt.subplot(331 + i)<br/>        plt.axis('off')<br/>        plt.imshow(upscale(np.squeeze(pred[0])), cmap='gray') <strong># upscale to avoid plotting errors</strong><br/>    plt.show()<br/>    <br/>    print('ORIGINAL')<br/>    for i in range(9):<br/>        plt.subplot(331 + i)<br/>        plt.axis('off')<br/>        plt.imshow(upscale(np.squeeze(original[i])), cmap='gray') <strong># upscale for plotting</strong><br/>    plt.show()</pre>
<p>The output of this function is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1a05ba02-d646-4d6b-87cd-a43022388eef.png" style="width:18.25em;height:42.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.9: Sample/expected output of the generated_images_plot <span>function</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting the training – part 2</h1>
                </header>
            
            <article>
                
<p>Let's define another function that plots the images generated during each epoch. To reflect the difference, we will also include the original and the masked/noised images in the plot.</p>
<p>The top row contains the original images, the middle row contains the masked images, and the bottom row contains the generated images.</p>
<p>The plot has <kbd>12</kbd> rows with the sequence, row 1 - original, row 2 - masked, row3 - generated, row 4 - original, row5 - masked,..., row 12 - generated.</p>
<p>Let's take a look at the code for the same:</p>
<pre>def <strong>plot_generated_images_combined(original, noised_data, generator)</strong>:<br/>    rows, cols = 4, 12<br/>    num = rows * cols<br/>    image_size = 28<br/><br/>    generated_images = generator.predict(noised_data[0:num])<br/>    <br/>    imgs = np.concatenate([original[0:num], noised_data[0:num], generated_images])<br/>    imgs = imgs.reshape((rows * 3, cols, image_size, image_size))<br/>    imgs = np.vstack(np.split(imgs, rows, axis=1))<br/>    imgs = imgs.reshape((rows * 3, -1, image_size, image_size))<br/>    imgs = np.vstack([np.hstack(i) for i in imgs])<br/>    imgs = upscale(imgs)<br/>    plt.figure(figsize=(8,16))<br/>    plt.axis('off')<br/>    plt.title('Original Images: top rows, '<br/>              'Corrupted Input: middle rows, '<br/>              'Generated Images: bottom rows')<br/>    plt.imshow(imgs, cmap='gray')<br/>    plt.show() </pre>
<p>The output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1433 image-border" src="assets/1b9b9b1b-321a-41d2-8a75-0c0494f4baae.png" style="width:38.92em;height:34.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.10: Sample/expected output from the plot_generated_images_combined function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training loop</h1>
                </header>
            
            <article>
                
<p>Now we are at the most important part of the code; the part where all of the functions we previously defined will be used. The following are the steps:</p>
<ol>
<li>Load the generator by calling the <kbd>img_generator()</kbd> function. </li>
<li>Load the discriminator by calling the <kbd>img_discriminator()</kbd> function and compile it with the binary cross-entropy loss and optimizer as <kbd>optimizer_d</kbd>, which we have defined under the hyperparameters section.</li>
<li>Feed the generator and the discriminator to the <kbd>dcgan()</kbd> function <span>and compile it with the binary cross-entropy loss and optimizer as <kbd>optimizer_g</kbd>, which we have defined under the hyperparameters section.</span></li>
<li>Create a new batch of original images and masked images. Generate new fake images by feeding the batch of masked images to the generator.</li>
<li>Concatenate the original and generated images so that the first 128 images are all original and the next 128 images are all fake. It is important that you do not shuffle the data here, otherwise it will be hard to train. Label the generated images as <kbd>0</kbd> and original images as <kbd>0.9</kbd> instead of 1. This is one-sided label smoothing on the original images. The reason for using label smoothing is to make the network resilient to adversarial examples. It's called one-sided because we are smoothing labels only for the real images.</li>
<li>S<span>et <kbd>discriminator.trainable</kbd> to <kbd>True</kbd> to enable training of the discriminator and f</span>eed this set of 256 images and their corresponding labels to the discriminator for classification. </li>
<li>Now, set <kbd>discriminator.trainable</kbd> to <kbd>False</kbd> and feed a new batch of 128 masked images labeled as 1 to the GAN (DCGAN) for classification. It is important to set <span><kbd>discriminator.trainable</kbd> to <kbd>False</kbd> to make sure the discriminator is not getting trained while training the generator.</span></li>
<li>Repeat steps 4 through 7 for the desired number of epochs.</li>
</ol>
<div class="packt_infobox"><span>Batch size used here is 128.</span></div>
<p>We have placed the <kbd>plot_generated_images_combined()</kbd> function and the <kbd>generated_images_plot()</kbd> function so that we get a plot generated by both functions<span> after the first iteration in the first epoch and after the end of each epoch.</span></p>
<p>Feel free to place these plot functions according to the frequency of plots you need displayed:</p>
<div>
<pre>def <strong>train(X_train, noised_train_data,</strong><br/><strong>          input_shape, smooth_real,</strong><br/><strong>          epochs, batch_size,</strong><br/><strong>          optimizer_g, optimizer_d)</strong>:<br/><br/>    <strong># define two empty lists to store the discriminator</strong><br/><strong>    # and the generator losses</strong><br/>    discriminator_losses = []<br/>    generator_losses = []<br/>    <br/>    <strong># Number of iteration possible with batches of size 128</strong><br/>    iterations = X_train.shape[0] // batch_size<br/><br/>    <strong># Load the generator and the discriminator</strong><br/>    generator = img_generator(input_shape)<br/>    discriminator = img_discriminator(input_shape)<br/>    <br/>    <strong># Compile the discriminator with binary_crossentropy loss</strong><br/>    discriminator.compile(loss='binary_crossentropy',optimizer=optimizer_d)<br/>    <br/>    <strong># Feed the generator and the discriminator to the function dcgan</strong><br/><strong>    # to form the DCGAN architecture</strong><br/>    gan = dcgan(discriminator, generator, input_shape)<br/>    <br/>    <strong># Compile the DCGAN with binary_crossentropy loss</strong><br/>    gan.compile(loss='binary_crossentropy', optimizer=optimizer_g)<br/><br/>    for i in range(epochs):<br/>        print ('Epoch %d' % (i+1))<br/>        <strong># Use tqdm to get an estimate of time remaining</strong><br/>        for j in tqdm(range(1, iterations+1)):<br/>            <br/>            <strong># batch of original images (batch = batchsize)</strong><br/>            original = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]<br/>            <br/>            <strong># batch of noised images (batch = batchsize)</strong><br/>            noise = noised_train_data[np.random.randint(0, noised_train_data.shape[0], size=batch_size)]<br/><br/>            <strong># Generate fake images</strong><br/>            generated_images = generator.predict(noise)<br/>            <br/>            <strong># Labels for generated data</strong><br/>            dis_lab = np.zeros(2*batch_size)<br/>            <br/>            <strong># data for discriminator</strong><br/>            dis_train = np.concatenate([original, generated_images])<br/>            <br/>            <strong># label smoothing for original images</strong><br/>            dis_lab[:batch_size] = smooth_real<br/>            <br/>            <strong># Train discriminator on original images</strong><br/>            discriminator.trainable = True<br/>            discriminator_loss = discriminator.train_on_batch(dis_train, dis_lab)<br/>            <br/>            <strong># save the losses</strong> <br/>            discriminator_losses.append(discriminator_loss)<br/>            <br/>            <strong># Train generator</strong><br/>            gen_lab = np.ones(batch_size)<br/>            discriminator.trainable = False<br/>            sample_indices = np.random.randint(0, X_train.shape[0], size=batch_size)<br/>            original = X_train[sample_indices]<br/>            noise = noised_train_data[sample_indices]<br/>            <br/>            generator_loss = gan.train_on_batch(noise, gen_lab)<br/>            <br/>            <strong># save the losses</strong><br/>            generator_losses.append(generator_loss)<br/>            <br/>            if i == 0 and j == 1:<br/>                print('Iteration - %d', j)<br/>                generated_images_plot(original, noise, generator)<br/>                plot_generated_images_combined(original, noise, generator)<br/>        <br/>        print("Discriminator Loss: ", discriminator_loss,\<br/>              ", Adversarial Loss: ", generator_loss)<br/>        <br/>        <strong># training plot 1</strong><br/>        generated_images_plot(original, noise, generator)<br/>        <strong># training plot 2</strong><br/>        plot_generated_images_combined(original, noise, generator)<br/>    <br/>    <br/>    <strong># plot the training losses</strong><br/>    plt.figure()<br/>    plt.plot(range(len(discriminator_losses)), discriminator_losses,<br/>             color='red', label='Discriminator loss')<br/>    plt.plot(range(len(generator_losses)), generator_losses,<br/>             color='blue', label='Adversarial loss')<br/>    plt.title('Discriminator and Adversarial loss')<br/>    plt.xlabel('Iterations')<br/>    plt.ylabel('Loss (Adversarial/Discriminator)')<br/>    plt.legend()<br/>    plt.show()<br/>    <br/>    return generator<br/><br/>generator = train(X_train, noised_train_data,<br/>                  input_shape, smooth_real,<br/>                  epochs, batch_size,<br/>                  optimizer_g, optimizer_d)</pre>
<p>The output is as follows:</p>
</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/9aeddfc1-d406-4376-98e2-ce070c29ce8b.png" style="width:20.33em;height:48.58em;"/></div>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1434 image-border" src="assets/13b3e1eb-d6ea-457b-a00e-cbefbeb24557.png" style="width:43.00em;height:38.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.11.1: Generated images plotted with training plots at the end of the first iteration of epoch 1</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/06d5ad24-8d9e-4841-b8c9-5d5d721ff8e1.png" style="width:20.58em;height:48.83em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign">           <img class="alignnone size-full wp-image-1440 image-border" src="assets/596bcf52-a662-4628-a6b4-c812009afc2e.png" style="width:43.00em;height:37.92em;"/>             </div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.11.2: <span><span>Generated images plotted with training plots at the end of epoch 2</span></span></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/514a895e-8ad0-441c-9a93-00c040784658.png" style="width:20.33em;height:48.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign">           <img src="assets/ea8432b1-3134-44eb-a3a5-a90d4f6fc350.png" style="width:41.67em;height:36.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.11.3: <span><span>Generated images plotted with training plots at the end of epoch 5</span></span></div>
<div class="CDPAlignCenter CDPAlign">   <img class="alignnone size-full wp-image-1437 image-border" src="assets/0a3a9e71-8a94-4698-b26d-a249f113c972.png" style="width:32.42em;height:23.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.12: Plot of discriminator and adversarial loss during training</div>
<div class="packt_tip">Play around with the learning rate for both the generator and the discriminator to find the optimal values for your use case. In general, when training GANs, you train it for a large number of epochs and then use the preceding loss versus iteration plot to identify the minimum spot you would like for the training to stop.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictions</h1>
                </header>
            
            <article>
                
<p>This is what we've been building to: making predictions!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN classifier predictions on the noised and generated images</h1>
                </header>
            
            <article>
                
<p><span>Now, we will call the generator on the masked MNIST test data to generate images, that is, fill in the missing part of the digits:</span></p>
<div>
<pre><strong># restore missing parts of the digit with the generator</strong><br/>gen_imgs_test = generator.predict(noised_test_data)</pre></div>
<p class="mce-root">Then, we will pass the generated MNIST digits to the digit classifier we have modeled already:</p>
<div>
<pre><strong># predict on the restored/generated digits</strong><br/>gen_pred_lab = mnist_model.predict_classes(gen_imgs_test)<br/>print('The model model accuracy on the generated images is:',np.mean(gen_pred_lab==y_test)*100)</pre></div>
<p>The MNIST CNN classifier is 87.82% accurate on the generated data. </p>
<p>The following is a plot showing 10 generated images by the generator, the actual label of the generated image, and the label predicted by the digit classifier after processing the generated image:</p>
<div>
<pre><strong># plot of 10 generated images and their predicted label</strong><br/>fig=plt.figure(figsize=(8, 4))<br/>plt.title('Generated Images')<br/>plt.axis('off') <br/>columns = 5<br/>rows = 2<br/>for i in range(0, rows*columns):<br/>    fig.add_subplot(rows, columns, i+1)<br/>    plt.title('Act: %d, Pred: %d'%(gen_pred_lab[i],y_test[i])) # label <br/>    plt.axis('off') # turn off axis<br/>    plt.imshow(upscale(np.squeeze(gen_imgs_test[i])), cmap='gray') # gray scale<br/>plt.show()</pre>
<p>The output is as follows:</p>
</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ff167a28-9428-405b-a0cd-6ec9973e1064.png" style="width:33.50em;height:18.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.13:  Plot of MNIST classifier predictions on the generated images</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scripts in modular form</h1>
                </header>
            
            <article>
                
<p>The entire script can be split into four modules named <kbd>train_mnist.py</kbd>, <kbd>training_plots.py</kbd>, <kbd>GAN.py</kbd>, and <kbd>train_gan.py</kbd>. Store these in a folder of your choice, for example, <kbd>gan</kbd>. Set <kbd>gan</kbd> as the project folder in your favorite source code editor and just run the <kbd>train_gan.py</kbd> file. </p>
<p>The <kbd><span>train_gan.py</span></kbd> Python file will import functions from all of the other modules in places where they're needed for execution.</p>
<p>Now, let's walk through the contents of each file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 1 – train_mnist.py</h1>
                </header>
            
            <article>
                
<p>This Python file contains the <kbd>train_mnist()</kbd> function that we have used previously to train a CNN classifier on MNIST digits:</p>
<pre>"""This module is used to train a CNN on mnist."""<br/>from keras.layers import Conv2D<br/>from keras.layers import Flatten, Activation<br/>from keras.models import Sequential<br/>from keras.layers.core import Dense, Dropout<br/><br/><br/>def train_mnist(input_shape, X_train, y_train):<br/>    """Train CNN on mnist data."""<br/>    model = Sequential()<br/>    model.add(Conv2D(32, (3, 3), strides=2, padding='same',<br/>                     input_shape=input_shape))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2))<br/>    model.add(Conv2D(64, (3, 3), strides=2, padding='same'))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2))<br/>    model.add(Conv2D(128, (3, 3), padding='same'))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.2))<br/>    model.add(Flatten())<br/>    model.add(Dense(1024, activation='relu'))<br/>    model.add(Dense(10, activation='softmax'))<br/>    model.compile(loss='sparse_categorical_crossentropy',<br/>                  optimizer='adam', metrics=['accuracy'])<br/>    model.fit(X_train, y_train, batch_size=128,<br/>              epochs=3, validation_split=0.2, verbose=1)<br/>    return model</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 2 – training_plots.py</h1>
                </header>
            
            <article>
                
<p>This Python file contains the four functions, <kbd>upscale()</kbd>, <kbd>generated_images_plot()</kbd>, <kbd>plot_generated_images_combined()</kbd>, and <kbd>plot_training_loss()</kbd>:</p>
<pre>"""This module contains functions to plot image generated when training GAN."""<br/><br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/><br/>def upscale(image):<br/>    """Scale the image to 0-255 scale."""<br/>    return (image*127.5 + 127.5).astype(np.uint8)<br/><br/><br/>def generated_images_plot(original, noised_data, generator):<br/>    """Plot subplot of images during training."""<br/>    print('NOISED')<br/>    for i in range(9):<br/>        plt.subplot(331 + i)<br/>        plt.axis('off')<br/>        plt.imshow(upscale(np.squeeze(noised_data[i])), cmap='gray')<br/>    plt.show()<br/>    print('GENERATED')<br/>    for i in range(9):<br/>        pred = generator.predict(noised_data[i:i+1], verbose=0)<br/>        plt.subplot(331 + i)<br/>        plt.axis('off')<br/>        plt.imshow(upscale(np.squeeze(pred[0])), cmap='gray')<br/>    plt.show()</pre>
<div class="packt_infobox">For the remaining part of this code, please visit: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/training_plots.py</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 3 – GAN.py</h1>
                </header>
            
            <article>
                
<p>This module contains the DCGAN components, namely <kbd>img_generator()</kbd>, <kbd>img_discriminator()</kbd>, and <kbd>dcgan()</kbd>:</p>
<pre>"""This module contains the DCGAN components."""<br/>from keras.layers import Input, Conv2D, AveragePooling2D<br/>from keras.layers import UpSampling2D, Flatten, Activation, BatchNormalization<br/>from keras.models import Model, Sequential<br/>from keras.layers.core import Dense, Dropout<br/>from keras.layers.advanced_activations import LeakyReLU<br/><br/><br/>def img_generator(input_shape):<br/>    """Generator."""<br/>    generator = Sequential()<br/>    generator.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))<br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu'))<br/>    generator.add(AveragePooling2D(pool_size=(2, 2)))<br/>    generator.add(Conv2D(64, (3, 3), padding='same'))<br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu'))<br/>    generator.add(AveragePooling2D(pool_size=(2, 2)))<br/>    generator.add(Conv2D(128, (3, 3), padding='same'))<br/>    generator.add(BatchNormalization())<br/>    generator.add(Activation('relu'))<br/>    generator.add(Conv2D(128, (3, 3), padding='same'))<br/>    generator.add(Activation('relu'))<br/>    generator.add(UpSampling2D((2, 2)))<br/>    generator.add(Conv2D(64, (3, 3), padding='same'))<br/>    generator.add(Activation('relu'))<br/>    generator.add(UpSampling2D((2, 2)))<br/>    generator.add(Conv2D(1, (3, 3), activation='tanh', padding='same'))<br/>    return generator</pre>
<div class="packt_infobox">For the remaining part of this code, please visit: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/GAN.py</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 4 – train_gan.py</h1>
                </header>
            
            <article>
                
<p>In this module, we will include the hyperparameters, pre-process the data, generate synthetic data, train the GAN, train the CNN classifier, and import all of the necessary functions from other modules:</p>
<pre>import numpy as np<br/>from training_plots import upscale, generated_images_plot, plot_training_loss<br/>from training_plots import plot_generated_images_combined<br/>from keras.optimizers import Adam<br/>from keras import backend as k<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm<br/><br/>from GAN import img_generator, img_discriminator, dcgan<br/><br/>from keras.datasets import mnist<br/>from train_mnist import train_mnist<br/><br/>%matplotlib inline<br/># Smoothing value<br/>smooth_real = 0.9<br/># Number of epochs<br/>epochs = 5<br/># Batchsize<br/>batch_size = 128<br/># Optimizer for the generator<br/>optimizer_g = Adam(lr=0.0002, beta_1=0.5)<br/># Optimizer for the discriminator<br/>optimizer_d = Adam(lr=0.0004, beta_1=0.5)<br/># Shape of the input image<br/>input_shape = (28, 28, 1)</pre>
<div class="packt_tip packt_infobox">For the remaining part of this module, please visit: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter13/train_gan.py</a><br/>
<br/>
You can use the same modules you have created to train on fashion MNIST data. All you have to do is replace line 11 in the <kbd>train_gan.py</kbd> file with (<kbd>from keras.datasets import fashion_mnist</kbd>) and replace line 28 with (<kbd>(X_train, y_train), (X_test, y_test) =  fashion_mnist.load_data()</kbd>). The results will be good but not excellent since the parameters set here work best on the MNIST digit data. This will be a good exercise for you to get incredible results without much effort. </div>
<div class="packt_tip">Here is a resource on tips to train GANs that you must check out:<br/>
<a href="https://github.com/soumith/ganhacks"><br/>
https://github.com/soumith/ganhacks.</a></div>
<p>The Jupyter Notebook code files for the preceding DCGAN MNIST inpainting can be found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_MNIST.ipynb</a>. The <span>Jupyter Notebook code files for the </span>DCGAN<span> Fashion MNIST inpainting can be found at </span><span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb</a>.<a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2014/DCGAN_Fashion_MNIST.ipynb"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The conclusion to the project</h1>
                </header>
            
            <article>
                
<p>The goal of this project was to build a GAN to solve the problem of regenerating missing parts/regions of handwritten digits. In the initial chapters, we applied deep learning to enable customers of a restaurant chain to write their phone numbers in a simple iPad application to get a text notification that their party could be seated. The use case of this chapter was to apply deep learning to generate missing parts of the digits of the phone number so that a text notification can be sent to the right person. </p>
<p>The CNN digit classifier model accuracy hit 98.84% on the MNIST validation data. With the data we generated to simulate missing parts of a digit when fed to the CNN digit classifier, the model was only 74.90% accurate.</p>
<p>The same dataset with missing sections of the digit was passed to the generator to recover the missing parts. The resulting digits were then passed to the CNN classifier and the model was 87.82% accurate. See if you can tweak both the CNN classifier and the GAN to generate clearer digits, as well as much higher accuracy on these generated images.</p>
<p>Let's follow the same technique we have been following in the previous chapters for evaluating the performance of the models from the restaurant chain point of view.</p>
<p>What are the implications of this accuracy? Let's calculate the incidence of an error occurring that would result in a customer service issue (that is, the customer not getting the text that their table is ready and getting upset for an excessively long wait time at the restaurant).</p>
<p>Each customer's phone number is ten digits long. Let's assume our hypothetical restaurant has an average of 30 tables at each location and those tables turn over two times per night during the rush hour when the system is likely to be used, and finally, the restaurant chain has 35 locations. This means that each day of operation there are approximately 21,000 handwritten numbers captured (30<span> </span>tables<span> </span>x 2 turns/day x 35 locations x 10 digit phone number).</p>
<p>Obviously, all digits must be correctly classified for the text to get to the proper waiting restaurant patron. So any single digit misclassification causes a failure. With the simulated data, the model accuracy was 74.90%, which means a total of 5,271 digits are misclassified. With the recovered data (on the simulated data) from the generator of the trained GAN, the model accuracy was 87.82%, which would improperly classify 2,558 digits per day in our example. The worst case for the hypothetical scenario would be if there occurred only one improperly classified digit in each phone number. Since there are only 2,100 patrons and corresponding phone numbers, this would mean that every phone number had an error in classification (100% failure) and not a single customer would get their text notification that their party could be seated! The best case scenario would be if all 10 digits were misclassified in each phone number and that would result in 263 wrong phone numbers out of 2,100 (12.5% failure rate). Still not a level of performance the restaurant chain would be likely to be happy with, so you can see why we'd need to continue fine-tuning the models to get the maximum performance possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In the project in this chapter, we have successfully built a deep convolution GAN in Keras on handwritten MNIST digits. We understood the function of the generator and the discriminator component of the GAN. We have defined some key hyperparameters, as well as, in some places, reasoned with why we used what we did. Finally, we tested the GAN's performance on unseen data and determined that we succeeded in achieving our goals.</p>


            </article>

            
        </section>
    </body></html>