- en: Parameter Learning Using Maximum Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the state inference in the case of a **Hidden
    Markov Model** (**HMM**). We tried to predict the next state for an HMM using
    the information of previous state transitions. But in each cases, we had assumed
    that we already knew the transition and emission probabilities of the model. But
    in real-life problems, we usually need to learn these parameters from our observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will try to estimate the parameters of our HMM model through
    data gathered from observations. We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood learning, with examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood learning in HMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expectation maximization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Baum-Welch algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into learning about **maximum likelihood estimation** (**MLE**)
    in HMMs, let''s try to understand the basic concepts of MLE in generic cases.
    As the name suggests, MLE tries to select the parameters of the model that maximizes
    the likelihood of observed data. The likelihood for any model with given parameters
    is defined as the probability of getting the observed data, and can be written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b83fac20-1c71-4634-a46f-dac1aaf79d54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *D={D[1], D[2], D[3], …, D[n]}* is the observed data, and *θ* is the
    set of parameters governing our model. In most cases, for simplicity, we assume
    that the datapoints are **independent and identically distributed** (**IID**). With
    that assumption, we can simplify the definition of our likelihood function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57cd5871-7d15-439f-8668-48a8c5630604.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have used the multiplication rule for independent random variables
    to decompose the joint distribution into product over individual datapoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to MLE, MLE tries to find the value of *θ* for which the value
    of *P(D|θ)* is at a maximum. So, basically we now have an optimization problem
    at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d3b5257-b933-46d8-b328-5a1f5f08da60.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next couple of subsections, we will try to apply MLE to some simple examples
    to understand it better.
  prefs: []
  type: TYPE_NORMAL
- en: MLE in a coin toss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's assume that we want to learn a model of a given coin using observations
    obtained from tossing it. Since a coin can only have two outcomes, heads or tails,
    it can be modeled using a single parameter. Let's say we define the parameter
    as *θ*, which is the probability of getting heads when the coin is tossed. The
    probability of getting tails will automatically be *1-θ* because getting either
    heads or tails are mutually exclusive events.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have our model ready, so let''s move on to computing the likelihood function
    of this model. Let''s assume that we are given some observations of coin tosses
    as *D={H,H,T,H,T,T}*. For the given data we can write our likelihood function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b32bf448-9d8a-4656-b29d-bf6641bb8652.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we would like to find the value of *θ* that would maximize *P(D|θ)*. For
    that, we take the derivative of our likelihood function, equate it to *0*, and
    then solve it for *θ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2c3b80e-bf69-4d12-873b-18a90aea69d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, our MLE estimator learned that the probability of getting heads on
    tossing the coin is *0.5*. Looking at our observations, we would expect the same
    probability as we have an equal number of heads and tails in our observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now try to write code to learn the parameter *θ* for our model. But
    as we know that finding the optimal value can run into numerical issues on a computer,
    is there a possible way to avoid that and directly be able to compute *θ[MLE]*?
    If we look closely at our likelihood equation, we realize that we can write a
    generic formula for the likelihood for this model. If we assume that our data
    has *n* heads and *m* tails, we can write the likelihood as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c241c3f-451d-4b92-9ec5-a9a6576f5673.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can actually find *θ[MLE]* in a closed-form using this likelihood function
    and avoid relying on any numerical method to compute the optimum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51def384-ba97-4278-8d00-35a663574cc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that we have been able to find a closed form solution for the MLE
    solution to *θ*. Now, coding this up would be to simply compute the preceding
    formula as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try out our function for different datapoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The outputs are as we expect, but one of the drawbacks of the MLE approach
    is that it is very sensitive to randomness in our data which, in some cases, might
    lead it to learn the wrong parameters. This is especially true in a case when
    the dataset is small in size. For example, let''s say that we toss a fair coin
    three times and we get heads in each toss. The MLE approach, in this case, would
    learn the value of *θ* to be 1, which is not correct since we had a fair coin.
    The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 5](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml), *Parameter Inference
    using Bayesian Approach*, we will try to solve this problem of MLE by starting
    with a prior distribution over the parameters, and it modifies its prior as it
    sees more and more data.
  prefs: []
  type: TYPE_NORMAL
- en: MLE for normal distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we had a model with a single parameter. In this section,
    we will apply the same concepts to a slightly more complex model. We will try
    to learn the parameters of a normal distribution (also known as the **Gaussian
    distribution**) from a given observed data. As we know, the normal distribution
    is parametrized by its mean and standard deviation and the distribution is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f1a73b2-ed97-47e4-8352-840dedaf7627.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *µ* is the mean and *σ* is the standard deviation of the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, for estimating parameters using MLE we would need some
    observed data, which, in this case, we are assuming to be coming from a normal
    distribution (or that it can be approximated using a normal distribution). Let's
    assume that we have some observed data: *X = {x[1], x[2],...,x[N]}*. We want to
    estimate the parameters *μ* (mean) and *σ²* (variance) for our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will follow the same steps as we took in the previous section. We will start
    by defining the likelihood function for the normal distribution. The likelihood
    is the probability of the data being observed, given the parameters. So, given
    the observed data, we can state the likelihood function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62dc53b8-f2d5-49fb-b859-d3f8595ab12e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One issue that we usually run into while trying to work with the product of
    small numbers is that the number can get too small for the computer to work with.
    To avoid running into this issue, we instead work with the log-likelihood instead
    of the simple likelihood. Since log is an increasing function, the maximum of
    the log-likelihood function would be for the same value of parameters as it would
    have been for the likelihood function. The log-likelihood can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90870fda-be88-45f8-ad5f-82fb52efdefa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then find the values of *μ[MLE]* and *σ[MLE]* that maximize the log-likelihood
    function by taking partial derivatives with respect to each of the variables,
    equating it to *0,* and solving the equation. To get the mean value, we need to
    take the partial derivative of the log-likelihood function with respect to *μ *while
    keeping *σ* as constant, and set it to *0**,* which gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e649282d-ea2b-49fb-9c3d-c810b4f0c286.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the MLE of standard deviation *σ²*can be computed by the partial
    derivative of the log-likelihood function with *σ²* while keeping *μ* constant,
    equating it to *0,* and then solving for *σ²*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bd38c12-3bb6-433f-83f2-4fd95a074cf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, we have again been able to derive a closed-form solution for
    the MLE and thus wouldn''t need to rely on numerical methods while coding it up.
    Let''s try to code this up and check if our MLE approach has learnt the correct
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have our learning function ready, so we can now generate some data from
    a known distribution and check if our function is able to learn the same parameters
    from the generated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we can see that the learned values are not very accurate.
    This is because of the problem with the MLE being too sensitive to the observed
    datapoints, as we discussed in the previous section. Let''s try to run this same
    example with more observed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, with more data, we can see that the learned values are much closer
    to our original values.
  prefs: []
  type: TYPE_NORMAL
- en: MLE for HMMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a basic understanding of MLE, we can now move on to applying these concepts
    to the case of HMMs. In the next few subsections, we will see two possible scenarios
    of learning in HMMs, namely, supervised learning and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of supervised learning, we use the data generated by sampling the
    process that we are trying to model. If we are trying to parameterize our HMM
    model using simple discrete distributions, we can simply apply the MLE to compute
    the transition and emission distributions by counting the number of transitions
    from any given state to another state. Similarly, we can compute the emission
    distribution by counting the output states from different hidden states. Therefore
    the transition and emission probabilities can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ebe8117-16f0-4638-a4c4-2235101afd0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *T(i,j)* is the transition probability from state *i* to state *j*. And
    *E(i,s) *is the emission probability of getting state *s* from state *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a very simple example to make this clearer. We want to model the
    weather and whether or not it would rain over a period of time. Also, we assume
    that the weather can take three possible states:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sunny (S)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cloudy (C)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Windy (W)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the *Rain* variable can have two possible states; *that it rained (R)*
    or *that it didn''t rain (NR)*. An HMM model would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And let''s say we have some observed data for this which looks something like *D={(S,NR),
    (S,NR), (C,NR), (C,R), (C,R), (W,NR), (S,NR), (W,R), (C,NR)}*. Here, the first
    element of each datapoint represents the observed weatherthat day and the second
    element represents whether it rained or not that day. Now, using the formulas
    that we derived earlier, we can easily compute the transition and emission probabilities.
    We will start with computing the transition probability from *S* to *S*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33c63d6c-3e65-4819-9696-899a449302b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can compute the transition probabilities for all the other combinations
    of states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db4f5682-7f09-4d02-a7b4-d7149798095c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, hence, we have our complete transition probability over all the possible
    states of the weather. We can represent it in tabular form to look nicer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Sunny(S)** | **Cloudy(C)** | **Windy(W)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Sunny(S)** | 0.33 | 0.33 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cloudy(C)** | 0 | 0.66 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| **Windy(W)** | 0.5 | 0.5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Transition probability for the weather model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, coming to computing the emission probability, we can again just follow
    the formula derived previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/710bbee7-885a-4fb0-8f3c-8fddb48e341c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can compute all the other values in the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b4d9790-4cd0-403b-861a-927dcdc42baf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And hence our emission probability can be written in tabular form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Sunny(S)** | **Cloudy(C)** | **Windy(W)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Rain (R)** | 0 | 0.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **No Rain (NR)** | 1 | 0.5 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Emission probability for the weather model'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we saw how we can compute the parameters of an HMM
    using MLE and some simple computations. But, because in this case we had assumed
    the transition and emission probabilities as simple discrete conditional distribution,
    the computation was much easier. With more complex cases, we will need to estimate
    more parameters than we did in the previous section in the case of the normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now try to code up the preceding algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate some data and try learning the parameters using the preceding
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how we can use supervised learning in a case
    where we have all the variables observed, including the hidden variables. But
    that is usually not the case with real-life problems. For such cases, we use unsupervised
    learning to estimate the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main learning algorithms used for this are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Viterbi learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Baum-Welch algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these in the next couple of subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Viterbi learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Viterbi learning algorithm (not to be confused with the Viterbi algorithm
    for state estimation) takes a set of training observations *O^r*, with *1≤r≤R,* and
    estimates the parameters of a single HMM by iteratively computing Viterbi alignments.
    When used to initialize a new HMM, the Viterbi segmentation is replaced by a uniform
    segmentation (that is, each training observation is divided into *N* equal segments)
    for the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than the first iteration on a new model, each training sequence *O* is
    segmented using a state alignment procedure which results from maximizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9db8ff8-70d9-4d24-b6cb-b6b75107d1c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'for *1<i<N* where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b6267fd-58d5-4263-b1a1-73900e2f01ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the initial conditions are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1892e2c6-f317-44fa-9d68-6d88fa772b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d8b3a899-783f-4721-bbad-d2e6f3b07fd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'for *1<j<N*. And, in the discrete case, the output probability ![](img/67e61a73-a5d6-43a1-886d-1273ee4fec13.png)is
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8b2df7f-45e2-48bd-9844-2685f99504f1.png)'
  prefs: []
  type: TYPE_IMG
- en: where *S* is the total number of streams, *v[s](O[st])* is the output given,
    the input *O[st,]* and *P[js][v]* is the probability of state *j* to give an output
    *v*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *A[ij]* represents the total number of transitions from state *i* to state
    *j* in performing the preceding maximizations, then the transition probabilities
    can be estimated from the relative frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d90b2a9-b716-4397-a074-b8f018b14a28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sequence of states that maximizes *∅[N](T)* implies an alignment of training
    data observations with states. Within each state, a further alignment of observations
    to mixture components is made. Usually, two mechanisms can be used for this, for
    each state and each output stream:'
  prefs: []
  type: TYPE_NORMAL
- en: Use clustering to allocate each observation *O[st]* to one of *M[s]* clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Associate each observation *O[st]* with the mixture component with the highest
    probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In either case, the net result is that every observation is associated with
    a single unique mixture component. This association can be represented by the
    indicator function ![](img/54848bcf-e5b3-42ce-a7d8-7afedce1e71c.png), which is
    *1* if ![](img/746625b8-e57b-4c27-b113-12cd1284871f.png) is associated with a
    mixture component *m* of stream *s* of state *j,* and is zero otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The means and variances are then estimated by computing simple means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/785f9fb8-df5e-4cdc-b8be-d0ea5d87eea2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f00c98ac-1363-4bda-8bee-58585d1e2345.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the mixture weights are based on the number of observations allocated to
    each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/862c8580-5b51-48fc-bdd2-75bcb49eda3c.png)'
  prefs: []
  type: TYPE_IMG
- en: The Baum-Welch algorithm (expectation maximization)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **expectation maximization** (**EM**) algorithm (known as **Baum-Welch**
    when applied to HMMs) is an iterative method used to find the maximum likelihood
    or **maximum a posteriori** (**MAP**) estimates of parameters in statistical models,
    where the model depends on unobserved latent variables. The EM iteration alternates
    between performing an **expectation** (**E**) step, which creates a function for
    the expectation of the log-likelihood evaluated using the current estimate for
    the parameters, and a **maximization **(**M**) step, which computes parameters
    maximizing the expected log-likelihood found on the *E* step. These parameter
    estimates are then used to determine the distribution of the latent variables
    in the next *E* step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The EM algorithm starts with initial value of parameters (*θ^(old)*). In the
    *E *step, we take these parameters and find the posterior distribution of latent
    variables *P(Z|X,θ^(old))*. We then use this posterior distribution to evaluate
    the expectation of the logarithm of the complete data likelihood function, as
    a function of the parameters *θ*, to give the function *Q(θ,θ^(old)),* defined
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b835312c-23b2-4ddd-b44c-87b037870f51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s introduce some terms that can help us in the future. *γ(Z[n])* to denote
    the marginal posterior distribution of a latent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e473d34c-bc1d-425f-b461-ebd6bf8a8246.png)'
  prefs: []
  type: TYPE_IMG
- en: '*ξ(z[n-1], z[n])* denoting the marginal posterior distribution of two successive
    latent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/936a734c-181c-4c4b-969b-c593f6048656.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, for each value of *n*, we can store *γ(Z[n])* as a vector of *K* non-negative
    numbers that sum to *1,* and, similarly we can use a *K×K* matrix of non-negative
    numbers that sum to *1* to save *ξ(z[n-1], z[n])*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have discussed in previous chapters, the latent variable *z[n]* can be
    represented as *K* dimensional binary variable where *z[nk] = 1* when *z[n]* is
    in state *k*. We can also use it to denote the conditional probability of *z[nk] =
    1,* and similarly *ξ(z[n-1], j, z[nk])* to denote the conditional probability
    of *zn-1*, *j = 1,* and *z[nk] = 1*. As the expectation of a binary random variable
    is just the probability of its value being *1*, we can state the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29aa84f0-6709-4d8e-9eed-2fdbb9d243aa.png)![](img/cace6a8f-3baa-4e7a-b215-cd7d51a9db15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we discussed in the previous chapter, the joint probability distribution
    of an HMM can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2a4b0e-4746-4014-994f-7107eadce813.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus we can write the data likelihood function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9185967c-2c6d-4a48-a76a-a43632f416ae.png)'
  prefs: []
  type: TYPE_IMG
- en: In the *E* step we try to evaluate the quantities* γ(z[n])* and *ξ(z[n-1], z[n])* efficiently. For
    efficient computation of these two terms we can use either a forward backward
    algorithm or the Viterbi algorithm as discussed in the previous chapter. And in
    the *M* step, we try to maximize the value of *Q(θ, θ^(old))* with respect to
    the parameters *θ={A, π, Φ}* in which we treat *γ(z[n])* and *ξ(z[n-1], z[n])* as
    constants.
  prefs: []
  type: TYPE_NORMAL
- en: 'In doing so, we get the MLE values of the parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77ebd97f-a907-45e7-a0f9-03dedef7cabc.png)![](img/84bef755-e69e-4bb8-98a4-7397e9e18821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we assume the emission distribution to be a normal distribution such that ![](img/dcabab1e-ffd3-473c-a5e6-8d5c9e8f1fd3.png),
    then the maximization of *Q(θ, θ^(old))* with respect to *Φ[k]* would result in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2920ee83-9f17-4334-a247-16c18f4f1a89.png)![](img/b922047a-e242-4346-b85e-478a777dfef6.png)'
  prefs: []
  type: TYPE_IMG
- en: The EM algorithm must be initialized by choosing starting values for *π* and
    *A*, which should, of course, be non-negative and should add up to *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithms for parameter estimation look quite complex but `hmmlearn`, a
    Python package for working with HMMs, has great implementations for it. `hmmlearn`
    is also hosted on PyPI so it can be installed directly using `pip:pip install
    hmmlearn`. For the code example, we will take an example of stock price prediction
    by learning a Gaussian HMM on stock prices. This example has been taken from the
    examples page of `hmmlearn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example, we also need the `matplotlib` and `datetime` packages which
    can also be installed using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Coming to the code, we should start by importing all the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will fetch our stock price data from Yahoo! Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a Gaussian HMM model and learn the parameters for our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now print out our learned parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot our hidden states over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/518698e7-beb2-4ed5-a1f0-2ce16f0b085c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Plot of hidden states over time'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced algorithms for doing parameter estimation of
    a given HMM model. We started by looking into the basics of MLE and then applied
    the concepts to HMMs. For HMM training, we looked into two different scenarios:
    supervised training, when we have the observations for the hidden states, and
    unsupervised training, when we only have the output observations.'
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about the problems with estimation using MLE. In the next chapter,
    we will introduce algorithms for doing parameter estimation using the Bayesian
    approach, which tries to solve these issues.
  prefs: []
  type: TYPE_NORMAL
