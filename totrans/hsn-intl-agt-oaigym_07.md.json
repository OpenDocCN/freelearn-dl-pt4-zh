["```py\nimport gym\n\nclass CustomEnv(gym.Env):\n    \"\"\"\n    A template to implement custom OpenAI Gym environments\n\n    \"\"\"\n\n    metadata = {'render.modes': ['human']}\n    def __init__(self):\n        self.__version__ = \"0.0.1\"\n        # Modify the observation space, low, high and shape values according to your custom environment's needs\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(3,))\n        # Modify the action space, and dimension according to your custom environment's needs\n        self.action_space = gym.spaces.Box(4)\n\n    def step(self, action):\n        \"\"\"\n        Runs one time-step of the environment's dynamics. The reset() method is called at the end of every episode\n        :param action: The action to be executed in the environment\n        :return: (observation, reward, done, info)\n            observation (object):\n                Observation from the environment at the current time-step\n            reward (float):\n                Reward from the environment due to the previous action performed\n            done (bool):\n                a boolean, indicating whether the episode has ended\n            info (dict):\n                a dictionary containing additional information about the previous action\n        \"\"\"\n```", "```py\n\n        # Implement your step method here\n        #   - Calculate reward based on the action\n        #   - Calculate next observation\n        #   - Set done to True if end of episode else set done to False\n        #   - Optionally, set values to the info dict\n        # return (observation, reward, done, info)\n\n    def reset(self):\n        \"\"\"\n        Reset the environment state and returns an initial observation\n\n        Returns\n        -------\n        observation (object): The initial observation for the new episode after reset\n        :return:\n        \"\"\"\n\n        # Implement your reset method here\n        # return observation\n\n    def render(self, mode='human', close=False):\n        \"\"\"\n\n        :param mode:\n        :return:\n        \"\"\"\n        return\n```", "```py\nCustomEnv class we implemented is as follows:\n```", "```py\nfrom gym.envs.registration import register\n\nregister(\n    id='CustomEnv-v0',\n    entry_point='custom_environments.envs:CustomEnv',\n)\n```", "```py\nmkdir ~/software && cd ~/software\n```", "```py\nexport CARLA_SERVER=~/software/CARLA_VER_NUM/CarlaUE4.sh\n```", "```py\n# Default environment configuration\nENV_CONFIG = {\n    \"enable_planner\": True,\n    \"use_depth_camera\": False,\n    \"discrete_actions\": True,\n    \"server_map\": \"/Game/Maps/\" + city,\n    \"scenarios\": [scenario_config[\"Lane_Keep_Town2\"]],\n    \"framestack\": 2, # note: only [1, 2] currently supported\n    \"early_terminate_on_collision\": True,\n    \"verbose\": False,\n    \"render_x_res\": 800,\n    \"render_y_res\": 600,\n    \"x_res\": 80,\n    \"y_res\": 80\n}\n```", "```py\ndef __init__(self, config=ENV_CONFIG):\n        self.config = config\n        self.city = self.config[\"server_map\"].split(\"/\")[-1]\n        if self.config[\"enable_planner\"]:\n            self.planner = Planner(self.city)\n\n        if config[\"discrete_actions\"]:\n            self.action_space = Discrete(len(DISCRETE_ACTIONS))\n        else:\n            self.action_space = Box(-1.0, 1.0, shape=(2,))\n        if config[\"use_depth_camera\"]:\n            image_space = Box(\n                -1.0, 1.0, shape=(\n```", "```py\n\n                    config[\"y_res\"], config[\"x_res\"],\n                    1 * config[\"framestack\"]))\n        else:\n            image_space = Box(\n                0.0, 255.0, shape=(\n                    config[\"y_res\"], config[\"x_res\"],\n                    3 * config[\"framestack\"]))\n        self.observation_space = Tuple(\n            [image_space,\n             Discrete(len(COMMANDS_ENUM)),  # next_command\n             Box(-128.0, 128.0, shape=(2,))])  # forward_speed, dist to goal\n\n        self._spec = lambda: None\n        self._spec.id = \"Carla-v0\"\n\n        self.server_port = None\n        self.server_process = None\n        self.client = None\n        self.num_steps = 0\n        self.total_reward = 0\n        self.prev_measurement = None\n        self.prev_image = None\n        self.episode_id = None\n        self.measurements_file = None\n        self.weather = None\n        self.scenario = None\n        self.start_pos = None\n        self.end_pos = None\n        self.start_coord = None\n        self.end_coord = None\n        self.last_obs = None\n```", "```py\nsettings = CarlaSettings()  # Initialize a CarlaSettings object with default values\nsettings.set(\n            SynchronousMode=True,\n            SendNonPlayerAgentsInfo=True,  # To receive info about all other objs\n            NumberOfVehicles=self.scenario[\"num_vehicles\"],\n            NumberOfPedestrians=self.scenario[\"num_pedestrians\"],\n            WeatherId=self.weather)\nSynchronousMode to True to enable the synchronous mode, in which the CARLA server halts the execution of each frame until a control message is received. Control messages are based on the actions the agent takes and are sent through the CARLA client.\n```", "```py\n# Create a RGB Camera Object\ncamera1 = Camera('CameraRGB')\n# Set the RGB camera image resolution in pixels\ncamera1.set_image_size(640, 480)\n# Set the camera/sensor position relative to the car in meters\ncamera1.set_positions(0.25, 0, 1.30)\n# Add the sensor to the Carla Settings object\nsettings.add_sensor(camera1)\n```", "```py\n# Create a depth camera object that can provide us the ground-truth depth of the driving scene\ncamera2 = Camera(\"CameraDepth\",PostProcessing=\"Depth\")\n# Set the depth camera image resolution in pixels\ncamera2.set_image_size(640, 480)\n# Set the camera/sensor position relative to the car in meters\ncamera2.set_position(0.30, 0, 1.30)\n# Add the sensor to the Carla settings object\nsettings.add_sensor(camera)Setting up the start and end positions in the scene for the Carla Simulation\n```", "```py\n# Create a LIDAR object. The default LIDAR supports 32 beams\nlidar = Lidar('Lidar32')\n# Set the LIDAR sensor's specifications\nlidar.set(\n    Channels=32,  # Number of beams/channels\n    Range=50,     # Range of the sensor in meters\n    PointsPerSecond=1000000,  # Sample rate\n    RotationFrequency=10,  # Frequency of rotation\n    UpperFovLimit=10,  # Vertical field of view upper limit angle\n    LowerFovLimit=-30) # Vertical field of view lower limit angle\n# Set the LIDAR position & rotation relative to the car in meters\nlidar.set_position(0, 0, 2.5)\nlidar.set_rotation(0, 0, 0)\n# Add the sensor to the Carla settings object\nsettings.add_sensor(lidar)\n```", "```py\nscene = self.client.load_settings(settings)\navailable_start_spots = scene.player_start_spots\n```", "```py\nstart_spot = random.randint(0, max(0, available_start_spots))\n```", "```py\nself.client.start_episode(start_spot)\n```", "```py\ndef _reset(self):\n        self.num_steps = 0\n        self.total_reward = 0\n        self.prev_measurement = None\n        self.prev_image = None\n        self.episode_id = datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S_%f\")\n        self.measurements_file = None\n\n        # Create a CarlaSettings object. This object is a wrapper around\n        # the CarlaSettings.ini file. Here we set the configuration we\n        # want for the new episode.\n        settings = CarlaSettings()\n        # If config[\"scenarios\"] is a single scenario, then use it if it's an array of scenarios, randomly choose one and init\n        self.config = update_scenarios_parameter(self.config)\n\n        if isinstance(self.config[\"scenarios\"],dict):\n            self.scenario = self.config[\"scenarios\"]\n        else: #ininstance array of dict\n            self.scenario = random.choice(self.config[\"scenarios\"])\n        assert self.scenario[\"city\"] == self.city, (self.scenario, self.city)\n        self.weather = random.choice(self.scenario[\"weather_distribution\"])\n        settings.set(\n            SynchronousMode=True,\n            SendNonPlayerAgentsInfo=True,\n            NumberOfVehicles=self.scenario[\"num_vehicles\"],\n            NumberOfPedestrians=self.scenario[\"num_pedestrians\"],\n            WeatherId=self.weather)\n        settings.randomize_seeds()\n\n        if self.config[\"use_depth_camera\"]:\n            camera1 = Camera(\"CameraDepth\", PostProcessing=\"Depth\")\n            camera1.set_image_size(\n                self.config[\"render_x_res\"], self.config[\"render_y_res\"])\n            camera1.set_position(30, 0, 130)\n            settings.add_sensor(camera1)\n\n        camera2 = Camera(\"CameraRGB\")\n        camera2.set_image_size(\n            self.config[\"render_x_res\"], self.config[\"render_y_res\"])\n        camera2.set_position(30, 0, 130)\n        settings.add_sensor(camera2)\n\n        # Setup start and end positions\n        scene = self.client.load_settings(settings)\n        positions = scene.player_start_spots\n        self.start_pos = positions[self.scenario[\"start_pos_id\"]]\n        self.end_pos = positions[self.scenario[\"end_pos_id\"]]\n        self.start_coord = [\n            self.start_pos.location.x // 100, self.start_pos.location.y // 100]\n        self.end_coord = [\n            self.end_pos.location.x // 100, self.end_pos.location.y // 100]\n        print(\n            \"Start pos {} ({}), end {} ({})\".format(\n                self.scenario[\"start_pos_id\"], self.start_coord,\n                self.scenario[\"end_pos_id\"], self.end_coord))\n\n        # Notify the server that we want to start the episode at the\n        # player_start index. This function blocks until the server is ready\n        # to start the episode.\n        print(\"Starting new episode...\")\n        self.client.start_episode(self.scenario[\"start_pos_id\"])\n\n        image, py_measurements = self._read_observation()\n        self.prev_measurement = py_measurements\n        return self.encode_obs(self.preprocess_image(image), py_measurements)\n```", "```py\nmeasurements, sensor_data = client.read_data()\n```", "```py\nrgb_image = sensor_data['CameraRGB'].data\n```", "```py\npixel_value_at_x_y = rgb_image[X, Y]\n```", "```py\ndepth_image = sensor_data['CameraDepth'].data\n```", "```py\naction_space = gym.space.Box(-1.0, 1.0, shape=2(,))\n```", "```py\naction_space = gym.spaces.Discrete(NUM_DISCRETE_ACTIONS)\n```", "```py\nDISCRETE_ACTIONS = {\n    0: [0.0, 0.0],    # Coast\n    1: [0.0, -0.5],   # Turn Left \n    2: [0.0, 0.5],    # Turn Right\n    3: [1.0, 0.0],    # Forward\n    4: [-0.5, 0.0],   # Brake\n    5: [1.0, -0.5],   # Bear Left & accelerate\n    6: [1.0, 0.5],    # Bear Right & accelerate\n    7: [-0.5, -0.5],  # Bear Left & decelerate\n    8: [-0.5, 0.5],   # Bear Right & decelerate\n}\n```", "```py\nthrottle = float(np.clip(action[0], 0, 1)\nbrake = float(np.abs(np.cllip(action[0], -1, 0)\nsteer = float(p.clip(action[1], -1, 1)\nhand_brake = False\nreverse = False\n```", "```py\ndef calculate_reward(self, current_measurement):\n        \"\"\"\n        Calculate the reward based on the effect of the action taken using the previous and the current measurements\n        :param current_measurement: The measurement obtained from the Carla engine after executing the current action\n        :return: The scalar reward\n        \"\"\"\n        reward = 0.0\n\n        cur_dist = current_measurement[\"distance_to_goal\"]\n\n        prev_dist = self.prev_measurement[\"distance_to_goal\"]\n\n        if env.config[\"verbose\"]:\n            print(\"Cur dist {}, prev dist {}\".format(cur_dist, prev_dist))\n\n        # Distance travelled toward the goal in m\n        reward += np.clip(prev_dist - cur_dist, -10.0, 10.0)\n\n        # Change in speed (km/hr)\n        reward += 0.05 * (current_measurement[\"forward_speed\"] - self.prev_measurement[\"forward_speed\"])\n\n        # New collision damage\n        reward -= .00002 * (\n            current_measurement[\"collision_vehicles\"] + current_measurement[\"collision_pedestrians\"] +\n            current_measurement[\"collision_other\"] - self.prev_measurement[\"collision_vehicles\"] -\n            self.prev_measurement[\"collision_pedestrians\"] - self.prev_measurement[\"collision_other\"])\n\n        # New sidewalk intersection\n        reward -= 2 * (\n            current_measurement[\"intersection_offroad\"] - self.prev_measurement[\"intersection_offroad\"])\n\n        # New opposite lane intersection\n        reward -= 2 * (\n            current_measurement[\"intersection_otherlane\"] - self.prev_measurement[\"intersection_otherlane\"])\n\n        return reward\n```", "```py\n# 1\\. Check if a collision has occured\nm = measurements_from_carla_server\ncollided = m[\"collision_vehicles\"] > 0 or m[\"collision_pedestrians\"] > 0 or m[\"collision_other\"] > 0\n\n# 2\\. Check if the ego/host car has reached the destination/goal\nplanner = carla_planner\ngoal_reached = planner[\"next_command\"] == \"REACHED_GOAL\"\n\n# 3\\. Check if the time-limit has been exceeded\ntime_limit = scenario_max_steps_config\ntime_limit_exceeded = num_steps > time_limit\n\n# Set \"done\" to True if either of the above 3 criteria becomes true\ndone = collided or goal_reached or time_limit_exceeded\n```", "```py\nCarlaEnv and runs five episodes with a fixed action of going forward. The ENV_CONFIG action, which we created during initialization, can be changed to use discrete or continuous action spaces, as follows:\n```", "```py\n# Part of https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/ch7/carla-gym/carla_gym/envs/carla_env.py\nif __name__ == \"__main__\":\n    for _ in range(5):\n        env = CarlaEnv()\n        obs = env.reset()\n        done = False\n        t = 0\n        total_reward = 0.0\n        while not done:\n            t += 1\n            if ENV_CONFIG[\"discrete_actions\"]:\n                obs, reward, done, info = env.step(3) # Go Forward\n            else:\n                obs, reward, done, info = env.step([1.0, 0.0]) # Full throttle, zero steering angle\n            total_reward += reward\n            print(\"step#:\", t, \"reward:\", round(reward, 4), \"total_reward:\", round(total_reward, 4), \"done:\", done)\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch7$ python carla-gym/carla_gym/envs/carla_env.py\n```"]