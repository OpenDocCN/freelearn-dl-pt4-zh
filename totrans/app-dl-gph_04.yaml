- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Deep Learning Models for Graphs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的深度学习模型
- en: In recent years, the field of machine learning has witnessed a paradigm shift
    with the emergence of **graph neural networks** ( **GNNs** ) as powerful tools
    for addressing prediction tasks on graph-structured data. Here, we'll delve into
    the transformative potential of GNNs, highlighting their role as optimizable transformations
    capable of handling diverse graph attributes, such as nodes, edges, and global
    context while preserving crucial graph symmetries, particularly permutation invariances.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着**图神经网络**（**GNNs**）的出现，机器学习领域迎来了范式转变，GNN成为了处理图结构数据预测任务的强大工具。在这里，我们将深入探讨GNN的变革潜力，强调其作为可优化变换的作用，能够处理多样的图属性，如节点、边缘和全局上下文，同时保持重要的图对称性，尤其是排列不变性。
- en: The foundation of GNNs lies in the **message-passing neural network** ( **MPNN**
    ) framework. Through this framework, GNNs leverage a sophisticated mechanism for
    information exchange and aggregation across graph structures, enabling the model
    to capture intricate relationships and dependencies within the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的基础在于**信息传递神经网络**（**MPNN**）框架。通过该框架，GNN利用一种复杂的机制，在图结构中进行信息交换和聚合，使得模型能够捕捉数据中的复杂关系和依赖性。
- en: One distinctive feature of GNNs is their adherence to a *graph-in, graph-out*
    architecture. This means that the model accepts a graph as input, equipped with
    information embedded in its nodes, edges, and global context. This inherent structure
    aligns with many real-world problems where data exhibits complex relationships
    and dependencies best represented as graphs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的一个显著特点是遵循*图输入，图输出*架构。这意味着模型接受一个图作为输入，该图包含嵌入在节点、边缘和全局上下文中的信息。这种固有的结构与许多现实世界的问题相吻合，数据通常展现出复杂的关系和依赖性，最适合通过图来表示。
- en: GNNs excel in their ability to perform a progressive embedding transformation
    on the input graph without altering its connectivity. This progressive transformation
    ensures that the model refines its understanding of the underlying patterns and
    structures within the data, contributing to enhanced predictive capabilities.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: GNN的优势在于其能够在不改变图的连接性的情况下，对输入图执行逐步的嵌入变换。这一逐步变换确保了模型不断优化对数据中潜在模式和结构的理解，从而提升预测能力。
- en: 'We''ll cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Message passing in graphs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中的信息传递
- en: Decoding GNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码GNN
- en: '**Graph convolutional** **networks** ( **GCNs** )'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图卷积网络**（**GCNs**）'
- en: '**Graph Sample and** **Aggregation** ( **GraphSAGE** )'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图采样与聚合**（**GraphSAGE**）'
- en: '**Graph attention** **networks** ( **GATs** )'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图注意力网络**（**GATs**）'
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires a basic understanding of graphs and representation learning
    as covered in previous chapters. The code present in the chapter and on GitHub
    can be used directly on Google Colab with an additional installation of the **PyTorch
    Geometric** ( **PyG** ) package. The code examples for the book are available
    in its GitHub repository: [https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs](https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs)
    .'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求读者具备图和表示学习的基本理解，这些内容已在前几章中讲解。章节中的代码以及GitHub上的代码可以直接在Google Colab上使用，只需额外安装**PyTorch
    Geometric**（**PyG**）包。书中的代码示例可以在其GitHub仓库中找到：[https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs](https://github.com/PacktPublishing/Applied-Deep-Learning-on-Graphs)。
- en: Message passing in graphs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图中的信息传递
- en: Unlike traditional neural networks, GNNs need to account for the inherent structure
    of the graph, allowing nodes to exchange information and update their representations
    based on their local neighborhoods. This core mechanism is achieved through **message
    passing** , a process of iteratively passing messages between nodes and aggregating
    information from their neighbors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的神经网络不同，GNN（图神经网络）需要考虑图的固有结构，使得节点能够交换信息并根据其局部邻域更新自身表示。这一核心机制是通过**信息传递**实现的，这是一个节点间迭代传递信息并聚合邻居信息的过程。
- en: GNNs operate on graph-structured data and use a message-passing mechanism to
    update node representations based on information from neighboring nodes. Let’s
    delve into the mathematical explanation of message passing in GNNs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GNN在图结构数据上进行操作，并使用信息传递机制基于邻居节点的信息更新节点表示。让我们深入探讨GNN中信息传递的数学解释。
- en: Consider an undirected graph ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/116.png)
    , where ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) is the set of
    nodes and ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:math>](img/118.png)
    is the set of edges. Each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    in ![<math ><mrow><mi>V</mi></mrow></math>](img/120.png) has an associated feature
    vector ![<mml:math  ><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/121.png)
    . The goal of a GNN is to learn a representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    for each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png) that captures
    information from its neighborhood.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic message-passing operation in a GNN can be broken down into a series
    of steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregation** **of messages** :'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png) , gather
    information   from its neighbors.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Let ![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    represent the set of neighbors of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    .
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The aggregated message ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/127.png)
    for node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/128.png) is computed
    by aggregating information from its neighbors:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/129.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: The aggregation function can vary (e.g., sum, mean, or attention-weighted sum).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**UPDATE function** :'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the node representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    based on the aggregated message ![<mml:math  ><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)
    and the current node   representation ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/122.png)
    .
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **UPDATE** function ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></mrow></math>](img/133.png)
    ( ![<mml:math  ><mml:mo>∙</mml:mo></mml:math>](img/134.png) ) is a neural network
    layer that takes the aggregated message and the current node representation as
    input and produces the updated representation:'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/135.png)
    = ![<math ><mrow><mrow><mi>u</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mfenced
    open="(" close=")"><mrow><msub><mi>h</mi><mi>v</mi></msub><mo>,</mo><msub><mi>m</mi><mi>v</mi></msub></mrow></mfenced></mrow></mrow></math>](img/136.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: The **UPDATE** function typically involves a neural network layer with learnable
    parameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps are iteratively applied for a fixed number of times or until convergence
    to refine the node representations. The overall process can be expressed in a
    few equations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/137.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/138.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: These equations capture the essence of the message-passing mechanism in a GNN.
    The specific choices for the aggregation function, **UPDATE** function, and the
    number of iterations depend on the architecture of the GNN (e.g., GraphSAGE, GCN,
    **gated graph neural networks** ( **GGNNs** ), etc.).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in a simple **GraphSAGE** formulation, the aggregation function
    might be a mean operation, and the **UPDATE** function might be a simple neural
    network layer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi> </mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi> </mml:mi><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi> </mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/139.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>''</mml:mi></mml:mrow></mml:msubsup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>σ</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:mi> </mml:mi><mml:mo>⋅</mml:mo><mml:mi> </mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/140.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math  ><mml:mi>σ</mml:mi></mml:math>](img/141.png) is an activation   function,
    ![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png) is a learnable weight
    matrix, and ![<mml:math  ><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/143.png)
    is the concatenation operation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Message passing in graphs](img/B22118_04_1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Message passing in graphs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Through these iterative steps, message passing enables nodes to learn representations
    that capture not only their own intrinsic features but also the information from
    their connected neighbors and the overall structure of the graph. This allows
    GNNs to effectively model complex relationships and dependencies within graph-structured
    data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s try to understand how to formally define a GNN.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Decoding GNNs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **GNN** is a neural network architecture designed to operate on graph-structured
    data. It learns a function that maps a graph and its associated features to a
    set of node-level, edge-level, or graph-level outputs. The following is a formal
    mathematical definition of a GNN.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Given a graph ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/144.png)
    , where ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) is the set of
    nodes and ![<mml:math  ><mml:mi>E</mml:mi></mml:math>](img/146.png) is the set
    of edges, let ![<mml:math  ><mml:mi>X</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math>](img/147.png)
    be the node feature matrix, where each row ![<mml:math  ><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math>](img/148.png)
    represents the features of node ![<math ><mrow><mrow><mi>v</mi><mspace width="0.25em"
    /><mo>∈</mo><mspace width="0.25em" /><mi>V</mi></mrow></mrow></math>](img/149.png)
    .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图 ![<mml:math  ><mml:mi>G</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/144.png)
    ，其中 ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png) 是节点集，![
- en: A GNN is a function ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>G</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/150.png)
    parameterized by learnable weights ![<mml:math  ><mml:mi>θ</mml:mi></mml:math>](img/151.png)
    , which maps the graph ![<mml:math  ><mml:mi>G</mml:mi></mml:math>](img/152.png)
    and its node features ![<mml:math  ><mml:mi>X</mml:mi></mml:math>](img/153.png)
    to a new set of node representations ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="|"
    close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math>](img/154.png)
    , where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msup></mml:math>](img/155.png)
    is the dimensionality of the output node representations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The function ![<mml:math  ><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub></mml:math>](img/156.png)
    is computed through a series of message passing and aggregation steps, typically
    organized into ![<mml:math  ><mml:mi>L</mml:mi></mml:math>](img/115.png) layers.
    At each layer ![<mml:math  ><mml:mi>l</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mfenced
    open="{" close="}" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/158.png)
    , the node representations are updated as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>u</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/159.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>u</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/159.png)'
- en: 'Let’s break this down:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: '![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mi>ϵ</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/160.png)
    is the representation of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/95.png)
    at layer ![<mml:math  ><mml:mi> </mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo></mml:math>](img/162.png)
    with ![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/163.png)
    . Here, ![<mml:math  ><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>](img/164.png)
    represents a real-valued vector space of dimension ![<mml:math  ><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/165.png)
    .'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **UPDATE function** ![<math ><mrow><mrow><mi>U</mi><mi>P</mi><mi>D</mi><mi>A</mi><mi>T</mi><msup><mi>E</mi><mi>l</mi></msup><mo>:</mo><mspace
    width="0.25em" /><msup><mi mathvariant="double-struck">R</mi><mfenced open="("
    close=")"><msub><mi>d</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mfenced></msup><mspace
    width="0.25em" /><mi mathvariant="normal">Χ</mi><mspace width="0.25em" /><msup><mi
    mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup><mo>→</mo><mspace width="0.25em" /><mspace width="0.25em"
    /><msup><mi mathvariant="double-struck">R</mi><mrow><mfenced open="(" close=")"><msub><mi>d</mi><mi>l</mi></msub></mfenced><mspace
    width="0.25em" /></mrow></msup></mrow></mrow></math>](img/166.png) is a learnable
    function that updates the node   representation based on its previous representation
    and the aggregated messages from its neighbors.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **aggregation (AGG) function** ![<mml:math  ><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/167.png)
    is a permutation-invariant aggregation function that   combines the representations
    of the neighboring nodes. Common choices include sum, mean, and max.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/125.png)
    denotes the set of neighbors of node ![<mml:math  ><mml:mi>v</mml:mi></mml:math>](img/89.png)
    in the graph ![<math ><mrow><mi>G</mi></mrow></math>](img/170.png) .'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After ![<mml:math  ><mml:mi> </mml:mi><mml:mi>L</mml:mi></mml:math>](img/171.png)
    layers of message passing   and aggregation, the final node representations are
    given by ![<mml:math  ><mml:mi>H</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/172.png)
    for all ![<mml:math  ><mml:mi>v</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mi>V</mml:mi></mml:math>](img/173.png)
    .
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The **UPDATE** function is typically implemented as neural networks, such as
    **multi-layer perceptrons** ( **MLPs** ) or attention mechanisms, with learnable
    parameters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'For graph-level tasks, a **READOUT function** is applied to the final node
    representations to obtain a graph-level representation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mi>T</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>v</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="normal"> </mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/174.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Here, **READOUT** is a permutation-invariant function that aggregates the node
    representations into a single vector, such as sum, mean, or a more complex pooling
    operation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The graph-level representation ![<mml:math  ><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math>](img/175.png)
    can then be used for downstream tasks, such as graph classification or regression.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: This is a general formulation of GNNs, and there are many specific architectures
    that fall under this framework, such as GCNs, GraphSAGE, GATs, and MPNNs, each
    with their own variations of the **UPDATE** , **AGG** , and **READOUT** functions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how graph learning borrows the concept of convolution networks
    and leverages it to extract learnings from a graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: GCNs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GCNs** are a specific type of GNN that extend the concept of convolution
    to graph-structured data. GCNs learn node representations by aggregating information
    from neighboring nodes, allowing for the capture of both node features and graph
    structure.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'In a GCN, the graph convolution operation at layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)
    is defined as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi
    mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/177.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Let’s break this down:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    open="|" close="|" separators="|"><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math>](img/178.png)
    is the matrix of node representations at layer ![<math ><mrow><mi>l</mi></mrow></math>](img/179.png)
    , with ![<mml:math  ><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mi>X</mml:mi></mml:math>](img/180.png)
    (input node features).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:math>](img/181.png)
    is the adjacency matrix ![<mml:math  ><mml:mi>A</mml:mi></mml:math>](img/43.png)
    with added self-loops, where ![<mml:math  ><mml:mi>I</mml:mi></mml:math>](img/183.png)
    is the identity matrix.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/184.png)
    is the diagonal degree matrix of ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/185.png)
    , with ![<mml:math  ><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/186.png)
    .'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mi>x</mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mi>​</mml:mi><mml:mi>​</mml:mi></mml:math>](img/187.png)
    is a learnable weight matrix for layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png)
    .'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:mi>σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math>](img/189.png)
    is a non-linear activation   function, such as the ** rectified linear unit **
    ( ** ReLU ** ) function or sigmoid function.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term ![<mml:math  ><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mi> </mml:mi><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math>](img/190.png)
    is the   symmetrically normalized adjacency matrix, which ensures that the scale
    of the node representations remains consistent across layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a citation network, where each node stands for a scientific paper and
    each edge represents a citation connecting two papers. Each paper has a feature
    vector representing its content (e.g., bag-of-words). A GCN can be used to classify
    the papers into different categories (e.g., computer science, physics, biology)
    by learning node representations that capture both the content and the citation
    structure of the network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Building on our mathematical understanding of the GCN, let’s look at a piece
    of sample code leveraging PyG:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For this example, we import the necessary libraries and load the **Cora dataset**
    using the **Planetoid** class from PyG. The Cora dataset is a citation network
    dataset, where *nodes* represent scientific papers and *edges* represent citations
    between papers. The dataset contains 2,708 nodes, 10,556 edges, and 7 classes
    representing different research areas:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we access the graph data using **dataset[0]** . We then print some statistics
    about the graph, including the number of nodes, edges, features, and classes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we understand what the data looks like, let’s put down the building
    blocks of the model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we define the GCN model. The model consists of two GCN layers ( **GCNConv**
    ) with a hidden layer in between. The **__init__** method initializes the layers
    with the specified input, hidden, and output dimensions. The **forward** method
    defines the forward pass of the model, where **x** and **edge_index** are passed
    through the GCN layers. ReLU activation and dropout are applied after the first
    layer, and **log-softmax** is applied to the output of the second layer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, we set the model parameters based on the dataset. The input dimension
    ( **in_channels** ) is set to the number of node features in the dataset, the
    hidden dimension ( **hidden_channels** ) is set to **16** , and the output dimension
    ( **out_channels** ) is set to the number of classes in the dataset. We then create
    an instance of the GCN model with these parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this part, we define the optimizer ( **Adam** ) and the loss function ( **negative
    log-likelihood loss (NLLLoss)** ) for training the model. We set the learning
    rate to **0.01** and the weight decay to **5e-4** .
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'We then train the model for 200 epochs. In each epoch, we do the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Zero the gradients of the optimizer.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a forward pass of the model on the node features and edge index.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss using the model’s output and the ground truth labels for the
    training nodes (specified by **data.train_mask** ).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a backward pass to compute the gradients.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the model parameters using the optimizer:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we evaluate the trained model on the test set. We set the model to
    evaluation mode using **model.eval()** . We perform a forward pass on the entire
    graph and obtain the predicted class labels using **max(dim=1)** . We then compute
    the accuracy by comparing the predicted labels with the ground truth labels for
    the test nodes (specified by **data.test_mask** ).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: This code provides a basic implementation of GCN using PyG on the Cora dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have PyG installed before running this code. You can install it
    using **pip** **install torch-geometric** .
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this code creates an instance of the GCN model, trains it on the Cora
    dataset using the specified hyperparameters (hidden units, learning rate, epochs),
    and evaluates the trained model on the test set to measure its performance in
    terms of accuracy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Using GCNs for different graph tasks
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GCNs can be utilized to learn and perform tasks at different levels in a graph.
    The following can be performed with GCNs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '**Node-level tasks** : GCNs can be used for **node classification** , where
    the goal is to predict the label of each node in the graph. This is demonstrated
    in the previous example, where the GCN is used to classify nodes in the Cora citation
    network.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge-level tasks** : GCNs can be adapted for **edge prediction** or **link**
    **prediction** tasks, where the goal is to predict the existence or attributes
    of edges in the graph. To do this, the node representations learned by the GCN
    can be used to compute edge scores or probabilities.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-level tasks** : GCNs can be used for **graph classification** or **regression**
    tasks, where the goal is to predict a label or a continuous value for an entire
    graph. To achieve this, a pooling operation (e.g., global mean pooling or global
    max pooling) is applied to the node representations learned by the GCN to obtain
    a graph-level representation, which is then fed into a classifier or regressor.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCNs are a powerful and widely used type of GNN that can effectively learn node
    representations by incorporating both node features and graph structure. They
    have shown strong performance on various graph-based tasks and can be adapted
    for node-level, edge-level, and graph-level problems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Over time, many optimizations over vanilla GCNs have been proposed and utilized
    in the industry. One such optimization, especially for scaling the graph learning
    process, is GraphSAGE.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: GraphSAGE
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GraphSAGE** introduces a scalable and adaptive approach to graph representation
    learning, addressing some limitations of GCN and enhancing scalability. At its
    core, GraphSAGE employs a neighborhood sampling and aggregation strategy, diverging
    from the fixed-weight aggregation mechanism of GCN.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In GraphSAGE, the process of learning node representations involves iteratively
    sampling and aggregating information from local neighborhoods. Let ![<mml:math  ><mml:mi>G</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/191.png)
    be a graph with nodes ![<mml:math  ><mml:mi>V</mml:mi></mml:math>](img/1.png)
    and edges ![<mml:math  ><mml:mi>E</mml:mi></mml:math>](img/2.png) , and ![<mml:math  ><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/194.png)
    denote the embedding of node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    at layer ![<mml:math  ><mml:mi>l</mml:mi></mml:math>](img/176.png) . The update
    rule for GraphSAGE can be expressed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math   display="block"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">​</mml:mi><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/197.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math  ><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/198.png)
    represents a dynamically sampled subset of neighbors for node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/199.png)
    at each iteration. This adaptability allows GraphSAGE to scale more efficiently
    compared to GCN, especially in scenarios where the graph is large or when computational
    resources are limited, maintaining scalability.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The structure for PyG code remains the same as for the GCN; we will just be
    using the **GraphSAGE** module from **torch_geometric.nn** .
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'To modify the previous code to use GraphSAGE instead of GCN, you need to make
    a few changes. Here are the lines you need to update:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace the **import** statement for **GCNConv** with **SAGEConv** :'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Update the **GCN** model class to use **SAGEConv** layers instead of **GCNConv**
    :'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the **model** creation line to use the **GraphSAGE** model instead of
    **GCN** :'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With these changes, the code will now use the GraphSAGE model instead of GCN.
    The rest of the code, including loading the dataset, training, and evaluation,
    remains the same.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – GraphSAGE network leveraging neighborhood sampling](img/B22118_04_2.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – GraphSAGE network leveraging neighborhood sampling
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: At times, it is a good idea to assign different weights to neighbors based on
    their relevance to a particular node. We will now look at GATs, which borrow the
    concept of attention from language models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: GATs
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GATs** are an extension of GCNs that incorporate an attention mechanism to
    assign different weights to neighboring nodes based on their relevance. While
    GCNs apply a fixed aggregation function to combine the features of neighboring
    nodes, GATs allow for a more flexible and adaptive approach by learning the importance
    of each neighbor during the aggregation process. The core of GATs is the attention
    network.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Attention networks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **attention network** , often referred to as an **attention mechanism** or
    **attention model** , is a powerful concept in machine learning and artificial
    intelligence, particularly in the field of neural networks. It’s inspired by how
    human attention works – focusing on specific parts of input data while processing
    information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: An attention mechanism allows the model to dynamically focus on different parts
    of the input data, assigning varying degrees of importance or attention to each
    part. This enables the model to weigh the relevance of different inputs when making
    predictions or decisions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Attention networks are commonly used in tasks involving sequential data, such
    as **natural language processing** ( **NLP** ) tasks such as machine translation,
    text summarization, and sentiment analysis. In these tasks, the model needs to
    process sequences of words or tokens and understand the contextual relationships
    between them. By using attention mechanisms, the model can effectively capture
    long-range dependencies and attend to relevant parts of the input sequence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how an attention mechanism is leveraged in graph learning:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In GATs, the attention mechanism is used to compute attention coefficients between
    a node and its neighbors.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention coefficients represent the importance of each neighbor’s features
    to the target node.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention mechanism is typically implemented using an MLP or a single-layer
    neural network.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention coefficients are computed based on the learned weights of the attention
    mechanism and the features of the target node and its neighbors.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at how we can compute the attention coefficient in a graph setting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Attention coefficients computation
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each node, the attention coefficients are computed for all its neighbors.
    The attention coefficient between node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    and its neighbor ![<math ><mrow><mi>j</mi></mrow></math>](img/201.png) is calculated
    as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![<math  display="block"><mrow><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>∥</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow><mfenced
    open="(" close=")"><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mi>exp</mi><mfenced
    open="(" close=")"><mrow><mi>L</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mfenced
    open="(" close=")"><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>⋅</mo><mfenced open="["
    close="]"><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>|</mo><msub><mi>h</mi><mi>k</mi></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow></mrow></mfenced></mfrac></mrow></mrow></math>](img/202.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math  ><mml:mo>∥</mml:mo></mml:math>](img/203.png) is the concatenation
    operation, and ![<mml:math  ><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:math>](img/204.png)
    is the leaky ReLU activation function.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/205.png)
    and ![<mml:math  ><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/206.png)
    are the learned node embeddings for nodes ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    and ![<mml:math  ><mml:mi>j</mml:mi></mml:math>](img/208.png) , respectively.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math  ><mml:mi>W</mml:mi></mml:math>](img/142.png) is a learnable attention
    weight vector that is shared across all nodes.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attention coefficients are normalized using the **softmax** function to
    ensure they sum up to 1 for each node.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to compute attention coefficients, let’s see how
    these are utilized in the aggregation step of a GNN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation of neighbor features
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the attention coefficients are computed, the features of the neighboring
    nodes are aggregated using a weighted sum.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'The aggregated features for node ![<mml:math  ><mml:mi>i</mml:mi></mml:math>](img/195.png)
    are calculated as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">''</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal"> </mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/211.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>σ</mml:mi></mml:math>](img/212.png)
    is a non-linear activation function, such as ReLU. The aggregated features ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">'</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/213.png)
    represent the updated representation of node ![<mml:math  ><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi></mml:math>](img/214.png)
    after considering the importance of its neighbors.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![  Figure 4.3 – Multi-head attention (with K = 3 heads) by node 1 on its neighborhood](img/B22118_04_3.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Multi-head attention (with K = 3 heads) by node 1 on its neighborhood
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: To capture multiple aspects of the relationships between two nodes, we can leverage
    the concept of multi-head attention.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In GNNs, **multi-head attention** can be applied to learn representations of
    nodes as an extension of vanilla attention. Each “head” can be seen as a different
    perspective or attention mechanism applied to a node’s neighborhood. By running
    multiple heads in parallel, the GNN can capture diverse aspects of the node’s
    local graph structure and feature space. This allows the model to aggregate information
    from neighboring nodes in multiple ways, enhancing its ability to learn informative
    node representations that incorporate various patterns and relationships within
    the graph.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several notable aspects to keep in mind for multi-head attention:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: GATs can employ multi-head attention to capture different aspects of the node
    relationships.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In multi-head attention, multiple attention mechanisms are used in parallel,
    each with its own set of learnable parameters.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output features from each attention head are concatenated or averaged to
    obtain the final node representations.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-head attention allows the model to learn diverse patterns and capture
    different types of dependencies between nodes.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At times, a single attention layer might be unable to capture complex relationships
    in a graph. Stacking multiple layers can help improve the learning space.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Stacking GAT layers
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use multiple GAT layers, similar to GCNs:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Like GCNs, GAT layers can be stacked to capture higher-order dependencies and
    learn more abstract representations of the graph.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each layer, the updated node representations from the previous layer serve
    as input to the next layer.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final node representations obtained after multiple GAT layers can be used
    for downstream tasks such as node classification or graph classification.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GATs seamlessly combine the advantages of an **adaptive receptive field** and
    an **interpretable attention mechanism** , making them powerful tools for processing
    graph-structured data. The adaptive receptive field of GATs allows nodes to dynamically
    adjust their focus on relevant neighbors during information aggregation. Importantly,
    GATs provide interpretable attention coefficients, enabling a clear understanding
    of the model’s decision-making process. The transparency in attention weights
    allows for intuitive insights into which neighbors contribute significantly to
    a node’s representation, fostering interpretability and facilitating model debugging.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: This combination of adaptability and interpretability makes GATs effective in
    capturing fine-grained local information while maintaining a global perspective,
    contributing to their success in various graph-based tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the model code for GAT:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this code, the GAT model class is defined using two **GATConv** layers. The
    first layer has multiple attention heads specified by the **heads** parameter,
    while the second layer has a single attention head. The activation function used
    is the **exponential linear unit** ( **ELU** ) function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the **__init__** method of the GAT class, we multiply **hidden_channels**
    by **heads** when specifying the input channels for the second **GATConv** layer.
    This is because the output of the first layer has **hidden_channels * heads dimensions**
    due to the multiple attention heads.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided a comprehensive overview of graph-based deep learning
    models, starting with the fundamental concept of message passing and then delving
    into specific GNN architectures such as GCNs, GraphSAGE, and GATs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based models rely on message passing, a key operation where nodes exchange
    information with neighbors to update their representations. GCNs perform convolutions
    on graphs, aggregating neighboring node information to learn node representations.
    GraphSAGE efficiently generates embeddings for large-scale graphs through neighborhood
    sampling. GATs integrate attention mechanisms, enabling nodes to assign varying
    importance weights to neighbors during message passing. These techniques enhance
    the capacity of graph-based models to capture complex relationships and patterns
    within data structures.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the foundational understanding of prevalent graph learning algorithms,
    we’ll explore the contemporary challenges confronting GNNs in the upcoming chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
