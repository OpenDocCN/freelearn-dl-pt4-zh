- en: Predicting Future Stock Prices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The financial market is a very important part of any economy. For an economy
    to thrive, its financial market must be solid. Since the advent of machine learning,
    companies have begun to adopt algorithmic trading in the purchase of stocks and
    other financial assets. There has been proven successful with this method, and
    it has risen in prominence over time. Given its rise, several machine models have
    been developed and adopted for algorithmic trading. One popular machine learning
    model for trading is the time series analysis. You have already learned about
    reinforcement learning and Keras, and in this chapter, they will be used to develop
    a model that can predict stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: Background problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automation is taking over in almost every sector, and the financial market is
    no exception. Creating automated algorithmic trading models will provide for a
    faster and more accurate analysis of stocks before purchase. Multiple indicators
    can be analyzed at a speed that humans are incapable of. Also, in trading, it
    is dangerous to operate with emotions. Machine learning models can solve that
    problem. There is also a reduction in transaction costs, as there is no need for
    continuous supervision.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, you will learn how to combine reinforcement learning with
    time series modeling, in order to predict the prices of stocks, based on real-life
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Data used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data that we will use will be the standard and poor's 500\. According to
    Wikipedia, it is *An American stock market index based on the market capitalizations of
    500 large companies having common stock listed on the NYSE or NASDAQ.* Here is
    a link to the data ([https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC](https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Date**: This indicates the date under consideration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Open**: This indicates the price at which the market opens on the date'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**High**: This indicates the highest market price on the date'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Low**: This indicates the lowest market price on the date'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Close**: This indicates the price at which the market closes on the date,
    adjusted for the split'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adj Close**: This indicates the adjusted closing price for both the split
    and dividends'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Volume**: This indicates the total volume of shares available'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The date under consideration for training the data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'On the website, filter the date as follows, and download the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd47755a-4b1b-423b-802d-a4121658a46e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For testing, we will use the following date range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the dates on the website accordingly, and download the dataset for testing,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9367771-52bb-4db0-8eda-5a61cf78a297.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will define some possible actions that the agent can
    carry out.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our solution uses an actor-critic reinforcement learning model, along with
    an infused time series, to help us predict the best action, based on the stock
    prices. The possible actions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hold**: This means that based on the price and projected profit, the trader
    should hold a stock'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sell**: This means that based on the price and projected profit, the trader
    should sell a stock'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Buy**: This means that based on the price and projected profit, the trader
    should buy a stock'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The actor-critic network is a family of reinforcement learning methods premised
    on two interacting network models. These models have two components: the actor
    and the critic. In our case, the network models that we will use will be neural
    networks. We will use the Keras package, which you have already learned about,
    to create the neural networks. The reward function that we are looking to improve
    is the profit.'
  prefs: []
  type: TYPE_NORMAL
- en: The actor takes in the state of the environment, then returns the best action,
    or a policy that refers to a probability distribution over actions. This seems
    like a natural way to perform reinforcement learning, as policies are directly
    returned as a function of the state.
  prefs: []
  type: TYPE_NORMAL
- en: The critic evaluates the actions returned by the actor-network. This is similar
    to the traditional deep Q network; in the environment state and an action to return
    a score representing the value of taking that action given the state. The job
    of the critic is to compute an approximation, which is then used to update the
    actor in the direction of its gradient. The critic is trained itself temporal
    difference algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: These two networks are trained simultaneously. With time, the critic network
    is able to improve its `Q_value` prediction, and the actor also learns how to
    make better decisions, given the state.
  prefs: []
  type: TYPE_NORMAL
- en: There are five scripts that make up this solution, and they will be described
    in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Actor script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The actor script is where the policy model is defined. We begin by importing
    certain modules from Keras: layers, optimizers, models, and the backend. These
    modules will help us to construct our neural network: Let''s start by importing
    the required functions from Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a class called `Actor`, whose object takes in the parameters of the
    `state` and `action` size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the state size, which represents the dimension of
    each state, and the action size, which represents the dimensions of the actions.
    Next, call a function to build the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a policy model that maps the states to actions, and start by defining
    the input layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add hidden layers to the model. There are two dense layers, each one followed
    by a batch normalization and an activation layer. The dense layers are regularized.
    The two layers have 16 and 32 hidden units, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The final output layer will predict the action probabilities that have an activation
    of `softmax`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss function by using the action value (`Q_value`) gradients, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `optimizer` and training function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The custom training function for the actor-network that makes use of the Q gradients
    with respect to the action probabilities. With this custom function, the training
    aims to maximize the profits (in other words, minimize the negatives of the `Q_values`).
  prefs: []
  type: TYPE_NORMAL
- en: Critic script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by importing certain modules from Keras: layers, optimizers, models,
    and the backend. These modules will help us to construct our neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a class called `Critic`, whose object takes in the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a critic (value) network that maps `state` and `action` pairs (`Q_values`),
    and define input layers, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the hidden layers for the state pathway, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the hidden layers for the action pathway, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine the state and action pathways, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the final output layer to produce the action values (`Q_values`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Keras model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `optimizer` and compile a model for training with the built-in loss
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the action gradients (the derivative of `Q_values`, with respect to
    `actions`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an additional function to fetch the action gradients (to be used by
    the actor model), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the critic script.
  prefs: []
  type: TYPE_NORMAL
- en: Agent script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will train an agent that will perform reinforcement learning
    based on the actor and critic networks. We will perform the following steps to
    achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an agent class whose initial function takes in the batch size, state
    size, and an evaluation Boolean function, to check whether the training is ongoing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the agent class, create the following methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `actor` and `critic` scripts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `numpy`, `random`, `namedtuple`, and `deque` from the `collections`
    package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `ReplayBuffer` class that adds, samples, and evaluates a buffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new experience to the replay buffer memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly sample a batch of experienced tuples from the memory. In the following
    function, we randomly sample states from a memory buffer. We do this so that the
    states that we feed to the model are not temporally correlated. This will reduce
    overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the current size of the buffer memory, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The reinforcement learning agent that learns using the actor-critic network
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The number of actions are defined as 3: sit, buy, sell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Define the replay memory size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define whether or not training is ongoing. This variable will be changed during
    the training and evaluation phase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Discount factor in Bellman equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'A soft update of the actor and critic networks can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The actor policy model maps states to actions and instantiates the actor networks
    (local and target models, for soft updates of parameters):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The critic (value) model that maps the state-action pairs to `Q_values` is
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the critic model (the local and target models are utilized to allow
    for soft updates), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code sets the target model parameters to local model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns an action, given a state, using the actor (policy network) and the output
    of the `softmax` layer of the actor-network, returning the probability for each
    action. An action method that returns an action, given a state, using the actor
    (policy network) is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns a stochastic policy, based on the action probabilities in the training
    model and a deterministic action corresponding to the maximum probability during
    testing. There is a set of actions to be carried out by the agent at every step
    of the episode. A method (step) that returns the set of actions to be carried
    out by the agent at every step of the episode is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code adds a new experience to the memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code asserts that enough experiences are present in the memory
    to train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code samples a random batch from the memory to train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Learn from the sampled experiences, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code updates the state to the next state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Learning from the sampled experiences through the actor and the critic. Create
    a method to learn from the sampled experiences through the actor and the critic,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Return a separate array for each experience in the replay component and predict
    actions based on the next states, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the `Q_value` of the actor output for the next state, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Target the `Q_value` to serve as a label for the critic network, based on the
    temporal difference, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the critic model to the time difference of the target, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the actor model (local) using the gradient of the critic network output
    with respect to the action probabilities fed from the actor-network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a custom training function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, initiate a soft update of the parameters of both networks, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This performs soft updates on the model parameters, based on the parameter
    `tau` to avoid drastic model changes. A method that updates the model by performing
    soft updates on the model parameters, based on the parameter `tau` (to avoid drastic
    model changes), is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the agent script.
  prefs: []
  type: TYPE_NORMAL
- en: Helper script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this script, we will create functions that will be helpful for training,
    via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` and `math` modules, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a function to format the price to two decimal places, to reduce
    the ambiguity of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Return a vector of stock data from the CSV file. Convert the closing stock
    prices from the data to vectors, and return a vector of all stock prices, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a function to generate states from the input vector. Create the
    time series by generating the states from the vectors created in the previous
    step. The function for this takes three parameters: the data; a time, *t* (the
    day that you want to predict); and a window (how many days to go back in time).
    The rate of change between these vectors will then be measured and based on the
    sigmoid function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, scale the state vector from 0 to 1 with a sigmoid function. The sigmoid
    function can map any input value, from 0 to 1\. This helps to normalize the values
    to probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: All of the necessary functions and classes are now defined, so we can start
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Training the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will proceed to train the data, based on our agent and helper methods. This
    will provide us with one of three actions, based on the states of the stock prices
    at the end of the day. These states can be to buy, sell, or hold. During training,
    the prescribed action for each day is predicted, and the price (profit, loss,
    or unchanged) of the action is calculated. The cumulative sum will be calculated
    at the end of the training period, and we will see whether there has been a profit
    or a loss. The aim is to maximize the total profit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the imports, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define the number of market days to consider as the window size, and
    define the batch size with which the neural network will be trained, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the stock agent with the window size and batch size, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, read the training data from the CSV file, using the helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the episode count is defined as `300`. The agent will look at the data
    for so many numbers of times. An episode represents a complete pass over the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can start to iterate through the episodes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Each episode has to be started with a state based on the data and window size.
    The inventory of stocks is initialized before going through the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, start to iterate over every day of the stock data. The action probability
    is predicted by the agent, based on the `state`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `action` can be held, if the agent decides not to do anything with the
    stock. Another possible action is to buy (hence, the stock will be added to the
    inventory), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `action` is `2`, the agent sells the stocks and removes it from the
    inventory. Based on the sale, the profit (or loss) is calculated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see logs similar to those that follow during the training process.
    The stocks are bought and sold at certain prices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the test data is read from the CSV file. The initial state is inferred
    from the data. The steps are very similar to a single episode of the training
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The profit starts at `0`. The agent is initialized with a zero inventory and
    in test mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, every day of trading is iterated, and the agent can act upon the data.
    Every day, the agent decides an action. Based on the action, the stock is held,
    sold, or bought:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'If the action is `0`, then there is no trade. The state can be called **holding**
    during that period:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'If the action is `1`, buy the stock by adding it to the inventory, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'If the action is `2`, the agent sells the stock by removing it from the inventory.
    The difference in price is recorded as a profit or a loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the script starts to run, the model will get better over time through
    training. You can see the logs, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The model has traded and made a total profit of $10,427\. Please note that this
    style of trading is not suitable for the real world, as trading involves more
    costs and uncertainty; hence, this trading style could have adverse effects.
  prefs: []
  type: TYPE_NORMAL
- en: Final result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After training the data, we tested it against the `test` dataset. Our model
    resulted in a total profit of `$10427.24`. The best thing about the model was
    that the profits kept improving over time, indicating that it was learning well
    and taking better actions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, machine learning can be applied to several industries and can
    be applied very efficiently in financial markets, as you saw in this chapter.
    We can combine different models, as we did with reinforcement learning and time
    series, to produce stronger models that suit our use cases. We discussed the use
    of reinforcement learning and time series to predict the stock market. We worked
    with an actor-critic model that determined the best action, based on the state
    of the stock prices, with the aim of maximizing profits. In the end, we obtained
    a result that boasted an overall profit and included increasing profits over time,
    indicating that the agent learned more with each state.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the future areas of work.
  prefs: []
  type: TYPE_NORMAL
