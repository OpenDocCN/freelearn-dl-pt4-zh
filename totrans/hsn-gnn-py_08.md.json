["```py\n    from torch_geometric.datasets import Planetoid\n    dataset = Planetoid(root='.', name=\"Pubmed\")\n    data = dataset[0]\n    print(f'Dataset: {dataset}')\n    print('-------------------')\n    print(f'Number of graphs: {len(dataset)}')\n    print(f'Number of nodes: {data.x.shape[0]}')\n    print(f'Number of features: {dataset.num_features}')\n    print(f'Number of classes: {dataset.num_classes}')\n    print('Graph:')\n    print('------')\n    print(f'Training nodes: {sum(data.train_mask).item()}')\n    print(f'Evaluation nodes: {sum(data.val_mask).item()}')\n    print(f'Test nodes: {sum(data.test_mask).item()}')\n    print(f'Edges are directed: {data.is_directed()}')\n    print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n    print(f'Graph has loops: {data.has_self_loops()}')\n    ```", "```py\n    Dataset: Pubmed()\n    -------------------\n    Number of graphs: 1\n    Number of nodes: 19717\n    Number of features: 500\n    Number of classes: 3\n    Graph:\n    ------\n    Training nodes: 60\n    Evaluation nodes: 500\n    Test nodes: 1000\n    Edges are directed: False\n    Graph has isolated nodes: False\n    Graph has loops: False\n    ```", "```py\n    from torch_geometric.loader import NeighborLoader\n    train_loader = NeighborLoader(\n        data,\n        num_neighbors=[10,10],\n        batch_size=16,\n        input_nodes=data.train_mask,\n    )\n    ```", "```py\n    for i, subgraph in enumerate(train_loader):\n        print(f'Subgraph {i}: {subgraph}')\n    Subgraph 0: Data(x=[400, 500], edge_index=[2, 455], y=[400], train_mask=[400], val_mask=[400], test_mask=[400], batch_size=16)\n    Subgraph 1: Data(x=[262, 500], edge_index=[2, 306], y=[262], train_mask=[262], val_mask=[262], test_mask=[262], batch_size=16)\n    Subgraph 2: Data(x=[275, 500], edge_index=[2, 314], y=[275], train_mask=[275], val_mask=[275], test_mask=[275], batch_size=16)\n    Subgraph 3: Data(x=[194, 500], edge_index=[2, 227], y=[194], train_mask=[194], val_mask=[194], test_mask=[194], batch_size=12)\n    ```", "```py\n    import numpy as np\n    import networkx as nx\n    import matplotlib.pyplot as plt\n    from torch_geometric.utils import to_networkx\n    fig = plt.figure(figsize=(16,16))\n    for idx, (subdata, pos) in enumerate(zip(train_loader, [221, 222, 223, 224])):\n        G = to_networkx(subdata, to_undirected=True)\n        ax = fig.add_subplot(pos)\n        ax.set_title(f'Subgraph {idx}', fontsize=24)\n        plt.axis('off')\n        nx.draw_networkx(G,\n                        pos=nx.spring_layout(G, seed=0),\n                        with_labels=False,\n                        node_color=subdata.y,\n                        )\n    plt.show()\n    ```", "```py\n    def accuracy(pred_y, y):\n        return ((pred_y == y).sum() / len(y)).item()\n    ```", "```py\n    import torchmport torch.nn.functional as F\n    from torch_geometric.nn import SAGEConv\n    class GraphSAGE(torch.nn.Module):\n        def __init__(self, dim_in, dim_h, dim_out):\n            super().__init__()\n            self.sage1 = SAGEConv(dim_in, dim_h)\n            self.sage2 = SAGEConv(dim_h, dim_out)\n    ```", "```py\n       def forward(self, x, edge_index):\n            h = self.sage1(x, edge_index)\n            h = torch.relu(h)\n            h = F.dropout(h, p=0.5, training=self.training)\n            h = self.sage2(h, edge_index)\n            return F.log_softmax(h, dim=1)\n    ```", "```py\n        def fit(self, data, epochs):\n            criterion = torch.nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n            self.train()\n            for epoch in range(epochs+1):\n                total_loss, val_loss, acc, val_acc = 0, 0, 0, 0\n    ```", "```py\n                for batch in train_loader:\n                    optimizer.zero_grad()\n                    out = self(batch.x, batch.edge_index)\n                    loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n                    total_loss += loss\n                    acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n                    loss.backward()\n                    optimizer.step()\n                    # Validation\n                    val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n                    val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n    ```", "```py\n           if epoch % 20 == 0:\n                    print(f'Epoch {epoch:>3} | Train Loss: {loss/len(train_loader):.3f} | Train Acc: {acc/len(train_loader)*100:>6.2f}% | Val Loss: {val_loss/len(train_loader):.2f} | Val Acc: {val_acc/len(train_loader)*100:.2f}%')\n    ```", "```py\n        @torch.no_grad()\n        def test(self, data):\n            self.eval()\n            out = self(data.x, data.edge_index)\n            acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n            return acc\n    ```", "```py\n    graphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes)\n    print(graphsage)\n    graphsage.fit(data, 200)\n    ```", "```py\n    GraphSAGE(\n      (sage1): SAGEConv(500, 64, aggr=mean)\n      (sage2): SAGEConv(64, 3, aggr=mean)\n    )\n    Epoch 0 | Train Loss: 0.317 | Train Acc: 28.77% | Val Loss: 1.13 | Val Acc: 19.55%\n    Epoch 20 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 75.07%\n    Epoch 40 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.55 | Val Acc: 80.56%\n    Epoch 60 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.35 | Val Acc: 86.11%\n    Epoch 80 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 73.58%\n    Epoch 100 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.79 | Val Acc: 74.72%\n    Epoch 120 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.71 | Val Acc: 76.75%\n    Epoch 140 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 67.50%\n    Epoch 160 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 73.54%\n    Epoch 180 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.47 | Val Acc: 86.11%\n    Epoch 200 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 78.37%\n    ```", "```py\n    acc = graphsage.test(data)\n    print(f'GraphSAGE test accuracy: {acc*100:.2f}%')\n    GraphSAGE test accuracy: 74.70%\n    ```", "```py\n    from torch_geometric.datasets import PPI\n    train_dataset = PPI(root=\".\", split='train')\n    val_dataset = PPI(root=\".\", split='val')\n    test_dataset = PPI(root=\".\", split='test')\n    ```", "```py\n    from torch_geometric.data import Batch\n    from torch_geometric.loader import NeighborLoader\n    train_data = Batch.from_data_list(train_dataset)\n    loader = NeighborLoader(train_data, batch_size=2048, shuffle=True, num_neighbors=[20, 10], num_workers=2, persistent_workers=True)\n    ```", "```py\n    from torch_geometric.loader import DataLoader\n    train_loader = DataLoader(train_dataset, batch_size=2)\n    val_loader = DataLoader(val_dataset, batch_size=2)\n    test_loader = DataLoader(test_dataset, batch_size=2)\n    ```", "```py\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    from torch_geometric.nn import GraphSAGE\n    model = GraphSAGE(\n        in_channels=train_dataset.num_features,\n        hidden_channels=512,\n        num_layers=2,\n        out_channels=train_dataset.num_classes,\n    ).to(device)\n    ```", "```py\n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    def fit():\n        model.train()\n        total_loss = 0\n        for data in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            out = model(data.x, data.edge_index)\n            loss = criterion(out, data.y)\n            total_loss += loss.item() * data.num_graphs\n            loss.backward()\n            optimizer.step()\n        return total_loss / len(train_loader.dataset)\n    ```", "```py\n    from sklearn.metrics import f1_score\n    @torch.no_grad()\n    def test(loader):\n        model.eval()\n        data = next(iter(loader))\n        out = model(data.x.to(device), data.edge_index.to(device))\n        preds = (out > 0).float().cpu()\n        y, pred = data.y.numpy(), preds.numpy()\n        return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n    ```", "```py\n    for epoch in range(301):\n        loss = fit()\n        val_f1 = test(val_loader)\n        if epoch % 50 == 0:\n            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Val F1 score: {val_f1:.4f}')\n    Epoch 0 | Train Loss: 0.589 | Val F1-score: 0.4245\n    Epoch 50 | Train Loss: 0.194 | Val F1-score: 0.8400\n    Epoch 100 | Train Loss: 0.143 | Val F1-score: 0.8779\n    Epoch 150 | Train Loss: 0.123 | Val F1-score: 0.8935\n    Epoch 200 | Train Loss: 0.107 | Val F1-score: 0.9013\n    Epoch 250 | Train Loss: 0.104 | Val F1-score: 0.9076\n    Epoch 300 | Train Loss: 0.090 | Val F1-score: 0.9154\n    ```", "```py\n    print(f'Test F1 score: {test(test_loader):.4f}')\n    Test F1 score: 0.9360\n    ```"]