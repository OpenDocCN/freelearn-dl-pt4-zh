- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trust Region Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a look at the approaches used to improve the
    stability of the stochastic policy gradient method. Some attempts have been made
    to make the policy improvement more stable, and in this chapter, we will focus
    on three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Proximal policy optimization (PPO)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trust region policy optimization (TRPO)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage actor-critic (A2C) using Kronecker-factored trust region (ACKTR) .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we will compare these methods to a relatively new off-policy method
    called soft actor-critic (SAC), which is the evolution of the deep deterministic
    policy gradients (DDPG) method described in Chapter [15](ch019.xhtml#x1-27200015).
    To compare them to the A2C baseline, we will use several environments from the
    so-called “locomotion gym environments” – environments shipped with Farama Gymnasium
    (using MuJoCo and PyBullet). We also will do a head-to-head comparison between
    PyBullet and MuJoCo (which we discussed in Chapter 15).
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the methods that we will look at is to improve the stability
    of the policy update during training. There is a dilemma: on the one hand, we’d
    like to train as fast as we can, making large steps during the stochastic gradient
    descent (SGD) update. On the other hand, a large update of the policy is usually
    a bad idea. The policy is a very nonlinear thing, so a large update could ruin
    the policy we’ve just learned.'
  prefs: []
  type: TYPE_NORMAL
- en: Things can become even worse in the reinforcement learning (RL) landscape because
    you can’t recover from making a bad update to the policy by subsequent updates.
    Instead, the bad policy will provide bad experience samples that we will use in
    subsequent training steps, which could break our policy completely. Thus, we want
    to avoid making large updates by all means possible. One of the naïve solutions
    would be to use a small learning rate to take baby steps during SGD, but this
    would significantly slow down the convergence.
  prefs: []
  type: TYPE_NORMAL
- en: To break this vicious cycle, several attempts have been made by researchers
    to estimate the effect that our policy update is going to have in terms of future
    outcomes. One of the popular approaches is the trust region optimization extension,
    which constrains the steps taken during the optimization to limit its effect on
    the policy. The main idea is to prevent a dramatic policy update during the loss
    optimization by checking the Kullback-Leibler (KL) divergence between the old
    and the new policy. Of course, this is an informal explanation, but it can help
    you understand the idea, especially as those methods are quite math-heavy (especially
    TRPO).
  prefs: []
  type: TYPE_NORMAL
- en: Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previous editions of this book used the Roboschool library from OpenAI ([https://openai.com/index/roboschool](https://openai.com/index/roboschool))
    to illustrate trust region methods. But eventually, OpenAI deprecated Roboschool
    and stopped its support.
  prefs: []
  type: TYPE_NORMAL
- en: 'But environments are still available in other sources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyBullet: The physics simulator we experimented with in the previous chapter,
    which includes a wide variety of environments that support Gym. PyBullet may be
    a bit outdated (the latest release was in 2022), but it is still workable with
    a bit of hacking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farama Gymnasium MuJoCo environments: MuJoCo is a physics simulator that we
    discussed in Chapter [15](ch019.xhtml#x1-27200015). After it was made open source,
    MuJoCo was adopted in various products, including Gymnasium, which ships several
    environments: [https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore two problems: HalfCheetah-v4, which models
    a two-legged creature, and Ant-v4, which has four legs. Their state and action
    spaces are very similar to the Minitaur environment that we saw in Chapter [15](ch019.xhtml#x1-27200015):
    the state includes characteristics from joints, and the actions are activations
    of those joints. The goal for each problem is to move as far as possible, minimizing
    the energy spent. The following figure shows screenshots of the two environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file215.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Screenshots of the cheetah and ant environments'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiment, we’ll use PyBullet and MuJoCo to do a comparison of both
    simulators in terms of speed and training dynamics (however, note that the internal
    structure of the PyBullet and MuJoCo environments might be different, and so the
    comparison of training dynamics may not always be reliable). To install the Gymnasium
    with MuJoCo extensions, you need to run the following command in your Python environment:
    pip install gymnasium[mujoco]==0.29.0.'
  prefs: []
  type: TYPE_NORMAL
- en: The A2C baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To establish the baseline results, we will use the A2C method in a very similar
    way to the previous chapter. The complete source code is in the Chapter16/01_train_a2c.py
    and Chapter16/lib/model.py files. There are a few differences between this baseline
    and the version we used before:'
  prefs: []
  type: TYPE_NORMAL
- en: 16 parallel environments are used to gather experience during the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They differ in model structure and the way that we perform exploration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate the differences between this baseline and the previously discussed
    version, let’s look at the model and the agent classes.
  prefs: []
  type: TYPE_NORMAL
- en: The actor and critic are placed in separate networks without sharing weights.
    They follow the approach used in Chapter [15](ch019.xhtml#x1-27200015), with our
    critic estimating the mean and the variance for the actions. However, now, variance
    is not a separate head of the base network; it is just a single parameter of the
    model. This parameter will be adjusted during the training by SGD, but it doesn’t
    depend on the observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor network has two hidden layers of 64 neurons, each with tanh nonlinearity
    (to push the output in the −1…1 range). The variance is modeled as a separate
    network parameter and is interpreted as a logarithm of the standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The critic network also has two hidden layers of the same size, with one single
    output value, which is the estimation of V (s), which is a discounted value of
    the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent that converts the state into the action also works by simply obtaining
    the predicted mean from the state and applying the noise with variance, dictated
    by the current value of the logstd parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training utility 01_train_a2c.py could be started in two different modes:
    with PyBullet as the physics simulator (without any extra command-line options)
    or with MuJoCo (if the --mujoco parameter is given).'
  prefs: []
  type: TYPE_NORMAL
- en: By default, the HalfCheetah environment is used, which simulates a flat two-legged
    creature that can jump around on its legs. With -e ant, you can switch to the
    Ant environment, which is a 3-dimensional 4-legged spider. You can also experiment
    with other environments shipped with Gymnasium and PyBullet, but this will require
    tweaking the common.py module.
  prefs: []
  type: TYPE_NORMAL
- en: Results for HalfCheetah on PyBullet are shown in Figure [16.2](#x1-294002r2).
    Performance on my machine (using the GPU) was about 1,600 frames per second during
    the training, so 100M training steps took 20 hours in total.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: The reward during training (left) and test reward (right) for
    HalfCheetah on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: The dynamics suggest that the policy could be further improved with more time
    given to optimization, but for our purpose of method comparison, it should be
    enough. Of course, if you’re curious and have plenty of time, you can run this
    for longer and find the point when the policy stops improving. According to research
    papers, HalfCheetah has a maximum score of around 4,000-5,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use MuJoCo as a physics simulation engine, training has to be started with
    the --mujoco command-line option. MuJoCo has a performance of 5,100 frames per
    second, which is 3 times faster than PyBullet, which is really nice. In addition,
    the training has much better dynamics, so in 90M training steps (which took about
    5 hours) the model got a reward of 4,500\. Plots for MuJoCo are shown in Figure [16.3](#x1-294004r3):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: The difference could be explained by a more accurate simulation, but could also
    be attributed to the difference in the observation space and the underlying model
    differences. PyBullet’s model has 26 parameters provided to the agent as observations,
    while MuJoCo has only 17, so those models are not identical.
  prefs: []
  type: TYPE_NORMAL
- en: To test our model in the Ant environment, the -e ant command-line option has
    to be passed to the training process. This model is more complex (due to the 3D
    nature of the model and more joints being used), so the simulation is slower.
    On PyBullet, the speed is around 1,400 frames per second. On MuJoCo, the speed
    is 2,500.
  prefs: []
  type: TYPE_NORMAL
- en: The MuJoCo Ant environment also has an additional check for “healthiness” –
    if the simulated creature is inclined more than a certain degree, the episode
    is terminated. This check is enabled by default and has a very negative effect
    on the training – in the early stage of the training, our method has no chance
    of figuring out how to make the ant stand on its legs. The reward in the environment
    is the distance traveled, but with this early termination, our training has no
    chance of discovering this. As a result, the training process got stuck forever
    in local minima without making progress. To overcome this, we need to disable
    this healthiness check by passing the --no-unhealthy command-line option (which
    only has to be done for MuJoCo training).
  prefs: []
  type: TYPE_NORMAL
- en: In principle, you can implement more advanced exploration methods, such as the
    OU process (discussed in Chapter [15](ch019.xhtml#x1-27200015)) or other methods
    (covered in Chapter [18](ch022.xhtml#x1-32800018)) to address the issue we just
    discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The results of the training in the Ant environment are shown in Figure [16.4](#x1-294006r4)
    and Figure [16.5](#x1-294007r5).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the MuJoCo plots in Figure [16.5](#x1-294007r5), the testing
    reward had almost no increase for the first 100M steps of training, but then grew
    to a score of 5,000 (the best model got 5,380 on testing). This result is quite
    impressive. According to the [https://paperswithcode.com](https://paperswithcode.com)
    website, the state of the art for Ant MuJoCo environment is 4,362.9, obtained
    by IQ-Learn in 2021: [https://paperswithcode.com/sota/mujoco-games-on-ant](https://paperswithcode.com/sota/mujoco-games-on-ant).'
  prefs: []
  type: TYPE_NORMAL
- en: Video recording
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in the previous chapter, there is a utility that can benchmark the trained
    model and record a video of the agent in action. As all the methods in this chapter
    share the same actor network, the tool is universal for all the methods illustrated
    here: 02_play.py.'
  prefs: []
  type: TYPE_NORMAL
- en: You need to pass the model file stored in the saves directory during training,
    change the environment using the -e ant command line, and enable the MuJoCo engine
    with the --mujoco parameter. This is important because the same environments in
    PyBullet and MuJoCo have different amounts of observations, and so the physics
    engine has to match to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the individual videos for the best A2C models as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HalfCheetah on PyBullet (score 2,189): [https://youtu.be/f3ZhjnORQm0](https://youtu.be/f3ZhjnORQm0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HalfCheetah on MuJoCo (score 4,718): [https://youtube.com/shorts/SpaWbS0hM8I](https://youtube.com/shorts/SpaWbS0hM8I)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on PyBullet (score 2,425): [https://youtu.be/SIUM_Q24zSk](https://youtu.be/SIUM_Q24zSk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on MuJoCo (score 5,380): [https://youtube.com/shorts/mapOraGKtG0](https://youtube.com/shorts/mapOraGKtG0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PPO method came from the OpenAI team, and it was proposed after TRPO, which
    is from 2015\. However, we will start with PPO because it is much simpler than
    TRPO. It was first proposed in the 2017 paper named Proximal Policy Optimization
    Algorithms by Schulman et al. [[Sch+17](#)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The core improvement over the classic A2C method is changing the formula used
    to estimate the policy gradients. Instead of using the gradient of the logarithm
    probability of the action taken, the PPO method uses a different objective: the
    ratio between the new and the old policy scaled by the advantages.'
  prefs: []
  type: TYPE_NORMAL
- en: In math form, the A2C objective could be written like this
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq60.png)'
  prefs: []
  type: TYPE_IMG
- en: which means our gradient on model 𝜃 is estimated as the logarithm of the policy
    π multiplied by the advantage A.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new objective proposed in PPO is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The reason for changing the objective is the same as with the cross-entropy
    method covered in Chapter [4](ch008.xhtml#x1-740004): importance sampling. However,
    if we just start to blindly maximize this value, it may lead to a very large update
    to the policy weights. To limit the update, the clipped objective is used. If
    we write the ratio between the new and the old policy as ![-π𝜃(at|st)- π𝜃old(at|st)](img/eq56.png),
    the clipped objective could be written as'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq57.png)'
  prefs: []
  type: TYPE_IMG
- en: This objective limits the ratio between the old and the new policy to be in
    the interval [1 −𝜖,1 + 𝜖], so by varying 𝜖, we can limit the size of the update.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference from the A2C method is the way that we estimate the advantage.
    In the A2C paper, the advantage obtained from the finite-horizon estimation of
    T steps is in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq58.png)'
  prefs: []
  type: TYPE_IMG
- en: In the PPO paper, the authors used a more general estimation
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq59.png)'
  prefs: []
  type: TYPE_IMG
- en: where σ[t] = r[t] + γV (s[t+1]) −V (s[t]).
  prefs: []
  type: TYPE_NORMAL
- en: 'The original A2C estimation is a special case of the proposed method with λ
    = 1\. The PPO method also uses a slightly different training procedure: a long
    sequence of samples is obtained from the environment and then the advantage is
    estimated for the whole sequence before several epochs of training are performed.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code of the sample is placed in two source code files: Chapter16/04_train_ppo.py
    and Chapter16/lib/model.py. The actor, the critic, and the agent classes are exactly
    the same as we had in the A2C baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences are in the training procedure and the way that we calculate
    advantages, but let’s start with the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The value of GAMMA is already familiar, but GAE_LAMBDA is the new constant that
    specifies the lambda factor in the advantage estimator. The authors chose to use
    a value of 0.95 in the PPO paper.
  prefs: []
  type: TYPE_NORMAL
- en: The method assumes that a large number of transitions will be obtained from
    the environment for every subiteration. (As mentioned previously in this section,
    when describing PPO, during training, it performs several epochs over the sampled
    training batch.) We also use two different optimizers for the actor and the critic
    (as they have no shared weights).
  prefs: []
  type: TYPE_NORMAL
- en: 'For every batch of TRAJECTORY_SIZE samples, we perform PPO_EPOCHES iterations
    of the PPO objective, with mini-batches of 64 samples. The value PPO_EPS specifies
    the clipping value for the ratio of the new and the old policy. The following
    function takes the trajectory with steps and calculates advantages for the actor
    and reference values for the critic training. Our trajectory is not a single episode,
    but could be several episodes concatenated together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As the first step, we ask the critic to convert states into values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next loop joins the values obtained and experience points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For every trajectory step, we need the current value (obtained from the current
    state) and the value for the subsequent step (to perform the estimation using
    the Bellman equation). We also traverse the trajectory in reverse order in order
    to calculate more recent values of the advantage in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In every step, our action depends on the done_trunc flag for this step. If this
    is the terminal step of the episode, we have no prior reward to take into account.
    (Remember, we’re processing the trajectory in reverse order.) So, our value of
    delta in this step is just the immediate reward minus the value predicted for
    the step. If the current step is not terminal, delta will be equal to the immediate
    reward plus the discounted value from the subsequent step, minus the value for
    the current step. In the classic A2C method, this delta was used as an advantage
    estimation, but here, the smoothed version is used, so the advantage estimation
    (tracked in the last_gae variable) is calculated as the sum of deltas with the
    discount factor γ^λ.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the function is to calculate advantages and reference values for
    the critic, so we save them in lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the function, we convert values to tensors and return them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the training loop, we gather a trajectory of the desired size using the
    ExperienceSource(steps_count=1) class from the PTAN library. This configuration
    provides us with individual steps from the environment in Experience dataclass
    instances, containing the state, action, reward, and termination flag. The following
    is the relevant part of the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we’ve got a trajectory that’s large enough for training (which is given
    by the TRAJECTORY_SIZE hyperparameter), we convert states and actions taken into
    tensors and use the already-described function to obtain advantages and reference
    values. Although our trajectory is quite long, the observations of our environments
    are small enough, so it’s fine to process our batch in one step. In the case of
    Atari frames, such a batch could cause a GPU memory error. In the next step, we
    calculate the logarithm of the probability of the actions taken. This value will
    be used as π[𝜃[old]] in the objective of PPO. Additionally, we normalize the advantage’s
    mean and variance to improve the training stability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The two subsequent lines drop the last entry from the trajectory to reflect
    the fact that our advantages and reference values are one step shorter than the
    trajectory length (as we shifted values in the loop inside the calc_adv_ref function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When all the preparations have been done, we perform several epochs of training
    on our trajectory. For every batch, we extract the portions from the corresponding
    arrays and do the critic and the actor training separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the critic, all we need to do is calculate the mean squared error
    (MSE) loss with the reference values calculated beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the actor training, we minimize the negated clipped objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq61.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To achieve this, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After being trained in both our test environments, the PPO method has shown
    much faster convergence than the A2C method. On HalfCheetah using PyBullet, PPO
    reached an average training reward of 1,800 and 2,500 during the testing after
    8 hours of training and 25M training steps. A2C got lower results after 110M steps
    and 20 hours. Figure [16.6](#x1-298002r6) shows the comparison plots.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: But on HalfCheetah using MuJoCo, the situation is the opposite – PPO growth
    was much slower, and I stopped it after 50M training steps (12 hours). Figure [16.7](#x1-298003r7)
    shows the plots.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: After checking the video of the model (links are provided later in this section),
    we might guess the reason for the low score – our agent learned how to flip the
    cheetah on its back and move forward in this position. During training, it wasn’t
    able to get from this suboptimal “local maximum.” Most likely, running the training
    several times might yield a better policy. Another approach to solving this might
    be to optimize hyperparameters. Again, this is something you can try experimenting
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Ant environment, PPO was better on both PyBullet and MuJoco and was
    able to reach the same level of reward almost twice as fast as A2C. This comparison
    is shown in the plots in Figure [16.8](#x1-298005r8) and Figure [16.9](#x1-298006r9):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, you can use the 02_play.py utility to benchmark saved models and
    record videos of the learned policy in action. This is the list of the best models
    for my training experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HalfCheetah on PyBullet (score 2,567): [https://youtu.be/Rai-smyfyeE](https://youtu.be/Rai-smyfyeE).
    The agent learned how to do long jumps with the back leg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HalfCheetah on MuJoCo (score 1,623): [https://youtube.com/shorts/VcyzNtbVzd4](https://youtube.com/shorts/VcyzNtbVzd4).
    Quite a funny video: the cheetah flips on its back and moves forward this way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on PyBullet (score 2,560): [https://youtu.be/8lty_Mdjnfs](https://youtu.be/8lty_Mdjnfs).
    The Ant policy is much better than A2C – it steadily moves forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on MuJoCo (score 5,108): [https://youtube.com/shorts/AcXxH2f_KWs](https://youtube.com/shorts/AcXxH2f_KWs).
    This model is much faster; most likely, the weight of the ant in the MuJoCo model
    is lower than in PyBullet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TRPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TRPO was proposed in 2015 by Berkeley researchers in a paper by Schulman et
    al., called Trust region policy optimization [[Sch15](#)]. This paper was a step
    towards improving the stability and consistency of stochastic policy gradient
    optimization and has shown good results on various control tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the paper and the method are quite math-heavy, so it can be hard
    to understand the details. The same could be said about the implementation, which
    uses the conjugate gradients method to efficiently solve the constrained optimization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first step, the TRPO method defines the discounted visitation frequencies
    of the state as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq63.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, P(s[i] = s) equals the sampled probability of state s to be
    met at position i of the sampled trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: Then, TRPO defines the optimization objective as
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq64.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq65.png)'
  prefs: []
  type: TYPE_IMG
- en: is the expected discounted reward of the policy and π̃ = arg max[a]A[π](s,a)
    defines the deterministic policy. To address the issue of large policy updates,
    TRPO defines the additional constraint on the policy update, which is expressed
    as the maximum KL divergence between the old and the new policies, which could
    be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a reminder, KL divergence measures the similarity between probability distributions
    and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq67.png)'
  prefs: []
  type: TYPE_IMG
- en: We met KL divergence in Chapter [4](ch008.xhtml#x1-740004) and Chapter [11](ch015.xhtml#x1-18200011).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the TRPO implementations available on GitHub, or in other open source
    repositories, are very similar to each other, probably because all of them grew
    from the original John Schulman TRPO implementation here: [https://github.com/joschu/modular_rl](https://github.com/joschu/modular_rl).
    My version of TRPO is also not very different and uses the core functions that
    implement the conjugate gradient method (used by TRPO to solve the constrained
    optimization problem) from this repository: [https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete example is in 03_train_trpo.py and lib/trpo.py, and the training
    loop is very similar to the PPO example: we sample the trajectory of transitions
    of the predefined length and calculate the advantage estimation using the smoothed
    formula discussed in the PPO section (historically, this estimator was proposed
    first in the TRPO paper.) Next, we do one training step of the critic using MSE
    loss with the calculated reference value, and one step of the TRPO update, which
    consists of finding the direction we should go in by using the conjugate gradients
    method and doing a linear search in this direction to find a step that preserves
    the desired KL divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the piece of the training loop that carries out both those
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the TRPO step, we need to provide two functions: the first will
    calculate the loss of the current actor policy, which uses the same ratio as the
    PPO of the new and the old policies multiplied by the advantage estimation. The
    second function has to calculate KL divergence between the old and the current
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In other words, the PPO method is TRPO that uses the simple clipping of the
    policy ratio to limit the policy update, instead of the complicated conjugate
    gradients and line search.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TRPO in the HalfCheetah environment was able to reach better rewards than PPO
    and A2C. In Figure [16.10](#x1-301002r10), the results from PyBullet training
    is shown. On MuJoCo, the results are even more impressive – the best reward was
    over 5,000\. The plots for MuJoCo are shown in Figure [16.11](#x1-301003r11):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: The reward during training (left) and test reward (right) for
    HalfCheetah on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, the Ant environment shows much less stable convergence. The
    plots shown in Figure [16.12](#x1-301005r12) and Figure [16.13](#x1-301006r13)
    compare the train and test rewards on A2C and TRPO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video recordings of the best actions could be done in the same way as before.
    Here are some videos for the best TRPO models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HalfCheetah on PyBullet (score 2,419): [https://youtu.be/NIfkt2lVT74](https://youtu.be/NIfkt2lVT74).
    Front leg joints are not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HalfCheetah on MuJoCo (score 5,753): [https://youtube.com/shorts/FLM2t-XWDLc?feature=share](https://youtube.com/shorts/FLM2t-XWDLc?feature=share).
    This is a really fast Cheetah!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on PyBullet (score 834): [https://youtu.be/Ny1WBPVluNQ](https://youtu.be/Ny1WBPVluNQ).
    The training got stuck in a “stand still” local minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on MuJoCo (score 993): [https://youtube.com/shorts/9sybZGvXQFs](https://youtube.com/shorts/9sybZGvXQFs).
    The same as PyBullet – the agent just stands still and does not move anywhere.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ACKTR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third method that we will compare, ACKTR, uses a different approach to address
    SGD stability. In the paper by Wu et al. called Scalable trust-region method for
    deep reinforcement learning using Kronecker-factored approximation, published
    in 2017 [[Wu+17](#)], the authors combined the second-order optimization methods
    and trust region approach.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the second-order methods is to improve the traditional SGD by taking
    the second-order derivatives of the optimized function (in other words, its curvature)
    to improve the convergence of the optimization process. To make things more complicated,
    working with the second-order derivatives usually requires you to build and invert
    a Hessian matrix, which can be prohibitively large, so the practical methods typically
    approximate it in some way. This area is currently very active in research because
    developing robust, scalable optimization methods is very important for the whole
    machine learning domain.
  prefs: []
  type: TYPE_NORMAL
- en: One of the second-order methods is called Kronecker-factored approximate curvature
    (K-FAC), which was proposed by James Martens and Roger Grosse in their paper Optimizing
    neural networks with Kronecker-factored approximate curvature, published in 2015
    [[MG15](#)]. However, a detailed description of this method is well beyond the
    scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are not very many implementations of this method available, and none of
    them are part of PyTorch (unfortunately). As far as I know, there are two versions
    of the K-FAC optimizer that work with PyTorch; one from Ilya Kostrikov ([https://github.com/ikostrikov/pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr))
    and one from Nicholas Gao ([https://github.com/n-gao/pytorch-kfac](https://github.com/n-gao/pytorch-kfac)).
    I’ve experimented only with the first one; you can give the second one a try.
    There is a version of K-FAC available for TensorFlow, which comes with OpenAI
    Baselines, but porting and testing it on PyTorch can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: For my experiments, I’ve taken the K-FAC implementation from Kostrikov and adapted
    it to the existing code, which required replacing the optimizer and doing an extra
    backward() call to gather Fisher information. The critic was trained in the same
    way as in A2C. The complete example is in 05_train_acktr.py and is not shown here,
    as it’s basically the same as A2C. The only difference is that a different optimizer
    was used.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, the ACKTR method was very unstable in both environments and physics
    engines. It could be due to a lack of fine-tuning of hyperparameters or some bugs
    in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The results of experiments on HalfCheetah are shown in Figure [16.14](#x1-304002r14)
    and Figure [16.15](#x1-304003r15).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: The reward during training (left) and test reward (right) for
    HalfCheetah on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: In the Ant environment, the ACKTR method shows bad results on PyBullet and no
    reward improvements compared to training on MuJoCo. Figure [16.16](#x1-304005r16)
    shows plots for PyBullet.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.16: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: SAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the final section, we will check our environments on a relatively new method
    called SAC, which was proposed by a group of Berkeley researchers and introduced
    in the paper Soft actor-critic: Off-policy maximum entropy deep reinforcement
    learning, by Haarnoja et al., published in 2018 [[Haa+18](#)].'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, it’s considered to be one of the best methods for continuous
    control problems and is very widely used. The core idea of the method is closer
    to the DDPG method than to A2C policy gradients. We will compare it directly with
    PPO’s performance, which has been considered to be the standard in continuous
    control problems for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The central idea of the SAC method is entropy regularization, which adds a
    bonus reward at each timestamp that is proportional to the entropy of the policy
    at this timestamp. In mathematical notation, the policy we’re looking for is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq68.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, H(P) = 𝔼 [x∼P] [−log P(x)] is the entropy of distribution P. In other
    words, we give the agent a bonus for getting into situations where the entropy
    is at its maximum, which is very similar to the advanced exploration methods covered
    in Chapter [18](ch022.xhtml#x1-32800018). In addition, the SAC method incorporates
    the clipped double-Q trick, where, in addition to the value function, we learn
    two networks predicting Q-values, and choose the minimum of them for Bellman approximation.
    According to researchers, this helps with dealing with Q-value overestimation
    during training. This problem was discussed in Chapter [8](ch012.xhtml#x1-1240008),
    but was addressed differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in total, we train four networks: the policy, π(s), value, V (s,a) and
    two Q-networks, Q[1](s,a) and Q[2](s,a). For the value network, V (s,a), the target
    network is used. So, in summary, SAC training looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q-networks are trained using the MSE objective by doing Bellman approximation
    using the target value network: y[q](r,s′) = r+γV [tgt](s′) (for non-terminating
    steps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The V-network is trained using the MSE objective with the following target,
    y[v](s) = min[i=1,2]Q[i](s,ã) −α log π[𝜃](ã|s), where ã is sampled from policy
    π[𝜃](⋅|s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy network, π[𝜃], is trained in DDPG style by maximizing the following
    objective, Q[1](s,ã[𝜃](s)) −α log π[𝜃](ã[𝜃](s)|s), where ã[𝜃] is a sample from
    π[𝜃](⋅|s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation of the SAC method is in 06_train_sac.py. The model consists
    of the following networks, defined in lib/model.py:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ModelActor: This is the same policy that we used in the previous examples in
    this chapter. As the policy variance is not parametrized by the state (the logstd
    field is not a network, but just a tensor), the training objective does not 100%
    comply with SAC. On the one hand, it might influence the convergence and performance,
    as the core idea of the SAC method is entropy regularization, which can’t be implemented
    without parametrized variance. On the other hand, it decreases the number of parameters
    in the model. If you’re curious, you can extend the example with the parametrized
    variance of the policy and implement a proper SAC method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ModelCritic: This is the same value network as in the previous examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ModelSACTwinQ: These two networks take the state and action as the input and
    predict Q-values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first function implementing the method is unpack_batch_sac(), and it is
    defined in lib/common.py. Its goal is to take the batch of trajectory steps and
    calculate target values for V-networks and twin Q-networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first step of the function uses the already defined unpack_batch_a2c() method,
    which unpacks the batch, converts states and actions into tensors, and calculates
    the reference for Q-networks using Bellman approximation. Once this is done, we
    need to calculate the reference for the V-network from the minimum of the twin
    Q-values minus the scaled entropy coefficient. The entropy is calculated from
    our current policy network. As was already mentioned, our policy has the parametrized
    mean value, but the variance is global and doesn’t depend on the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main training loop, we use the function defined previously and do three
    different optimization steps: for V, for Q, and for the policy. The following
    is the relevant part of the training loop defined in 06_train_sac.py:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the beginning, we unpack the batch to get the tensors and targets for the
    Q- and V-networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The twin Q-networks are optimized by the same target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The critic network is also optimized with the trivial MSE objective using the
    already calculated target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we optimize the actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In comparison with the formulas given previously, the code is missing the entropy
    regularization term and corresponds to DDPG training. As our variance doesn’t
    depend on the state, it can be omitted from the optimization objective.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I ran SAC training in the HalfCheetah and Ant environments for 9-13 hours, with
    5M observations. The results are a bit contradictory. On the one hand, the sample
    efficiency and reward growing dynamics of SAC were better than the PPO method.
    For example, SAC was able to reach a reward of 900 after just 0.5M observations
    on HalfCheetah. PPO required more than 1M observations to reach the same policy.
    In the MuJoCo environment, SAC was able to find the policy that got a reward of
    7,063, which is an absolute record (demonstrating state-of-the-art performance
    on this environment).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, due to the off-policy nature of SAC, the training speed was
    much slower, as we did more calculations than with on-policy methods. On my machine,
    5M frames on HalfCheetah took 10 hours. As a reminder, A2C did 50M observations
    in the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This demonstrates the trade-offs between on-policy and off-policy methods,
    as you have seen many times in this book so far: if your environment is fast and
    observations are cheap to obtain, an on-policy method like PPO might be the best
    choice. But if your observations are hard to obtain, off-policy methods will do
    a better job, but require more calculations to be performed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [16.17](#x1-307002r17) and Figure [16.18](#x1-307003r18) show the reward
    dynamics on HalfCheetah:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.17: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results in the Ant environment are much worse – according to the score,
    the learned policy can barely stand. The PyBullet plots are shown in Figure [16.19](#x1-307005r19);
    MuJoCo plots are shown in Figure [16.20](#x1-307006r20):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_16_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.20: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the videos for the best SAC models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HalfCheetah on PyBullet (score 1,765): [https://youtu.be/80afu9OzQ5s](https://youtu.be/80afu9OzQ5s).
    Our creature is a bit clumsy here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HalfCheetah on MuJoCo (score 7,063): [https://youtube.com/shorts/0Ywn3LTJxxs](https://youtube.com/shorts/0Ywn3LTJxxs).
    This result is really impressive – a super-fast Cheetah.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ant on PyBullet (score 630): [https://youtu.be/WHqXJ3VqX4k](https://youtu.be/WHqXJ3VqX4k).
    After a couple of steps, the ant got stuck for some reason.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To simplify the comparison of the methods, I put all the numbers related to
    the best rewards obtained in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | HalfCheetah | Ant |'
  prefs: []
  type: TYPE_TB
- en: '|  | PyBullet | MuJoCo | PyBullet | MuJoCo |'
  prefs: []
  type: TYPE_TB
- en: '| A2C | 2,189 | 4,718 | 2,425 | 5,380 |'
  prefs: []
  type: TYPE_TB
- en: '| PPO | 2,567 | 1,623 | 2,560 | 5,108 |'
  prefs: []
  type: TYPE_TB
- en: '| TRPO | 2,419 | 5,753 | 834 | 993 |'
  prefs: []
  type: TYPE_TB
- en: '| ACKTR | 250 | 3,100 | 1,820 | — |'
  prefs: []
  type: TYPE_TB
- en: '| SAC | 1,765 | 7,063 | 630 | — |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16.1: Summary table'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is no single winning method – some do well in some environments
    but get worse results in others. In principle, we can call A2C and PPO as quite
    consistent methods because they’re getting good results everywhere (PPO’s “backflip
    cheetah” on MuJoCo could be attributed to a bad starting seed, so rerunning the
    training might lead to a better policy).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we checked three different methods with the aim of improving
    the stability of the stochastic policy gradient and compared them to the A2C implementation
    on two continuous control problems. Along with the methods covered in the previous
    chapter (DDPG and D4PG), these methods are basic tools to work with a continuous
    control domain. Finally, we checked a relatively new off-policy method that is
    an extension of DDPG: SAC. Here, we have just scratched the surface of this topic,
    but it could be a good starting point to dive into it in more depth. These methods
    are widely used in robotics and related areas.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will switch to a different set of RL methods that have
    been becoming popular recently: black-box or gradient-free methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
