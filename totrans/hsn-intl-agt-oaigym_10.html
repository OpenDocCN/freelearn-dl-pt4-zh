<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring the Learning Algorithm Landscape - DDPG (Actor-Critic), PPO (Policy-Gradient), Rainbow (Value-Based)</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter, we looked at several promising learning environments that you can use to train agents to solve a variety of different tasks. In <a href="part0131.html#3STPM0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 7</a>, <em class="calibre13">Creating Custom OpenAI Gym Environments – CARLA Driving Simulator</em>, we also saw how you can create your own environments to solve the task or problem that you may be interested in developing a solution for, using intelligent and autonomous software agents. That provides you with directions on where you can head after finishing in order to explore and play around with all the environments, tasks, and problems we discussed in this book. Along the same lines, in this chapter, we will discuss several promising learning algorithms that serve as future references for your intelligent agent development endeavors. </p>
<p class="calibre2"><span class="calibre5">So far in this book, we have gone through the step-by-step process of implementing intelligent agents that can learn to improve and solve discrete decision making/control problems (<a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>) and continuous action/control problems (<a href="part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 8</a>, <em class="calibre13">Implementing an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm</em>). They served as good starting points in the development of such learning agents. Hopefully, the previous chapters gave you a holistic picture of an autonomous intelligent software agent/system that can learn to improve given the task or problem at hand. We also looked at the overall pipeline with useful utilities and routines (such as logging, visualization, parameter management, and so on) that help when developing, training, and testing such complex systems. We saw two main classes of algorithms: deep Q-learning (and its extensions) and deep actor-critic (and their extensions)-based deep reinforcement learning algorithms. They are good baseline algorithms and in fact are still referenced in state-of-the art research papers in this area. This area of research has been under active development in recent years, and several new algorithms have been proposed. Some have better sample complexity, which is the number of samples the agent collects from the environment before it reaches a certain level of performance. Some other algorithms have stable learning characteristics and find optimal policies, given enough time, for most problems with little or no tuning. Several new architectures, such as IMPALA and Ape-X, have also been introduced and enable highly scaleable learning algorithm implementations. </span></p>
<p class="calibre2">We will have a quick look at these promising algorithms, their advantages, and their potential application types. We will also look at code examples for the key components that these algorithms add to what we already know. Sample implementations of these algorithms are available in this book's code repository under the <kbd class="calibre12">ch10</kbd> folder at <a href="https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym" class="calibre9">https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep Deterministic Policy Gradients</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre4">Deep Deterministic Policy Gradient</strong> (<strong class="calibre4">DDPG</strong>) is an off-policy, model-free, actor-critic algorithm and is based on the <strong class="calibre4">Deterministic Policy Gradient</strong> (<strong class="calibre4">DPG</strong>) theorem (<a href="http://proceedings.mlr.press/v32/silver14.pdf" class="calibre9">proceedings.mlr.press/v32/silver14.pdf</a>). <span class="calibre5">Unlike the deep Q-learning-based methods, actor-critic policy gradient-based methods are easily applicable to continuous action spaces, in addition to problems/tasks with discrete action spaces.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Core concepts</h1>
                
            
            <article>
                
<p class="calibre2">In <a href="part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 8</a>, <em class="calibre13">Implementing an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm</em>, we walked you through the derivation of the policy gradient theorem and reproduced the following for bringing in context:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation158" src="../images/00315.jpeg"/></div>
<p class="calibre2">You may recall that the policy we considered was a stochastic function that assigned a probability to each action given the <strong class="calibre4">state</strong> (<strong class="calibre4">s</strong>) and the parameters (<img class="fm-editor-equation156" src="../images/00316.jpeg"/>). In deterministic policy gradients, the stochastic policy is replaced by a deterministic policy that prescribes a fixed policy for a given state and set of parameters <img class="fm-editor-equation156" src="../images/00317.jpeg"/><span class="calibre5">. In short, DPG can be represented using the following two equations:</span></p>
<p class="mce-root1">This is the policy objective function:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation159" src="../images/00318.jpeg"/></div>
<p class="calibre2">Here, <img class="fm-editor-equation160" src="../images/00319.jpeg"/> is the deterministic policy parametrized by <img class="fm-editor-equation156" src="../images/00320.jpeg"/>, r(s,a) is the reward function for taking action <em class="calibre13">a</em> in state s, and <img class="fm-editor-equation161" src="../images/00321.jpeg"/> is the discounted state distribution under the policy.</p>
<p class="calibre2">The gradient of the deterministic policy objective function is proven (in the paper linked before) to be:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation162" src="../images/00322.jpeg"/></div>
<p class="calibre2">We now see the familiar action-value function term, which we typically call the critic. DDPG builds on this result and uses a deep neural network to represent the action-value function, like we did in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, along with a few other modifications to stabilize the training. Specifically, a Q-target network is used (like what we discussed in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>), but now this target network is slowly updated rather than keeping it fixed for a few update steps and then updating it. DDPG also uses the experience replay buffer and uses a noisy version of <img class="fm-editor-equation163" src="../images/00323.jpeg"/>, represented using the equation <img class="fm-editor-equation164" src="../images/00324.jpeg"/>, to encourage exploration as the policy. <img class="fm-editor-equation163" src="../images/00325.jpeg"/> is deterministic.</p>
<div class="packt_infobox">There is an extension to DDPG called D4PG, short for Distributed Distributional DDPG. I can guess what you might be thinking: DPG -&gt; DDPG -&gt; {Missing?}-&gt; DDDDPG. Yes! The missing item is for you to implement.<br class="calibre42"/>
<br class="calibre42"/>
The D4PG algorithm applies four main improvements to the DDPG algorithm, which are listed here briefly if you are interested:<br class="calibre42"/>
<ul class="calibre150">
<li class="calibre151">Distributional critic (the critic now estimates a distribution for Q-values rather than a single Q-value for a given state and action)</li>
</ul>
<ul class="calibre150">
<li class="calibre151">N-step returns (similar to what we used in <a href="part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre45">Chapter 8</a>, <em class="calibre46">Implementing an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm</em>, n-step TD returns are used instead of the usual 1-step return)</li>
</ul>
<ul class="calibre150">
<li class="calibre151">Prioritized experience replay (this is used to sample experiences from the experience replay memory)</li>
</ul>
<ul class="calibre150">
<li class="calibre151">Distributed parallel actors (utilizes K independent actors, gathering experience in parallel and populating the experience replay memory)</li>
</ul>
</div>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Proximal Policy Optimization</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre4">Proximal Policy Optimization</strong> (<strong class="calibre4">PPO</strong>) is a policy gradient-based method and is one of the algorithms that have been proven to be stable as well as scalable. In fact, PPO was the algorithm used by the OpenAI Five team of agents that played (and won) against several human DOTA II players, which we discussed in our previous chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Core concept</h1>
                
            
            <article>
                
<p class="calibre2">In policy gradient methods, the algorithm performs rollouts to collect samples of transitions and (potentially) rewards, and updates the parameters of the policy using gradient descent to minimize the objective function. The idea is to keep updating the parameters to improve the policy until a good policy is obtained. To improve the training stability, the <strong class="calibre4">Trust Region Policy Optimization</strong> (<strong class="calibre4">TRPO</strong>) algorithm enforces a <span class="calibre5"><strong class="calibre4">Kullback-Liebler</strong> (</span><strong class="calibre4">KL</strong>) divergence constraint on the policy updates, so that the policy is not updated too much in one step when compared to the old policy. TRPO was the precursor to the PPO algorithm. Let's briefly discuss the objective function used in the TRPO algorithm in order to get a better understanding of PPO.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Off-policy learning</h1>
                
            
            <article>
                
<p class="calibre2">As we know, in the case of off-policy learning, the agent follows a behavioral policy that is different from the policy that the agent is trying to optimize. Just to remind you, Q-learning, which we discussed in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, along with several extensions, is also an off-policy algorithm. Let's denote the behavior policy using <img class="fm-editor-equation165" src="../images/00326.jpeg"/>. Then, we can write the objective function of the agent to be the total advantage over the state-visitation distribution and actions given by the following:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation166" src="../images/00327.jpeg"/></div>
<p class="calibre2">Here, <img class="fm-editor-equation167" src="../images/00328.jpeg"/> is the policy parameters before the update and <img class="fm-editor-equation168" src="../images/00329.jpeg"/> is the state visitation probability distribution under the old policy parameters. <span class="calibre5">We can multiply and divide the terms in the inner summation by the behavior policy </span><img class="fm-editor-equation169" src="../images/00330.jpeg"/><span class="calibre5">, with the idea being the use of importance sampling to account for the fact that the</span><span class="calibre5"> transitions are sampled using the behavior policy </span><img class="fm-editor-equation169" src="../images/00331.jpeg"/><span class="calibre5">:</span></p>
<div class="cdpaligncenter"><img class="fm-editor-equation170" src="../images/00332.jpeg"/></div>
<div class="packt_infobox"><span class="packt_screen">The changed terms in the preceding equation compared to the previous equation are shown in red.</span></div>
<p class="calibre2">We can write the previous summations over a distribution as an expectation, like so:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation171" src="../images/00333.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">On-policy</h1>
                
            
            <article>
                
<p class="calibre2">In the case of on-policy learning, the behavior policy and the target policy for the agent are one and the same. So, naturally the current policy (before the update) that the agent is using to collect samples is going to be <img class="fm-editor-equation172" src="../images/00334.jpeg"/>, which is the behavior policy, and therefore the objective function becomes this:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation173" src="../images/00335.jpeg"/></div>
<div class="packt_infobox">The changed terms in the preceding equation compared to the previous equation are shown in red.</div>
<p class="calibre2">TRPO optimizes the previous object function with a <em class="calibre13">trust region</em> constraint, which using the KL divergence metric given by the following equation:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation174" src="../images/00336.jpeg"/></div>
<p class="calibre2">This is the constraint that makes sure that the new update to the policy is not diverging too much from the current policy. Although the idea behind TRPO was neat and intuitively simple, the implementation and gradient updates involved complexities. PPO simplifies the approach using a clipped surrogate objective that was effective and simple as well. Let's get a deeper understanding of the core concepts behind PPO using the math behind the algorithm. Let the probability ratio of taking action <em class="calibre13">a</em> given state <em class="calibre13">s</em> between the new policy <img class="fm-editor-equation155" src="../images/00337.jpeg"/> and the old policy <img class="fm-editor-equation172" src="../images/00338.jpeg"/> be defined as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation175" src="../images/00339.jpeg"/></div>
<p class="calibre2">Substituting this into the on-policy objective function equation of TRPO that we discussed earlier results in the objective function:</p>
<p class="calibre2"> </p>
<div class="cdpaligncenter"><img class="fm-editor-equation176" src="../images/00340.jpeg"/></div>
<p class="calibre2">Simply removing the KL divergence constraint will result in instability, due to the large number of parameter updates that may result. PPO imposes the constraint by forcing <img class="fm-editor-equation177" src="../images/00341.jpeg"/> to lie within the interval <img class="fm-editor-equation178" src="../images/00342.jpeg"/>, where <img class="fm-editor-equation59" src="../images/00343.jpeg"/> is a tunable hyperparameter. Effectively, the objective function used in PPO takes the minimum value between the original parameter values and the clipped version, which can mathematically be described as shown here:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation179" src="../images/00344.jpeg"/></div>
<p class="calibre2">This results in a stable learning objective with monotonically improving policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Rainbow </h1>
                
            
            <article>
                
<p class="calibre2">Rainbow (<a href="https://arxiv.org/abs/1710.02298" class="calibre9">https://arxiv.org/pdf/1710.02298.pdf</a>) is an off-policy deep reinforcement learning algorithm based on DQN. We looked at and implemented deep Q-learning (DQN) and some of the extensions to DQN in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>. There have been several more extensions and improvements to the DQN algorithm. Rainbow combines six of those extensions and shows that the combination works much better. Rainbow is a state-of-the art algorithm that currently holds the record for the highest score on all Atari games. If you are wondering why the algorithm is named <em class="calibre13">Rainbow</em>, it is most probably due to the fact that it combines seven (the number of colors in a rainbow) extensions to the Q-learning algorithm, namely:</p>
<ul class="calibre10">
<li class="calibre11">DQN</li>
<li class="calibre11">Double Q-Learning</li>
<li class="calibre11">Prioritized experience replay</li>
<li class="calibre11">Dueling networks</li>
<li class="calibre11">Multi-step learning/n-step learning</li>
<li class="calibre11">Distributional RL</li>
<li class="calibre11">Noisy nets</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Core concept</h1>
                
            
            <article>
                
<p class="calibre2">Rainbow combines DQN with six selected extensions that were shown to address the limitations of the original DQN algorithm. We will briefly look at the six extensions to understand how they contributed to the overall performance boost and landed Rainbow in the top spot on the Atari benchmark, and also how they proved to be successful in the OpenAI Retro contest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">DQN</h1>
                
            
            <article>
                
<p class="calibre2">By now, you should be very familiar with DQN, as we went through the step-by-step implementation of a deep Q-learning agent in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, where we discussed <span class="calibre5">DQN</span><span class="calibre5"> in detail and how it extends standard Q-learning with a deep neural network function approximation, replay memory, and a target network. Let's recall the Q-learning loss that we used in the deep Q-learning agent in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>:</span></p>
<p class="calibre2"> </p>
<div class="cdpaligncenter"><img class="fm-editor-equation180" src="../images/00345.jpeg"/></div>
<p class="calibre2">This is basically the mean squared error between the TD target and DQN's Q-estimate, as we noted in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, where <img class="fm-editor-equation181" src="../images/00346.jpeg"/> is the slow-moving target network and <img class="fm-editor-equation182" src="../images/00347.jpeg"/> is the main Q network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Double Q-Learning</h1>
                
            
            <article>
                
<p class="calibre2">In Double Q-Learning, there are two action-value/Q functions. Let's call them Q1 and Q2. The idea in double Q-learning is to <em class="calibre13">decouple action selection from the value estimation</em>. That is, when we want to update Q1, we select the best action according to Q1, but use Q2 to find the value of the selected action. Similarly, when Q2 is being updated, we select the action based on Q2, but use Q1 to determine the value of the selected action. In practice, we can use the main Q network <img class="fm-editor-equation182" src="../images/00348.jpeg"/> as Q1 and the slow-moving target network <img class="fm-editor-equation181" src="../images/00349.jpeg"/> as Q2, which gives us the following Double Q-Learning loss equation (the differences from the DQN equation are shown in red):</p>
<div class="cdpaligncenter"><img class="fm-editor-equation183" src="../images/00350.jpeg"/></div>
<p class="calibre2">The motivation behind this change in the loss function is that Q-learning is affected by overestimation bias, and this can harm learning. The overestimation is <span class="calibre5">due to the fact that the expectation of a maximum is greater than or equal to the maximum of the expectation (often the inequality is the one that holds) which arises due to the maximization step in the Q-learning algorithm and DQN</span><span class="calibre5">. The change introduced by double Q-learning was shown to reduce overestimations that were harmful to the learning process, thereby improving performance over DQN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Prioritized experience replay</h1>
                
            
            <article>
                
<p class="calibre2">When we implemented deep Q-learning in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, we used an experience replay memory to store and retrieve sampled transition experience. In our implementation, and in the DQN algorithm, the experiences from the replay memory buffer are sampled uniformly. Intuitively, we would want to sample those experiences more frequently, as there is much to learn. Prioritized experience replay samples transition with probability <img class="fm-editor-equation184" src="../images/00351.jpeg"/> relative to the last encountered absolute TD error, given by the following equation:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation185" src="../images/00352.jpeg"/></div>
<p class="calibre2">Here, <img class="fm-editor-equation19" src="../images/00353.jpeg"/> is a hyperparameter that determines the shape of the distribution. This makes sure that we sample those transitions in which the predictions of the Q-values were more different from the correct values. In practice, new transitions are inserted into the replay memory with maximum priority to signify the importance of recent transition experiences.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Dueling networks</h1>
                
            
            <article>
                
<p class="calibre2">Dueling networks is a neural network architecture designed for value-based reinforcement learning. The name <em class="calibre13">dueling</em> stems from the main feature of this architecture, which is that there are two streams of computations, one for the value function and the other for the advantage. The following diagram, from a research paper (<a href="https://arxiv.org/pdf/1511.06581.pdf" class="calibre9">https://arxiv.org/pdf/1511.06581.pdf</a>), shows the comparison of the dueling network architecture (the network shown at the bottom of the diagram) with the typical DQN architecture (shown at the top of the diagram):</p>
<div class="cdpaligncenter"><img src="../images/00354.jpeg" class="calibre152"/></div>
<p class="calibre2">The convolutional layers that encode features are shared by both the value and advantage streams, and are merged by a special aggregation function, as discussed in the paper that corresponds to the following factorization of the action values:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation186" src="../images/00355.jpeg"/></div>
<p class="calibre2"><img class="fm-editor-equation187" src="../images/00356.jpeg"/>, and <img class="fm-editor-equation188" src="../images/00357.jpeg"/> are, respectively, the parameters of the value stream, the shared convolutional encoder, and the advantage stream, and <img class="fm-editor-equation178" src="../images/00358.jpeg"/> is their concatenation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Multi-step learning/n-step learning</h1>
                
            
            <article>
                
<p class="calibre2">In <a href="part0151.html#4G04U0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 8</a>, <em class="calibre13">Implementing an Intelligent Autonomous Car Driving Agent Using the Deep Actor-Critic algorithm</em>, we implemented the n-step return TD return method and discussed how forward-view multi-step targets can be used in place of a single/one-step TD target. We can use that n-step return with DQN, and that is essentially the idea behind this extension. Recall that the truncated n-step return from state <img class="fm-editor-equation189" src="../images/00359.jpeg"/> is given as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation190" src="../images/00360.jpeg"/></div>
<p class="calibre2">Using this equation, a multi-step variant of DQN can be defined to minimize the following loss (the differences from the DQN equation are shown in red):</p>
<div class="cdpaligncenter"><img class="fm-editor-equation191" src="../images/00361.jpeg"/></div>
<p class="calibre2">This equation shows the change introduced to DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Distributional RL</h1>
                
            
            <article>
                
<p class="calibre2">The distributional RL method (<a href="https://arxiv.org/abs/1707.06887" class="calibre9">https://arxiv.org/abs/1707.06887</a>) is about learning to approximate the distribution of returns rather than the expected (average) return. The distributional RL method proposes the use of probability masses placed on a discrete support to model such distributions. This, in essence, means that rather than trying to model one action-value given the state, a distribution of action-values for each action given the state is sought. Without going too much into the details (as that would require a lot of background information), we will look at one of the key contributions of this method to RL in general, which is the formulation of the Distributional Bellman equation. As you may recall from the previous chapters of this book, the action-value function, using a one-step Bellman backup for it, can be returned as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation192" src="../images/00362.jpeg"/></div>
<p class="calibre2">In the case of Distributional Bellman equations, the scalar quantity <img class="fm-editor-equation193" src="../images/00363.jpeg"/> is replaced by a random variable <img class="fm-editor-equation194" src="../images/00364.jpeg"/>, which gives us the following equation:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation195" src="../images/00365.jpeg"/></div>
<p class="calibre2">Because the quantity is no longer a scalar, the update equation needs to be dealt with more car than just adding the discounted value of the next state-action-value to the step-return. The update step of the distributional bellman equation can be understood easily with the help of the following diagram (stages from left to right):</p>
<div class="cdpaligncenter"><img src="../images/00366.jpeg" class="calibre153"/></div>
<p class="calibre2">In the previous illustration, the distribution of the next state action-value is depicted in red on the left, which is then scaled by the discount factor <img class="fm-editor-equation196" src="../images/00367.jpeg"/> (middle), and finally the distribution is shifted by <img class="fm-editor-equation59" src="../images/00368.jpeg"/> to yield the Distributional Bellman update. After the update, the target distribution <img class="fm-editor-equation197" src="../images/00369.jpeg"/>that results from the previous update operation is projected onto the supports of the current distribution <img class="fm-editor-equation198" src="../images/00370.jpeg"/> by minimizing the cross entropy loss between <img class="fm-editor-equation198" src="../images/00371.jpeg"/> and <img class="fm-editor-equation197" src="../images/00372.jpeg"/>.</p>
<p class="calibre2">With this background, you can briefly glance through the pseudo code of the C51 algorithm from the Distributional RL paper, which is integrated into the Rainbow agent:</p>
<div class="cdpaligncenter"><img src="../images/00373.jpeg" class="calibre154"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Noisy nets</h1>
                
            
            <article>
                
<p class="calibre2">If you recall, we used an <img class="fm-editor-equation59" src="../images/00374.jpeg"/>-greedy policy for the deep Q-learning agent in <a href="part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 6</a>, <em class="calibre13">Implementing an Intelligent Agent for Optimal Discrete Control Using Deep Q-Learning</em>, to take action based on the action-values learned by the deep Q-network, which basically means taking the action with the highest action-value for a given state most of the time, except when, for some tiny fraction of the time (that is, with a very small probability <img class="fm-editor-equation59" src="../images/00375.jpeg"/>), the agent selects a random action. This may prevent the agent from exploring more reward states, especially if the action-values it has converged to are not the optimal action-values. The limitations of exploring using <img class="fm-editor-equation59" src="../images/00376.jpeg"/>-greedy policies were evident from the performance of the DQN variants and the value-based learning methods in the Atari game Montezuma's Revenge, where a long sequence of actions have to be executed in the right way to collect the first reward. To overcome this exploration limitation, Rainbow uses the idea of noisy nets—which are a simple but effective method proposed in 2017. </p>
<p class="calibre2">The main idea behind noisy nets is a noisy version of the linear neural network layer that combines a deterministic and a noisy stream, as exemplified in the following equation for the case of the linear neural network layer:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation199" src="../images/00377.jpeg"/></div>
<p class="calibre2">Here, <img class="fm-editor-equation200" src="../images/00378.jpeg"/> and <img class="fm-editor-equation201" src="../images/00379.jpeg"/> are the parameters of the noisy layer, which are learned along with the other parameters of the DQN using gradient descent. The <img class="fm-editor-equation202" src="../images/00380.jpeg"/> represents the element-wise product operation and <img class="fm-editor-equation203" src="../images/00381.jpeg"/> and <img class="fm-editor-equation204" src="../images/00382.jpeg"/> are zero-mean random noise. We can use the noisy linear layer in place of the usual linear layer in our DQN implementation, which will have the added advantage of exploring better. Because <img class="fm-editor-equation200" src="../images/00383.jpeg"/><span class="calibre5"> and</span><span class="calibre5"> </span><img class="fm-editor-equation201" src="../images/00384.jpeg"/> are learnable parameters, the network can learn to ignore the noisy stream. Because this happens over time for each of the neurons, the noisy stream decays at different rates in different parts of the state space, allowing better exploration with a form of self-annealing.</p>
<p class="calibre2">The Rainbow agent implementation combines all of these methods to achieve state-of-the art results with better performance than any other method on the Atari suite of 57 games. Overall, the performance of the Rainbow agent against the previous best-performing agent algorithms on the combined benchmark for Atari games is summarized in the following graph from the Rainbow paper:</p>
<div class="cdpaligncenter"><img src="../images/00385.jpeg" class="calibre155"/></div>
<p class="calibre2">From the plot, it is clearly evident that the methods incorporated into the Rainbow agent lead to substantially improved performance across 57 different Atari games.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quick summary of advantages and applications</h1>
                
            
            <article>
                
<p class="calibre2">A few of the key advantages of the Rainbow agent are summarized here for your quick reference:</p>
<ul class="calibre10">
<li class="calibre11">Combines several notable extensions to Q-learning developed over the past several years</li>
<li class="calibre11">Achieves state-of-the art results in the Atari benchmarks</li>
<li class="calibre11">n-step targets with a suitably tuned value for <em class="calibre25">n</em> often leads to faster learning</li>
<li class="calibre11">Unlike other DQN variants, the Rainbow agent can start learning with 40% less frames collected in the experience replay memory</li>
<li class="calibre11">Matches the best performance of DQN in under 10 hours (7 million frames) on a single-GPU machine</li>
</ul>
<p class="calibre2">The Rainbow algorithm has become the most sought after agent for discrete control problems where the action space is small and discrete. It has been very successful with other game environments, such as Gym-Retro, and notably a tweaked version of the Rainbow agent placed second in the OpenAI Retro contest held in 2018, which is a transfer learning contest where the task is to learn to play the retro Genesis console games Sonic The Hedgehog, Sonic The Hedgehog II, and Sonic &amp; Knuckles on some levels, and then be able to play well on other levels that the agent was not trained on. Considering the fact that in typical reinforcement learning settings, agents are trained and tested in the same environment, the retro contest measured the learning algorithm's ability to generalize its learning from previous experience. In general, the Rainbow agent is the best first bet to try on any RL problem/task in a discrete action space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">Being the final chapter of this book, this chapter provided summaries of key learning algorithms that are currently state of the art in this domain. We looked at the core concepts behind three different state-of-the-art algorithms, each with their own unique elements and their own categories (actor-critic/policy based/value-function based).</p>
<p class="calibre2">Specifically, we discussed the deep deterministic policy gradient algorithm, which is an actor-critic architecture method that uses a deterministic policy rather than the usual stochastic policy, and achieves good performance on several continuous control tasks.</p>
<p class="calibre2">We then looked at the PPO algorithm, which is a policy gradient-based method that uses a clipped version of the TRPO objective and learns a monotonically better and stable policy, and has been successfully used in very high-dimensional environments such as DOTA II. </p>
<p class="calibre2">Finally, we looked at the Rainbow algorithm, which is a value-based method and combines several extensions to the very popular Q-learning algorithm, namely DQN, d<span class="calibre5">ouble Q-learning, p</span><span class="calibre5">rioritized experience replay, d</span><span class="calibre5">ueling networks, m</span><span class="calibre5">ulti-step learning/n-step learning, d</span><span class="calibre5">istributional reinforcement learning, and n</span><span class="calibre5">oisy-network layers. The Rainbow agent achieved significantly better performance in the Atari benchmark suite of 57 games and also performed very well in transfer learning tasks in the OpenAI Retro contest.</span></p>
<p class="calibre2">With that, we are into the last paragraph of this book! I hope you enjoyed your journey through the book, learned a lot, and acquired a lot of hands-on skills to implement intelligent agent algorithms and the necessary building blocks to train and test the agents on the learning environment/problem of your choice. You can use the issue-tracking system in the<span class="calibre5"> </span><span class="calibre5">book's code repository</span><span class="calibre5"> to report issues with the code, or if you would like to discuss a topic further, or need any additional references/pointers to move to the next level.</span></p>


            </article>

            
        </section>
    </body></html>