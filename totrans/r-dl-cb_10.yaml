- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the concept of Transfer Learning. The following
    are the topics that will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Illustrating the use of a pretrained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the Transfer Learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an image classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a deep learning model on a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing performance using CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of development has happened within the deep learning domain in recent
    years, to enhance algorithmic efficacy and computational efficiency across different
    domains such as text, images, audio, and video. However, when it comes to training
    on new datasets, machine learning usually rebuilds the model from scratch, as
    is done in traditional data science problem solving. This becomes challenging
    when a new big dataset need to be trained as it will require very high computation
    power a lot of and time to reach the desired model efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning is a mechanism to learn new scenarios from existing models.
    This approach is very useful to train on big datasets, not necessarily from a
    similar domain or problem statement. For example, researchers have shown examples
    of Transfer Learning where they have trained Transfer Learning for completely
    different problem scenarios, such as when a model built using classifications
    of cat and dog is used for classifying objects such as aeroplane vs automobile.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of analogy, it''s more about passing the learned relationship to new
    architecture in order to fine-tune weights. An example of how Transfer Learning
    is used is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of Transfer Learning flow
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows the steps of Transfer Learning, where the weights/architectures
    from a predeveloped deep learning model are reused to predict a new problem statement.
    Transfer Learning helps provide a good starting point for deep learning architectures.
    There are different open source projects going on in different domains, which
    facilitate Transfer Learning, for example, ImageNet ([http://image-net.org/index](http://image-net.org/index))
    is an open source project for image classification where a lot of different architectures
    such as Alexnet, VGG16, and VGG19 have been developed. Similarly, in text mining,
    there is a Word2Vec representation of Google News trained using three billion
    running words.
  prefs: []
  type: TYPE_NORMAL
- en: Details on word2vec can be found at [https://code.google.com/archive/p/word2vec/.](https://code.google.com/archive/p/word2vec/)
  prefs: []
  type: TYPE_NORMAL
- en: Illustrating the use of a pretrained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The current recipe will cover the set-up for using a pretrained model. We will
    use TensorFlow to demonstrate the recipe. The current recipe will use VGG16 architecture
    built using the ImageNet as dataset. The ImageNet is an open source image repository
    of images used for building image recognition algorithms. The database has more
    than 10 millions tagged images and more than 1 million images have bounding box
    to capture objects.
  prefs: []
  type: TYPE_NORMAL
- en: Lot of different deep learning architectures are developed using ImageNet dataset.
    Once of the popular one is VGG networks are convolution neural networks proposed
    by Zisserman and Simonyan (2014) and trained over ImageNet data with 1,000 classes.
    The current recipe will consider VGG16 variant of VGG architecture which is known
    for it's simplicity. The network uses input of 224 x 224 RGB image. The network
    utilizes 13 convolution layers with different width x height x depth. The maximum
    pooling layer is used to reduce volume size. The network uses 5 maxpooling layer.
    The output from convolution layer is passed through 3 fully connected layer. Outcome
    from fully connected layer go through softmax function to evaluate probability
    of 1000 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed architecture of VGG16 is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: VGG16 architecture
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section covers required to use VGG16 pretrained model for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download VGG16 weights from [http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz).
    The file can be downloaded using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Install TensorFlow in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install R and the `tensorflow` package in R.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download a sample image from [http://image-net.org/download-imageurls](http://image-net.org/download-imageurls).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current section provides steps to use pretrained models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `tensorflow` in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the `slim` library from TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `slim` library in TensorFlow is used to maintain complex neural network
    models in terms of definition, training, and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reset graph in TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Define input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Redefine the VGG16 network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function defines the network architecture used for the VGG16
    network. The network can be assigned using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the VGG16 weights `vgg_16_2016_08_28.tar.gz` downloaded in the *Getting
    started* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Download a sample test image. Let''s download an example image from a `testImgURL`
    location as shown in following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script downloads the following image from URL mention in variable
    `testImgURL`. The following is the downloaded image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample image used to evaluate imagenet
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine the class using the VGG16 pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The maximum probability achieved is 0.62 for class 672, which refers to the
    category--mountain bike, all-terrain bike, off-roader--in the VGG16 trained dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Transfer Learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The current recipe will cover Transfer Learning using the CIFAR-10 dataset.
    The previous recipe presented how to use a pretrained model. The current recipe
    will demonstrate how to use a pretrained model for different problem statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use another very good deep learning package, MXNET, to demonstrate
    the concept with another architecture, Inception. To simplify the computation,
    we will reduce the problem complexity from 10 classes to two classes: aeroplane
    and automobile. The recipe focuses on data preparation for Transfer Learning using
    Inception-BN.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section prepares for the upcoming section for setting-up Transfer Learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Download the CIFAR-10 dataset from [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html).
    The `download.cifar.data` function from [Chapter 3](part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Convolution Neural Networks,* can be used to download the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the `imager` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The current part of the recipe will provide a step-by-step guide to prepare
    the dataset for the Inception-BN pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dependent packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the downloaded CIFAR-10 dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the dataset for aeroplane and automobile. This is an optional step and
    is done to reduce complexity later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform to image. This step is required as the CIFAR-10 dataset is a 32 x
    32 x 3 image, which is flattened to a 1024 x 3 format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step involve padding images with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the image to a specified folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script saves the aeroplane images into the `aero` folder and the
    automobile images in the `auto` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert to the recording format `.rec` supported by MXNet. This conversion
    requires `im2rec.py` MXnet module from Python as conversion is not supported in
    R. However, it can be called from R once MXNet is installed in Python using the
    system command. The splitting of the dataset into train and test can be obtained
    using the following file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script will generate two list files: `pks.lst_train.lst` and
    `pks.lst_train.lst`. The splitting of train and validation is controlled by the
    `-train-ratio` parameter in the preceding script. The number of classes is based
    on the number of folders in the `trainf` directory. In this scenario, two classes
    are picked: automotive and aeroplane.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the `*.rec` file for training and validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script will create the `pks.lst_train.rec` and `pks.lst_val.rec`
    files to be used in the next recipe to train the model using a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Building an image classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipe focuses on building an image classification model using Transfer
    Learning. It will utilize the dataset prepared in the previous recipes and use
    the Inception-BN architecture. The BN in Inception-BN stands for **batch normalization**.
    Details of the Inception model in computer vision can be found in Szegedy et al.
    (2015).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section cover's the prerequisite to set-up a classification model using
    INCEPTION-BN pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Convert images into `.rec` file for train and validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the Inception-BN architecture from [http://data.dmlc.ml/models/imagenet/inception-bn/.](http://data.dmlc.ml/models/imagenet/inception-bn/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install R and the `mxnet` package in R.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the `.rec` file as iterators. The following is the function to load the
    `.rec` data as iterators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, `mx.io.ImageRecordIter` reads batches of images from
    the `RecordIO` (`.rec`) files*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Load data using the `data.iterator` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Inception-BN pretrained model from the `Inception-BN` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the layers of the Inception-BN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a new layer to replace the `flatten_output` layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize weights for the newly defined layer. To retrain the last layer,
    weight initialization is performed using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the aforementioned layer, weights are assigned using uniform distribution
    between [*-0.2*, *0.2*]. The `ctx` define the device on which the execution is
    to be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrain the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding model is set to run on CPU with five rounds, using accuracy as
    the evaluation metric. The following screenshot shows the execution of the described
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Output from Inception-BN model, trained using CIFAR-10 dataset
  prefs: []
  type: TYPE_NORMAL
- en: The trained model has produced a training accuracy of 0.97 and a validation
    accuracy of 0.95.
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning model on a GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Graphical processing unit** (**GPU**) is hardware used for rendering images
    using a lot of cores. Pascal is the latest GPU micro architecture released by
    NVIDIA. The presence of hundreds of cores in GPU helps enhance the computation.
    This section provides the recipe for running a deep learning model using GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides dependencies required to run GPU and CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: The experiment performed in this recipe uses GPU hardware such as GTX 1070.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install `mxnet` for GPU. To install `mxnet` for GPU for a specified machine,
    follow the installation instruction from `mxnet.io`. Select the requirement as
    shown in the screenshot, and follow the instructions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Steps to get installation instruction for MXNet
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how you train a deep learning model on a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Inception-BN-transferred learning recipe discussed in the previous section
    can be made to run on the GPU installed and the configured machine by changing
    the device settings as shown in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the aforementioned model, the device setting is changed from `mx.cpu` to
    `mx.gpu`. The CPU for five iteration tools takes ~2 hours of computational effort,
    whereas the same iteration is completed in ~15 min with GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing performance using CPU and GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the questions with device change is why so much improvement is observed
    when the device is switched from CPU to GPU. As the deep learning architecture
    involves a lot of matrix computations, GPUs help expedite these computations using
    a lot of parallel cores, which are usually used for image rendering.
  prefs: []
  type: TYPE_NORMAL
- en: The power of GPU has been utilized by a lot of algorithms to accelerate the
    execution. The following recipe provides some benchmarks of matrix computation
    using the `gpuR` package. The `gpuR` package is a general-purpose package for
    GPU computing in R.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section covers requirement to set-up a comparison between GPU Vs CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Use GPU hardware installed such as GTX 1070.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CUDA toolkit installation using URL [https://developer.nvidia.com/cuda-downloads.](https://developer.nvidia.com/cuda-downloads)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the `gpuR` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Test `gpuR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started by loading the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the package, and set the precision to `float` (by default, the GPU precision
    is set to a single digit):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Matrix assignment to GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command will contain details of the object. An
    illustration is shown in the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consider the evaluation of CPU vs GPU*.* As most of the deep learning
    will be using GPUs for matrix computation, the performance is evaluated by matrix
    multiplication using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script computes the matrix multiplication using CPU and GPU;
    the time is stored for different dimensions of the matrix. The output from the
    preceding script is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between CPU and GPU
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that the computation effort required by CPUs increases exponentially
    with the CPU. Thus, GPUs help expedite it drastically.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GPUs are new arena in machine learning computing, and a lot of packages
    have been developed in R to access GPUs while keeping you in a familiar R environment
    such as `gputools` , `gmatrix` , and `gpuR`. The other algorithms are also developed
    and implemented while accessing GPUs to enhance their computational power such
    as `RPUSVM`, which implements SVM using GPUs. Thus, the topic requires a lot of
    creativity with some exploration to deploy algorithms while utilizes full capability
    of hardware.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more on parallel computing using R, go through *Mastering Parallel
    Programming with R* by Simon R. Chapple et al*.* (2016).
  prefs: []
  type: TYPE_NORMAL
