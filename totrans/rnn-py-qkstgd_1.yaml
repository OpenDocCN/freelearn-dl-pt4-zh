- en: Introducing Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce you to the theoretical side of the **recurrent neural
    network** (**RNN**) model. Gaining knowledge about what lies behind this powerful
    architecture will give you a head start on mastering the practical examples that
    are provided later in the book. Since you may often find yourself in a situation
    where a critical decision for your application is needed, it is essential to be
    aware of the building parts of this model. This will help you act appropriately
    for the situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prerequisite knowledge for this chapter includes basic linear algebra (matrix
    operations). A basic knowledge in deep learning and neural networks is also a
    plus. If you are new to that field, I would recommend first watching the great
    series of videos made by Andrew Ng ([https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0));
    they will help you make your first steps so you are prepared to expand your knowledge.
    After reading the chapter, you will be able to answer questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an RNN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is an RNN better than other solutions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you train an RNN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some problems with the RNN model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an RNN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RNN is one powerful model from the deep learning family that has shown incredible
    results in the last five years. It aims to make predictions on sequential data
    by utilizing a powerful memory-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how is it different from a standard neural network? A normal (also called
    **feedforward**) neural network acts like a mapping function, where a single input
    is associated with a single output. In this architecture, no two inputs share
    knowledge and the each moves in only one direction—starting from the input nodes,
    passing through hidden nodes, and finishing at the output nodes. Here is an illustration
    of the aforementioned model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0960058e-e745-455b-ae03-1896a3a317d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the contrary, a recurrent (also called feedback) neural network uses an
    additional memory state. When an input A[1] (word **I**) is added, the network
    produces an output B[1] (word **love**) and stores information about the input
    A[1] in the memory state. When the next input A[2] (word **love**) is added, the
    network produces the associated output B[2] (word **to**) with the help of the
    memory state. Then, the memory state is updated using information from the new
    input A[2]. This operation is repeated for each input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2afa42f1-d542-46e4-9c28-3317f227173a.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see how with this method our predictions depend not only on the current
    input, but also on previous data. This is the reason why RNNs are the state-of-the-art
    model for dealing with sequences. Let's illustrate this with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case for the feedforward architecture is image recognition. We
    can see its application in agriculture for analyzing plants, in healthcare for
    diagnosing diseases, and in driverless cars for detecting pedestrians. Since no
    output in any of these examples requires specific information from a previous
    input, the feedforward network is a great fit for such problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another set of problems, which are based on sequential data.
    In these cases, predicting the next element in the sequence depends on all the
    previous elements. The following is a list of several examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Translating text to speech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the next word in a sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting audio to text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captioning videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs were first introduced in the 1980s with the invention of the Hopfield network.
    Later, in 1997, Hochreiter and Schmidhuber proposed an advanced RNN model called
    **long short-term memory** (**LSTM**). It aims to solve some major issues with
    the simplest recurrent neural network model, which we will reveal later in the
    chapter. A more recent improvement to the RNN family was presented in 2014 by
    Chung et al. This new architecture, called Gated Recurrent Unit, solves the same
    problem as LSTM but in a simpler manner.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters of this book, we will go over the aforementioned models
    and see how they work and why researchers and large companies are using them on
    a daily basis to solve fundamental problems.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing recurrent neural networks with similar models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, RNNs, similarly to any neural network model, have become widely
    popular due to the easier access to large amounts of structured data and increases
    in computational power. But researchers have been solving sequence-based problems
    for decades with the help of other methods, such as the Hidden Markov Model. We
    will briefly compare this technique to an RNNs and outline the benefits of both
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The **Hidden Markov Model** (**HMM**) is a probabilistic sequence model that
    aims to assign a label (class) to each element in a sequence. HMM computes the
    probability for each possible sequence and picks the most likely one.
  prefs: []
  type: TYPE_NORMAL
- en: Both the HMM and RNN are powerful models that yield phenomenal results but,
    depending on the use case and resources available, RNN can be much more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the pros and cons of a Hidden Markov Model when solving sequence-related
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:** Less complex to implement, works faster and as efficiently as RNNs
    on problems of medium difficulty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:** HMM becomes exponentially expensive with the desire to increase accuracy.
    For example, predicting the next word in a sentence may depend on a word from
    far behind. HMM needs to perform some costly operations to obtain this information.
    That is the reason why this model is not ideal for complex tasks that require
    large amounts of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These costly operations include calculating the probability for each possible
    element with respect to all the previous elements in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the pros and cons of a recurrent neural network when solving
    sequence-related tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Performs significantly better and is less expensive when working
    on complex tasks with large amounts of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Complex to build the right architecture suitable for a specific problem.
    Does not yield better results if the prepared data is relatively small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result of our observations, we can state that RNNs are slowly replacing
    HMMs in the majority of real-life applications. One ought to be aware of both
    models, but with the right architecture and data, RNNs often end up being the
    better choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, if you are interested in learning more about hidden Markov models,
    I strongly recommend going through some video series ([https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)) and
    papers of example applications, such as *Introduction to Hidden Markov Models* by
    Degirmenci (Harvard University) ([https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf))
    or *Issues and Limitations of HMM in Speech Processing: A Survey* ([https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how recurrent neural networks work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the use of a memory state, the RNN architecture perfectly addresses every
    sequence-based problem. In this section of the chapter, we will go over a full
    explanation of how this works. You will obtain knowledge about the general characteristics
    of a neural network as well as what makes RNNs special. This section emphasizes on
    the theoretical side (including mathematical equations), but I can assure you
    that once you grasp the fundamentals, any practical example will go smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: To make the explanations understandable, let's discuss the task of generating
    text and, in particular, producing a new chapter based on one of my favorite book
    series, *The Hunger Games*, by Suzanne Collins.
  prefs: []
  type: TYPE_NORMAL
- en: Basic neural network overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the highest level, a neural network, which solves supervised problems, works
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain training data (such as images for image recognition or sentences for
    generating text)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the data (neural networks work with numbers so a numeric representation
    of the data is required)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the architecture of your neural network model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model until you are satisfied with the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate your model by making a fresh new prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see how these steps are applied for an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the problem of generating a new book chapter based on the book series *The
    Hunger Games*, you can extract the text from all books in *The Hunger Games* series
    (*The Hunger Games*, *Mockingjay*,and *Catching Fire*) by copying and pasting
    it. To do that, you need to find the books, content online.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use *word embeddings* ([https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/))
    for this purpose. Word embedding is a collective name of all techniques where
    words or phrases from a vocabulary are mapped to vectors of real numbers. Some
    methods include *one-hot encoding*, *word2vec*, and *GloVe*. You will learn more
    about them in the forthcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Building the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each neural network consists of three sets of layers—input, hidden, and output.
    There is always one input and one output layer. If the neural network is deep,
    it has multiple hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e6fdf95-417b-4f9e-b287-d43d59d98fcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The difference between an RNN and the standard feedforward network comes in
    the cyclical hidden states. As seen in the following diagram, recurrent neural
    networks use cyclical hidden states. This way, data propagates from one time step
    to another, making each one of these steps dependent on the previous:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ab51cdd-7c1a-4589-8cd3-e42648ca7991.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A common practice is to unfold the preceding diagram for better and more fluent
    understanding. After rotating the illustration vertically and adding some notations
    and labels, based on the example we picked earlier (generating a new chapter based
    on *The Hunger Game*s books), we end up with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f692adf-92d4-45dc-88a9-d6481ba195b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an unfolded RNN with one hidden layer. The identically looking sets
    of (input + hidden RNN unit + output) are actually the different time steps (or
    cycles) in the RNN. For example, the combination of ![](img/2c7fd095-312a-4ed7-bbcc-ee8f3bb1e87a.png) +
    RNN + ![](img/ba8c3aef-052e-45c6-aded-7f0fdf5900a7.png) illustrates what is happening
    at time step  ![](img/aa6cc93f-ec7f-43b5-9cb7-857d56873e7a.png). At each time
    step, these operations perform as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The network encodes the word at the current time step (for example, *t-1*) using
    any of the word embedding techniques and produces a vector ![](img/03170e2f-c7b6-46e2-a41c-73b750d49306.png) (The
    produced vector can be ![](img/e38865d2-edc6-4d1e-b2e4-a04c750037fd.png) or ![](img/7baad917-f493-4b4f-86f4-5a73c6bc2803.png) depending
    on the specific time step)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, ![](img/d1ffcf32-69ed-49ad-8513-ac64521a3811.png), the encoded version
    of the input word **I** at time step *t-1*, is plugged into the RNN cell (located
    in the hidden layer). After several equations (not displayed here but happening
    inside the RNN cell), the cell produces an output ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png) and
    a memory state ![](img/0e571081-ba4c-46fc-894e-082186548e71.png). The memory state
    is the result of the input ![](img/6ebaf1d4-ff95-459d-8d31-1e23e76381e9.png) and
    the previous value of that memory state ![](img/5cc65152-194e-4699-add4-6b897aa1ae88.png).
    For the initial time step, one can assume that ![](img/c87969e4-4c5c-471c-bcc1-670173b3560e.png) is
    a zero vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Producing the actual word (volunteer) at time step *t-1* happens after decoding
    the output ![](img/3a8d2142-2b9f-49cc-ac38-c7d2a8418570.png) using a *text corpus* specified
    at the beginning of the training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the network moves multiple time steps forward until reaching the final
    step where it predicts the word
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see how each one of {…, ![](img/5017ca77-a53e-4b7e-9266-f1da40891353.png), ![](img/ca7494e8-58cb-442f-91f4-e9ad07bc8180.png), ![](img/d376e907-27bb-42b1-a831-7fb348d249e6.png),
    …} holds information about all the previous inputs. This makes RNNs very special
    and really good at predicting the next unit in a sequence. Let's now see what
    mathematical equations sit behind the preceding operations.
  prefs: []
  type: TYPE_NORMAL
- en: Text corpus—an array of all words in the example vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the magic in this model lies behind the RNN cells. In our simple example,
    each cell presents the same equations, just with a different set of variables.
    A detailed version of a single cell looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc1142dc-a296-4282-929c-c837bb8156ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, let''s explain the new terms that appear in the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weights** (![](img/2ab90e1a-84a7-4944-8ea9-ac7dd4926e98.png), ![](img/1f8e671b-caa3-4eb3-9834-073e1f82f11e.png),
    ![](img/5e662750-3c2c-46ce-b196-e610129a9f75.png)): A weight is a matrix (or a
    number) that represents the strength of the value it is applied to. For example, ![](img/ed73bf73-06a9-471c-ba23-8ecff48cc905.png)determines
    how much of the input ![](img/6a3d0d98-3604-4252-b2f9-80c042e561aa.png)should
    be considered in the following equations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If  ![](img/5548d652-c336-497d-92ca-f608725bd586.png)consists of high values,
    then ![](img/aa76c78b-9a21-4286-ab8f-65d4db13ea2a.png)should have significant
    influence on the end result. The weight values are often initialized randomly
    or with a distribution (such as normal/Gaussian distribution). It is important
    to be noted that  ![](img/0bd1b875-eaa0-4056-83bd-614aa9e82f23.png),  ![](img/cb748064-2362-413f-91dd-f5f1dbf0f713.png), 
    and ![](img/ef1c08c8-a815-4898-a69a-5b83fb4ff560.png) are the same for each step.
    Using the backpropagation algorithm, they are being modified with the aim of 
    producing accurate predictions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Biases** (![](img/cf5c1a7d-e905-40b6-b72a-296f678a6497.png), ![](img/5f181ca2-832e-44a0-a169-6125059fe5ac.png)):
    An offset vector (different for each layer), which adds a change to the value
    of the output ![](img/b5e6a945-1bf6-4006-a131-89b958f982df.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function** (**tanh**): This determines the final value of the
    current memory state ![](img/498e4f45-afe2-4a6d-920b-246b0c8ba1af.png) and the
    output ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png). Basically, the activation
    functions map the resultant values of several equations similar to the following
    ones into a desired range: (`-1, 1`) if we are using the **tanh** function, (`0,
    1`) if we are using sigmoid function, and (`0, +infinity`) if we are using ReLu
    ([https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s go over the process of computing the variables. To calculate ![](img/bc7b3fd5-4fb7-4d84-94f5-a69d8a1c8fc7.png)
    and ![](img/2ea2f288-c311-445e-bbc9-46e4131d20f8.png), we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b42d2cb-3a54-4673-b594-9d26434eb57e.png)![](img/ff12e2a1-875f-403f-97d0-b1055df7b1e2.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the memory state ![](img/2f13a990-cb41-4a06-9cc0-76179861ef76.png)
    is a result of the previous value ![](img/a1a73217-c1a7-4e18-b3ef-c21d6c69f20a.png)
    and the input ![](img/7c0302c2-2f04-4f75-9594-ec6f3b7c1b0e.png). Using this formula
    helps in retaining information about all the previous states.
  prefs: []
  type: TYPE_NORMAL
- en: The input ![](img/398dabbd-5043-437e-bfee-1082a8fb5968.png) is a one-hot representation
    of the word *volunteer*. Recall from before that one-hot encoding is a type of
    word embedding. If the text corpus consists of 20,000 unique words and volunteer is
    the 19th word, then ![](img/7d341d05-cdab-47a1-acb7-5434a25cdd8c.png) is a 20,000-dimensional
    vector where all elements are 0 except the one at the 19^(th) position, which
    has a value of 1, which suggests that we only taking into account this particular
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum between ![](img/c16273cd-dad0-49f0-913d-e5235ff8d9cd.png), ![](img/7c299c35-24cf-4226-a201-2840b5309a7a.png), and ![](img/b0fb3e81-ef6f-4853-b4d5-d63c819ff4b6.png)
    is passed to the *tanh* activation function, which squashes the result between
    `-1` and `1` using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e3eb640-9563-4b39-b371-cef15c75b12b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this, `e = 2.71828` (Euler's number) and *z* is any real number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output ![](img/2a4b4321-8736-45c7-82f6-d05d8acab57d.png) at time step t
    is calculated using ![](img/18f550fa-70c9-4d40-bb6e-26eb0fa20a22.png) and the
    `softmax` function. This function can be categorized as an activation with the
    exception that its primary usage is at the output layer when a probability distribution
    is needed. For example, predicting the correct outcome in a classification problem
    can be achieved by picking the highest probable value from a vector where all
    the elements sum up to `1`. Softmax produces this vector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8e8ea49-faae-45b9-8d4b-c16bf01d50b5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this, `e = 2.71828` (Euler's number) and z is a K-dimensional vector. The
    formula calculates probability for the value at the `i`^(th) position in the vector
    z.
  prefs: []
  type: TYPE_NORMAL
- en: After applying the `softmax` function, ![](img/5779c435-a0bb-465d-8950-49978065336d.png)
    becomes a vector of the same dimension as ![](img/aad81213-464e-4c00-b983-da58fb2b719a.png)
    (the corpus size `20,000`) with all its elements having a total sum of `1`. With
    that in mind, finding the predicted word from the text corpus becomes straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once an assumption for the next word in the sequence is made, we need to assess
    how good this prediction is. To do that, we need to compare the predicted word ![](img/13a825fd-b8f2-4656-8bf5-b6745ce0c43d.png)
    with the actual word from the training data (let''s call it  ![](img/baadadd4-3f8c-497c-984a-cb2e098c697d.png)).
    This operation can be accomplished using a loss (cost) function. These types of
    functions aim to find the error between predicted and actual values. Our choice
    will be the cross-entropy loss function, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6ebb6e0-25eb-4d9a-9706-e72df4bb8ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we are not going to give a detailed explanation of this formula, you can
    treat it as a black box. If you are curious about how it works, I recommend reading
    the article *Improving the way neural networks work* by Michael Nielson ([http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function)).
    A useful thing to know is that the cross-entropy function performs really well
    on classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: After computing the error, we came to one of the most complex and, at the same
    time, powerful techniques in deep learning, called backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, we can state that the backpropagation algorithm traverses backward
    through all (or several) time steps while updating the weights and biases of the
    network. After repeating this procedure, and a certain amount of training steps,
    the network learns the correct parameters and will be able to yield better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: To clear out any confusion, training and time steps are completely different
    terms. In one time step, we get a single element from the sequence and predict
    the next one. A training step is composed of multiple time steps where the number
    of time steps depends on how large the sequence for this training step is. In
    addition, time steps are only used in RNNs, but training ones are a general neural
    network concept.
  prefs: []
  type: TYPE_NORMAL
- en: After each training step, we can see that the value from the loss function decreases.
    Once it crosses a certain threshold, we can state that the network has successfully
    learned to predict new words in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to generate the new chapter. This can happen by choosing a
    random word as a start (such as: games) and then predicting the next words using
    the preceding formulas with the pre-trained weights and biases. Finally, we should
    end up with somewhat meaningful text.'
  prefs: []
  type: TYPE_NORMAL
- en: Key problems with the standard recurrent neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, now you have a good understanding of how a recurrent neural network
    works. Unfortunately, this simple model fails to make good predictions on longer
    and complex sequences. The reason behind this lies in the so-called vanishing/exploding
    gradient problem that prevents the network from learning efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, the training process updates the weights and biases using
    the backpropagation algorithm. Let's dive one step further into the mathematical
    explanations. In order to know how much to adjust the parameters (weights and
    biases), the network computes the derivative of the loss function (at each time
    step) with respect to the current value of these parameters. When this operation
    is done for multiple time steps with the same set of parameters, the value of
    the derivative can become too large or too small. Since we use it to update the
    parameters, a large value can result in undefined weights and biases and a small
    value can result in no significant update, and thus no *learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Derivative is a way to show the rate of change; that is, the amount by which
    a function is changing at one given point. In our case, this is the rate of change
    of the loss function with respect to the given weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: This issue was first addressed by Bengio et al. in 1994, which led to an introduction
    of the LSTM network with the aim of solving the vanishing/exploding gradient problem.
    Later in the book, we will reveal how LSTM does this in an excellent fashion.
    Another model, which also overcomes this challenge, is the gated recurrent unit. In
    [Chapter 3](05d1d826-3bc6-48e4-ad62-a53c7a609059.xhtml), *Generating Your Own
    Book Chapter,* you will see how this is being done.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the vanishing/exploding gradient problem, it would be
    useful to go over Lecture 8 from the course *Natural Language Processing with
    Deep Learning* by Stanford University ([https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8))
    and the paper *On the difficulty of training recurrent neural networks* ([http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce the recurrent neural network model using theoretical
    explanations together with a particular example. The aim is to grasp the fundamentals
    of this powerful system so you can understand the programming exercises better.
    Overall, the chapter included the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between RNNs and other popular models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustrating the use of RNNs through an example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main problems with a standard RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will go over our first practical exercise using recurrent
    neural networks. You will get to know the popular TensorFlow library, which makes
    it easy to build machine learning models. The next section will give you a nice
    first hands-on experience and prepare you for solving more difficult problems.
  prefs: []
  type: TYPE_NORMAL
- en: External links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrew Ng's deep learning course: [https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden Markov model: [https://www.youtube.com/watch?v=TPRoLreU9lA](https://www.youtube.com/watch?v=TPRoLreU9lA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Hidden Markov Model*s by Degirmenci: [https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf](https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Issues and Limitations of HMM in Speech Processing: A Survey: *[https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf](https://pdfs.semanticscholar.org/8463/dfee2b46fa813069029149e8e80cec95659f.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words embeddings: [https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/ ](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)and [https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding activation functions: *[https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks](https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Improving the way neural networks work* by Michael Nielson: [http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function](http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecture 8 from the course, *Natural Language Processing with Deep Learning* by
    Stanford University: [https://www.youtube.com/watch?v=Keqep_PKrY8](https://www.youtube.com/watch?v=Keqep_PKrY8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O*n the difficulty of training recurrent neural networks*: [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
