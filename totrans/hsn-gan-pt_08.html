<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image-to-Image Translation and Its Applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will push the label-based image generation to the next level: we will use pixel-wise labeling to perform image-to-image translation and transfer image styles.</p>
<p>You will learn how to use pixel-wise label information to perform <span>image-to-image translation</span> with pix2pix and translate high-resolution images with pix2pixHD. Following this, you will learn how to perform style transfer between unpaired image collections with CycleGAN.</p>
<p>By the end of this chapter, combined with the knowledge from the previous chapter, you will have grasped the core methodology of using image-wise and pixel-wise label information to improve the quality, or manipulate the attributes, of generated images. You will also know how to flexibly design model architectures to accomplish your goals, including generating larger images or transferring textures between different styles of images.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Using pixel-wise labels to translate images with pix2pix</li>
<li>Pix2pixHD <span>–</span> high-resolution image translation</li>
<li>CycleGAN <span>–</span> image-to-image translation from unpaired collections</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using pixel-wise labels to translate images with pix2pix</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned how to use auxiliary information such as labels and attributes to improve the quality of images that are generated by GANs. The labels we used in the previous chapter were image-wise, which means that each image has only one or several labels. Labels can be assigned to specific pixels, which are known as pixel-wise labels. Pixel-wise labels are playing an increasingly important role in the realm of deep learning. For example, one of the most famous online image classification contests, the <strong>ImageNet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>, <a href="http://www.image-net.org/challenges/LSVRC/">http://www.image-net.org/challenges/LSVRC</a>), is no longer being hosted since its last event in 2017, whereas object detection and <span>segmentation </span>challenges such as COCO (<a href="http://cocodataset.org">http://cocodataset.org</a>) are receiving more attention.</p>
<p>An iconic application of pixel-wise labeling is semantic segmentation. <strong>Semantic segmentation</strong> (or image/object segmentation) is a task in which every pixel in the image must belong to one object. The most promising application of semantic segmentation is autonomous cars (or self-driving cars). If each and every pixel that's captured by the camera that's mounted on the self-driving car is correctly classified, all of the objects in the image will be easily recognized, which makes it much easier for the <span>vehicle</span> to properly analyze the current environment and make the right decision upon whether it should, for example, turn or slow down to avoid other vehicles and pedestrians. To understand more about semantic segmentation, please refer to the following link: <a href="https://devblogs.nvidia.com/image-segmentation-using-digits-5">https://devblogs.nvidia.com/image-segmentation-using-digits-5</a>.</p>
<p>Transforming the original color image into a segmentation map (as shown in the following diagram) can <span>be considered as an image-to-image translation problem, which is a much larger field and includes style transfer, image colorization, and more. Image <strong>style transfer</strong> is about moving the iconic textures and colors from one image to another, such as combining your photo with a Vincent van Gogh painting to create a unique artistic portrait of you. <strong>Image colorization</strong> is a task where we feed a 1-channel grayscale image to the model and let it predict the color information for each pixel, which leads to a 3-channel color image. </span></p>
<p>GANs can be used in image-to-image translation as well. In this section, we will use a classic <span>image-to-image translation model, pix2pix, to transform images from one domain to another. Pix2pix was proposed by Phillip Isola, Jun-Yan Zhu, and Tinghui Zhou, et. al. in their paper <em>Image-to-Image Translation with Conditional Adversarial Networks</em>. Pix2pix was designed to learn of the connections between paired collections of images, for example, transforming an aerial photo taken by a satellite into a regular map, or a sketch image into a color image, and vice versa.</span></p>
<p class="mce-root"/>
<p><span>The authors of the paper have kindly provided the full source code for their work, which runs perfectly on PyTorch 1.3. The source code is also well organized. Therefore, we will use their code directly in order to train and evaluate the pix2pix model and learn how to organize our models in a different way.</span></p>
<p>First, open a Terminal and download the code for this section using the following command. This is also available under the <kbd>pix2pix</kbd> directory in this chapter's code repository:</p>
<pre><strong>$ git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git</strong></pre>
<p>Then, <kbd>install</kbd> the prerequisites to be able to visualize the results during training:</p>
<pre><strong>$ pip install dominate visdom</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator architecture</h1>
                </header>
            
            <article>
                
<p>The architecture of the generator network of pix2pix is as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6b0e72d5-fb06-4e4f-9642-65a97cfe2f52.png" style="width:57.58em;height:33.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Generator architecture of pix2pix</div>
<p class="mce-root"/>
<p><span>Here, we assume that both the input and output data are 3-channel 256x256 images. </span>In order to illustrate the generator <span>structure of pix2pix, feature maps are represented by colored blocks and convolution operations are represented by gray and blue arrows</span><span>, in which</span> <span>gray arrows are </span>convolution layers for reducing the feature map sizes and blue arrows are for doubling the <span>feature map sizes. Identity mapping (including skip connections) is represented by black arrows.</span></p>
<p>We can see that the first half layers of this network gradually transform the input image into 1x1 feature maps (with wider channels) and the last half layers transform these very small feature maps into an output image with the same size of the input image. It compresses the input data into much lower dimensions and changes them back to their original dimensions. Therefore, this U-shaped kind of network structure is often known as U-Net. There are also many skip connections in the U-Net that connect the mirrored layers in order to help information (including details coming from previous layers in the forward pass and gradients coming from the latter layers in the backward pass) flow through the network. Without these skip connections, the network is also known as an encoder-decoder model, meaning that we stack a decoder at the end of an encoder.</p>
<p>The pix2pix model is defined in the <kbd>models.pix2pix_model.Pix2PixModel</kbd> class, which is derived from an <strong>abstract base class</strong> (<strong>ABC</strong>) known as <kbd>models.base_model.BaseModel</kbd>.</p>
<div class="packt_infobox"><span>An </span><strong>abstract base class</strong><span> in Python is a class containing at least one </span><strong>abstract method</strong><span> (that's declared and not implemented). It cannot be instantiated. You can only </span><span>create objects with its subclasses after providing the implementations for all the abstract methods.</span></div>
<p>The generator network, <kbd>netG</kbd>, is created by the <kbd>models.networks.define_G</kbd> method. By default, it takes <kbd>'unet_256'</kbd> as the <kbd>netG</kbd> argument value (which is specified at line 32 in <kbd>models/pix2pix_model.py</kbd> <span>and overrides the initialized value, that is, <kbd>"resnet_9blocks"</kbd>, at line 34 in</span> <kbd>options/base_options.py</kbd><span>). Therefore, <kbd>models.networks.UnetGenerator</kbd> is used to create the U-Net. In order to show how the U-Net is created in a recursive manner, we replace the arguments with their actual values, as shown in the following code:</span></p>
<pre><br/>import torch.nn as nn<br/>class UnetGenerator(nn.Module):<br/>    def __init__(self):<br/>        super(UnetGenerator, self).__init__()<br/>        unet_block = UnetSkipConnectionBlock(64 * 8, 64 * 8, submodule=None, innermost=True)<br/>        for i in range(8 - 5):<br/>            unet_block = UnetSkipConnectionBlock(64 * 8, 64 * 8, submodule=unet_block, use_dropout=True)<br/>        unet_block = UnetSkipConnectionBlock(64 * 4, 64 * 8, submodule=unet_block)<br/>        unet_block = UnetSkipConnectionBlock(64 * 2, 64 * 4, submodule=unet_block)<br/>        unet_block = UnetSkipConnectionBlock(64, 64 * 2, submodule=unet_block)<br/>        self.model = UnetSkipConnectionBlock(3, 64, input_nc=3, submodule=unet_block, outermost=True)<br/><br/>    def forward(self, input):<br/>        return self.model(input)</pre>
<p>At the fourth line in the preceding code snippet, the innermost block is defined, which creates the layers in the middle of the U-Net. The innermost block is defined as follows. Note that the following code should be treated as pseudocode since it's simply to show you how different blocks are designed:</p>
<pre>class UnetSkipConnectionBlock(nn.Module):<br/>    # Innermost block */<br/>    def __init__(self):<br/>        down = [nn.LeakyReLU(0.2, inplace=True),<br/>                nn.Conv2d(64 * 8, 64 * 8, kernel_size=4,<br/>                          stride=2, padding=1, bias=False)]<br/>        up = [nn.ReLU(inplace=True),<br/>              nn.ConvTranspose2d(64 * 8, 64 * 8,<br/>                                 kernel_size=4, stride=2,<br/>                                 padding=1, bias=False),<br/>              nn.BatchNorm2d(64 * 8)]<br/>        model = down + up<br/>        self.model = nn.Sequential(*model)<br/><br/>    def forward(self, x):<br/>        return torch.cat([x, self.model(x)], 1)</pre>
<p>The <kbd>nn.Conv2d</kbd> layer in <kbd>down</kbd> transforms 2x2 input feature maps into 1x1 ones (because <kbd>kernel_size</kbd>=4 and <kbd>padding</kbd>=1), and the <kbd>nn.ConvTranspose2d</kbd> layer transforms them back so that they're 2x2 in size.</p>
<div class="packt_tip">Remember the calculation formula of the output size for <kbd>nn.Conv2d</kbd> and <kbd>nn.ConvTranspose2d</kbd>? The output size of the convolution is <img class="fm-editor-equation" src="assets/1539e7fa-afb0-43b4-8182-da522a169eb1.png" style="width:37.83em;height:1.33em;"/>, while the output size of the transposed convolution is <img class="fm-editor-equation" src="assets/9050dcbb-74e6-46ec-b1e7-764b92864e6b.png" style="width:38.17em;height:1.33em;"/>.</div>
<p><span>In the forward pass, it </span><span>concatenates the output with a skip connection (that is,</span><span> the input <em>x</em> itself) along the depth channel, which doubles the number of channels (and leads to the first 1,024-channel feature maps in the preceding diagram).</span></p>
<div class="packt_tip">When designing complex networks, it's been observed that the concatenation of the feature maps from two branches is better than their sum because the <span>concatenation reserves more information. Of course, this concatenation costs a little more memory as well.</span></div>
<p>Then, the rest of the layers are built recursively, as follows:</p>
<pre>class UnetSkipConnectionBlock(nn.Module):<br/>    # Other blocks */<br/>    def __init__(self, out_channels, in_channels, submodule, use_dropout):<br/>        down = [nn.LeakyReLU(0.2, inplace=True),<br/>                nn.Conv2d(out_channels, in_channels, kernel_size=4,<br/>                          stride=2, padding=1, bias=False),<br/>                nn.BatchNorm2d(in_channels)]<br/>        up = [nn.ReLU(inplace=True),<br/>              nn.ConvTranspose2d(in_channels * 2, out_channels,<br/>                                 kernel_size=4, stride=2,<br/>                                 padding=1, bias=False),<br/>              nn.BatchNorm2d(out_channels)]<br/>        if use_dropout:<br/>            model = down + [submodule] + up + [nn.Dropout(0.5)]<br/>        else:<br/>            model = down + [submodule] + up<br/>        self.model = nn.Sequential(*model)<br/><br/>    def forward(self, x):<br/>        return torch.cat([x, self.model(x)], 1)</pre>
<p>Although in <kbd>models.networks.UnetGenerator</kbd>, the <kbd>unet_block</kbd> object is recursively passed as a <kbd>submodule</kbd> to a new <kbd>unet_block</kbd>, thanks to the compact design to the implementation of tensors, the actual modules will be created and saved on memory properly.</p>
<p>Finally, the first and last layers (which can be seen in the outermost block) are defined as follows:</p>
<pre>class UnetSkipConnectionBlock(nn.Module):<br/>    # Outermost block */<br/>    def __init__(self):<br/>        down = [nn.Conv2d(3, 64, kernel_size=4,<br/>                          stride=2, padding=1, bias=False)]<br/>        up = [nn.ReLU(inplace=True),<br/>              nn.ConvTranspose2d(64 * 2, 3,<br/>                                 kernel_size=4, stride=2,<br/>                                 padding=1),<br/>              nn.Tanh()]<br/>        model = down + [submodule] + up<br/>        self.model = nn.Sequential(*model)<br/><br/>    def forward(self, x):<br/>        return self.model(x)</pre>
<p>All the convolution kernels in the generator network are initialized based on a normal distribution with a mean of 0 and a standard deviation of 0.02. The scale factors in all the batch normalization layers are initialized based on the <span>normal distribution with a mean of 1 and a standard deviation of 0.02.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminator architecture</h1>
                </header>
            
            <article>
                
<p><span>The architecture of the discriminator network of pix2pix is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/496267dd-f9f8-4c85-a1ad-de0fb427a0ed.png" style="width:40.83em;height:19.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Discriminator architecture of pix2pix</span></div>
<p>A pair of samples (one from each collection) are concatenated along the depth channel, and this 6-channel image is treated as the actual input of the discriminator network. The <span>discriminator network maps the 6-channel 256x256 image to a 1-channel 30x30 image, which is used to calculate the discriminator loss.</span></p>
<p class="mce-root"/>
<p>The <span>discriminator network, <kbd>netD</kbd>, is created by the <kbd>models.networks.define_G</kbd> method. By default, it takes <kbd>"basic"</kbd> as the argument value of <kbd>netD</kbd>, which is defined at line 33 in</span> <kbd>options/base_options.py</kbd><span>. The <kbd>models.networks.NLayerDiscriminator</kbd> module, which has <kbd>n_layer=3</kbd>, is initialized so that it can serve as the discriminator network. Again, we've simplified the code so that it's easy to read. You may refer to the full code in the</span> <kbd>models/networks.py</kbd> <span>file:</span></p>
<pre>class NLayerDiscriminator(nn.Module):<br/>    def __init__(self, n_layers=3):<br/>        super(NLayerDiscriminator, self).__init__()<br/>        sequence = [nn.Conv2d(3 + 3, 64, kernel_size=4, stride=2, padding=1),<br/>                    nn.LeakyReLU(0.2, True)]<br/>        channel_scale = 1<br/>        channel_scale_prev = 1<br/>        for n in range(1, n_layers):<br/>            channel_scale_prev = channel_scale<br/>            channel_scale = 2**n<br/>            sequence += [<br/>                nn.Conv2d(64 * channel_scale_prev, 64 * channel_scale, kernel_size=4, stride=2, padding=1, bias=False),<br/>                nn.BatchNorm2d(64 * channel_scale),<br/>                nn.LeakyReLU(0.2, True)<br/>            ]<br/>        channel_scale_prev = channel_scale<br/>        sequence += [<br/>            nn.Conv2d(64 * channel_scale_prev, 64 * 8, kernel_size=4, stride=1, padding=1, bias=False),<br/>            nn.BatchNorm2d(64 * 8),<br/>            nn.LeakyReLU(0.2, True)<br/>        ]<br/>        sequence += [nn.Conv2d(64 * 8, 1, kernel_size=4, stride=1, padding=1)]<br/>        self.model = nn.Sequential(*sequence)<br/><br/>    def forward(self, input):<br/>        return self.model(input)</pre>
<p>Here, we provide a short snippet so that we can print the sizes of all the feature maps if the model is created, as follows:</p>
<pre>class SomeModel(nn.Module):<br/>    def __init__(self):<br/>        super(SomeModel, self).__init__()<br/>        sequence = [layer1, layer2, ...]<br/>        self.model = nn.Sequential(*sequence)<br/><br/>    def forward(self, input):<br/>        return self.model(input)</pre>
<p>You can replace the line <kbd>return self.model(input)</kbd> with the following code to check the feature map sizes in all the layers (including the normalization and activation function layers):</p>
<pre> def forward(self, input):<br/> x = input<br/> for i in range(len(self.model)):<br/> print(x.shape)<br/> x = self.model[i](x)<br/> print(x.shape)<br/> return x</pre>
<p>Alternatively, you can always use TensorBoard or other tools, which we will introduce in the last chapter of this book, so that you can easily examine the architectures of your models.</p>
<p>The discriminator network creates a 30x30 feature map to represent the loss. This kind of architecture is called <strong>PatchGAN</strong>, which means that every small image patch in the original image is mapped to a pixel in the final loss map. A big advantage of PatchGAN is that it can handle the arbitrary sizes of input images as long as the labels have been transformed so that they're the same size as the loss map. It also evaluates the quality of the input image according to the quality of the local patches, rather than their global property. Here, we will show you how the size of the image patch (that is, 70) is calculated.</p>
<p>First, let's consider a single convolution layer with a kernel size of <kbd>k</kbd> and a stride size of <kbd>s</kbd>. For each pixel in the output feature map, its value is only determined by a small patch of pixels in the input image, whose size is the same as the convolution kernel.</p>
<p>When there are more than two convolution layers, the size of the input patch is calculated with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ed0ae0fe-f0b9-4825-999a-41e71a2e3bf7.png" style="width:26.08em;height:1.33em;"/></p>
<p>Therefore, the size of the input patch corresponding to a single pixel in the output feature map in each layer of the discriminator network can be obtained:</p>
<ul>
<li>5th layer (k=4, s=1): Input patch size is 4 (which is the size of the kernel)</li>
<li><span>4th layer (k=4, s=1): Input patch size is 4+1*(4-1)=7</span></li>
<li><span>3rd layer (k=4, s=2): Input patch size is 4+2*(7-1)=16</span></li>
<li><span>2nd layer (k=4, s=2): Input patch size is 4+2*(16-1)=34</span></li>
<li><span>1st layer (k=4, s=2): Input patch size is 4+2*(34-1)=70</span></li>
</ul>
<p class="mce-root"/>
<p>This means that all of these 70x70 overlapping image patches are transformed by convolution layers into individual pixels in the 30x30 loss map. Any pixel outside this <span>70x70</span> image patch has no influence over the corresponding pixel in the loss map.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and evaluation of pix2pix</h1>
                </header>
            
            <article>
                
<p>The training of pix2pix is very similar to conditional GANs, which we introduced in the previous chapter. When training the discriminator network, a pair of real data and a label should be mapped to 1, whereas a pair of generated data and a label (that fake data is generated from) is mapped to 0. When training the generator network, the gradients are passed through both of the discriminator and generator networks when the parameters in the generator network are updated. This <span>generated data and the label should be mapped to 1 by the discriminator network. The major difference is that the labels are image-wise in CGAN and are pixel-wise in pix2pix. This process is described in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7ccb49b6-c6c5-415b-a0a2-65129f267e48.png" style="width:34.58em;height:15.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Basic training process of image-wise and pixel-wise labeled GANs. A* and B* denote real samples. Networks in red boxes are actually updated.</div>
<p>Note that, when training pix2pix, in order to let the generated samples be as similar to the real ones as possible, an additional term is added to the loss function when training the generator network, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cd1117ed-2fbc-4293-add1-c65349fd6b54.png" style="width:20.17em;height:2.00em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/bd348fa5-fff8-4911-bc8d-58bb0795875f.png" style="width:3.25em;height:1.17em;"/> represents the L1-loss between the generated samples and the real ones from the paired collection. The purpose of the L1-loss is to reserve the low-frequency information in the images for better image quality.</p>
<div class="packt_tip">It is worth mentioning that using L1-norm or L2-<span>norm</span> alone will generate blurry or blocky images. <span>A short explanation of this can be found here: <a href="https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry">https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry</a>.</span> It is also common to use them as regularization terms in the traditional image restoration methods, in which the gradients of the restored images control the sharpness. If you are interested in the roles of L1-loss and L2-loss in the field of image processing, feel free to check out the famous paper, <em>Total variation blind deconvolution</em> by Tony F. Chan and C.K. Wong in 1998.</div>
<p>Now, we can define the training procedure of pix2pix, as follows (pseudocode):</p>
<pre>class Pix2PixModel(BaseModel):<br/>    def __init__(self):<br/>        BaseModel.__init__(self)<br/>        self.netG = networks.define_G()<br/>        self.netD = networks.define_D()<br/><br/>        self.criterionGAN = torch.nn.BCEWithLogitsLoss()<br/>        self.criterionL1 = torch.nn.L1Loss()<br/>        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=0.0002, betas=(0.5, 0.999))<br/>        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=0.0002, betas=(0.5, 0.999))<br/>        self.optimizers.append(self.optimizer_G)<br/>        self.optimizers.append(self.optimizer_D)<br/><br/>    def forward(self):<br/>        self.fake_B = self.netG(self.real_A)<br/><br/>    def backward_D(self):<br/>        fake_AB = torch.cat((self.real_A, self.fake_B), 1)<br/>        pred_fake = self.netD(fake_AB.detach())<br/>        self.loss_D_fake = self.criterionGAN(pred_fake, False)<br/><br/>        real_AB = torch.cat((self.real_A, self.real_B), 1)<br/>        pred_real = self.netD(real_AB)<br/>        self.loss_D_real = self.criterionGAN(pred_real, True)<br/><br/>        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5<br/>        self.loss_D.backward()<br/><br/>    def backward_G(self):<br/>        fake_AB = torch.cat((self.real_A, self.fake_B), 1)<br/>        pred_fake = self.netD(fake_AB)<br/>        self.loss_G_GAN = self.criterionGAN(pred_fake, True)<br/><br/>        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B)<br/><br/>        self.loss_G = self.loss_G_GAN + self.loss_G_L1 * 100.0<br/>        self.loss_G.backward()<br/><br/>    def optimize_parameters(self):<br/>        self.forward()<br/>        # update D<br/>        self.set_requires_grad(self.netD, True)<br/>        self.optimizer_D.zero_grad()<br/>        self.backward_D()<br/>        self.optimizer_D.step()<br/>        # update G<br/>        self.set_requires_grad(self.netD, False)<br/>        self.optimizer_G.zero_grad()<br/>        self.backward_G()<br/>        self.optimizer_G.step()</pre>
<p>The <kbd>Pix2PixModel</kbd> class serves a similar purpose to the <kbd>Model</kbd> class in <kbd>build_gan.py</kbd> from the previous chapter, which creates the generator and discriminator networks, defines their optimizers, and controls the training procedures of the networks.</p>
<p>Now, let's download some images and train the pix2pix model to perform image-to-image translation.</p>
<p>Run the <kbd>datasets/download_pix2pix_dataset.sh</kbd> script to download the dataset files, as follows:</p>
<pre><strong>$ ./datasets/download_pix2pix_dataset.sh maps</strong></pre>
<p>Alternatively, you can go to <a href="http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/">http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/</a> to download the dataset files manually and <span>extract them to any location you like (for example, an external hard drive</span> such as <kbd>/media/john/HouseOfData/image_transfer/maps</kbd><span>). The</span> maps <span>dataset file is approximately 239 MB in size and contains a few more than 1,000 images in the collections of the train, validation, and test sets.</span></p>
<div class="packt_infobox">Note that collection A in the maps dataset contains satellite photos and that collection B contains map images, which is opposite to what was shown in the diagrams in the previous subsections.</div>
<p>Next, open a Terminal and run the following script to start training. Make sure you modify the <kbd>dataroot</kbd> argument so that it specifies your own location. You may also try other datasets and change <kbd>direction</kbd> from <kbd>BtoA</kbd> to <kbd>AtoB</kbd> to change the translation direction between two image collections:</p>
<pre>$ python train.py --dataroot /media/john/HouseOfData/image_transfer/maps --name maps_pix2pix --model pix2pix --direction BtoA</pre>
<p>For the first time of training, you may encounter an error stating <kbd>Could not connect to Visdom server</kbd>. This is because the training script calls the <kbd>Visdom</kbd> module to dynamically update the generated results so that we can monitor the training process via web browsers. You can manually open the <kbd>checkpoints/maps_pix2pix/web/index.html</kbd> file with your favorite browser to keep an eye on the generated images as the model is being trained. <span>Note that there is a chance that closing the</span> <kbd>index.html</kbd> <span>page in the web browser could cause the training process to </span><span>freeze</span><span>.</span></p>
<p>It takes about 6.7 hours to finish 200 epochs of training and costs about 1,519 MB GPU memory on a GTX 1080Ti graphics card.</p>
<p>The results are also saved in the <kbd>checkpoints/maps_pix2pix/web/images</kbd> directory. The images that are generated by doing this are as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a9938fd6-2b4d-4da7-b15e-33470c8bf30c.png" style="width:34.00em;height:23.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Generated images by pix2pix</div>
<p>As we can see, the generated satellite photos look pretty convincing on their own. Compared to real satellite photos, they do a good job of organizing the trees along the trails in the park.</p>
<p>In this section, we managed to translate and generate 256 x 256 images. In the next section, we will learn how to generate high-resolution images with an upgraded version of pix2pix: pix2pixHD.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pix2pixHD – high-resolution image translation</h1>
                </header>
            
            <article>
                
<p>Pix2pixHD was proposed by Ting-Chun Wang, Ming-Yu Liu, and Jun-Yan Zhu, et. al. in their paper, <em>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</em>, which was an upgraded version of the pix2pix model. The biggest improvement of pix2pixHD over pix2pix is that it supports image-to-image translation at 2,048x1,024 resolution and with high quality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p>To make this happen, they designed a two-stage approach to gradually train and refine the networks, as shown in the following diagram. First, a lower resolution image of 1,024x512 is generated by a generator network, <img class="fm-editor-equation" src="assets/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png" style="width:1.42em;height:1.08em;"/>, called the <strong>global generator</strong> (the red box). Second, the image is enlarged by a generator network, <img class="fm-editor-equation" src="assets/1d7444a7-949b-4f66-871d-8ecd2f72d759.png" style="width:1.42em;height:1.08em;"/>, called the <strong>local enhancer network</strong> so that it's around 2,048x1,024 in size (the black box). It is also viable to put another <span>local enhancer network at the end to generate 4,096x2,048 images. Note that the last feature map in <img class="fm-editor-equation" src="assets/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png" style="width:1.42em;height:1.08em;"/> is also inserted into <img class="fm-editor-equation" src="assets/1d7444a7-949b-4f66-871d-8ecd2f72d759.png" style="width:1.42em;height:1.08em;"/> (before the residual blocks) via an element-wise sum to introduce more global information into higher resolution images:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/084d449a-d919-449f-9768-40daf14dfb27.png" style="width:43.75em;height:12.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Architecture of the generator model in pix2pixHD (image retrieved from the paper by T. C. Wang, et. al., 2018)</div>
<p>The discriminator network in pix2pixHD is also designed in a multi-scale fashion. Three identical discriminator networks work on different image scales (original size, 1/2 size, and 1/4 size) and their loss values are added together. It is reported by the authors that, without multi-scale design in the discriminator, repeated patterns are often observed in the generated images. Also, an additional term, called the <strong>feature matching loss</strong>, is added to the final discriminator loss, as shown the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fecc2df3-fec9-4df6-bf1a-0cc22388f685.png" style="width:28.50em;height:3.08em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/8df2c128-fd0c-4a48-b0eb-e61089bf1d48.png" style="width:6.17em;height:1.17em;"/> measures the L1-loss between the feature maps of the generated and real images at multiple layers in the discriminator networks. It forces the generator to approximate the real data at different scales, thereby generating more realistic images.</p>
<p>Sometimes, several objects with the same label may find their way together, which makes it difficult for the generator to correctly distinguish these objects. It would help if the generator knew which pixels belong to which object compared to which class they belong to. Therefore, in pix2pixHD, an <strong>instance boundary map</strong> (which is a binary map denoting the boundaries of all the objects) is <span>channel-wise </span>concatenated to the <span>semantic </span>label map before it's fed into the generator. Similarly, the instance boundary map is also concatenated to the semantic label map and the image (the generated one or the real one), before being fed into the discriminator.</p>
<p>Furthermore, in order to make it easier to manipulate the attributes of the generated images, pix2pixHD uses an additional <strong>encoder</strong> to extract features from the real images and performs instance-wise average pooling (averages all the pixels in one object and then broadcasts back to these pixels) on the features. These features are also part of the input to the generator. K-means clustering is performed on the features of all the objects in each class, and several available textures or colors can be chosen for the objects during inference.</p>
<p>We will not dive deep into the specific architecture designs of pix2pixHD since the main structure of its source code is similar to pix2pix. You can check out the source code if you're interested.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p>The training of pix2pixHD is both time- and memory-consuming. It requires about 24 GB GPU memory to train 2,048x1,024 images. Therefore, we will only train on a 1,024x512 resolution in order to fit this on a single graphic card.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>NVIDIA has already open-sourced the full source code of pix2pixHD for PyTorch. All we need to do is download the source code and dataset to produce our own high resolution synthesized images. Let's do this now:</p>
<ol>
<li>Install the prerequisites (dominate and apex). <span>We previously installed the</span><span> </span><kbd>dominate</kbd><span> </span><span>library. <strong>Apex</strong> is a mixed precision and distributed training library that's developed by NVIDIA. </span></li>
<li>Use <span><strong>Automatic Mixed Precision</strong> (<strong>AMP</strong>) to reduce the GPU memory consumption (or even the training time) during training by replacing the standard floating-point values with lower bit floats. </span></li>
<li><span>Open a Terminal in Ubuntu and type in the following scripts to install </span><kbd>apex</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/NVIDIA/apex</strong><br/><strong>$ cd apex</strong><br/><strong>$ pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" .</strong></pre>
<ol start="4">
<li><span>Download the source code of pix2pixHD (also available under the code repository for this chapter):</span></li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/NVIDIA/pix2pixHD</strong></pre>
<ol start="5">
<li><span>Use the </span><strong>Cityscapes</strong><span> dataset to train the pix2pixHD model. It is available at </span><a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a><span> and you'll need to register first before being granted access to the download links. We need to download the </span><kbd>gtFine_trainvaltest.zip</kbd><span> </span><em>(</em><span>241 MB</span><em>)</em><span> and </span><kbd>leftImg8bit_trainvaltest.zip</kbd><span> </span><em>(</em><span>11 GB</span><em>)</em><span> files for this experiment.</span></li>
<li>When the download is finished, we need to reorganize the images so that the training script can pick up the images correctly:</li>
</ol>
<ul>
<li style="padding-left: 30px">Put all the image files in the <kbd>gtFine/train/*</kbd> folders that end with <kbd>_gtFine_instanceIds.png</kbd> into the <kbd>datasets/cityscapes/train_inst</kbd> directory.</li>
<li style="padding-left: 30px">Put all the <span>image</span> files <span>in the </span><kbd>gtFine/train/*</kbd><span> folders </span>that end with <kbd>_gtFine_labelIds.png</kbd> into the <kbd>datasets/cityscapes/train_label</kbd> directory.</li>
<li style="padding-left: 30px">Put all the image files in the <kbd>leftImg8bit/train/*</kbd> folders that end with <kbd>_leftImg8bit.png</kbd> into the <kbd>datasets/cityscapes/train_img</kbd> directory.</li>
</ul>
<ol start="7">
<li>The test and validation sets can be ignored since we only need the training images. There should be 2,975 images in each of the training folders.</li>
<li>Run <kbd>scripts/train_512p.sh</kbd> to start the training process or simply type the following in the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python train.py --name label2city_512p</strong></pre>
<p>All the intermediate results (arguments taken, generated images, logging information, and model files) will be saved in the <kbd>checkpoints/label2city_512p</kbd> folder. You can always check the <kbd>checkpoints/label2city_512p/web/index.html</kbd> file in your favorite browser or directly check out the images in the <kbd>checkpoints/label2city_512p/web/images</kbd> folder to monitor the training process.</p>
<p>Here are the results after 35 epochs of training (about 20 hours):</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/655ba229-1df2-4a56-92a5-579539a25957.png" style="width:57.08em;height:14.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Generated image after 35 epochs of training by pix2pixHD</div>
<p>Here, we can see that the model has already figured out where to put vehicles, trees, buildings, and pedestrians based on the label information from the instance map, although the objects themselves still have much to improve on in terms of appearance. It is interesting to observe that the model is trying to put road lines in the correct positions and that the badge of the car that the images have been captured from has an almost perfect reflection on the front hood (which makes sense since the badge and the hood appear in every image).</p>
<p>If you are willing to wait long enough (approximately 110 hours), the results are pretty impressive:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/fd1df132-f082-48aa-9526-74cc7e3e8450.png" style="width:57.08em;height:13.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Generated image by pix2pixHD (<span>images </span>retrieved from https://github.com/NVIDIA/pix2pixHD)</div>
<p>It costs about 8,077 MB GPU memory to train on a <span>1,024x512 resolution. When AMP is enabled (trained with <kbd>--fp16</kbd>), the GPU memory consumption starts with 7,379 MB at first and gradually increases to 7,829 MB after a few epochs, which is indeed lower than before. However, the training time is almost half as long than it is without AMP. Therefore, you should go without AMP for now, until its performance is improved in the future.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CycleGAN – image-to-image translation from unpaired collections</h1>
                </header>
            
            <article>
                
<p>You may have noticed that, when training pix2pix, we need to determine a direction (<kbd>AtoB</kbd> or <kbd>BtoA</kbd>) that the images are translated to. Does this mean that, if we want to freely translate from image set A to image set B and vice versa, we need to train two models separately? Not with CycleGAN, we say!</p>
<p>CycleGAN was proposed by Jun-Yan Zhu, Taesung Park, and Phillip Isola, et. al. in their paper, <em>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</em>. It is a bidirectional generative model based on unpaired image collections. The core idea of CycleGAN is built on the assumption of cycle consistency, which means that if we have two generative models, G and F, that translate between two sets of images, X and Y, in which Y=G(X) and X=F(Y), we can naturally assume that F(G(X)) should be very similar to X and G(F(Y)) should be very similar to Y. This means that we can train two sets of generative models at the same time that can freely translate between two sets of images.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>CycleGAN is specifically designed for unpaired image collections, which means that the training samples are not necessarily strictly paired like they were in the previous sections when we looked at pix2pix and pix2pixHD (for example, semantic segmentation maps versus street views from the same perspective, or regular maps versus satellite photos of the same location). This makes CycleGAN more than just an image-to-image translation tool. It unlocks the potential to <strong>transfer style</strong> from any kind of images to your own images, for example, turning apples into oranges, horses into zebras, photos into oil paintings, and vice versa. Here, we'll perform image-to-image translation on landscape photos and Vincent van Gogh's paintings as an example to show you how CycleGAN is designed and trained.</p>
<p>Note that, in this section, the code layout is similar to CGAN in the previous chapter. The full source code is available under the code repository for this chapter. The models are defined in <kbd>cyclegan.py</kbd>, the training process is defined in <kbd>build_gan.py</kbd>, and the main entry is located at <kbd>main.py</kbd>. The source code is based on the implementation provided by <a href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a>. It is worth mentioning that our implementation trains 1.2x faster and costs 28% less GPU memory than that implementation. Also, in the source code of pix2pix, which can be found in the first section of this chapter, an implementation of CycleGAN was provided. You may choose whichever implementation you like since there isn't much of a difference between the two.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cycle consistency-based model design</h1>
                </header>
            
            <article>
                
<p>Two pairs of generator and discriminator networks are used, with each being responsible for a translation direction. <span>In order to understand why CycleGAN is designed as such, we need to understand how the cycle consistency is constructed.</span></p>
<p>In the following diagram, the generator, <img class="fm-editor-equation" src="assets/f940b35c-4e1c-481b-94a6-ca18e1505cea.png" style="width:2.08em;height:1.08em;"/>maps sample A to sample B and its performance is measured by the discriminator, <img class="fm-editor-equation" src="assets/8cb29fbf-9cbb-4552-8a33-f05dd7f5090e.png" style="width:1.67em;height:1.08em;"/>. At the same time, another generator, <img class="fm-editor-equation" src="assets/844e96ce-5649-43b2-a58c-d543f0629aa1.png" style="width:2.08em;height:1.08em;"/>, is trained to map sample B back to sample A, whose performance is measured by the discriminator, <img class="fm-editor-equation" src="assets/999a4fc7-a3ca-4056-a923-716408abd75c.png" style="width:1.50em;height:1.08em;"/>. In this process, the distance between a generated sample, <img class="fm-editor-equation" src="assets/ec2bfd57-c769-41de-aba9-61400cf32047.png" style="width:14.00em;height:1.17em;"/>, and the corresponding original real sample, <img class="fm-editor-equation" src="assets/ecc613f1-2253-46d4-b76c-041603a3d2b0.png" style="width:1.42em;height:1.08em;"/>, tells us whether a cycle consistency exists in our model, as shown in the dotted box in the following diagram. The distance between <img class="fm-editor-equation" src="assets/deac1805-46a7-4b5c-a2df-42904c879e9a.png" style="width:7.33em;height:1.25em;"/> and <img class="fm-editor-equation" src="assets/ecc613f1-2253-46d4-b76c-041603a3d2b0.png" style="width:1.42em;height:1.08em;"/> is measured by the <strong><span class="packt_screen">cycle consistency loss</span></strong>, which takes the form of the L1-norm.</p>
<p class="mce-root"/>
<p>Besides the traditional <span class="packt_screen"><strong>adversarial loss</strong></span> (distance between <img class="fm-editor-equation" src="assets/b1979e0b-0650-450b-b9ec-aea0e7b059dc.png" style="width:5.92em;height:1.08em;"/> and 1), the <strong><span class="packt_screen">identity loss</span></strong> (which means that <img class="fm-editor-equation" src="assets/58282974-6e86-4511-a954-dddbfcfaec32.png" style="width:4.08em;height:1.17em;"/> should be very close to <img class="fm-editor-equation" src="assets/ecc613f1-2253-46d4-b76c-041603a3d2b0.png" style="width:1.42em;height:1.08em;"/> itself) is also added to help maintain the color style of the images:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-603 image-border" src="assets/dc7bfabe-2c11-425e-9ebb-793ad0ad6e00.png" style="width:42.75em;height:39.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The calculation of loss in CycleGAN. A* and B* denote real samples. Networks denoted by red boxes are updated while training the generators.</span></div>
<p>The two generator networks, <img class="fm-editor-equation" src="assets/f940b35c-4e1c-481b-94a6-ca18e1505cea.png" style="width:2.08em;height:1.08em;"/>and, are identical. The architecture of the generator network can be seen in the following diagram. The 256x256 input image is downsampled by multiple convolution layers to 64x64, processed by nine successive residual blocks, and finally upsampled by convolutions back to 256x256:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-604 image-border" src="assets/53aa9cf0-ebf0-43f4-a5b4-c1ddd3a6b58d.png" style="width:51.08em;height:20.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Generator architecture in CycleGAN</div>
<p>We'll start the code with a blank file named <kbd>cyclegan.py</kbd>, as we mentioned previously. Let's start with the imports:</p>
<pre>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import numpy as np</pre>
<p>Next, we'll create the code for the definition of the residual block, as follows:</p>
<pre>class ResidualBlock(nn.Module):<br/>    def __init__(self, channels):<br/>        super(ResidualBlock, self).__init__()<br/><br/>        block = [nn.ReflectionPad2d(1),<br/>                 nn.Conv2d(channels, channels, 3),<br/>                 nn.InstanceNorm2d(channels),<br/>                 nn.ReLU(inplace=True),<br/>                 nn.ReflectionPad2d(1),<br/>                 nn.Conv2d(channels, channels, 3),<br/>                 nn.InstanceNorm2d(channels)]<br/>         self.block = nn.Sequential(*block)<br/><br/>     def forward(self, x):<br/>         return x + self.block(x)</pre>
<p class="mce-root"/>
<p>Now, we can define the generator network, as follows:</p>
<pre>class Generator(nn.Module):<br/>    def __init__(self, channels, num_blocks=9):<br/>        super(Generator, self).__init__()<br/>        self.channels = channels<br/><br/>        model = [nn.ReflectionPad2d(3)]<br/>        model += self._create_layer(self.channels, 64, 7, stride=1, padding=0, transposed=False)<br/>        # downsampling<br/>        model += self._create_layer(64, 128, 3, stride=2, padding=1, transposed=False)<br/>        model += self._create_layer(128, 256, 3, stride=2, padding=1, transposed=False)<br/>        # residual blocks<br/>        model += [ResidualBlock(256) for _ in range(num_blocks)]<br/>        # upsampling<br/>        model += self._create_layer(256, 128, 3, stride=2, padding=1, transposed=True)<br/>        model += self._create_layer(128, 64, 3, stride=2, padding=1, transposed=True)<br/>        # output<br/>        model += [nn.ReflectionPad2d(3),<br/>                  nn.Conv2d(64, self.channels, 7),<br/>                  nn.Tanh()]<br/><br/>        self.model = nn.Sequential(*model)<br/><br/>    def _create_layer(self, size_in, size_out, kernel_size, stride=2, padding=1, transposed=False):<br/>        layers = []<br/>        if transposed:<br/>            layers.append(nn.ConvTranspose2d(size_in, size_out, kernel_size, stride=stride, padding=padding, output_padding=1))<br/>        else:<br/>            layers.append(nn.Conv2d(size_in, size_out, kernel_size, stride=stride, padding=padding))<br/>        layers.append(nn.InstanceNorm2d(size_out))<br/>        layers.append(nn.ReLU(inplace=True))<br/>        return layers<br/><br/>    def forward(self, x):<br/>        return self.model(x)</pre>
<p class="mce-root">As you may have noticed, here, we used <kbd>torch.nn.InstanceNorm2d</kbd> instead of <kbd>torch.nn.BatchNorm2d</kbd><span>. The former normalization layer is more suitable for style transfer.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Similarly, two identical discriminator networks are used in CycleGAN and their relationship can be seen in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-798 image-border" src="assets/eb67f62f-54fd-4654-916a-6ee30cc817ab.png" style="width:29.42em;height:11.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Relationship between two discriminator networks in CycleGAN. Networks denoted by red boxes are updated during training.</div>
<p>The architecture of the discriminator network is almost the same as it is in pix2pix (which is called PatchGAN), except that the input image has a depth channel of 3, instead of 6, and <span><kbd>torch.nn.BatchNorm2d</kbd> is replaced with <kbd>torch.nn.InstanceNorm2d</kbd>.</span></p>
<p>The code for the definition of the discriminator network is as follows:</p>
<pre>class Discriminator(nn.Module):<br/>    def __init__(self, channels):<br/>        super(Discriminator, self).__init__()<br/>        self.channels = channels<br/><br/>        self.model = nn.Sequential(<br/>            *self._create_layer(self.channels, 64, 2, normalize=False),<br/>            *self._create_layer(64, 128, 2),<br/>            *self._create_layer(128, 256, 2),<br/>            *self._create_layer(256, 512, 1),<br/>            nn.Conv2d(512, 1, 4, stride=1, padding=1)<br/>        )<br/><br/>    def _create_layer(self, size_in, size_out, stride, normalize=True):<br/>        layers = [nn.Conv2d(size_in, size_out, 4, stride=stride, padding=1)]<br/>        if normalize:<br/>            layers.append(nn.InstanceNorm2d(size_out))<br/>        layers.append(nn.LeakyReLU(0.2, inplace=True))<br/>        return layers<br/><br/>    def forward(self, x):<br/>        return self.model(x)</pre>
<p>Now, let's learn how the model can be trained and evaluated.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training and evaluation</h1>
                </header>
            
            <article>
                
<p>Now, we'll create the <kbd>build_gan.py</kbd> file. As usual, we'll begin with the imports:</p>
<pre>import itertools<br/>import os<br/>import time<br/><br/>from datetime import datetime<br/><br/>import numpy as np<br/>import torch<br/>import torchvision.utils as vutils<br/>import utils<br/><br/>from cyclegan import Generator as cycG<br/>from cyclegan import Discriminator as cycD</pre>
<p>We'll need a function to initialize the weights:</p>
<pre>def _weights_init(m):<br/>    classname = m.__class__.__name__<br/>    if classname.find('Conv') != -1:<br/>        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)<br/>    elif classname.find('BatchNorm') != -1:<br/>        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)<br/>        torch.nn.init.constant_(m.bias.data, 0.0)</pre>
<p>Now, we will create the <kbd>Model</kbd> class:</p>
<pre>class Model(object):<br/>    def __init__(self,<br/>                 name,<br/>                 device,<br/>                 data_loader,<br/>                 test_data_loader,<br/>                 channels,<br/>                 img_size,<br/>                 num_blocks):<br/>        self.name = name<br/>        self.device = device<br/>        self.data_loader = data_loader<br/>        self.test_data_loader = test_data_loader<br/>        self.channels = channels<br/>        self.img_size = img_size<br/>        self.num_blocks = num_blocks<br/>        assert self.name == 'cyclegan'<br/>        self.netG_AB = cycG(self.channels, self.num_blocks)<br/>        self.netG_AB.apply(_weights_init)<br/>        self.netG_AB.to(self.device)<br/>        self.netG_BA = cycG(self.channels, self.num_blocks)<br/>        self.netG_BA.apply(_weights_init)<br/>        self.netG_BA.to(self.device)<br/>        self.netD_A = cycD(self.channels)<br/>        self.netD_A.apply(_weights_init)<br/>        self.netD_A.to(self.device)<br/>        self.netD_B = cycD(self.channels)<br/>        self.netD_B.apply(_weights_init)<br/>        self.netD_B.to(self.device)<br/>        self.optim_G = None<br/>        self.optim_D_A = None<br/>        self.optim_D_B = None<br/>        self.loss_adv = torch.nn.MSELoss()<br/>        self.loss_cyc = torch.nn.L1Loss()<br/>        self.loss_iden = torch.nn.L1Loss()<br/><br/>    @property<br/>    def generator_AB(self):<br/>        return self.netG_AB<br/><br/>    @property<br/>    def generator_BA(self):<br/>        return self.netG_BA<br/><br/>    @property<br/>    def discriminator_A(self):<br/>        return self.netD_A<br/><br/>    @property<br/>    def discriminator_B(self):<br/>        return self.netD_B<br/><br/>    def create_optim(self, lr, alpha=0.5, beta=0.999):<br/>        self.optim_G = torch.optim.Adam(itertools.chain(self.netG_AB.parameters(), self.netG_BA.parameters()),<br/>                                        lr=lr,<br/>                                        betas=(alpha, beta))<br/>        self.optim_D_A = torch.optim.Adam(self.netD_A.parameters(),<br/>                                          lr=lr,<br/>                                          betas=(alpha, beta))<br/>        self.optim_D_B = torch.optim.Adam(self.netD_B.parameters(),<br/>                                          lr=lr,<br/>                                          betas=(alpha, beta))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The training processes for the generator and discriminator networks were shown previously. Here, we will dive into the implementation of <kbd>build_gan.train()</kbd>.</p>
<p>First, we need to train the generator networks:</p>
<pre>def train(self,<br/>          epochs,<br/>          log_interval=100,<br/>          out_dir='',<br/>          verbose=True):<br/>        self.netG_AB.train()<br/>        self.netG_BA.train()<br/>        self.netD_A.train()<br/>        self.netD_B.train()<br/>        lambda_cyc = 10<br/>        lambda_iden = 5<br/>        real_label = torch.ones((self.data_loader.batch_size, 1, self.img_size//2**4, self.img_size//2**4), device=self.device)<br/>        fake_label = torch.zeros((self.data_loader.batch_size, 1, self.img_size//2**4, self.img_size//2**4), device=self.device)<br/>        image_buffer_A = utils.ImageBuffer()<br/>        image_buffer_B = utils.ImageBuffer()<br/>        total_time = time.time()<br/>        for epoch in range(epochs):<br/>            batch_time = time.time()<br/>            for batch_idx, data in enumerate(self.data_loader):<br/>                real_A = data['trainA'].to(self.device)<br/>                real_B = data['trainB'].to(self.device)<br/><br/>                # Train G<br/>                self.optim_G.zero_grad()<br/><br/>                # adversarial loss<br/>                fake_B = self.netG_AB(real_A)<br/>                _loss_adv_AB = self.loss_adv(self.netD_B(fake_B),  <br/>                  real_label)<br/>                fake_A = self.netG_BA(real_B)<br/>                _loss_adv_BA = self.loss_adv(self.netD_A(fake_A), <br/>                  real_label)<br/>                adv_loss = (_loss_adv_AB + _loss_adv_BA) / 2<br/><br/>                # cycle loss<br/>                recov_A = self.netG_BA(fake_B)<br/>                _loss_cyc_A = self.loss_cyc(recov_A, real_A)<br/>                recov_B = self.netG_AB(fake_A)<br/>                _loss_cyc_B = self.loss_cyc(recov_B, real_B)<br/>                cycle_loss = (_loss_cyc_A + _loss_cyc_B) / 2<br/><br/>                # identity loss<br/>                _loss_iden_A = self.loss_iden(self.netG_BA(real_A), real_A)<br/>                _loss_iden_B = self.loss_iden(self.netG_AB(real_B), real_B)<br/>                iden_loss = (_loss_iden_A + _loss_iden_B) / 2<br/><br/>                g_loss = adv_loss + lambda_cyc * cycle_loss + <br/>                  lambda_iden * iden_loss<br/>                g_loss.backward()<br/>                self.optim_G.step()</pre>
<p>Then, we need to train the discriminator networks:</p>
<pre>                # Train D_A<br/>                self.optim_D_A.zero_grad()<br/><br/>                _loss_real = self.loss_adv(self.netD_A(real_A), real_label)<br/>                fake_A = image_buffer_A.update(fake_A)<br/>                _loss_fake = self.loss_adv(self.netD_A(fake_A.detach()), <br/>                 fake_label)<br/>                d_loss_A = (_loss_real + _loss_fake) / 2<br/><br/>                d_loss_A.backward()<br/>                self.optim_D_A.step()<br/><br/>                # Train D_B<br/>                self.optim_D_B.zero_grad()<br/><br/>                _loss_real = self.loss_adv(self.netD_B(real_B), real_label)<br/>                fake_B = image_buffer_B.update(fake_B)<br/>                _loss_fake = self.loss_adv(self.netD_B(fake_B.detach()), <br/>                  fake_label)<br/>                d_loss_B = (_loss_real + _loss_fake) / 2<br/><br/>                d_loss_B.backward()<br/>                self.optim_D_B.step()<br/><br/>                d_loss = (d_loss_A + d_loss_B) / 2</pre>
<p>The last variable, <kbd>d_loss</kbd>, is simply for logging and has been omitted here. You can refer to the source code file for this chapter if you want to find out more about logging printing and image exporting:</p>
<pre>                if verbose and batch_idx % log_interval == 0 and batch_idx &gt; 0:<br/>                    print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(<br/>                          epoch, batch_idx, len(self.data_loader),<br/>                          d_loss.mean().item(),<br/>                          g_loss.mean().item(),<br/>                          time.time() - batch_time))<br/>                    with torch.no_grad():<br/>                        imgs = next(iter(self.test_data_loader))<br/>                        _real_A = imgs['testA'].to(self.device)<br/>                        _fake_B = self.netG_AB(_real_A)<br/>                        _real_B = imgs['testB'].to(self.device)<br/>                        _fake_A = self.netG_BA(_real_B)<br/>                        viz_sample = torch.cat(<br/>                            (_real_A, _fake_B, _real_B, _fake_A), 0)<br/>                        vutils.save_image(viz_sample,<br/>                                          os.path.join(<br/>                                              out_dir, 'samples_{}_{}.png'.format(epoch, batch_idx)),<br/>                                          nrow=self.test_data_loader.batch_size,<br/>                                          normalize=True)<br/>                    batch_time = time.time()<br/><br/>            self.save_to(path=out_dir, name=self.name, verbose=False)<br/>        if verbose:<br/>            print('Total train time: {:.2f}'.format(time.time() - total_time))<br/>   def eval(self,<br/>             batch_size=None):<br/>        self.netG_AB.eval()<br/>        self.netG_BA.eval()<br/>        self.netD_A.eval()<br/>        self.netD_B.eval()<br/>        if batch_size is None:<br/>            batch_size = self.test_data_loader.batch_size<br/><br/>        with torch.no_grad():<br/>            for batch_idx, data in enumerate(self.test_data_loader):<br/>                _real_A = data['testA'].to(self.device)<br/>                _fake_B = self.netG_AB(_real_A)<br/>                _real_B = data['testB'].to(self.device)<br/>                _fake_A = self.netG_BA(_real_B)<br/>                viz_sample = torch.cat((_real_A, _fake_B, _real_B, _fake_A), 0)<br/>                vutils.save_image(viz_sample,<br/>                                  'img_{}.png'.format(batch_idx),<br/>                                  nrow=batch_size,<br/>                                  normalize=True)<br/><br/>    def save_to(self,<br/>                path='',<br/>                name=None,<br/>                verbose=True):<br/>        if name is None:<br/>            name = self.name<br/>        if verbose:<br/>            print('\nSaving models to {}_G_AB.pt and such ...'.format(name))<br/>        torch.save(self.netG_AB.state_dict(), os.path.join(<br/>            path, '{}_G_AB.pt'.format(name)))<br/>        torch.save(self.netG_BA.state_dict(), os.path.join(<br/>            path, '{}_G_BA.pt'.format(name)))<br/>        torch.save(self.netD_A.state_dict(), os.path.join(<br/>            path, '{}_D_A.pt'.format(name)))<br/>        torch.save(self.netD_B.state_dict(), os.path.join(<br/>            path, '{}_D_B.pt'.format(name)))<br/><br/>    def load_from(self,<br/>                  path='',<br/>                  name=None,<br/>                  verbose=True):<br/>        if name is None:<br/>            name = self.name<br/>        if verbose:<br/>            print('\nLoading models from {}_G_AB.pt and such ...'.format(name))<br/>        ckpt_G_AB = torch.load(os.path.join(path, '{}_G_AB.pt'.format(name)))<br/>        if isinstance(ckpt_G_AB, dict) and 'state_dict' in ckpt_G_AB:<br/>            self.netG_AB.load_state_dict(ckpt_G_AB['state_dict'], strict=True)<br/>        else:<br/>            self.netG_AB.load_state_dict(ckpt_G_AB, strict=True)<br/>        ckpt_G_BA = torch.load(os.path.join(path, '{}_G_BA.pt'.format(name)))<br/>        if isinstance(ckpt_G_BA, dict) and 'state_dict' in ckpt_G_BA:<br/>            self.netG_BA.load_state_dict(ckpt_G_BA['state_dict'], strict=True)<br/>        else:<br/>            self.netG_BA.load_state_dict(ckpt_G_BA, strict=True)<br/>        ckpt_D_A = torch.load(os.path.join(path, '{}_D_A.pt'.format(name)))<br/>        if isinstance(ckpt_D_A, dict) and 'state_dict' in ckpt_D_A:<br/>            self.netD_A.load_state_dict(ckpt_D_A['state_dict'], strict=True)<br/>        else:<br/>            self.netD_A.load_state_dict(ckpt_D_A, strict=True)<br/>        ckpt_D_B = torch.load(os.path.join(path, '{}_D_B.pt'.format(name)))<br/>        if isinstance(ckpt_D_B, dict) and 'state_dict' in ckpt_D_B:<br/>            self.netD_B.load_state_dict(ckpt_D_B['state_dict'], strict=True)<br/>        else:            self.netD_B.load_state_dict(ckpt_D_B, strict=True)</pre>
<p>Here, as suggested in the paper, we update the discriminators by randomly picking an image from the history of generated images, rather than the fake samples in real-time. The <span>history of generated images is maintained by the <kbd>ImageBuffer</kbd> class, which is defined as follows. Copy the <kbd>utils.py</kbd> file from the previous chapter and add the <kbd>ImageBuffer</kbd> class to it:<br/></span></p>
<pre>class ImageBuffer(object):<br/>    def __init__(self, depth=50):<br/>        self.depth = depth<br/>        self.buffer = []<br/><br/>    def update(self, image):<br/>        if len(self.buffer) == self.depth:<br/>            i = random.randint(0, self.depth-1)<br/>            self.buffer[i] = image<br/>        else:<br/>            self.buffer.append(image)<br/>        if random.uniform(0,1) &gt; 0.5:<br/>            i = random.randint(0, len(self.buffer)-1)<br/>            return self.buffer[i]<br/>        else:<br/>            return image</pre>
<p>We also need to write a custom dataset reader that picks up unpaired images from separate folders. Place the following content into a new file called <kbd>datasets.py</kbd>:</p>
<pre>import glob<br/>import random<br/>import os<br/><br/>import torchvision<br/><br/>from torch.utils.data import Dataset<br/>from PIL import Image<br/><br/><br/>class ImageDataset(Dataset):<br/>    def __init__(self, root_dir, transform=None, unaligned=False, mode='train'):<br/>        self.transform = torchvision.transforms.Compose(transform)<br/>        self.unaligned = unaligned<br/>        self.train = (mode == 'train')<br/><br/>        self.files_A = sorted(glob.glob(os.path.join(root_dir, '%sA' % mode) + '/*.*'))<br/>        self.files_B = sorted(glob.glob(os.path.join(root_dir, '%sB' % mode) + '/*.*'))<br/><br/>    def __getitem__(self, index):<br/>        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))<br/><br/>        if self.unaligned:<br/>            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))<br/>        else:<br/>            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))<br/><br/>        if self.train:<br/>            return {'trainA': item_A, 'trainB': item_B}<br/>        else:<br/>            return {'testA': item_A, 'testB': item_B}<br/><br/>    def __len__(self):<br/>        return max(len(self.files_A), len(self.files_B))</pre>
<p>The shapes of the paintings and photos are not always square. Therefore, we need to crop 256x256 patches from the original images. We preprocess the data (<strong><span>data augmentation</span></strong>) in <kbd>main.py</kbd>. Here, we're only showing a part of the code. You can find the rest of the code in the <kbd>main.py</kbd> file:</p>
<pre>def main():<br/>    device = torch.device("cuda:0" if FLAGS.cuda else "cpu")<br/><br/>    if FLAGS.train:<br/>        print('Loading data...\n')<br/>        transform = [transforms.Resize(int(FLAGS.img_size*1.12), Image.BICUBIC),<br/>                     transforms.RandomCrop((FLAGS.img_size, FLAGS.img_size)),<br/>                     transforms.RandomHorizontalFlip(),<br/>                     transforms.ToTensor(),<br/>                     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]<br/>        dataloader = DataLoader(ImageDataset(os.path.join(FLAGS.data_dir, FLAGS.dataset),<br/>                                             transform=transform, unaligned=True, mode='train'),<br/>                                batch_size=FLAGS.batch_size, shuffle=True, num_workers=2)<br/>        test_dataloader = DataLoader(ImageDataset(os.path.join(FLAGS.data_dir, FLAGS.dataset),<br/>                                                  transform=transform, unaligned=True, mode='test'),<br/>                                     batch_size=FLAGS.test_batch_size, shuffle=True, num_workers=2)<br/><br/>        print('Creating model...\n')<br/>        model = Model(FLAGS.model, device, dataloader, test_dataloader, FLAGS.channels, FLAGS.img_size, FLAGS.num_blocks)<br/>        model.create_optim(FLAGS.lr)<br/><br/>        # Train<br/>        model.train(FLAGS.epochs, FLAGS.log_interval, FLAGS.out_dir, True)</pre>
<p>Don't forget to adjust the argument parsing for CycleGAN. Remember, you should change the <kbd>--data_dir</kbd> default so that it matches your own setup, so be sure to include the following on the command line:</p>
<pre>    parser.add_argument('--data_dir', type=str, default='/media/john/HouseOfData/image_transfer', help='Directory for dataset.')<br/>    parser.add_argument('--dataset', type=str, default='vangogh2photo', help='Dataset name.')<br/>    ...<br/>    parser.add_argument('--num_blocks', type=int, default=9, help='number of residual blocks')</pre>
<p>Now, it's time to download the datasets and start having fun! Go to <a href="https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets">https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets</a> to manually download the dataset files. Alternatively, you can use the <kbd>datasets/download_cyclegan_dataset.sh</kbd> script that's located in the source code of pix2pix to download the <kbd>vangogh2photo.zip</kbd> file, which is about 292 MB in size and contains 400 Van Gogh paintings and 7,038 photos (<span>6,287 in train and 751 in test). When the download is finished, extract the images to a folder (for example, an external hard drive such as</span> <kbd>/media/john/HouseOfData/image_transfer</kbd><span>).</span></p>
<p>Open a Terminal and type the following script to start training:</p>
<pre><strong>$ python main.py --dataset vangogh2photo</strong></pre>
<p>It takes about 10 hours to train CycleGAN for 20 epochs and costs about 4,031 MB GPU memory on a GTX 1080Ti graphics card. Some of the results can be seen in the following image. Here, we can see that the style transfer capability of CycleGAN is pretty amazing. You can also check out this site to learn about more applications of CycleGAN: <a href="https://junyanz.github.io/CycleGAN">https://junyanz.github.io/CycleGAN</a>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/74b56dc8-0c58-4e0d-81cb-ed1689b9868e.png" style="width:44.17em;height:44.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Generated images by CycleGAN. Top two rows: Painting to photo; Bottom two rows: Photo to painting.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have been getting familiar with image generation for several chapters now. Although it is always challenging and fulfilling to successfully train GANs to generate amazing images, we should recognize that GANs can also be used to fix things and restore images.</p>
<p>In the next chapter, we will explore the generative power of GANs to address some of the challenging problems in image restoration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Furthering reading</h1>
                </header>
            
            <article>
                
<ol>
<li>Le J. (May 3, 2018) <em>How to do Semantic Segmentation using Deep learning</em>. Retrieved from <a href="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef">https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef</a>.</li>
<li>Rainy J. (Feb 12, 2018) <em>Stabilizing neural style-transfer for video</em>. Retrieved from <a href="https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42">https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42</a>.</li>
<li>Isola P, Zhu JY, Zhou T, Efros A. (2017) <em>Image-to-Image Translation with Conditional Adversarial Networks</em>. CVPR.</li>
<li>Agustinus K. (Feb 9, 2017) <em>Why does L2 reconstruction loss yield blurry images?</em> Retrieved from <a href="https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry">https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry</a>.</li>
<li>Chan T F, Wong C K. (1998) <em>Total Variation Blind Deconvolution. IEEE Transactions on Image Processing</em>. 7(3): 370-375.</li>
<li>Wang T C, Liu M Y, Zhu J Y, et. al. (2018) <em>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</em>. CVPR.</li>
<li>Zhu J Y, Park T, Isola P, et. al. (2017) <em>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</em>. ICCV.</li>
</ol>


            </article>

            
        </section>
    </body></html>