<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Pose Estimation on 3D models Using ConvNets</h1>
                </header>
            
            <article>
                
<p>Welcome to our chapter on human pose estimation. In this chapter, we will be building a neural network that will predict 3D human poses using 2D images. We will do this with the help of transfer learning by using the VGG16 model architecture and modifying it accordingly for our current problem. By the end of this chapter, you will have a <strong>deep learning</strong> (<strong>DL</strong>) model that does a really good job of predicting human poses.</p>
<p class="mce-root"><strong>Visual effects</strong> (<strong>VFX</strong>) in movies are expensive. They involve using a lot of expensive sensors that will be placed on the body of the actor when shooting. The information from these sensors will then be used to build visual effects, all of which ends up being super expensive. We have been asked <span>(in this hypothetical use case)</span> by a major movie studio whether we can help their graphics department build cheaper and better visual effects <span>by building a human pose estimator, which they will use to better estimate poses on the screen while editing.</span></p>
<p>For this task, we will be using images from <strong>Frames Labeled In Cinema</strong> (<strong>FLIC</strong>). These images are not ready to be used for modeling just yet. So, get ready to spend a bit more time on preparing the image data in this chapter. Also, we will only be estimating the pose of arms, shoulders, and the head.</p>
<p>In this chapter, we'll learn about the following topics:</p>
<ul>
<li>Processing/preparing images for pose estimation</li>
<li>The VGG16 model</li>
<li>Transfer learning</li>
<li>Building and understanding the training loop</li>
<li>Testing the model</li>
</ul>
<p class="mce-root"/>
<p><span>It would be better if you implement the code snippets as you go along in this chapter, either in a </span>Jupyter<span> notebook or any source code editor</span><span>. This will make it easier for you to follow along, as well as understand what each part of the code does.</span></p>
<p>All of the Python files and the Jupyter Notebooks for this chapter can be found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12</a>.<a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter%2013"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code implementation</h1>
                </header>
            
            <article>
                
<p>In this exercise, we will be using the Keras deep learning library, which is a high-level neural network API capable of running on top of TensorFlow, Theano, and CNTK.</p>
<div class="packt_tip"><span>If you ever have a question related to Keras, r</span>efer to this easy-to-understand Keras documentation at <a href="https://keras.io">https://keras.io</a>.<a href="https://keras.io"/></div>
<p>Please download the <kbd>Chapter12</kbd> folder from GitHub before moving forward with this chapter. </p>
<p>This project involves downloading files from various sources that will be called inside the scripts. To make sure that the Python scripts or the Jupyter Notebook have no issues locating the downloaded files, follow these steps:</p>
<ol>
<li>Open a Terminal and change your directory by using the <kbd>cd</kbd> command in the <kbd>Chapter12</kbd> folder.</li>
<li>Download the <kbd>FLIC-full</kbd> data file with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>wget http://vision.grasp.upenn.edu/video/FLIC-full.zip</strong></pre>
<ol start="3">
<li>Unzip the ZIP file with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>unzip FLIC-full.zip</strong></pre>
<ol start="4">
<li>Remove the ZIP file with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>rm -rf FLIC-full.zip</strong></pre>
<ol start="5">
<li><span>Change directories </span>in the <kbd>FLIC-full</kbd> folder by using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd<span> </span><span>FLIC-full</span></strong></pre>
<p class="mce-root"/>
<ol start="6">
<li>Download the file containing the training indices:</li>
</ol>
<pre style="padding-left: 60px"><strong>wget http://cims.nyu.edu/~tompson/data/tr_plus_indices.mat</strong></pre>
<ol start="7">
<li><span>Change the directory</span> back to the <kbd>Chapter12</kbd> folder.</li>
<li>Launch your Jupyter Notebook or run the Python scripts from the <kbd>Chapter12</kbd> directory.</li>
</ol>
<div class="packt_infobox">Further information on the <kbd>FLIC-full</kbd> data folder can be found at <a href="https://bensapp.github.io/flic-dataset.html">https://bensapp.github.io/flic-dataset.html</a>.<a href="https://bensapp.github.io/flic-dataset.html"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing the dependencies</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">We will be using <kbd>numpy</kbd>, <kbd>matplotlib</kbd>, <kbd>keras</kbd>, <kbd>tensorflow</kbd>, and the <kbd>tqdm</kbd> package in this exercise. Here, TensorFlow is used as the backend for Keras. You can install these packages with <kbd>pip</kbd>. For the MNIST data, we will be using the dataset that's available in the <kbd>keras</kbd> module with a simple <kbd>import</kbd>:</p>
<pre>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>import os<br/>import random<br/>import glob<br/>import h5py<br/>from scipy.io import loadmat<br/>import numpy as np<br/>import pandas as pd<br/>import cv2 as cv<br/>from __future__ import print_function<br/><br/>from sklearn.model_selection import train_test_split<br/><br/>from keras.models import Sequential, Model<br/>from keras.layers.core import Flatten, Dense, Dropout<br/>from keras.optimizers import Adam<br/>from keras import backend as K<br/>from keras import applications<br/>K.clear_session()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It is important that you set <kbd>seed</kbd> for reproducibility:</p>
<pre><strong># set seed for reproducibility</strong><br/>seed_val = 9000<br/>np.random.seed(seed_val)<br/>random.seed(seed_val)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring and pre-processing the data</h1>
                </header>
            
            <article>
                
<p>With the <kbd>FLIC-full</kbd> data folder downloaded and unpacked, inside the <kbd><span>FLIC-full</span></kbd> folder you should find <span>the <kbd>tr_plus_indices.mat</kbd> and <kbd>examples.mat</kbd> MATLAB files, </span>and also the folder named <kbd>images</kbd>, inside which are the images that will be used in this project.</p>
<p>You will find that the images have been captured from movies such as <em>2 Fast 2 Furious</em>, <em>Along Came Polly</em>, <em>American Wedding</em>, and a few others. Each of these images is 480*720 px in size. These images are nothing but screenshots of scenes involving actors from the selected movies, which we will use for pose estimation.</p>
<p>Let's load the MATLAB file <kbd>examples.mat</kbd>. We will do this with the help of the<span> </span><kbd>loadmat</kbd> module, which we have imported already, along with other imports. Also, let's print out some of the information from this file:</p>
<pre><strong># load the examples file</strong><br/>examples = loadmat('FLIC-full/examples.mat')<br/><br/><strong># print type of the loaded file</strong><br/>print('examples variable is of', type(examples))<br/><br/><strong># print keys in the dictionary examples</strong><br/>print('keys in the dictionary examples:\n', examples.keys())ut</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/218fec22-6838-416c-af68-de63f548ad27.png" style="width:38.00em;height:4.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12.1: Example file information from printout 1</div>
<p><span>From the printout, we can see that the MATLAB file has been loaded as a dictionary with four keys, one of which is the one we need: the <kbd>examples</kbd> key. Let's see what this key holds:</span></p>
<pre><strong># print type and shape of values in examples key</strong><br/>print('Shape of value in examples key: ',examples['examples'].shape)<br/><br/><strong># print examples</strong><br/>print('Type: ',type(examples['examples']))<br/><br/><strong># reshape the examples array</strong> <br/>examples = examples['examples'].reshape(-1,)<br/><br/><strong># print shape of examples array</strong><br/>print("Shape of reshaped 'examples' array:", examples.shape)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9b8d6cd-c3f0-4827-884a-2e649bbb0a4b.png" style="width:30.25em;height:4.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12.2: <span>Example file information from printout 1</span></div>
<p class="mce-root">The notable thing here is that the value of the <kbd>examples</kbd> key is a numpy array of shape (1, 20928). You will also see that the array has been reshaped to shape <kbd>(20928,)</kbd>. The <kbd>examples</kbd> key contains the IDs of the images (in the <kbd>images</kbd> folder) and the corresponding pose coordinates that can be used for modeling. </p>
<p>Let's print out an image ID and its corresponding coordinates array with its shape. The image ID we need is stored at index <kbd>3</kbd>, and the corresponding coordinates are at index <kbd>2</kbd>. Let's print these out:</p>
<pre>print('Coordinates at location/index 3 of example 0:\n' ,examples[0][2].T)<br/><br/>print('\n Data type in which the coordinates are stored: ',type(examples[0][2]))<br/><br/>print('\n Shape of the coordinates:', examples[0][2].shape)<br/><br/>print('\n Name of the image file the above coordinates correspond to :\n ',examples[0][3][0])</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/212857ff-d2e7-4450-bcdc-f2048900451f.png" style="width:46.08em;height:48.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12.3: <span>Example file information from printout 2</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From the preceding screenshot, we can see that the coordinates array is of shape (2,29):</p>
<pre><strong># each coordinate corresponds to the the below listed body joints/locations and in the same order</strong><br/>joint_labels = ['lsho', 'lelb', 'lwri', 'rsho', 'relb', 'rwri', 'lhip',<br/>               'lkne', 'lank', 'rhip', 'rkne', 'rank', 'leye', 'reye',<br/>                'lear', 'rear', 'nose', 'msho', 'mhip', 'mear', 'mtorso',<br/>                'mluarm', 'mruarm', 'mllarm', 'mrlarm', 'mluleg', 'mruleg',<br/>                'mllleg', 'mrlleg']<br/><br/><strong># print joint_labels</strong><br/>print(joint_labels)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c0036acc-54ae-44b1-8f58-654bf1668521.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.4: List of joint labels</span></div>
<p><span>But, if you look back at the coordinates array that we printed in the preceding screenshot, out of the 29 coordinates, we only have information on 11 of the body joints/locations. These are as follows:</span></p>
<pre><strong># print list of known joints</strong><br/>known_joints = [x for i,x in enumerate(joint_labels) if i in np.r_[0:7, 9, 12:14, 16]]<br/>print(known_joints)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/24a820f3-2079-4162-9183-9a3a15431900.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.5: List of joint labels with coordinates </span></div>
<p>For the purpose of this project, we only need information on the following body joints/locations:</p>
<pre><strong># print needed joints for the task</strong><br/>target_joints = ['lsho', 'lelb', 'lwri', 'rsho', 'relb',<br/>                 'rwri', 'leye', 'reye', 'nose']<br/>print('Joints necessary for the project:\n', target_joints)<br/><br/><strong># print the indices of the needed joints in the coordinates array</strong><br/>joints_loc_id = np.r_[0:6, 12:14, 16]<br/>print('\nIndices of joints necessary for the project:\n',joints_loc_id)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/33982f76-450e-4511-9f30-0b326d58db3f.png"/></p>
<div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.6: Required joints and their indices in the array </span></div>
</div>
<div class="packt_infobox"><kbd>lsho</kbd>: Left shoulder<br/>
<kbd>lelb</kbd>: Left elbow<br/>
<kbd>lwri</kbd>: Left wrist<br/>
<kbd>rsho</kbd>: Right shoulder<br/>
<kbd>relb</kbd>: Right elbow<br/>
<kbd>rwri</kbd>: Right wrist<br/>
<kbd>leye</kbd>: Left eye<br/>
<kbd>reye</kbd>: Right eye<br/>
<kbd>nose</kbd>: Nose</div>
<p>Now, let's define a function that takes in a dictionary of nine joint labels and coordinates and returns a list with seven coordinates (<em>7</em> (<em>x</em>,<em>y</em>) pairs). The reason for seven coordinates is that the <kbd>leye</kbd>, <kbd>reye</kbd>, and the <kbd>nose</kbd> coordinates are converted into one head coordinate when we take the mean across them:</p>
<pre>def <strong>joint_coordinates(joint)</strong>:<br/>    """Store necessary coordinates to a list"""<br/>    joint_coor = []<br/>    <strong># Take mean of the leye, reye, nose to obtain coordinates for the head</strong><br/>    joint['head'] = (joint['leye']+joint['reye']+joint['nose'])/3<br/>    joint_coor.extend(joint['lwri'].tolist())<br/>    joint_coor.extend(joint['lelb'].tolist())<br/>    joint_coor.extend(joint['lsho'].tolist())<br/>    joint_coor.extend(joint['head'].tolist())<br/>    joint_coor.extend(joint['rsho'].tolist())<br/>    joint_coor.extend(joint['relb'].tolist())<br/>    joint_coor.extend(joint['rwri'].tolist())<br/>    return joint_coor</pre>
<p>Now let's load the <kbd>tr_plus_indices.mat</kbd> MATLAB file, just like we did previously:</p>
<div class="packt_infobox">The reason why we need to use the <kbd>tr_plus_indices.mat</kbd> file is because it contains indices of images that should only be used for training, as well as some unlisted ones for testing. The reason for such a split is to make sure that the train set and the test set have frames from completely different movies so as to avoid overfitting. More on this can be found at <a href="https://bensapp.github.io/flic-dataset.html">https://bensapp.github.io/flic-dataset.html</a>.<a href="https://bensapp.github.io/flic-dataset.html"/></div>
<pre><strong># load the indices matlab file</strong><br/>train_indices = loadmat('FLIC-full/tr_plus_indices.mat')<br/><br/><strong># print type of the loaded file</strong><br/>print('train_indices variable is of', type(train_indices))<br/><br/><strong># print keys in the dictionary training_indices</strong><br/>print('keys in the dictionary train_indices:\n', train_indices.keys())</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/facd390a-cf75-42f5-aa65-a71b2491ef90.png" style="width:38.50em;height:4.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.7: train_indices file information printout 1 </span></div>
<p>From the preceding screenshot, you can see that the MATLAB file has been loaded as a dictionary with four keys, one of which is <kbd>tr_plus_indices</kbd>, which is the one we need. Let's look at the content of this key:</p>
<pre><strong># print type and shape of values in tr_plus_indices key</strong><br/>print('Shape of values in tr_plus_indices key: ',train_indices['tr_plus_indices'].shape)<br/><br/><strong># print tr_plus_indices</strong><br/>print('Type: ',type(train_indices['tr_plus_indices']))<br/><br/><strong># reshape the training_indices array</strong> <br/>train_indices = train_indices['tr_plus_indices'].reshape(-1,)<br/><br/><strong># print shape of train_indices array</strong><br/>print("Shape of reshaped 'train_indices' array:", train_indices.shape)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d444061b-4688-40f8-910a-e6371a2d6dee.png" style="width:31.92em;height:3.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.8: train_indices file information printout 2</span></div>
<p>We can see that the <span><kbd>tr_plus_indices</kbd> key corresponds to a (17380*1) shaped array. We will reshape this to <kbd>(17380, )</kbd> for convenience.</span></p>
<p><kbd><span>tr_plus_indices</span></kbd> contains the indices of the data in the <kbd>examples</kbd> key of the <kbd>examples.mat</kbd> file, which should only be used for training. Using this information, we will subset the data into a train set and a test set:</p>
<pre><strong># empty list to store train image ids</strong><br/>train_ids = []<br/><strong># empty list to store train joints</strong><br/>train_jts = []<br/><strong># empty list to store test image ids</strong><br/>test_ids = []<br/><strong># empty list to store test joints</strong><br/>test_jts = []<br/><br/>for i, example in enumerate(examples):<br/>    <strong># image id</strong><br/>    file_name = example[3][0]<br/>    <strong># joint coordinates</strong><br/>    joint = example[2].T<br/>    <strong># dictionary that goes into the joint_coordinates function</strong><br/>    joints = dict(zip(target_joints, [x for k,x in enumerate(joint) if k in joints_loc_id]))<br/>    <strong># obtain joints for the task</strong><br/>    joints = joint_coordinates(joints)<br/>    <strong># use train indices list to decide if an image is to be used for training or testing</strong><br/>    if i in train_indices:<br/>        train_ids.append(file_name)<br/>        train_jts.append(joints)<br/>    else:<br/>        test_ids.append(file_name)<br/>        test_jts.append(joints)<br/><br/><strong># Concatenate image ids dataframe and the joints dataframe and save it as a csv</strong></pre>
<div class="packt_infobox">For the remaining part of this code snippet, please refer to the <kbd>deeppose.ipynb</kbd> file here : <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb</a></div>
<p>We can see that the train data has 17,380 data points, with each data point having an image ID and <em>7</em>(<em>x</em>,<em>y</em>) joint coordinates. Similarly, the test data has 3,548 data points.</p>
<p>In the preceding snippet, we first initialize four empty lists, two for saving train and test image IDs, and two for saving train and test joints. Then, for each data point in the <kbd>examples</kbd> key, we do the following:</p>
<ol>
<li>Extract the file name.</li>
<li>Extract the joint coordinates.</li>
<li>ZIP the target joints (target joint labels) and the corresponding joint coordinates and convert them into a dictionary.</li>
<li>Feed the dictionary to the <kbd>joint_coordinates</kbd> function to obtain the joints needed for this task.</li>
<li>Append the image IDs and the resulting joints from the previous step to a train or test list by using the <kbd>train_indices</kbd> list.</li>
</ol>
<p>Finally, convert the lists into train and test data frames and save them as a CSV file. Make sure that you don't set the index and header parameters to <kbd>False</kbd> when saving the data frame as a CSV file.</p>
<p>Let's load the <kbd>train_joints.csv</kbd> and <kbd>test_joints.csv</kbd> files we saved in the previous step and print out some details:</p>
<pre><strong># load train_joints.csv</strong><br/>train_data = pd.read_csv('FLIC-full/train_joints.csv', header=None)<br/><br/><strong># load test_joints.csv</strong><br/>test_data = pd.read_csv('FLIC-full/test_joints.csv', header = None)<br/><br/><strong># train image ids</strong><br/>train_image_ids = train_data[0].values<br/>print('train_image_ids shape', train_image_ids.shape)<br/><br/><strong># train joints</strong><br/>train_joints = train_data.iloc[:,1:].values<br/>print('train_image_ids shape', train_joints.shape)<br/><br/><strong># test image ids</strong><br/>test_image_ids = test_data[0].values<br/>print('train_image_ids shape', test_image_ids.shape)<br/><br/><strong># test joints</strong><br/>test_joints = test_data.iloc[:,1:].values<br/>print('train_image_ids shape', test_joints.shape)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2bc02243-2847-4e3f-a809-0e189df92240.png" style="width:23.17em;height:6.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.9: Printout of image IDs and the joint's array shape</span></div>
<p>Now, let's load some images from the <kbd>images</kbd> folder and plot them to see what they look like:</p>
<pre>import glob<br/>image_list = glob.glob('FLIC-full/images/*.jpg')[0:8]<br/><br/>plt.figure(figsize=(12,5))<br/>for i in range(8):<br/>    plt.subplot(2,4,(i+1))<br/>    img = plt.imread(image_list[i])<br/>    plt.imshow(img, aspect='auto')<br/>    plt.axis('off')<br/>    plt.title('Shape: '+str(img.shape))<br/><br/>plt.tight_layout()<br/>plt.show()</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7856b703-fe37-4781-8fdb-66b7bdacda1e.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.10: Plot of eight images from the images folder in the FLIC_full folder </span></div>
<p>We can see that each image is of shape (480*720*3). Our next task will be to crop the original image and focus on the person of interest by using the joint coordinates that are available to us. We do this by resizing the images into a shape of 224*24*3 so that we can feed them into the VGG16 model. Finally, we will also build a <kbd>plotting</kbd> function to plot the joints on the image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9fe4c5d7-476a-4402-a583-bfdd7c2a9121.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.11: Plot showing the transformation each image has to go through</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Now let's implement the functions that will perform the tasks that we discussed when we ended the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cropping</h1>
                </header>
            
            <article>
                
<p>We will first start off with the <kbd>image_cropping()</kbd> function. This function accepts an image ID and its corresponding joint coordinates. It loads the image into memory and then crops the image so that it only includes the section of the image that's bound within the coordinates. The cropped image is then padded so that the joints and limbs are completely visible. For the added padding, the joint coordinates are also adjusted accordingly. When it has done this, the image is returned. This is the most important part of the transformation. Take your time and dissect the function to see exactly what is happening (the <kbd>crop_pad_inf</kbd> and <kbd>crop_pad_sup</kbd> parameters control the amount of padding):</p>
<pre>def <strong>image_cropping(image_id, joints, crop_pad_inf = 1.4, crop_pad_sup=1.6, shift = 5, min_dim = 100)</strong>:<br/>    """Function to crop original images"""<br/>    <strong>## image cropping</strong><br/><strong>    # load the image</strong> <br/>    image = cv.imread('FLIC-full/images/%s' % (image_id))<br/>    <strong># convert joint list to array</strong> <br/>    joints = np.asarray([int(float(p)) for p in joints])<br/>    <strong># reshape joints to shape (7*2)</strong><br/>    joints = joints.reshape((len(joints) // 2, 2))<br/>    <strong># transform joints to list of (x,y) tuples</strong><br/>    posi_joints = [(j[0], j[1]) for j in joints if j[0] &gt; 0 and j[1] &gt; 0]<br/>    <strong># obtain the bounding rectangle using opencv boundingRect</strong><br/>    x_loc, y_loc, width, height = cv.boundingRect(np.asarray([posi_joints]))<br/>    if width &lt; min_dim:<br/>        width = min_dim<br/>    if height &lt; min_dim:<br/>        height = min_dim<br/><br/>    <strong>## bounding rect extending</strong><br/>    inf, sup = crop_pad_inf, crop_pad_sup<br/>    r = sup - inf<br/>    <strong># define width padding</strong><br/>    pad_w_r = np.random.rand() * r + inf # inf~sup<br/>    <strong># define height padding</strong><br/>    pad_h_r = np.random.rand() * r + inf # inf~sup<br/>    <strong># adjust x, y, w and h by the defined padding</strong><br/>    x_loc -= (width * pad_w_r - width) / 2<br/>    y_loc -= (height * pad_h_r - height) / 2<br/>    width *= pad_w_r<br/>    height *= pad_h_r<br/><br/>    <strong>## shifting</strong><br/>    <strong>## clipping</strong><br/>    <strong>## joint shifting</strong></pre>
<div class="packt_infobox">For the remaining part of this code snippet please refer to the file <kbd>deeppose.ipynb</kbd> here : <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb</a><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank"><br/></a></div>
<p>Let's pass an image ID and its joints to the <kbd>image_cropping()</kbd> function and plot the output image:</p>
<pre><strong># plot the original image</strong> <br/>plt.figure(figsize = (15,5))<br/>plt.subplot(1,2,1)<br/>plt.title('Original')<br/>plt.imshow(plt.imread('FLIC-full/images/'+train_image_ids[0]))<br/><br/><strong># plot the cropped image</strong><br/>image, joint = image_cropping(train_image_ids[0], train_joints[0])<br/>plt.subplot(1,2,2)<br/>plt.title('Cropped')<br/>plt.imshow(image)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cf6ad6e1-8611-4155-8cab-5e90b983a008.png" style="width:44.92em;height:18.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.12: Plot of the resulting cropped image compared to the original image</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resizing</h1>
                </header>
            
            <article>
                
<p>In the <em>Cropping</em> section, we saw that the original image of shape (480*720*3) is cropped to shape (393*254*3). However, the VGG16 architecture accepts images of shape (224*224*3). Hence, we will define a function called <kbd>image_resize()</kbd> that does the resizing for us. It accepts the cropped image and the joint resulting from the <kbd>image_cropping()</kbd> function as input and returns the resized image and its joint coordinates:</p>
<pre>def <strong>image_resize(image, joints, new_size = 224)</strong>:<br/>    """Function resize cropped images"""<br/>    orig_h, orig_w = image.shape[:2]<br/>    joints[0::2] = joints[0::2] / float(orig_w) * new_size<br/>    joints[1::2] = joints[1::2] / float(orig_h) * new_size<br/>    image = cv.resize(image, (new_size, new_size), interpolation=cv.INTER_NEAREST)<br/>    return image, joints<br/><strong># plot resized image</strong><br/>image, joint = image_resize(image, joint)<br/>plt.title('Cropped + Resized')<br/>plt.imshow(image)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d66c1090-a53d-4026-985b-34ba59dc99ee.png" style="width:19.33em;height:18.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.13: Plot of the resized image </span></div>
<p>After passing the cropped image to the <kbd>image_resize()</kbd> function, we can see that the resulting image is of shape (224*224*3). Now this image and its joints can be passed into the model for training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting the joints and limbs</h1>
                </header>
            
            <article>
                
<p>Let's also define the plotting functions that will plot the limbs on the resized image. The following defined <kbd>plot_joints()</kbd> function accepts the resized image and its joints and returns an image of the same shape with the limbs plotted on top:</p>
<pre>def <strong>plot_limb(img, joints, i, j, color)</strong>:<br/>    """Function to plot the limbs"""<br/>    cv.line(img, joints[i], joints[j], (255, 255, 255), thickness=2, lineType=16)<br/>    cv.line(img, joints[i], joints[j], color, thickness=1, lineType=16)<br/>    return img<br/><br/>def <strong>plot_joints(img, joints, groundtruth=True, text_scale=0.5)</strong>:<br/>    """Function to draw the joints"""<br/>    h, w, c = img.shape<br/>    if groundtruth:<br/>        <strong># left hand to left elbow</strong><br/>        img = plot_limb(img, joints, 0, 1, (0, 255, 0))<br/>        <br/>        <strong># left elbow to left shoulder</strong><br/>        img = plot_limb(img, joints, 1, 2, (0, 255, 0))<br/>        <br/>        <strong># left shoulder to right shoulder</strong><br/>        img = plot_limb(img, joints, 2, 4, (0, 255, 0))<br/>        <br/>        <strong># right shoulder to right elbow</strong><br/>        img = plot_limb(img, joints, 4, 5, (0, 255, 0))<br/>        <br/>        <strong># right elbow to right hand</strong><br/>        img = plot_limb(img, joints, 5, 6, (0, 255, 0))<br/>        <br/>        <strong># neck coordinate</strong><br/>        neck = tuple((np.array(joints[2]) + np.array(joints[4])) // 2)<br/>        joints.append(neck)<br/>        <strong># neck to head</strong><br/>        img = plot_limb(img, joints, 3, 7, (0, 255, 0))<br/>        joints.pop()<br/><br/>    <strong># joints</strong><br/>    for j, joint in enumerate(joints):<br/>        <strong># plot joints</strong><br/>        cv.circle(img, joint, 5, (0, 255, 0), -1)<br/>        <strong># plot joint number black</strong><br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (0, 0, 0), thickness=2, lineType=16)<br/>        <strong># plot joint number white</strong><br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (255, 255, 255), thickness=1, lineType=16)<br/><br/>    else:</pre>
<div class="packt_infobox">For the remaining part of this code snippet please refer to the <kbd>deeppose.ipynb</kbd> file here: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb</a></div>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/04effbe1-f2bc-41a1-8d0c-3d789946e1d5.png" style="width:21.58em;height:21.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.14: Plot showing the true joint coordinates on top of the image</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming the images</h1>
                </header>
            
            <article>
                
<p>Now let's transform the images and their corresponding joints to the desired form by using the functions we have defined previously. We will do this with the help of the <kbd>model_data()</kbd> function, which is defined as follows:</p>
<pre>def <strong>model_data(image_ids, joints, train = True)</strong>:<br/>    """Function to generate train and test data."""<br/>    if train:<br/>        <strong># empty list</strong> <br/>        train_img_joints = []<br/>        <br/>        <strong># create train directory inside FLIC-full</strong><br/>        if not os.path.exists(os.path.join(os.getcwd(), 'FLIC-full/train')):<br/>            os.mkdir('FLIC-full/train')<br/><br/>        for i, (image, joint) in enumerate(zip(image_ids, joints)):<br/>            <strong># crop the image using the joint coordinates</strong><br/>            image, joint = image_cropping(image, joint)<br/>            <br/>            <strong># resize the cropped image to shape (224*224*3)</strong><br/>            image, joint = image_resize(image, joint)<br/>            <br/>            <strong># save the image in train folder</strong><br/>            cv.imwrite('FLIC-full/train/train{}.jpg'.format(i), image)<br/>            <br/>            <strong># store joints and image id/file name of the saved image in the initialized list</strong><br/>            train_img_joints.append(['train{}.jpg'.format(i)] + joint.tolist())<br/>        <br/>        <strong># convert to a dataframe and save as a csv</strong><br/>        pd.DataFrame(train_img_joints).to_csv('FLIC-full/train/train_joints.csv', index=False, header=False)<br/>    else:<br/>        <strong># empty list</strong> <br/>        test_img_joints = []</pre>
<div class="packt_infobox">For the remaining part of this code snippet, please refer to the <kbd>deeppose.ipynb</kbd> file here: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb</a></div>
<p>The preceding defined <kbd>model_data()</kbd> function accepts three parameters: <kbd>image_ids</kbd> (array of image IDs), <kbd>joints</kbd> (array of joints), and a Boolean parameter called <kbd>train</kbd>. Set the <kbd>train</kbd> parameter to <kbd>True</kbd> when transforming the training images and joints and to <kbd>False</kbd> when transforming the test images and joints. </p>
<p>When the <kbd>train</kbd> parameter is set to <kbd>True</kbd>, perform the following steps:</p>
<ol>
<li>Initialize an empty list to store the ID of the transformed image and its joint coordinates.</li>
<li>A new <span>directory called <kbd>train</kbd> will be created inside the <kbd>images</kbd> folder if the folder does not exist.</span></li>
<li><span>An image and its joint coordinates are first passed to the <kbd>image_cropping</kbd> function we defined previously, which will return the cropped image and joint coordinates.</span></li>
<li>The result of <em>Step 3</em> is passed to the <kbd>image_resize</kbd> function, which will then resize the image to the desired shape. In our case, this is 224*224*3.</li>
</ol>
<ol start="5">
<li>The resized image is then written into the <kbd>train</kbd> folder via the OpenCV <kbd>imwrite()</kbd> function with a new image ID (for example, <kbd>train0.jpg</kbd>).</li>
<li>The new image ID and its joints are appended to the list initialized in <em>Step 1.</em></li>
<li><em>Step 3</em> through <em>Step 6</em> are repeated until all of the training images are transformed.</li>
<li>The list defined in <em>Step 1</em> now contains the new image IDs and the joint coordinates, which are then converted to a data frame and saved as a CSV file in the <kbd>train</kbd> folder.</li>
</ol>
<p>For transforming the test data, the preceding procedure is repeated by setting the <kbd>train</kbd> parameter to <kbd>False</kbd> and feeding the test image IDs and the joints.</p>
<div class="packt_infobox">The train and test data frames that get generated inside the <kbd>model_data()</kbd> function are stored as a CSV file with no header and no index column. Take this into consideration when loading these files.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining hyperparameters for training</h1>
                </header>
            
            <article>
                
<p>The following are some of the hyperparameters that have been defined that we will be using throughout our code. These are totally configurable: </p>
<pre><strong># Number of epochs</strong><br/>epochs = 3<br/><br/><strong># Batchsize</strong><br/>batch_size = 128<br/><br/><strong># Optimizer for the model</strong><br/>optimizer = Adam(lr=0.0001, beta_1=0.5)<br/><br/><strong># Shape of the input image</strong><br/>input_shape = (224, 224, 3)<br/><br/><strong># Batch interval at which loss is to be stored</strong><br/>store = 40</pre>
<div class="packt_tip">Experiment with different learning rates, optimizers, batch size, as well as smoothing value to see how these factors affect the quality of your model. If you get better results, show these to the deep learning community.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the VGG16 model</h1>
                </header>
            
            <article>
                
<p><span>The VGG16 model is a deep convolution neural network image classifier. The model uses a combination of Conv2D, MaxPooling2D, and Dense layers to form the final architecture, and the activation function that's used is ReLU.</span><span> It accepts color images of shape 224*224*3, and is capable of predicting 1,000 classes. </span><span>This means that the final Dense layer has 1,000 neurons, and it uses softmax activation to get scores for each class.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the VGG16 model</h1>
                </header>
            
            <article>
                
<p><span>In this project, we want to feed in images of shape 224*224*3 and be able to predict the joint coordinates for the body in the image. That is, we want to be able to predict 14 numerical values (<em>7</em> (<em>x</em>,<em>y</em>) pairs). Therefore, we modify the final Dense layer to have 14 neurons and use ReLU activation instead of sigmoid. </span></p>
<p>Training a deep learning model such as VGG16 can take up to a week on a local machine. This is a lot of time. An alternative to this in our case is to use the weights of a trained VGG16 model through transfer learning.</p>
<p>We will do this with the help of the applications module in Keras that we imported in the beginning, along with the other imports.</p>
<p>In the following code, we will load part of the VGG16 model up to, but not including,<span> </span>the Flatten layer and<span> the corresponding weights</span>. Setting the <kbd>include_top</kbd> parameter to <kbd>False</kbd> does this for us:</p>
<div class="packt_tip">The first line of the following snippet will also download the VGG16 weights from the Keras server, so you don't have to worry about downloading the weights file from anywhere else.</div>
<pre><strong># load the VGG16 model</strong> <br/>model = applications.VGG16(weights = "imagenet", include_top=False, input_shape = input_shape)<br/><br/><strong># print summary of VGG16 model</strong><br/>model.summary()</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8ceeddcd-bb4b-4e32-a851-699bd00f3362.png" style="width:40.75em;height:49.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.15: Summary of the VGG16 model (up to Flatten) </span></div>
<p>From the summary, we can see that all of the layers of the VGG16 model up to,<span> </span>but not including,<span> </span>the Flatten layer have been loaded with their weights. </p>
<div class="packt_infobox">To learn more about the additional functionality of the applications module of Keras, take a look at the official documentation at <a href="https://keras.io/applications/">https://keras.io/applications/</a>.<a href="https://keras.io/applications/"/></div>
<p>We don't want weights of any of these layers to be trained. So, in the following code, we need to set the <kbd>trainable</kbd> parameter of each layer to <kbd>False</kbd>:</p>
<pre><strong># set layers as non trainable</strong><br/>for layer in model.layers:<br/>    layer.trainable = False</pre>
<p>As a next step, flatten the output of the preceding section of the model and then add three Dense layers, of which two layers have 1,024 neurons each with a dropout between them, and a final Dense layer with 14 neurons to obtain the 14 joint coordinates. We will only be training the weights of the layers defined in the following code snippet:</p>
<pre><strong># Adding custom Layers</strong><br/>x = model.output<br/>x = Flatten()(x)<br/>x = Dense(1024, activation="relu")(x)<br/>x = Dropout(0.5)(x)<br/>x = Dense(1024, activation="relu")(x)<br/><br/><strong># Dense layer with 14 neurons for predicting 14 numeric values</strong><br/>predictions = Dense(14, activation="relu")(x)</pre>
<p>Once all of the layers have been defined and configured, we will combine them by using the <kbd>Model</kbd> function in Keras, as follows:</p>
<pre><strong># creating the final model</strong> <br/>model_final = Model(inputs = model.input, outputs = predictions)<br/><br/><strong># print summary</strong><br/>model_final.summary()</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/656b47f0-c68e-4ac3-9fa9-77310a06ebc9.png" style="width:33.83em;height:52.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 12.16: Summary of the customized VGG16 model</span></div>
<p class="mce-root"/>
<p>From the summary, we can see that <kbd>26,755,086</kbd> parameters are trainable and that <kbd>14,714,688</kbd> parameters are untrainable, since we have set them as untrainable.</p>
<p>The model is then compiled with <kbd>mean_squared_error</kbd> as <kbd>loss</kbd>. The <kbd>optimizer</kbd> used here is Adam, which has a learning rate of 0.0001, as defined by the optimizer variable in the hyperparameter section:</p>
<pre><strong># compile the model</strong> <br/>model_final.compile(loss = "mean_squared_error", optimizer = optimizer)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training loop</h1>
                </header>
            
            <article>
                
<p>Now that the VGG16 model is all set to be used for training, let's load the <kbd>train_joints.csv</kbd> file from the <kbd>train</kbd> folder containing the IDs of the cropped and resized images with their joint coordinates. </p>
<p>Then, split the data into an 80:20 train and validation set by using the <kbd>train_test_split</kbd> module from <kbd>sklearn</kbd>. We imported it with the other imports at the beginning of this chapter. Since the validation data is small, load all of the corresponding images into memory:</p>
<div class="packt_infobox"><span>Be mindful of how many validation images you load into memory, as this may become an issue with systems that have less RAM.</span></div>
<pre><strong># load the train data</strong><br/>train = pd.read_csv('FLIC-full/train/train_joints.csv', header = None)<br/><br/><strong># split train into train and validation</strong><br/>train_img_ids, val_img_ids, train_jts, val_jts = train_test_split(train.iloc[:,0], train.iloc[:,1:], test_size=0.2, random_state=42)<br/><br/><strong># load validation images</strong><br/>val_images = np.array([cv.imread('FLIC-full/train/{}'.format(x)) for x in val_img_ids.values])<br/><br/><strong># convert validation images to dtype float</strong> <br/>val_images = val_images.astype(float)</pre>
<div class="packt_tip packt_infobox"><span>Explore the data with the pandas <kbd>head</kbd>, <kbd>tail</kbd>, and <kbd>info</kbd> functions. Please note that when loading the <kbd>.csv</kbd> file using pandas, set the <kbd>header</kbd> parameter to <kbd>False</kbd> so that pandas knows that the file has no header.</span></div>
<p>We will now define the <kbd>training()</kbd> function, which will train the VGG16 model on the train images. This function accepts the VGG16 model, train image IDs, train joints, validation images, and validation joints as parameters. The following steps define what is happening in the <kbd>training()</kbd> function:</p>
<ol>
<li>The function defines empty lists by using <kbd>loss_lst</kbd> <span>to store the train loss and</span> <kbd>val_loss_lst</kbd> to store the validation loss. It also defines a counter count to keep track of the total number of batches.</li>
<li>It then creates a batch of train image IDs and their corresponding joints.</li>
<li>Using the batch image IDs, it loads the corresponding images into memory by using the OpenCV <kbd>imread()</kbd> function.</li>
<li>It then converts the loaded train images into a <kbd>float</kbd>, which it feeds along with the joint IDs to the <kbd>train_on_batch()</kbd> function of the model for the fit.</li>
<li>At every 40<sup>th</sup> batch, it evaluates the model on the validation data and stores the train and validation loss in the defined lists.</li>
<li>It then repeats <em>Steps 2</em> through <em>5</em> for the desired number of epochs.</li>
</ol>
<p>Following is the code:</p>
<pre>def <strong>training(model, image_ids, joints ,val_images, val_jts, batch_size = 128, epochs=3, store = 40)</strong>:<br/>    <strong># empty train loss list</strong><br/>    loss_lst = []<br/>    <br/>    <strong># empty validation loss list</strong><br/>    val_loss_lst = []<br/>    <br/>    <strong># counter</strong><br/>    count = 0<br/>    count_lst = []<br/>    <br/>    <strong># create shuffled batches</strong><br/>    batches = np.arange(len(image_ids)//batch_size)<br/>    data_idx = np.arange(len(image_ids))<br/>    random.shuffle(data_idx)<br/>    print('......Training......')<br/>    for epoch in range(epochs):<br/>        for batch in (batches):<br/>            <strong># batch of training image ids</strong><br/>            imgs = image_ids[data_idx[batch*batch_size : (batch+1)*batch_size:]]<br/>            <br/>            <strong># corresponding joints for the above images</strong><br/>            jts = joints[data_idx[batch*batch_size : (batch+1)*batch_size:]]<br/>            <br/>            <strong># load the training image batch</strong><br/>            batch_imgs = np.array([cv.imread('FLIC-full/train/{}'.format(x)) for x in imgs])<br/><br/>            <strong># fit model on the batch</strong><br/>            loss = model.train_on_batch(batch_imgs.astype(float), jts)<br/>            <br/>            if batch%store==0:</pre>
<div class="packt_infobox">For the remaining part of this code snippet, please refer to the <kbd>deeppose.ipynb</kbd> file here: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb</a></div>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2296d449-0116-40b7-a1ca-55132b90f859.png" style="width:29.67em;height:16.83em;"/></p>
<p>The following is the output at the end of the code's execution:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/048171cb-94b9-4623-a205-f0b0a8cc14c1.png" style="width:23.75em;height:10.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.17: Loss output when training the model </span></div>
<div class="packt_infobox">If you are using a small GPU for training, reduce the batch size to avoid GPU memory issues. Also, remember that a smaller batch size may or may not result in the same fit that this chapter indicates.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plot training and validation loss</h1>
                </header>
            
            <article>
                
<p>With <kbd>loss_lst</kbd> and <kbd>val_loss_lst</kbd> containing the train and validation MSE loss at intervals of 40 batches, let's plot this and see how the learning has progressed:</p>
<pre>plt.style.use('ggplot')<br/>plt.figure(figsize=(10, 6))<br/>plt.plot(count_lst, loss_lst, marker='D', label = 'training_loss')<br/>plt.plot(count_lst, val_loss_lst, marker='o', label = 'validation_loss')<br/>plt.ylabel('Mean Squared Error')<br/>plt.title('Plot of MSE over time')<br/>plt.legend(loc = 'upper right')<br/>plt.show()</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/445bc33e-63bd-4581-bbce-2f481f74228f.png" style="width:42.08em;height:26.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.18: Plot of train and validation loss </span></div>
<div class="packt_tip">A smoother train validation loss plot can be obtained by reducing the store hyperparameter. A small store value will result in a longer training time.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictions</h1>
                </header>
            
            <article>
                
<p>This is what we have been waiting for...</p>
<p>Making test predictions!</p>
<p>We will define a function that takes the model as input and tests the model on the test data we have preprocessed and saved in the <kbd>test</kbd> folder. Along with predictions, it will also save test images with the true and predicted joints plotted on it by using the <kbd>plot_limb()</kbd> and the <kbd>plot_joints()</kbd> functions we defined in the preceding section:</p>
<pre>def <strong>test(model, nrows=200, batch_size=128)</strong>:<br/>    <strong># load the train data</strong><br/>    test = pd.read_csv('FLIC-full/test/test_joints.csv', header = None, nrows=nrows)<br/>    test_img_ids = test.iloc[:,0].values<br/>    <br/>    <strong># load validation images</strong><br/>    test_images = np.array([cv.imread('FLIC-full/test/{}'.format(x)) for x in test_img_ids])<br/><br/>    <strong># convert validation images to dtype float</strong> <br/>    test_images = test_images.astype(float)<br/>    <br/>    <strong># joints</strong><br/>    test_joints = test.iloc[:,1:].values<br/>    <br/>    <strong># evaluate</strong><br/>    test_loss = model.evaluate(test_images, test_joints, verbose = 0, batch_size=batch_size)<br/>    <br/>    <strong># predict</strong><br/>    predictions = model.predict(test_images, verbose = 0, batch_size=batch_size)<br/><br/>    <strong># folder to save the results</strong><br/>    if not os.path.exists(os.path.join(os.getcwd(), 'FLIC-full/test_plot')):<br/>        os.mkdir('FLIC-full/test_plot')<br/>    <br/>    for i, (ids, image, joint, pred) in enumerate(zip(test_img_ids, test_images, test_joints, predictions)):<br/>        joints = joint.tolist()<br/>        joints = list(zip(joints[0::2], joints[1::2]))<br/>        <br/>        <strong># plot original joints</strong><br/>        image = plot_joints(image.astype(np.uint8), joints, groundtruth=True, text_scale=0.5)<br/>        <br/>        pred = pred.astype(np.uint8).tolist()<br/>        pred = list(zip(pred[0::2], pred[1::2]))<br/>        <br/>        <strong># plot predicted joints</strong><br/>        image = plot_joints(image.astype(np.uint8), pred, groundtruth=False, text_scale=0.5)<br/>        <br/>        <strong># save resulting images with the same id</strong><br/>        plt.imsave('FLIC-full/test_plot/'+ids, image)<br/>    return test_loss<br/><br/><strong># test and save results</strong><br/>test_loss = test(m, batch_size)<br/><br/><strong># print test loss</strong><br/>print('Test Loss:', test_loss)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42596d64-7380-405a-8807-c875b26a8887.png" style="width:16.75em;height:1.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.19: Test loss </span></div>
<p>On a test set with 200 images, the test MSE loss is 454.80, which is very close to the validation MSE loss of 503.85, indicating that the model is not overfitting on the train data. </p>
<div class="packt_tip packt_infobox">Train the model for a few more epochs if possible, and check if a better fit is possible. Be mindful of how many test images you want to load into memory for evaluation since it might become a problem on machines with RAM limitations.</div>
<p>Now let's plot the images we saved during testing to get a measure of how the true joints compare to the predicted joints:</p>
<pre>image_list = glob.glob('FLIC-full/test_plot/*.jpg')[8:16]<br/><br/>plt.figure(figsize=(16,8))<br/>for i in range(8):<br/>    plt.subplot(2,4,(i+1))<br/>    img = cv.imread(image_list[i])<br/>    plt.imshow(img, aspect='auto')<br/>    plt.axis('off')<br/>    plt.title('Green-True/Red-Predicted Joints')<br/><br/>plt.tight_layout()<br/>plt.show()</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dc8f0519-8697-48cb-98e9-306819b4afd3.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 12.20: Test images with true and predicted joints plotted on top </span></div>
<p>From the preceding picture, we can see that the model is doing a really good job of predicting the seven joints on unseen images. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scripts in modular form</h1>
                </header>
            
            <article>
                
<p>The entire script can be split into four modules named <kbd>train.py</kbd>, <kbd>test.py</kbd>, <kbd>plotting.py</kbd>, and <kbd>crop_resize_transform.py</kbd>. You should be able to find these scripts in the <span><kbd>Chapter12</kbd> </span>folder. Follow the instructions under the <em>Code implementation</em> section of this chapter to run the scripts. Set <kbd>Chapter12</kbd> as the project folder in your favorite source code editor and just run the <span><kbd>train.py</kbd> </span>file. </p>
<p>The <span><kbd>train.py</kbd> </span>Python file will import functions from all of the other modules in places where they are needed for execution.</p>
<p>Now let's walk through the contents of each file.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 1 – crop_resize_transform.py</h1>
                </header>
            
            <article>
                
<p>This Python file contains the <kbd>image_cropping()</kbd>, <kbd>image_resize()</kbd>, and <kbd>model_data()</kbd> functions, as shown:</p>
<pre>"""This module contains functions to crop and resize images."""<br/><br/>import os<br/>import cv2 as cv<br/>import numpy as np<br/>import pandas as pd<br/><br/><br/>def image_cropping(image_id, joints, crop_pad_inf=1.4, crop_pad_sup=1.6,<br/>                   shift=5, min_dim=100):<br/>    """Crop Function."""<br/>    # # image cropping<br/>    # load the image<br/>    image = cv.imread('FLIC-full/images/%s' % (image_id))<br/>    # convert joint list to array<br/>    joints = np.asarray([int(float(p)) for p in joints])<br/>    # reshape joints to shape (7*2)<br/>    joints = joints.reshape((len(joints) // 2, 2))<br/>    # transform joints to list of (x,y) tuples<br/>    posi_joints = [(j[0], j[1]) for j in joints if j[0] &gt; 0 and j[1] &gt; 0]<br/>    # obtain the bounding rectangle using opencv boundingRect<br/>    x_loc, y_loc, width, height = cv.boundingRect(np.asarray([posi_joints]))<br/>    if width &lt; min_dim:<br/>        width = min_dim<br/>    if height &lt; min_dim:<br/>        height = min_dim<br/><br/>    # # bounding rect extending<br/>    inf, sup = crop_pad_inf, crop_pad_sup<br/>    r = sup - inf<br/>    # define width padding<br/>    pad_w_r = np.random.rand() * r + inf # inf~sup<br/>    # define height padding<br/>    pad_h_r = np.random.rand() * r + inf # inf~sup<br/>    # adjust x, y, w and h by the defined padding<br/>    x_loc -= (width * pad_w_r - width) / 2<br/>    y_loc -= (height * pad_h_r - height) / 2<br/>    width *= pad_w_r<br/>    height *= pad_h_r<br/><br/>    # # shifting<br/>    x_loc += np.random.rand() * shift * 2 - shift<br/>    y_loc += np.random.rand() * shift * 2 - shift<br/><br/>    # # clipping<br/>    x_loc, y_loc, width, height = [int(z) for z in [x_loc, y_loc,<br/>                                                    width, height]]<br/>    x_loc = np.clip(x_loc, 0, image.shape[1] - 1)<br/>    y_loc = np.clip(y_loc, 0, image.shape[0] - 1)<br/>    width = np.clip(width, 1, image.shape[1] - (x_loc + 1))<br/>    height = np.clip(height, 1, image.shape[0] - (y_loc + 1))<br/>    image = image[y_loc: y_loc + height, x_loc: x_loc + width]<br/><br/>    # # joint shifting<br/>    # adjust joint coordinates onto the padded image<br/>    joints = np.asarray([(j[0] - x_loc, j[1] - y_loc) for j in joints])<br/>    joints = joints.flatten()<br/><br/>    return image, joints<br/><br/><br/>def image_resize(image, joints, new_size=224):<br/>    """Resize Function."""<br/>    orig_h, orig_w = image.shape[:2]<br/>    joints[0::2] = joints[0::2] / float(orig_w) * new_size<br/>    joints[1::2] = joints[1::2] / float(orig_h) * new_size<br/>    image = cv.resize(image, (new_size, new_size),<br/>                      interpolation=cv.INTER_NEAREST)<br/>    return image, joints<br/><br/><br/>def model_data(image_ids, joints, train=True):<br/>    """Function to generate train and test data."""<br/>    if train:<br/>        # empty list<br/>        train_img_joints = []<br/>        # create train directory inside FLIC-full<br/>        if not os.path.exists(os.path.join(os.getcwd(), 'FLIC-full/train')):<br/>            os.mkdir('FLIC-full/train')<br/><br/>        for i, (image, joint) in enumerate(zip(image_ids, joints)):<br/>            # crop the image using the joint coordinates<br/>            image, joint = image_cropping(image, joint)<br/>            # resize the cropped image to shape (224*224*3)<br/>            image, joint = image_resize(image, joint)<br/>            # save the image in train folder<br/>            cv.imwrite('FLIC-full/train/train{}.jpg'.format(i), image)<br/>            # store joints and image id/file name of the saved image in<br/>            # the initialized list<br/>            train_img_joints.append(['train{}.jpg'.format(i)] + joint.tolist())<br/><br/>        # convert to a dataframe and save as a csv<br/>        pd.DataFrame(train_img_joints<br/>                     ).to_csv('FLIC-full/train/train_joints.csv',<br/>                              index=False, header=False)<br/>    else:<br/>        # empty list<br/>        test_img_joints = []<br/>        # create test directory inside FLIC-full<br/>        if not os.path.exists(os.path.join(os.getcwd(), 'FLIC-full/test')):<br/>            os.mkdir('FLIC-full/test')<br/><br/>        for i, (image, joint) in enumerate(zip(image_ids, joints)):<br/>            # crop the image using the joint coordinates<br/>            image, joint = image_cropping(image, joint)<br/>            # resize the cropped image to shape (224*224*3)<br/>            image, joint = image_resize(image, joint)<br/>            # save the image in test folder<br/>            cv.imwrite('FLIC-full/test/test{}.jpg'.format(i), image)<br/>            # store joints and image id/file name of the saved image<br/>            # in the initialized list<br/>            test_img_joints.append(['test{}.jpg'.format(i)] + joint.tolist())<br/><br/>        # convert to a dataframe and save as a csv<br/>        pd.DataFrame(test_img_joints).to_csv('FLIC-full/test/test_joints.csv',<br/>                                             index=False, header=False)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 2 – plotting.py</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This Python file contains two functions, namely <kbd>plot_limb()</kbd> and </span><kbd>plot_joints()</kbd><span>, as shown</span><span>:</span></p>
<pre>"""This module contains functions to plot the joints and the limbs."""<br/>import cv2 as cv<br/>import numpy as np<br/><br/><br/>def plot_limb(img, joints, i, j, color):<br/>    """Limb plot function."""<br/>    cv.line(img, joints[i], joints[j], (255, 255, 255), thickness=2,<br/>            lineType=16)<br/>    cv.line(img, joints[i], joints[j], color, thickness=1, lineType=16)<br/>    return img<br/><br/><br/>def plot_joints(img, joints, groundtruth=True, text_scale=0.5):<br/>    """Joint and Limb plot function."""<br/>    h, w, c = img.shape<br/>    if groundtruth:<br/>        # left hand to left elbow<br/>        img = plot_limb(img, joints, 0, 1, (0, 255, 0))<br/><br/>        # left elbow to left shoulder<br/>        img = plot_limb(img, joints, 1, 2, (0, 255, 0))<br/><br/>        # left shoulder to right shoulder<br/>        img = plot_limb(img, joints, 2, 4, (0, 255, 0))<br/><br/>        # right shoulder to right elbow<br/>        img = plot_limb(img, joints, 4, 5, (0, 255, 0))<br/><br/>        # right elbow to right hand<br/>        img = plot_limb(img, joints, 5, 6, (0, 255, 0))<br/><br/>        # neck coordinate<br/>        neck = tuple((np.array(joints[2]) + np.array(joints[4])) // 2)<br/>        joints.append(neck)<br/>        # neck to head<br/>        img = plot_limb(img, joints, 3, 7, (0, 255, 0))<br/>        joints.pop()<br/><br/>    # joints<br/>    for j, joint in enumerate(joints):<br/>        # plot joints<br/>        cv.circle(img, joint, 5, (0, 255, 0), -1)<br/>        # plot joint number black<br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (0, 0, 0), thickness=2, lineType=16)<br/>        # plot joint number white<br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (255, 255, 255), thickness=1, lineType=16)<br/><br/>    else:<br/>        # left hand to left elbow<br/>        img = plot_limb(img, joints, 0, 1, (0, 0, 255))<br/><br/>        # left elbow to left shoulder<br/>        img = plot_limb(img, joints, 1, 2, (0, 0, 255))<br/><br/>        # left shoulder to right shoulder<br/>        img = plot_limb(img, joints, 2, 4, (0, 0, 255))<br/><br/>        # right shoulder to right elbow<br/>        img = plot_limb(img, joints, 4, 5, (0, 0, 255))<br/><br/>        # right elbow to right hand<br/>        img = plot_limb(img, joints, 5, 6, (0, 0, 255))<br/><br/>        # neck coordinate<br/>        neck = tuple((np.array(joints[2]) + np.array(joints[4])) // 2)<br/>        joints.append(neck)<br/><br/>        # neck to head<br/>        img = plot_limb(img, joints, 3, 7, (0, 0, 255))<br/>        joints.pop()<br/><br/>    # joints<br/>    for j, joint in enumerate(joints):<br/>        # plot joints<br/>        cv.circle(img, joint, 5, (0, 0, 255), -1)<br/>        # plot joint number black<br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (0, 0, 0), thickness=3, lineType=16)<br/>        # plot joint number white<br/>        cv.putText(img, '%d' % j, joint, cv.FONT_HERSHEY_SIMPLEX, text_scale,<br/>                   (255, 255, 255), thickness=1, lineType=16)<br/><br/>    return img</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 3 – test.py</h1>
                </header>
            
            <article>
                
<p><span>This module contains the <kbd>test()</kbd> function that will be called in the <kbd>train_dqn.py</kbd> script so that it can test the performance of the trained model, as shown:</span></p>
<pre>"""This module contains the function to test the vgg16 model performance."""<br/>from plotting import *<br/><br/>import os<br/>import pandas as pd<br/>import numpy as np<br/>import cv2 as cv<br/>import matplotlib.pyplot as plt<br/><br/><br/>def test(model, nrows=200, batch_size=128):<br/>    """Test trained vgg16."""<br/>    # load the train data<br/>    test = pd.read_csv('FLIC-full/test/test_joints.csv', header=None,<br/>                       nrows=nrows)<br/>    test_img_ids = test.iloc[:, 0].values<br/><br/>    # load validation images<br/>    test_images = np.array(<br/>            [cv.imread('FLIC-full/test/{}'.format(x)) for x in test_img_ids])<br/><br/>    # convert validation images to dtype float<br/>    test_images = test_images.astype(float)<br/><br/>    # joints<br/>    test_joints = test.iloc[:, 1:].values<br/><br/>    # evaluate<br/>    test_loss = model.evaluate(test_images, test_joints,<br/>                               verbose=0, batch_size=batch_size)<br/><br/>    # predict<br/>    predictions = model.predict(test_images, verbose=0, batch_size=batch_size)<br/><br/>    # folder to save the results<br/>    if not os.path.exists(os.path.join(os.getcwd(), 'FLIC-full/test_plot')):<br/>        os.mkdir('FLIC-full/test_plot')<br/><br/>    for i, (ids, image, joint, pred) in enumerate(zip(test_img_ids,<br/>                                                      test_images,<br/>                                                      test_joints,<br/>                                                      predictions)):<br/>        joints = joint.tolist()<br/>        joints = list(zip(joints[0::2], joints[1::2]))<br/><br/>        # plot original joints<br/>        image = plot_joints(image.astype(np.uint8), joints,<br/>                            groundtruth=True, text_scale=0.5)<br/><br/>        pred = pred.astype(np.uint8).tolist()<br/>        pred = list(zip(pred[0::2], pred[1::2]))<br/><br/>        # plot predicted joints<br/>        image = plot_joints(image.astype(np.uint8), pred,<br/>                            groundtruth=False, text_scale=0.5)<br/><br/>        # save resulting images with the same id<br/>        plt.imsave('FLIC-full/test_plot/'+ids, image)<br/>    return test_loss</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module 4 – train.py</h1>
                </header>
            
            <article>
                
<p><span>In this module, we have the <kbd>joint_coordinates()</kbd> and <kbd>training()</kbd> functions, as well as the calls to train and test the VGG16 model:</span></p>
<pre>"""This module imports other modules to train the vgg16 model."""<br/>from __future__ import print_function<br/><br/>from crop_resize_transform import model_data<br/>from test import test<br/><br/>import matplotlib.pyplot as plt<br/><br/>import random<br/>from scipy.io import loadmat<br/>import numpy as np<br/>import pandas as pd<br/>import cv2 as cv<br/>import glob<br/><br/>from sklearn.model_selection import train_test_split<br/><br/>from keras.models import Model<br/>from keras.optimizers import Adam<br/>from keras.layers import Flatten, Dense, Dropout<br/>from keras import backend as K<br/>from keras import applications<br/>K.clear_session()<br/><br/><br/># set seed for reproducibility<br/>seed_val = 9000<br/>np.random.seed(seed_val)<br/>random.seed(seed_val)<br/><br/># load the examples file<br/>examples = loadmat('FLIC-full/examples.mat')<br/># reshape the examples array<br/>examples = examples['examples'].reshape(-1,)<br/><br/># each coordinate corresponds to the the below listed body joints/locations<br/># in the same order<br/>joint_labels = ['lsho', 'lelb', 'lwri', 'rsho', 'relb', 'rwri', 'lhip',<br/>                'lkne', 'lank', 'rhip', 'rkne', 'rank', 'leye', 'reye',<br/>                'lear', 'rear', 'nose', 'msho', 'mhip', 'mear', 'mtorso',<br/>                'mluarm', 'mruarm', 'mllarm', 'mrlarm', 'mluleg', 'mruleg',<br/>                'mllleg', 'mrlleg']<br/><br/># print list of known joints<br/>known_joints = [x for i, x in enumerate(joint_labels) if i in np.r_[0:7, 9,<br/>                                                                    12:14, 16]]<br/>target_joints = ['lsho', 'lelb', 'lwri', 'rsho', 'relb',<br/>                 'rwri', 'leye', 'reye', 'nose']<br/># indices of the needed joints in the coordinates array<br/>joints_loc_id = np.r_[0:6, 12:14, 16]<br/><br/><br/>def joint_coordinates(joint):<br/>    """Store necessary coordinates to a list."""<br/>    joint_coor = []<br/>    # Take mean of the leye, reye, nose to obtain coordinates for the head<br/>    joint['head'] = (joint['leye']+joint['reye']+joint['nose'])/3<br/>    joint_coor.extend(joint['lwri'].tolist())<br/>    joint_coor.extend(joint['lelb'].tolist())<br/>    joint_coor.extend(joint['lsho'].tolist())<br/>    joint_coor.extend(joint['head'].tolist())<br/>    joint_coor.extend(joint['rsho'].tolist())<br/>    joint_coor.extend(joint['relb'].tolist())<br/>    joint_coor.extend(joint['rwri'].tolist())<br/>    return joint_coor<br/><br/><br/># load the indices matlab file<br/>train_indices = loadmat('FLIC-full/tr_plus_indices.mat')<br/># reshape the training_indices array<br/>train_indices = train_indices['tr_plus_indices'].reshape(-1,)<br/><br/># empty list to store train image ids<br/>train_ids = []<br/># empty list to store train joints<br/>train_jts = []<br/># empty list to store test image ids<br/>test_ids = []<br/># empty list to store test joints<br/>test_jts = []<br/><br/>for i, example in enumerate(examples):<br/>    # image id<br/>    file_name = example[3][0]<br/>    # joint coordinates<br/>    joint = example[2].T<br/>    # dictionary that goes into the joint_coordinates function<br/>    joints = dict(zip(target_joints,<br/>                      [x for k, x in enumerate(joint) if k in joints_loc_id]))<br/>    # obtain joints for the task<br/>    joints = joint_coordinates(joints)<br/>    # use train indices list to decide if an image is to be used for training<br/>    # or testing<br/>    if i in train_indices:<br/>        train_ids.append(file_name)<br/>        train_jts.append(joints)<br/>    else:<br/>        test_ids.append(file_name)<br/>        test_jts.append(joints)<br/><br/># Concatenate image ids dataframe and the joints dataframe and save it as a csv<br/>train_df = pd.concat([pd.DataFrame(train_ids), pd.DataFrame(train_jts)],<br/>                     axis=1)<br/>test_df = pd.concat([pd.DataFrame(test_ids), pd.DataFrame(test_jts)], axis=1)<br/>train_df.to_csv('FLIC-full/train_joints.csv', index=False, header=False)<br/>test_df.to_csv('FLIC-full/test_joints.csv', index=False, header=False)<br/><br/># load train_joints.csv<br/>train_data = pd.read_csv('FLIC-full/train_joints.csv', header=None)<br/># load test_joints.csv<br/>test_data = pd.read_csv('FLIC-full/test_joints.csv', header=None)<br/><br/># train image ids<br/>train_image_ids = train_data[0].values<br/># train joints<br/>train_joints = train_data.iloc[:, 1:].values<br/># test image ids<br/>test_image_ids = test_data[0].values<br/># test joints<br/>test_joints = test_data.iloc[:, 1:].values<br/><br/>model_data(train_image_ids, train_joints, train=True)<br/>model_data(test_image_ids, test_joints, train=False)<br/><br/># Number of epochs<br/>epochs = 3<br/># Batchsize<br/>batch_size = 128<br/># Optimizer for the model<br/>optimizer = Adam(lr=0.0001, beta_1=0.5)<br/># Shape of the input image<br/>input_shape = (224, 224, 3)<br/># Batch interval at which loss is to be stores<br/>store = 40<br/><br/># load the vgg16 model<br/>model = applications.VGG16(weights="imagenet", include_top=False,<br/>                           input_shape=input_shape)<br/><br/># set layers as non trainable<br/>for layer in model.layers:<br/>    layer.trainable = False<br/><br/># Adding custom Layers<br/>x = model.output<br/>x = Flatten()(x)<br/>x = Dense(1024, activation="relu")(x)<br/>x = Dropout(0.5)(x)<br/>x = Dense(1024, activation="relu")(x)<br/><br/># Dense layer with 14 neurons for predicting 14 numeric values<br/>predictions = Dense(14, activation="relu")(x)<br/># creating the final model<br/>model_final = Model(inputs=model.input, outputs=predictions)<br/># compile the model<br/>model_final.compile(loss="mean_squared_error", optimizer=optimizer)<br/># load the train data<br/>train = pd.read_csv('FLIC-full/train/train_joints.csv', header=None)<br/># split train into train and validation<br/>train_img_ids, val_img_ids, train_jts, val_jts = train_test_split(<br/>        train.iloc[:, 0], train.iloc[:, 1:], test_size=0.2, random_state=42)<br/><br/># load validation images<br/>val_images = np.array(<br/>    [cv.imread('FLIC-full/train/{}'.format(w)) for w in val_img_ids.values])<br/><br/># convert validation images to dtype float<br/>val_images = val_images.astype(float)<br/><br/><br/>def training(model, image_ids, joints, val_images, val_jts,<br/>             batch_size=128, epochs=2):<br/>    """Train vgg16."""<br/>    # empty train loss and validation loss list<br/>    loss_lst = []<br/>    val_loss_lst = []<br/>    count = 0 # counter<br/>    count_lst = []<br/><br/>    # create shuffled batches<br/>    batches = np.arange(len(image_ids)//batch_size)<br/>    data_idx = np.arange(len(image_ids))<br/>    random.shuffle(data_idx)<br/>    print('......Training......')<br/>    for epoch in range(epochs):<br/>        for batch in (batches):<br/>            # batch of training image ids<br/>            imgs = image_ids[data_idx[batch*batch_size:(batch+1)*batch_size:]]<br/>            # corresponding joints for the above images<br/>            jts = joints[data_idx[batch*batch_size:(batch+1)*batch_size:]]<br/>            # load the training image batch<br/>            batch_imgs = np.array(<br/>                    [cv.imread('FLIC-full/train/{}'.format(x)) for x in imgs])<br/>            # fit model on the batch<br/>            loss = model.train_on_batch(batch_imgs.astype(float), jts)<br/>            if batch % 40 == 0:<br/>                # evaluate model on validation set<br/>                val_loss = model.evaluate(val_images, val_jts, verbose=0,<br/>                                          batch_size=batch_size)<br/>                # store train and val loss<br/>                loss_lst.append(loss)<br/>                val_loss_lst.append(val_loss)<br/>                print('Epoch:{}, End of batch:{}, loss:{:.2f},val_loss:{:.2f}\<br/>                '.format(epoch+1, batch+1, loss, val_loss))<br/><br/>                count_lst.append(count)<br/>            else:<br/>                print('Epoch:{}, End of batch:{}, loss:{:.2f}\<br/>                '.format(epoch+1, batch+1, loss))<br/>            count += 1<br/>    count_lst.append(count)<br/>    loss_lst.append(loss)<br/>    val_loss = model.evaluate(val_images, val_jts, verbose=0,<br/>                              batch_size=batch_size)<br/>    val_loss_lst.append(val_loss)<br/>    print('Epoch:{}, End of batch:{}, VAL_LOSS:{:.2f}\<br/>    '.format(epoch+1, batch+1, val_loss))<br/>    return model, loss_lst, val_loss_lst, count_lst<br/><br/><br/>m, loss_lst, val_loss_lst, count_lst = training(model_final,<br/>                                                train_img_ids.values,<br/>                                                train_jts.values,<br/>                                                val_images,<br/>                                                val_jts.values,<br/>                                                epochs=epochs,<br/>                                                batch_size=batch_size)<br/><br/># plot the learning<br/>plt.style.use('ggplot')<br/>plt.figure(figsize=(10, 6))<br/>plt.plot(count_lst, loss_lst, marker='D', label='training_loss')<br/>plt.plot(count_lst, val_loss_lst, marker='o', label='validation_loss')<br/>plt.xlabel('Batches')<br/>plt.ylabel('Mean Squared Error')<br/>plt.title('Plot of MSE over time')<br/>plt.legend(loc='upper right')<br/>plt.show()<br/><br/># test and save results<br/>test_loss = test(m)<br/><br/># print test_loss<br/>print('Test Loss:', test_loss)<br/><br/>image_list = glob.glob('FLIC-full/test_plot/*.jpg')[8:16]<br/><br/>plt.figure(figsize=(16, 8))<br/>for i in range(8):<br/>    plt.subplot(2, 4, (i+1))<br/>    img = cv.imread(image_list[i])<br/>    plt.imshow(img, aspect='auto')<br/>    plt.axis('off')<br/>    plt.title('Green-True/Red-Predicted Joints')<br/><br/>plt.tight_layout()<br/>plt.show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conclusion</h1>
                </header>
            
            <article>
                
<p>This project was all about building a <strong>convolutional neural network </strong>(<strong>CNN</strong>) classifier to solve the problem of estimating 3D human poses using frames captured from movies. Our hypothetical use case was to enable visual effects specialists to easily estimate the pose of actors (from their shoulders, necks, and heads from the frames in a video. Our task was to build the intelligence for this application.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The modified VGG16 architecture we built using transfer learning has a test mean squared error loss of 454.81 squared units over 200 test images for each of the 14 coordinates (that is, the <em>7</em>(<em>x</em>, <em>y</em>) pairs). We can also say that the test root mean squared error over 200 test images <span>for each of the 14 coordinates</span> is 21.326 units. What does this mean?</p>
<p>The <strong>root mean squared error</strong> (<strong><span>RMSE</span></strong>), in this case, is a measure of how far off the predicted joint coordinates/joint pixel location are from the actual joint coordinate/joint pixel location.</p>
<p>An RMSE loss of 21.32 units is equivalent to having each predicted coordinate off by 21.32 pixels within an image of shape 224*224*3. The test results plotted in <em>Figure 13.20</em> represent this measure.</p>
<p>Each coordinate being off by 21.32 pixels is good at a general level, but we want to build a product that will be used in movies for which the margin for error is much less, and being off by 21 pixels is not acceptable. </p>
<p>To improve the model, you can do the following:</p>
<ul>
<li>Try using a lower learning rate for a larger number of epochs</li>
<li>Try using a different loss function (for example, <strong>mean absolute error</strong> (<strong>MAE</strong>))</li>
<li>Try using an even deeper model, such as RESNET50 or VGG19</li>
<li>Try centering and scaling the data</li>
<li>Get more data</li>
</ul>
<p>These are some of the additional steps you should take if you are interested in becoming an expert in this specific area once you are done with this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we successfully built a deep convolution neural network/VGG16 model in Keras on FLIC images. We got hands-on experience in preparing these images for modeling. We successfully implemented transfer learning, and understood that doing so will save us a lot of time. We defined some key hyperparameters as well in some places, and reasoned about why we used what we used. Finally, we tested the modified VGG16 model performance on unseen data and determined that we succeeded in achieving our goals.</p>


            </article>

            
        </section>
    </body></html>