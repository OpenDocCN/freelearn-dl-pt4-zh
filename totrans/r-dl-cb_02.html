<html><head></head><body>
        <section id="1GKCM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep Learning with R</h1>
                
            
            <article>
                
<p class="calibre2">This chapter will build a foundation for neural networks followed by deep learning foundation and trends. We will cover the following topics:</p>
<ul class="calibre12">
<li class="calibre13">Starting with logistic regression</li>
<li class="calibre13">Introducing the dataset</li>
<li class="calibre13">Performing logistic regression using H2O</li>
<li class="calibre13">Performing logistic regression using TensorFlow</li>
<li class="calibre13">Visualizing TensorFlow graphs</li>
<li class="calibre13">Starting with multilayer perceptrons</li>
<li class="calibre13">Setting up a neural network using H2O</li>
<li class="calibre13">Tuning hyper-parameters using grid searches in H2O</li>
<li class="calibre13">Setting up a neural network using MXNet</li>
<li class="calibre13">Setting up a neural network using TensorFlow</li>
</ul>


            </article>

            
        </section>
    

        <section id="1HIT81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Starting with logistic regression</h1>
                
            
            <article>
                
<p class="calibre2">Before we delve into neural networks and deep learning models, let's take a look at logistic regression, which can be viewed as a single layer neural network. Even the <strong class="calibre1">sigmoid</strong> function commonly used in logistic regression is used as an activation function in neural networks.</p>


            </article>

            
        </section>
    

        <section id="1IHDQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Logistic regression is a supervised machine learning approach for the classification of dichotomous/ordinal (order discrete) categories.</p>


            </article>

            
        </section>
    

        <section id="1JFUC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Logistic regression serves as a building block for complex neural network models using sigmoid as an activation function. The logistic function (or sigmoid) can be represented as follows:</p>
<div class="cdpaligncenter"><img src="../images/00003.jpeg" class="calibre28"/></div>
<p class="calibre2">The preceding sigmoid function forms a continuous curve with a value bound between [0, 1], as illustrated in the following screenshot:</p>
<div class="packt_figref"><img class="image-border20" src="../images/00005.gif"/></div>
<div class="packt_figref">Sigmoid functional form</div>
<p class="calibre2">The formulation of a logistic regression model can be written as follows:</p>
<div class="cdpaligncenter"><img src="../images/00008.jpeg" class="calibre29"/></div>
<p class="cdpalignleft">Here, <em class="calibre9">W</em> is the weight associated with features <em class="calibre9">X</em><strong class="calibre1">=</strong> [<em class="calibre9">x<sub class="calibre30">1</sub>, x<sub class="calibre30">2</sub>, ..., x<sub class="calibre30">m</sub></em>] and <em class="calibre9">b</em> is the model intercept, also known as the model bias. The whole objective is to optimize <em class="calibre9">W</em> for a given loss function such as cross entropy. Another view of the logistic regression model to attain <em class="calibre9">Pr</em>(<em class="calibre9">y=1|</em><strong class="calibre1">X)</strong> is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border21" src="../images/00009.jpeg"/></div>
<div class="packt_figref">Logistic regression architecture with the sigmoid activation function</div>


            </article>

            
        </section>
    

        <section id="1KEEU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introducing the dataset</h1>
                
            
            <article>
                
<p class="calibre2">This recipe shows how to prepare a dataset to be used to demonstrate different models.</p>


            </article>

            
        </section>
    

        <section id="1LCVG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">As logistic regression is a linear classifier, it assumes linearity in independent variables and log odds. Thus, in scenarios where independent features are linear-dependent on log odds, the model performs very well. Higher-order features can be included in the model to capture nonlinear behavior. Let's see how to build logistic regression models using major deep learning packages as discussed in the previous chapter. Internet connectivity will be required to download the dataset from the UCI repository.</p>


            </article>

            
        </section>
    

        <section id="1MBG21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="cdpalignleft">In this chapter, the Occupancy Detection dataset from the <strong class="calibre1">UC Irivine ML repository</strong> is used to build models on logistic regression and neural networks. It is an experimental dataset primarily used for binary classification to determine whether a room is occupied (1) or not occupied (0) based on multivariate predictors as described in the following table. The contributor of the dataset is <em class="calibre9">Luis Candanedo</em> from UMONS.</p>
<div class="title-page-name">
<div class="cdpalignleft1">
<p class="calibre31">Download the dataset at <a href="https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+" class="calibre25">https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+</a>.</p>
</div>
</div>
<p class="calibre2">There are three datasets tobe downloaded; however, we will use <kbd class="calibre10">datatraining.txt</kbd> for training/cross validation purposes and <kbd class="calibre10">datatest.txt</kbd> for testing purposes.</p>
<p class="calibre2">The dataset has seven attributes (including response occupancy) with 20,560 instances. The following table summarizes the attribute information:</p>
<table class="calibre32">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<div class="cdpaligncenter"><strong class="calibre1">Attribute</strong></div>
</td>
<td class="calibre8"><strong class="calibre1">Description</strong></td>
<td class="calibre8"><strong class="calibre1">Characteristic</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8">Date time</td>
<td class="calibre8">Year-month-day hour:minute:second format</td>
<td class="calibre8">Date</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Temperature</td>
<td class="calibre8">In Celsius</td>
<td class="calibre8">Real</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Relative Humidity</td>
<td class="calibre8">In %</td>
<td class="calibre8">Real</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Light</td>
<td class="calibre8">In Lux</td>
<td class="calibre8">Real</td>
</tr>
<tr class="calibre7">
<td class="calibre8">CO2</td>
<td class="calibre8">In ppm</td>
<td class="calibre8">Real</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Humidity ratio</td>
<td class="calibre8">Derived quantity from temperature and relative humidity, in kg water-vapor/kg-air</td>
<td class="calibre8">Real</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Occupancy</td>
<td class="calibre8">
<p class="calibre2">0 for not occupied;</p>
<p class="calibre2">1 for occupied</p>
</td>
<td class="calibre8">Binary class</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section id="1NA0K1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing logistic regression using H2O</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Generalized linear models</strong> (<strong class="calibre1">GLM</strong>) are widely used in both regression- and classification-based predictive analysis. These models optimize using maximum likelihood and scale well with larger datasets. In H2O, GLM has the flexibility to handle both L1 and L2 penalties (including elastic net). It supports Gaussian, Binomial, Poisson, and Gamma distributions of dependent variables. It is efficient in handling categorical variables, computing full regularizations, and performing distributed <em class="calibre9">n-fold</em> cross validations to control for model overfitting. It has a feature to optimize hyperparameters such as elastic net (α) using distributed grid searches along with handling upper and lower bounds for predictor attribute coefficients. It can also handle automatic missing value imputation. It uses the Hogwild method for optimization, a parallel version of stochastic gradient descent.</p>


            </article>

            
        </section>
    

        <section id="1O8H61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The previous chapter provided the details for the installation of H2O in R along with a working example using its web interface. To start modeling, load the <kbd class="calibre10">h20</kbd> package in the R environment:</p>
<pre class="calibre20">
require(h2o)
</pre>
<div class="title-page-name">
<p class="calibre2">Then, initialize a single-node H2O instance using the <kbd class="calibre10">h2o.init()</kbd> function on eight cores and instantiate the corresponding client module on the IP address <kbd class="calibre10">localhost</kbd> and port number <kbd class="calibre10">54321</kbd>:</p>
<pre class="calibre20">
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,min_mem_size = "20G",nthreads = 8)
</pre></div>
<p class="cdpalignleft">The H2O package has dependency on the Java JRE. Thus, it should be pre-installed before executing the initialization command.</p>


            </article>

            
        </section>
    

        <section id="1P71O1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">The section will demonstrate steps to build the GLM model using H2O.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Now, load the occupancy train and test datasets in R:</li>
</ol>
<pre class="calibre23">
# Load the occupancy data <br class="title-page-tagline"/>occupancy_train &lt;-read.csv("C:/occupation_detection/datatraining.txt",stringsAsFactors = T)<br class="title-page-tagline"/>occupancy_test &lt;- read.csv("C:/occupation_detection/datatest.txt",stringsAsFactors = T)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The following independent (<kbd class="calibre10">x</kbd>) and dependent (<kbd class="calibre10">y</kbd>) variables will be used to model GLM:</li>
</ol>
<pre class="calibre23">
# Define input (x) and output (y) variables"<br class="title-page-tagline"/>x = c("Temperature", "Humidity", "Light", "CO2", "HumidityRatio")<br class="title-page-tagline"/>y = "Occupancy"
</pre></div>
<div class="title-page-name">
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Based on the requirement for H2O, convert the dependent variables into factors as follows:</li>
</ol>
<pre class="calibre23">
# Convert the outcome variable into factor<br class="title-page-tagline"/>occupancy_train$Occupancy &lt;- as.factor(occupancy_train$Occupancy)<br class="title-page-tagline"/>occupancy_test$Occupancy &lt;- as.factor(occupancy_test$Occupancy)
</pre></div>
<div class="title-page-name">
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Then, convert the datasets to H2OParsedData objects:</li>
</ol>
<pre class="calibre23">
occupancy_train.hex &lt;- as.h2o(x = occupancy_train, destination_frame = "occupancy_train.hex")<br class="title-page-tagline"/>occupancy_test.hex &lt;- as.h2o(x = occupancy_test, destination_frame = "occupancy_test.hex")
</pre></div>
<div class="title-page-name">
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Once the data is loaded and converted to H2OParsedData objects, run a GLM model using the <kbd class="calibre10">h2o.glm</kbd> function. In the current setup, we intend to train for parameters such as five-fold cross validation, elastic net regularization (<em class="calibre9">α = 5</em>), and optimal regularization strength (with <kbd class="calibre10">lamda_search = TRUE</kbd>):</li>
</ol>
<pre class="calibre23">
# Train the model<br class="title-page-tagline"/>occupancy_train.glm &lt;- h2o.glm(x = x, # Vector of predictor variable names<br class="title-page-tagline"/>                               y = y, # Name of response/dependent variable<br class="title-page-tagline"/>                               training_frame = occupancy_train.hex, # Training data<br class="title-page-tagline"/>                               seed = 1234567,        # Seed for random numbers<br class="title-page-tagline"/>                               family = "binomial",   # Outcome variable<br class="title-page-tagline"/>                               lambda_search = TRUE,  # Optimum regularisation lambda<br class="title-page-tagline"/>                               alpha = 0.5,           # Elastic net regularisation<br class="title-page-tagline"/>                               nfolds = 5             # N-fold cross validation<br class="title-page-tagline"/>                               )
</pre></div>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">In addition to the preceding command, you can also define other parameters to fine-tune the model performance. The following list does not cover all the functional parameters, but covers some based on importance. The complete list of parameters can be found in the documentation of the <kbd class="calibre10">h2o</kbd> package.</li>
</ol>
<ul class="calibre12">
<li class="calibre16">
<ul class="calibre33">
<li class="calibre13">Specify the strategy of generating cross-validation samples such as random sampling, stratified sampling, modulo sampling, and auto (select) using fold_assignment. The sampling can also be performed on a particular attribute by specifying the column name (fold_column).</li>
<li class="calibre13">Option to handle skewed outcomes (imbalanced data) by specifying weights to each observation using weights_column or performing over/under sampling using balance_classes.</li>
<li class="calibre13">Option to handle missing values by mean imputation or observation skip using missing_values_handling.</li>
<li class="calibre13">Option to restrict the coefficients to be non-negative using non_negative and constrain their values using beta_constraints.</li>
<li class="calibre13">Option to provide prior probability for y==1(logistic regression) in the case of sampled data if its mean of response does not reflect the reality (prior).</li>
<li class="calibre13">Specify the variables to be considered for interactions (interactions).</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section id="1Q5IA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The performance of the model can be assessed using many metrics such as accuracy, <strong class="calibre1">Area under curve</strong> (<strong class="calibre1">AUC</strong>), misclassification error (%), misclassification error count, F1-score, precision, recall, specificity, and so on. However, in this chapter, the assessment of model performance is based on AUC.</p>
<p class="calibre2">The following is the training and cross validation accuracy of the trained model:</p>
<pre class="calibre20">
# Training accuracy (AUC)<br class="title-page-tagline"/>&gt; occupancy_train.glm@model$training_metrics@metrics$AUC<br class="title-page-tagline"/>[1] 0.994583<br class="title-page-tagline"/><br class="title-page-tagline"/># Cross validation accuracy (AUC)<br class="title-page-tagline"/>&gt; occupancy_train.glm@model$cross_validation_metrics@metrics$AUC<br class="title-page-tagline"/>[1] 0.9945057
</pre>
<div class="title-page-name">
<p class="calibre2">Now, let's assess the performance of the model on test data. The following code helps in predicting the outcome of the test data:</p>
<pre class="calibre20">
# Predict on test data<br class="title-page-tagline"/>yhat &lt;- h2o.predict(occupancy_train.glm, occupancy_test.hex)
</pre></div>
<div class="title-page-name">
<p class="calibre2">Then, evaluate the <kbd class="calibre10">AUC</kbd> value based on the actual test outcome as follows:</p>
<pre class="calibre20">
# Test accuracy (AUC)<br class="title-page-tagline"/>&gt; yhat$pmax &lt;- pmax(yhat$p0, yhat$p1, na.rm = TRUE) <br class="title-page-tagline"/>&gt; roc_obj &lt;- pROC::roc(c(as.matrix(occupancy_test.hex$Occupancy)),<br class="title-page-tagline"/>                       c(as.matrix(yhat$pmax)))<br class="title-page-tagline"/>&gt; auc(roc_obj)<br class="title-page-tagline"/>Area under the curve: 0.9915
</pre></div>
<div class="title-page-name">
<p class="calibre2">In H2O, one can also compute variable importance from the GLM model, as shown in the figure following this command:</p>
<pre class="calibre20">
#compute variable importance and performance<br class="title-page-tagline"/>h2o.varimp_plot(occupancy_train.glm, num_of_features = 5)
</pre></div>
<div class="cdpaligncenter"><img class="image-border22" src="../images/00130.jpeg"/></div>
<div class="packt_figref">Variable importance using H2O</div>


            </article>

            
        </section>
    

        <section id="1R42S1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">See also</h1>
                
            
            <article>
                
<div class="cdpalignleft1"><span class="calibre22">More functional parameters for <kbd class="calibre34">h2o.glm</kbd> can be found at <a href="https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm" target="_blank" class="calibre25">https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm</a>.</span></div>


            </article>

            
        </section>
    

        <section id="1S2JE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing logistic regression using TensorFlow</h1>
                
            
            <article>
                
<p class="cdpalignleft">In this section, we will cover the application of TensorFlow in setting up a logistic regression model. The example will use a similar dataset to that used in the H2O model setup.</p>


            </article>

            
        </section>
    

        <section id="1T1401-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="cdpalignleft">The previous chapter provided details for the installation of TensorFlow. The code for this section is created on Linux but can be run on any operating system. To start modeling, load the <kbd class="calibre10">tensorflow</kbd> package in the environment. R loads the default TensorFlow environment variable and also the NumPy library from Python in the <kbd class="calibre10">np</kbd> variable:</p>
<pre class="calibre20">
library("tensorflow") # Load TensorFlow <br class="title-page-tagline"/>np &lt;- import("numpy") # Load numpy library
</pre>


            </article>

            
        </section>
    

        <section id="1TVKI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The data is imported using a standard function from R, as shown in the following code.</p>
<ol class="calibre15">
<li value="1" class="calibre13">The data is imported using the <kbd class="calibre10">read.csv</kbd> file and transformed into the matrix format followed by selecting the features used to model as defined in <kbd class="calibre10">xFeatures</kbd> and <kbd class="calibre10">yFeatures</kbd><em class="calibre9">.</em> The next step in TensorFlow is to set up a graph to run optimization:</li>
</ol>
<pre class="calibre23">
# Loading input and test data<br class="title-page-tagline"/>xFeatures = c("Temperature", "Humidity", "Light", "CO2", "HumidityRatio")<br class="title-page-tagline"/>yFeatures = "Occupancy"<br class="title-page-tagline"/>occupancy_train &lt;-as.matrix(read.csv("datatraining.txt",stringsAsFactors = T))<br class="title-page-tagline"/>occupancy_test &lt;- as.matrix(read.csv("datatest.txt",stringsAsFactors = T))<br class="title-page-tagline"/><br class="title-page-tagline"/># subset features for modeling and transform to numeric values<br class="title-page-tagline"/>occupancy_train&lt;-apply(occupancy_train[, c(xFeatures, yFeatures)], 2, FUN=as.numeric) <br class="title-page-tagline"/>occupancy_test&lt;-apply(occupancy_test[, c(xFeatures, yFeatures)], 2, FUN=as.numeric)<br class="title-page-tagline"/><br class="title-page-tagline"/># Data dimensions<br class="title-page-tagline"/>nFeatures&lt;-length(xFeatures)<br class="title-page-tagline"/>nRow&lt;-nrow(occupancy_train)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Before setting up the graph, let's reset the graph using the following command:</li>
</ol>
<pre class="calibre23">
# Reset the graph<br class="title-page-tagline"/>tf$reset_default_graph()
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Additionally, let's start an interactive session as it will allow us to execute variables without referring to the session-to-session object:</li>
</ol>
<pre class="calibre23">
# Starting session as interactive session<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define the logistic regression model in TensorFlow:</li>
</ol>
<pre class="calibre23">
# Setting-up Logistic regression graph<br class="title-page-tagline"/>x &lt;- tf$constant(unlist(occupancy_train[, xFeatures]), shape=c(nRow, nFeatures), dtype=np$float32) # <br class="title-page-tagline"/>W &lt;- tf$Variable(tf$random_uniform(shape(nFeatures, 1L)))<br class="title-page-tagline"/>b &lt;- tf$Variable(tf$zeros(shape(1L)))<br class="title-page-tagline"/>y &lt;- tf$matmul(x, W) + b
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">The input feature <kbd class="calibre10">x</kbd> is defined as a constant as it will be an input to the system. The weight <kbd class="calibre10">W</kbd> and bias <kbd class="calibre10">b</kbd> are defined as variables that will be optimized during the optimization process. The y is set up as a symbolic representation between <kbd class="calibre10">x</kbd>, <kbd class="calibre10">W</kbd>, and <kbd class="calibre10">b</kbd>. The weight <kbd class="calibre10">W</kbd> is set up to initialize random uniform distribution and <kbd class="calibre10">b</kbd> is assigned the value zero.</li>
<li value="7" class="calibre13">The next step is to set up the cost function for logistic regression:</li>
</ol>
<pre class="calibre23">
# Setting-up cost function and optimizer<br class="title-page-tagline"/>y_ &lt;- tf$constant(unlist(occupancy_train[, yFeatures]), dtype="float32", shape=c(nRow, 1L))<br class="title-page-tagline"/>cross_entropy&lt;-tf$reduce_mean(tf$nn$sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name="cross_entropy"))<br class="title-page-tagline"/>optimizer &lt;- tf$train$GradientDescentOptimizer(0.15)$minimize(cross_entropy)
</pre>
<p class="calibre35">The variable <kbd class="calibre10">y_</kbd> is the response variable. Logistic regression is set up using cross entropy as the loss function. The loss function is passed to the gradient descent optimizer with a learning rate of 0.15. Before running the optimization, initialize the global variables:</p>
<pre class="calibre23">
# Start a session<br class="title-page-tagline"/>init &lt;- tf$global_variables_initializer()<br class="title-page-tagline"/>sess$run(init)
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Execute the gradient descent algorithm for the optimization of weights using cross entropy as the loss function:</li>
</ol>
<pre class="calibre23">
# Running optimization <br class="title-page-tagline"/>for (step in 1:5000) {<br class="title-page-tagline"/>  sess$run(optimizer)<br class="title-page-tagline"/>  if (step %% 20== 0)<br class="title-page-tagline"/>    cat(step, "-", sess$run(W), sess$run(b), "==&gt;", sess$run(cross_entropy), "n")<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="1UU541-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="cdpalignleft">The performance of the model can be evaluated using AUC:</p>
<pre class="calibre20">
# Performance on Train<br class="title-page-tagline"/>library(pROC) <br class="title-page-tagline"/>ypred &lt;- sess$run(tf$nn$sigmoid(tf$matmul(x, W) + b))<br class="title-page-tagline"/>roc_obj &lt;- roc(occupancy_train[, yFeatures], as.numeric(ypred))<br class="title-page-tagline"/><br class="title-page-tagline"/><br class="title-page-tagline"/># Performance on test<br class="title-page-tagline"/>nRowt&lt;-nrow(occupancy_test)<br class="title-page-tagline"/>xt &lt;- tf$constant(unlist(occupancy_test[, xFeatures]), shape=c(nRowt, nFeatures), dtype=np$float32)<br class="title-page-tagline"/>ypredt &lt;- sess$run(tf$nn$sigmoid(tf$matmul(xt, W) + b))<br class="title-page-tagline"/>roc_objt &lt;- roc(occupancy_test[, yFeatures], as.numeric(ypredt)).<br class="title-page-tagline"/><br class="title-page-tagline"/>
</pre>
<p class="calibre2">AUC can be visualized using the <kbd class="calibre10">plot.auc</kbd> function from the <kbd class="calibre10">pROC</kbd> package, as shown in the screenshot following this command. The performance for training and testing (hold-out) is very similar.</p>
<pre class="calibre20">
plot.roc(roc_obj, col = "green", lty=2, lwd=2)<br class="title-page-tagline"/>plot.roc(roc_objt, add=T, col="red", lty=4, lwd=2)
</pre>
<div class="packt_figref"><img class="image-border23" src="../images/00010.jpeg"/></div>
<div class="packt_figref">Performance of logistic regression using TensorFlow</div>


            </article>

            
        </section>
    

        <section id="1VSLM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Visualizing TensorFlow graphs</h1>
                
            
            <article>
                
<p class="calibre2">TensorFlow graphs can be visualized using TensorBoard. It is a service that utilizes TensorFlow event files to visualize TensorFlow models as graphs. Graph model visualization in TensorBoard is also used to debug TensorFlow models.</p>


            </article>

            
        </section>
    

        <section id="20R681-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">TensorBoard can be started using the following command in the terminal:</p>
<pre class="calibre20">
$ tensorboard --logdir home/log --port 6006 
</pre>
<p class="calibre2">The following are the major parameters for TensorBoard:</p>
<ul class="calibre12">
<li class="calibre13"><span><kbd class="calibre10">--logdir</kbd> : To map to the directory to load TensorFlow events</span></li>
<li class="calibre13"><span><span><kbd class="calibre10">--debug</kbd>: To increase log verbosity</span></span></li>
<li class="calibre13"><span><kbd class="calibre10">--host</kbd>: To define the host to listen to its localhost (<kbd class="calibre10">127.0.0.1</kbd>) by default</span></li>
<li class="calibre13"><span><kbd class="calibre10">--port</kbd>: To define the port to which TensorBoard will serve</span></li>
</ul>
<p class="calibre2">The preceding command will launch the TensorFlow service on localhost at port <kbd class="calibre10">6006</kbd>, as shown in the following screenshot:</p>
<div class="packt_figref"><img class="image-border24" src="../images/00132.jpeg"/></div>
<div class="packt_figref">TensorBoard</div>
<p class="calibre2">The tabs on the TensorBoard capture relevant data generated during graph execution.</p>


            </article>

            
        </section>
    

        <section id="21PMQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The section covers how to visualize TensorFlow models and output in TernsorBoard.</p>
<ol class="calibre15">
<li value="1" class="calibre13">To visualize summaries and graphs, data from TensorFlow can be exported using the <kbd class="calibre10">FileWriter</kbd> command from the summary module. A default session graph can be added using the following command:</li>
</ol>
<pre class="calibre23">
# Create Writer Obj for log<br class="title-page-tagline"/>log_writer = tf$summary$FileWriter('c:/log', sess$graph)
</pre>
<p class="calibre24">The graph for logistic regression developed using the preceding code is shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border25" src="../images/00067.jpeg"/></div>
<div class="packt_figref">Visualization of the logistic regression graph in TensorBoard</div>
<div class="packt_infobox">Details about symbol descriptions on TensorBoard can be found at <a href="https://www.tensorflow.org/get_started/graph_viz" class="calibre25">https://www.tensorflow.org/get_started/graph_viz</a>.</div>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Similarly, other variable summaries can be added to the TensorBoard using correct summaries, as shown in the following code:</li>
</ol>
<pre class="calibre23">
# Adding histogram summary to weight and bias variable<br class="title-page-tagline"/>w_hist = tf$histogram_summary("weights", W)<br class="title-page-tagline"/>b_hist = tf$histogram_summary("biases", b)
</pre>
<p class="calibre24">The summaries can be a very useful way to determine how the model is performing. For example, for the preceding case, the cost function for test and train can be studied to understand optimization performance and convergence.</p>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Create a cross entropy evaluation for test. An example script to generate the cross entropy cost function for test and train is shown in the following command:</li>
</ol>
<pre class="calibre23">
# Set-up cross entropy for test<br class="title-page-tagline"/>nRowt&lt;-nrow(occupancy_test)<br class="title-page-tagline"/>xt &lt;- tf$constant(unlist(occupancy_test[, xFeatures]), shape=c(nRowt, nFeatures), dtype=np$float32)<br class="title-page-tagline"/>ypredt &lt;- tf$nn$sigmoid(tf$matmul(xt, W) + b)<br class="title-page-tagline"/>yt_ &lt;- tf$constant(unlist(occupancy_test[, yFeatures]), dtype="float32", shape=c(nRowt, 1L))<br class="title-page-tagline"/>cross_entropy_tst&lt;-tf$reduce_mean(tf$nn$sigmoid_cross_entropy_with_logits(labels=yt_, logits=ypredt, name="cross_entropy_tst"))
</pre>
<p class="calibre24">The preceding code is similar to training cross entropy calculations with a different dataset. The effort can be minimized by setting up a function to return tensor objects.</p>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Add summary variables to be collected:</li>
</ol>
<pre class="calibre23">
# Add summary ops to collect data<br class="title-page-tagline"/>w_hist = tf$summary$histogram("weights", W)<br class="title-page-tagline"/>b_hist = tf$summary$histogram("biases", b)<br class="title-page-tagline"/>crossEntropySummary&lt;-tf$summary$scalar("costFunction", cross_entropy)<br class="title-page-tagline"/>crossEntropyTstSummary&lt;-tf$summary$scalar("costFunction_test", cross_entropy_tst)
</pre>
<p class="calibre24">The script defines the summary events to be logged in the file.</p>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Open the writing object, <kbd class="calibre10">log_writer</kbd>. It writes the default graph to the location, <kbd class="calibre10">c:/log</kbd>:</li>
</ol>
<pre class="calibre23">
# Create Writer Obj for log<br class="title-page-tagline"/>log_writer = tf$summary$FileWriter('c:/log', sess$graph)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Run the optimization and collect the summaries:</li>
</ol>
<pre class="calibre23">
for (step in 1:2500) {<br class="title-page-tagline"/>  sess$run(optimizer)<br class="title-page-tagline"/><br class="title-page-tagline"/>  # Evaluate performance on training and test data after 50 Iteration<br class="title-page-tagline"/>  if (step %% 50== 0){<br class="title-page-tagline"/>   ### Performance on Train<br class="title-page-tagline"/>   ypred &lt;- sess$run(tf$nn$sigmoid(tf$matmul(x, W) + b))<br class="title-page-tagline"/>   roc_obj &lt;- roc(occupancy_train[, yFeatures], as.numeric(ypred))<br class="title-page-tagline"/><br class="title-page-tagline"/>   ### Performance on Test<br class="title-page-tagline"/>   ypredt &lt;- sess$run(tf$nn$sigmoid(tf$matmul(xt, W) + b))<br class="title-page-tagline"/>   roc_objt &lt;- roc(occupancy_test[, yFeatures], as.numeric(ypredt))<br class="title-page-tagline"/>   cat("train AUC: ", auc(roc_obj), " Test AUC: ", auc(roc_objt), "n")<br class="title-page-tagline"/><br class="title-page-tagline"/>   # Save summary of Bias and weights<br class="title-page-tagline"/>   log_writer$add_summary(sess$run(b_hist), global_step=step)<br class="title-page-tagline"/>   log_writer$add_summary(sess$run(w_hist), global_step=step)<br class="title-page-tagline"/>   log_writer$add_summary(sess$run(crossEntropySummary), global_step=step)<br class="title-page-tagline"/>   log_writer$add_summary(sess$run(crossEntropyTstSummary), global_step=step)<br class="title-page-tagline"/>} }
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Collect all the summaries to a single tensor using the<kbd class="calibre10">merge_all</kbd> <span><span><span>command from the summary module:<br class="title-page-tagline"/></span></span></span></li>
</ol>
<pre class="calibre23">
summary = tf$summary$merge_all() 
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Write the summaries to the log file using the <kbd class="calibre10">log_writer</kbd> object:</li>
</ol>
<pre class="calibre23">
log_writer = tf$summary$FileWriter('c:/log', sess$graph)<br class="title-page-tagline"/>summary_str = sess$run(summary)<br class="title-page-tagline"/>log_writer$add_summary(summary_str, step)<br class="title-page-tagline"/>log_writer$close()
</pre>


            </article>

            
        </section>
    

        <section id="22O7C1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The section covers model performance visualization using TensorBoard. The cross entropy for train and test are recorded in the SCALARS tab, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border26" src="../images/00011.jpeg"/></div>
<div class="packt_figref">Cross entropy for train and test data</div>
<p class="calibre2">The objective function shows similar behaviors for train and test cost function; thus, the model seems to be stable for the given case with convergence attaining around 1,600 iterations.</p>


            </article>

            
        </section>
    

        <section id="23MNU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Starting with multilayer perceptrons</h1>
                
            
            <article>
                
<p class="calibre2">This section will focus on extending the logistic regression concept to neural networks.</p>
<div class="packt_infobox">The neural network, also known as <strong class="calibre36">Artificial neural network</strong> (<strong class="calibre36">ANN</strong>), is a computational paradigm that is inspired by the neuronal structure of the biological brain.</div>


            </article>

            
        </section>
    

        <section id="24L8G1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The ANN is a collection of artificial neurons that perform simple operations on the data; the output from this is passed to another neuron. The output generated at each neuron is called its <strong class="calibre1">activation function</strong>. An example of a multilayer perceptron model can be seen in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border27" src="../images/00012.jpeg"/></div>
<div class="packt_figref">An example of a multilayer neural network</div>
<p class="calibre2">Each link in the preceding figure is associated to weights processed by a neuron. Each neuron can be looked at as a processing unit that takes input processing and the output is passed to the next layer, as shown in the following screenshot:</p>
<div class="cdpaligncenter">
<div class="cdpaligncenter1"><img class="image-border28" src="../images/00101.jpeg"/></div>
</div>
<div class="packt_figref">An example of a neuron getting three inputs and one output</div>
<p class="calibre2">The preceding figure demonstrates three inputs combined at neuron to give an output that may be further passed to another neuron. The processing conducted at the neuron could be a very simple operation such as the input multiplied by weights followed by summation or a transformation operation such as the sigmoid activation function.</p>


            </article>

            
        </section>
    

        <section id="25JP21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">This section covers type activation functions in multilayer perceptrons. Activation is one of the critical component of ANN as it defines the output of that node based on the given input. There are many different activation functions used while building a neural network:</p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Sigmoid</strong>: The sigmoid activation function is a continuous function also known as a logistic function and has the form, <em class="calibre9">1/(1+exp(-x))</em>. The sigmoid function has a tendency to zero out the backpropagation terms during training leading to saturation in response. In TensorFlow, the sigmoid activation function is defined using the <kbd class="calibre10">tf.nn.sigmoid</kbd> function.</li>
<li class="calibre13"><strong class="calibre1">ReLU</strong>: Rectified linear unit <span><span>(ReLU) is one of the most famous continuous, but not smooth, activation functions used in neural networks to capture non-linearity. The ReLU function is defined as</span></span> <em class="calibre9">max(0,x)</em>. <span><span>In TensorFlow, the ReLU activation function is defined as</span></span> <kbd class="calibre10">tf.nn.relu</kbd><em class="calibre9">.</em></li>
<li class="calibre13"><span><strong class="calibre1">ReLU6</strong>: It caps the ReLU function at 6 and is defined as</span> <em class="calibre9">min(max(0,x), 6)</em>, <span>thus the value does not become very small or large. The function is defined in TensorFlow as</span> <kbd class="calibre10">tf.nn.relu6</kbd>.</li>
<li class="calibre13"><strong class="calibre1">tanh</strong>: Hypertangent is another smooth function used as an activation function in neural networks and is bound [ -1 to 1] and implemented as <kbd class="calibre10">tf.nn.tanh</kbd>.</li>
<li class="calibre13"><strong class="calibre1">softplus</strong>: It is a continuous version of ReLU, so the differential exists and is defined as <em class="calibre9">log(exp(x)+1)</em>. In TensorFlow the softplus is defined as <kbd class="calibre10">tf.nn.softplus</kbd>.</li>
</ul>


            </article>

            
        </section>
    

        <section id="26I9K1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">There's more...</h1>
                
            
            <article>
                
<p class="calibre2">There are three main neural network architectures in neural networks:</p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Feedforward ANN</strong>: This is a class of neural network models where the flow of information is unidirectional from input to output; thus, the architecture does not form any cycle. An example of a Feedforward network is shown in the following image:</li>
</ul>
<div class="cdpaligncenter"><img class="image-border29" src="../images/00013.jpeg"/></div>
<div class="packt_figref">Feedforward architecture of neural networks</div>
<ul class="calibre12">
<li class="calibre13">
<p class="calibre37"><strong class="calibre1">Feedback ANN</strong>: This is also known as the Elman recurrent network and is a class of neural networks where the error at the output node used as feedback to update iteratively to minimize errors. An example of a one layer Feedback neural network architecture is shown in the following image:</p>
</li>
</ul>
<div class="cdpaligncenter"><img class="image-border30" src="../images/00014.jpeg"/></div>
<div class="packt_figref">xFeedback architecture of neural networks</div>
<ul class="calibre12">
<li class="calibre13">
<p class="calibre37"><strong class="calibre1">Lateral ANN</strong>: This is a class of neural networks between Feedback and Feedforward neural networks with neurons interacting within layers. An example lateral neural network architecture is shown in the following image:</p>
</li>
</ul>
<div class="cdpaligncenter"><img class="image-border31" src="../images/00144.jpeg"/></div>
<div class="packt_figref">Lateral neural network architecture</div>


            </article>

            
        </section>
    

        <section id="27GQ61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">See also</h1>
                
            
            <article>
                
<p class="calibre2">More activation functions supported in TensorFlow can be found at <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_" class="calibre4">https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_.</a></p>


            </article>

            
        </section>
    

        <section id="28FAO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a neural network using H2O</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will cover the application of H2O in setting up a neural network. The example will use a similar dataset as used in logistic regression.</p>


            </article>

            
        </section>
    

        <section id="29DRA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">We first load all the required packages with the following code:</p>
<pre class="calibre20">
# Load the required packages<br class="title-page-tagline"/>require(h2o)
</pre>
<p class="calibre2">Then, initialize a single-node H2O instance using the <kbd class="calibre10">h2o.init()</kbd> function on eight cores and instantiate the corresponding client module on the IP address <kbd class="calibre10">localhost</kbd> and port number <kbd class="calibre10">54321</kbd>:</p>
<pre class="calibre20">
# Initialize H2O instance (single node)<br class="title-page-tagline"/>localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,min_mem_size = "20G",nthreads = 8)
</pre>


            </article>

            
        </section>
    

        <section id="2ACBS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The section shows how to build neural network using H20.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the occupancy train and test datasets in R:</li>
</ol>
<pre class="calibre23">
# Load the occupancy data <br class="title-page-tagline"/>occupancy_train &lt;-read.csv("C:/occupation_detection/datatraining.txt",stringsAsFactors = T)<br class="title-page-tagline"/>occupancy_test &lt;- read.csv("C:/occupation_detection/datatest.txt",stringsAsFactors = T)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The following independent (<kbd class="calibre10">x</kbd>) and dependent (<kbd class="calibre10">y</kbd>) variables will be used to model GLM:</li>
</ol>
<pre class="calibre23">
# Define input (x) and output (y) variables<br class="title-page-tagline"/>x = c("Temperature", "Humidity", "Light", "CO2", "HumidityRatio")<br class="title-page-tagline"/>y = "Occupancy"
</pre>
<div class="title-page-name">
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Based on the requirement by H2O, convert dependent variables to factors as follows:</li>
</ol>
<pre class="calibre23">
# Convert the outcome variable into factor<br class="title-page-tagline"/>occupancy_train$Occupancy &lt;- as.factor(occupancy_train$Occupancy)<br class="title-page-tagline"/>occupancy_test$Occupancy &lt;- as.factor(occupancy_test$Occupancy)
</pre></div>
<div class="title-page-name">
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Then convert the datasets to H2OParsedData objects:</li>
</ol>
</div>
<pre class="calibre23">
# Convert Train and Test datasets into H2O objects<br class="title-page-tagline"/>occupancy_train.hex &lt;- as.h2o(x = occupancy_train, destination_frame = "occupancy_train.hex")<br class="title-page-tagline"/>occupancy_test.hex &lt;- as.h2o(x = occupancy_test, destination_frame = "occupancy_test.hex")
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Once the data is loaded and converted to H2OParsedData objects, build a multilayer Feedforward neural network using the <kbd class="calibre10">h2o.deeplearning</kbd> function. In the current setup, the following parameters are used to build the NN model:</li>
</ol>
<ul class="calibre12">
<li class="calibre16">
<ul class="calibre33">
<li class="calibre13"><span>Single hidden layer with five neurons using</span> <kbd class="calibre10">hidden</kbd></li>
<li class="calibre13"><span>50 iterations using</span> <kbd class="calibre10">epochs</kbd></li>
<li class="calibre13"><span>Adaptive learning rate (</span><kbd class="calibre10">adaptive_rate</kbd><span>) instead of a fixed learning rate (rate)</span></li>
<li class="calibre13"><kbd class="calibre10">Rectifier</kbd> <span>activation function based on ReLU</span></li>
<li class="calibre13"><span>Five-fold cross validation using</span> <kbd class="calibre10">nfold</kbd></li>
</ul>
</li>
</ul>
<pre class="calibre23">
# H2O based neural network to Train the model <br class="title-page-tagline"/>occupancy.deepmodel &lt;- h2o.deeplearning(x = x, <br class="title-page-tagline"/>                                        y = y, <br class="title-page-tagline"/>                                        training_frame = occupancy_train.hex, <br class="title-page-tagline"/>                                        validation_frame = occupancy_test.hex, <br class="title-page-tagline"/>                                        standardize = F, <br class="title-page-tagline"/>                                        activation = "Rectifier", <br class="title-page-tagline"/>                                        epochs = 50, <br class="title-page-tagline"/>                                        seed = 1234567, <br class="title-page-tagline"/>                                        hidden = 5, <br class="title-page-tagline"/>                                        variable_importances = T,<br class="title-page-tagline"/>                                        nfolds = 5,<br class="title-page-tagline"/>                                        adpative_rate = TRUE)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">In addition to the command described in the recipe <em class="calibre9">Performing logistic regression using H2O</em>, you can also define other parameters to fine-tune the model performance. The following list does not cover all the functional parameters, but covers some based on importance. The complete list of parameters is available in the documentation of the <kbd class="calibre10">h2o</kbd> package.</li>
</ol>
<ul class="calibre12">
<li class="calibre16">
<ul class="calibre33">
<li class="calibre13">Option to initialize a model using a pretrained autoencoder model.</li>
<li class="calibre13">Provision to fine-tune the adaptive learning rate via an option to modify the time decay factor (<em class="calibre9">rho</em>) and smoothing factor (<em class="calibre9">epsilon</em>). In the case of a fixed learning rate (<em class="calibre9">rate</em>), an option to modify the annealing rate (<em class="calibre9">rate_annealing</em>) and decay factor between layers (<em class="calibre9">rate_decay</em>).</li>
<li class="calibre13">Option to initialize weights and biases along with weight distribution and scaling.</li>
<li class="calibre13">Stopping criteria based on the error fraction in the case of classification and mean squared errors with regard to regression (<em class="calibre9">classification_stop</em> and <em class="calibre9">regression_stop</em>). An option to also perform early stopping.</li>
<li class="calibre13">Option to improve distributed model convergence using the elastic averaging method with parameters such as moving rate and regularization strength.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section id="2BASE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The performance of the model can be assessed using many metrics such as accuracy, AUC, misclassification error (%), misclassification error count, F1-score, precision, recall, specificity, and so on. However, in this chapter, the assessment of the model performance is based on AUC.</p>
<p class="calibre2">The following is the training and cross validation accuracy for the trained model. The training and cross validation AUC is <kbd class="calibre10">0.984</kbd> and <kbd class="calibre10">0.982</kbd> respectively:</p>
<pre class="calibre20">
# Get the training accuracy (AUC)<br class="title-page-tagline"/>&gt; train_performance &lt;- h2o.performance(occupancy.deepmodel,train = T)<br class="title-page-tagline"/>&gt; train_performance@metrics$AUC<br class="title-page-tagline"/>[1] 0.9848667<br class="title-page-tagline"/><br class="title-page-tagline"/># Get the cross-validation accuracy (AUC)<br class="title-page-tagline"/>&gt; xval_performance &lt;- h2o.performance(occupancy.deepmodel,xval = T)<br class="title-page-tagline"/>&gt; xval_performance@metrics$AUC<br class="title-page-tagline"/>[1] 0.9821723
</pre>
<p class="calibre2">As we have already provided test data in the model (as a validation dataset), the following is its performance. The AUC on the test data is <kbd class="calibre10">0.991</kbd>.</p>
<pre class="calibre20">
# Get the testing accuracy(AUC)<br class="title-page-tagline"/>&gt; test_performance &lt;- h2o.performance(occupancy.deepmodel,valid = T)<br class="title-page-tagline"/>&gt; test_performance@metrics$AUC<br class="title-page-tagline"/>[1] 0.9905056
</pre>


            </article>

            
        </section>
    

        <section id="2C9D01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Tuning hyper-parameters using grid searches in H2O</h1>
                
            
            <article>
                
<p class="calibre2">H2O packages also allow you to perform hyper-parameter tuning using grid search (<kbd class="calibre10">h2o.grid</kbd>).</p>


            </article>

            
        </section>
    

        <section id="2D7TI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">We first load and initialize the H2O package with the following code:</p>
<pre class="calibre20">
# Load the required packages<br class="title-page-tagline"/>require(h2o)<br class="title-page-tagline"/><br class="title-page-tagline"/># Initialize H2O instance (single node)<br class="title-page-tagline"/>localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,min_mem_size = "20G",nthreads = 8)
</pre>
<p class="calibre2">The occupancy dataset is loaded, converted to hex format, and named <em class="calibre9">occupancy_train.hex</em>.</p>


            </article>

            
        </section>
    

        <section id="2E6E41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The section will focus on optimizing hyper parameters in H2O using grid searches.</p>
<ol class="calibre15">
<li value="1" class="calibre13">In our case, we will optimize for the activation function, the number of hidden layers (along with the number of neurons in each layer), <kbd class="calibre10">epochs</kbd>, and regularization lambda (<kbd class="calibre10">l1</kbd> and <kbd class="calibre10">l2</kbd>):</li>
</ol>
<pre class="calibre23">
# Perform hyper parameter tuning<br class="title-page-tagline"/>activation_opt &lt;- c("Rectifier","RectifierWithDropout", "Maxout","MaxoutWithDropout")<br class="title-page-tagline"/>hidden_opt &lt;- list(5, c(5,5))<br class="title-page-tagline"/>epoch_opt &lt;- c(10,50,100)<br class="title-page-tagline"/>l1_opt &lt;- c(0,1e-3,1e-4)<br class="title-page-tagline"/>l2_opt &lt;- c(0,1e-3,1e-4)<br class="title-page-tagline"/><br class="title-page-tagline"/>hyper_params &lt;- list(activation = activation_opt,<br class="title-page-tagline"/>                     hidden = hidden_opt,<br class="title-page-tagline"/>                     epochs = epoch_opt,<br class="title-page-tagline"/>                     l1 = l1_opt,<br class="title-page-tagline"/>                     l2 = l2_opt)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The following search criteria have been set to perform a grid search. Adding to the following list, one can also specify the type of stopping metric, the minimum tolerance for stopping, and the maximum number of rounds for stopping:</li>
</ol>
<pre class="calibre23">
#set search criteria<br class="title-page-tagline"/>search_criteria &lt;- list(strategy = "RandomDiscrete", max_models=300)
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Now, let's perform a grid search on the training data as follows:</li>
</ol>
<pre class="calibre23">
# Perform grid search on training data<br class="title-page-tagline"/>dl_grid &lt;- h2o.grid(x = x,<br class="title-page-tagline"/>                    y = y,<br class="title-page-tagline"/>                    algorithm = "deeplearning",<br class="title-page-tagline"/>                    grid_id = "deep_learn",<br class="title-page-tagline"/>                    hyper_params = hyper_params,<br class="title-page-tagline"/>                    search_criteria = search_criteria,<br class="title-page-tagline"/>                    training_frame = occupancy_train.hex,<br class="title-page-tagline"/>                    nfolds = 5)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Once the grid search is complete (here, there are 216 different models), the best model can be selected based on multiple metrics such as logloss, residual deviance, mean squared error, AUC, accuracy, precision, recall, f1, and so on. In our scenario, let's select the best model with the highest AUC:</li>
</ol>
<pre class="calibre23">
#Select best model based on auc<br class="title-page-tagline"/>d_grid &lt;- h2o.getGrid("deep_learn",sort_by = "auc", decreasing = T)<br class="title-page-tagline"/>best_dl_model &lt;- h2o.getModel(d_grid@model_ids[[1]])
</pre>


            </article>

            
        </section>
    

        <section id="2F4UM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The following is the performance of the grid-searched model on both the training and cross-validation datasets. We can observe that the AUC has increased by one unit in both training and cross-validation scenarios, after performing a grid search. The training and cross validation AUC after the grid search is <kbd class="calibre10">0.996</kbd> and <kbd class="calibre10">0.997</kbd> respectively.</p>
<pre class="calibre20">
# Performance on Training data after grid search<br class="title-page-tagline"/>&gt; train_performance.grid &lt;- h2o.performance(best_dl_model,train = T)<br class="title-page-tagline"/>&gt; train_performance.grid@metrics$AUC<br class="title-page-tagline"/>[1] 0.9965881<br class="title-page-tagline"/><br class="title-page-tagline"/># Performance on Cross validation data after grid search<br class="title-page-tagline"/>&gt; xval_performance.grid &lt;- h2o.performance(best_dl_model,xval = T)<br class="title-page-tagline"/>&gt; xval_performance.grid@metrics$AUC<br class="title-page-tagline"/>[1] 0.9979131
</pre>
<p class="calibre2">Now, let's assess the performance of the best grid-searched model on the test dataset. We can observe that the AUC has increased by 0.25 units after performing the grid search. The AUC on the test data is <kbd class="calibre10">0.993</kbd>.</p>
<pre class="calibre20">
# Predict the outcome on test dataset<br class="title-page-tagline"/>yhat &lt;- h2o.predict(best_dl_model, occupancy_test.hex) <br class="title-page-tagline"/><br class="title-page-tagline"/># Performance of the best grid-searched model on the Test dataset<br class="title-page-tagline"/>&gt; yhat$pmax &lt;- pmax(yhat$p0, yhat$p1, na.rm = TRUE) <br class="title-page-tagline"/>&gt; roc_obj &lt;- pROC::roc(c(as.matrix(occupancy_test.hex$Occupancy)), c(as.matrix(yhat$pmax)))<br class="title-page-tagline"/>&gt; pROC::auc(roc_obj)<br class="title-page-tagline"/>Area under the curve: 0.9932
</pre>


            </article>

            
        </section>
    

        <section id="2G3F81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a neural network using MXNet</h1>
                
            
            <article>
                
<p class="calibre2">The previous chapter provided the details for the installation of MXNet in R along with a working example using its web interface. To start modeling, load the MXNet package in the R environment.</p>


            </article>

            
        </section>
    

        <section id="2H1VQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Load the required packages:</p>
<pre class="calibre20">
# Load the required packages<br class="title-page-tagline"/>require(mxnet)
</pre>


            </article>

            
        </section>
    

        <section id="2I0GC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Load the occupancy train and test datasets in R:</li>
</ol>
<pre class="calibre23">
# Load the occupancy data <br class="title-page-tagline"/>occupancy_train &lt;-read.csv("C:/occupation_detection/datatraining.txt",stringsAsFactors = T)<br class="title-page-tagline"/>occupancy_test &lt;- read.csv("C:/occupation_detection/datatest.txt",stringsAsFactors = T)
</pre>
<div class="title-page-name">
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The following independent (<kbd class="calibre10">x</kbd>) and dependent (<kbd class="calibre10">y</kbd>) variables will be used to model GLM:</li>
</ol>
<pre class="calibre23">
# Define input (x) and output (y) variables<br class="title-page-tagline"/>x = c("Temperature", "Humidity", "Light", "CO2", "HumidityRatio")<br class="title-page-tagline"/>y = "Occupancy"
</pre></div>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Based on the requirement by MXNet, convert the train and test datasets to a matrix and ensure that the class of the outcome variable is numeric (instead of factor as in the case of H2O):</li>
</ol>
<pre class="calibre23">
# convert the train data into matrix<br class="title-page-tagline"/>occupancy_train.x &lt;- data.matrix(occupancy_train[,x])<br class="title-page-tagline"/>occupancy_train.y &lt;- occupancy_train$Occupancy<br class="title-page-tagline"/><br class="title-page-tagline"/># convert the test data into matrix<br class="title-page-tagline"/>occupancy_test.x &lt;- data.matrix(occupancy_test[,x])<br class="title-page-tagline"/>occupancy_test.y &lt;- occupancy_test$Occupancy
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Now, let's configure a neural network manually. First, <span>configure</span> a symbolic variable with a specific name. Then <span>configure</span> a symbolic fully connected network with five neurons in a single hidden layer followed with the softmax activation function with logit loss (or cross entropy loss). One can also create additional (fully connected) hidden layers with different activation functions.</li>
</ol>
<pre class="calibre23">
# Configure Neural Network structure <br class="title-page-tagline"/>smb.data &lt;- mx.symbol.Variable("data")<br class="title-page-tagline"/>smb.fc &lt;- mx.symbol.FullyConnected(smb.data, num_hidden=5) <br class="title-page-tagline"/>smb.soft &lt;- mx.symbol.SoftmaxOutput(smb.fc)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Once the neural network is configured, let's create (or train) the (Feedforward) neural network model using the <kbd class="calibre10">mx.model.FeedForward.create</kbd> function. The model is fine-tuned for parameters such as the number of iterations or epochs (<em class="calibre9">100</em>), the metric for evaluation (classification accuracy), the size of each iteration or epoch (100 observations), and the learning rate (<em class="calibre9">0.01</em>):</li>
</ol>
<pre class="calibre23">
# Train the network<br class="title-page-tagline"/>model.nn &lt;- mx.model.FeedForward.create(symbol = smb.soft,<br class="title-page-tagline"/>                                        X = occupancy_train.x,<br class="title-page-tagline"/>                                        y = occupancy_train.y,<br class="title-page-tagline"/>                                        ctx = mx.cpu(),<br class="title-page-tagline"/>                                        num.round = 100,<br class="title-page-tagline"/>                                        eval.metric = mx.metric.accuracy,<br class="title-page-tagline"/>                                        array.batch.size = 100,<br class="title-page-tagline"/>                                        learning.rate = 0.01)
</pre>


            </article>

            
        </section>
    

        <section id="2IV0U1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Now, let's assess the performance of the model on train and test datasets. The AUC on the train data is <kbd class="calibre10">0.978</kbd> and on the test data is <kbd class="calibre10">0.982</kbd>:</p>
<pre class="calibre20">
# Train accuracy (AUC)<br class="title-page-tagline"/>&gt; train_pred &lt;- predict(model.nn,occupancy_train.x)<br class="title-page-tagline"/>&gt; train_yhat &lt;- max.col(t(train_pred))-1<br class="title-page-tagline"/>&gt; roc_obj &lt;- pROC::roc(c(occupancy_train.y), c(train_yhat))<br class="title-page-tagline"/>&gt; pROC::auc(roc_obj)<br class="title-page-tagline"/>Area under the curve: 0.9786<br class="title-page-tagline"/><br class="title-page-tagline"/>#Test accuracy (AUC)<br class="title-page-tagline"/>&gt; test_pred &lt;- predict(nnmodel,occupancy_test.x)<br class="title-page-tagline"/>&gt; test_yhat &lt;- max.col(t(test_pred))-1<br class="title-page-tagline"/>&gt; roc_obj &lt;- pROC::roc(c(occupancy_test.y), c(test_yhat))<br class="title-page-tagline"/>&gt; pROC::auc(roc_obj)<br class="title-page-tagline"/>Area under the curve: 0.9824
</pre>


            </article>

            
        </section>
    

        <section id="2JTHG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a neural network using TensorFlow</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will cover an application of TensorFlow in setting up a two-layer neural network model.</p>


            </article>

            
        </section>
    

        <section id="2KS221-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="cdpalignleft">To start modeling, load the <kbd class="calibre10">tensorflow</kbd> package in the environment. R loads the default tf environment variable and also the NumPy library from Python in the <kbd class="calibre10">np</kbd> variable:</p>
<pre class="calibre20">
library("tensorflow") # Load Tensorflow <br class="title-page-tagline"/>np &lt;- import("numpy") # Load numpy library
</pre>


            </article>

            
        </section>
    

        <section id="2LQIK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The data is imported using the standard function from R, as shown in the following code. The data is imported using the <kbd class="calibre10">read.csv</kbd> file and transformed into the matrix format followed by selecting the features used for the modeling as defined in <kbd class="calibre10">xFeatures</kbd> and <kbd class="calibre10">yFeatures</kbd><em class="calibre9">:</em></li>
</ol>
<pre class="calibre23">
# Loading input and test data<br class="title-page-tagline"/>xFeatures = c("Temperature", "Humidity", "Light", "CO2", "HumidityRatio")<br class="title-page-tagline"/>yFeatures = "Occupancy"<br class="title-page-tagline"/>occupancy_train &lt;-as.matrix(read.csv("datatraining.txt",stringsAsFactors = T))<br class="title-page-tagline"/>occupancy_test &lt;- as.matrix(read.csv("datatest.txt",stringsAsFactors = T))<br class="title-page-tagline"/><br class="title-page-tagline"/># subset features for modeling and transform to numeric values<br class="title-page-tagline"/>occupancy_train&lt;-apply(occupancy_train[, c(xFeatures, yFeatures)], 2, FUN=as.numeric) <br class="title-page-tagline"/>occupancy_test&lt;-apply(occupancy_test[, c(xFeatures, yFeatures)], 2, FUN=as.numeric)<br class="title-page-tagline"/><br class="title-page-tagline"/># Data dimensions<br class="title-page-tagline"/>nFeatures&lt;-length(xFeatures)<br class="title-page-tagline"/>nRow&lt;-nrow(occupancy_train)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Now load both the network and model parameters. The network parameters define the structure of the neural network and the model parameters define its tuning criteria. As stated earlier, the neural network is built using two hidden layers, each with five neurons. The <kbd class="calibre10">n_input</kbd> parameter defines the number of independent variables and <kbd class="calibre10">n_classes</kbd> defines one fewer than the number of output classes. In cases where the output variable is one-hot encoded (one attribute with yes occupancy and a second attribute with no occupancy), then <kbd class="calibre10">n_classes</kbd> will be 2L (equal to the number of one-hot encoded attributes). Among model parameters, the learning rate is <kbd class="calibre10">0.001</kbd> and the number of epochs (or iterations) for model building is <kbd class="calibre10">10000</kbd>:</li>
</ol>
<pre class="calibre23">
# Network Parameters<br class="title-page-tagline"/>n_hidden_1 = 5L # 1st layer number of features<br class="title-page-tagline"/>n_hidden_2 = 5L # 2nd layer number of features<br class="title-page-tagline"/>n_input = 5L    # 5 attributes<br class="title-page-tagline"/>n_classes = 1L  # Binary class<br class="title-page-tagline"/><br class="title-page-tagline"/># Model Parameters<br class="title-page-tagline"/>learning_rate = 0.001<br class="title-page-tagline"/>training_epochs = 10000
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">The next step in TensorFlow is to set up a graph to run the optimization. Before setting up the graph, let's reset the graph using the following command:</li>
</ol>
<pre class="calibre23">
# Reset the graph<br class="title-page-tagline"/>tf$reset_default_graph()
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Additionally, let's start an interactive session as it will allow us to execute variables without referring to the session-to-session object:</li>
</ol>
<pre class="calibre23">
# Starting session as interactive session<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">The following script defines the graph input (x for independent variables and y for dependent variable). <span>The input feature</span> <kbd class="calibre10">x</kbd> <span>is defined as a constant as it will be input to the system. Similarly, the output feature</span> <kbd class="calibre10">y</kbd> <span>is also defined as a constant with the <kbd class="calibre10">float32</kbd> type:</span></li>
</ol>
<pre class="calibre23">
# Graph input<br class="title-page-tagline"/>x = tf$constant(unlist(occupancy_train[,xFeatures]), shape=c(nRow, n_input), dtype=np$float32)<br class="title-page-tagline"/>y = tf$constant(unlist(occupancy_train[,yFeatures]), dtype="float32", shape=c(nRow, 1L))
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Now, let's create a multilayer perceptron with two hidden layers. Both the hidden layers are built using the ReLU activation function and the output layer is built using the linear activation function. The weights and biases are defined as variables that will be optimized during the optimization process. The initial values are randomly selected from a normal distribution. The following script is used to initialize and store a hidden layer's weights and biases along with a mulitilayer perceptron model:</li>
</ol>
<div class="title-page-name">
<pre class="calibre23">
# Initializes and store hidden layer's weight &amp; bias<br class="title-page-tagline"/>weights = list(<br class="title-page-tagline"/> "h1" = tf$Variable(tf$random_normal(c(n_input, n_hidden_1))),<br class="title-page-tagline"/> "h2" = tf$Variable(tf$random_normal(c(n_hidden_1, n_hidden_2))),<br class="title-page-tagline"/> "out" = tf$Variable(tf$random_normal(c(n_hidden_2, n_classes)))<br class="title-page-tagline"/>)<br class="title-page-tagline"/>biases = list(<br class="title-page-tagline"/> "b1" = tf$Variable(tf$random_normal(c(1L,n_hidden_1))),<br class="title-page-tagline"/> "b2" = tf$Variable(tf$random_normal(c(1L,n_hidden_2))),<br class="title-page-tagline"/> "out" = tf$Variable(tf$random_normal(c(1L,n_classes)))<br class="title-page-tagline"/>)<br class="title-page-tagline"/><br class="title-page-tagline"/># Create model<br class="title-page-tagline"/>multilayer_perceptron &lt;- function(x, weights, biases){<br class="title-page-tagline"/> # Hidden layer with RELU activation<br class="title-page-tagline"/> layer_1 = tf$add(tf$matmul(x, weights[["h1"]]), biases[["b1"]])<br class="title-page-tagline"/> layer_1 = tf$nn$relu(layer_1)<br class="title-page-tagline"/> # Hidden layer with RELU activation<br class="title-page-tagline"/> layer_2 = tf$add(tf$matmul(layer_1, weights[["h2"]]), biases[["b2"]])<br class="title-page-tagline"/> layer_2 = tf$nn$relu(layer_2)<br class="title-page-tagline"/> # Output layer with linear activation<br class="title-page-tagline"/> out_layer = tf$matmul(layer_2, weights[["out"]]) + biases[["out"]]<br class="title-page-tagline"/> return(out_layer)<br class="title-page-tagline"/>}
</pre></div>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Now, construct the model using the initialized <kbd class="calibre10">weights</kbd> and <kbd class="calibre10">biases</kbd>:</li>
</ol>
<pre class="calibre23">
pred = multilayer_perceptron(x, weights, biases)
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">The next step is to define the <kbd class="calibre10">cost</kbd> and <kbd class="calibre10">optimizer</kbd> functions of the neural network:</li>
</ol>
<pre class="calibre23">
# Define cost and optimizer<br class="title-page-tagline"/>cost = tf$reduce_mean(tf$nn$sigmoid_cross_entropy_with_logits(logits=pred, labels=y))<br class="title-page-tagline"/>optimizer = tf$train$AdamOptimizer(learning_rate=learning_rate)$minimize(cost)
</pre>
<ol start="9" class="calibre15">
<li value="9" class="calibre13">The neural network is set up using sigmoid cross entropy as the cost function. The cost function is then passed to a gradient descent optimizer (Adam) with a learning rate of 0.001. <span>Before running the optimization, initialize the global variables as follows:</span></li>
</ol>
<pre class="calibre23">
# Initializing the global variables<br class="title-page-tagline"/>init = tf$global_variables_initializer()<br class="title-page-tagline"/>sess$run(init)
</pre>
<ol start="10" class="calibre15">
<li value="10" class="calibre13">Once the global variables are initialized along with the cost and optimizer functions, let's begin training on the train dataset:</li>
</ol>
<pre class="calibre23">
# Training cycle<br class="title-page-tagline"/>for(epoch in 1:training_epochs){<br class="title-page-tagline"/>    sess$run(optimizer)<br class="title-page-tagline"/>   if (epoch %% 20== 0)<br class="title-page-tagline"/>    cat(epoch, "-", sess$run(cost), "n") <br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="2MP361-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The performance of the model can be evaluated using AUC:</p>
<pre class="calibre20">
# Performance on Train<br class="title-page-tagline"/>library(pROC) <br class="title-page-tagline"/>ypred &lt;- sess$run(tf$nn$sigmoid(multilayer_perceptron(x, weights, biases)))<br class="title-page-tagline"/>roc_obj &lt;- roc(occupancy_train[, yFeatures], as.numeric(ypred))<br class="title-page-tagline"/><br class="title-page-tagline"/># Performance on Test<br class="title-page-tagline"/>nRowt&lt;-nrow(occupancy_test)<br class="title-page-tagline"/>xt &lt;- tf$constant(unlist(occupancy_test[, xFeatures]), shape=c(nRowt, nFeatures), dtype=np$float32) #<br class="title-page-tagline"/>ypredt &lt;- sess$run(tf$nn$sigmoid(multilayer_perceptron(xt, weights, biases)))<br class="title-page-tagline"/>roc_objt &lt;- roc(occupancy_test[, yFeatures], as.numeric(ypredt))
</pre>
<p class="cdpalignleft"><span>AUC can be visualized using the</span> <kbd class="calibre10">plot.auc</kbd> <span>function from the <kbd class="calibre10">pROC</kbd> package, as shown in the image following the next command. The performance of train and test (hold out) is very similar.</span></p>
<pre class="calibre20">
plot.roc(roc_obj, col = "green", lty=2, lwd=2)<br class="title-page-tagline"/>plot.roc(roc_objt, add=T, col="red", lty=4, lwd=2)
</pre>
<div class="cdpaligncenter"><img class="image-border32" src="../images/00026.jpeg"/></div>
<div class="packt_figref">Performance of multilayer perceptron using TensorFlow</div>


            </article>

            
        </section>
    

        <section id="2NNJO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">There's more...</h1>
                
            
            <article>
                
<p class="cdpalignleft">Neural networks are based on the philosophy from the brain; however, the brain consists of around 100 billion neurons with each neuron connected to 10,000 other neurons. Neural networks developed in the early 90s faced a lot of challenges in building deeper neural networks due to computation and algorithmic limitations.</p>
<p class="cdpalignleft">With advances in big data, computational resources (such as GPUs), and better algorithms, the concept of deep learning has emerged and allows us to capture a deeper representation from all kinds of data such as text, image, audio, and so on.<br class="title-page-tagline"/>
<strong class="calibre1"><br class="title-page-tagline"/></strong></p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Trends in Deep Learning</strong>: Deep learning is an advance <span>on neural networks, which are very much driven by technology enhancement. The main factors that have impacted the development of deep learning as a dominant area in artificial intelligence are as follows:</span></li>
</ul>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Computational power</strong>: The consistency of Moore's law, which states that the acceleration power of hardware will double every two years, helped in training more layers and bigger data within time limitations</li>
<li class="calibre13"><strong class="calibre1">Storage and better compression algorithms</strong>: The ability to store big models due to cheaper storage and better compression algorithms have pushed this area with practitioners focusing on capturing real-time data feeds in the form of image, text, audio, and video formats</li>
<li class="calibre13"><strong class="calibre1">Scalability</strong>: The ability to scale from a simple computer to a farm or using GPU devices has given a great boost to training deep learning models</li>
<li class="calibre13"><strong class="calibre1">Deep learning architectures</strong>: With new architectures such as Convolution Neural network, re-enforcement learning has provided a boost to what we can learn and also helped expedite learning rates</li>
<li class="calibre13"><strong class="calibre1">Cross-platform programming:</strong> The ability to program and build models in a cross-platform architecture significantly helped increase the user base and in drastic development in the domain</li>
<li class="calibre13"><strong class="calibre1">Transfer learning:</strong> This allows reusing pretrained models and further helps in significantly reducing training times</li>
</ul>
<p class="cdpalignleft"/>


            </article>

            
        </section>
    </body></html>