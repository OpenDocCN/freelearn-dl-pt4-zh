- en: Cats and Dogs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml), *Your First
    Classifier*, we constructed a simple neural network for our character recognition
    effort. We ended the chapter with commendable mid-80% accuracy. Good start, but
    we can do much better!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will retrofit our earlier classifier with far more powerful
    network architecture. Then, we'll delve into a much more difficult problem—handling
    color images from the CIFAR-10 dataset. The images will be much more difficult
    (cats, dogs, airplanes, and so on), so we'll bring more powerful tools to the
    table—specifically, a convolutional neural network. Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting notMNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start our effort incrementally by trying the technical changes on the
    `notMNIST` dataset we used in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. You can write the code as you go through the chapter,
    or work on the book''s repository at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are not many substantial changes here. The real horsepower is already
    imported with the `tensorflow` package. You'll notice that we reuse our `data_utils`
    work from before. However, we'll need some changes there.
  prefs: []
  type: TYPE_NORMAL
- en: The only difference from before is the `math` package, which we will use for
    ancillary `math` functions, such as `ceiling`.
  prefs: []
  type: TYPE_NORMAL
- en: Program configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at our old program configurations, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need more configurations this time. Here is what we will use now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first four configurations are familiar:'
  prefs: []
  type: TYPE_NORMAL
- en: We will still train for a certain number of steps (`num_steps`), just as we
    did earlier. But, you'll notice the number of steps has gone up. They will get
    even higher because our datasets will be more complex and require more training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will revisit subtleties around the learning rate (`learning_rate`) later,
    but to start with, you are already familiar with it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will review results intermediately every five hundred steps, which is trivially
    controlled by the `data_showing_step` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `log_location` controls where our TensorBoard logs are dumped. We are
    quite familiar with this from [Chapter 3](a6bb2a79-d492-4620-a28b-72ec62523593.xhtml),
    *The TensorFlow Toolbox*. We will use it again in this chapter but without explanations
    this time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next configuration—the **random seed** (`SEED`) variable - can be helpful.
    This can be left unset and TensorFlow will randomize numbers on each run. However,
    having a `seed` variable set, and constant across runs, will allow consistency
    from run to run as we debug our system. If you do use it, which you should do
    to start off, you can set it to any number you wish: your birthday, anniversary
    date, first phone number, or lucky number. I use the ZIP code for my beloved neighborhood.
    Enjoy the small things.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will encounter seven new variables—`batch_size`, `patch_size`, `depth_inc`,
    `num_hidden_inc`, `conv_layers`, `stddev`, and `dropout_prob`. These are at the
    heart of how our newer, more advanced **Convolutional neural networks** (**CNNs**)
    works and will be introduced in context as we explore the network we're using.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs are more advanced neural networks specialized for machine learning with
    images. Unlike the hidden layers we used before, CNNs have some layers that are
    not fully connected. These convolutional layers have depth in addition to just
    width and height. The general principle is that an image is analyzed patch by
    patch. We can visualize the 7x7 patch in the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83aa17bb-6c42-4643-b9b5-81c484187653.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This reflects a 32x32 greyscale image, with a 7x7 patch. Example of sliding
    the patch from left to right is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdaa4b88-36da-4346-8a89-59584f189dcd.png)![](img/461f4a5e-855a-41cc-8dc8-fae26ab4656c.png)'
  prefs: []
  type: TYPE_IMG
- en: If this were a color image, we'd be sliding our patch simultaneously over three
    identical layers.
  prefs: []
  type: TYPE_NORMAL
- en: You probably noticed that we slid the patch over by one pixel. That is a configuration
    as well; we could have slid more, perhaps by two or even three pixels each time.
    This is the stride configuration. As you can guess, the larger the stride, the
    fewer the patches we will end up covering and thus, the smaller the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix math, which we will not get into here, is performed to reduce the patch
    (with the full depth driven by the number of channels) into an output depth column.
    The output is just a single in height and width but many pixels deep. As we will
    slide the patch across, over, and across iteratively, the sequence of depth columns
    form a block with a new length, width, and height.
  prefs: []
  type: TYPE_NORMAL
- en: There is another configuration at play here—the padding along the sides of the
    image. As you can imagine, the more padding you have the more room the patch has
    to slide and veer off the edge of the image. This allows more strides and thus,
    a larger length and width for the output volume. You'll see this in the code later
    as `padding='SAME'` or `padding='VALID'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how these add up. We will first select a patch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cfad793-1383-4f9d-9b06-f7f4b4c88927.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the patch is not just the square, but the full depth (for color images):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73820b82-074d-41dc-ae5c-53cd8b6981b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then convolve that into a 1x1 volume, but with depth, as shown in the
    following diagram. The depth of the resulting volume is configurable and we will
    use `inct_depth` for this configuration in our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05389cd9-d779-4c51-b334-0d7609676e19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, as we slide the patch across, over, and across again, through the
    original image, we will produce many such 1x1xN volumes, which itself creates
    a volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c88a030-d437-434d-907f-c037089ade35.png)'
  prefs: []
  type: TYPE_IMG
- en: We will then convolve that into a 1x1 volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will squeeze each layer of the resulting volume using a `POOL`
    operation. There are many types, but simple **max pooling** is typical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01223d35-9aa6-4dbb-ba9c-3df61cadfa9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Much like with the sliding patches we used earlier, there will be a patch (except
    this time, we will take the maximum number of the patch) and a stride (this time,
    we'll want a larger stride to squeeze the image). We are essentially reducing
    the size. Here, we will use a 3x3 patch with a stride of 2.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have been introduced to convolutional neural networks, let''s revisit
    the configurations that we encountered earlier: `batch_size`, `patch_size`, `depth_inc`,
    `num_hidden_inc`, `conv_layers`, `stddev`, and `dropout_prob`:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size (`batch_size`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patch size (`patch_size`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth increment (`depth_inc`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number hidden increment (`num_hidden_inc`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers (`conv_layers`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard deviation (`stddev`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout probability (`dropout_prob`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing the convolutional network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will skip explanations for the two utility functions, reformat and accuracy,
    as we''ve already encountered these in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. Instead, we will jump directly to the neural network
    configuration. For comparison, the following figure shows our model from [Chapter
    2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml), *Your First Classifier*, and the
    next figure shows our new model. We''ll run the new model on the same `notMNIST`
    dataset to see the accuracy boost that we will get (hint: good news!):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ab3311b-ae1f-46ce-96c4-b3f095eae98b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure is our new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/365f9caf-dc2c-4ac9-bfd2-1df7927d6cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will encounter a `helper` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will call it later, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `fc_first_layer_dimen` function calculates the dimensions of the first fully
    connected layer. Recall how CNN's typically use a series of layers with a smaller
    window layer after layer. Here, we've decided to reduce the dimensions by half
    for each convolutional layer we used. This also shows why having input images
    highly divisible by powers of two makes things nice and clean.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now parse the actual network. This is generated using the `nn_model` method
    and called later when training the model, and again when testing against the validation
    and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall how CNN''s are usually composed of the following layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectified linear unit layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional layers are usually paired with **RELU** layers and repeated.
    That is what we've done—we've got three nearly identical **CONV-RELU** layers
    stacked on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the paired layers appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The major difference across the three nearly identical layers (`Layer_1`, `Layer_2`,
    and `Layer_3`) is how the output of one is fed to the next in a series. So, the
    first layer begins by taking in data (the image data) but the second layer begins
    by taking in the pooling layer output from the first layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the third layer begins by taking in the pooling layer output from
    the second layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is another major difference across the three `CONV`-`RELU` layers, that
    is, the layers get squeezed. It might help to peek at the `conv` variable after
    each layer is declared using a couple of `print` statements like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will reveal the following structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We ran this with the `notMNIST` dataset, so we will see an original input size
    of 28x28 to no surprise. More interesting are the sizes of successive layers—14x14
    and 7x7\. Notice how the filters for successive convolutional layers are squeezed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make things more interesting and examine the entire stack. Add the following
    `print` statements to peek at the `CONV`, `RELU`, and `POOL` layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add similar statements after the other two `CONV`-`RELU`-`POOL` stacks and
    you''ll find the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We will ignore the outputs from the validation and test instances (those are
    the same, except with a height of 10000 instead of `32` as we're processing the
    validation and test sets rather than a minibatch).
  prefs: []
  type: TYPE_NORMAL
- en: We will see from the outputs how the dimension is squeezed at the `POOL` layer
    (`28` to `14`) and how that squeeze then carries to the next `CONV` layer. At
    the third and final `POOL` layer, we will end up with a 4x4 size.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another feature on the final `CONV` stack—a `dropout` layer that we
    will use when training, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This layer utilizes the `dropout_prob = 0.8` configuration we set earlier. It
    randomly drops neurons on the layer to prevent overfitting by disallowing nodes
    from coadapting to neighboring nodes with dropouts; they can never rely on a particular
    node being present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed through our network. We''ll find a fully connected layer followed
    by a `RELU`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will end with a fully connected layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is typical for the convolutional network. Typically, we will end up with
    a fully connected, `RELU` layer and finally a fully connected layer that holds
    scores for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We skipped some details along the way. Most of our layers were initialized
    with three other values—`weights`, `biases`, and `strides`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a019ee4-889d-4cec-a30c-b3ca3bce9577.png)'
  prefs: []
  type: TYPE_IMG
- en: The `weights` and `biases` are themselves initialized with other variables.
    I didn't say this will be easy.
  prefs: []
  type: TYPE_NORMAL
- en: The most important variable here is `patch_size`, which denotes the size of
    the filter we slide across the image. Recall that we set this to 5 early on, so
    we will use 5x5 patches. We will also get reintroduced to the `stddev` and `depth_inc`
    configurations that we set up earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Fulfilment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Likely, by now, you must have many questions running through your mind—why three
    convolutional layers rather than two or four? Why a stride of one? Why a patch
    size of five? Why end up with fully connected layers rather than start with them?
  prefs: []
  type: TYPE_NORMAL
- en: There is some method to the madness here. At the core, CNN's are built around
    image processing and patches are built around the features being sought. Why some
    configurations work well while others do not is not fully understood, though general
    rules do follow intuition. The exact network architectures are discovered, honed,
    and increasingly inch toward perfection through thousands of trials and many errors.
    It continues to be a research-grade task.
  prefs: []
  type: TYPE_NORMAL
- en: The practitioner's general approach is often to find a well working, existing
    architecture (for example, AlexNet, GoogLeNet, ResNet) and tweak them for use
    with a specific dataset. That is what we did; we started with AlexNet and tweaked
    it. Perhaps, that is not fulfilling, but it works and remains the state of practice
    in 2016.
  prefs: []
  type: TYPE_NORMAL
- en: Training day
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It will be more fulfilling, however, to see our training in action and how we
    will improve upon what we did earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will prepare the training dataset and labels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will run the trainer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is very similar to what we did in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. We instantiated the network, passed in an initial set
    of weights and biases, and defined a `loss` function using the training labels.
    Our optimizer is then defined to minimize that `loss`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then use the `weights` and `biases` to predict labels for the validation
    and, eventually, the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for training session is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/511d8afb-d417-4be7-b0da-a6d71bf75963.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we will run the session. We will use the `num_steps` variable that
    we set earlier and run through the training data in chunks (`batch_size`.) We
    will load small chunks of the training data and associated labels, and run the
    session as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will get back predictions on the minibatch, which we will compare against
    the actual labels to get accuracy on the minibatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following `valid_prediction` that we declared earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will evaluate the validation set predictions against the actual labels
    we know, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After we''ve run through all the steps, we will do the same in our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see the actual execution of the training, validation, and test was
    not that different from before. What is different from before is the accuracy.
    Notice that we''ve broken out of the 80s into the 90s on test set accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c25f2c5-ac0d-46c3-a118-6171a8660e77.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual cats and dogs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've demonstrated our new tools on the `notMNIST` dataset, which was helpful
    as it served to provide a comparison to our earlier simpler network setup. Now,
    let's progress to a more difficult problem—actual cats and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll utilize the CIFAR-10 dataset. There will be more than just cats and
    dogs, there are 10 classes—airplanes, automobiles, birds, cats, deer, dogs, frogs,
    horses, ships, and trucks. Unlike the `notMNIST` set, there are two major complexities,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There is far more heterogeneity in the photos, including background scenes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The photos are color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have not worked with color datasets before. Luckily, it is not that different
    from the usual black and white dataset—we will just add another dimension. Recall
    that our previous 28x28 images were flat matrices. Now, we'll have 32x32x3 matrices
    - the extra dimension represents a layer for each red, green, and blue channels.
    This does make visualizing the dataset more difficult, as stacking up images will
    go into a fourth dimension. So, our training/validation/test sets will now be
    32x32x3xSET_SIZE in dimension. We'll just need to get used to having matrices
    that we cannot visualize in our familiar 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanics of the color dimension are the same though. Just as we had floating
    point numbers representing shades of grey earlier, we will now have floating point
    numbers representing shades of red, green, and blue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall how we loaded the `notMNIST` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `num_channels` variable dictated the color channels. It was just one until
    now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll load the CIFAR-10 set similarly, except this time, we''ll have three
    channels returned, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Not reinventing the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Recall how we automated the dataset grab, extraction, and preparation for our
    `notMNIST` dataset in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*? We put those pipeline functions into the `data_utils.py`
    file to separate our pipeline code from our actual machine learning code. Having
    that clean separation and maintaining clean, generic functions allows us to reuse
    those for our current project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will reuse nine of those functions, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`download_hook_function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download_file`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extract_file`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_class`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make_pickles`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`randomize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make_arrays`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merge_datasets`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pickle_whole`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall how we used those functions inside an overarching function, `prepare_not_mnist_dataset`,
    which ran the entire pipeline for us. We just reused that function earlier, saving
    ourselves quite a bit of time.
  prefs: []
  type: TYPE_NORMAL
- en: Let's create an analogous function for the CIFAR-10 set. In general, you should
    save your own pipeline functions, try to generalize them, isolate them into a
    single module, and reuse them across projects. As you do your own projects, this
    will help you focus on the key machine learning efforts rather than spending time
    on rebuilding pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the revised version of `data_utils.py`; we have an overarching function
    called `prepare_cifar_10_dataset` that isolates the dataset details and pipelines
    for this new dataset, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a quick overview of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We will grab the dataset from Alex Krizhevsky's site at the University of Toronto
    using `cifar_dataset_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use `dataset_size = 170498071` to validate whether we've received the
    file successfully, rather than some truncated half download
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also declare some details based on our knowledge of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will segment our set of 60,000 images into training, validation, and test
    sets of `45000`, `5000`, and `10000` images respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are ten classes of images, so we have `num_of_classes = 10`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are color images with red, green, and blue channels, so we have `num_of_channels
    = 3`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will know the images are 32x32 pixels, so we have `image_size = 32` that
    we'll use for both width and height
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will know the images are 8-bit on each channel, so we have `image_depth
    = 255`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data will end up at `/datasets/CIFAR-10/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much like we did with the `notMNIST` dataset, we will download the dataset only
    if we don't already have it. We will unarchive the dataset, do the requisite transformations,
    and save preprocessed matrices as pickles using `pickle_cifar_10`. If we find
    the `pickle` files, we can reload intermediate data using the `load_cifar_10_from_pickles`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three helper methods that we will use to keep the complexity
    of the main method manageable:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pickle_cifar_10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_cifar_10_from_pickles`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_cifar_10_pickle`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The functions are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4169bb8d-f6cc-4ac9-968a-1895971a4c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `load_cifar_10_pickle` method allocates numpy arrays to train and test
    data and labels as well as load existing pickle files into these arrays. As we
    will need to do everything twice, we will isolate the `load_cifar_10_pickle` method,
    which actually loads the data and zero-centers it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f4b7d1d-507d-481a-b64d-87b48fbcea69.png)'
  prefs: []
  type: TYPE_IMG
- en: Much like earlier, we will check to see if the `pickle` files exist already
    and if so, load them. Only if they don't exist (the `else` clause), we actually
    save `pickle` files with the data we've prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the model for ongoing use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To save variables from the tensor flow session for future use, you can use
    the `Saver()` function. Let''s start by creating a `saver` variable right after
    the `writer` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the training loop, we will add the following code to save the model
    after every `model_saving_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, whenever we want to restore the model using the `saved` model,
    we can easily create a new `Saver()` instance and use the `restore` function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we use the `tf.train.latest_checkpoint` so that TensorFlow
    will automatically choose the latest model checkpoint. Then, we create a new `Saver`
    instance named restore. Finally, we can use the `restore` function to load the
    `saved` model to the session graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You should note that we must restore after we run the `tf.global_variables_initializer`.
    Otherwise, the loaded variables will be overridden by the initializer.
  prefs: []
  type: TYPE_NORMAL
- en: Using the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve enhanced the classifier to load random images, we''ll start
    with choosing these random images with the exact size and shape of our training/testing
    images. We''ll need to add placeholders for these user-provided images, so we''ll
    add the following lines in the appropriate locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will grab the image provided by the user via the following command-line
    parameter and run our session on the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5f0d571-ec3a-4f81-846e-5d210979b15d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will follow almost the exact sequence as we did earlier. Running a `test`
    file through the script using the `-e` switch will yield an extra output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Voila! We just classified an arbitrary image.
  prefs: []
  type: TYPE_NORMAL
- en: Skills learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have learned these skills in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing more advanced color training and test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a convolutional neural network graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters and configurations associated with CNN's
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a full system including hooks for TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piping in real-world data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Excellent! We just built a much more advanced classifier, swapped in and out
    models, and even started applying our classifier to arbitrary models. True to
    our chapter's name, we've also trained our system to differentiate cats and dogs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start working with sequence-to-sequence models
    and write an English to French translator with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
