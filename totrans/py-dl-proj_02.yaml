- en: Training NN for Prediction Using Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to our first proper project in Python deep learning! What we'll be doing
    today is building a classifier to solve the problem of identifying specific handwriting
    samples from a dataset of images. We've been asked (in this hypothetical use case)
    to do this by a restaurant chain that has the need to accurately classify handwritten
    numbers into digits. What they have their customers do is write their phone numbers
    in a simple iPad application. At the time when they can be seated, the guest will
    get a text prompting them to come and see the restaurant's host. We need to accurately
    classify the handwritten numbers, so that the output from the app will be accurately
    predicted labels for the digits of a phone number. This can then be sent to their
    (hypothetical) auto dialer service for text messages, and the notice gets to the
    right hungry customer!
  prefs: []
  type: TYPE_NORMAL
- en: '**Define success**: A good practice is to define the criteria for success at
    the beginning of a project. What metric should we use for this project? Let''s
    use a global accuracy test as a percentage to measure our performance in this
    project.'
  prefs: []
  type: TYPE_NORMAL
- en: The data science approach to the problem of classification can be configured
    in a number of ways. In fact, later in this book, we'll look at how to increase
    accuracy in image classification with convolutional neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning**: This means pretraining a deep learning model on a different
    (but quite similar) dataset to speed up the rate of learning and accuracy on another
    (often smaller) dataset. In this project and our hypothetical use case, the pretraining
    of our deep learning **multi-layer perceptron** (**MLP**) on the MNIST dataset
    would enable the deployment of a production system of handwriting classification,
    without having a huge period of time where we were collecting data samples in
    a live but non-functional system. Python deep learning projects are cool!'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the baseline deep neural network model architecture. We will
    get our intuition and skills firmly established, and this will prepare us for
    learning more complex architectures to solve a wider variety of problems as we
    go progress through the projects in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we''ll learn in this chapter includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an MLP?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring a common open source handwriting dataset—the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building our intuition and preparations for model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the model and defining hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the training loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a regression model for prediction using an MLP deep neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In any real job working in an AI team, one of the primary goals will be to
    build regression models that can make predictions in non-linear datasets. Because
    of the complexity of the real world and the data that you''ll be working with,
    simple linear regression models won''t provide the predictive power you''re seeking.
    That is why, in this chapter, we will discuss how to build world-class prediction
    models using MLP. More information can be found at [http://www.deeplearningbook.org/contents/mlp.html](http://www.deeplearningbook.org/contents/mlp.html),
    and an example of the MLP architecture is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c57522c1-d374-41bd-a8f7-edcd12debda7.png)'
  prefs: []
  type: TYPE_IMG
- en: An MLP with two hidden layers
  prefs: []
  type: TYPE_NORMAL
- en: We will implement a neural network with a simple architecture of only two layers,
    using TensorFlow, that will perform regression on the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)) that
    we will provide. We can (and will) go deeper in architecture in later projects! We
    assume that you are already familiar with backpropagation (if not, please read
    article on backpropagation by Michal Nielsen at [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)).
    We'll not spend much time on how TensorFlow works, but you can refer to the official
    tutorial, available at [https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html), if
    you are interested in looking under the hood of that technology.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into building our awesome neural network, let's first have a
    look at the famous MNIST dataset. So let's visualize the MNIST dataset in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Words of wisdom**: You must know your data and how it has been preprocessed,
    in order to know why the models you build perform the way they do. This section
    reviews the significant work that has been done in preparation on the dataset,
    to make our current job of building the MLP easier. Always remember: data science
    begins with DATA!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start therefore by downloading the data, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we examine the `mnist` variable content, we can see that it is structured
    in a specific format, with three major components—**TRAIN**, **TEST**, and **VALIDATION**.
    Each set has handwritten images and their respective labels. The images are stored
    in a flattened way as a single vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8aa4383e-28f6-4c9d-b7bd-064a4d75da37.png)'
  prefs: []
  type: TYPE_IMG
- en: The format of the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extract one image from the dataset and plot it. Since the stored shape
    of a single image matrix is `[1,784]`, we need to reshape these vectors into `[28,28]`
    to visualize the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the image matrix, we will use `matplotlib` to plot it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e396298d-51ec-46f8-969d-f86bf1a17a99.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample of the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: In the same vein as this image, there are a total of 55,000 similar images of
    handwritten digits [0-9]. The labels in the MNIST dataset are the true value of
    the digits present in the image. Our objective, then, is to train a model with
    this set of images and labels, so that it can predict the labels of any image provided from
    the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Be a deep learning explorer**: If you are interested in playing around with
    the dataset, you can try the Colab Notebook, available at [https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing](https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's build our intuition around this project. What we need to do is build a
    deep learning technology that accurately assigns class labels to an input image.
    We're using a deep neural network, known as an MLP, to do this. The core of this
    technology is the mathematics of regression. The specific calculus proofs are
    outside the scope of this book, but in this section, we provide a foundational
    basis for your understanding. We also outline the structure of the project, so
    that it's easy to understand the primary steps needed to create our desired results.
  prefs: []
  type: TYPE_NORMAL
- en: Defining regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first task is to define the model that will perform regression on the provided
    MNIST dataset. So, we will create a TensorFlow model with two hidden layers as
    part of a fully connected neural network. You may also hear it referred to as
    MLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model will perform the operation that will fit the following equation,
    where *y* is the label, *x* is the image, *W* is the weight that the model will
    learn, and *b* is the bias, which will also be learned by the model, following
    is the regression equation for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a9fcc2d-2c3f-4db7-8253-0e83d069b765.png)'
  prefs: []
  type: TYPE_IMG
- en: The regression equation for the model
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: When you have data and accurate labels for the training
    set (that is, you know the answer), you are in a supervised deep learning paradigm.
    Model training is a mathematical process by which the features of the data are
    learned and associated with the proper labels, so that when a new data point (test
    data) is presented, the accurate output class label can be produced. In other
    words, when you present a new data point and do not have the label (that is, you
    don''t know the answer), your model can produce it for you with a highly reliable
    class prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Each iteration will try to generalize the values of weight and bias and reduce
    the error rate. Also, keep in mind that we need to ensure that the model is not
    overfitting, which may lead to wrong predictions for the unseen dataset. We'll
    show you how to code this and visualize the progress to aid in your intuition
    of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the project structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s structure our project as shown in the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hy_param.py`: All the hyperparameters and other configurations are defined
    here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.py`: The definition and architecture of the model are defined here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.py`: The code to train the model is written here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inference.py`: The code to execute the trained model and make predictions
    is defined here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/runs`: This folder will store all of the checkpoints that get created during
    the training process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can clone the code from the repository—the code for this can be found in
    the `Chapter02` folder, available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/).
  prefs: []
  type: TYPE_NORMAL
- en: Let's code the implementation!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To code the implementation, we'll start by defining the hyperparameters, then
    we will define the model, followed by building and executing the training loop.
    We conclude by checking to see if our model is overfitting and build an inference
    code that loads the latest checkpoints and then makes predictions on the basis
    of learned parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will define all of the required hyperparameters in the `hy_param.py` file
    and then import it as a module in our other codes. This makes it easy in deployment,
    and is good practice to make your code as modular as possible. Let''s look into
    the hyperparameter configurations that we have in our `hy_param.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will be using these values throughout our code, and they're totally configurable.
  prefs: []
  type: TYPE_NORMAL
- en: As a Python deep learning projects exploration opportunity, we invite you, our
    project teammate and reader, to try different values of learning rate and numbers
    of hidden layers to experiment and build better models!
  prefs: []
  type: TYPE_NORMAL
- en: Since the flat vectors of images shown previously are of a size of [1 x 786],
    the `num_input=784` is fixed in this case. In addition, the class count in the
    MNIST dataset is `10`. We have digits from 0-9, so obviously we have `num_classes=10`.
  prefs: []
  type: TYPE_NORMAL
- en: Model definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will load the Python modules; in this case, the TensorFlow package
    and the hyperparameters that we defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we define the placeholders that we will be using to input data into the
    model. `tf.placeholder` allows us to feed input data to the computational graph.
    We can define constraints with the shape of the placeholder to only accept a tensor
    of a certain shape. Note that it is common to provide `None` for the first dimension,
    which allows us to the size of the batch at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '**Master your craft**: Batch size can often have a big impact on the performance
    of deep learning models. Explore different batch sizes in this project. What changes
    as a result? What''s your intuition? Batch size is another tool in your data science
    toolkit!'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also assigned names to the placeholders, so that we can use them later
    on while building our inference code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will define variables that will hold values for weights and bias. `tf.Variable` allows
    us to store and update tensors in our graph. To initialize our variables with
    random values from a normal distribution, we will use `tf.random_normal()` (more
    details can be found at [https://www.tensorflow.org/api_docs/python/tf/random_normal](https://www.tensorflow.org/api_docs/python/tf/random_normal)).
    The important thing to notice here is the mapping variable size between layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s set up the operation that we defined in the equation earlier in
    this chapter. This is the logistic regression operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The logistic values are converted into the probabilistic values using `tf.nn.softmax()`. The
    softmax activation squashes the output values of each unit to a value between
    zero and one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s use `tf.nn.softmax_cross_entropy_with_logits` to define our cost
    function. We will optimize our performance using the Adam Optimizer. Finally,
    we can use the built-in `minimize()` function to calculate the **stochastic gradient
    descent** (**SGD**) update rule for each parameter in our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make our prediction. These functions are needed to calculate and capture
    the accuracy values in a batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Hurray! The heavy lifting part of the code is done. We save the model code
    in the `model.py` file. So, up until now, we''ve defined the simple two-hidden-layer
    model architecture, with 300 neurons in each layer, which will try to learn the
    best weight distribution using the Adam Optimizer and predict the probability
    of ten classes. These layers are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0616a53-71bd-456e-8dd2-c56116074037.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of the model that we created
  prefs: []
  type: TYPE_NORMAL
- en: Building the training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step is to utilize the model for training, and record the learned model
    parameters, which we will accomplish in `train.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the variables that we require to be fed into our MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the folder to which we will save our checkpoints. Checkpoints
    are basically the intermediate steps that capture the values of `W` and `b` in
    the process of learning. Then, we will use the `tf.train.Saver()` function (more
    details on this function can be found at [https://www.tensorflow.org/api_docs/python/tf/train/Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver))
    to save and restore `checkpoints`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to begin training, we need to create a new session in TensorFlow.
    In this session, we''ll initialize the graph variables and feed the model operations
    the valid data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will extract batches of 128 training image-label pairs from the MNIST dataset
    and feed them into the model. After subsequent steps or epochs, we will store
    the checkpoints using the `saver` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have executed the `train.py` file, you will see the progress on your
    console, as shown in the preceding screenshot. This depicts the loss being reduced
    after every step, along with accuracy increasing over each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d036ba50-4562-4694-95c1-e5c7dd2152a6.png)'
  prefs: []
  type: TYPE_IMG
- en: The training epoch's output with minibatch loss and training accuracy parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you can see in the plot of minibatch loss, shown in the following diagram, that
    it approaches toward the minima with each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f77e584b-d4d2-4679-8406-d44b89911846.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting the loss values computed at each step
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to visualize how your model is performing, so that you
    can analyze and prevent it from underfitting or overfitting. Overfitting is a
    very common scenario when you are dealing with the deeper models. Let's spend
    some time getting to understand them in detail and learning a few tricks to overcome
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With great power comes great responsibility and with deeper models come deeper
    problems. A fundamental challenge with deep learning is striking the right balance
    between generalization and optimization. In the deep learning process, we are
    tuning hyperparameters and often continuously configuring and tweaking the model
    to produce the best results, based on the data we have for training. This is **optimization**.
    The key question is, how well does our model generalize in performing predictions
    on unseen data?
  prefs: []
  type: TYPE_NORMAL
- en: 'As professional deep learning engineers, our goal is to build models with good
    real-world generalization. However, generalization is subjective to the model
    architecture and the training dataset. We work to guide our model for maximum
    utility by reducing the likelihood that it learns irrelevant patterns or simple
    similar patterns found in the data used for training. If this is not done, it
    can affect the generalization process. A good solution is to provide the model
    with more information that is likely to have a better (that is, more complete
    and often complex) signal of what you''re trying to actually model, by getting more
    data to train on and to work to optimize the model architecture. Here are few
    quick tricks that can improve your model by preventing overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting more data for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing network capacity by altering the number of layers or nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing L2 (and trying L1) weight regularization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding dropout layers or polling layers in the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization, where the cost added is proportional to the absolute value
    of the weights coefficients, is also known as *L1 norm*. L2 regularization, where
    the cost added is proportional to the square of the value of the weight's coefficients,
    is also known as *L2 norm* or *weight decay.*
  prefs: []
  type: TYPE_NORMAL
- en: 'When the model gets trained completely, its output, as checkpoints, will get
    dumped into the `/runs` folder, which will contain the binary dump of `checkpoints`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a78e26a-0e10-4b5a-ae44-43f3ca99e16f.png)'
  prefs: []
  type: TYPE_IMG
- en: The checkpoint folder after the training process is completed
  prefs: []
  type: TYPE_NORMAL
- en: Building inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will create an inference code that loads the latest checkpoints and
    then makes predictions on the basis of learned parameters. For that, we need to
    create a `saver` operation that will pick the latest checkpoints and load the metadata.
    Metadata contains the information regarding the variables and the nodes that we
    created in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We know the importance of this, because we want to load similar variables and
    operations back from the stored checkpoint. We load them into memory using `tf.get_default_graph().get_operation_by_name()`*, *by
    passing the operation name in the parameter that we defined in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to initialize the session and pass data for a test image to the
    operation that makes the prediction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And with that, we are done with our first project that predicts the digits
    provided in a handwritten image! Here are some of the results that the model predicted
    when provided with the test image from the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c276e6f7-1b28-4bab-9be0-94d4680b1928.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the model, depicting the prediction of the model and the input
    image
  prefs: []
  type: TYPE_NORMAL
- en: Concluding the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today's project was to build a classifier to solve the problem of identifying
    specific handwriting samples from a dataset of images. Our hypothetical use case
    was to apply deep learning to enable customers of a restaurant chain to write
    their phone numbers in a simple iPad application, so that they could get a text
    notification that their party was ready to be seated. Our specific task was to
    build the intelligence that would drive this application.
  prefs: []
  type: TYPE_NORMAL
- en: '**Revisit our success criteria**: How did we do? Did we succeed? What was the
    impact of our success? Just as we defined success at the beginning of the project,
    these are the key questions that we need to ask as deep learning data scientists,
    as we look to wrap up a project.'
  prefs: []
  type: TYPE_NORMAL
- en: Our MLP model accuracy hit 87.42%! Not bad, given the depth of the model and
    the hyperparameters that we chose at the beginning. See if you can tweak the model
    to get an even higher test set accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: What are the implications of this accuracy? Let's calculate the incidence of
    an error occurring that would result in a customer service issue (that is, the
    customer not getting the text that their table is ready, and getting upset due
    to an excessively long wait time at the restaurant).
  prefs: []
  type: TYPE_NORMAL
- en: Each customer's phone number is ten digits long. Let's say that our hypothetical
    restaurant has an average of 30 tables at each location, and those tables turn
    over two times per night during the rush hour, when the system is likely to be
    used, and finally, the restaurant chain has 35 locations. This means that each
    day of operation, there are approximately 21,000 handwritten numbers captured
    (30 tables x 2 turns/day x 35 locations x 10 digit phone number).
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, all digits must be correctly classified for the text to get to the
    waiting restaurant patron. So, any single digit misclassification causes a failure.
    A model accuracy of 87.42% would improperly classify 2,642 digits per day in our
    example. The worst case for the hypothetical scenario would be if there occurred
    only one improperly classified digit in each phone number. Since there are only
    2,100 patrons and corresponding phone numbers, this would mean that every phone
    number had an error in classification (a 100% failure rate), and not a single
    customer would get their text notification that their party could be seated! The
    best case, in this scenario, would be if all 10 digits were misclassified in each
    phone number, which would result in 263 wrong phone numbers out of 2,100 (a 12.5%
    failure rate). This is still not a level of performance that the restaurant chain
    would be likely be happy with.
  prefs: []
  type: TYPE_NORMAL
- en: '**Words of wisdom**: Model performance may not equal system or app performance.
    Many factors contribute to a system being robust or fragile in the real world.
    Model performance is a key factor, but other items with individual fault tolerances
    definitely play a part. Know how your deep learning models integrate into the
    larger project so that you can set proper expectations!'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the project in this chapter, we successfully built an MLP to produce a regression
    classification prediction, based on handwritten digits. We gained experience with
    the MNIST dataset and a deep neural network model architecture, which gave us
    the added opportunity to define some key hyperparameters. Finally, we looked at
    the model performance in testing and determined whether we succeeded in achieving
    our goals.
  prefs: []
  type: TYPE_NORMAL
