- en: Deep Feedforward Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, you learned about the mathematics which drives the logic
    behind all kinds of neural networks. In this chapter, we are going to focus on
    the most fundamental neutral networks, which are called **feedforward neural networks**.
    We will also look at deep feedforward networks with multiple hidden layers to
    improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing feedforward networks in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the Iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating feedforward networks for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining feedforward networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep feedforward networks, also called feedforward neural networks, are sometimes
    also referred to as **Multilayer Perceptrons** (**MLPs**). The goal of a feedforward
    network is to approximate the function of *f∗*. For example, for a classifier,
    *y=f∗(x)* maps an input *x* to a label *y.* A feedforward network defines a mapping
    from input to label *y=f(x;θ)*. It learns the value of the parameter *θ* that
    results in the best function approximation.
  prefs: []
  type: TYPE_NORMAL
- en: We discuss RNNs in [Chapter 5](5f7c0191-1c53-462e-aa51-634572f20214.xhtml), *Recurrent
    Neural Networks*. Feedforward networks are a conceptual stepping stone on the
    path to recurrent networks, which power many natural language applications. Feedforward
    neural networks are called networks because they compose together many different
    functions which represent them. These functions are composed in a directed acyclic
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: The model is associated with a directed acyclic graph describing how the functions
    are composed together. For example, there are three functions *f(1)*, *f(2)*,
    and *f(3)* connected to form *f(x) =f(3)(f(2)(f(1)(x)))*. These chain structures
    are the most commonly used structures of neural networks. In this case, *f(1)*
    i*s* called the **first layer** of the network, *f(2)* is called the **second
    layer**, and so on. The overall length of the chain gives the depth of the model.
    It is from this terminology that the name deep learning arises. The final layer
    of a feedforward network is called the **output layer**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7e58a6b-8f57-4648-9ee4-024e28d62900.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram showing various functions activated on input x to form a neural network
  prefs: []
  type: TYPE_NORMAL
- en: These networks are called neural because they are inspired by neuroscience.
    Each hidden layer is a vector. The dimensionality of these hidden layers determines
    the width of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a feedforward neural network is used to accept an input *x* and produce
    an output *yˆ*, information flows forward through the network elements. The input
    *x* provides the information that then propagates up to the hidden units at each
    layer and produces *yˆ*. This is called **forward propagation**. During training,
    forward propagation continues onward until it produces a scalar cost *J(θ)*. The
    backpropagation algorithm, often called backprop, allows the information from
    the cost to then flow backward through the network in order to compute the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Computing an analytical expression for the gradient is straightforward, but
    numerically evaluating such an expression can be computationally expensive. The
    backpropagation algorithm does so using a simple and inexpensive procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation refers only to the method to compute the gradient, while another
    algorithm, such as stochastic gradient descent, refers to the actual mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing feedforward networks with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feedforward networks can be easily implemented using TensorFlow by defining
    placeholders for hidden layers, computing the activation values, and using them
    to calculate predictions. Let''s take an example of classification with a feedforward
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the predicted value tensor has been defined, we calculate the `cost` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `OPERATION_NAME` could be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.sigmoid_cross_entropy_with_logits`: Calculates sigmoid cross entropy
    on incoming `logits` and `labels`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`_sentinel`: Used to prevent positional parameters. Internal, do not use.'
  prefs: []
  type: TYPE_NORMAL
- en: '`labels`: A tensor of the same type and shape as logits.'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits`: A tensor of type `float32` or `float64`. The formula implemented
    is ( *x = logits*, *z = labels*) `max(x, 0) - x * z + log(1 + exp(-abs(x)))`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.softmax`: Performs `softmax` activation on the incoming tensor. This
    only normalizes to make sure all the probabilities in a tensor row add up to one.
    It cannot be directly used in a classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`logits`: A non-empty tensor. Must be one of the following types--half, `float32`,
    or `float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim`: The dimension `softmax` will be performed on. The default is `-1`, which
    indicates the last dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.log_softmax`: Calculates the log of the `softmax` function and helps
    in normalizing underfitting. This function is also just a normalization function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`logits`: A non-empty tensor. Must be one of the following types--half, `float32`,
    or `float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim`: The dimension `softmax` will be performed on. The default is `-1`, which
    indicates the last dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.softmax_cross_entropy_with_logits`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`_sentinel`: Used to prevent positional parameters. For internal use only.'
  prefs: []
  type: TYPE_NORMAL
- en: '`labels`: Each rows `labels[i]` must be a valid probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits`: Unscaled log probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim`: The class dimension. Defaulted to `-1`, which is the last dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.nn.sparse_softmax_cross_entropy_with_logits`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`labels`: Tensor of shape [*d_0*, *d_1*, *...*, *d_(r-1)*] (where *r* is the
    rank of labels and result) and `dtype`, `int32`, or `int64`. Each entry in labels
    must be an index in [*0*, `num_classes`). Other values will raise an exception
    when this operation is run on the CPU and return NaN for corresponding loss and
    gradient rows on the GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits`: Unscaled log probabilities of shape [*d_0*, *d_1*, *...*, *d_(r-1)*,
    `num_classes`] and `dtype`, `float32`, or `float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code computes sparse `softmax` cross entropy between `logits`
    and `labels`. The probability of a given label is considered exclusive. Soft classes
    are not allowed, and the label's vector must provide a single specific index for
    the true class for each row of `logits`.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.weighted_cross_entropy_with_logits`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`targets`: A tensor of the same type and shape as logits.'
  prefs: []
  type: TYPE_NORMAL
- en: '`logits`: A tensor of type `float32` or `float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`pos_weight`: A coefficient to use on the positive examples.'
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to `sigmoid_cross_entropy_with_logits()` except that `pos_weight`
    allows a trade-off of recall and precision by up or down weighting the cost of
    a positive error relative to a negative error.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at a feedforward example using the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the dataset from [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv)
    and the target labels from [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Iris dataset, we will use 150 rows of data made up of 50 samples from
    each of three Iris species: Iris setosa, Iris virginica, and Iris versicolor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Petal geometry compared from three iris species:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iris Setosa**, **Iris Virginica**, and **Iris Versicolor**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca899bdb-1bb7-4491-b8b4-79db174760e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the dataset, each row contains data for each flower sample: sepal length,
    sepal width, petal length, petal width, and flower species. Flower species are
    stored as integers, with 0 denoting Iris setosa, 1 denoting Iris versicolor, and
    2 denoting Iris virginica.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a `run()` function that takes three parameters--hidden
    layer size `h_size`, standard deviation for weights `stddev`, and Step size of
    Stochastic Gradient Descent `sgd_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Input data loading is done using the `genfromtxt` function in `numpy`. The
    Iris data loaded has a shape of L: 150 and W: 4\. Data is loaded in the `all_X`
    variable. Target labels are loaded from `target.csv` in `all_Y` with the shape
    of L: 150, W:3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once data is loaded, we initialize the weights matrix based on `x_size`, `y_size`,
    and `h_size` with standard deviation passed to the `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_size`= 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_size`= 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h_size`= 128 (or any other number chosen for neurons in the hidden layer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make the prediction using `sigmoid` as the activation function defined
    in the `forward_propagration()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, `sigmoid` output is calculated from input `X` and `weights_1`. This
    is then used to calculate `y` as a matrix multiplication of `sigmoid` and `weights_2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define the cost function and optimization using gradient descent. Let's
    look at the `GradientDescentOptimizer` being used. It is defined in the `tf.train.GradientDescentOptimizer`
    class and implements the gradient descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct an instance, we use the following constructor and pass `sgd_step`
    as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Arguments passed are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: A tensor or a floating point value. The learning rate to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_locking`: If True, use locks for update operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: Optional name prefix for the operations created when applying gradients.
    The default name is `"GradientDescent"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following list shows the code to implement the `cost` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will implement the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Initialize all the variables using `tf.initialize_all_variables()`; the return
    object is used to instantiate the session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over `steps` (1 to 50).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each step in `train_x` and `train_y`, execute `updates_sgd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the `train_accuracy` and `test_accuracy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We stored the accuracy for each step in a list so that we could plot a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Code execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s run this code for `h_size` of `128`, standard deviation of `0.1`, and
    `sgd_step` of `0.01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the following graph, which plots the steps versus
    the test and train accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ae79ae5-fae3-4cf2-88ff-a77d94bf6a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's compare the change in SGD steps and its effect on training accuracy. The
    following code is very similar to the previous code example, but we will rerun
    it for multiple SGD steps to see how SGD steps affect accuracy levels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of the preceding code will be an array with training and test accuracy
    for each SGD step value. In our example, we called the function `sgd_steps` for
    an SGD step value of `[0.01, 0.02, 0.03]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This is the plot showing how training accuracy changes with `sgd_steps`. For
    an SGD value of `0.03`, it reaches a higher accuracy faster as the step size is
    larger.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3106868-376b-44e2-bbe3-c544ef83747d.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing feedforward networks with images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will look at how to use feedforward networks to classify images. We will
    be using `notMNIST` data. The dataset consists of images for nine letters, A to
    I.
  prefs: []
  type: TYPE_NORMAL
- en: NotMNIST dataset is similar to MNIST dataset but focuses on Alphabets instead
    of numbers ([http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html](http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html))
  prefs: []
  type: TYPE_NORMAL
- en: We have reduced the original dataset to a smaller version for the training so
    that you can easily get started. Download the ZIP files and extract them to the
    folder where the dataset is contained, [https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs](https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs).
  prefs: []
  type: TYPE_NORMAL
- en: The pickle module of python implements an algorithm for serializing and de-serializing
    a Python object structure. **Pickling** is the process in which a Python object
    hierarchy is converted into a byte stream, unpickling is the inverse operation,
    where a byte stream is converted back into an object hierarchy. Pickling (and
    unpickling) is alternatively known as **serialization**, **marshaling**, [1] or
    **flattening**.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the images in `numpy.ndarray` from the following list of folders
    using the `maybe_pickle(..)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `maybe_pickle` uses the `load_letter` method to load the image to `ndarray`
    from a single folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `maybe_pickle` method is called for two sets of folders, `train_folders`
    and `test_folders`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Output is similar to the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first screenshot shows the `dataset_names` list variable value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe21686e-bc89-45c9-b244-e2bfd65e636f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the value of the `dataset_names` variable for
    the `notMNIST_small` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d87bb91f-bc8b-4f5c-865c-ce0aa3ab5400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, `merge_datasets` is called, where pickle files from each character are
    combined into the following `ndarray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`valid_dataset`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`valid_labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dataset`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of the preceding code is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `noMNIST.pickle` file is created by storing each of these `ndarray`
    in key-value pairs where the keys are `train_dataset`, `train_labels`, `valid_dataset`,
    `valid_labels`, `test_dataset`, and `test_labels`, and values are the respective
    `ndarray`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the full code for generating the `notMNIST.pickle` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at how the pickle file created earlier loads data and runs a network
    with one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the training, testing, and validation datasets (`ndarray`)
    from the `notMNIST.pickle` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see an output similar to the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we `reformat` the `dataset` into a two-dimensional array so that data
    is easier to process with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define the graph that will return the content to which all the variables
    will be loaded.
  prefs: []
  type: TYPE_NORMAL
- en: The size of each weight and bias is listed here, where `image_size` = 28 and
    `no_of_neurons` = 1024.
  prefs: []
  type: TYPE_NORMAL
- en: Number of neurons in the hidden layer should be optimal. Too few neurons leads
    to lower accuracy, while too high a number leads to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Layer in the neural network** | **Weight** | **Bias** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | row = 28 x 28 = 784columns = 1024 | 1024 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | row = 1024columns = 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: We will initialize the TensorFlow graph and initialize placeholder variables
    from training, validation, and test the datasets and labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also define weights and biases for two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the hidden layer tensor and the calculated logit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Loss function for our network is going to be based on the `softmax` function
    applied over cross entropy with `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the `logits` (predicted values); note that we are using
    `softmax` to normalize the `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the test and validation predictions. Notice the activation function
    `RELU` being used here to calculate `w1` and `b1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create a TensorFlow session and pass the datasets loaded through
    the neural network created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The full code can be found at [https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code can be found at the preceding GitHub link. Notice that we
    are appending the validation and minibatch accuracy to an array that we will plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the plot generated by the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80586153-ad97-4844-b423-d2a50c9046e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Minibatch accuracy reaches 100 by iteration number 8- while validation accuracy
    stops at 60.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the effect of activation functions on the feedforward networks accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we used `RELU` as the activation function. TensorFlow
    supports multiple activation functions. Let''s look at how each of these activation
    functions affects validation accuracy. We will generate some random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then generate the activation output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the activation against `x_val`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Plots are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/331e4f58-6df4-42eb-9a67-6618c6f7a212.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot comparing **Activation functions with Vanishing Gradient** is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ed73c91-5b7d-42ac-b254-e7b90f3765c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's look at the activation function and how it affects validation accuracy
    for NotMNIST data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have modified the previous example so that we can pass the activation function
    as a parameter in `main()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run()` function definition encompasses the login that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Plots from the preceding list are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64631b8e-2376-4387-bb86-1321549ddfc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Validation accuracy for various activation functions
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen in the preceding graphs, RELU and RELU6 provide maximum validation
    accuracy, which is close to 60 percent. Now let''s look at how training loss behaves
    as we progress through the steps for various activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/214cb862-ba33-41ed-adab-3925dd4ac740.png)'
  prefs: []
  type: TYPE_IMG
- en: Training loss for various activations as a function of steps
  prefs: []
  type: TYPE_NORMAL
- en: Training loss converges to zero for most of the activation functions, though
    RELU is the least effective in the short-term.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built our first neural network, which was feedforward only,
    and used it for classification with the Iris dataset and later the NotMNIST dataset.
    You learned how various activation functions like affect the validation accuracy
    of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore a convoluted neural network, which is more
    advanced and effective for an image dataset.
  prefs: []
  type: TYPE_NORMAL
