- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it to the end of the book! Let us take a moment and
    see how far we have come since we started.
  prefs: []
  type: TYPE_NORMAL
- en: If you are like most readers, you started with some knowledge of Python and
    some background in machine learning, but you were interested in learning more
    about deep learning and wanted to be able to apply these deep learning skills
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to install Keras on your machine and started using it to build
    simple deep learning models. You then learned about the original deep learning
    model, the multi-layer perceptron, also called the **fully connected network**
    (**FCN**). You learned how to build this network using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about the many tunable parameters that you need to tweak to
    get good results from your network. With Keras, a lot of the hard work has been
    done for you since it comes with sensible defaults, but there are occasions where
    this knowledge will be helpful to you.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing on from there, you were introduced to **convolutional neural network**
    (**CNN**), originally built to exploit feature locality of images, although you
    can also use them for other types of data such as text, audio or video. Once again,
    you saw how to build a CNN using Keras. You also saw the functionality that Keras
    provides to build CNNs easily and intuitively. You saw how to use pre-trained
    image networks to make predictions about your own images, via the process of transfer
    learning and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: From there, you learned about **generative adversarial network** (**GAN**),
    which are a pair of networks (usually CNN) that attempt to work against each other
    and, in the process, make each other stronger. GANs are a cutting-edge technology
    in the deep learning space; a lot of recent work is going on around GANs.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we turned our attention to text and we learned about **word embeddings**,
    which have become the most common technology used for the vector representation
    of text in the last couple of years. We looked at various popular word embedding
    algorithms and saw how to use pre-trained word embeddings to represent collections
    of words, as well as support for word embeddings in Keras and gensim.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at **recurrent neural network** (**RNN**), a class of neural
    network optimized for handing sequence data such as text or time series. We learned
    about the shortcomings of the basic RNN model and how these are alleviated in
    the more powerful variants such as the **long short term model** (**LSTM**) and
    **gated recurrent unit** (**GRU**). We looked at a few examples where these components
    are used. We also looked briefly at Stateful RNN models and where they might be
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we looked at a few additional models that don't quite fit the molds
    of the models we have spoken so far. Among them are **autoencoders**, a model
    for unsupervised learning—**regression networks** that predict a continuous value
    rather than a discrete label. We introduced the **Keras functional API**, which
    allows us to build complex networks with multiple inputs and outputs and share
    components among multiple pipelines. We looked at ways to customize Keras to add
    functionality that doesn't currently exist.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at training deep learning networks using **reinforcement
    learning** in the context of playing arcade games, which many consider a first
    step toward a general artificial intelligence. We provided a Keras example of
    training a simple game. We then briefly described advances in this field in the
    context of networks playing even harder games such as Go and Poker at a superhuman
    level.
  prefs: []
  type: TYPE_NORMAL
- en: We believe you are now equipped with the skills to solve new machine learning
    problems using deep learning and Keras. This is an important and valuable skill
    in your journey to becoming a deep learning expert.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to thank you for letting us help you on your journey to deep learning
    mastery.
  prefs: []
  type: TYPE_NORMAL
- en: Keras 2.0 — what is new
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to Francois Chollet, Keras was released two years ago, in March, 2015\.
    It then proceeded to grow from one user to one hundred thousand. The following
    image, taken from the Keras blog, shows the growth of number of Keras users over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: <q>![](img/keras_users_2015_2016.png)</q>
  prefs: []
  type: TYPE_NORMAL
- en: One important update with Keras 2.0 is that the API will now be a part of TensorFlow,
    starting with TensorFlow 1.2\. Indeed, Keras is becoming more and more the *lingua
    franca* for deep learning, a *spec* used in an increasing number of deep learning
    contexts. For instance, Skymind is implementing Keras spec in Scala for ScalNet,
    and Keras.js is doing the same for JavaScript for running of deep learning directly
    in the browser. Efforts are also underway to provide a Keras API for MXNET and
    CNTK deep learning toolkits.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing Keras 2.0 is very simple via the `pip install keras --upgrade` followed
    by `pip install tensorflow --upgrade`.
  prefs: []
  type: TYPE_NORMAL
- en: API changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Keras 2.0 changes implied the need to rethink some APIs. For full details,
    please refer to the release notes ([https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes](https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes)).
    This module `legacy.py` summarizes the most impactful changes and prevents warnings
    when using Keras 1.x calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are also a number of breaking changes. In particular:'
  prefs: []
  type: TYPE_NORMAL
- en: The maxout dense, time distributed dense, and highway legacy layers have been
    removed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch normalization layer no longer supports the mode argument, because
    Keras internals have changed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom layers have to be updated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any undocumented Keras functionality could have broken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the Keras code base has been instrumented to detect the use of
    the Keras 1.x API calls and show deprecation warnings that show how to change
    the call to conform to the Keras 2 API. If you have some volume of Keras 1.x code
    already and are hesitant to try Keras 2 because of the fear of non-breaking changes,
    these deprecation warnings from the Keras 2 code base can be very helpful in making
    the transition.
  prefs: []
  type: TYPE_NORMAL
