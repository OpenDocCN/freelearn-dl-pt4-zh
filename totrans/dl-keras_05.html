<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Word Embeddings</h1>
            </header>

            <article>
                
<p>Wikipedia defines word embedding as the collective name for a set of language modeling and feature learning techniques in <strong>natural language processing</strong> (<strong>NLP</strong>) where words or phrases from the vocabulary are mapped to vectors of real numbers.</p>
<p>Word embeddings are a way to transform words in text to numerical vectors so that they can be analyzed by standard machine learning algorithms that require vectors as numerical input.</p>
<p>You have already learned about one type of word embedding called <strong>one-hot encoding</strong>, in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>. One-hot encoding is the most basic embedding approach. To recap, one-hot encoding represents a word in the text by a vector of the size of the vocabulary, where only the entry corresponding to the word is a one and all the other entries are zero.</p>
<p>A major problem with one-hot encoding is that there is no way to represent the similarity between words. In any given corpus, you would expect words such as (<em>cat</em>, <em>dog</em>), (<em>knife</em>, <em>spoon</em>), and so on to have some similarity. Similarity between vectors is computed using the dot product, which is the sum of element-wise multiplication between vector elements. In the case of one-hot encoded vectors, the dot product between any two words in a corpus is always zero.</p>
<p>To overcome the limitations of one-hot encoding, the NLP community has borrowed techniques from <strong>information retrieval</strong> (<strong>IR</strong>) to vectorize text using the document as the context. Notable techniques are TF-IDF (<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a>), <strong>latent semantic analysis</strong> (<strong>LSA</strong>) (<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a>), and topic modeling (<a href="https://en.wikipedia.org/wiki/Topic_model" target="_blank">https://en.wikipedia.org/wiki/Topic_model</a>). However, these representations capture a slightly different document-centric idea of semantic similarity.</p>
<p>Development of word embedding techniques began in earnest in 2000. Word embedding differs from previous IR-based techniques in that they use words as their context, which leads to a more natural form of semantic similarity from a human understanding perspective. Today, word embedding is the technique of choice for vectorizing text for all kinds of NLP tasks, such as text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis, and so on.</p>
<p>In this chapter, we will learn about two specific forms of word embedding, GloVe and word2vec, collectively known as distributed representations of words. These embeddings have proven more effective and have been widely adopted in the deep learning and NLP communities.</p>
<p>We will also learn different ways in which you can generate your own embeddings in your Keras code, as well as how to use and fine-tune pre-trained word2vec and GloVe models.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Building various distributional representations of words in context</li>
<li>Building models for leveraging embeddings to perform NLP tasks such as sentence parsing and sentiment analysis</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Distributed representations</h1>
            </header>

            <article>
                
<p>Distributed representations attempt to capture the meaning of a word by considering its relations with other words in its context. The idea is captured in this quote from J. R. Firth (for more information refer to the article: <span><em>Document Embedding with Paragraph Vectors</em>, by Andrew M. Dai, Christopher Olah, and Quoc V. Le, arXiv:1507.07998, 2015</span>), a linguist who first proposed this idea:</p>
<div class="packt_quote">You shall know a word by the company it keeps.</div>
<p>Consider the following pair of sentences:</p>
<p><em>Paris is the capital of France.<br/></em><em>Berlin is the capital of Germany.</em></p>
<p>Even assuming you have no knowledge of world geography (or English for that matter), you would still conclude without too much effort that the word pairs (<em>Paris</em>, <em>Berlin</em>) and (<em>France</em>, <em>Germany</em>) were related in some way, and that corresponding words in each pair were related in the same way to each other, that is:</p>
<p><em>Paris : France :: Berlin : Germany</em></p>
<p>Thus, the aim of distributed representations is to find a general transformation function φ to convert each word to its associated vector such that relations of the following form hold true:</p>
<div class="CDPAlignCenter CDPAlign"><img height="15" src="assets/paris-framce-eqn.png" width="415"/></div>
<p>In other words, distributed representation aims to convert words to vectors where the similarity between the vectors correlate with the semantic similarity between the words.</p>
<p>The most well-known word embeddings are word2vec and GloVe, which we cover in more detail in subsequent sections.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">word2vec</h1>
            </header>

            <article>
                
<p>The word2vec group of models was created in 2013 by a team of researchers at Google led by Tomas Mikolov. The models are unsupervised, taking as input a large corpus of text and producing a vector space of words. The dimensionality of the word2vec embedding space is usually lower than the dimensionality of the one-hot embedding space, which is the size of the vocabulary. The embedding space is also more dense compared to the sparse embedding of the one-hot embedding space.</p>
<p>The two architectures for word2vec are as follows:</p>
<ul>
<li><strong>Continuous Bag Of Words</strong> (<strong>CBOW</strong>)</li>
<li><strong>Skip-gram</strong></li>
</ul>
<p>In the CBOW architecture, the model predicts the current word given a window of surrounding words. In addition, the order of the context words does not influence the prediction (that is, the bag of words assumption). In the case of skip-gram architecture, the model predicts the surrounding words given the center word. According to the authors, CBOW is faster but skip-gram does a better job at predicting infrequent words.</p>
<p>An interesting thing to note is that even though word2vec creates embeddings that are used in deep learning NLP models, both flavors of word2vec that we will discuss, which also happens to be the most successful and acknowledged recent models, are shallow neural networks.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The skip-gram word2vec model</h1>
            </header>

            <article>
                
<p>The skip-gram model is trained to predict the surrounding words given the current word. To understand how the skip-gram word2vec model works, consider the following example sentence:</p>
<p><em>I love green eggs and ham.</em></p>
<p>Assuming a window size of three, this sentence can be broken down into the following sets of (context, word) pairs:</p>
<p><em>([I, green], love)</em><br/>
<em>([love, eggs], green)</em><br/>
<em>([green, and], eggs)</em><br/>
<em>...</em></p>
<p>Since the skip-gram model predicts a context word given the center word, we can convert the preceding dataset to one of (input, output) pairs. That is, given an input word, we expect the skip-gram model to predict the output word:</p>
<p><em>(love, I), (love, green), (green, love), (green, eggs), (eggs, green), (eggs, and), ...</em></p>
<p>We can also generate additional negative samples by pairing each input word with some random word in the vocabulary. For example:</p>
<p><em>(love, Sam), (love, zebra), (green, thing), ...</em></p>
<p>Finally, we generate positive and negative examples for our classifier:</p>
<p><em>((love, I), 1), ((love, green), 1), ..., ((love, Sam), 0), ((love, zebra), 0), ...</em></p>
<p>We can now train a classifier that takes in a word vector and a context vector and learns to predict one or zero depending on whether it sees a positive or negative sample. The deliverables from this trained network are the weights of the word embedding layer (the gray box in the following figure):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="287" src="assets/word2vec-skipgram.png" width="290"/></div>
<p>The skip-gram model can be built in Keras as follows. Assume that the vocabulary size is set at <kbd>5000</kbd>, the output embedding size is <kbd>300</kbd>, and the window size is <kbd>1</kbd>. A window size of one means that the context for a word is the words immediately to the left and right. We first take care of the imports and set our variables to their initial values:</p>
<pre>
from keras.layers import Merge<br/>from keras.layers.core import Dense, Reshape<br/>from keras.layers.embeddings import Embedding<br/>from keras.models import Sequential<br/><br/>vocab_size = 5000<br/>embed_size = 300
</pre>
<p>We then create a sequential model for the word. The input to this model is the word ID in the vocabulary. The embedding weights are initially set to small random values. During training, the model will update these weights using backpropagation. The next layer reshapes the input to the embedding size:</p>
<pre>
word_model = Sequential()<br/>word_model.add(Embedding(vocab_size, embed_size,<br/>                         embeddings_initializer="glorot_uniform",<br/>                         input_length=1))<br/>word_model.add(Reshape((embed_size, )))
</pre>
<p>The other model that we need is a sequential model for the context words. For each of our skip-gram pairs, we have a single context word corresponding to the target word, so this model is identical to the word model:</p>
<pre>
context_model = Sequential()<br/>context_model.add(Embedding(vocab_size, embed_size,<br/>                  embeddings_initializer="glorot_uniform",<br/>                  input_length=1))<br/>context_model.add(Reshape((embed_size,)))
</pre>
<p>The outputs of the two models are each a vector of size (<kbd>embed_size</kbd>). These outputs are merged into one using a dot product and fed into a dense layer, which has a single output wrapped in a sigmoid activation layer. You have seen the sigmoid activation function in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Network Foundations</em>. As you will recall, it modulates the output so numbers higher than 0.5 tend rapidly to 1 and flatten out, and numbers lower than 0.5 tend rapidly to 0 and also flatten out:</p>
<pre>
model = Sequential()<br/>model.add(Merge([word_model, context_model], mode="dot"))<br/>model.add(Dense(1, init="glorot_uniform", activation="sigmoid"))<br/>model.compile(loss="mean_squared_error", optimizer="adam")
</pre>
<p>The loss function used is the <kbd>mean_squared_error</kbd>; the idea is to minimize the dot product for positive examples and maximize it for negative examples. If you recall, the dot product multiplies corresponding elements of two vectors and sums up the result—this causes similar vectors to have higher dot products than dissimilar vectors, since the former has more overlapping elements.</p>
<p>Keras provides a convenience function to extract skip-grams for a text that has been converted to a list of word indices. Here is an example of using this function to extract the first 10 of 56 skip-grams generated (both positive and negative).</p>
<p>We first declare the necessary imports and the text to be analyzed:</p>
<pre>
from keras.preprocessing.text import *<br/>from keras.preprocessing.sequence import skipgrams<br/><br/>text = "I love green eggs and ham ."
</pre>
<p>The next step is to declare the <kbd>tokenizer</kbd> and run the text against it. This will produce a list of word tokens:</p>
<pre>
tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts([text])
</pre>
<p>The <kbd>tokenizer</kbd> creates a dictionary mapping each unique word to an integer ID and makes it available in the <kbd>word_index</kbd> attribute. We extract this and create a two-way lookup table:</p>
<pre>
word2id = tokenizer.word_index<br/>id2word = {v:k for k, v in word2id.items()}
</pre>
<p>Finally, we convert our input list of words to a list of IDs and pass it to the <kbd>skipgrams</kbd> function. We then print the first 10 of the 56 (pair, label) skip-gram tuples generated:</p>
<pre>
wids = [word2id[w] for w in text_to_word_sequence(text)]<br/>pairs, labels = skipgrams(wids, len(word2id))<br/><span>print(len(pairs), len(labels))</span><br/>for i in range(10):<br/>    print("({:s} ({:d}), {:s} ({:d})) -&gt; {:d}".format(<br/>          id2word[pairs[i][0]], pairs[i][0], <br/>          id2word[pairs[i][1]], pairs[i][1], <br/>          labels[i]))
</pre>
<p>The results from the code is shown below. Note that your results may be different since the skip-gram method randomly samples the results from the pool of possibilities for the positive examples. Additionally, the process of negative sampling, used for generating the negative examples, consists of randomly pairing up arbitrary tokens from the text. As the size of the input text increases, this is more likely to pick up unrelated word pairs. In our example, since our text is very short, there is a chance that it can end up generating positive examples as well.</p>
<pre>
<strong>(and (1), ham (3)) -&gt; 0<br/>(green (6), i (4)) -&gt; 0<br/>(love (2), i (4)) -&gt; 1<br/>(and (1), love (2)) -&gt; 0<br/>(love (2), eggs (5)) -&gt; 0<br/>(ham (3), ham (3)) -&gt; 0<br/>(green (6), and (1)) -&gt; 1<br/>(eggs (5), love (2)) -&gt; 1<br/>(i (4), ham (3)) -&gt; 0<br/>(and (1), green (6)) -&gt; 1<br/></strong>
</pre>
<p>The code for this example can be found in <kbd>skipgram_example.py</kbd> in the source code download for the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The CBOW word2vec model</h1>
            </header>

            <article>
                
<p>Let us now look at the CBOW word2vec model. Recall that the CBOW model predicts the center word given the context words. Thus, in the first tuple in the following example, the CBOW model needs to predict the output word <em>love</em>, given the context words <em>I</em> and <em>green</em>:</p>
<p><em>([I, green], love) ([love, eggs], green) ([green, and], eggs) ...</em></p>
<p>Like the skip-gram model, the CBOW model is also a classifier that takes the context words as input and predicts the target word. The architecture is somewhat more straightforward than the skip-gram model. The input to the model is the word IDs for the context words. These word IDs are fed into a common embedding layer that is initialized with small random weights. Each word ID is transformed into a vector of size (<kbd>embed_size</kbd>) by the embedding layer. Thus, each row of the input context is transformed into a matrix of size (<kbd>2*window_size</kbd>, <kbd>embed_size</kbd>) by this layer. This is then fed into a lambda layer, which computes an average of all the embeddings. This average is then fed to a dense layer, which creates a dense vector of size (<kbd>vocab_size</kbd>) for each row. The activation function on the dense layer is a softmax, which reports the maximum value on the output vector as a probability. The ID with the maximum probability corresponds to the target word.</p>
<p>The deliverable for the CBOW model is the weights from the embedding layer shown in gray in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/word2vec-cbow.png"/></div>
<p>The corresponding Keras code for the model is shown as follows. Once again, assume a vocabulary size of <kbd>5000</kbd>, an embedding size of <kbd>300</kbd>, and a context window size of <kbd>1</kbd>. Our first step is to set up all our imports and these values:</p>
<pre>
from keras.models import Sequential<br/>from keras.layers.core import Dense, Lambda<br/>from keras.layers.embeddings import Embedding<br/>import keras.backend as K<br/><br/>vocab_size = 5000<br/>embed_size = 300<br/>window_size = 1
</pre>
<p>We then construct a sequential model, to which we add an embedding layer whose weights are initialized with small random values. Note that the <kbd>input_length</kbd> of this embedding layer is equal to the number of context words. So each context word is fed into this layer and will update the weights jointly during backpropagation. The output of this layer is a matrix of context word embeddings, which are averaged into a single vector (per row of input) by the lambda layer. Finally, the dense layer will convert each row into a dense vector of size (<kbd>vocab_size</kbd>). The target word is the one whose ID has the maximum value in the dense output vector:</p>
<pre>
model = Sequential()<br/>model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, <br/>                    embeddings_initializer='glorot_uniform',<br/>                    input_length=window_size*2))<br/>model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=  (embed_size,)))<br/>model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))<br/><br/>model.compile(loss='categorical_crossentropy', optimizer="adam")
</pre>
<p>The loss function used here is <kbd>categorical_crossentropy</kbd>, which is a common choice for cases where there are two or more (in our case, <kbd>vocab_size</kbd>) categories.</p>
<p>The source code for the example can be found in the <kbd>keras_cbow.py</kbd> file in the source code download for the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting word2vec embeddings from the model</h1>
            </header>

            <article>
                
<p>As noted previously, even though both word2vec models can be reduced to a classification problem, we are not really interested in the classification problem itself. Rather, we are interested in the side effect of this classification process, that is, the weight matrix that transforms a word from the vocabulary to its dense, low-dimensional distributed representation.</p>
<p>There are many examples of how these distributed representations exhibit often surprising syntactic and semantic information. For example, as shown in the following figure from Tomas Mikolov's presentation at NIPS 2013 (for more information refer to the article: <span><em>Learning Representations of Text using Neural Networks</em>, by T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Q. Le, and T. Strohmann, NIPS 2013</span>), vectors connecting words that have similar meanings but opposite genders are approximately parallel in the reduced 2D space, and we can often get very intuitive results by doing arithmetic with the word vectors. The presentation provides many other examples.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/word2vec_regularities.png"/></div>
<p>Intuitively, the training process imparts enough information to the internal encoding to predict an output word that occurs in the context of an input word. So points representing words shift in this space to be nearer to words with which it co-occurs. This causes similar words to clump together. Words that co-occur with these similar words also clump together in a similar way. As a result, vectors connecting points representing semantically related points tend to exhibit these regularities in the distributed representation.</p>
<p>Keras provides a way to extract weights from trained models. For the skip-gram example, the embedding weights can be extracted as follows:</p>
<pre>
merge_layer = model.layers[0]<br/>word_model = merge_layer.layers[0]<br/>word_embed_layer = word_model.layers[0]<br/>weights = word_embed_layer.get_weights()[0]
</pre>
<p>Similarly, the embedding weights for the CBOW example can be extracted using the following one-liner:</p>
<pre>
weights = model.layers[0].get_weights()[0]
</pre>
<p>In both cases, the shape of the weights matrix is <kbd>vocab_size</kbd> and <kbd>embed_size</kbd>. In order to compute the distributed representation for a word in the vocabulary, you will need to construct a one-hot vector by setting the position of the word index to one in a zero vector of size (<kbd>vocab_size</kbd>) and multiply it with the matrix to get the embedding vector of size (<kbd>embed_size</kbd>).</p>
<p>A visualization of word embeddings from work done by Christopher Olah (for more information refer to the article: <span><em>Document Embedding with Paragraph Vectors</em>, by Andrew M. Dai, Christopher Olah, and Quoc V. Le, arXiv:1507.07998, 2015</span>) is shown as follows. This is a visualization of word embeddings reduced to two dimensions and visualized with T-SNE. The words forming entity types were chosen using WordNet synset clusters. As you can see, points corresponding to similar entity types tend to cluster together:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="389" src="assets/word_embeddings_colah.png" width="475"/></div>
<p>The source code for the example can be found in <kbd>keras_skipgram.py</kbd> in the source code download.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using third-party implementations of word2vec</h1>
            </header>

            <article>
                
<p>We have covered word2vec extensively over the past few sections. At this point, you understand how the skip-gram and CBOW models work and how to build your own implementation of these models using Keras. However, third-party implementations of word2vec are readily available, and unless your use case is very complex or different, it makes sense to just use one such implementation instead of rolling your own.</p>
<p>The gensim library provides an implementation of word2vec. Even though this is a book about Keras and not gensim, we include a discussion on this because Keras does not provide any support for word2vec, and integrating the gensim implementation into Keras code is very common practice.</p>
<div class="packt_infobox">Installation of gensim is fairly simple and described in detail on the gensim installation page (<a href="https://radimrehurek.com/gensim/install.html" target="_blank">https://radimrehurek.com/gensim/install.html</a>). </div>
<p>The following code shows how to build a word2vec model using gensim and train it with the text from the text8 corpus, available for download at: <a href="http://mattmahoney.net/dc/text8.zip">http://mattmahoney.net/dc/text8.zip</a>. The text8 corpus is a file containing about 17 million words derived from Wikipedia text. Wikipedia text was cleaned to remove markup, punctuation, and non-ASCII text, and the first 100 million characters of this cleaned text became the text8 corpus. This corpus is commonly used as an example for word2vec because it is quick to train and produces good results. First we set up the imports as usual:</p>
<pre>
from gensim.models import KeyedVectors<br/>import logging<br/>import os
</pre>
<p>We then read in the words from the text8 corpus, and split up the words into sentences of 50 words each. The gensim library provides a built-in text8 handler that does something similar. Since we want to illustrate how to generate a model with any (preferably large) corpus that may or may not fit into memory, we will show you how to generate these sentences using a Python generator.</p>
<p>The <kbd>Text8Sentences</kbd> class will generate sentences of <kbd>maxlen</kbd> words each from the text8 file. In this case, we do ingest the entire file into memory, but when traversing through directories of files, generators allows us to load parts of the data into memory at a time, process them, and yield them to the caller:</p>
<pre>
class Text8Sentences(object):<br/>  def __init__(self, fname, maxlen):<br/>    self.fname = fname<br/>    self.maxlen = maxlen<br/>      <br/>  def __iter__(self):<br/>    with open(os.path.join(DATA_DIR, "text8"), "rb") as ftext:<br/>      text = ftext.read().split(" ")<br/>      sentences, words = [], []<br/>      for word in text:<br/>        if len(words) &gt;= self.maxlen:<br/>          yield words<br/>          words = []<br/>          words.append(word)<br/>          yield words
</pre>
<p>We then set up the caller code. The gensim word2vec uses Python logging to report on progress, so we first enable it. The next line declares an instance of the <kbd>Text8Sentences</kbd> class, and the line after that trains the model with the sentences from the dataset. We have chosen the size of the embedding vectors to be <kbd>300</kbd>, and we only consider words that appear a minimum of 30 times in the corpus. The default window size is <kbd>5</kbd>, so we will consider the words <em>w<sub>i-5</sub></em>, <em>w<sub>i-4</sub></em>, <em>w<sub>i-3</sub></em>, <em>w<sub>i-2</sub></em>, <em>w<sub>i-1</sub></em>, <em>w<sub>i+1</sub></em>, <em>w<sub>i+2</sub></em>, <em>w<sub>i+3</sub></em>, <em>w<sub>i+4</sub></em>, and <em>w<sub>i+5</sub></em> as the context for word <em>w<sub>i</sub></em>. By default, the word2vec model created is CBOW, but you can change that by setting <kbd>sg=1</kbd> in the parameters:</p>
<pre>
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)<br/><br/>DATA_DIR = "../data/"<br/>sentences = Text8Sentences(os.path.join(DATA_DIR, "text8"), 50)<br/>model = word2vec.Word2Vec(sentences, size=300, min_count=30)
</pre>
<p>The word2vec implementation will make two passes over the data, first to generate a vocabulary and then to build the actual model. You can see its progress on the console as it runs:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="391" src="assets/ss-5-1.png" width="649"/></div>
<p>Once the model is created, we should normalize the resulting vectors. According to the documentation, this saves lots of memory. Once the model is trained, we can optionally save it to disk:</p>
<pre>
model.init_sims(replace=True)<br/>model.save("word2vec_gensim.bin")
</pre>
<p>The saved model can be brought back into memory using the following call:</p>
<pre>
model = Word2Vec.load("word2vec_gensim.bin")
</pre>
<p>We can now query the model to find all the words it knows about:</p>
<pre>
<strong>&gt;&gt;&gt; model.vocab.keys()[0:10]</strong><br/><strong>['homomorphism',<br/>'woods',<br/>'spiders',<br/>'hanging',<br/>'woody',<br/>'localized',<br/>'sprague',<br/>'originality',<br/>'alphabetic',<br/>'hermann']</strong>
</pre>
<p>We can find the actual vector embedding for a given word:</p>
<pre>
&gt;&gt;&gt; model["woman"]<br/> array([ -3.13099056e-01, -1.85702944e+00, 1.18816841e+00,<br/> -1.86561719e-01, -2.23673001e-01, 1.06527400e+00,<br/> &amp;mldr;<br/> 4.31755871e-01, -2.90115297e-01, 1.00955181e-01,<br/> -5.17173052e-01, 7.22485244e-01, -1.30940580e+00], dtype=”float32”)
</pre>
<p>We can also find words that are most similar to a certain word:</p>
<pre>
<strong>&gt;&gt;&gt; model.most_similar("woman")<br/> [('child', 0.7057571411132812),<br/> ('girl', 0.702182412147522),<br/> ('man', 0.6846336126327515),<br/> ('herself', 0.6292711496353149),<br/> ('lady', 0.6229539513587952),<br/> ('person', 0.6190367937088013),<br/> ('lover', 0.6062309741973877),<br/> ('baby', 0.5993420481681824),<br/> ('mother', 0.5954475402832031),<br/> ('daughter', 0.5871444940567017)]</strong>
</pre>
<p>We can provide hints for finding word similarity. For example, the following command returns the top 10 words that are like <kbd>woman</kbd> and <kbd>king</kbd> but unlike <kbd>man</kbd>:</p>
<pre>
<strong>&gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)<br/> [('queen', 0.6237582564353943),<br/> ('prince', 0.5638638734817505),<br/> ('elizabeth', 0.5557916164398193),<br/> ('princess', 0.5456407070159912),<br/> ('throne', 0.5439794063568115),<br/> ('daughter', 0.5364126563072205),<br/> ('empress', 0.5354889631271362),<br/> ('isabella', 0.5233952403068542),<br/> ('regent', 0.520746111869812),<br/> ('matilda', 0.5167444944381714)]</strong>
</pre>
<p>We can also find similarities between individual words. To give a feel of how the positions of the words in the embedding space correlates with their semantic meanings, let us look at the following word pairs:</p>
<pre>
<strong>&gt;&gt;&gt; model.similarity("girl", "woman")<br/> 0.702182479574<br/> &gt;&gt;&gt; model.similarity("girl", "man")<br/> 0.574259909834<br/> &gt;&gt;&gt; model.similarity("girl", "car")<br/> 0.289332921793<br/> &gt;&gt;&gt; model.similarity("bus", "car")<br/> 0.483853497748</strong>
</pre>
<p>As you can see, <kbd>girl</kbd> and <kbd>woman</kbd> are more similar than <kbd>girl</kbd> and <kbd>man</kbd>, and <kbd>car</kbd> and <kbd>bus</kbd> are more similar than <kbd>girl</kbd> and <kbd>car</kbd>. This agrees very nicely with our human intuition about these words.</p>
<p>The source code for the example can be found in <kbd>word2vec_gensim.py</kbd> in the source code download.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Exploring GloVe</h1>
            </header>

            <article>
                
<p>The global vectors for word representation, or GloVe, embeddings was created by Jeffrey Pennington, Richard Socher, and Christopher Manning (for more information refer to the article: <span><em>GloVe: Global Vectors for Word Representation</em>, by J. Pennington, R. Socher, and C. Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Pp. 1532–1543, 2013</span>). The authors describe GloVe as an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</p>
<p>GloVe differs from word2vec in that word2vec is a predictive model while GloVe is a count-based model. The first step is to construct a large matrix of (word, context) pairs that co-occur in the training corpus. Each element of this matrix represents how often a word represented by the row co-occurs in the context (usually a sequence of words) represented by the column, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="178" src="assets/glove-matfact.png" width="584"/></div>
<p>The GloVe process converts the co-occurrence matrix into a pair of (word, feature) and (feature, context) matrices. This process is known as <strong>matrix factorization</strong> and is done using <strong>stochastic gradient descent</strong> (<strong>SGD</strong>), an iterative numerical method. Rewriting in equation form:</p>
<div class="CDPAlignCenter CDPAlign"><img height="13" src="assets/matfact-eqn.png" width="99"/></div>
<p>Here, <em>R</em> is the original co-occurrence matrix. We first populate <em>P</em> and <em>Q</em> with random values and attempt to reconstruct a matrix <em>R'</em> by multiplying them. The difference between the reconstructed matrix <em>R'</em> and the original matrix <em>R</em> tells us how much we need to change the values of <em>P</em> and <em>Q</em> to move <em>R'</em> closer to <em>R</em>, to minimize the reconstruction error. This is repeated multiple times until the SGD converges and the reconstruction error is below a specified threshold. At that point, the (word, feature) matrix is the GloVe embedding. To speed up the process, SGD is often used in parallel mode, as outlined in the <em>HOGWILD!</em> paper.</p>
<p>One thing to note is that predictive neural network based models such as word2vec and count based models such as GloVe are very similar in intent. Both of them build a vector space where the position of a word is influenced by its neighboring words. Neural network models start with individual examples of word co-occurrences and count based models start with aggregate co-occurrence statistics between all words in the corpus. Several recent papers have demonstrated the correlation between these two types of model.</p>
<p>We will not cover generation of GloVe vectors in more detail in this book. Even though GloVe generally shows higher accuracy than word2vec and is faster to train if you use parallelization, Python tooling is not as mature as for word2vec. The only tool available to do this as of the time of writing is the GloVe-Python project (<a href="https://github.com/maciejkula/glove-python" target="_blank">https://github.com/maciejkula/glove-python</a>), which provides a toy implementation for GloVe on Python.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using pre-trained embeddings</h1>
            </header>

            <article>
                
<p>In general, you will train your own word2vec or GloVe model from scratch only if you have a very large amount of very specialized text. By far the most common use case for Embeddings is to use pre-trained embeddings in some way in your network. The three main ways in which you would use embeddings in your network are as follows:</p>
<ul>
<li>Learn embeddings from scratch</li>
<li>Fine-tune learned embeddings from pre-trained GloVe/word2vec models</li>
<li>Look up embeddings from pre-trained GloVe/word2vec models</li>
</ul>
<p>In the first option, the embedding weights are initialized to small random values and trained using backpropagation. You saw this in the examples for skip-gram and CBOW models in Keras. This is the default mode when you use a Keras Embedding layer in your network.</p>
<p>In the second option, you build a weight matrix from a pre-trained model and initialize the weights of your embedding layer with this weight matrix. The network will update these weights using backpropagation, but the model will converge faster because of good starting weights.</p>
<p>The third option is to look up word embeddings from a pre-trained model, and transform your input to embedded vectors. You can then train any machine learning model (that is, not necessarily even a deep learning network) on the transformed data. If the pre-trained model is trained on a similar domain as the target domain, this usually works very well and is the least expensive option.</p>
<p>For general use with English language text, you can use Google's word2vec model trained over 10 billion words from the Google news dataset. The vocabulary size is about 3 million words and the dimensionality of the embedding is 300. The Google news model (about 1.5 GB) can be downloaded from here: <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing" target="_blank">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing</a>.</p>
<p>Similarly, a pre-trained model trained on 6 billion tokens from English Wikipedia and the gigaword corpus can be downloaded from the GloVe site. The vocabulary size is about 400,000 words and the download provides vectors with dimensions 50, 100, 200, and 300. The model size is about 822 MB. Here is the direct download URL (<a href="http://nlp.stanford.edu/data/glove.6B.zip" target="_blank">http://nlp.stanford.edu/data/glove.6B.zip</a>) for this model. Larger models based on the Common Crawl and Twitter are also available from the same location.</p>
<p>In the following sections, we will look at how to use these pre-trained models in the three ways listed.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Learn embeddings from scratch</h1>
            </header>

            <article>
                
<p>In this example, we will train a one-dimensional <strong>convolutional neural network</strong> (<strong>CNN</strong>) to classify sentences as either positive or negative. You have already seen how to classify images using two-dimensional CNNs in <a href="4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml" target="_blank">Chapter 3</a>, <em>Deep Learning with ConvNets</em>. Recall that CNNs exploit spatial structure in images by enforcing local connectivity between neurons of adjacent layers.</p>
<p>Words in sentences exhibit linear structure in the same way as images exhibit spatial structure. Traditional (non-deep learning) NLP approaches to language modeling involve creating word <em>n</em>-grams (<a href="https://en.wikipedia.org/wiki/N-gram">https://en.wikipedia.org/wiki/N-gram</a><a href="https://en.wikipedia.org/wiki/N-gram" target="_blank">)</a> to exploit this linear structure inherent among words. One-dimensional CNNs do something similar, learning convolution filters that operate on sentences a few words at a time, and max pooling the results to create a vector that represents the most important ideas in the sentence.</p>
<p>There is another class of neural network, called <strong>recurrent neural network</strong> (<strong>RNN</strong>), which is specially designed to handle sequence data, including text, which is a sequence of words. The processing in RNNs is different from that in a CNN. We will learn about RNNs in a future chapter.</p>
<p>In our example network, the input text is converted to a sequence of word indices. Note that we have used the <strong>natural language toolkit</strong> (<strong>NLTK</strong>) to parse the text into sentences and words. We could also have used regular expressions to do this, but the statistical models supplied by NLTK are more powerful at parsing than regular expressions. If you are working with word embeddings, it is very likely that you are also working with NLP, in which case you would have NLTK installed already.</p>
<div class="packt_infobox">This link (<a href="http://www.nltk.org/install.html" target="_blank">http://www.nltk.org/install.html</a>) has information to help you install NLTK on your machine. You will also need to install NLTK data, which is some trained corpora that comes standard with NLTK. Installation instructions for NLTK data are available here: <a href="http://www.nltk.org/data.html" target="_blank">http://www.nltk.org/data.html</a>.</div>
<p>The sequence of word indices is fed into an array of embedding layers of a set size (in our case, the number of words in the longest sentence). The embedding layer is initialized by default to random values. The output of the embedding layer is connected to a 1D convolutional layer that convolves (in our example) word trigrams in 256 different ways (essentially, it applies different learned linear combinations of weights on the word embeddings). These features are then pooled into a single pooled word by a global max pooling layer. This vector (256) is then input to a dense layer, which outputs a vector (2). A softmax activation will return a pair of probabilities, one corresponding to positive sentiment and another corresponding to negative sentiment. The network is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="282" src="assets/umich_conv1d.png" width="290"/></div>
<p class="CDPAlignLeft CDPAlign">Let us look at how to code this up using Keras. First we declare our imports. Right after the constants, you will notice that I set the <kbd>random.seed</kbd> value to <kbd>42</kbd>. This is because we want consistent results between runs. Since the initializations of the weight matrices are random, differences in initialization can lead to differences in output, so this is a way to control that:</p>
<pre>
from keras.layers.core import Dense, Dropout, SpatialDropout1D<br/>from keras.layers.convolutional import Conv1D<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers.pooling import GlobalMaxPooling1D<br/>from kera<br/>s.models import Sequential<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.utils import np_utils<br/>from sklearn.model_selection import train_test_split<br/>import collections<br/>import matplotlib.pyplot as plt<br/>import nltk<br/>import numpy as np<br/><br/>np.random.seed(42)
</pre>
<p>We declare our constants. For all subsequent examples in this chapter, we will classify sentences from the UMICH SI650 sentiment classification competition on Kaggle. The dataset has around 7,000 sentences, and is labeled <em>1</em> for positive and <em>0</em> for negative. The <kbd>INPUT_FILE</kbd> defines the path to this file of sentences and labels. The format of the file is a sentiment label (<em>0</em> or <em>1</em>) followed by a tab, followed by a sentence.</p>
<p>The <kbd>VOCAB_SIZE</kbd> setting indicates that we will consider only the top 5,000 tokens in the text. The <kbd>EMBED_SIZE</kbd> setting is the size of the embedding that will be generated by the embedding layer in the network. <kbd>NUM_FILTERS</kbd> is the number of convolution filters we will train for our convolution layer, and <kbd>NUM_WORDS</kbd> is the size of each filter, that is, how many words we will convolve at a time. The <kbd>BATCH_SIZE</kbd> and <kbd>NUM_EPOCHS</kbd> is the number of records to feed the network each time and how many times we will run through the entire dataset during training:</p>
<pre>
INPUT_FILE = "../data/umich-sentiment-train.txt"<br/>VOCAB_SIZE = 5000<br/>EMBED_SIZE = 100<br/>NUM_FILTERS = 256<br/>NUM_WORDS = 3<br/>BATCH_SIZE = 64<br/>NUM_EPOCHS = 20
</pre>
<p>In the next block, we first read our input sentences and construct our vocabulary out of the most frequent words in the corpus. We then use this vocabulary to convert our input sentences into a list of word indices:</p>
<pre>
counter = collections.Counter()<br/>fin = open(INPUT_FILE, "rb")<br/>maxlen = 0<br/>for line in fin:<br/>    _, sent = line.strip().split("t")<br/>    words = [x.lower() for x in   nltk.word_tokenize(sent)]<br/>    if len(words) &gt; maxlen:<br/>        maxlen = len(words)<br/>    for word in words:<br/>        counter[word] += 1<br/>fin.close()<br/><br/>word2index = collections.defaultdict(int)<br/>for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):<br/>    word2index[word[0]] = wid + 1<br/>vocab_size = len(word2index) + 1<br/>index2word = {v:k for k, v in word2index.items()}
</pre>
<p>We pad each of our sentences to predetermined length <kbd>maxlen</kbd> (in this case the number of words in the longest sentence in the training set). We also convert our labels to categorical format using a Keras utility function. The last two steps are a standard workflow for handling text input that we will see again and again:</p>
<pre>
xs, ys = [], []<br/>fin = open(INPUT_FILE, "rb")<br/>for line in fin:<br/>    label, sent = line.strip().split("t")<br/>    ys.append(int(label))<br/>    words = [x.lower() for x in nltk.word_tokenize(sent)]<br/>    wids = [word2index[word] for word in words]<br/>    xs.append(wids)<br/>fin.close()<br/>X = pad_sequences(xs, maxlen=maxlen)<br/>Y = np_utils.to_categorical(ys)
</pre>
<p>Finally, we split up our data into a <em>70/30</em> training and test set. The data is now in a form ready to be fed into the network:</p>
<pre>
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)
</pre>
<p>We define the network that we described earlier in this section:</p>
<pre>
model = Sequential()<br/>model.add(Embedding(vocab_size, EMBED_SIZE, input_length=maxlen)<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,<br/>activation="relu"))<br/>model.add(GlobalMaxPooling1D())<br/>model.add(Dense(2, activation="softmax"))
</pre>
<p>We then compile the model. Since our target is binary (positive or negative) we choose <kbd>categorical_crossentropy</kbd> as our loss function. For the optimizer, we choose <kbd>adam</kbd>. We then train the model using our training set, using a batch size of 64 and training for 20 epochs:</p>
<pre>
model.compile(loss="categorical_crossentropy", optimizer="adam",<br/>              metrics=["accuracy"])<br/>history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,<br/>                    epochs=NUM_EPOCHS,<br/>                    validation_data=(Xtest, Ytest))
</pre>
<p>The output from the code looks as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="363" src="assets/ss-5-4.png" width="650"/></div>
<p>As you can see, the network gives us 98.6% accuracy on the test set.</p>
<p>The source code for this example can be found in <kbd>learn_embedding_from_scratch.py</kbd> in the source code download for the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Fine-tuning learned embeddings from word2vec</h1>
            </header>

            <article>
                
<p>In this example, we will use the same network as the one we used to learn our embeddings from scratch. In terms of code, the only major difference is an extra block of code to load the word2vec model and build up the weight matrix for the embedding layer.</p>
<p>As always, we start with the imports and set up a random seed for repeatability. In addition to the imports we have seen previously, there is an additional one to import the word2vec model from gensim:</p>
<pre>
from gensim.models import KeyedVectors<br/>from keras.layers.core import Dense, Dropout, SpatialDropout1D<br/>from keras.layers.convolutional import Conv1D<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers.pooling import GlobalMaxPooling1D<br/>from keras.models import Sequential<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.utils import np_utils<br/>from sklearn.model_selection import train_test_split<br/>import collections<br/>import matplotlib.pyplot as plt<br/>import nltk<br/>import numpy as np<br/><br/>np.random.seed(42)
</pre>
<p>Next up is setting up the constants. The only difference here is that we reduced the <kbd>NUM_EPOCHS</kbd> setting from <kbd>20</kbd> to <kbd>10</kbd>. Recall that initializing the matrix with values from a pre-trained model tends to set them to good values that converge faster:</p>
<pre>
INPUT_FILE = "../data/umich-sentiment-train.txt"<br/>WORD2VEC_MODEL = "../data/GoogleNews-vectors-negative300.bin.gz"<br/>VOCAB_SIZE = 5000<br/>EMBED_SIZE = 300<br/>NUM_FILTERS = 256<br/>NUM_WORDS = 3<br/>BATCH_SIZE = 64<br/>NUM_EPOCHS = 10
</pre>
<p>The next block extracts the words from the dataset and creates a vocabulary of the most frequent terms, then parses the dataset again to create a list of padded word lists. It also converts the labels to categorical format. Finally, it splits the data into a training and a test set. This block is identical to the previous example and has been explained in depth there:</p>
<pre>
counter = collections.Counter()<br/>fin = open(INPUT_FILE, "rb")<br/>maxlen = 0<br/>for line in fin:<br/>   _, sent = line.strip().split("t")<br/>   words = [x.lower() for x in nltk.word_tokenize(sent)]<br/>   if len(words) &gt; maxlen:<br/>       maxlen = len(words)<br/>   for word in words:<br/>       counter[word] += 1<br/>fin.close()<br/><br/>word2index = collections.defaultdict(int)<br/>for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):<br/>    word2index[word[0]] = wid + 1<br/>vocab_sz = len(word2index) + 1<br/>index2word = {v:k for k, v in word2index.items()}<br/><br/>xs, ys = [], []<br/>fin = open(INPUT_FILE, "rb")<br/>for line in fin:<br/>    label, sent = line.strip().split("t")<br/>    ys.append(int(label))<br/>    words = [x.lower() for x in nltk.word_tokenize(sent)]<br/>    wids = [word2index[word] for word in words]<br/>    xs.append(wids)<br/>fin.close()<br/>X = pad_sequences(xs, maxlen=maxlen)<br/>Y = np_utils.to_categorical(ys)<br/><br/>Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3,<br/>     random_state=42)
</pre>
<p>The next block loads up the word2vec model from a pre-trained model. This model is trained with about 10 billion words of Google News articles and has a vocabulary size of 3 million. We load it and look up embedding vectors from it for words in our vocabulary, and write out the embedding vector into our weight matrix <kbd>embedding_weights</kbd>. Rows of this weight matrix correspond to words in the vocabulary, and columns of each row constitute the embedding vector for the word.</p>
<p>The dimensions of the <kbd>embedding_weights</kbd> matrix is <kbd>vocab_sz</kbd> and <kbd>EMBED_SIZE</kbd>. The <kbd>vocab_sz</kbd> is one more than the maximum number of unique terms in the vocabulary, the additional pseudo-token <kbd>_UNK_</kbd> representing words that are not seen in the vocabulary.</p>
<p>Note that it is possible that some words in our vocabulary may not be there in the Google News word2vec model, so when we encounter such words, the embedding vectors for them remain at the default value of all zeros:</p>
<pre>
# load word2vec model<br/>word2vec = Word2Vec.load_word2vec_format(WORD2VEC_MODEL, binary=True)<br/>embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))<br/>for word, index in word2index.items():<br/>    try:<br/>        embedding_weights[index, :] = word2vec[word]<br/>    except KeyError:<br/>        pass
</pre>
<p>We define our network. The difference in this block from our previous example is that we initialize the weights of the embedding layer with the <kbd>embedding_weights</kbd> matrix we built in the previous block:</p>
<pre>
model = Sequential()<br/>model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,<br/>          weights=[embedding_weights]))<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,<br/>                        activation="relu"))<br/>model.add(GlobalMaxPooling1D())<br/>model.add(Dense(2, activation="softmax"))
</pre>
<p>We then compile our model with the categorical cross-entropy loss function and the Adam optimizer, and train the network with batch size 64 and for 10 epochs, then evaluate the trained model:</p>
<pre>
model.compile(optimizer="adam", loss="categorical_crossentropy",<br/>              metrics=["accuracy"])<br/>history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,<br/>                    epochs=NUM_EPOCHS,<br/>                    validation_data=(Xtest, Ytest))<br/><br/>score = model.evaluate(Xtest, Ytest, verbose=1)<br/>print("Test score: {:.3f}, accuracy: {:.3f}".format(score[0], score[1]))
</pre>
<p>Output from running the code is shown as follows:</p>
<pre>
<strong>((4960, 42), (2126, 42), (4960, 2), (2126, 2))<br/> Train on 4960 samples, validate on 2126 samples<br/> Epoch 1/10<br/> 4960/4960 [==============================] - 7s - loss: 0.1766 - acc: 0.9369 - val_loss: 0.0397 - val_acc: 0.9854<br/> Epoch 2/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0725 - acc: 0.9706 - val_loss: 0.0346 - val_acc: 0.9887<br/> Epoch 3/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0553 - acc: 0.9784 - val_loss: 0.0210 - val_acc: 0.9915<br/> Epoch 4/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0519 - acc: 0.9790 - val_loss: 0.0241 - val_acc: 0.9934<br/> Epoch 5/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0576 - acc: 0.9746 - val_loss: 0.0219 - val_acc: 0.9929<br/> Epoch 6/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0515 - acc: 0.9764 - val_loss: 0.0185 - val_acc: 0.9929<br/> Epoch 7/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0528 - acc: 0.9790 - val_loss: 0.0204 - val_acc: 0.9920<br/> Epoch 8/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0373 - acc: 0.9849 - val_loss: 0.0221 - val_acc: 0.9934<br/> Epoch 9/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0360 - acc: 0.9845 - val_loss: 0.0194 - val_acc: 0.9929<br/> Epoch 10/10<br/> 4960/4960 [==============================] - 7s - loss: 0.0389 - acc: 0.9853 - val_loss: 0.0254 - val_acc: 0.9915<br/> 2126/2126 [==============================] - 1s<br/> Test score: 0.025, accuracy: 0.993</strong>
</pre>
<p>The model gives us an accuracy of 99.3% on the test set after 10 epochs of training. This is an improvement over the previous example, where we got an accuracy of 98.6% accuracy after 20 epochs.</p>
<p>The source code for this example can be found in  <kbd>finetune_word2vec_embeddings.py</kbd> in the source code download for the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Fine-tune learned embeddings from GloVe</h1>
            </header>

            <article>
                
<p>Fine tuning using pre-trained GloVe embeddings is very similar to fine tuning using pre-trained word2vec embeddings. In fact, all of the code, except for the block that builds the weight matrix for the embedding layer, is identical. Since we have already seen this code twice, I will just focus on the block of code that builds the weight matrix from the GloVe embeddings.</p>
<p>GloVe embeddings come in various flavors. We use the model pre-trained on 6 billion tokens from the English Wikipedia and the gigaword corpus. The vocabulary size for the model is about 400,000, and the download provides vectors of dimensions 50, 100, 200, and 300. We will use embeddings from the 300 dimensional model.</p>
<p>The only thing we need to change in the code for the previous example is to replace the block that instantiated a word2vec model and loaded the embedding matrix using the following block of code. If we use a model with vector size other than 300, then we also need to update <kbd>EMBED_SIZE</kbd>.</p>
<p>The vectors are provided in space-delimited text format, so the first step is to read the code into a dictionary, <kbd>word2emb</kbd>. This is analogous to the line instantiating the Word2Vec model in our previous example:</p>
<pre>
GLOVE_MODEL = "../data/glove.6B.300d.txt"<br/>word2emb = {}<br/>fglove = open(GLOVE_MODEL, "rb")<br/>for line in fglove:<br/>    cols = line.strip().split()<br/>    word = cols[0]<br/>    embedding = np.array(cols[1:], dtype="float32")<br/>    word2emb[word] = embedding<br/>fglove.close()
</pre>
<p>We then instantiate an embedding weight matrix of size (<kbd>vocab_sz</kbd> and <kbd>EMBED_SIZE</kbd>) and populate the vectors from the <kbd>word2emb</kbd> dictionary. Vectors for words that are found in the vocabulary but not in the GloVe model remain set to all zeros:</p>
<pre>
embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))<br/>for word, index in word2index.items():<br/>    try:<br/>        embedding_weights[index, :] = word2emb[word]<br/>    except KeyError:<br/>        pass
</pre>
<p>The full code for this program can be found in <kbd>finetune_glove_embeddings.py</kbd> in the book's code repository on GitHub. The output of the run is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="345" src="assets/ss-5-2.png" width="682"/></div>
<p>This gives us 99.1% accuracy in 10 epochs, which is almost as good as the results we got from fine-tuning the network using word2vec <kbd>embedding_weights</kbd>.</p>
<p>The source code for this example can be found in <kbd>finetune_glove_embeddings.py</kbd> in the source code download for this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Look up embeddings</h1>
            </header>

            <article>
                
<p>Our final strategy is to look up embeddings from pre-trained networks. The simplest way to do this with the current examples is to just set the <kbd>trainable</kbd> parameter of the embedding layer to <kbd>False</kbd>. This ensures that backpropagation will not update the weights on the embedding layer:</p>
<pre>
model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,<br/>                     weights=[embedding_weights],<br/>                     trainable=False))<br/>model.add(SpatialDropout1D(Dropout(0.2)))
</pre>
<p>Setting this value with the word2vec and GloVe examples gave us accuracies of 98.7% and 98.9% respectively after 10 epochs of training.</p>
<p>However, in general, this is not how you would use pre-trained embeddings in your code. Typically, it involves preprocessing your dataset to create word vectors by looking up words in one of the pre-trained models, and then using this data to train some other model. The second model would not contain an Embedding layer, and may not even be a deep learning network.</p>
<p>The following example describes a dense network that takes as its input a vector of size <kbd>100</kbd>, representing a sentence, and outputs a <kbd>1</kbd> or <kbd>0</kbd> for positive or negative sentiment. Our dataset is still the one from the UMICH S1650 sentiment classification competition with around 7,000 sentences.</p>
<p>As previously, large parts of the code are repeated, so we only explain the parts that are new or otherwise need explanation.<br/>
We begin with the imports, set the random seed for repeatability, and set some constant values. In order to create the 100-dimensional vectors for each sentence, we add up the GloVe 100-dimensional vectors for the words in the sentence, so we choose the <kbd>glove.6B.100d.txt</kbd> file:</p>
<pre>
from keras.layers.core import Dense, Dropout, SpatialDropout1D<br/>from keras.models import Sequential<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.utils import np_utils<br/>from sklearn.model_selection import train_test_split<br/>import collections<br/>import matplotlib.pyplot as plt<br/>import nltk<br/>import numpy as np<br/><br/>np.random.seed(42)<br/><br/>INPUT_FILE = "../data/umich-sentiment-train.txt"<br/>GLOVE_MODEL = "../data/glove.6B.100d.txt"<br/>VOCAB_SIZE = 5000<br/>EMBED_SIZE = 100<br/>BATCH_SIZE = 64<br/>NUM_EPOCHS = 10
</pre>
<p>The next block reads the sentences and creates a word frequency table. From this, the most common 5,000 tokens are selected and lookup tables (from word to word index and back) are created. In addition, we create a pseudo-token <kbd>_UNK_</kbd> for tokens that do not exist in the vocabulary. Using these lookup tables, we convert each sentence to a sequence of word IDs, padding these sequences so that all sequences are of the same length (the maximum number of words in a sentence in the training set). We also convert the labels to categorical format:</p>
<pre>
counter = collections.Counter()<br/>fin = open(INPUT_FILE, "rb")<br/>maxlen = 0<br/>for line in fin:<br/>    _, sent = line.strip().split("t")<br/>    words = [x.lower() for x in nltk.word_tokenize(sent)]<br/>    if len(words) &gt; maxlen:<br/>        maxlen = len(words)<br/>    for word in words:<br/>        counter[word] += 1<br/>fin.close()<br/><br/>word2index = collections.defaultdict(int)<br/>for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):<br/>     word2index[word[0]] = wid + 1<br/>vocab_sz = len(word2index) + 1<br/>index2word = {v:k for k, v in word2index.items()}<br/>index2word[0] = "_UNK_"<br/><br/>ws, ys = [], []<br/>fin = open(INPUT_FILE, "rb")<br/>for line in fin:<br/>    label, sent = line.strip().split("t")<br/>    ys.append(int(label))<br/>    words = [x.lower() for x in nltk.word_tokenize(sent)]<br/>    wids = [word2index[word] for word in words]<br/>    ws.append(wids)<br/>fin.close()<br/>W = pad_sequences(ws, maxlen=maxlen)<br/>Y = np_utils.to_categorical(ys)
</pre>
<p>We load the GloVe vectors into a dictionary. If we wanted to use word2vec here, all we have to do is replace this block with a gensim <kbd>Word2Vec.load_word2vec_format()</kbd> call and replace the following block to look up the word2vec model instead of the <kbd>word2emb</kbd> dictionary:</p>
<pre>
word2emb = collections.defaultdict(int)<br/>fglove = open(GLOVE_MODEL, "rb")<br/>for line in fglove:<br/>    cols = line.strip().split()<br/>    word = cols[0]<br/>    embedding = np.array(cols[1:], dtype="float32")<br/>    word2emb[word] = embedding<br/>fglove.close()
</pre>
<p>The next block looks up the words for each sentence from the word ID matrix <kbd>W</kbd> and populates a matrix <kbd>E</kbd> with the corresponding embedding vector. These embedding vectors are then added to create a sentence vector, which is written back into the <kbd>X</kbd> matrix. The output of this code block is the matrix <kbd>X</kbd> of size (<kbd>num_records</kbd> and <kbd>EMBED_SIZE</kbd>):</p>
<pre>
X = np.zeros((W.shape[0], EMBED_SIZE))<br/>for i in range(W.shape[0]):<br/>    E = np.zeros((EMBED_SIZE, maxlen))<br/>    words = [index2word[wid] for wid in W[i].tolist()]<br/>    for j in range(maxlen):<br/>         E[:, j] = word2emb[words[j]]<br/>    X[i, :] = np.sum(E, axis=1)
</pre>
<p>We have now preprocessed our data using the pre-trained model and are ready to use it to train and evaluate our final model. Let us split the data into <em>70/30</em> training/test as usual:</p>
<pre>
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)
</pre>
<p>The network we will train for doing the sentiment analysis task is a simple dense network. We compile it with a categorical cross-entropy loss function and the Adam optimizer, and train it with the sentence vectors that we built out of the pre-trained embeddings. Finally, we evaluate the model on the 30% test set:</p>
<pre>
model = Sequential()<br/>model.add(Dense(32, input_dim=100, activation="relu"))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(2, activation="softmax"))<br/><br/>model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])<br/>history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,<br/>                    epochs=NUM_EPOCHS,<br/>                    validation_data=(Xtest, Ytest))<br/><br/>score = model.evaluate(Xtest, Ytest, verbose=1)<br/>print("Test score: {:.3f}, accuracy: {:.3f}".format(score[0], score[1])<kbd>)</kbd>
</pre>
<p>The output for the code using GloVe embeddings is shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="370" src="assets/ss-5-3.png" width="695"/></div>
<p>The dense network gives us 96.5% accuracy on the test set after 10 epochs of training when preprocessed with the 100-dimensional GloVe embeddings. With preprocessed with the word2vec embeddings (300-dimensional fixed) the network gives us 98.5% on the test set.</p>
<p>The source code for this example can be found in <kbd>transfer_glove_embeddings.py</kbd> (for the GloVe example) and <kbd>transfer_word2vec_embeddings.py</kbd> (for the word2vec example) in the source code download for the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we learned how to transform words in text into vector embeddings that retain the distributional semantics of the word. We also now have an intuition of why word embeddings exhibit this kind of behavior and why word embeddings are useful for working with deep learning models for text data.</p>
<p>We then looked at two popular word embedding schemes, word2vec and GloVe, and understood how these models work. We also looked at using gensim to train our own word2vec model from data.</p>
<p>Finally, we learned about different ways of using embeddings in our network. The first was to learn embeddings from scratch as part of training our network. The second was to import embedding weights from pre-trained word2vec and GloVe models into our networks and fine-tune them as we train the network. The third was to use these pre-trained weights as is in our downstream applications.</p>
<p>In the next chapter, we will learn about recurrent neural networks, a class of network that is optimized for handling sequence data such as text.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>