<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Playing Atari Games</h1>
                </header>
            
            <article>
                
<p>an a machine learn how to play video games by itself and beat human players? Solving this problem is the first step toward general <strong>artificial intelligence</strong> (<strong>AI</strong>) in the field of gaming. The key technique to creating an AI player is <strong>deep reinforcement learning</strong>. In 2015, Google's DeepMind, one of the foremost AI/machine learning research teams (who are famous for building AlphaGo, the machine that beat Go champion Lee Sedol) proposed the deep Q-learning algorithm to build an AI player that can learn to play Atari 2600 games, and surpass a human expert on several games. This work made a great impact on AI research, showing the possibility of building general AI systems.</p>
<p>In this chapter, we will introduce how to use gym to play Atari 2600 games, and then explain why the deep Q-learning algorithm works and how to implement it using TensorFlow. The goal is to be able to understand deep reinforcement learning algorithms and how to apply them to solve real tasks. This chapter will be a solid foundation to understanding later chapters, where we will be introducing more complex methods.</p>
<p>The topics that we will cover in this chapter are as follows:</p>
<ul>
<li>Introduction to Atari games</li>
<li>Deep Q-learning</li>
<li>Implementation of DQN</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Atari games</h1>
                </header>
            
            <article>
                
<p>Atari, Inc. was an American video game developer and home computer company founded in 1972 by Nolan Bushnell and Ted Dabney. In 1976, Bushnell developed the Atari video computer system, or Atari VCS (later renamed Atari 2600). Atari VCS was a flexible console that was capable of playing the existing Atari games, which included a console, two joysticks, a pair of paddles, and the combat game cartridge. The following screenshot depicts an Atari console:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-618 image-border" src="assets/c9a3ad57-527f-4974-8906-2430734aae5a.png" style="width:21.17em;height:12.50em;"/></div>
<p>Atari 2600 has more than 500 games that were published by Atari, Sears, and some third parties. Some famous games are Breakout, Pac-Man, Pitfall!, Atlantis, Seaquest, and Space Invaders.</p>
<p>As a direct result of the North American video game crash of 1983, Atari, Inc. was closed and its properties were split in 1984. The home computing and game console divisions of Atari were sold to Jack Tramiel under the name Atari corporation in July 1984.</p>
<p>For readers who are interested in playing Atari games, here are several online Atari 2600 emulator websites where you can find many popular Atari 2600 games:</p>
<ul>
<li class="packt_figref CDPAlignLeft CDPAlign"><a href="http://www.2600online.com/">http://www.2600online.com/</a></li>
<li class="packt_figref CDPAlignLeft CDPAlign"><a href="http://www.free80sarcade.com/all2600games.php">http://www.free80sarcade.com/all2600games.php</a></li>
<li class="packt_figref CDPAlignLeft CDPAlign"><a href="http://www.retrogames.cz/index.php">http://www.retrogames.cz/index.php</a></li>
</ul>
<p>Because our goal is to develop an AI player for these games, it is better to play with them first and understand their difficulties. The most important thing is to: relax and have fun!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an Atari emulator</h1>
                </header>
            
            <article>
                
<p>OpenAI gym provides an Atari 2600 game environment with a Python interface. The games are simulated by the arcade learning environment, which uses the Stella Atari emulator. For more details, read the following papers:</p>
<ul>
<li>MG Bellemare, Y Naddaf, J Veness, and M Bowling, <em>The arcade learning environment: An evaluation platform for general agents</em>, journal of Artificial Intelligence Research (2012)</li>
<li>Stella: A Multi-Platform Atari 2600 VCS emulator, <a href="http://stella.sourceforge.net/">http://stella.sourceforge.net/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p>If you don't have a full install of OpenAI <kbd>gym</kbd>, you can install the Atari environment dependencies via the following:</p>
<pre class="CDPAlignCenter CDPAlign CDPAlignLeft"><strong>pip install gym[atari]</strong></pre>
<p>This requires the <kbd>cmake</kbd> tools. This command will automatically compile the arcade learning environment and its Python interface, <kbd>atari-py</kbd>. The compilation will take a few minutes on a common laptop, so go have a cup of coffee.</p>
<p>After the Atari environment is installed, try the following:</p>
<pre>import gym<br/>atari = gym.make('Breakout-v0')<br/>atari.reset()<br/>atari.render()</pre>
<p>If it runs successfully, a small window will pop up, showing the screen of the game <kbd>Breakout</kbd>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-619 image-border" src="assets/cb33c342-7720-4f1e-b366-2e5335aad1bb.png" style="width:10.08em;height:14.08em;"/></div>
<p>The meaning of the v0 suffix in the <kbd>Breakout</kbd> rom name will be explained later. We will use <kbd>Breakout</kbd> to test our algorithm for training an AI game player. In <kbd>Breakout</kbd>, several layers of bricks lie on the top of the screen. A ball travels across the screen, bouncing off the top and side walls of the screen. When a brick is hitted, the ball bounces away and the brick is destroyed, giving the player several points according to the color of the brick. The player loses a turn when the ball touches the bottom of the screen. In order to prevent this from happening, the player has to move the paddle to bounce the ball back.</p>
<p>Atari VCS uses a joystick as the input device for controlling Atari 2600 games. The total number of inputs that a joystick and a paddle can make is 18. In the <kbd>gym</kbd> Atari environment, these actions are labeled as the integers ranged from 0 to 17. The meaning of each action is as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>NO OPERATION</td>
<td>FIRE</td>
<td>UP</td>
<td>RIGHT</td>
<td>LEFT</td>
<td>DOWN</td>
</tr>
<tr>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>11</td>
</tr>
<tr>
<td>UP+RIGHT</td>
<td>UP+LEFT</td>
<td>DOWN+RIGHT</td>
<td>DOWN+LEFT</td>
<td>UP+FIRE</td>
<td>RIGHT+FIRE</td>
</tr>
<tr>
<td>12</td>
<td>13</td>
<td>14</td>
<td>15</td>
<td>16</td>
<td>17</td>
</tr>
<tr>
<td>LEFT+FIRE</td>
<td>DOWN+FIRE</td>
<td>UP+RIGHT+FIRE</td>
<td>UP+LEFT+FIRE</td>
<td>DOWN+RIGHT+FIRE</td>
<td>DOWN+LEFT+FIRE</td>
</tr>
</tbody>
</table>
<p> </p>
<p>One can use the following code to get the meanings of the valid actions for a game:</p>
<pre><span>actions = atari.env.get_action_meanings()</span></pre>
<p>For <kbd>Breakout</kbd>, the actions include the following:</p>
<pre class="CDPAlignLeft CDPAlign">[0, 1, 3, 4] or ['NOOP', 'FIRE', 'RIGHT', 'LEFT']</pre>
<p>To get the number of the actions, one can also use the following:</p>
<pre>num_actions = atari.env.action_space.n</pre>
<p>Here, the member variable, <kbd>action_space</kbd>, in <kbd>atari.env</kbd> stores all the information about the valid actions for a game. Typically, we only need to know the total number of valid actions.</p>
<p>We now know how to access the action information in the Atari environment. But, how do you control the game given these actions? To take an action, one can call the <kbd>step</kbd> function:</p>
<pre>observation, reward, done, info = atari.step(a)</pre>
<p>The input argument, <kbd>a</kbd>, is the action you want to execute, which is the index in the valid action list. For example, if one wants to take the <kbd>LEFT</kbd> action, the input should be <kbd>3</kbd> not <kbd>4</kbd>, or if one takes no action, the input should be <kbd>0</kbd>. The <kbd>step</kbd> function returns one of the following four values:</p>
<ul>
<li style="font-weight: 400"><kbd>Observation</kbd>: An environment-specific object representing your observation of the environment. For Atari, it is the screen image of the frame after the action is executed.</li>
<li style="font-weight: 400"><kbd>Reward</kbd>: The amount of reward achieved by the action.</li>
<li style="font-weight: 400"><kbd>Done</kbd>: Whether it's time to reset the environment again. In Atari, if you lost your last life, <kbd>done</kbd> will be true, otherwise it is false.</li>
<li style="font-weight: 400"><kbd>Info</kbd>: Diagnostic information useful for debugging. It is not allowed to use this information in the learning algorithm, so usually we can ignore it.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of the Atari emulator</h1>
                </header>
            
            <article>
                
<p>We are now ready to build a simple Atari emulator using gym. As with other computer games, the keyboard input used to control Atari games is as shown here:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><em>w</em></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><em>a</em></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><em>s</em></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><em>d</em></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><em>space</em></div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">UP</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">LEFT</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">DOWN</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">RIGHT</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">FIRE</div>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>To detect the keyboard inputs, we use the <kbd>pynput.keyboard</kbd> package, which allows us to control and monitor the keyboard (<a href="http://pythonhosted.org/pynput/">http://pythonhosted.org/pynput/</a>). If the <kbd>pynput</kbd> package is not installed, run the following:</p>
<pre><strong>pip install pynput</strong></pre>
<p class="mce-root"/>
<p><kbd>pynput.keyboard</kbd> provides a keyboard listener used to capture keyboard events. Before creating a keyboard listener, the <kbd>Listener</kbd> class should be imported:</p>
<pre>import gym<br/>import queue, threading, time<br/>from pynput.keyboard import Key, Listener</pre>
<p>Besides the <kbd>Listener</kbd> class, the other packages, such as <kbd>gym</kbd> and <kbd>threading</kbd>, are also necessary in this program.</p>
<p>The following code shows how to use <kbd>Listener</kbd> to capture keyboard inputs, that is, where one of the <em>w</em>, <em>a</em>, <em>s</em>, <em>d</em>, and <em>space</em> keys is pressed:</p>
<pre>def keyboard(queue):<br/>    <br/>    def on_press(key):<br/>        if key == Key.esc:<br/>            queue.put(-1)<br/>        elif key == Key.space:<br/>            queue.put(ord(' '))<br/>        else:<br/>            key = str(key).replace("'", '')<br/>            if key in ['w', 'a', 's', 'd']:<br/>                queue.put(ord(key))<br/><br/>    def on_release(key):<br/>        if key == Key.esc:<br/>            return False<br/><br/>    with Listener(on_press=on_press, on_release=on_release) as listener:<br/>        listener.join()</pre>
<p>Actually, a keyboard listener is a Python <kbd>threading.Thread</kbd> object, and all callbacks will be invoked from the thread. In the <kbd>keyboard</kbd> function, the listener registers two callbacks: <kbd>on_press</kbd> , which is invoked when a key is pressed and <kbd>on_release</kbd> invoked when a key is released. This function uses a synchronized queue to share data between different threads. When <em>w</em>, <em>a</em>, <em>s</em>, <em>d</em>, or <em>space</em> is pressed, its ASCII value is sent to the queue, which can be accessed from another thread. If <em>esc</em> is pressed, a termination signal, <kbd><em>-</em></kbd>, is sent to the queue. Then, the listener thread stops when <em>esc</em> is released.</p>
<p>Starting a keyboard listener has some restrictions on macOS X; that is, one of the following should be true:</p>
<ul>
<li style="font-weight: 400">The process must run as root</li>
<li style="font-weight: 400">The application must be white-listed under enable access for assistive devices</li>
</ul>
<p>For more information, visit <a href="https://pythonhosted.org/pynput/keyboard.html">https://pythonhosted.org/pynput/keyboard.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atari simulator using gym</h1>
                </header>
            
            <article>
                
<p>The other part of the emulator is the <kbd>gym</kbd> Atari simulator:</p>
<pre>def start_game(queue):<br/>    <br/>    atari = gym.make('Breakout-v0')<br/>    key_to_act = atari.env.get_keys_to_action()<br/>    key_to_act = {k[0]: a for k, a in key_to_act.items() if len(k) &gt; 0}<br/>    observation = atari.reset()<br/>    <br/>    import numpy<br/>    from PIL import Image<br/>    img = numpy.dot(observation, [0.2126, 0.7152, 0.0722])<br/>    img = cv2_resize_image(img)<br/>    img = Image.fromarray(img)<br/>    img.save('save/{}.jpg'.format(0))<br/>    <br/>    while True:<br/>        atari.render()<br/>        action = 0 if queue.empty() else queue.get(block=False)<br/>        if action == -1:<br/>            break<br/>        action = key_to_act.get(action, 0)<br/>        observation, reward, done, _ = atari.step(action)<br/>        if action != 0:<br/>            print("Action {}, reward {}".format(action, reward))<br/>        if done:<br/>            print("Game finished")<br/>            break<br/>        time.sleep(0.05)</pre>
<p>The first step is to create an <kbd>Atari</kbd> environment using <kbd>gym.make</kbd>. If you are interested in playing other games such as Seaquest or Pitfall, just change Breakout-v0 to Seaquest-v0 or <kbd>Pitfall-v0</kbd>. Then, <kbd>get_keys_to_action</kbd> is called to get the <kbd>key to action</kbd> mapping, which maps the ASCII values of <em>w</em>, <em>a</em>, <em>s</em>, <em>d,</em> and <em>space</em> to internal actions. Before the Atari simulator starts, the <kbd>reset</kbd> function must be called to reset the game parameters and memory, returning the first game screen image. In the main loop, <kbd>render</kbd> is called to render the Atari game at each step. The input action is pulled from the queue without blocking. If the action is the termination signal, -1, the game quits. Otherwise, this action is taken at the current step by running <kbd>atari.step</kbd>.</p>
<p>To start the emulator, run the following code:</p>
<pre>if __name__ == "__main__":<br/>    queue = queue.Queue(maxsize=10)<br/>    game = threading.Thread(target=start_game, args=(queue,))<br/>    game.start()<br/>    keyboard(queue)</pre>
<p>Press the <span class="packt_screen">fire</span> button to start the game and enjoy it! This emulator provides a basic framework for testing AI algorithms on the <kbd>gym</kbd> Atari environment. Later, we will replace the <kbd>keyboard</kbd> function with our AI player.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>Careful readers may notice that a suffix, v0, follows each game name, and come up with the following questions: <em>What is the meaning of v0?</em> <em>Is it allowable to replace it with v1 or v2?</em> Actually, this suffix has a relationship with the data preprocessing step for the screen images (observations) extracted from the Atari environment.</p>
<p>There are three modes for each game, for example, Breakout, BreakoutDeterministic, and BreakoutNoFrameskip, and each mode has two versions, for example, Breakout-v0 and Breakout-v4. The main difference between the three modes is the value of the frameskip parameter in the Atari environment. This parameter indicates the number of frames (steps) the one action is repeated on. This is called the <strong>frame-skipping</strong> technique, which allows us to play more games without significantly increasing the runtime.</p>
<p>For Breakout, frameskip is randomly sampled from 2 to 5. The following <span>screenshots</span> show the frame images returned by the <kbd>step</kbd> function when the action <kbd>LEFT</kbd> is submitted:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-707 image-border" src="assets/9434b162-7c3b-4322-8015-f484166ff2cc.png" style="width:39.08em;height:17.25em;"/></div>
<p class="mce-root"/>
<p>For BreakoutDeterministic, frameskip is set to 3 for the game Space Invaders, and 4 for the other games. With the same <kbd>LEFT</kbd> action, the <kbd>step</kbd><span> </span>function returns the following:</p>
<div class="CDPAlignCenter"><img src="assets/a7d881b9-f685-45ea-ac9f-2dbd28a0cdb9.png"/></div>
<p> </p>
<p>For BreakoutNoFrameskip, frameskip is always 1 for all of the games, meaning no frame-skipping. Similarly, the <kbd>LEFT</kbd> action is taken at each step:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-713 image-border" src="assets/162abb44-4fb4-471b-a75e-a70638960a01.png" style="width:35.42em;height:15.67em;"/></div>
<p>These <span>screenshots</span> demonstrate that although the step function is called four times with the same action, <kbd>LEFT</kbd>, the final positions of the paddle are quite different. Because frameskip is 4 for BreakoutDeterministic, its paddle is the closest one to the left wall. For BreakoutNoFrameskip, frameskip is set to 1 so that its paddle is farthest from the left wall. For Breakout, its paddle lies in the middle because of frameskip being sampled from [2, 5] at each step.</p>
<p>From this simple experiment, we can see the effect of the frameskip parameter. Its value is usually set to 4 for fast learning. Recall that there are two versions, v0 and v4, for each mode. Their main difference is the <kbd>repeat_action_probability</kbd> parameter. This parameter indicates the probability that a <strong>no operation</strong> (<strong><span>NOOP</span></strong>) action is taken, although another action is submitted. It is set to 0.25 for v0, and 0.0 for v4. Because we want a deterministic Atari environment, the v4 version is selected in this chapter.</p>
<p>If you have played some Atari games, you have probably noticed that the top region of the screen in a game usually contains the scoreboard, showing the current score you got and the number of lives you have. This information is not related to game playing, so that the top region can be cropped. Besides, the frame images returned by the step function are RGB images. Actually, in the Atari environment, colorful images do not provide more information than grayscale images; namely, one can play Atari games as usual with a gray screen. Therefore, it is necessary to keep only useful information by cropping frame images and converting them to grayscale.</p>
<p>Converting an RGB image into a grayscale image is quite easy. The value of each pixel in a grayscale image represents the light intensity, which can be calculated by this formula:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55c3aeec-c33c-49d0-ba30-6e35ed922f3f.png" style="width:16.58em;height:1.00em;"/></div>
<p>Here, R, G, and B are the red, green, and blue channels of the RGB image, respectively. Given a RGB image with shape (height, width, channel), the following Python code can be used to convert it into grayscale:</p>
<pre>def rgb_to_gray(self, im):<br/>    return numpy.dot(im, [0.2126, 0.7152, 0.0722])</pre>
<p>The following image gives an example:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-709 image-border" src="assets/3ef54390-7f38-4876-b5b9-ac784d86c1ca.png" style="width:31.17em;height:14.58em;"/></div>
<p>For cropping frame images, we use the <kbd>opencv-python</kbd> <span>package </span>or <kbd>cv2</kbd>, a Python wrapper around the original C++ OpenCV implementation. For more information, please visit the <kbd>opencv-python</kbd> website at <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/index.html">http://opencv-python-tutroals.readthedocs.io/en/latest/index.html</a>. The <kbd>opencv-python</kbd> package provides basic image transformation operations such as image scaling, translation, and rotation. In this chapter, we only need the image scaling function resize, which takes the input image, image size, and interpolation method as the input arguments, and returns the resized image.</p>
<p>The following code shows the image cropping operation, which involves two steps:</p>
<ol>
<li>Reshaping the input image such that the width of the resulting image equals the resized width, <kbd>84</kbd>, indicated by the <kbd>resized_shape</kbd> parameter.</li>
<li>Cropping the top region of the reshaped image using <kbd>numpy</kbd> <span>slicing:</span></li>
</ol>
<pre style="padding-left: 60px">def cv2_resize_image(image, resized_shape=(84, 84), <br/>                     method='crop', crop_offset=8):<br/>        <br/>    height, width = image.shape<br/>    resized_height, resized_width = resized_shape<br/>    <br/>    if method == 'crop':<br/>        h = int(round(float(height) * resized_width / width))<br/>        resized = cv2.resize(image, <br/>                             (resized_width, h), <br/>                             interpolation=cv2.INTER_LINEAR)<br/>        crop_y_cutoff = h - crop_offset - resized_height<br/>        cropped = resized[crop_y_cutoff:crop_y_cutoff+resized_height, :]<br/>        return numpy.asarray(cropped, dtype=numpy.uint8)<br/>    elif method == 'scale':<br/>        return numpy.asarray(cv2.resize(image, <br/>                                        (resized_width, resized_height), <br/>                                        interpolation=cv2.INTER_LINEAR), <br/>                                        dtype=numpy.uint8)<br/>    else:<br/>        raise ValueError('Unrecognized image resize method.')</pre>
<p>For example, given a grayscale input image, the <kbd>cv2_resize_image</kbd> function returns a cropped image with size <img src="assets/e6e3287e-f259-4902-a935-7c2ca2ce9e38.png" style="width:3.42em;height:0.92em;"/>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-710 image-border" src="assets/9ac9969b-d898-4ba6-8020-60c06f247212.png" style="width:33.50em;height:19.08em;"/></div>
<p>So far, we have finished the data preparation. The data is now ready to be used to train our AI player.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Q-learning</h1>
                </header>
            
            <article>
                
<p>Here comes the fun part—the brain design of our AI Atari player. The core algorithm is based on deep reinforcement learning or deep RL. In order to understand it better, some basic mathematical formulations are required. Deep RL is a perfect combination of deep learning and traditional reinforcement learning. Without understanding the basic concepts about reinforcement learning, it is difficult to apply deep RL correctly in real applications, for example, it is possible that someone may try to use deep RL without defining state space, reward, and transition properly.</p>
<p>Well, don't be afraid of the difficulty of the formulations. We only need high school-level mathematics, and will not go deep into the mathematical proofs of why traditional reinforcement learning algorithms work. The goal of this chapter is to learn the basic Q-learning algorithm, to know how to extend it into the <strong>deep Q-learning algorithm</strong> (<strong>DQN</strong>), and to understand the intuition behind these algorithms. Besides, you will also learn what the advantages and disadvantages are of DQN, what exploration and exploitation are, why a replay memory is necessary, why a target network is needed, and how to design a convolutional neural network for state feature representation.</p>
<p>It looks quite interesting, doesn't it? We hope this chapter not only helps you to understand how to apply deep reinforcement learning to solve practical problems, but also opens a door for deep reinforcement learning research. For the readers who are familiar with convolutional neural networks, the Markov decision process, and Q-learning, skip the first section and go directly to the implementation of DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic elements of reinforcement learning</h1>
                </header>
            
            <article>
                
<p>First, let's us recall some basic elements of reinforcement learning that we discussed in the first chapter:</p>
<ul>
<li style="font-weight: 400"><strong>State</strong>: The state space defines all the possible states of the environment. In Atari games, a state is a screen image or a set of several consecutive screen images observed by the player at a given time, indicating the game status of that moment.</li>
</ul>
<ul>
<li style="font-weight: 400"><strong>Reward function</strong>: A reward function defines the goal of a reinforcement learning problem. It maps a state or a state-action pair of the environment to a real number, indicating the desirability of that state. The reward is the score received after taking a certain action in Atari games.</li>
<li style="font-weight: 400"><strong>Policy function</strong>: A policy function defines the behavior of the player at a given time, which maps the states of the environment to actions to be taken in those states.</li>
<li style="font-weight: 400"><strong>Value function</strong>: A value function indicates which state or state-action pair is good in the long run. The value of a state is the total (or discounted) amount of reward a player can expect to accumulate over the future, starting from that state.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Demonstrating basic Q-learning algorithm</h1>
                </header>
            
            <article>
                
<p>To demonstrate the basic Q-learning algorithm, let's consider a simple problem. Imagine that our agent (player) lives in a grid world. One day, she was trapped in a weird maze, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-620 image-border" src="assets/6b46b3ad-be5c-4d13-abb5-f669a88e09b1.png" style="width:19.08em;height:13.58em;"/></div>
<p>The maze contains six rooms. Our agent appears in Room 1, while she has no knowledge about the maze, that is, she doesn't know Room 6 has the sweetheart that is able to send her back home, or that Room 4 has a lightning bolt that strikes her. Therefore, she has to explore the maze carefully to escape as soon as possible. So, how do we  make our lovely agent learn from experience?</p>
<p>Fortunately, her good friend Q-learning can help her survive. This problem can be represented as a state diagram, where each room is taken as a state and her movement from one room to another is considered as an action. The state diagram is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-621 image-border" src="assets/509db329-0ece-43df-8691-d44997a5aa75.png" style="width:29.08em;height:13.08em;"/></div>
<p>Here, an action is represented by an arrow and the number associated with an arrow is the reward of that state-action pair. For example, when our agent moves from Room 5 to Room 6, she gets 100 points because of achieving the goal. When she moves from Room 3 to Room 4, she get a negative reward because the lightning strike hurts her. This state diagram can also be represented by a matrix:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>state\action</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>1</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>2</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>3</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>4</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>5</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>6</strong></div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">1</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">2</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">3</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-50</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">4</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">5</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">0</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">100</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">6</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The dash line in the matrix indicates that the action is not available in that state. For example, our agent cannot move from Room 1 to Room 6 directly because there is no door connecting them.</p>
<p class="mce-root"/>
<p>Let's <img class="fm-editor-equation" src="assets/c8412c07-9c66-46b2-a692-158c6b140f63.png" style="width:0.75em;height:1.00em;"/> be a state, <img class="fm-editor-equation" src="assets/61f535eb-890a-4d2c-af5f-905d0ad05cfc.png" style="width:0.83em;height:1.00em;"/> be an action, <img class="fm-editor-equation" src="assets/91112b13-bd5a-48aa-bcff-dd917bda7680.png" style="width:3.25em;height:1.25em;"/> be the reward function, and <img class="fm-editor-equation" src="assets/1baa24f0-4939-44da-afbe-c8572b2f76b7.png" style="width:3.08em;height:1.17em;"/> be the value function. Recall that <img class="fm-editor-equation" src="assets/57c05ff8-0396-4939-96b9-98ad751f6b59.png" style="width:3.08em;height:1.17em;"/> is the desirability of the state-action pair <img class="fm-editor-equation" src="assets/550d9049-162a-41a8-a884-8310644d63db.png" style="width:2.42em;height:1.25em;"/> in the long run, meaning that our agent is able to make decisions about which room she enters based on <img class="fm-editor-equation" src="assets/39cbb45d-6168-4c09-b7de-c13c0ec7ddcc.png" style="width:2.67em;height:1.00em;"/>. The Q-learning algorithm is very simple, which estimates <img class="fm-editor-equation" src="assets/1f073dd4-441f-4c3a-b5a3-79025c0a1641.png" style="width:3.08em;height:1.17em;"/> for each state-action pair via the following update rule:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation" src="assets/d6a07f4a-aab3-47ac-a2cd-37a192af0a27.png" style="width:29.58em;height:1.17em;"/></div>
<p class="mce-root"/>
<p>Here, <img class="fm-editor-equation" src="assets/46756389-7363-4b76-a9f9-dc9cd63afc31.png" style="width:1.25em;height:1.17em;"/> is the current state, <img class="fm-editor-equation" src="assets/7e3b8629-ecde-424d-bcab-aceae5e87687.png" style="width:2.08em;height:1.00em;"/> is the next state after taking action <img class="fm-editor-equation" src="assets/21ea2c3c-7fd9-4214-92e9-02031ef4d5b9.png" style="width:1.17em;height:1.00em;"/> at <img class="fm-editor-equation" src="assets/1f98e7be-c917-41e5-b561-0859edb01f46.png" style="width:0.67em;height:0.58em;"/>, <img class="fm-editor-equation" src="assets/4fd06b2f-e807-4fc6-99dd-ddd4eb34b03c.png" style="width:2.67em;height:0.92em;"/> is the set of the available actions at <img class="fm-editor-equation" src="assets/8cdc93c0-28ac-41b7-878e-2dae3b464e3e.png" style="width:1.92em;height:0.92em;"/>, is the discount factor, and <img class="fm-editor-equation" src="assets/0b053543-d0c4-485f-98eb-db30fd849896.png" style="width:1.00em;height:1.00em;"/> is the learning rate. The discount factor <img class="fm-editor-equation" src="assets/2f3f8ed1-0f61-4f7d-8112-f7763323f680.png" style="width:0.83em;height:1.25em;"/> lies in [0,1]. A discount factor smaller than 1 means that our agent prefers the current reward more than past rewards.</p>
<p>In the beginning, our agent knows nothing about the value function, so <img class="fm-editor-equation" src="assets/25114801-7fc6-40bd-b295-c57bc1ea4aa5.png" style="width:2.92em;height:1.17em;"/> is initialized to 0 for all state-action pairs. She will explore from state to state until she reaches the goal. We call each exploration an episode, which consists of moving from the initial state (for example, Room 1) to the final state (for example, Room 6). The Q-learning algorithm is shown as follows:</p>
<pre>Initialize <img class="fm-editor-equation" src="assets/1a3d0ae4-37e6-4e46-b7fc-e7103cc0f96d.png" style="width:0.50em;height:0.67em;"/> to zero and set parameters <img class="fm-editor-equation" src="assets/b691f74a-b1fe-4086-bb1c-05863a659ee3.png" style="width:0.67em;height:0.67em;"/>,<img class="fm-editor-equation" src="assets/4587a979-db3f-454e-930d-8c206bea5d14.png" style="width:0.42em;height:0.58em;"/>;<br/>Repeat for each episode:<br/>   Randomly select an initial state <img class="fm-editor-equation" src="assets/42397a58-6f26-4f01-b18d-3e5f9987b1dc.png" style="width:0.50em;height:0.67em;"/>;<br/>   While the goal state hasn't been reached:<br/>       Select action <img class="fm-editor-equation" src="assets/730eb985-d0b9-46bb-b457-67b909327912.png" style="width:0.58em;height:0.67em;"/> among all the possible actions in state <img class="fm-editor-equation" src="assets/98989ce7-6c51-46bf-a674-2a7402342554.png" style="width:0.42em;height:0.50em;"/> (e.g., using <img class="fm-editor-equation" src="assets/ac4fa48e-242f-4ca1-a5e1-817b98b2f577.png" style="width:1.50em;height:1.00em;"/>greedy);<br/>       Take action <img class="fm-editor-equation" src="assets/62dcd22b-f6cc-467b-9753-f08e80ea95a2.png" style="width:0.42em;height:0.50em;"/> and observe reward <img class="fm-editor-equation" src="assets/2c26f74e-671a-47e2-a505-d7482e49cc3d.png" style="width:2.00em;height:0.75em;"/>, next state <img class="fm-editor-equation" src="assets/a0262c78-620a-4d79-bcb9-3ba05adc234e.png" style="width:0.67em;height:0.83em;"/>;<br/>       Update <img class="fm-editor-equation" src="assets/f0a63b03-24ce-4b1f-83ea-ac1c046b2d8f.png" style="width:22.42em;height:1.08em;"/>;<br/>       Set the current state <img class="fm-editor-equation" src="assets/65a2b9fb-e7e8-4c9e-a8a6-e30051e60951.png" style="width:2.42em;height:0.92em;"/>;<br/>   End while</pre>
<p>A careful reader may ask a question about how to select action <img class="fm-editor-equation" src="assets/4cb8f468-d816-409c-a988-c026494017a1.png" style="width:0.83em;height:1.00em;"/> in state <img class="fm-editor-equation" src="assets/6472d012-4927-484b-abb1-fbae05294eea.png" style="width:0.75em;height:1.00em;"/>, for example, is action <img class="fm-editor-equation" src="assets/69c74860-fd8d-4d7e-a71b-13eb0e694079.png" style="width:0.83em;height:1.00em;"/> randomly selected among all the possible actions or chosen using the policy derived from the current estimated value function, <img class="fm-editor-equation" src="assets/62e69dcf-841b-492e-a49d-2c200bda5be8.png" style="width:0.92em;height:1.25em;"/>? What is <img class="fm-editor-equation" src="assets/996156af-478d-4a3e-b683-28051be43f3c.png" style="width:1.83em;height:1.25em;"/>greedy? These questions are related to two important concepts, namely, exploration and exploitation. Exploration means trying something new to gather more information about the environment, while exploitation means making the best decision based on all the information you have. For example, trying a new restaurant is exploration and going to your favorite restaurant is exploitation. In our maze problem, the exploration is that our agent tries to enter a new room she hasn't visited before, while the exploitation is that she chooses her favorite room based on the information she gathered from the environment.</p>
<p>Both exploration and exploitation are necessary in reinforcement learning. Without exploration, our agent is not able to get new knowledge about the environment, so she will make bad decisions again and again. Without exploitation, the information she got from exploration becomes meaningless since she doesn't learn from it to better make a decision. Therefore, a balance or a trade-off between exploration and exploitation is indispensable. <img class="fm-editor-equation" src="assets/a19c9030-f51d-47c0-b48f-d23865dad042.png" style="width:1.83em;height:1.25em;"/>greedy is the simplest way to make such a trade-off:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p>With probability</p>
</td>
<td>
<p>Randomly select an action among all the possible actions</p>
</td>
</tr>
<tr>
<td>
<p>With probability <img class="fm-editor-equation" src="assets/dbc78da5-ebe1-41d6-85ab-4b87ab72140c.png" style="width:2.50em;height:1.00em;"/></p>
</td>
<td>
<p>Select the best action based on <img class="fm-editor-equation" src="assets/172b2c80-720c-47f7-8788-ae4c1a1f2bc3.png" style="width:0.92em;height:1.25em;"/>,that is, pick so that <img class="fm-editor-equation" src="assets/d7ff4c51-1c70-4dee-9b4a-4db1f51f9b18.png" style="width:3.58em;height:1.33em;"/> is the largest among all the possible actions in state <img class="fm-editor-equation" src="assets/1c87ec34-7ab7-4140-825a-f568094ad8e2.png" style="width:0.75em;height:1.00em;"/></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>To further understand how Q-learning works, let's go through several steps by hand. For clarity, let's set the learning rate <img class="fm-editor-equation" src="assets/93d46ebc-a126-41e8-9278-35e55cc2b8d3.png" style="width:3.08em;height:1.00em;"/> and discount factor <img class="fm-editor-equation" src="assets/9837b06c-85ec-4df6-b321-95c61facad22.png" style="width:3.58em;height:1.17em;"/>. The following code shows the implementation of Q-learning in Python:</p>
<pre>import random, numpy<br/><br/>def Q_learning_demo():<br/>    <br/>    alpha = 1.0<br/>    gamma = 0.8<br/>    epsilon = 0.2<br/>    num_episodes = 100<br/>    <br/>    R = numpy.array([<br/>        [-1, 0, -1, -1, -1, -1],<br/>        [ 0, -1, 0, -1, 0, -1],<br/>        [-1, 0, -1, -50, -1, -1],<br/>        [-1, -1, 0, -1, -1, -1],<br/>        [-1, 0, -1, -1, -1, 100],<br/>        [-1, -1, -1, -1, -1, -1]<br/>        ])<br/>    # Initialize Q<br/>    Q = numpy.zeros((6, 6))<br/>    # Run for each episode<br/>    for _ in range(num_episodes):<br/>        # Randomly choose an initial state<br/>        s = numpy.random.choice(5)<br/>        while s != 5:<br/>            # Get all the possible actions<br/>            actions = [a for a in range(6) if R[s][a] != -1]<br/>            # Epsilon-greedy<br/>            if numpy.random.binomial(1, epsilon) == 1:<br/>                a = random.choice(actions)<br/>            else:<br/>                a = actions[numpy.argmax(Q[s][actions])]<br/>            next_state = a<br/>            # Update Q(s,a)<br/>            Q[s][a] += alpha * (R[s][a] + gamma * numpy.max(Q[next_state]) - Q[s][a])<br/>            # Go to the next state<br/>            s = next_state<br/>    return Q</pre>
<p>After running for 100 episodes, the value function, <img class="fm-editor-equation" src="assets/61445d4b-2ad1-4162-ad47-dc6144988430.png" style="width:0.92em;height:1.25em;"/>, converges to the following(for the readers who are curious about why this algorithm converges, refer to <em>Reinforcement Learning: An Introduction</em> by Andrew Barto and Richard S. Sutton):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>state\action</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>1</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>2</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>3</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>4</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>5</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>6</strong></div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">1</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">64</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">2</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">51.2</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">51.2</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">80</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">3</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">64</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-9.04</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">4</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">51.2</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">5</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">64</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">100</div>
</td>
</tr>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign">6</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign">-</div>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Therefore, the resulting state diagram becomes this:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-622 image-border" src="assets/b7faa768-8c30-4890-9abf-eeca5a176ca0.png" style="width:20.75em;height:8.92em;"/></div>
<p>This indicates the following optimal paths to the goal state for all the other states:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-623 image-border" src="assets/8beda865-6339-4c70-b8a2-7fd7c8104adf.png" style="width:22.50em;height:9.67em;"/></div>
<p>Based on this knowledge, our agent is able to go back home no matter which room she is in. More importantly, she becomes smarter and happier, achieving our goal to train a smart AI agent or player.</p>
<p>This simplest Q-learning algorithm can only handle discrete states and actions. For continuous states, it fails because the convergence is not guaranteed due to the existence of infinite states. How can we apply Q-learning in an infinite state space such as Atari games? The answer is replacing the tableau with neural networks to approximate the action-value function <img class="fm-editor-equation" src="assets/95e1fcb0-c839-457f-b9cd-f05c1bd9b255.png" style="width:3.08em;height:1.17em;"/>. This is the intuition behind the <em>Playing Atari with deep reinforcement learning,</em> by Google DeepMind paper.</p>
<p>To extend the basic Q-learning algorithm into the deep Q-learning algorithm, there are two key questions that need to be answered:</p>
<ol>
<li>What kind of neural networks can be used to extract high-level features from observed data such as screen images in the Atari environment?</li>
<li>How can we update the action-value function, <img class="fm-editor-equation" src="assets/38244a64-bc38-4734-8b1f-48b3af6f641f.png" style="width:2.92em;height:1.08em;"/>, at each training step?</li>
</ol>
<p>For the first question, there are several possible ways of approximating the action-value function, <img class="fm-editor-equation" src="assets/ba2ff8d1-d8b1-42d2-a019-2eef187c1257.png" style="width:3.33em;height:1.25em;"/>. One approach is that both the state and the action are used as the inputs to the neural network, which outputs the scalar estimates of their Q-value, as shown in the following <span>diagram</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-624 image-border" src="assets/7d9f4c83-e398-4c2a-982a-d26f7b745e53.png" style="width:10.83em;height:8.33em;"/></div>
<p>The main disadvantage of this approach is that an additional forward pass is required to compute <img class="fm-editor-equation" src="assets/078a4917-00a8-4cbe-b266-891afc82f98a.png" style="width:3.08em;height:1.17em;"/> as the action is taken as one of the inputs to the network, resulting in a cost that scales linearly with the number of all the possible actions. Another approach is that the state is the only input to the neural network, while there is a separate output for each possible action:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-625 image-border" src="assets/aa2fd706-8f54-4775-b55f-acd13be9d66b.png" style="width:11.08em;height:8.83em;"/></div>
<p>The main advantage of this approach is the ability to compute Q-values for all possible actions in a given state with only a single forward pass through the network, and the simplicity to access the Q-value for an action by picking the corresponding output head.</p>
<p>In the deep Q-network, the second architecture is applied. Recall that the output in the data preprocessing step is an <img class="fm-editor-equation" src="assets/99b1f14f-8ad5-49e5-aca9-58720f322382.png" style="width:4.08em;height:1.08em;"/> grayscale frame image. However, the current screen is not enough for playing Atari games because it doesn't contain the dynamic information about game status. Take Breakout as an example; if we only see one frame, we can only know the locations of the ball and the paddle, but we cannot know the direction or the velocity of the ball. Actually, the direction and the velocity are quite important for making decisions about how to move the paddle. Without them, the game is unplayable. Therefore, instead of taking only one frame image as the input, the last four frame images of a history are stacked together to produce the input to the neural network. These four frames form an <img class="fm-editor-equation" src="assets/47201bd5-f4f3-43b5-bded-eef0f697882e.png" style="width:6.25em;height:1.08em;"/> image. Besides the input layer, the Q-network contains three convolutional layers and one fully connected layer, which is  shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-626 image-border" src="assets/b942fdc1-a948-431a-9cae-d435343282b0.png" style="width:27.42em;height:16.83em;"/></div>
<p>The first convolutional layer has 64 <img class="fm-editor-equation" src="assets/ed5987c9-3441-4a2d-b1b4-0057f5970083.png" style="width:2.75em;height:1.00em;"/> filters with stride 4, followed by a <strong>rectifier nonlinearity</strong> (<strong>RELU</strong>). The second convolutional layer has 64 <img class="fm-editor-equation" src="assets/8594e75f-261f-4df0-9826-e7cf9ca7bdcb.png" style="width:3.42em;height:1.33em;"/> filters with stride 2, followed by RELU. The third convolutional layer has 64 <img class="fm-editor-equation" src="assets/6d194e05-96a0-4f1f-a044-322cf4760b64.png" style="width:2.25em;height:0.83em;"/> filters with stride 2, followed by RELU. The fully connected hidden layer has 512 hidden units, again followed by RELU. The output layer is also a fully connected layer with a single output for each action.</p>
<p>Readers who are familiar with convolutional neural networks may ask why the first convolutional layer uses a <img class="fm-editor-equation" src="assets/bdc4b421-2e99-4d82-a67c-543baa012ebf.png" style="width:2.75em;height:1.00em;"/> filter, instead of a <img class="fm-editor-equation" src="assets/f7b07854-81ff-47a0-8555-a3c3dc499e66.png" style="width:2.75em;height:1.00em;"/> filter or a <img class="fm-editor-equation" src="assets/030bd2b9-7e5f-455c-9a76-72f3cb6c1c26.png" style="width:2.25em;height:0.83em;"/> filter that is widely applied in computer vision applications. The main reason of using a big filter is that Atari games usually contain very small objects such as a ball, a bullet, or a pellet. A convolutional layer with larger filters is able to zoom in on these small objects, providing benefits for learning feature representations of states. For the second and third convolutional layers, a relatively small filter is enough to capture useful features.</p>
<p>So far, we have discussed the architecture of the Q-network. But, how do we train this Q-network in the Atari environment with an infinite state space? Is it possible to develop an algorithm based on the basic Q-learning to train it? Fortunately, the answer is YES. Recall that the update rule for <img class="fm-editor-equation" src="assets/f85e009b-d049-4650-ba9e-1263cc12b425.png" style="width:3.08em;height:1.17em;"/> in  basic Q-learning is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c1e957d0-cd19-47d2-b5f5-3191f8d8a01c.png" style="width:33.00em;height:1.58em;"/></div>
<p>When the learning rate <img class="fm-editor-equation" src="assets/898498da-07ed-4c98-bfd1-6a7e3b3cbad2.png" style="width:2.58em;height:0.83em;"/>, this update rule becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/68f68524-574b-45f6-a65f-6e6192463730.png" style="width:19.92em;height:1.58em;"/></div>
<p>This is called the <strong>Bellman equation</strong>. Actually, the Bellman equation is the backbone of many reinforcement learning algorithms. The algorithms using the Bellman equation as an iterative update are called value iteration algorithms. In this book, we will not go into  detail about value iteration or policy iteration. If you are interested in them, refer to <em>Reinforcement Learning: An Introduction,</em> by Andrew Barto and Richard S. Sutton.</p>
<p>The equation just shown is only suitable for a deterministic environment where the next state <img class="fm-editor-equation" src="assets/ebe2a0c8-a332-4a3f-9fe2-8e72203d3dd6.png" style="width:1.17em;height:1.50em;"/> is fixed given the current state <img class="fm-editor-equation" src="assets/8fb40cc4-2a7a-4e08-8db1-8ee4b723133a.png" style="width:0.75em;height:1.00em;"/> and the action <img class="fm-editor-equation" src="assets/f4ad75de-a4fe-42a7-95b4-78c0a40e42a6.png" style="width:0.83em;height:1.00em;"/>. In a nondeterministic environment, the Bellman equation should be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c35d79b3-5e13-4c9f-940f-797b4f83d451.png" style="width:25.42em;height:1.58em;"/></div>
<p>Here, the right-hand side takes the expectation of <img class="fm-editor-equation" src="assets/af3e312f-7d10-4d3f-a22a-3fe178d92f04.png" style="width:11.42em;height:1.17em;"/> with respect to the next state <img class="fm-editor-equation" src="assets/3e11b272-3878-432a-ba38-a196a97e28e3.png" style="width:1.17em;height:1.50em;"/> (for example, the distribution of <img class="fm-editor-equation" src="assets/4cea44a6-e9d9-460d-b93b-b7d30189f66b.png" style="width:1.17em;height:1.50em;"/> is determined by the Atari emulator). For an infinite state space, it is common to use a function approximator such as the Q-network to estimate the action-value function <img class="fm-editor-equation" src="assets/42b48d0e-14b8-46e4-b9c4-be0abead3f0d.png" style="width:3.08em;height:1.17em;"/>. Then, instead of iteratively updating <img class="fm-editor-equation" src="assets/06dc14b7-65d8-45cb-a20f-dbbe702cb163.png" style="width:3.08em;height:1.17em;"/>, the Q-network can be trained by minimizing the following loss function at the <em>i</em><sup>th</sup> iteration:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/61ea8633-6814-45b2-aa03-2dc4d7421f53.png" style="width:19.75em;height:1.75em;"/></div>
<p>Here, <em>Q(s,a;)</em> represents the Q-network parameterized by, <img class="fm-editor-equation" src="assets/6f25c491-93b4-47e4-b64e-ac207c62e209.png" style="width:22.33em;height:1.33em;"/> is the target for the i<sup>th</sup> iteration, and <img class="fm-editor-equation" src="assets/acba632a-dfff-4842-a8aa-32899fa0be04.png" style="width:3.25em;height:1.25em;"/> is a probability distribution over sequences and actions. The parameters from the previous iteration <em>i-1</em> are fixed when optimizing the loss function <img class="fm-editor-equation" src="assets/2fd194bc-effd-41fc-87aa-0aef0063524b.png" style="width:2.08em;height:0.92em;"/> over <img class="fm-editor-equation" src="assets/c40e80b6-15d7-4440-85eb-c61a1d535023.png" style="width:0.92em;height:1.17em;"/>. In practice, it is impossible to exactly calculate the expectations in <img class="fm-editor-equation" src="assets/899b8f6b-bdc7-4c4a-bdbe-e6aa278babbd.png" style="width:2.17em;height:0.92em;"/>. Instead of optimizing <img class="fm-editor-equation" src="assets/1ae281d7-5523-445f-afaf-69e80f91703b.png" style="width:3.08em;height:1.33em;"/> directly, we minimize the empirical loss of <img class="fm-editor-equation" src="assets/fdb4b635-9045-4ce5-a410-ceb03c5644da.png" style="width:2.50em;height:1.08em;"/>, which replaces the expectations by samples <img class="fm-editor-equation" src="assets/2cd96ade-1021-4337-af9b-8ab6e8f99a8d.png" style="width:5.50em;height:1.08em;"/> from the probability distribution <img class="fm-editor-equation" src="assets/e4001b8d-2541-49a0-a2b9-1a4003a0d067.png" style="width:3.08em;height:1.17em;"/> and the Atari emulator. As with other deep learning algorithms, the empirical loss function can be optimized by stochastic gradient descent.</p>
<p>This algorithm doesn't need to construct an estimate of the emulator, for example, it doesn't need to know the internal game mechanism about the Atari emulator, because it only uses samples from the emulator to solve the reinforcement learning problem. This property is called <strong>model-free</strong>, namely, it can treat the underlying model as a black box. Another property of this algorithm is off-policy. It learns about the greedy policy <img class="fm-editor-equation" src="assets/9a9692db-dc07-4a3d-be2a-87472934c71d.png" style="width:10.25em;height:1.83em;"/> while following the probability distribution <img class="fm-editor-equation" src="assets/127e4a41-2c25-4614-91bd-00e1c3d60cf4.png" style="width:3.25em;height:1.25em;"/> that balances exploration and exploitation of the state space. As discussed previously, <img class="fm-editor-equation" src="assets/84a647b9-5a68-495a-bf24-7993adaac0ad.png" style="width:2.67em;height:1.00em;"/> can be selected as an <img class="fm-editor-equation" src="assets/0a4f00c0-58d2-4cee-a24e-6fc913e2b9c7.png" style="width:1.50em;height:1.00em;"/>greedy strategy.</p>
<p>The derivation of the deep Q-learning algorithm may be a little bit difficult for readers who are not familiar with reinforcement learning or the Markov decision process. In order to make it more understandable, let's see the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-757 image-border" src="assets/3b7b51a9-94e8-402b-8f3c-381677f65707.png" style="width:27.33em;height:21.42em;"/></div>
<p>The brain of our AI player is the Q-network controller. At each time step t, she observes the screen image <img class="fm-editor-equation" src="assets/d3b118a9-7148-4a00-bf27-891876015028.png" style="width:1.00em;height:0.92em;"/> (recall that st is an <img class="fm-editor-equation" src="assets/c1550c24-4a86-4411-bca9-7e8c4ed1ef98.png" style="width:4.83em;height:0.83em;"/> image that stacks the last four frames). Then, her brain analyzes this observation and comes up with an action, <img class="fm-editor-equation" src="assets/669306ba-08b7-442e-903a-164890128a34.png" style="width:1.33em;height:1.17em;"/>. The Atari emulator receives this action and returns the next screen image, <img class="fm-editor-equation" src="assets/9a8cf78c-56e3-457a-b1af-3c0f1920b3b2.png" style="width:2.33em;height:1.08em;"/>, and the reward, <img class="fm-editor-equation" src="assets/6395f46a-cf4f-488b-9ae2-4b9c536374e1.png" style="width:1.00em;height:0.92em;"/>. The quadruplet <img class="fm-editor-equation" src="assets/e3b66ad7-170d-4ee5-8713-db4be648c25a.png" style="width:6.58em;height:1.17em;"/> is stored in the memory and is taken as a sample for training the Q-network by minimizing the empirical loss function via stochastic gradient descent.</p>
<p>How do we draw samples from the quadruplets stored in the memory? One approach is that these samples, <img class="fm-editor-equation" src="assets/eb191497-6ef0-432c-89c5-e568b5e7056d.png" style="width:9.25em;height:1.17em;"/>, are drawn from our AI player's interactions with the environment, for example, samples <img class="fm-editor-equation" src="assets/a58a78de-c408-499e-b397-c6031a560ffb.png" style="width:36.83em;height:1.42em;"/> are used to train the Q-network. The main drawback of this approach is that the samples in one batch are strongly correlated. The strong correlation breaks the assumption that the samples for constructing the empirical loss function are independent, making the training procedure unstable and leading to bad performance:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-628 image-border" src="assets/720330a7-48d0-4f8c-933f-abb8ad89a26f.png" style="width:34.50em;height:17.25em;"/></div>
<p>The deep Q-learning algorithm applies another approach, utilizing a technique known as experience replay. The AI player's experiences at each time step <img class="fm-editor-equation" src="assets/4a7b4f5c-f375-4546-a0c1-dbed21263257.png" style="width:8.00em;height:1.42em;"/> are stored into the replay memory from which a batch of samples are randomly drawn in order to train the Q-network. Mathematically, we cannot guarantee the independence between the samples we drew. But practically, this approach can stabilize the training procedure and generate reasonable results:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-629 image-border" src="assets/616efae0-11e3-47a2-81e5-2497eb2c9e5d.png" style="width:33.92em;height:16.92em;"/></div>
<p>So far, we have discussed all the components in the deep Q-learning algorithm. The full algorithm is shown as follows:</p>
<pre class="mce-root">Initialize replay memory <img class="fm-editor-equation" src="assets/98659ad9-b5cd-47e2-a81f-182bcb5b248d.png" style="width:0.67em;height:0.75em;"/> to capacity <img class="fm-editor-equation" src="assets/152e7ef7-40c0-42fe-85dd-64b690a2ca1d.png" style="width:0.75em;height:0.67em;"/>;<br/>Initialize the Q-network <img class="fm-editor-equation" src="assets/ce7a9d1c-c4aa-494d-82f8-e1c18c8ee57f.png" style="width:3.25em;height:0.92em;"/> with random weights <img class="fm-editor-equation" src="assets/60f735a3-ed82-4803-8af2-718087198981.png" style="width:0.33em;height:0.58em;"/>;<br/>Repeat for each episode:<br/>    Set time step <img class="fm-editor-equation" src="assets/47a8eee1-ea02-4ca6-a883-7b27d5b9e97c.png" style="width:1.83em;height:0.67em;"/>;<br/>    Receive an initial screen image <img class="fm-editor-equation" src="assets/440c2ae1-4943-414f-bab0-d4ae9a651aab.png" style="width:1.25em;height:0.92em;"/> and do preprocessing <img class="fm-editor-equation" src="assets/af785d0a-2961-469d-953c-77d618281f5e.png" style="width:3.75em;height:0.92em;"/>;<br/>    While the terminal state hasn't been reached:<br/>        Select an action at via <img class="fm-editor-equation" src="assets/307a5a98-b765-479d-b4ca-d91f55b98da2.png" style="width:1.33em;height:0.92em;"/>greedy, i.e., select a random action with probability , otherwise select <img class="fm-editor-equation" src="assets/cf80770d-661f-48fb-bf8a-4b9f10375b72.png" style="width:10.42em;height:1.67em;"/>;<br/>        Execute action at in the emulator and observe reward <img class="fm-editor-equation" src="assets/706c902c-a5de-46c4-bd93-e261847b54eb.png" style="width:0.83em;height:0.75em;"/> and image <img class="fm-editor-equation" src="assets/99f131d2-a065-41d1-9d04-233297460553.png" style="width:2.08em;height:0.92em;"/>;<br/>        Set <img class="fm-editor-equation" src="assets/99f7758c-448d-44f2-9479-8ef4c3edb9c6.png" style="width:8.25em;height:1.25em;"/> and store transition <img class="fm-editor-equation" src="assets/e42df924-327e-4720-9ea5-3e2324a241f9.png" style="width:6.08em;height:1.08em;"/> into replay memory <img class="fm-editor-equation" src="assets/b6654d3b-05d9-40df-84ac-b34d4a7403d3.png" style="width:0.50em;height:0.58em;"/>;<br/>        Randomly sample a batch of transitions <img class="fm-editor-equation" src="assets/77c20a14-c6c8-4678-b14d-091ecc4bee3e.png" style="width:5.17em;height:0.92em;"/> from <img class="fm-editor-equation" src="assets/95f9e916-9832-4669-9042-9a4a2af00789.png" style="width:0.58em;height:0.67em;"/>;<br/>        Set <img class="fm-editor-equation" src="assets/55c84633-232f-4bc4-9b96-a3b8ae40d043.png" style="width:3.33em;height:0.92em;"/> if <img class="fm-editor-equation" src="assets/66c3397b-7b35-40d0-a229-61611c24b9ea.png" style="width:1.75em;height:0.83em;"/> is a terminal state or <img class="fm-editor-equation" src="assets/294a3370-0058-422e-b72f-cc0e889995c2.png" style="width:8.75em;height:1.17em;"/> if <img class="fm-editor-equation" src="assets/91e249ea-130d-442f-ac88-54e6a5cd5bae.png" style="width:1.58em;height:0.75em;"/> is a non-terminal state;<br/>        Perform a gradient descent step on <img class="fm-editor-equation" src="assets/db69d96d-4d24-4459-9a55-5147d0209474.png" style="width:5.17em;height:1.25em;"/>;<br/>    End while</pre>
<p>This algorithm works well for some Atari games, for example, Breakout, Seaquest, Pong, and Qbert, but it still cannot reach human-level control. One drawback is that computing the targets <img class="fm-editor-equation" src="assets/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png" style="width:0.92em;height:0.83em;"/> uses the current estimate of the action-value function <img class="fm-editor-equation" src="assets/fcf4a15e-0ffc-40f4-955a-0b733c40555e.png" style="width:0.67em;height:0.92em;"/>, which makes the training step unstable, that is, an update that increases <img class="fm-editor-equation" src="assets/011ebf97-d241-4fd7-a1da-ba8490f2e1a9.png" style="width:3.00em;height:1.00em;"/> usually also increases <img class="fm-editor-equation" src="assets/98fc7dbf-c711-494e-8187-55bd6369945c.png" style="width:3.75em;height:1.00em;"/> for all and hence also increases the target <img class="fm-editor-equation" src="assets/60bb25a5-2d55-444e-bfaf-8678c9ffeb6b.png" style="width:0.92em;height:0.83em;"/>, possibly leading to oscillations or divergence of the policy.</p>
<p>To address this problem, Google DeepMind introduced the target network in their paper, <em>Human-level control through deep reinforcement learning</em>, which was published in Nature. The idea behind the target network is quite simple: a separate network is used for generating the targets <img class="fm-editor-equation" src="assets/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png" style="width:0.92em;height:0.83em;"/> in the Q-learning update. More precisely, for every <img class="fm-editor-equation" src="assets/6fd13b9a-c296-40cb-b309-cf0f3b6c762a.png" style="width:1.08em;height:0.83em;"/> Q-learning updates, the network Q is cloned to obtain a target network Q, which is used for generating the targets <img class="fm-editor-equation" src="assets/b49f9246-7588-483e-a7c6-6b5b6318f825.png" style="width:1.17em;height:1.08em;"/> in the following <img class="fm-editor-equation" src="assets/1b664cc6-cc42-45fd-a8a3-4c671c2bb82b.png" style="width:1.25em;height:1.00em;"/> updates to Q. Therefore, the deep Q-learning algorithm becomes as follows:</p>
<pre class="mce-root">Initialize replay memory <img class="fm-editor-equation" src="assets/98659ad9-b5cd-47e2-a81f-182bcb5b248d.png" style="width:0.58em;height:0.67em;"/> to capacity <img class="fm-editor-equation" src="assets/152e7ef7-40c0-42fe-85dd-64b690a2ca1d.png" style="width:0.58em;height:0.50em;"/>;<br/>Initialize the Q-network <img class="fm-editor-equation" src="assets/9dae5080-1f93-46f9-805b-9388305709a8.png" style="width:2.67em;height:0.75em;"/> with random weights <img class="fm-editor-equation" src="assets/f7b04bcb-f2f6-4965-a7b8-e4e245ef1c08.png" style="width:0.25em;height:0.50em;"/>;<br/>Initialize the target network <img class="fm-editor-equation" src="assets/17e53430-2acc-4e43-a192-20416808517e.png" style="width:2.75em;height:0.92em;"/> with weights <img class="fm-editor-equation" src="assets/6ff4f25b-4572-4a39-b3d6-de197ce87fd0.png" style="width:1.58em;height:0.75em;"/>;<br/>Repeat for each episode:<br/> </pre>
<pre class="mce-root">Set time step <img class="fm-editor-equation" src="assets/9145e75b-1140-4e50-9121-7478f87bc53f.png" style="width:1.83em;height:0.67em;"/>;<br/>    Receive an initial screen image <img class="fm-editor-equation" src="assets/c1808792-a91d-4562-932b-e1cbee80ed48.png" style="width:1.17em;height:0.83em;"/> and do preprocessing <img class="fm-editor-equation" src="assets/da2569c0-b32d-4ca3-877b-2e1c5c20e7e3.png" style="width:4.42em;height:1.08em;"/>;<br/>    While the terminal state hasn't been reached:<br/>        Select an action at via <img class="fm-editor-equation" src="assets/307a5a98-b765-479d-b4ca-d91f55b98da2.png" style="width:1.33em;height:0.92em;"/>greedy, i.e., select a random action with probability , otherwise select <img class="fm-editor-equation" src="assets/fcb2413a-bcea-427a-bd0a-cec88ac0181d.png" style="width:9.92em;height:1.58em;"/>;<br/>        Execute action at in the emulator and observe reward <img class="fm-editor-equation" src="assets/8404e2b2-2e3a-4905-a1c1-4e776bf9e730.png" style="width:0.83em;height:0.75em;"/> and image <img class="fm-editor-equation" src="assets/3da6850f-ca90-4a88-96a7-cdff0d216e77.png" style="width:1.67em;height:0.75em;"/>;<br/>        Set <img class="fm-editor-equation" src="assets/a2bb6d1d-8832-4452-84b5-759797b33549.png" style="width:7.17em;height:1.08em;"/> and store transition <img class="fm-editor-equation" src="assets/5843521e-69e2-4d38-b685-ee59a0c255f7.png" style="width:6.08em;height:1.08em;"/> into replay memory <img class="fm-editor-equation" src="assets/3c7b154b-5b19-4245-848b-815dd645952f.png" style="width:0.42em;height:0.50em;"/>;<br/>        Randomly sample a batch of transitions <img class="fm-editor-equation" src="assets/910999fa-fcdb-44cb-94f0-8734e361bc26.png" style="width:7.50em;height:1.42em;"/> from <img class="fm-editor-equation" src="assets/f3fe7f1c-f955-471a-9603-55ef41e5bed3.png" style="width:0.42em;height:0.50em;"/>;<br/>        Set <img class="fm-editor-equation" src="assets/00139bdf-fe9f-4946-9d55-abc69f3f46bd.png" style="width:2.75em;height:0.75em;"/> if <img class="fm-editor-equation" src="assets/2613e180-21f4-4b73-9948-dffee846a99f.png" style="width:1.92em;height:0.92em;"/> is a terminal state or <img class="fm-editor-equation" src="assets/7ebe2f15-88a8-43ad-ad77-52015835fa58.png" style="width:9.42em;height:1.42em;"/> if <img class="fm-editor-equation" src="assets/2d4153b3-f3cc-4193-880f-3fdde4dc9d38.png" style="width:1.92em;height:0.92em;"/> is a non-terminal state;<br/>        Perform a gradient descent step on <img class="fm-editor-equation" src="assets/7fcf4907-750c-470a-a080-f99b2c45bd58.png" style="width:6.33em;height:1.50em;"/>;</pre>
<pre class="mce-root"><br/>        Set <img class="fm-editor-equation" src="assets/a1495177-52a3-4b6c-ad86-0fac8419fefc.png" style="width:1.67em;height:0.75em;"/> for every <img class="fm-editor-equation" src="assets/f0b53bc2-3f04-43c0-a86f-29d1633c4295.png" style="width:1.17em;height:0.92em;"/> steps;<br/> End while</pre>
<p>With the target network, the AI player trained by the deep Q-learning algorithm is able to surpass the performance of most previous reinforcement learning algorithms and achieves a human-level performance across a set of 49 Atari 2600 games, for example, Star Gunner, Atlantis, Assault, and Space Invaders.</p>
<p>The deep Q-learning algorithm has made an important step toward general artificial intelligence. Although it performs well in the Atari 2600 games, it still has a lot of unsolved issues:</p>
<ul>
<li><strong>Slow convergence</strong>: It requires a long time (7 days on one GPU) to reach human-level performance</li>
<li><strong>Failing with sparse rewards</strong>: It doesn't work for the game Montezuma's Revenge, which requires long-term planning</li>
<li><strong>Need for a large amount of data</strong>: This is a common issue among most reinforcement learning algorithms</li>
</ul>
<p>In order to solve these issues, different variants of the deep Q-learning algorithm have been proposed recently, for example, double Q-learning, prioritized experience replay, bootstrapped DQN, and dueling network architectures. We will not discuss these algorithms in this book. For readers who want to learn more about DQN, please refer to the related papers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of DQN</h1>
                </header>
            
            <article>
                
<p>This chapter will show you how to implement all the components, for example, Q-network, replay memory, trainer, and Q-learning optimizer, of the deep Q-learning algorithm with Python and TensorFlow.</p>
<p>We will  implement the <kbd>QNetwork</kbd> class for the Q-network that we discussed in the previous chapter, which is defined as follows:</p>
<pre>class QNetwork:<br/>    <br/>    def __init__(self, input_shape=(84, 84, 4), n_outputs=4, <br/>                 network_type='cnn', scope='q_network'):<br/>        <br/>        self.width = input_shape[0]<br/>        self.height = input_shape[1]<br/>        self.channel = input_shape[2]<br/>        self.n_outputs = n_outputs<br/>        self.network_type = network_type<br/>        self.scope = scope<br/>        <br/>        # Frame images<br/>        self.x = tf.placeholder(dtype=tf.float32, <br/>                                shape=(None, self.channel, <br/>                                       self.width, self.height))<br/>        # Estimates of Q-value<br/>        self.y = tf.placeholder(dtype=tf.float32, shape=(None,))<br/>        # Selected actions<br/>        self.a = tf.placeholder(dtype=tf.int32, shape=(None,))<br/>        <br/>        with tf.variable_scope(scope):<br/>            self.build()<br/>            self.build_loss()</pre>
<p>The constructor requires four arguments, <kbd>input_shape</kbd>, <kbd>n_outputs</kbd>, <kbd>network_type</kbd> and <kbd>scope</kbd>. <kbd>input_shape</kbd> is the size of the input image. After data preprocessing, the input is an <img class="fm-editor-equation" src="assets/11c9512b-c3c6-4a42-830f-8d17d1475355.png" style="width:5.25em;height:0.92em;"/> image, so that the default parameter is <img class="fm-editor-equation" src="assets/90bd34c3-690d-4b8b-b708-caecfce0be56.png" style="width:4.67em;height:1.25em;"/>. <kbd>n_outputs</kbd> is the number of all the possible actions, for example, <kbd>n_outputs</kbd> is four in Breakout. <kbd>network_type</kbd> ,indicates the type of the Q-network we want to use. Our implementation contains three different networks. Two of them are the convolutional neural networks proposed by Google DeepMind. The other one is a feed-forward neural network used for testing. <kbd>scope</kbd> is the name of the Q-network object, which can be set to <kbd>q_network</kbd> or <kbd>target_network</kbd>.</p>
<p>In the constructor, three input tensors are created. The <kbd>x</kbd> variable represents the input state (a batch of <img class="fm-editor-equation" src="assets/033fb901-89a8-47df-abd0-13017738e6ce.png" style="width:5.75em;height:1.00em;"/> images). The <kbd>y</kbd> and <kbd>a</kbd> variables are the estimates of the action-value function and the selected actions corresponding to the input state, which are used for training the Q-network. After creating the input tensors, two functions, <kbd>build</kbd> and <kbd>build_loss</kbd>, are called to build the Q-network.</p>
<p>Constructing the Q-network using TensorFlow is quite easy, as shown here:</p>
<pre>    def build(self):<br/>        <br/>        self.net = {}<br/>        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))<br/>        <br/>        init_b = tf.constant_initializer(0.01)<br/>        if self.network_type == 'cnn':<br/>            self.net['conv1'] = conv2d(self.net['input'], 32, <br/>                                       kernel=(8, 8), stride=(4, 4), <br/>                                       init_b=init_b, name='conv1')<br/>            self.net['conv2'] = conv2d(self.net['input'], 64, <br/>                                       kernel=(4, 4), stride=(2, 2), <br/>                                       init_b=init_b, name='conv2')<br/>            self.net['conv3'] = conv2d(self.net['input'], 64, <br/>                                       kernel=(3, 3), stride=(1, 1), <br/>                                       init_b=init_b, name='conv3')<br/>            self.net['feature'] = dense(self.net['conv2'], 512, <br/>                                        init_b=init_b, name='fc1')<br/>        elif self.network_type == 'cnn_nips':<br/>            self.net['conv1'] = conv2d(self.net['input'], 16, <br/>                                       kernel=(8, 8), stride=(4, 4), <br/>                                       init_b=init_b, name='conv1')<br/>            self.net['conv2'] = conv2d(self.net['conv1'], 32, <br/>                                       kernel=(4, 4), stride=(2, 2), <br/>                                       init_b=init_b, name='conv2')<br/>            self.net['feature'] = dense(self.net['conv2'], 256, <br/>                                        init_b=init_b, name='fc1')<br/>        elif self.network_type == 'mlp':<br/>            self.net['fc1'] = dense(self.net['input'], 50, <br/>                                    init_b=init_b), name='fc1')<br/>            self.net['feature'] = dense(self.net['fc1'], 50, <br/>                                        init_b=init_b, name='fc2')<br/>        else:<br/>            raise NotImplementedError('Unknown network type')<br/>            <br/>        self.net['values'] = dense(self.net['feature'], <br/>                                   self.n_outputs, activation=None,<br/>                                   init_b=init_b, name='values')<br/>        <br/>        self.net['q_value'] = tf.reduce_max(self.net['values'], <br/>                                            axis=1, name='q_value')<br/>        self.net['q_action'] = tf.argmax(self.net['values'], <br/>                                         axis=1, name='q_action', <br/>                                         output_type=tf.int32)<br/>        <br/>        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, <br/>                                      tf.get_variable_scope().name)</pre>
<p class="mce-root">As discussed in the previous chapter, the Q-network for the Atari environment contains three convolutional layers and one hidden layer, which can be constructed when the <kbd>network_type</kbd> is <kbd>cnn</kbd>. The <kbd>cnn_nips</kbd> type is a simplified Q-network for Atari games, which only contains two convolutional layers and one hidden layer with less filters and hidden units. The <kbd>mlp</kbd> type is a feed-forward neural network with two hidden layers, which is used for debugging. The <kbd>vars</kbd> variable is the list of all the trainable variables in the Q-network.</p>
<p class="mce-root">Recall that the loss function is <img class="fm-editor-equation" src="assets/311d8928-3b5a-4ad1-b94c-f07867e654c2.png" style="width:5.25em;height:1.25em;"/>, which can be implemented as follows:</p>
<pre>    def build_loss(self):<br/>        <br/>        indices = tf.transpose(tf.stack([tf.range(tf.shape(self.a)[0]), <br/>                                         self.a], axis=0))<br/>        value = tf.gather_nd(self.net['values'], indices)<br/>        <br/>        self.loss = 0.5 * tf.reduce_mean(tf.square((value - self.y)))<br/>        self.gradient = tf.gradients(self.loss, self.vars)<br/>        <br/>        tf.summary.scalar("loss", self.loss, collections=['q_network'])<br/>        self.summary_op = tf.summary.merge_all('q_network')</pre>
<p>The <kbd>tf.gather_nd</kbd> function is used to get the action-value <img class="fm-editor-equation" src="assets/a242dccc-2a1a-46cd-abbe-ab115852f00b.png" style="width:4.08em;height:1.00em;"/> given a batch of action,s ai. The variable loss represents the loss function, and gradient is the gradient of the loss function with respect to the trainable variables. <kbd>summary_op</kbd> is for TensorBoard visualization.</p>
<p class="mce-root"/>
<p>The implementation of the replay memory doesn't involve TensorFlow:</p>
<pre>class ReplayMemory:<br/>    <br/>    def __init__(self, history_len=4, capacity=1000000, <br/>                 batch_size=32, input_scale=255.0):<br/>        <br/>        self.capacity = capacity<br/>        self.history_length = history_len<br/>        self.batch_size = batch_size<br/>        self.input_scale = input_scale<br/>        <br/>        self.frames = deque([])<br/>        self.others = deque([])</pre>
<p>The <kbd>ReplayMemory</kbd> class takes four input parameters, that is, <kbd>history_len</kbd>, <kbd>capacity</kbd>, <kbd>batch_size</kbd>, and <kbd>input_scale</kbd>. <kbd>history_len</kbd> is the number of frames stacked together. Typically, <kbd>history_len</kbd> is set to 4 for Atari games, forming an <img class="fm-editor-equation" src="assets/1c32790d-8486-44c4-9e05-18ec2582983e.png" style="width:5.75em;height:1.00em;"/> input image. <kbd>capacity</kbd> is the capacity of the replay memory, namely, the maximum number of frames that can be stored in it. <kbd>batch_size</kbd> is the size of one batch for training. <kbd>input_scale</kbd> is the normalization factor for input images, for example, it is set to 255 for RGB images. The variable frames record all the frame images and the variable others record the corresponding actions, rewards, and termination signals.</p>
<p><kbd>ReplayMemory</kbd> provides a function for adding a record (frame image, action, reward, termination signal) into the memory:</p>
<pre>    def add(self, frame, action, r, termination):<br/>        <br/>        if len(self.frames) == self.capacity:<br/>            self.frames.popleft()<br/>            self.others.popleft()<br/>        self.frames.append(frame)<br/>        self.others.append((action, r, termination))<br/>        <br/>    def add_nullops(self, init_frame):<br/>        for _ in range(self.history_length):<br/>            self.add(init_frame, 0, 0, 0)</pre>
<p>It also provides a function for constructing an <img class="fm-editor-equation" src="assets/33f6663f-2d51-4cbb-87b4-fe3c25f809d6.png" style="width:5.25em;height:0.92em;"/> input image by concatenating the last four frame images of a history:</p>
<pre>    def phi(self, new_frame):<br/>        assert len(self.frames) &gt; self.history_length<br/>        images = [new_frame] + [self.frames[-1-i] for i in range(self.history_length-1)]<br/>        return numpy.concatenate(images, axis=0)</pre>
<p>The following function randomly draws a transition (state, action, reward, next state, termination signal) from the replay memory:</p>
<pre>    def sample(self):<br/>        <br/>        while True:<br/>            <br/>            index = random.randint(a=self.history_length-1, <br/>                                   b=len(self.frames)-2)<br/>            infos = [self.others[index-i] for i in range(self.history_length)]<br/>            # Check if termination=1 before "index"<br/>            flag = False<br/>            for i in range(1, self.history_length):<br/>                if infos[i][2] == 1:<br/>                    flag = True<br/>                    break<br/>            if flag:<br/>                continue<br/>            <br/>            state = self._phi(index)<br/>            new_state = self._phi(index+1)<br/>            action, r, termination = self.others[index]<br/>            state = numpy.asarray(state / self.input_scale, <br/>                                  dtype=numpy.float32)<br/>            new_state = numpy.asarray(new_state / self.input_scale, <br/>                                      dtype=numpy.float32)<br/>                <br/>            return (state, action, r, new_state, termination)</pre>
<p>Note that only the termination signal corresponding to the last frame in a state is allowed to be True. The <kbd>_phi(index)</kbd> function stacks the four frames together:</p>
<pre>    def _phi(self, index):<br/>        images = [self.frames[index-i] for i in range(self.history_length)]<br/>        return numpy.concatenate(images, axis=0)</pre>
<p>The <kbd>Optimizer</kbd> class is used for training the Q-network:</p>
<pre>class Optimizer:<br/>    <br/>    def __init__(self, config, feedback_size, <br/>                 q_network, target_network, replay_memory):<br/>        <br/>        self.feedback_size = feedback_size<br/>        self.q_network = q_network<br/>        self.target_network = target_network<br/>        self.replay_memory = replay_memory<br/>        self.summary_writer = None<br/>        <br/>        self.gamma = config['gamma']<br/>        self.num_frames = config['num_frames']<br/>        <br/>        optimizer = create_optimizer(config['optimizer'], <br/>                                     config['learning_rate'], <br/>                                     config['rho'], <br/>                                     config['rmsprop_epsilon'])<br/>        <br/>        self.train_op = optimizer.apply_gradients(<br/>                 zip(self.q_network.gradient, <br/>                 self.q_network.vars))</pre>
<p>It takes the Q-network, the target network, the replay memory, and the size of input images as the input arguments. In the constructor, it creates an optimizer (one of the popular optimizers such as ADAM, RMSPROP, or MOMENTUM) and then builds an operator for training.</p>
<p>To train the Q-network, it needs to construct a mini-batch of samples (states, actions, targets) corresponding to <img class="fm-editor-equation" src="assets/96f94be4-2663-4447-bdcb-0ad52ff91e8c.png" style="width:1.00em;height:0.92em;"/>, <img class="fm-editor-equation" src="assets/5a68c450-24b6-453a-b227-b89c5279dd64.png" style="width:0.92em;height:0.83em;"/>, and <img class="fm-editor-equation" src="assets/e23a5362-570b-4f5b-9f34-451d4bde8460.png" style="width:0.83em;height:0.75em;"/> in the loss function <img class="fm-editor-equation" src="assets/84bead4d-efad-4314-b78c-7cee54ccdfd2.png" style="width:5.75em;height:1.42em;"/>:</p>
<pre>    def sample_transitions(self, sess, batch_size):<br/>        <br/>        w, h = self.feedback_size<br/>        states = numpy.zeros((batch_size, self.num_frames, w, h), <br/>                             dtype=numpy.float32)<br/>        new_states = numpy.zeros((batch_size, self.num_frames, w, h), <br/>                                 dtype=numpy.float32)<br/>        targets = numpy.zeros(batch_size, dtype=numpy.float32)<br/>        actions = numpy.zeros(batch_size, dtype=numpy.int32)<br/>        terminations = numpy.zeros(batch_size, dtype=numpy.int32)<br/>        <br/>        for i in range(batch_size):<br/>            state, action, r, new_state, t = self.replay_memory.sample()<br/>            states[i] = state<br/>            new_states[i] = new_state<br/>            actions[i] = action<br/>            targets[i] = r<br/>            terminations[i] = t<br/><br/>        targets += self.gamma * (1 - terminations) * self.target_network.get_q_value(sess, new_states)<br/>        return states, actions, targets</pre>
<p>Note that the targets <img class="fm-editor-equation" src="assets/e23a5362-570b-4f5b-9f34-451d4bde8460.png" style="width:1.33em;height:1.25em;"/> are computed by the target network instead of the Q-network. Given a mini-batch of states, actions, or targets, the Q-network can be easily trained by use of the following:</p>
<pre>    def train_one_step(self, sess, step, batch_size):<br/>        <br/>        states, actions, targets = self.sample_transitions(sess, batch_size)<br/>        feed_dict = self.q_network.get_feed_dict(states, actions, targets)<br/>        <br/>        if self.summary_writer and step % 1000 == 0:<br/>            summary_str, _, = sess.run([self.q_network.summary_op, <br/>                                        self.train_op], <br/>                                       feed_dict=feed_dict)<br/>            self.summary_writer.add_summary(summary_str, step)<br/>            self.summary_writer.flush()<br/>        else:<br/>            sess.run(self.train_op, feed_dict=feed_dict)</pre>
<p>Besides the training procedure, for each <kbd>1000</kbd> steps, the summary is written to the log file. This summary is for monitoring the training process, helping to tune the parameters, and debugging.</p>
<p>Combining these modules together, we can implement the class DQN for the main deep Q-learning algorithm:</p>
<pre>class DQN:<br/>    <br/>    def __init__(self, config, game, directory, <br/>                 callback=None, summary_writer=None):<br/>        <br/>        self.game = game<br/>        self.actions = game.get_available_actions()<br/>        self.feedback_size = game.get_feedback_size()<br/>        self.callback = callback<br/>        self.summary_writer = summary_writer<br/>        <br/>        self.config = config<br/>        self.batch_size = config['batch_size']<br/>        self.n_episode = config['num_episode']<br/>        self.capacity = config['capacity']<br/>        self.epsilon_decay = config['epsilon_decay']<br/>        self.epsilon_min = config['epsilon_min']<br/>        self.num_frames = config['num_frames']<br/>        self.num_nullops = config['num_nullops']<br/>        self.time_between_two_copies = config['time_between_two_copies']<br/>        self.input_scale = config['input_scale']<br/>        self.update_interval = config['update_interval']<br/>        self.directory = directory<br/><br/>        self._init_modules()</pre>
<p>Here, <kbd>config</kbd> includes all the parameters of DQN, for example, batch size and learning rate for training. <kbd>game</kbd> is an instance of the Atari environment. In the constructor, the replay memory, Q-network, target network, and optimizer are initialized. To begin the training process, the following function can be called:</p>
<pre>    def train(self, sess, saver=None):<br/>        <br/>        num_of_trials = -1<br/>        for episode in range(self.n_episode):<br/>            self.game.reset()<br/>            frame = self.game.get_current_feedback()<br/>            for _ in range(self.num_nullops):<br/>                r, new_frame, termination = self.play(action=0)<br/>                self.replay_memory.add(frame, 0, r, termination)<br/>                frame = new_frame<br/>            <br/>            for _ in range(self.config['T']):<br/>                num_of_trials += 1<br/>                epsilon_greedy = self.epsilon_min + \<br/>                    max(self.epsilon_decay - num_of_trials, 0) / \<br/>                    self.epsilon_decay * (1 - self.epsilon_min)<br/><br/>                if num_of_trials % self.update_interval == 0:<br/>                    self.optimizer.train_one_step(sess, <br/>                                                  num_of_trials, <br/>                                                  self.batch_size)<br/>                <br/>                state = self.replay_memory.phi(frame)<br/>                action = self.choose_action(sess, state, epsilon_greedy) <br/>                r, new_frame, termination = self.play(action)<br/>                self.replay_memory.add(frame, action, r, termination)<br/>                frame = new_frame<br/>                <br/>                if num_of_trials % self.time_between_two_copies == 0:<br/>                    self.update_target_network(sess)<br/>                    self.save(sess, saver)<br/>                <br/>                if self.callback:<br/>                    self.callback()<br/>                if termination:<br/>                    score = self.game.get_total_reward()<br/>                    summary_str = sess.run(self.summary_op, <br/>                                           feed_dict={self.t_score: score})<br/>                    self.summary_writer.add_summary(summary_str, <br/>                                                    num_of_trials)<br/>                    self.summary_writer.flush()<br/>                    break</pre>
<p>This function is easy to understand. In each episode, it calls <kbd>replay_memory.phi</kbd> to get the current state and calls the <kbd>choose_action</kbd> function to select an action via the  <img class="fm-editor-equation" src="assets/ced55201-ffc9-4a33-a0a0-71ec35883db7.png" style="width:1.83em;height:1.25em;"/>greedy policy. This action is submitted into the Atari emulator by calling the <kbd>play</kbd> function, which returns the corresponding reward, next frame image, and termination signal. Then, the transition (current frame image, action, reward, termination) is stored in the replay memory. For every <kbd>update_interval</kbd> step (<kbd>update_interval = 1</kbd> by default), the Q-network is trained with a batch of transitions randomly sampled from the replay memory. For every <kbd>time_between_two_copies</kbd> step, the target network copies the Q-network, and the weights of the Q-network are saved to the hard disk.</p>
<p>After the training step, the following function can be called for evaluating the AI player's performance:</p>
<pre>    def evaluate(self, sess):<br/>        <br/>        for episode in range(self.n_episode):<br/>            self.game.reset()<br/>            frame = self.game.get_current_feedback()<br/>            for _ in range(self.num_nullops):<br/>                r, new_frame, termination = self.play(action=0)<br/>                self.replay_memory.add(frame, 0, r, termination)<br/>                frame = new_frame<br/>            <br/>            for _ in range(self.config['T']):<br/>                state = self.replay_memory.phi(frame)<br/>                action = self.choose_action(sess, state, self.epsilon_min) <br/>                r, new_frame, termination = self.play(action)<br/>                self.replay_memory.add(frame, action, r, termination)<br/>                frame = new_frame<br/><br/>                if self.callback:<br/>                    self.callback()<br/>                    if termination:<br/>                        break</pre>
<p>Now, we are ready to train our first AI player for Atari games. The implementation is not hard if you understand the intuition behind the algorithm, is it? Now is the time to run the program and witness the magic!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experiments</h1>
                </header>
            
            <article>
                
<p>The full implementation of the deep Q-learning algorithm can be downloaded from GitHub (link xxx). To train our AI player for Breakout, run the following command under the <kbd>src</kbd> folder:</p>
<pre><strong>python train.py -g Breakout -d gpu</strong></pre>
<p>There are two arguments in <kbd>train.py</kbd>. One is <kbd>-g</kbd> or <kbd>--game</kbd>, indicating the name of the game one wants to test. The other one is <kbd>-d</kbd> or <kbd>--device</kbd>, which specifies the device (CPU or GPU) one wants to use to train the Q-network.</p>
<p>For Atari games, even with a high-end GPU, it will take 4-7 days to make our AI player achieve human-level performance. In order to test the algorithm quickly, a special game called <strong>demo</strong> is implemented as a lightweight benchmark. Run the demo via the following:</p>
<pre><strong>python train.py -g demo -d cpu</strong></pre>
<p class="mce-root"/>
<p>The demo game is based on the GridWorld game on the website at <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html</a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-630 image-border" src="assets/3bd4e47a-d8d9-4fe3-98f9-b5d936dba390.png" style="width:37.17em;height:29.42em;"/></div>
<p>In this game, a robot in a 2D grid world has nine eyes pointing in different angles, and each eye senses three values along its direction: distance to a wall, distance to a green <span><span>bean</span></span>, or distance to a red <span><span>bean</span></span>. It navigates by using one of five actions that turn it different angles. It gets a positive reward (+1) for eating green beans while a negative reward (-1) for eating red beans. The goal is to eat green beans as much as possible in one episode.</p>
<p>The training will take several minutes. During the training, you can open a new terminal and type the following command to visualize the architecture of the Q-network and the training procedure:</p>
<pre><strong>tensorboard --logdir=log/demo/train</strong></pre>
<p>Here, <kbd>logdir</kbd> points to the folder where the log file of demo is stored. Once TensorBoard is running, navigate your web browser to <kbd>localhost:6006</kbd> to view the TensorBoard:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-711 image-border" src="assets/aedc4c3e-6dd9-40e4-b072-f1a3b1a409fa.png" style="width:57.50em;height:17.75em;"/></div>
<p>The two graphs plot the loss and the score against the training step, respectively. Clearly, after 100k training steps, the performance of the robot becomes stable, for example, the score is around 40.</p>
<p>You can also visualize the weights of the Q-network through TensorBoard. For more details, visit the TensorBoard guide at <a href="https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard">https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard</a>. This tool is quite useful for debugging and optimizing the code, especially for complicated algorithms such as DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Congratulations! You just learned four important things. The first one is how to implement an Atari game emulator using gym, and how to play Atari games for relaxation and having fun. The second one is that you learned how to preprocess data in reinforcement learning tasks such as Atari games. For practical machine learning applications, you will spend a great deal of time on understanding and refining data, which affects the performance of an AI system a lot. The third one is the deep Q-learning algorithm. You learned the intuition behind it, for example, why the replay memory is necessary, why the target network is needed, where the update rule comes from, and so on. The final one is that you learned how to implement DQN using TensorFlow, and how to visualize the training process. Now, you are ready for the more advanced topics that we will discuss in the following chapters.</p>
<p>In the next chapter, you will learn how to simulate classic control tasks, and how to implement the state-of-the-art actor-critic algorithms for control.</p>


            </article>

            
        </section>
    </body></html>