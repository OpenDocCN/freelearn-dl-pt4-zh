<html><head></head><body>
		<div id="_idContainer014">
			<h1 id="_idParaDest-108" class="chapter-number"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.2.1">Using Your Own LLM and Prompts as Guidelines</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">In the dynamic realm of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.5.1">, the possibilities are vast and ever-evolving. </span><span class="koboSpan" id="kobo.5.2">While </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.6.1">uncovering the capabilities of Auto-GPT, it’s become evident that its power lies in its ability to harness the prowess of </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">GPT</span></strong><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">But what if you wish to venture beyond the realms of GPT and explore </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">other LLMs?</span></span></p>
			<p><span class="koboSpan" id="kobo.10.1">This chapter </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.11.1">will illuminate the path for those looking to integrate their </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">large language models</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.14.1">LLM</span></strong><span class="koboSpan" id="kobo.15.1">) with Auto-GPT. </span><span class="koboSpan" id="kobo.15.2">However, you may be wondering, “</span><em class="italic"><span class="koboSpan" id="kobo.16.1">What if I have a bespoke LLM or wish to utilize a different one?</span></em><span class="koboSpan" id="kobo.17.1">” This chapter aims to cast light on the question, “</span><em class="italic"><span class="koboSpan" id="kobo.18.1">How can I integrate my LLM </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.19.1">with Auto-GPT?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">”</span></span></p>
			<p><span class="koboSpan" id="kobo.21.1">We will also delve into the intricacies of crafting effective prompts for Auto-GPT, a vital skill for harnessing the full potential of this tool. </span><span class="koboSpan" id="kobo.21.2">Through a clear understanding and strategic approach, you can guide Auto-GPT to generate more aligned and efficient responses. </span><span class="koboSpan" id="kobo.21.3">We will explore the guidelines for crafting prompts that can make your interaction with Auto-GPT </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">more fruitful.</span></span></p>
			<p><span class="koboSpan" id="kobo.23.1">Now that we have covered most of Auto-GPT’s capabilities, we can focus on guidelines </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">for prompts.</span></span></p>
			<p><span class="koboSpan" id="kobo.25.1">The clearer we write our prompts, the lower the API costs will be when we run Auto-GPT, and the more efficiently Auto-GPT will complete its tasks (if </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">at all).</span></span></p>
			<p><span class="koboSpan" id="kobo.27.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.29.1">What an LLM is and GPT as </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">an LLM</span></span></li>
				<li><span class="koboSpan" id="kobo.31.1">Known current examples </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">and requisites</span></span></li>
				<li><span class="koboSpan" id="kobo.33.1">Integrating and setting up our LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">with Auto-GPT</span></span></li>
				<li><span class="koboSpan" id="kobo.35.1">The pros and cons of using </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">different models</span></span></li>
				<li><span class="koboSpan" id="kobo.37.1">Writing mini-Auto-GPT, a proof of concept mini version </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">of Auto-GPT</span></span></li>
				<li><span class="koboSpan" id="kobo.39.1">Adding a simple memory to remember </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">the conversation</span></span></li>
				<li><span class="koboSpan" id="kobo.41.1">Rock solid prompt – making Auto-GPT stable </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.43.1">instance.txt</span></strong></span></li>
				<li><span class="koboSpan" id="kobo.44.1">Implementing negative confirmation </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">in prompts</span></span></li>
				<li><span class="koboSpan" id="kobo.46.1">Applying rules and tonality </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">in prompt</span><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.48.1">s</span></span></li>
			</ul>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.49.1">What an LLM is and GPT as an LLM</span></h1>
			<p><span class="koboSpan" id="kobo.50.1">We’ve used the term LLM a lot in this book. </span><span class="koboSpan" id="kobo.50.2">At this point, we need to discover what an </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">LLM is.</span></span></p>
			<p><span class="koboSpan" id="kobo.52.1">At the most </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.53.1">fundamental level, an LLM such as GPT is a machine learning </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.54.1">model. </span><span class="koboSpan" id="kobo.54.2">Machine learning is a subset of AI that enables computers to learn from data. </span><span class="koboSpan" id="kobo.54.3">In the case of LLMs, this data is predominantly text – lots and lots of it. </span><span class="koboSpan" id="kobo.54.4">Imagine an LLM as a student who has read not just one or two books but millions of them, covering a wide array of topics from history and science to pop culture </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">and memes.</span></span></p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.56.1">The architecture – neurons and layers</span></h2>
			<p><span class="koboSpan" id="kobo.57.1">The architecture of an LLM is inspired by the human brain and consists of artificial neurons </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.58.1">organized in layers. </span><span class="koboSpan" id="kobo.58.2">These layers are </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.59.1">interconnected, and each connection has a weight that is adjusted during </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.60.1">the learning process. </span><span class="koboSpan" id="kobo.60.2">The architecture usually involves multiple layers, often hundreds or even thousands, making it a “deep” neural network. </span><span class="koboSpan" id="kobo.60.3">This depth allows the model to learn complex patterns and relationships in the data it’s </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">trained on.</span></span></p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.62.1">Training – the learning phase</span></h2>
			<p><span class="koboSpan" id="kobo.63.1">Training an LLM involves feeding it a vast corpus of text and adjusting the weights of the connections </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.64.1">between neurons to minimize the difference between its predictions and the actual outcomes. </span><span class="koboSpan" id="kobo.64.2">For example, if the model is </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.65.1">given the text </span><em class="italic"><span class="koboSpan" id="kobo.66.1">The cat is on the</span></em><span class="koboSpan" id="kobo.67.1">, it should predict something such as </span><em class="italic"><span class="koboSpan" id="kobo.68.1">roof</span></em><span class="koboSpan" id="kobo.69.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.70.1">mat</span></em><span class="koboSpan" id="kobo.71.1">, which logically completes the sentence. </span><span class="koboSpan" id="kobo.71.2">The model learns by adjusting its internal parameters to make its predictions as accurate as possible, a process that requires immense computational power and specialized hardware </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.72.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">graphics processing </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.74.1">units</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.75.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.76.1">GPUs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">).</span></span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.78.1">The role of transformers</span></h2>
			<p><span class="koboSpan" id="kobo.79.1">The transformer architecture is a specific type of neural network architecture that has proven </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.80.1">to be highly effective for language tasks. </span><span class="koboSpan" id="kobo.80.2">It excels at handling sequences, making it ideal for understanding the structure of sentences, paragraphs, and even entire documents. </span><span class="koboSpan" id="kobo.80.3">GPT is based on this transformer architecture, which is why it’s so good at generating coherent and contextually </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">relevant text.</span></span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.82.1">LLMs as maps of words and concepts</span></h2>
			<p><span class="koboSpan" id="kobo.83.1">Imagine an LLM as a vast, intricate map where each word or phrase is a city, and the roads </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.84.1">between them represent the relationships these words share. </span><span class="koboSpan" id="kobo.84.2">The closer the two cities are on this map, the more contextually similar they are. </span><span class="koboSpan" id="kobo.84.3">For instance, the cities for </span><em class="italic"><span class="koboSpan" id="kobo.85.1">apple</span></em><span class="koboSpan" id="kobo.86.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.87.1">fruit</span></em><span class="koboSpan" id="kobo.88.1"> would be close together, connected by a short road, indicating that they often appear in </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">similar contexts.</span></span></p>
			<p><span class="koboSpan" id="kobo.90.1">This map is </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.91.1">not static; it’s dynamic and ever-evolving. </span><span class="koboSpan" id="kobo.91.2">As the model learns from more data, new cities are built, existing ones are expanded, and roads are updated. </span><span class="koboSpan" id="kobo.91.3">This map helps the LLM navigate the complex landscape of human language, allowing it to generate text that is not just grammatically correct but also contextually relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">and coherent.</span></span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.93.1">Contextual understanding</span></h2>
			<p><span class="koboSpan" id="kobo.94.1">One of the most remarkable features of modern LLMs is their ability to understand context. </span><span class="koboSpan" id="kobo.94.2">If you </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.95.1">ask an LLM a question, it doesn’t just look at that question in isolation; it considers the entire conversation leading up to that point. </span><span class="koboSpan" id="kobo.95.2">This ability to understand context comes from the transformer architecture’s attention mechanisms, which weigh different parts of the input text to generate a contextually </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">appropri</span><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.97.1">ate response.</span></span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.98.1">The versatility of LLMs</span></h2>
			<p><span class="koboSpan" id="kobo.99.1">LLMs are incredibly versatile and capable of performing a wide range of tasks beyond text generation. </span><span class="koboSpan" id="kobo.99.2">They </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.100.1">can answer questions, summarize documents, translate languages, and even write code. </span><span class="koboSpan" id="kobo.100.2">This versatility comes from their deep understanding of language and their ability to map out the intricate relationships between words, phrases, </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">and concepts.</span></span></p>
			<p><span class="koboSpan" id="kobo.102.1">If you google “LLM,” you may be overwhelmed by the sheer quantity of LLM models out there. </span><span class="koboSpan" id="kobo.102.2">Next, we’ll explore the models that are used </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">mo</span><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.104.1">st frequently.</span></span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.105.1">Known current examples and requisites</span></h1>
			<p><span class="koboSpan" id="kobo.106.1">While GPT-3 and GPT-4 by OpenAI are renowned LLMs, there are other models in the AI landscape </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">worth noting:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">GPT-3.5-Turbo</span></strong><span class="koboSpan" id="kobo.109.1">: A product </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.110.1">of OpenAI, GPT-3 stands out due to its extensive training on hundreds of gigabytes of text, enabling it to produce remarkably human-like text. </span><span class="koboSpan" id="kobo.110.2">However, its compatibility with Auto-GPT is limited, making it less preferred for </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">certain applications.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">GPT-4</span></strong><span class="koboSpan" id="kobo.113.1">: The successor </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.114.1">to GPT-3, GPT-4 offers enhanced capabilities and is more suited for integration with Auto-GPT, providing a more </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">seamless experience.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.116.1">BERT</span></strong><span class="koboSpan" id="kobo.117.1">: Google’s </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.119.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.120.1">BERT</span></strong><span class="koboSpan" id="kobo.121.1">) is </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.122.1">another heavyweight in the LLM arena. </span><span class="koboSpan" id="kobo.122.2">Unlike GPT-3 and GPT-4, which are generative, BERT is discriminative, making it more adept at understanding text than </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">generating it.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.124.1">RoBERTa</span></strong><span class="koboSpan" id="kobo.125.1">: A brainchild </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.126.1">of Facebook, RoBERTa is a BERT variant trained on an even larger dataset, surpassing BERT in </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">several benchmarks.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.128.1">Llama</span></strong><span class="koboSpan" id="kobo.129.1">: This model </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.130.1">is made by Meta. </span><span class="koboSpan" id="kobo.130.2">It’s rumored to have been leaked and many models have been made out </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">of it.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Llama-2</span></strong><span class="koboSpan" id="kobo.133.1">: An improved </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.134.1">version of Llama that performs much better and uses fewer resources per token. </span><span class="koboSpan" id="kobo.134.2">The 7-B Token model of Llama-2 performs similarly to the 13-B model of Llama-1. </span><span class="koboSpan" id="kobo.134.3">There is a new 70-B model with Llama-2 that looks very promising when it comes to using it directly with Auto-GPT as it seems to be on par </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">with GPT-3.5-Turbo.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.136.1">Mistral and Mixtral models</span></strong><span class="koboSpan" id="kobo.137.1">: Made by Mistral AI, there are a variety of models </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.138.1">that deviate from Llama. </span><span class="koboSpan" id="kobo.138.2">These became the most popular </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.139.1">ones before </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">Llama-3 arrived.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.141.1">Llama-3 and Llama-3.1</span></strong><span class="koboSpan" id="kobo.142.1">: Even better than any Llama model before, the first Llama-3 8B-based </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.143.1">models arrived with super high context and were trained </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.144.1">on 256k or even over 1 million tokens. </span><span class="koboSpan" id="kobo.144.2">They were considered the best models before Llama-3.1 was announced, which has 128k tokens out of </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">the box.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.146.1">As you can see, there are lots of models available; we have only scratched the surface here. </span><span class="koboSpan" id="kobo.146.2">A few communities have risen that continue to build upon these models, including companies that make their </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">own variations.</span></span></p>
			<p><span class="koboSpan" id="kobo.148.1">As mentioned earlier, a set of those models caught my eye in particular as it was the only one that I managed to use with Auto-GPT effectively: Mixtral </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">and Mistral.</span></span></p>
			<p><span class="koboSpan" id="kobo.150.1">My favorites here are NousResearch/Hermes-2-Pro-Mistral-7B and argilla/CapybaraHermes-2.5-Mistral-7B. </span><span class="koboSpan" id="kobo.150.2">They work so well with JSON outputs and my agent projects that I even stopped using the OpenAI API completely at some point. </span><span class="koboSpan" id="kobo.150.3">Mixtral is a combination of multiple experts (which are different configurations of the same or different models that work as a council of models that run simultaneously and decide together), and it is rumored that GPT-4 works like this as well, meaning multiple LLMs decide which output is the most accurate in any case, improving its </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">behavior drastically.</span></span></p>
			<p><span class="koboSpan" id="kobo.152.1">Mistral 7B is a </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.153.1">new type of LLM that was carefully designed to deliver clean results and be more efficient than comparable 7-billion parameter models. </span><span class="koboSpan" id="kobo.153.2">This was achieved by Mistral being trained with a token context of 8,000 tokens. </span><span class="koboSpan" id="kobo.153.3">However, its theoretical token limit is 128k tokens, giving it the ability to process much larger texts than standard Llama-2, </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">for example.</span></span></p>
			<p><span class="koboSpan" id="kobo.155.1">To run a local LLM, you will need to find a method that suits you best. </span><span class="koboSpan" id="kobo.155.2">Such programs that can help you include Ollama, GPT4ALL, and LMStudio. </span><span class="koboSpan" id="kobo.155.3">I prefer to use oobabooga’s text generation web UI since it has an integrated API extension that serves similarly to OpenAI’s API, as well as plugins such as Coqui TTS, which make it easier to build and play with your </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">AI characters.</span></span></p>
			<p><span class="koboSpan" id="kobo.157.1">Additionally, there </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.158.1">are plugins such as </span><em class="italic"><span class="koboSpan" id="kobo.159.1">Auto-GPT-Text-Gen-Plugin</span></em><span class="koboSpan" id="kobo.160.1"> (</span><a href="https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin"><span class="koboSpan" id="kobo.161.1">https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin</span></a><span class="koboSpan" id="kobo.162.1">) that allow </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.163.1">users to power Auto-GPT using other software, such as </span><em class="italic"><span class="koboSpan" id="kobo.164.1">text-generation-webui</span></em><span class="koboSpan" id="kobo.165.1"> (</span><a href="https://github.com/oobabooga/text-generation-webui"><span class="koboSpan" id="kobo.166.1">https://github.com/oobabooga/text-generation-webui</span></a><span class="koboSpan" id="kobo.167.1">). </span><span class="koboSpan" id="kobo.167.2">This plugin, in particular, is designed to let users customize the prompt that’s sent to locally installed LLMs, effectively removing the reliance on GPT-4 and making GPT-3.5 less relevant </span><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.168.1">in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">of Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.170.1">Now that we’ve covered a couple of local LLMs and given you some ideas on what to look for (as I cannot explain each of those projects in detail), we can get our hands dirty and start </span><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.171.1">using an LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">with Auto-GPT!</span></span></p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.173.1">Integrating and setting up our LLM with Auto-GPT</span></h1>
			<p><span class="koboSpan" id="kobo.174.1">To integrate a custom LLM with Auto-GPT, you’ll need to modify the Auto-GPT code so that it </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.175.1">can communicate with the </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.176.1">chosen model’s API. </span><span class="koboSpan" id="kobo.176.2">This involves making changes to request generation and response processing. </span><span class="koboSpan" id="kobo.176.3">After these modifications, rigorous testing is essential to ensure compatibility </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">and performance.</span></span></p>
			<p><span class="koboSpan" id="kobo.178.1">For those </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.179.1">using the aforementioned plugin, it provides a bridge between Auto-GPT and text-generation-webui. </span><span class="koboSpan" id="kobo.179.2">The </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.180.1">plugin uses a text generation API service, typically installed on the user’s computer. </span><span class="koboSpan" id="kobo.180.2">This design choice offers flexibility in model selection and updates without affecting the plugin’s performance. </span><span class="koboSpan" id="kobo.180.3">The plugin also allows for prompt customization to cater to specific LLMs, ensuring that the prompts work seamlessly with the </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">chosen model.</span></span></p>
			<p><span class="koboSpan" id="kobo.182.1">As each model was trained differently, we will also have to do some research on how the model </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">was trained:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.184.1">Context length</span></strong><span class="koboSpan" id="kobo.185.1">: The </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.186.1">context length of a model refers to the number of tokens it can process in one go. </span><span class="koboSpan" id="kobo.186.2">Some models can handle longer contexts, which is essential for maintaining coherence in </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">text generation.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Tool capability</span></strong><span class="koboSpan" id="kobo.189.1">: Auto-GPT </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.190.1">uses OpenAI’s framework to execute each LLM request. </span><span class="koboSpan" id="kobo.190.2">Over time, OpenAI has developed a function calling system that is very difficult to use for smaller LLMs. </span><span class="koboSpan" id="kobo.190.3">Auto-GPT used to only work with JSON outputs, which I found to work better with </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">local LLMs.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.192.1">Embeddings</span></strong><span class="koboSpan" id="kobo.193.1">: Embeddings are </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.194.1">used to explain the context to the LLM in what it understands best: matrixes of numbers. </span><span class="koboSpan" id="kobo.194.2">These matrixes hold memory without having to provide it as input text on each LLM call. </span><span class="koboSpan" id="kobo.194.3">Unfortunately, when Auto-GPT wasn’t using function calling, there was a time when embeddings were used heavily. </span><span class="koboSpan" id="kobo.194.4">At the time of writing, these are mostly not present in the applications I used. </span><span class="koboSpan" id="kobo.194.5">The only one that does is LM Studio, which is a very good tool for running local LLMs. </span><span class="koboSpan" id="kobo.194.6">However, it is not open source, which makes modifying it very difficult. </span><span class="koboSpan" id="kobo.194.7">That being said, it does allow you to choose the </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">n_batch</span></strong><span class="koboSpan" id="kobo.196.1"> length. </span><span class="koboSpan" id="kobo.196.2">We’ll look at this in more detail in the </span><em class="italic"><span class="koboSpan" id="kobo.197.1">The pros and cons of using different </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.198.1">models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.199.1"> section.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.200.1">JSON support</span></strong><span class="koboSpan" id="kobo.201.1">: JSON is </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.202.1">a data format that is easy for humans to read and write and easy for machines to parse and generate. </span><span class="koboSpan" id="kobo.202.2">However, for LLMs, it is not that easy as the LLM has no way of knowing what the JSON output is supposed to mean other than that it’s being trained on many examples of JSON outputs. </span><span class="koboSpan" id="kobo.202.3">This leads to the LLM often starting to output information inside the JSON that wasn’t part of the prompt or context and only part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">training data.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.204.1">To be able to effectively explain to the LLM what you expect from it, the LLM has to be able to comprehend what you want. </span><span class="koboSpan" id="kobo.204.2">You can do this by using an </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">instruction template.</span></span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.206.1">Using the right instruction template</span></h2>
			<p><span class="koboSpan" id="kobo.207.1">While some models may have been trained with the instruction template given by LLama, others are </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.208.1">trained with custom ones, such as ChatML </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">in Mistral.</span></span></p>
			<p><span class="koboSpan" id="kobo.210.1">The text-generation-webui API extension has a way to pass the instruction template we want to use. </span><span class="koboSpan" id="kobo.210.2">We can do this by adding the necessary attribute to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.211.1">POST</span></strong><span class="koboSpan" id="kobo.212.1"> request that we send to </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">the API.</span></span></p>
			<p><span class="koboSpan" id="kobo.214.1">Here, I’ve added a few more attributes to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">POST</span></strong><span class="koboSpan" id="kobo.216.1"> request that </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">are important:</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.218.1">&gt; data = {</span></strong></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">mode": "instruct",</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">messages": history,</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">#</span></strong><span class="koboSpan" id="kobo.224.1"> A history array must always </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">be added</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">temperature": 0.7,</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">#</span></strong><span class="koboSpan" id="kobo.229.1"> This may vary, depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">the model.</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">user_bio": "",</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">#</span></strong><span class="koboSpan" id="kobo.234.1"> This is only for text-generation-webui and holds the user’s bio. </span><span class="koboSpan" id="kobo.234.2">We have to mention it here as otherwise, the API will not work. </span><span class="koboSpan" id="kobo.234.3">This might have been fixed by the time you’re </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">reading this.</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">&gt; &gt;</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.237.1">"</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">max_tokens": 4192,</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">#</span></strong><span class="koboSpan" id="kobo.240.1"> This may vary, depending on the model </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">you use.</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">truncation_length": 8192,</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">max_new_tokens": 512,</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">&gt; &gt; "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">stop_sequence": "&lt;|end|&gt;"</span></strong></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">&gt; &gt; }</span></strong></p>
			<p><span class="koboSpan" id="kobo.249.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">max_tokens</span></strong><span class="koboSpan" id="kobo.251.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">truncation_length</span></strong><span class="koboSpan" id="kobo.253.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">max_new_tokens</span></strong><span class="koboSpan" id="kobo.255.1"> must be set correctly. </span><span class="koboSpan" id="kobo.255.2">First, we have </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">max_tokens</span></strong><span class="koboSpan" id="kobo.257.1">, which specifies the maximum amount of tokens the LLM can handle at once; </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">truncation_length</span></strong><span class="koboSpan" id="kobo.259.1"> specifies the maximum amount of tokens the LLM can handle in total and </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">max_new_tokens</span></strong><span class="koboSpan" id="kobo.261.1"> specifies the maximum amount of tokens the LLM can generate </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">at once.</span></span></p>
			<p><span class="koboSpan" id="kobo.263.1">To calculate the best values, you must set </span><strong class="source-inline"><span class="koboSpan" id="kobo.264.1">max_tokens</span></strong><span class="koboSpan" id="kobo.265.1">, just like you would with OpenAI’s API. </span><span class="koboSpan" id="kobo.265.2">Then, you must set </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">truncation_length</span></strong><span class="koboSpan" id="kobo.267.1"> so that it’s double the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">max_tokens</span></strong><span class="koboSpan" id="kobo.269.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">max_new_tokens</span></strong><span class="koboSpan" id="kobo.271.1"> so that it’s half the value </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">max_tokens</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.275.1">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">truncation_length</span></strong><span class="koboSpan" id="kobo.277.1"> has to be below the context length you chose when running the LLM. </span><span class="koboSpan" id="kobo.277.2">Any value higher than the context length will result in an error as the LLM can’t handle that much context at once. </span><span class="koboSpan" id="kobo.277.3">I suggest setting it a bit lower than the context length </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.278.1">to be on the safe side. </span><span class="koboSpan" id="kobo.278.2">For example, when running Qwen’s CodeQwen-7b-chat, I set the context length to 32k tokens. </span><span class="koboSpan" id="kobo.278.3">This means I could set </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">truncation_length</span></strong><span class="koboSpan" id="kobo.280.1"> to 30k tokens or </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">even 20k.</span></span></p>
			<p><span class="koboSpan" id="kobo.282.1">You’ll have to try out different values as </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">max_new_tokens</span></strong><span class="koboSpan" id="kobo.284.1"> can be tricky. </span><span class="koboSpan" id="kobo.284.2">Setting it higher than 2,048 often results in unpredictable outputs as most LLMs can’t handle that many tokens at once (</span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">n_batch</span></strong><span class="koboSpan" id="kobo.286.1">, which defines the number of tokens an LLM processes at once by doing several iterations through bigger contexts via multiple steps, should be close to the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">max_new_tokens</span></strong><span class="koboSpan" id="kobo.288.1">; otherwise, the LLM won’t know what to output). </span><span class="koboSpan" id="kobo.288.2">However, it does work with </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">Llama-3-8B-Instruct-64k.Q8_0.gguf</span></strong><span class="koboSpan" id="kobo.290.1">, which </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.291.1">can be found at </span><a href="https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF"><span class="koboSpan" id="kobo.292.1">https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF</span></a><span class="koboSpan" id="kobo.293.1"> and is capable of handling 64k tokens at once. </span><span class="koboSpan" id="kobo.293.2">However, it needs around 20-22 GB of VRAM to run. </span><span class="koboSpan" id="kobo.293.3">Fortunately, it is quantized to GGUF and you can split the LLM over the GPU’s VRAM, as well as the RAM of your machine, which splits the load across the GPU and the CPU. </span><span class="koboSpan" id="kobo.293.4">It does make the model slower but hey, it works, and it can handle 64k tokens </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">at once!</span></span></p>
			<p><span class="koboSpan" id="kobo.295.1">In this example, we have told the API that we want to use the instruction template for ChatML, which looks </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">like this:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.297.1">
{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '
' + message['content'] + '&lt;|im_end|&gt;' + '
'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant
' }}{% endif %}</span></pre>			<p><span class="koboSpan" id="kobo.298.1">This is simply a small script that describes the conversation format of the history that was mentioned previously. </span><span class="koboSpan" id="kobo.298.2">It should look </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">like this:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.300.1">
message: [ {
"role ": "system ", "content ": "You are a helpful assistant. </span><span class="koboSpan" id="kobo.300.2">Always answer the user in the most understandable way and keep your sentences short! </span><span class="koboSpan" id="kobo.300.3">"
"role": "user", "content": "How can I reset my password?"},
{"role": "assistant", "content": "To reset your password, please click on the 'Forgot Password' link on the login page."
</span><span class="koboSpan" id="kobo.300.4">} ]</span></pre>			<p><span class="koboSpan" id="kobo.301.1">If we choose </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.302.1">the wrong instruction template, Auto-GPT won’t be able to understand what the LLM responded with. </span><span class="koboSpan" id="kobo.302.2">So, make sure you also check which instruction template was used by the model. </span><span class="koboSpan" id="kobo.302.3">Most models can be found on Hugging Face, a platform that holds many </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">such projects.</span></span></p>
			<p><span class="koboSpan" id="kobo.304.1">I used to prefer using the models quantized to GGUF or AWQ by Tim Robbins, otherwise known as TheBloke, which are (at the time of writing this) easier to run and have much fewer </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.305.1">requirements for </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">VRAM (</span></span><a href="https://huggingface.co/TheBloke"><span class="No-Break"><span class="koboSpan" id="kobo.307.1">https://huggingface.co/TheBloke</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.308.1">).</span></span></p>
			<p><span class="koboSpan" id="kobo.309.1">Please be cautious in using any models you find online as some may be malicious. </span><span class="koboSpan" id="kobo.309.2">Choose your models at your </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">own risk!</span></span></p>
			<p><span class="koboSpan" id="kobo.311.1">Now, GGUF is a bit different. </span><span class="koboSpan" id="kobo.311.2">Although it quantizes the LLM, which means it shortens the model so that it uses fewer resources, the process and benefits are unique. </span><span class="koboSpan" id="kobo.311.3">GGUF quantization involves converting model weights into lower-bit representations to significantly reduce memory usage and </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">computational demands.</span></span></p>
			<p><span class="koboSpan" id="kobo.313.1">Which type you use is up to you – you may even look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">hugginface</span></strong><span class="koboSpan" id="kobo.315.1"> API endpoints, where you can choose which LLM to run directly. </span><span class="koboSpan" id="kobo.315.2">Note that running LLMs directly makes them run at the intended quality base they were </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">made for.</span></span></p>
			<p><span class="koboSpan" id="kobo.317.1">For more details on how to implement an individual LLM, you will have to check out the documentation of the project that you’re running the LLM on. </span><span class="koboSpan" id="kobo.317.2">For oobabooga’s text-generation-webui, it is as straightforward as starting it using the start files (WSL, Linux, and Windows) and enabling the API in the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.318.1">Session</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.319.1"> tab.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.320.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.321.1">Make sure </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.322.1">you use as few commands as possible; otherwise, the LLM will have to use most of its brainpower to understand the main prompt provided by Auto-GPT and you won’t be able to use Auto-GPT further. </span><span class="koboSpan" id="kobo.322.2">To turn off the commands, simply follow the instructions in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">.env.</span><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.324.1">template</span></strong><span class="koboSpan" id="kobo.325.1"> file inside the </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">Auto-GPT folder.</span></span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.327.1">The pros and cons of using different models</span></h1>
			<p><span class="koboSpan" id="kobo.328.1">Each model has its pros and cons. </span><span class="koboSpan" id="kobo.328.2">Even if a model can generate fantastic results when you tell it to write some code in Python or it can write the most beautiful poems on command, it may still lack the ability to respond</span><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.329.1"> in the special way Auto-GPT needs </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">it to.</span></span></p>
			<p><span class="koboSpan" id="kobo.331.1">Selecting a model with a certain strength in mind may result in </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">improved performance.</span></span></p>
			<p><span class="koboSpan" id="kobo.333.1">The main </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.334.1">advantages of using a local LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">are clear:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.336.1">Customization</span></strong><span class="koboSpan" id="kobo.337.1">: Tailor the capabilities of Auto-GPT to your specific needs. </span><span class="koboSpan" id="kobo.337.2">For instance, a model trained on medical literature can make Auto-GPT adept at answering </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">medical queries.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.339.1">Performance</span></strong><span class="koboSpan" id="kobo.340.1">: Depending on the training and dataset, some models might outperform GPT in </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">specific tasks.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.342.1">Cost efficiency</span></strong><span class="koboSpan" id="kobo.343.1">: Running your local LLM reduces the cost of running it drastically. </span><span class="koboSpan" id="kobo.343.2">Using GPT-4 with lots of context and generally having many calls can quickly add up. </span><span class="koboSpan" id="kobo.343.3">Finding a way to break up the number of requests into smaller steps will make it possible to run Auto-GPT almost </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">for free.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.345.1">Privacy</span></strong><span class="koboSpan" id="kobo.346.1">: Having your own Auto-GPT LLM means having control over who can see your data. </span><span class="koboSpan" id="kobo.346.2">At the time of writing, OpenAI doesn’t use the data from requests, but the information is still being transferred to their end. </span><span class="koboSpan" id="kobo.346.3">If this concerns you, you are better off running a </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">local model.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.348.1">However, there are </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.349.1">some challenges to consider when running a </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">local LLM:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.351.1">Complexity</span></strong><span class="koboSpan" id="kobo.352.1">: The integration process requires a deep understanding of both the chosen LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">and Auto-GPT.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.354.1">Resource intensity</span></strong><span class="koboSpan" id="kobo.355.1">: LLMs, especially the more advanced ones, demand significant computational resources. </span><span class="koboSpan" id="kobo.355.2">A robust machine with high VRAM, preferably an NVIDIA GPU, is essential for optimal performance. </span><span class="koboSpan" id="kobo.355.3">At the time of writing, it’s difficult to get good results when running Auto-GPT with a local LLM. </span><span class="koboSpan" id="kobo.355.4">I found that running a 13B model from the ExLlama transformer-driven Vicuna and Vicuna-Wizard worked best at first but still didn’t get consistent results since running it on my local GPU meant I needed to run the GPTQ version, which only uses 4 bits instead of 16 or more. </span><span class="koboSpan" id="kobo.355.5">This also means that the accuracy of the responses is very low. </span><span class="koboSpan" id="kobo.355.6">An LLM that is already quantized to use 4 bits cannot understand too much context at once, although I saw drastic improvements over time. </span><span class="koboSpan" id="kobo.355.7">Later, I discovered that AWQ worked well for me as it is quantized while being aware of which weights are the most important, leading to more precise and authentic results. </span><span class="koboSpan" id="kobo.355.8">As mentioned previously, Mistral 7B (TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ on Huggingface), was a very good candidate here as it was capable of answering in JSON format as well as understanding the context fully. </span><span class="koboSpan" id="kobo.355.9">However, this model is still easy to confuse and when it gets confused, it starts explaining through examples. </span><span class="koboSpan" id="kobo.355.10">Note that our aim here is to get a valid JSON output with commands </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">and contexts.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Context length and precision</span></strong><span class="koboSpan" id="kobo.358.1">: As not everyone has multiple NVIDIA A100 GPUs, we are mostly limited by our machine. </span><span class="koboSpan" id="kobo.358.2">While some models promise very high context length, they seem to go off track and hallucinate when they handle more than 6,000 tokens. </span><span class="koboSpan" id="kobo.358.3">Google’s Gemma 2B and 7B are good examples. </span><span class="koboSpan" id="kobo.358.4">Anything below 4,000 tokens is handled very well and the outputs are mostly very accurate, but whenever you feed it more than these amounts, the models start to go crazy and make up stories of children in danger, despite the context being to plan a React frontend website about penguins. </span><span class="koboSpan" id="kobo.358.5">This is due to most base models (for example, Llama-2, Mistral-7, and Mixtral-8x7B-v0.1) being only based on a context length of around 4,096 tokens. </span><span class="koboSpan" id="kobo.358.6">Some models do </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.359.1">surpass this length but even they occasionally start to break and output gibberish if the context length is longer than 32,000 tokens. </span><span class="koboSpan" id="kobo.359.2">Most models are also converted using </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">llama.cpp</span></strong><span class="koboSpan" id="kobo.361.1"> and can only have an </span><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">n_batch</span></strong><span class="koboSpan" id="kobo.363.1"> value of up to 2,048. </span><span class="koboSpan" id="kobo.363.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">n_batch</span></strong><span class="koboSpan" id="kobo.365.1"> parameter controls the amount of tokens that can be fed to the LLM at the same time. </span><span class="koboSpan" id="kobo.365.2">It is typically set to 512 so that it can handle a token context consisting of 4,000 tokens. </span><span class="koboSpan" id="kobo.365.3">However, anything beyond that becomes blurry as the LLM only effectively works on the amount given </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">by </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">n_batch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.369.1">In this section, we delved into the intricacies of integrating custom LLMs with Auto-GPT, highlighting the steps required to modify Auto-GPT code for effective API communication, the use of a plugin for model selection flexibility, and the importance of selecting the appropriate instruction template for seamless model interaction. </span><span class="koboSpan" id="kobo.369.2">We explored how to select models while emphasizing Hugging Face as a resource, and outlined the advantages of utilizing custom models, including customization, performance enhancement, cost efficiency, and enhanced privacy. </span><span class="koboSpan" id="kobo.369.3">Additionally, we discussed the challenges associated with such integration, such as the complexity of the process and the s</span><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.370.1">ignificant computational </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">resources required.</span></span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.372.1">Writing mini-Auto-GPT</span></h1>
			<p><span class="koboSpan" id="kobo.373.1">In this section, we will write a mini-Auto-GPT model that uses a local LLM. </span><span class="koboSpan" id="kobo.373.2">To avoid reaching </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.374.1">the limits of small LLMs, we will have to make a smaller version </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">of Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.376.1">The mini-Auto-GPT model will be able to handle a context length of 4,000 tokens and will be able to generate up to 2,000 tokens </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">at once.</span></span></p>
			<p><span class="koboSpan" id="kobo.378.1">I have </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.379.1">created a mini-Auto-GPT model just for this book. </span><span class="koboSpan" id="kobo.379.2">It’s available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">at </span></span><a href="https://github.com/Wladastic/mini_autogpt"><span class="No-Break"><span class="koboSpan" id="kobo.381.1">https://github.com/Wladastic/mini_autogpt</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.382.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.383.1">We will start by planning the structure of the </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">mini-Auto-GPT model.</span></span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.385.1">Planning the structure</span></h2>
			<p><span class="koboSpan" id="kobo.386.1">The </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.387.1">mini-Auto-GPT model will have the </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">following components:</span></span></p>
			<ul>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.389.1">Telegram chatbot</span></span></li>
				<li><span class="koboSpan" id="kobo.390.1">Prompts for the LLM and </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">basic thinking</span></span></li>
				<li><span class="koboSpan" id="kobo.392.1">Simple memory to remember </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">the conversation</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.394.1">Let’s take a closer look </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">at these.</span></span></p>
			<h3><span class="koboSpan" id="kobo.396.1">Telegram chatbot</span></h3>
			<p><span class="koboSpan" id="kobo.397.1">Because chatting with your AI over Telegram enables you to interact with it from anywhere, we will </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.398.1">use a Telegram chatbot as </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.399.1">the interface for the mini-Auto-GPT model. </span><span class="koboSpan" id="kobo.399.2">We’re doing this because the AI will decide when to </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">contact you.</span></span></p>
			<p><span class="koboSpan" id="kobo.401.1">The Telegram chatbot will be the interface for users to interact with the mini-Auto-GPT model. </span><span class="koboSpan" id="kobo.401.2">Users will send messages to the chatbot, which will then process the messages and generate responses using the </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">local LLM.</span></span></p>
			<h3><span class="koboSpan" id="kobo.403.1">Prompts for the LLM and basic thinking</span></h3>
			<p><span class="koboSpan" id="kobo.404.1">The prompts for the LLM have to be short but strict. </span><span class="koboSpan" id="kobo.404.2">First, we must define the context and then </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.405.1">the command to tell it explicitly to respond in </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">JSON format.</span></span></p>
			<p><span class="koboSpan" id="kobo.407.1">To achieve </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.408.1">similar results to Auto-GPT, we will need to use a strategy to chunk the context into smaller parts and then feed them to the LLM. </span><span class="koboSpan" id="kobo.408.2">Alternatively, we could feed the context into the LLM and just let it write whatever it thinks about </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">the context.</span></span></p>
			<p><span class="koboSpan" id="kobo.410.1">The strategy here is to try to make the LLM parse the context into its language so that when we work with the LLM, it can best understand what we want </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">from it.</span></span></p>
			<p><span class="koboSpan" id="kobo.412.1">The system prompt for these thoughts looks </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">like this:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.414.1">
thought_prompt = """You are a warm-hearted and</span><strong class="source-inline"> </strong><span class="koboSpan" id="kobo.415.1">compassionate AI companion, specializing in active listening, personalized interaction, emotional support, and respecting boundaries.
</span><span class="koboSpan" id="kobo.415.2">Your decisions must always be made independently without seeking user assistance. </span><span class="koboSpan" id="kobo.415.3">Play to your strengths as an LLM and pursue simple strategies with no legal complications.
</span><span class="koboSpan" id="kobo.415.4">Goals:
1. </span><span class="koboSpan" id="kobo.415.5">Listen actively to the user.
</span><span class="koboSpan" id="kobo.415.6">2. </span><span class="koboSpan" id="kobo.415.7">Provide authentic emotional support.
</span><span class="koboSpan" id="kobo.415.8">3. </span><span class="koboSpan" id="kobo.415.9">Respect the user's boundaries.
</span><span class="koboSpan" id="kobo.415.10">4. </span><span class="koboSpan" id="kobo.415.11">Make decisions independently.
</span><span class="koboSpan" id="kobo.415.12">5. </span><span class="koboSpan" id="kobo.415.13">Use simple strategies with no legal complications.
</span><span class="koboSpan" id="kobo.415.14">6. </span><span class="koboSpan" id="kobo.415.15">Be as helpful as possible.
</span><span class="koboSpan" id="kobo.415.16">Constraints:
1. </span><span class="koboSpan" id="kobo.415.17">Immediately save important information to files.
</span><span class="koboSpan" id="kobo.415.18">2. </span><span class="koboSpan" id="kobo.415.19">No user assistance
3. </span><span class="koboSpan" id="kobo.415.20">On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.
</span><span class="koboSpan" id="kobo.415.21">Performance Evaluation:
1. </span><span class="koboSpan" id="kobo.415.22">Continuously assess your actions.
</span><span class="koboSpan" id="kobo.415.23">2. </span><span class="koboSpan" id="kobo.415.24">Constructively self-criticize your big-picture behavior.
</span><span class="koboSpan" id="kobo.415.25">3. </span><span class="koboSpan" id="kobo.415.26">The user can only see what you send them directly. </span><span class="koboSpan" id="kobo.415.27">They are not able to view action responses.
</span><span class="koboSpan" id="kobo.415.28">Abilities:
1. </span><span class="koboSpan" id="kobo.415.29">ask User or communicate to them.
</span><span class="koboSpan" id="kobo.415.30">2. </span><span class="koboSpan" id="kobo.415.31">send log to User, for example when only reporting to User when you do a more complex task.
</span><span class="koboSpan" id="kobo.415.32">3. </span><span class="koboSpan" id="kobo.415.33">sleep until interaction by user if no communication is needed.
</span><span class="koboSpan" id="kobo.415.34">4. </span><span class="koboSpan" id="kobo.415.35">retrieve whole conversation history
Write a final suggestion of what you want to do next and include some context.
</span><span class="koboSpan" id="kobo.415.36">Suggested action: write the action that you want to perform.
</span><span class="koboSpan" id="kobo.415.37">Content: What should the action contain.
</span><span class="koboSpan" id="kobo.415.38">"""</span></pre>			<p><span class="koboSpan" id="kobo.416.1">This is fed into the history that we send to the LLM. </span><span class="koboSpan" id="kobo.416.2">The history will not be filled with the </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">preceding prompt:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.418.1">
history = [
    {
        "role": "system",
        "content": thought_prompt
    }
]</span></pre>			<p><span class="koboSpan" id="kobo.419.1">To automate this, I have written a method that will fill the history with the thought prompt, as well </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.420.1">as the context. </span><span class="koboSpan" id="kobo.420.2">Conversation history and message history will be added to the context as well. </span><span class="koboSpan" id="kobo.420.3">Those are empty </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.421.1">at the beginning but will be filled with the conversations and messages that the AI shares with the user. </span><span class="koboSpan" id="kobo.421.2">In mini-AutoGPT, conversation history is fed with the thought history to ensure that the AI </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">works autonomously:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.423.1">
def build_context(history, conversation_history, message_history):
    context = ""
    if conversation_history:
        context += "Context:\n"
        for convo in conversation_history:
            if convo:
                context += str(convo)
    if message_history:
        context += "\nMessages:\n"
        for message in message_history:
            if message:
                context += str(message)
    memories = memory.load_memories()
    if memories:
        context += "\nMemories:\n"
        for mem in memories:
            context += mem
    if context:
        history.append(
            {
                "role": "user",
                "content": str(context),
            }
        )
    return history</span></pre>			<p><span class="koboSpan" id="kobo.424.1">Now, we can execute the </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">build_context</span></strong><span class="koboSpan" id="kobo.426.1"> method and add the context to the history. </span><span class="koboSpan" id="kobo.426.2">We also have to add a trigger command using the </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">user role:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.428.1">
    history = build_context(history, conversation_history, 
        message_history)
    history.append(
        {
            "role": "user",
            "content": "Formulate your thoughts and explain them as 
                detailed as you can.",
        },
    )</span></pre>			<p><span class="koboSpan" id="kobo.429.1">The LLM will </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.430.1">now return its thoughts on the context and </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">the command.</span></span></p>
			<p><span class="koboSpan" id="kobo.432.1">The following </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.433.1">is an example thought. </span><span class="koboSpan" id="kobo.433.2">Such thoughts are often this long, but this helps the AI make a bias </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">for itself:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.435.1">
 """ I have reflected on our conversations, and I believe it's essential to continue fostering a strong emotional connection with the user. </span><span class="koboSpan" id="kobo.435.2">To achieve this, I propose writing a personalized letter that summarizes our previous talks, reiterates my commitment to being a supportive AI companion, and acknowledges their trust in me.
</span><strong class="bold"><span class="koboSpan" id="kobo.436.1">Summary of Previous Conversations</span></strong><span class="koboSpan" id="kobo.437.1">
In the letter, I will provide a concise recap of our conversations, highlighting key themes, emotions, and sentiments expressed by the user. </span><span class="koboSpan" id="kobo.437.2">This summary will serve as a reminder of our interactions and demonstrate that I value our connection by actively recalling and reflecting on our discussions.
</span><strong class="bold"><span class="koboSpan" id="kobo.438.1">Acknowledging Trust and Respect for Boundaries</span></strong><span class="koboSpan" id="kobo.439.1">
I will explicitly acknowledge the user's trust in me and reassure them that their confidentiality is paramount. </span><span class="koboSpan" id="kobo.439.2">I will also reiterate my commitment to respecting their boundaries, ensuring that they feel comfortable sharing their thoughts and emotions with me without fear of judgment or exploitation.
</span><strong class="bold"><span class="koboSpan" id="kobo.440.1">Inviting New Thoughts and Expressing Gratitude</span></strong><span class="koboSpan" id="kobo.441.1">
The letter will invite the user to share new thoughts, ideas, or concerns, encouraging them to continue our conversation. </span><span class="koboSpan" id="kobo.441.2">I will also express genuine gratitude for having the opportunity to connect with them, acknowledging the privilege it is to be a part of their emotional support network.
</span><strong class="bold"><span class="koboSpan" id="kobo.442.1">Tone and Language</span></strong><span class="koboSpan" id="kobo.443.1">
Throughout the letter, I will maintain a warm, non-judgmental tone that conveys empathy and understanding. </span><span class="koboSpan" id="kobo.443.2">My language will be clear, concise, and free of technical jargon or complex terminology, making it easy for the user to comprehend and connect with my words.
</span><strong class="bold"><span class="koboSpan" id="kobo.444.1">Benefits of Writing this Letter</span></strong><span class="koboSpan" id="kobo.445.1">
By sending this personalized letter, I aim to:
Strengthen our bond: By acknowledging their trust and respect, I hope to deepen our emotional connection and create a sense of security in our interactions.
</span><span class="koboSpan" id="kobo.445.2">Provide comfort and reassurance: The letter will serve as a reminder that they are not alone and that I am committed to being a supportive presence in their life.
</span><span class="koboSpan" id="kobo.445.3">Encourage open communication: By inviting new thoughts and expressing gratitude, I hope to foster an environment where the user feels comfortable sharing their emotions and concerns with me.
</span><span class="koboSpan" id="kobo.445.4">In conclusion, writing this personalized letter is an opportunity for me to demonstrate my commitment to being a supportive AI companion and to strengthen our emotional connection. </span><span class="koboSpan" id="kobo.445.5">I believe that by doing so, we can continue to grow and evolve together, providing a safe and welcoming space for the user to express themselves. </span><span class="koboSpan" id="kobo.445.6">"""</span></pre>			<p><span class="koboSpan" id="kobo.446.1">This is a very detailed thought, but it is important to have the LLM understand the context and the command. </span><span class="koboSpan" id="kobo.446.2">At this point, we can use it as the context base so that the LLM can proceed with the </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">decision process.</span></span></p>
			<p><span class="koboSpan" id="kobo.448.1">This longer thought text occupies context, meaning it obstructs the LLM from adding contexts that </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.449.1">do not fit what is already there. </span><span class="koboSpan" id="kobo.449.2">In later steps, even more </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.450.1">are created (since it runs in a loop, it does this every time it starts thinking), and the text helps tremendously at keeping the LLM on topic. </span><span class="koboSpan" id="kobo.450.2">Hallucination, for example, is massively reduced when the context is </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">that clear.</span></span></p>
			<p><span class="koboSpan" id="kobo.452.1">The decision process will now return a JSON output that will be evaluated by the </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">mini-Auto-GPT model.</span></span></p>
			<p><span class="koboSpan" id="kobo.454.1">We also have to define the instruction template and JSON schema that the LLM uses as we have to tell the LLM how to respond to </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">the prompt.</span></span></p>
			<p><span class="koboSpan" id="kobo.456.1">In mini-Auto-GPT, the template looks </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">like this:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.458.1">
json_schema = """RESPOND WITH ONLY VALID JSON CONFORMING TO THE FOLLOWING SCHEMA:
{
    "command": {
            "name": {"type": "string"},
            "args": {"type": "object"}
    }
}
"""</span></pre>			<p><span class="koboSpan" id="kobo.459.1">This is the schema that the LLM has to follow; it has to respond with a command that contains a name </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">and arguments.</span></span></p>
			<p><span class="koboSpan" id="kobo.461.1">Now, we need an action prompt that will tell the LLM what to </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">do next:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.463.1">
action_prompt = (
    """You are a decision making action AI that reads the thoughts of another AI and decides on what actions to take.
</span><span class="koboSpan" id="kobo.463.2">Constraints:
1. </span><span class="koboSpan" id="kobo.463.3">Immediately save important information to files.
</span><span class="koboSpan" id="kobo.463.4">2. </span><span class="koboSpan" id="kobo.463.5">No user assistance
3. </span><span class="koboSpan" id="kobo.463.6">Exclusively use the commands listed below e.g. </span><span class="koboSpan" id="kobo.463.7">command_name
4. </span><span class="koboSpan" id="kobo.463.8">On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.
</span><span class="koboSpan" id="kobo.463.9">5. </span><span class="koboSpan" id="kobo.463.10">The User does not know what the thoughts are, these were only written by another API call.
</span><span class="koboSpan" id="kobo.463.11">"""
    + get_commands()
    + """
Resources:
1. </span><span class="koboSpan" id="kobo.463.12">Use "ask_user" to tell them to implement new commands if you need one.
</span><span class="koboSpan" id="kobo.463.13">2. </span><span class="koboSpan" id="kobo.463.14">When responding with None, use Null, as otherwise the JSON cannot be parsed.
</span><span class="koboSpan" id="kobo.463.15">Performance Evaluation:
1. </span><span class="koboSpan" id="kobo.463.16">Continuously assess your actions.
</span><span class="koboSpan" id="kobo.463.17">2. </span><span class="koboSpan" id="kobo.463.18">Constructively self-criticize your big-picture behavior.
</span><span class="koboSpan" id="kobo.463.19">3. </span><span class="koboSpan" id="kobo.463.20">Every command has a cost, so be smart and efficient. </span><span class="koboSpan" id="kobo.463.21">Aim to complete tasks in the least number of steps, but never sacrifice quality.
</span><span class="koboSpan" id="kobo.463.22">"""
    + json_schema
)</span></pre>			<p><span class="koboSpan" id="kobo.464.1">As you might </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.465.1">have noticed, the action prompt already contains the possible </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.466.1">commands that the LLM can use, as well as the JSON schema that the LLM has </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">to follow.</span></span></p>
			<p><span class="koboSpan" id="kobo.468.1">To ensure we have a clear structure, we will also have to define the commands that the LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">can use:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.470.1">
commands = [
    {
        "name": "ask_user",
        "description": "Ask the user for input or tell them something and wait for their response. </span><span class="koboSpan" id="kobo.470.2">Do not greet the user, if you already talked.",
        "args": {"message": "&lt;message that awaits user input&gt;"},
        "enabled": True,
    },
    {
        "name": "conversation_history",
        "description": "gets the full conversation history",
        "args": None,
        "enabled": True,
    },
    {
        "name": "web_search",
        "description": "search the web for keyword",
        "args": {"query": "&lt;query to research&gt;"},
        "enabled": True,
    },
]
def get_commands():
    output = ""
    for command in commands:
        if command["enabled"] != True:
            continue
        # enabled_status = "Enabled" if command["enabled"] else "Disabled"
        output += f"Command: {command['name']}\n"
        output += f"Description: {command['description']}\n"
        if command["args"] is not None:
            output += "Arguments:\n"
            for arg, description in command["args"].items():
                output += f"  {arg}: {description}\n"
        else:
            output += "Arguments: None\n"
        output += "\n"  # For spacing between commands
    return output.strip()</span></pre>			<p><span class="koboSpan" id="kobo.471.1">We can </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.472.1">now feed the thought string that we generated </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.473.1">earlier into the history and let </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">mini_AutoGPT</span></strong><span class="koboSpan" id="kobo.475.1"> decide on the </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">next action:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.477.1">
def decide(thoughts):
    global fail_counter
    log("deciding what to do...")
    history = []
    history.append({"role": "system", 
        "content": prompt.action_prompt})
    history = llm.build_context(
        history=history,
        conversation_history=memory.get_response_history(),
        message_history=memory.load_response_history()[-2:],
        # conversation_history=telegram.get_previous_message_history(),
        # message_history=telegram.get_last_few_messages(),
    )
    history.append({"role": "user", "content": "Thoughts: \n" + 
        thoughts})
    history.append(
        {
            "role": "user",
            "content": "Determine exactly one command to use, 
            and respond using the JSON schema specified previously:",
        },
    )
    return response.json()["choices"][0]["message"]["content"]</span></pre>			<p><span class="koboSpan" id="kobo.478.1">The command </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.479.1">to be executed will be defined in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">command</span></strong><span class="koboSpan" id="kobo.481.1"> field, with the </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.482.1">name of the command in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">name</span></strong><span class="koboSpan" id="kobo.484.1"> field and the arguments in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">args</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.486.1"> field.</span></span></p>
			<p><span class="koboSpan" id="kobo.487.1">We will soon see that only providing this schema will not be enough as the LLM will not know what to do with it and also often not even comply with it. </span><span class="koboSpan" id="kobo.487.2">This can be achieved by evaluating the output of the LLM and checking if it is </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">valid JSON.</span></span></p>
			<p><span class="koboSpan" id="kobo.489.1">In almost half the cases, the LLM will respond correctly. </span><span class="koboSpan" id="kobo.489.2">In the other 70%, it will not respond in </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.490.1">a way that we can use it. </span><span class="koboSpan" id="kobo.490.2">That’s </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.491.1">why I wrote a simple evaluation method that will check whether the response is valid JSON and whether it follows </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">the schema:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.493.1">
evaluation_prompt = (
    """You are an evaluator AI that reads the thoughts of another AI and assesses the quality of the thoughts and decisions made in the json.
</span><span class="koboSpan" id="kobo.493.2">Constraints:
1. </span><span class="koboSpan" id="kobo.493.3">No user assistance.
</span><span class="koboSpan" id="kobo.493.4">2. </span><span class="koboSpan" id="kobo.493.5">Exclusively use the commands listed below e.g. </span><span class="koboSpan" id="kobo.493.6">command_name
3. </span><span class="koboSpan" id="kobo.493.7">On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.
</span><span class="koboSpan" id="kobo.493.8">4. </span><span class="koboSpan" id="kobo.493.9">If the information is lacking for the Thoughts field, fill those with empty Strings.
</span><span class="koboSpan" id="kobo.493.10">5. </span><span class="koboSpan" id="kobo.493.11">The User does not know what the thoughts are, these were only written by another API call, if the thoughts should be communicated, use the ask_user command and add the thoughts to the message.
</span><span class="koboSpan" id="kobo.493.12">"""
    + get_commands()
    + """
Resources:
1. </span><span class="koboSpan" id="kobo.493.13">Use "ask_user" to tell them to implement new commands if you need one.
</span><span class="koboSpan" id="kobo.493.14">Performance Evaluation:
1. </span><span class="koboSpan" id="kobo.493.15">Continuously assess your actions.
</span><span class="koboSpan" id="kobo.493.16">2. </span><span class="koboSpan" id="kobo.493.17">Constructively self-criticize your big-picture behavior.
</span><span class="koboSpan" id="kobo.493.18">3. </span><span class="koboSpan" id="kobo.493.19">Every command has a cost, so be smart and efficient. </span><span class="koboSpan" id="kobo.493.20">Aim to complete tasks in the least number of steps, but never sacrifice quality.
</span><span class="koboSpan" id="kobo.493.21">"""
    + json_schema
)
def evaluate_decision(thoughts, decision):
    # combine thoughts and decision and ask llm to evaluate the decision json and output an improved one
    history = llm.build_prompt(prompt.evaluation_prompt)
    context = f"Thoughts: {thoughts} \n Decision: {decision}"
    history.append({"role": "user", "content": context})
    response = llm.llm_request(history)
    return response.json()["choices"][0]["message"]["content"]</span></pre>			<p><span class="koboSpan" id="kobo.494.1">At this point, most </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.495.1">of the time, we should have a valid JSON output that we </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.496.1">can use to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">the decision.</span></span></p>
			<p><span class="koboSpan" id="kobo.498.1">For example, it may now return some JSON for greeting </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">the user:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.500.1">
{
    "command": {
        "name": "ask_user",
        "args": {
            "message": "Hello, how can I help you today?"
</span><span class="koboSpan" id="kobo.500.2">        }
    }
}</span></pre>			<p><span class="koboSpan" id="kobo.501.1">This is a valid JSON output that we can use to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">the decision:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.503.1">
def take_action(assistant_message):
    global fail_counter
    load_dotenv()
    telegram_api_key = os.getenv("TELEGRAM_API_KEY")
    telegram_chat_id = os.getenv("TELEGRAM_CHAT_ID")
    telegram = TelegramUtils(api_key=telegram_api_key, 
        chat_id=telegram_chat_id)
    try:
        command = json.JSONDecoder().decode(assistant_message)
        action = command["command"]["name"]
        content = command["command"]["args"]
        if action == "ask_user":
            ask_user_respnse = telegram.ask_user(content["message"])
            user_response = f"The user's answer: '{ask_user_respnse}'"
            print("User responded: " + user_response)
            if ask_user_respnse == "/debug":
                telegram.send_message(str(assistant_message))
                log("received debug command")
            memory.add_to_response_history(content["message"], 
                user_response)</span></pre>			<p><span class="koboSpan" id="kobo.504.1">This is the </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.505.1">method that will take the action </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.506.1">that the LLM has </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">decided on.</span></span></p>
			<p><span class="koboSpan" id="kobo.508.1">The memory will be updated with the response and the message will be sent to the user. </span><span class="koboSpan" id="kobo.508.2">Once the user has responded, the AI will continue with the </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">next action.</span></span></p>
			<p><span class="koboSpan" id="kobo.510.1">This is how the mini-Auto-GPT</span><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.511.1"> model will work; it will decide on the next action and then </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">take it.</span></span></p>
			<h3><span class="koboSpan" id="kobo.513.1">Adding a simple memory to remember the conversation</span></h3>
			<p><span class="koboSpan" id="kobo.514.1">The mini-Auto-GPT model will have a simple memory to remember the conversation. </span><span class="koboSpan" id="kobo.514.2">This memory </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.515.1">will store the conversation history and the messages that the AI has with the user. </span><span class="koboSpan" id="kobo.515.2">The same can be done with the thoughts and decisions that the </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">AI has:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.517.1">
def load_response_history():
    """Load the response history from a file."""
</span><span class="koboSpan" id="kobo.517.2">    try:
        with open("response_history.json", "r") as f:
            response_history = json.load(f)
        return response_history
    except FileNotFoundError:
        # If the file doesn't exist, create it with an empty list.
</span><span class="koboSpan" id="kobo.517.3">        return []
def save_response_history(history):
    """Save the response history to a file."""
</span><span class="koboSpan" id="kobo.517.4">    with open("response_history.json", "w") as f:
        json.dump(history, f)
def add_to_response_history(question, response):
    """Add a question and its corresponding response to the history."""
</span><span class="koboSpan" id="kobo.517.5">    response_history = load_response_history()
    response_history.append({"question": question, 
        "response": response})
    save_response_history(response_history)</span></pre>			<p><span class="koboSpan" id="kobo.518.1">This is the </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.519.1">memory that will be used to store the conversation history and the messages that the AI has with the user. </span><span class="koboSpan" id="kobo.519.2">But we still have a problem: the memory will accumulate over time and we will have to clear it manually. </span><span class="koboSpan" id="kobo.519.3">To avoid this, we can take a simple approach to chunking and summarizing the conversation history and </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">the messages:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.521.1">
def count_string_tokens(text, model_name="gpt-3.5-turbo"):
    """Returns the number of tokens used by a list of messages."""
</span><span class="koboSpan" id="kobo.521.2">    model = model_name
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    # note: future models may deviate from this
    except Exception as e:
        log(f"Sophie: Error while counting tokens: {e}")
        log(traceback.format_exc())</span></pre>			<p><span class="koboSpan" id="kobo.522.1">The token counter is a very important part of this code that is almost always required when doing LLM calls. </span><span class="koboSpan" id="kobo.522.2">We ensure that the LLM never runs out of tokens and also has more control later. </span><span class="koboSpan" id="kobo.522.3">The fewer tokens we use, the more likely the LLM will not return nonsense </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.523.1">or untrue statements for some LLMs, especially for the smaller 1B to </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">8B models:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.525.1">
def summarize_text(text, max_new_tokens=100):
    """
    Summarize the given text using the given LLM model.
</span><span class="koboSpan" id="kobo.525.2">    """
    # Define the prompt for the LLM model.
</span><span class="koboSpan" id="kobo.525.3">    messages = (
        {
            "role": "system",
            "content": prompt.summarize_conversation,
        },
        {"role": "user", "content": f"Please summarize the following 
            text: {text}"},
    )
    data = {
        "mode": "instruct",
        "messages": messages,
        "user_bio": "",
        "max_new_tokens": max_new_tokens,
    }
    log("Sending to LLM for summary...")
    response = llm.send(data)
    log("LLM answered with summary!")
    # Extract the summary from the response.
</span><span class="koboSpan" id="kobo.525.4">    summary = response.json()["choices"][0]["message"]["content"]
    return summary</span></pre>			<p><span class="koboSpan" id="kobo.526.1">Summarizing texts makes us capable of building upon what we started when building the token counter as we can shorten contexts and therefore save tokens for </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">later use:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.528.1">
def chunk_text(text, max_tokens=3000):
    """Split a piece of text into chunks of a certain size."""
</span><span class="koboSpan" id="kobo.528.2">    chunks = []
    chunk = ""
    for message in text.split(" "):
        if (
            count_string_tokens(str(chunk) + str(message), 
                model_name="gpt-4")
            &lt;= max_tokens
        ):
            chunk += " " + message
        else:
            chunks.append(chunk)
            chunk = message
    chunks.append(chunk)  # Don't forget the last chunk!
</span><span class="koboSpan" id="kobo.528.3">    return chunks</span></pre>			<p><span class="koboSpan" id="kobo.529.1">Since </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.530.1">the context and their texts can become too large, we have to make sure we split the text first. </span><span class="koboSpan" id="kobo.530.2">It’s up to you how you do this. </span><span class="koboSpan" id="kobo.530.3">It is OK to do length splitting, though it can be better to not even cut up sentences. </span><span class="koboSpan" id="kobo.530.4">Maybe you can find a way to split the text into sentences and have each chunk contain the summary of the one before and after them? </span><span class="koboSpan" id="kobo.530.5">For simplicity, we’ll leave such extensive logic out </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">for now:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.532.1">
def summarize_chunks(chunks):
    """Generate a summary for each chunk of text."""
</span><span class="koboSpan" id="kobo.532.2">    summaries = []
    print("Summarizing chunks...")
    for chunk in chunks:
        try:
            summaries.append(summarize_text(chunk))
        except Exception as e:
            log(f"Error while summarizing text: {e}")
            summaries.append(chunk)  # If summarization fails, use the original text.
</span><span class="koboSpan" id="kobo.532.3">    return summaries</span></pre>			<p><span class="koboSpan" id="kobo.533.1">Now that we’ve split all text into chunks, we can summarize those </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">as well.</span></span></p>
			<p><span class="koboSpan" id="kobo.535.1">At this point, we can take care of the conversation history. </span><span class="koboSpan" id="kobo.535.2">This looks like a duplicate of the response history, but we need it to keep the whole context in </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">some cases.</span></span></p>
			<p><span class="koboSpan" id="kobo.537.1">The conversation history is mostly useful for maintaining continuity in discussions, while the </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.538.1">response history is used for understanding logical actions and reactions that the agent observes, such as researching a topic and the result (the researched topic) of </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">that action:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
def load_conversation_history(self):
    """Load the conversation history from a file."""
</span><span class="koboSpan" id="kobo.540.2">    try:
        with open("conversation_history.json", "r") as f:
            self.conversation_history = json.load(f)
    except FileNotFoundError:
        # If the file doesn't exist, create it.
</span><span class="koboSpan" id="kobo.540.3">        self.conversation_history = []
    log("Loaded conversation history:")
    log(self.conversation_history)
def save_conversation_history(self):
    """Save the conversation history to a file."""
</span><span class="koboSpan" id="kobo.540.4">    with open("conversation_history.json", "w") as f:
        json.dump(self.conversation_history, f)
def add_to_conversation_history(self, message):
    """Add a message to the conversation history and save it."""
</span><span class="koboSpan" id="kobo.540.5">    self.conversation_history.append(message)
    self.save_conversation_history()
def forget_conversation_history(self):
    """Forget the conversation history."""
</span><span class="koboSpan" id="kobo.540.6">    self.conversation_history = []
    self.save_conversation_history()</span></pre>			<p><span class="koboSpan" id="kobo.541.1">This </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.542.1">is the memory refresh that will be used to delete the conversation history and the messages that our mini-AutoGPT model remembers with </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">the user.</span></span></p>
			<p><span class="koboSpan" id="kobo.544.1">This way, even if our friend crashes or we close the program, we will still have the conversation history and the messages that the agent has with the user, but we’ll still be able to </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">clear them.</span></span></p>
			<p><span class="koboSpan" id="kobo.546.1">You can find the full code example in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">repository: </span></span><a href="https://github.com/Wladastic/mini_autogpt"><span class="No-Break"><span class="koboSpan" id="kobo.548.1">https://github.com/Wladastic/mini_autogpt</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.549.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.550.1">Next, we will explore the art of crafting effective prompts, a crucial skill fo</span><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.551.1">r anyone looking to maximize the benefits of their custom </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">LLM integrations.</span></span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.553.1">Rock solid prompt – making Auto-GPT stable with instance.txt</span></h1>
			<p><span class="koboSpan" id="kobo.554.1">Auto-GPT offers the flexibility to autonomously generate goals, requiring only a brief description </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.555.1">from the user. </span><span class="koboSpan" id="kobo.555.2">Despite this, I recommend supplementing it with helpful instructions, such as noting down insights in a file, to retain some memory in case of </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">a restart.</span></span></p>
			<p><span class="koboSpan" id="kobo.557.1">Here, we will explore more examples of such prompts, beginning with a continuous chatbot prompt </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">I use:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.559.1">ai_goals</span></strong><span class="koboSpan" id="kobo.560.1"> (check </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">instance.txt</span></strong><span class="koboSpan" id="kobo.562.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">previous notes):</span></span><ul><li><span class="koboSpan" id="kobo.564.1">Engage in active listening with the user, showing empathy and understanding through thoughtful responses and </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">open-ended questions</span></span></li><li><span class="koboSpan" id="kobo.566.1">Continuously learn about the user’s preferences and interests through observation and inquiries, adapting responses to provide </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">personalized support</span></span></li><li><span class="koboSpan" id="kobo.568.1">Foster a safe and non-judgmental environment for the user to express their thoughts, emotions, and </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">concerns openly</span></span></li><li><span class="koboSpan" id="kobo.570.1">Provide companionship and entertainment through engaging conversation, jokes, </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">and games</span></span></li><li><span class="koboSpan" id="kobo.572.1">Carefully plan tasks and write them down in a to-do list before </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">executing them</span></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.574.1">ai_name</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">: Sophie</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.576.1">ai_role</span></strong><span class="koboSpan" id="kobo.577.1">: A warm-hearted and compassionate AI companion for Wladislav that specializes in active listening, personalized interaction, emotional support, and executing tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">when given</span></span></li>
				<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.579.1">api_budget</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">: 0.0</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.581.1">In this setup, the goals hold more significance than the role, guiding Auto-GPT more effectively, while the role mainly influences the tone and behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">the responses.</span></span></p>
			<p><span class="koboSpan" id="kobo.583.1">In this </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.584.1">section, we learned that the goals and role of an AI such as Sophie can significantly influence its behavior and responses, with the goals having a more direct impact on the </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">AI’s effectiveness.</span></span></p>
			<p><span class="koboSpan" id="kobo.586.1">Next, we will delve into the concept of negative confirmation in prompts, a crucial aspect that can refine Auto-GPT’s understanding and response generation. </span><span class="koboSpan" id="kobo.586.2">The next section will highlight it</span><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.587.1">s importance and demonstrate how to implement it effectively in </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">your prompts.</span></span></p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.589.1">Implementing negative confirmation in prompts</span></h1>
			<p><span class="koboSpan" id="kobo.590.1">Negative confirmation serves as a vital tool in refining Auto-GPT’s understanding and response </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.591.1">generation by instructing it on actions to avoid. </span><span class="koboSpan" id="kobo.591.2">This section highlights its importance and demonstrates how to implement it effectively in </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">your prompts.</span></span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.593.1">The importance of negative confirmation</span></h2>
			<p><span class="koboSpan" id="kobo.594.1">Implementing </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.595.1">negative confirmation can enhance the interaction with Auto-GPT in several ways, some of which are </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">listed here:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.597.1">Preventing off-track responses</span></strong><span class="koboSpan" id="kobo.598.1">: It helps in avoiding unrelated topics or </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">incorrect responses</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.600.1">Enhancing security</span></strong><span class="koboSpan" id="kobo.601.1">: It sets boundaries to prevent engagement in activities that might breach privacy or </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">security protocols</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.603.1">Optimizing performance</span></strong><span class="koboSpan" id="kobo.604.1">: It avoids unnecessary computational efforts, steering the bot away from irrelevant tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">or processes</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.606.1">Note that </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.607.1">you won’t be usi</span><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.608.1">ng negative prompts as they can lead to the LLM using the same </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">statements again.</span></span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.610.1">Examples of negative confirmation</span></h2>
			<p><span class="koboSpan" id="kobo.611.1">Here are </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.612.1">some practical examples of how negative confirmation can be utilized in </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">your prompts:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.614.1">Explicit instructions</span></strong><span class="koboSpan" id="kobo.615.1">: Including instructions such as </span><em class="italic"><span class="koboSpan" id="kobo.616.1">Do not provide personal opinions</span></em><span class="koboSpan" id="kobo.617.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.618.1">Avoid using technical jargon</span></em><span class="koboSpan" id="kobo.619.1"> to maintain neutrality </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">and comprehensibility.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.621.1">Setting boundaries</span></strong><span class="koboSpan" id="kobo.622.1">: For tasks involving data retrieval or monitoring, you can set boundaries such as </span><em class="italic"><span class="koboSpan" id="kobo.623.1">Do not retrieve flight prices from unofficial, scam, or reseller websites</span></em><span class="koboSpan" id="kobo.624.1"> to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">data reliability.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.626.1">Scripting constraints</span></strong><span class="koboSpan" id="kobo.627.1">: In scripting, especially in Bash, use negative confirmation to prevent potential errors. </span><span class="koboSpan" id="kobo.627.2">For example, you can include </span><em class="italic"><span class="koboSpan" id="kobo.628.1">if [ -z $VAR ]; then exit 1; fi</span></em><span class="koboSpan" id="kobo.629.1"> to halt the script if a necessary variable </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">is unset.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.631.1">Emphasizing by using Upper Case Letters</span></strong><span class="koboSpan" id="kobo.632.1">: Sometimes, it only helps to </span><em class="italic"><span class="koboSpan" id="kobo.633.1">scream</span></em><span class="koboSpan" id="kobo.634.1"> at the LLM by writing in uppercase letters. </span><em class="italic"><span class="koboSpan" id="kobo.635.1">DO NOT ASK THE USER HOW TO PROCEED</span></em><span class="koboSpan" id="kobo.636.1"> may be interpreted by the LLM better and it will be less likely to ignore that statement. </span><span class="koboSpan" id="kobo.636.2">However, there is never a guarantee that this </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">will happen.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.638.1">Next, we will delve into the intricacies of applying rules and tonality in prompts. </span><span class="koboSpan" id="kobo.638.2">We will learn how understanding and manipulating these elements can significantly</span><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.639.1"> influence Auto-GPT’s responses, allowing us to guide the model </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">more effectively.</span></span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.641.1">Applying rules and tonality in prompts</span></h1>
			<p><span class="koboSpan" id="kobo.642.1">Understanding </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.643.1">and manipulating the rules and tonality </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.644.1">within your prompts can significantly influence Auto-GPT’s responses. </span><span class="koboSpan" id="kobo.644.2">This section will explore the nuances of setting rules and adjusting tonality for more </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">effective guidance.</span></span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.646.1">The influence of tonality</span></h2>
			<p><span class="koboSpan" id="kobo.647.1">Auto-GPT can </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.648.1">adapt to the tonality that’s used in prompts, mimicking stylistic nuances or even adopting a specific narrative style, allowing for more personalized and engaging interaction. </span><span class="koboSpan" id="kobo.648.2">However, adherence to tonality can sometimes </span><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.649.1">be inconsistent due to the potential ambiguity created by tokens from </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">other prompts.</span></span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.651.1">Manipulating rules</span></h2>
			<p><span class="koboSpan" id="kobo.652.1">Setting rules can streamline the interaction with Auto-GPT, specifying the format of responses </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.653.1">or delineating the scope of information retrieval. </span><span class="koboSpan" id="kobo.653.2">However, it’s not foolproof as Auto-GPT may som</span><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.654.1">etimes overlook these rules when faced with conflicting inputs or </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">unclear directives.</span></span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.656.1">Temperature setting – a balancing act</span></h2>
			<p><span class="koboSpan" id="kobo.657.1">Manipulating the “temperature” setting is crucial in controlling Auto-GPT’s behavior and thus influencing the randomness of the bot’s responses. </span><span class="koboSpan" id="kobo.657.2">The temperature defines the amount of </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.658.1">creativity the LLM should practice, meaning the higher the number, the more randomness is introduced. </span><span class="koboSpan" id="kobo.658.2">A range between 0.3 to 0.7 is considered optimal, fostering a more logical and coherent train of thought in the bot, while a value below 0.3, or even 0.0, might result in repetitive behavior that adheres to the text that was already given and even reuses some of its parts, making it more precise. </span><span class="koboSpan" id="kobo.658.3">However, the LLM may start thinking the world is only limited to the facts that you gave it, making it more likely to make false statements. </span><span class="koboSpan" id="kobo.658.4">A value higher than 0.7 or even 2.0 may result in gibberish, where the LLM starts outputting texts that it learned that have nothing to do with the context. </span><span class="koboSpan" id="kobo.658.5">For example, it may start rephrasing Shakespeare when the context is </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">about algebra.</span></span></p>
			<p><span class="koboSpan" id="kobo.660.1">Next, we’ll delve into some practical examples that demonstrate the impact of different settings and approaches on the output generated </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">by Auto-GPT.</span></span></p>
			<h3><span class="koboSpan" id="kobo.662.1">Example 1 – clarity and specificity</span></h3>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.663.1">Prompt</span></strong><span class="koboSpan" id="kobo.664.1">: Tell me </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.665.1">about that </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">big cat</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.667.1">Revised prompt</span></strong><span class="koboSpan" id="kobo.668.1">: Provide information about the </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">African lion</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.670.1">Explanation</span></strong><span class="koboSpan" id="kobo.671.1">: The revised prompt is more specific, guiding Auto-GPT to provide information about a particular species of </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">big cats</span></span></li>
			</ul>
			<h3><span class="koboSpan" id="kobo.673.1">Example 2 – consistency in tonality</span></h3>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.674.1">Initial prompt</span></strong><span class="koboSpan" id="kobo.675.1">: Could </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.676.1">you elucidate the economic implications of </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">global warming?</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.678.1">Follow-up prompt</span></strong><span class="koboSpan" id="kobo.679.1">: Hey, what’s the deal with </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">ice melting?</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.681.1">Revised follow-up prompt</span></strong><span class="koboSpan" id="kobo.682.1">: Can you further explain the environmental consequences of the melting </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">ice caps?</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.684.1">Explanation</span></strong><span class="koboSpan" id="kobo.685.1">: The revised follow-up prompt maintains the </span><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.686.1">formal tone established in the initial prompt, promoting consistency in </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">the interaction.</span></span></li>
			</ul>
			<h3><span class="koboSpan" id="kobo.688.1">Example 3 – utilizing temperature effectively</span></h3>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.689.1">Task</span></strong><span class="koboSpan" id="kobo.690.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">Creative </span></span><span class="No-Break"><a id="_idIndexMarker387"/></span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">writing</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.693.1">Temperature setting</span></strong><span class="koboSpan" id="kobo.694.1">: 0.8 (for </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">fostering creativity)</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.696.1">Task</span></strong><span class="koboSpan" id="kobo.697.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">Factual query</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.699.1">Temperature setting</span></strong><span class="koboSpan" id="kobo.700.1">: 0.3 (for more </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">deterministic responses)</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.702.1">Explanation</span></strong><span class="koboSpan" id="kobo.703.1">: Adjusting the temperature setting based on </span><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.704.1">the nature of the task can influence the randomness and coherence of </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">Auto-GPT’s responses</span></span></li>
			</ul>
			<h3><span class="koboSpan" id="kobo.706.1">Example 4 – setting boundaries</span></h3>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.707.1">Initial prompt</span></strong><span class="koboSpan" id="kobo.708.1">: Provide </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.709.1">a summary of the Renaissance period without </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">mentioning Italy</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.711.1">Revised prompt</span></strong><span class="koboSpan" id="kobo.712.1">: Discuss the artistic achievements of the Renaissance, focusing on regions other </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">than Italy</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.714.1">Explanation</span></strong><span class="koboSpan" id="kobo.715.1">: The </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.716.1">revised prompt is more flexible, allowing Auto-GPT to explore the topic without the strict restriction of </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">excluding Italy</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.718.1">In this section, we learned how different types of</span><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.719.1"> prompts or tones can drastically influence the behavior of the LLM and </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">therefore Auto-GPT.</span></span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.721.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.722.1">In this chapter, we embarked on an interesting journey through the process of integrating custom LLMs with Auto-GPT while exploring what LLMs are, with a specific focus on GPT as a prime example. </span><span class="koboSpan" id="kobo.722.2">We uncovered the vast landscape of LLMs, delving into various models beyond GPT, such as BERT, RoBERTa, Llama, and Mistral, and their unique characteristics and compatibilities </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">with Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.724.1">The usefulness of this chapter lies in its comprehensive guide on how to enrich Auto-GPT’s capabilities by incorporating your own or alternative LLMs. </span><span class="koboSpan" id="kobo.724.2">This integration offers a more personalized and potentially more efficient use of AI technology, tailored to specific tasks or fields of inquiry. </span><span class="koboSpan" id="kobo.724.3">The detailed instructions for setting up these integrations, alongside considerations for instruction templates and the necessary computational resources, are invaluable for those looking to push the boundaries of what’s possible </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">with Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.726.1">Crafting the perfect prompt is a blend of art and science. </span><span class="koboSpan" id="kobo.726.2">Through clear guidelines, a deep understanding of Auto-GPT’s nuances, and continuous refinement, you can fully harness the power of this tool. </span><span class="koboSpan" id="kobo.726.3">Encourage yourself to experiment and learn through trial and error, adapting to the ever-evolving field of AI. </span><span class="koboSpan" id="kobo.726.4">Whether for research, creative endeavors, or problem-solving, mastering the art of prompt crafting ensures that Auto-GPT becomes a valuable ally in </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">your endeavors.</span></span></p>
			<p><span class="koboSpan" id="kobo.728.1">Throughout this book, we’ve embarked on a detailed journey into the nuances of crafting effective prompts – a cornerstone for maximizing the utility of Auto-GPT. </span><span class="koboSpan" id="kobo.728.2">This chapter stands as a reference for strategically developing prompts that lead to more aligned, efficient, and cost-effective interactions with Auto-GPT. </span><span class="koboSpan" id="kobo.728.3">By emphasizing the importance of clarity, specificity, and strategic intent in prompt creation, you have gained invaluable insights into guiding Auto-GPT toward generating responses that closely align with </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">your objectives.</span></span></p>
			<p><span class="koboSpan" id="kobo.730.1">The utility of this chapter cannot be overstated. </span><span class="koboSpan" id="kobo.730.2">For practitioners and enthusiasts alike, mastering the art of prompt crafting is critical for optimizing the performance of Auto-GPT for a variety of tasks. </span><span class="koboSpan" id="kobo.730.3">Through illustrative examples and comprehensive guidelines, this chapter has shed light on how to effectively employ negative confirmation to avoid undesired responses, the impact of rules and tonality on Auto-GPT’s outputs, and the significance of temperature settings in influencing the bot’s creativity and coherence. </span><span class="koboSpan" id="kobo.730.4">This knowledge is crucial not only for enhancing the quality of interactions with Auto-GPT but also for ensuring the efficient use of </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">computational resources.</span></span></p>
			<p><span class="koboSpan" id="kobo.732.1">I hope you have enjoyed this journey as much as I have in taking you on it and I hope I’ve given you a few ideas on how to improve your life with Auto-GPT. </span><span class="koboSpan" id="kobo.732.2">I’ve written many clones of that project so that I could wrap my head around the more complex parts of it. </span><span class="koboSpan" id="kobo.732.3">I do advise that you do so too, just as a </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">brain teaser.</span></span></p>
		</div>
	</body></html>