<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Natural Language Processing with TF-IDF</h1>
                </header>
            
            <article>
                
<p>In this chapter, the following recipes will be covered:</p>
<ul>
<li>Downloading the therapy bot session text dataset</li>
<li>Analyzing the therapy bot session dataset</li>
<li>Visualizing word counts in the dataset</li>
<li>Calculating sentiment analysis of text</li>
<li>Removing stop words from the text</li>
<li>Training the TF-IDF model</li>
<li>Evaluating TF-IDF model performance</li>
<li>Comparing model performance to a baseline score</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Natural language processing</strong> (<strong>NLP</strong>) is all<span> </span>over the news lately,<span> </span>and if you ask five different people, you will get ten different definitions. Recently NLP has been used to help identify bots or trolls on the internet trying to spread fake news or, even worse, tactics such as cyberbullying. In fact, recently there was a case in Spain where a student at a school was getting cyberbullied through social media accounts and it was having such a serious effect on the health of the student that the teachers started to get involved. The school reached out to researchers who were able to help identify several potential sources for the trolls using NLP methods such as TF-IDF. Ultimately, the list of potential students was presented to the school and when confronted the actual suspect admitted to being the perpetrator. The story was published in a paper titled<span> </span><em>Supervised Machine Learning for the Detection of Troll Profiles in Twitter Social Network: Application to a Real Case of Cyberbullying</em> by Patxi Galan-Garcıa, Jose Gaviria de la Puerta, Carlos Laorden Gomez, Igor Santos, and Pablo Garcıa Bringas.</p>
<p class="mce-root">This paper highlights the ability to utilize several varying methods to analyze text and develop human-like language processing. It is this methodology that incorporates NLP into machine learning, deep learning, and artificial intelligence. Having machines able to ingest text data and potentially make decisions from that same text data is the core of natural language processing. There are many algorithms that are used for NLP, such as the following:</p>
<ul>
<li class="mce-root">TF-IDF</li>
<li class="mce-root">Word2Vec</li>
<li class="mce-root">N-grams</li>
<li class="mce-root">Latent Dirichlet allocation (LDA)</li>
<li class="mce-root">Long short-term memory (LSTM)</li>
</ul>
<p>This chapter will focus on a dataset that contains conversations between an individual and a chatbot from an online therapy website. The purpose of the chatbot is to recognize conversations that need to be flagged for immediate attention to an individual rather than continued discussion with the chatbot. Ultimately, we will focus on using a TF-IDF algorithm to perform text analysis on the dataset to determine whether the chat conversation warrants a classification that needs to be escalated to an individual or not.<span> </span><strong>TF-IDF</strong><span> </span>stands for<span> </span><strong>Term Frequency-Inverse Document Frequency</strong>. This is a technique commonly used in algorithms to identify the importance of a word in a document. Additionally, TF-IDF is easy to compute especially when dealing with high word counts in documents and has the ability to measure the uniqueness of a word. This comes in handy when dealing with a chatbot data. The main goal is to quickly identify a unique word that would trigger escalation to an individual to provide immediate support.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the therapy bot session text dataset</h1>
                </header>
            
            <article>
                
<p class="mce-root">This section will focus on downloading and setting up the dataset that will be used for NLP in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The dataset that we will use in this chapter is based on interactions between a therapy bot and visitors to an online therapy website. It contains 100 interactions and each interaction is tagged as either <kbd>escalate</kbd> or <kbd>do_not_escalate</kbd>. If the discussion warrants a more serious conversation, the bot will tag the discussion as <kbd>escalate</kbd> to an individual. Otherwise, the bot will continue the discussion with the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps for downloading the chatbot data.</p>
<ol>
<li>Access the dataset from the following GitHub repository: <a href="https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data">https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data</a></li>
<li>Once you arrive at the repository, right-click on the file seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1510 image-border" src="assets/5333b70f-7e8a-43ff-a537-87c7392b5af1.png" style="width:162.50em;height:72.75em;"/></div>
<ol start="3">
<li>Download <kbd>TherapyBotSession.csv</kbd> <span>and save to the same local directory as the Jupyter notebook</span> <kbd>SparkSession</kbd><span><span>.</span></span></li>
</ol>
<p> </p>
<ol start="4">
<li>Access the dataset through the Jupyter notebook using the following script to build the <kbd>SparkSession</kbd> called <kbd>spark</kbd>, as well as to assign the dataset to a dataframe in Spark, called <kbd>df</kbd>:</li>
</ol>
<pre style="padding-left: 60px">spark = SparkSession.builder \<br/>        .master("local") \<br/>        .appName("Natural Language Processing") \<br/>        .config("spark.executor.memory", "6gb") \<br/>        .getOrCreate()<br/>df = spark.read.format('com.databricks.spark.csv')\<br/>     .options(header='true', inferschema='true')\<br/>     .load('TherapyBotSession.csv')  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>This section explains how the chatbot data makes its way into our Jupyter notebook.</span></p>
<ol>
<li><span>The contents of the dataset can be seen by clicking on <span class="packt_screen">TherapyBotSession.csv</span> on the repository as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1511 image-border" src="assets/b237f63c-d351-495b-bb66-37dcc5dac0d7.png" style="width:166.17em;height:95.83em;"/></div>
<ol start="2">
<li>Once the dataset is downloaded, it can be uploaded and converted into a dataframe, <kbd>df</kbd>. The dataframe can be viewed by executing <kbd>df.show()</kbd>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b869c90a-6637-4115-b795-9a9ed5a4af32.png" style="width:34.00em;height:32.17em;"/></div>
<ol start="3">
<li>There are 3 main fields that are of particular interest to us from the dataframe:
<ol>
<li><kbd>id</kbd>: the unique id of each transaction between a visitor to the website and the chatbot.</li>
<li><kbd>label</kbd>: since this is a supervised modeling approach where we know the outcome that we are trying to predict, each transaction has been classified as either <kbd>escalate</kbd> or <kbd>do_not_escalate</kbd>. This field will be used during the modeling process to train the text to identify words that would classify falling under one of these two scenarios.</li>
<li><kbd>chat</kbd>: lastly we have the <kbd>chat</kbd> text from the visitor on the website that our model will classify.</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The dataframe, <kbd>df</kbd>, has some additional columns, <kbd>_c3</kbd>, <kbd>_c4</kbd>, <kbd>_c5</kbd>, and <kbd>_c6</kbd> that will not be used in the model and therefore, can be excluded from the dataset using the following script:</p>
<pre>df = df.select('id', 'label', 'chat')<br/>df.show()</pre>
<p>The output of the script can be seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ff0dad66-0172-447e-897f-7d74f9138643.png" style="width:27.83em;height:26.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the therapy bot session dataset</h1>
                </header>
            
            <article>
                
<p><span>It is always important to first analyze any dataset before applying models on that same dataset</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require importing <kbd>functions</kbd> from <kbd>pyspark.sql</kbd> to be performed on our dataframe.</p>
<pre>import pyspark.sql.functions as F</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The following section walks through the steps to profile the text data.</p>
<ol>
<li>Execute the following script to group the <kbd>label</kbd> column and to generate a count distribution:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy("label") \<br/>   .count() \<br/>   .orderBy("count", ascending = False) \<br/>   .show()</pre>
<ol start="2">
<li>Add a new column,<span> </span><kbd>word_count</kbd>, to the dataframe,<span> </span><kbd>df</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">import pyspark.sql.functions as F<br/>df = df.withColumn('word_count', F.size(F.split(F.col('response_text'),' ')))</pre>
<ol start="3">
<li>Aggregate the average word count,<span> </span><kbd>avg_word_count</kbd>, by<span> </span><kbd>label</kbd><span> </span>using the following script:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy('label')\<br/>  .agg(F.avg('word_count').alias('avg_word_count'))\<br/>  .orderBy('avg_word_count', ascending = False) \<br/>  .show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The following section explains the feedback obtained from analyzing the text data.</span></p>
<ol>
<li>It is useful to collect data across multiple rows and group the results by a dimension. In this case, the dimension is <kbd>label</kbd>. A<span> </span><kbd>df.groupby()</kbd><span> </span>function is used to measure the count of 100 therapy transactions online distributed by<span> </span><kbd>label</kbd>. We can see that there is a <kbd>65</kbd>:<kbd>35</kbd> distribution of <kbd>do_not_escalate</kbd> to <kbd>escalate</kbd> as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9c270c9f-e510-4548-878c-cdd70926792c.png" style="width:27.08em;height:10.75em;"/></div>
<ol start="2">
<li>A new column,<span> </span><kbd>word_count</kbd>, is created to calculate how many words are used in each of the 100 transactions between the chatbot and the online visitor. The newly created column,<span> </span><kbd>word_count</kbd>, can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4d3b4867-b4cc-4174-b011-28f25f2622ac.png" style="width:30.17em;height:22.33em;"/></div>
<ol start="3">
<li>Since the <kbd>word_count</kbd> is now added to the dataframe, it can be aggregated to calculate the average word count by<span> </span><kbd>label</kbd>. Once this is performed, we can see that <kbd>escalate</kbd> conversations on average are more than twice as long as <kbd>do_not_escalate</kbd> conversations, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a4bb5dfb-f9cf-4c26-b408-3ef612192169.png" style="width:33.83em;height:11.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing word counts in the dataset</h1>
                </header>
            
            <article>
                
<p>A picture is worth a thousand words and this section will set out to prove that. Unfortunately, Spark does not have any inherent plotting capabilities as of version 2.2. In order to plot values in a dataframe, we must convert to <kbd>pandas</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require importing <kbd>matplotlib</kbd> for plotting:</p>
<pre>import matplotlib.pyplot as plt<br/>%matplotlib inline</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to convert the Spark dataframe into a visualization that can be seen in the Jupyter notebook. </p>
<ol>
<li>Convert Spark dataframe to a <kbd>pandas</kbd> dataframe using the following script:</li>
</ol>
<pre style="padding-left: 60px">df_plot = df.select('id', 'word_count').toPandas()</pre>
<ol start="2">
<li>Plot the dataframe using the following script:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>df_plot.set_index('id', inplace=True)<br/>df_plot.plot(kind='bar', figsize=(16, 6))<br/>plt.ylabel('Word Count')<br/>plt.title('Word Count distribution')<br/>plt.show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how the Spark dataframe is converted to <kbd>pandas</kbd> and then plotted.</p>
<ol>
<li>A subset of the Spark dataframe is collected and converted to <kbd>pandas</kbd> using the <kbd>toPandas()</kbd> method in Spark.</li>
<li>That subset of data is then plotted using matplotlib setting the y-values to be <kbd>word_count</kbd> and the x-values to be the <kbd>id</kbd> as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/07a5e35f-d769-4487-bd3f-84995ef9ace5.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>There are other plotting capabilities in Python other than <kbd>matplotlib</kbd> such as <kbd>bokeh</kbd>, <kbd>plotly</kbd>, and <kbd>seaborn</kbd>.</p>
<p>To learn more about <kbd>bokeh</kbd>, visit the following website:</p>
<p><a href="https://bokeh.pydata.org/en/latest/">https://bokeh.pydata.org/en/latest/</a></p>
<p>To learn more about <kbd>plotly</kbd>, visit the following website:</p>
<p><a href="https://plot.ly/">https://plot.ly/</a></p>
<p>To learn more about <kbd>seaborn</kbd>, visit the following website:</p>
<p><a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating sentiment analysis of text</h1>
                </header>
            
            <article>
                
<p>Sentiment analysis is the ability to derive tone and feeling behind a word or series of words. This section will utilize techniques in python to calculate a sentiment analysis score from the 100 transactions in our dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require using functions and data types within PySpark. Additionally, we well importing the <kbd>TextBlob</kbd> library for sentiment analysis. In order to use SQL and data type functions within PySpark, the following must be imported:</p>
<pre>from pyspark.sql.types import FloatType </pre>
<p>Additionally, in order to use <kbd>TextBlob</kbd>, the following library must be imported:</p>
<pre>from textblob import TextBlob</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The following section walks through the steps to apply sentiment score to the dataset.</p>
<ol start="1">
<li>Create a sentiment score function, <kbd>sentiment_score</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">from textblob import TextBlob<br/>def sentiment_score(chat):<br/>    return TextBlob(chat).sentiment.polarity</pre>
<ol start="2">
<li><span>Apply <kbd>sentiment_score</kbd> to each conversation response in the dataframe using the following script:</span></li>
<li>Create a <kbd>lambda</kbd> function, called <kbd>sentiment_score_udf</kbd>, that maps <kbd>sentiment_score</kbd> into a user-defined function within Spark, <kbd>udf</kbd>, to each transaction and specifies the output type of <kbd>FloatType()</kbd> as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.sql.types import FloatType<br/>sentiment_score_udf = F.udf(lambda x: sentiment_score(x), FloatType())</pre>
<ol start="4">
<li>Apply the function, <kbd>sentiment_score_udf</kbd>, to each <kbd>chat</kbd> column in the dataframe as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">df = df.select('id', 'label', 'chat','word_count',<br/>                   sentiment_score_udf('chat').alias('sentiment_score'))</pre>
<ol start="5">
<li>Calculate the average sentiment score, <kbd>avg_sentiment_score</kbd>, by <kbd>label</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy('label')\<br/>     .agg(F.avg('sentiment_score').alias('avg_sentiment_score'))\<br/>     .orderBy('avg_sentiment_score', ascending = False) \<br/>     .show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how a Python function is converted into a user-defined function, <kbd>udf</kbd>, within Spark to apply a sentiment analysis score to each column in the dataframe.</p>
<ol>
<li class="mce-root"><kbd>Textblob</kbd> <span>is a sentiment analysis library in Python. It can calculate the sentiment score from a method called</span> <kbd>sentiment.polarity</kbd> <span>that is scored from -1 (very negative) to +1 (very positive) with 0 being neutral. Additionally,</span> <kbd>Textblob</kbd> <span>can measure subjectivity from 0 (very objective) to 1 (very subjective); although, we will not be measuring subjectivity in this chapter.</span></li>
<li class="mce-root">There are a couple of steps to applying a Python function to Spark dataframe:
<ol>
<li><kbd>Textblob</kbd> <span>is imported and a function called</span> <kbd>sentiment_score</kbd> <span>is applied to the</span> <kbd>chat</kbd> <span>column to generate the sentiment polarity of each bot conversation in a new column, also called</span> <kbd>sentiment_score</kbd><span>.</span></li>
<li><span>A Python function cannot be directly applied to a Spark dataframe without first going through a user-defined function transformation,</span> <kbd>udf</kbd><span>, within Spark.</span></li>
<li><span>Additionally, the output of the function must also be explicitly stated, whether it be an integer or float data type. In our situation, we explicitly state that the output of the function will be using the</span> <kbd>FloatType() from pyspark.sql.types</kbd><span>. Finally, the sentiment is applied across each row using a</span> <kbd>lambda</kbd> <span>function within the</span> <kbd>udf</kbd> <span>sentiment score function, called</span> <kbd>sentiment_score_udf</kbd><span><span><span>.</span></span></span></li>
</ol>
</li>
</ol>
<ol start="3">
<li><span>The updated dataframe with the newly created field,</span><kbd>sentiment score</kbd><span>, can be seen by executing </span><kbd>df.show()</kbd>, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1638 image-border" src="assets/c71a115d-071e-44ba-a103-39370ba0b7d1.png" style="width:114.83em;height:98.17em;"/></div>
<ol start="4">
<li>Now that the <kbd>sentiment_score</kbd> is calculated for each response from the chat conversation, we can denote a value range of -1 (very negative polarity) to +1 (very positive polarity) for each row. Just as we did with counts and average word count, we can compare whether <kbd>escalate</kbd> conversations are more positive or negative in sentiment than <kbd>do_not_escalate</kbd> conversations on average. We can calculate an average sentiment score, <kbd>avg_sentiment_score</kbd>, by <kbd>label</kbd> as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c0f5585d-bb43-490e-9d4f-b73cb674d47e.png" style="width:36.08em;height:10.08em;"/></div>
<ol start="5">
<li>Initially, it would make sense to assume that <kbd>escalate</kbd> conversations would be more negative from a polarity score than <kbd>do_not_escalate</kbd>. We actually find that <kbd>escalate</kbd> is slightly more positive in polarity than <kbd>do_not_escalate</kbd>; however, both are pretty neutral as they are close to 0.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about the <kbd>TextBlob</kbd> library, visit the following website:</p>
<p><a href="http://textblob.readthedocs.io/en/dev/">http://textblob.readthedocs.io/en/dev/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stop words from the text</h1>
                </header>
            
            <article>
                
<p><span>A stop word is a very common word used in the English language and is often removed from common NLP techniques because they can be distracting. Common stop word would be words such as</span><span> </span><em>the</em><span> </span><span>or</span><span> </span><em>and</em><span>. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section requires importing the following libraries:</p>
<pre>from pyspark.ml.feature import StopWordsRemover <br/>from pyspark.ml import Pipeline</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to remove stop words.</p>
<ol>
<li>Execute the following script to extract each word in <kbd>chat</kbd> into a string within an array:</li>
</ol>
<pre style="padding-left: 60px">df = df.withColumn('words',F.split(F.col('chat'),' '))</pre>
<ol start="2">
<li>Assign a list<span> </span>of<span> </span>common words to a variable, <kbd>stop_words</kbd>, that will be considered stop words using the following script:</li>
</ol>
<pre style="padding-left: 60px">stop_words = ['i','me','my','myself','we','our','ours','ourselves',<br/>'you','your','yours','yourself','yourselves','he','him',<br/>'his','himself','she','her','hers','herself','it','its',<br/>'itself','they','them','their','theirs','themselves',<br/>'what','which','who','whom','this','that','these','those',<br/>'am','is','are','was','were','be','been','being','have',<br/>'has','had','having','do','does','did','doing','a','an',<br/>'the','and','but','if','or','because','as','until','while',<br/>'of','at','by','for','with','about','against','between',<br/>'into','through','during','before','after','above','below',<br/>'to','from','up','down','in','out','on','off','over','under',<br/>'again','further','then','once','here','there','when','where',<br/>'why','how','all','any','both','each','few','more','most',<br/>'other','some','such','no','nor','not','only','own','same',<br/>'so','than','too','very','can','will','just','don','should','now']</pre>
<ol start="3">
<li>Execute the following script to import the <kbd>StopWordsRemover</kbd><span> </span>function from PySpark and configure the input and output columns, <kbd>words</kbd> and <kbd>word without stop</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.feature import StopWordsRemover <br/><br/>stopwordsRemovalFeature = StopWordsRemover(inputCol="words", <br/>                   outputCol="words without stop").setStopWords(stop_words)</pre>
<ol start="4">
<li>Execute the following script to import Pipeline and define the <kbd>stages</kbd> for the stop word transformation process that will be applied to the dataframe:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from pyspark.ml import Pipeline<br/><br/>stopWordRemovalPipeline = Pipeline(stages=[stopwordsRemovalFeature])<br/>pipelineFitRemoveStopWords = stopWordRemovalPipeline.fit(df)</pre>
<ol start="5">
<li>Finally, apply the stop word removal transformation, <kbd>pipelineFitRemoveStopWords</kbd>, to the dataframe, <kbd>df</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">df = pipelineFitRemoveStopWords.transform(df)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how to remove stop words from the text.</p>
<ol>
<li>Just as we did by applying some analysis when profiling and exploring the <kbd>chat</kbd> data, we can also tweak the text of the <kbd>chat</kbd> conversation and break up each word into a separate array. This will be used to isolate stop words and remove them.</li>
<li>The new column with each word extracted as a string is called <kbd>words</kbd> and can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/10f69edb-5e82-4230-9d8d-0356a7b7df3f.png" style="width:46.42em;height:21.58em;"/></div>
<ol start="3">
<li>There are many ways to assign a group of words to a stop word list. Some of these words can be automatically downloaded and updated using a proper Python library called<span> </span><kbd>nltk</kbd>, which stands for natural language toolkit. For our purposes, we will utilize a common list of 124 stop words to generate our own list. Additional words can be easily added or removed from the list manually.</li>
</ol>
<p> </p>
<ol start="4">
<li>Stop words do not add any value to the text and will be removed from the newly created column by specifying <kbd>outputCol="words<span> </span>without stop"</kbd>. Additionally, the column that will serve as the source for the transformation is set by specifying<span> </span><kbd>inputCol = "words"</kbd>.</li>
</ol>
<ol start="5">
<li>We create a pipeline,<span> </span><kbd>stopWordRemovalPipeline</kbd>, to define the sequence of steps or <kbd>stages</kbd> that will transform the data. In this situation, the only stage that will be used to transform the data is the feature,<span> </span><kbd>stopwordsRemover</kbd>.</li>
<li>Each stage in a pipeline can have a transforming role and an estimator role. The estimator role,<span> </span><kbd>pipeline.fit(df)</kbd>, is called on to produce a transformer function called <kbd>pipelineFitRemoveStopWords</kbd>. Finally, the<span> </span><kbd>transform(df)</kbd><span> </span>function is called on the dataframe to produce an updated dataframe with a new column called<span> </span><kbd>words without stop</kbd>. We can compare both columns side by side to examine the differences as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/164af41e-b68a-4e6d-aaa4-ba4236e2f1d3.png" style="width:57.08em;height:36.42em;"/></div>
<ol start="7">
<li>The new column, <kbd>words without stop</kbd>, contains none of the strings that are considered stop words from the original column, <kbd>words</kbd>. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about stop words from <kbd>nltk</kbd>, visit the following website:</p>
<p><a href="https://www.nltk.org/data.html">https://www.nltk.org/data.html</a></p>
<p>To learn more about Spark machine learning pipelines, visit the following website:</p>
<p><a href="https://spark.apache.org/docs/2.2.0/ml-pipeline.html">https://spark.apache.org/docs/2.2.0/ml-pipeline.html</a></p>
<p>To learn more about the<span> </span><kbd>StopWordsRemover</kbd><span> </span>feature in PySpark, visit the following website:</p>
<p><a href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover">https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the TF-IDF model</h1>
                </header>
            
            <article>
                
<p>We are now ready to train our TF-IDF NLP model and see if we can classify these transactions as either <kbd>escalate</kbd> or <kbd>do_not_escalate</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require importing from <kbd>spark.ml.feature</kbd> and <kbd>spark.ml.classification</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>The following section walks through the steps to train the TF-IDF model.</span></p>
<ol start="1">
<li>Create a new user-defined function, <kbd>udf</kbd>, to define numerical values for the <kbd>label</kbd> column using the following script:</li>
</ol>
<pre style="padding-left: 60px">label = F.udf(lambda x: 1.0 if x == 'escalate' else 0.0, FloatType())<br/>df = df.withColumn('label', label('label'))</pre>
<ol start="2">
<li>Execute the following script to set the TF and IDF columns for the vectorization of the words:</li>
</ol>
<pre style="padding-left: 60px">import pyspark.ml.feature as feat<br/>TF_ = feat.HashingTF(inputCol="words without stop", <br/>                     outputCol="rawFeatures", numFeatures=100000)<br/>IDF_ = feat.IDF(inputCol="rawFeatures", outputCol="features")</pre>
<ol start="3">
<li>Set up a pipeline,<span> </span><kbd>pipelineTFIDF</kbd>, to set the sequence of stages for<span> </span><kbd>TF_</kbd><span> </span>and<span> </span><kbd>IDF_</kbd><span> </span>using the following script:</li>
</ol>
<pre style="padding-left: 60px">pipelineTFIDF = Pipeline(stages=[TF_, IDF_])</pre>
<ol start="4">
<li> Fit and transform the IDF estimator onto the dataframe, <kbd>df</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">pipelineFit = pipelineTFIDF.fit(df)<br/>df = pipelineFit.transform(df)</pre>
<ol start="5">
<li>Split the dataframe into a 75:25 split for model evaluation purposes using the following script:</li>
</ol>
<pre style="padding-left: 60px">(trainingDF, testDF) = df.randomSplit([0.75, 0.25], seed = 1234)</pre>
<ol start="6">
<li>Import and configure a classification model, <kbd>LogisticRegression</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.classification import LogisticRegression<br/>logreg = LogisticRegression(regParam=0.25)</pre>
<ol start="7">
<li>Fit the logistic regression model, <kbd>logreg</kbd>, onto the training dataframe, <kbd>trainingDF.</kbd> A new dataframe, <kbd>predictionDF</kbd>, is created based on the<span> </span><kbd>transform()</kbd><span> </span>method from the<span> </span>logistic<span> </span>regression model, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">logregModel = logreg.fit(trainingDF)<br/>predictionDF = logregModel.transform(testDF)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The following section explains to effectively train a TF-IDF NLP model.</p>
<ol>
<li>It is ideal to have labels in a numerical format rather than a categorical form as the model is able to interpret numerical values while classifying outputs between 0 and 1. Therefore, all labels under the <kbd>label</kbd> column are converted to a numerical <kbd>label</kbd> of <span class="packt_screen">0.0</span> or <span class="packt_screen">1.0</span>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/41d1c54e-e6c8-43c9-9027-0d4cad7e3fcc.png" style="width:41.92em;height:28.92em;"/></div>
<ol start="2">
<li>TF-IDF models require a two-step approach by importing both <kbd>HashingTF</kbd> and <kbd>IDF</kbd> from<span> </span><kbd>pyspark.ml.feature</kbd><span> </span>to handle separate tasks. The first task merely involves importing both<span> </span><kbd>HashingTF</kbd><span> </span>and<span> </span><kbd>IDF</kbd><span> </span>and assigning values for the input and subsequent output columns. The<span> </span><kbd>numfeatures</kbd><span> parameter is set to 100,000 to ensure that it is larger than the distinct number of words in the dataframe. If <kbd>numfeatures</kbd> were to be </span><span><span>than the distinct word count, the model would be inaccurate.</span></span></li>
</ol>
<p> </p>
<ol start="3">
<li>As stated earlier, each step of the pipeline contains a transformation process and an estimator process. The pipeline,<span> </span><kbd>pipelineTFIDF</kbd>, is configured to order the sequence of steps where<span> </span><kbd>IDF</kbd><span> </span>will follow<span> </span><kbd>HashingTF</kbd>.</li>
<li><kbd>HashingTF</kbd><span> </span>is used to transform the <kbd>words without stop</kbd> into vectors within a new column called<span> </span><kbd>rawFeatures</kbd>. Subsequently,<span> </span><kbd>rawFeatures</kbd><span> </span>will then be consumed by<span> </span><kbd>IDF</kbd><span> </span>to estimate the size and fit the dataframe to produce a new column called<span> </span><kbd>features</kbd>, as seen in the following screenshot: </li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/988894fc-443a-47ba-a01b-b3154e088e6a.png" style="width:39.08em;height:35.92em;"/></div>
<ol start="5">
<li>For training purposes, our dataframe will be conservatively split into a <kbd>75</kbd>:<kbd>25</kbd> ratio with a random seed set at <kbd>1234</kbd>.</li>
</ol>
<p> </p>
<ol start="6">
<li>Since our main goal is to classify each conversation as either<span> </span><kbd>escalate</kbd><span> </span>for escalation or<span> </span><kbd>do_not_escalate</kbd><span> </span>for continued bot chat, we can use a traditional classification algorithm such as a logistic regression model from the PySpark library. The logistic regression model is configured with a regularization parameter, <kbd>regParam</kbd>, of 0.025. We use the parameter to slightly improve the model by minimizing overfitting at the expense of a little bias. </li>
<li>The logistic regression model is trained and fitted on<span> </span><kbd>trainingDF</kbd>, and then a new dataframe,<span> </span><kbd>predictionDF</kbd>, is created with the newly transformed field,<span> </span><kbd>prediction</kbd>, as seen in the following screenshot: </li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/180abef6-2eba-44ba-9da6-f2688e4996f6.png" style="width:38.33em;height:35.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While we did use the user-defined function, <kbd>udf</kbd>, to manually create a numerical label column, we also could have used a built-in feature from PySpark called <kbd>StringIndexer</kbd> to assign numerical values to categorical labels. To see <kbd>StringIndexer</kbd> in action, visit <a href="9cc3bf45-b46d-4c37-920c-87d6fdab58c2.xhtml">Chapter 5</a>, <em>Predicting Fire Department Calls with Spark ML</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about the TF-IDF model within PySpark, visit the following website:</p>
<p><a href="https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf">https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating TF-IDF model performance</h1>
                </header>
            
            <article>
                
<p>At this point, we are ready to evaluate our model's performance</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require importing the following libraries:</p>
<ul>
<li><kbd>metrics</kbd> from <kbd>sklearn</kbd> </li>
<li><kbd>BinaryClassificationEvaluator</kbd> from <kbd>pyspark.ml.evaluation</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to evaluate the TF-IDF NLP model.</p>
<ol start="1">
<li>Create a confusion matrix using the following script:</li>
</ol>
<pre style="padding-left: 60px">predictionDF.crosstab('label', 'prediction').show()</pre>
<ol start="2">
<li>Evaluate the model using <kbd>metrics</kbd> from <span><span>sklearn</span></span> with the following script:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import metrics<br/><br/>actual = predictionDF.select('label').toPandas()<br/>predicted = predictionDF.select('prediction').toPandas()<br/>print('accuracy score: {}%'.format(round(metrics.accuracy_score(actual,         predicted),3)*100))</pre>
<ol start="3">
<li>Calculate the ROC score using the following script:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/><br/>scores = predictionDF.select('label', 'rawPrediction')<br/>evaluator = BinaryClassificationEvaluator()<br/>print('The ROC score is {}%'.format(round(evaluator.evaluate(scores),3)*100))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how we use the evaluation calculations to determine the accuracy of our model.</p>
<ol start="1">
<li>A confusion matrix is helpful to quickly summarize the accuracy numbers between actual results and predicted results. Since we had a 75:25 split, we should see 25 predictions from our training dataset. We can build a build a confusion matric using the following script: <kbd>predictionDF.crosstab('label', 'prediction').show()</kbd>. The output of the script can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7ec9d6e3-2748-475a-a432-bc4427fcc349.png" style="width:33.00em;height:8.17em;"/></div>
<ol start="2">
<li>We are now at the stage of evaluating the accuracy of the model by comparing the <kbd>prediction</kbd> values against the actual <kbd>label</kbd> values. <kbd>sklearn.metrics</kbd> <span>intakes two parameters, the</span> <kbd>actual</kbd> <span>values tied to the</span> <kbd>label</kbd> <span>column, as well as the</span> <kbd>predicted</kbd> <span>values derived from the logistic regression model. </span></li>
</ol>
<div class="packt_tip"><span>Please note that once again we are converting the column values from Spark dataframes to pandas dataframes using the <kbd>toPandas()</kbd> method.</span></div>
<ol start="3">
<li><span>Two variables are created,</span> <kbd>actual</kbd> <span>and</span> <kbd>predicted</kbd><span>, and an accuracy score of <span class="packt_screen">91.7%</span> is calculated using the <kbd>metrics.accuracy_score()</kbd> function, as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39d21f82-8cb9-4106-ba4a-aee671c529b8.png"/></div>
<ol start="4">
<li>The ROC (Receiver Operating Characteristic) is often associated with a curve measuring the true positive rate against the false positive rate. The greater the area under the curve, the better. The ROC score associated with the curve is another indicator that can be used to measure the performance of the model. We can calculate the <kbd>ROC</kbd> using the <kbd>BinaryClassificationEvaluator</kbd> as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/18d393de-bf20-448d-a5c3-2a7d35018680.png" style="width:39.08em;height:6.25em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about the <kbd>BinaryClassificationEvaluator</kbd> from PySpark, visit the following website:</p>
<p><a href="https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html">https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing model performance to a baseline score</h1>
                </header>
            
            <article>
                
<p><span>While it is great that we have a high accuracy score from our model of 91.7 percent, it is also important to compare this to a baseline score. We dig deeper into this concept in this section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to calculate the baseline accuracy.</p>
<ol>
<li>Execute the following script to retrieve the mean value from the <kbd>describe()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">predictionDF.describe('label').show()</pre>
<ol start="2">
<li>Subtract <kbd>1- mean value score</kbd> to calculate baseline accuracy.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains the concept behind the baseline accuracy and how we can use it to understand the effectiveness of our model.</p>
<ol>
<li>What if every <kbd>chat</kbd> conversation was flagged for <kbd>do_not_escalate</kbd><span> </span>or vice versa. Would we have a baseline accuracy higher than 91.7 percent? The easiest way to figure this out is to run the<span> </span><kbd>describe()</kbd><span> </span>method on the<span> </span><kbd>label</kbd><span> </span>column from <kbd>predictionDF</kbd> using the following script:<span> </span><kbd>predictionDF.describe('label').show()</kbd></li>
<li>The output of the script can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ae9f9a3b-b15f-4c4d-8e6c-8b49a096f6a9.png" style="width:26.25em;height:10.42em;"/></div>
<ol start="3">
<li><span>The mean of</span><span> </span><kbd>label</kbd><span> </span><span>is at 0.2083 or ~21%, which means that a</span><span> </span><kbd>label</kbd><span> </span><span>of <span class="packt_screen">1</span> occurs only 21% of the time. Therefore, if we labeled each conversation as</span><span> </span><kbd>do_not_escalate</kbd><span>, we would be correct ~79% of the time, which is less than our model accuracy of 91.7%. </span></li>
<li>Therefore, we can say that our model performs better than a blind baseline performance model.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about the <kbd>describe()</kbd> method in a PySpark dataframe, visit the following website:</p>
<p><a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe">http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe</a></p>


            </article>

            
        </section>
    </body></html>