- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional networks (reference *LeCun[1]*, 2013), also known as **Convolutional** **neural
    networks**
  prefs: []
  type: TYPE_NORMAL
- en: or **CNNs**, are a particular type of neural network that process data with
    a grid-like topology. Examples include time-series data, which can be thought
    of as a 1D grid taking samples at regular time intervals, or image data that is
    a 2D grid of pixels. The name convolutional neural network means that the network
    employs a mathematical operation called **convolution**. Convolution is a specific
    kind of linear operation. Convolutional networks are neural networks that use
    convolution (a mathematical operation) in place of general matrix multiplication
    in at least one of their layers.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will describe the mathematical operation of convolution. Then we will
    discuss the concept of pooling and how it helps CNN. We will also look at convolution
    networks implementation in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end of this chapter, we will use TensorFlow's CNN implementation
    to classify dogs and cats from the Stanford dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Lecun[1] : [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview and the intuition of CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification with convolutional networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview and the intuition of CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNN consists of multiple layers of convolutions, polling and finally fully connected
    layers. This is much more efficient than pure feedforward networks we discussed
    in [Chapter 2](99346436-65d0-4059-81eb-e29091747df3.xhtml), *Deep Feedforward
    Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7b7f21-ff98-4900-9eed-778fde9122d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram takes images through **Convolution Layer** | **Max Pooling**
    | **Convolution** | **Max Pooling** | **Fully Connected Layers** this is an CNN
    architecture
  prefs: []
  type: TYPE_NORMAL
- en: Single Conv Layer Computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first discuss what the conv layer computes intuitively. The Conv layer's
    parameters consist of a set of learnable filters (also called **tensors**). Each
    filter is small spatially (depth, width, and height), but extends through the
    full depth of the input volume (image). A filter on the first layer of a ConvNet
    typically has a size of 5 x 5 x 3 (that is, five pixels width and height, and
    three for depth, because images have three depths for color channels). During
    the forward pass, filters slide (or **convolve**) across the width and height
    of the input volume and compute the dot product between the entries of the filter
    and the input at any point. As the filter slides over the width and height of
    the input volume, it produces a 2D activation that gives the responses of that
    filter at every spatial position. The network will learn filters that activate
    when they see some kind of visual feature, such as an edge of some orientation
    or a blotch of some color on the first layer, or it might detect an entire honeycomb
    or wheel-like patterns on higher layers of the network. Once we have an entire
    set of filters in each conv layer (for example, 12 filters), each of them produces
    a separate 2D activation map. We stack these activation maps along the depth dimension
    and produce the output volume.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee47f9f9-11ab-48ab-beb6-551faa50bb8d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Image of 32 x 32 pixels convolved by 5 x 5 filter
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image shows a 32 x 32 x 3 image on which a filter of 5 x 5 x 3
    is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d72b474-77a0-422b-82a5-4fb0c91002c1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each dot product between filter and image chunk results in a single number
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's convolve the filter created above the whole image, moving it one
    pixel at a time. The final output will be sized 28 x 28 x 1\. This is called an
    **activation map**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bac5eb6-fd65-4de5-85f9-7c8b79337c3b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Activation map generated by applying a filter on a image
  prefs: []
  type: TYPE_NORMAL
- en: Consider using two filters one after the other; this will result in two activation
    maps of size 28 x 28 x 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eb97f4c-5cb4-46c7-9260-87f46e754008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Applying two filters on a single image results in two activation maps
  prefs: []
  type: TYPE_NORMAL
- en: If we use six such filters, we will end up with a new image sized 28 x 28 x
    3\. A ConvNet is a sequence of such convolution layers interspersed with activation
    functions such as **Relu**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1629fe0d-d370-4f34-bb9d-3bce9ec47bfe.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Result of applying six filters of 5 x 5 x 3 on image results in activation map
    of 28 x 28 x 6
  prefs: []
  type: TYPE_NORMAL
- en: Let us formally defined CNN according to TensorFlow parlance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**: A CNN is a neural network that has at least one layer (`tf.nn.conv2d`)
    that makes a convolution between its input and a configurable kernel generating
    the layer''s output. A convolution applies a kernel (filter) to every point in
    the input layer (a tensor). It generates a filtered output by sliding the kernel
    over an input tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use Case**: Following example is an edge detection filter applied on an input
    image using  a Convolution'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49356c6c-9b70-48f9-9701-5a61b24f70b9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Edge detection by applying kernel on an input image
  prefs: []
  type: TYPE_NORMAL
- en: CNNs follow a process that matches information similar to the structure found
    in the cellular layout of a cat's striate cortex. As signals are passed through
    a cat's striate cortex, certain layers signal when a visual pattern is highlighted.
    For example, one layer of cells activates (increases its output signal) when a
    horizontal line passes through it. A CNN will exhibit a similar behavior where
    clusters of neurons activate based on patterns learned from training. After training
    based on prelabeled data, a CNN will have certain layers that activate when a
    horizontal/vertical line passes through it.
  prefs: []
  type: TYPE_NORMAL
- en: Matching horizontal/vertical lines would be a useful neural network architecture,
    but CNNs layer multiple simple patterns to match complex patterns. These patterns
    are called **filters** or **kernels**. The goal of training is to adjust these
    kernel weights to minimize the loss function. Training these filters is accomplished
    by combining multiple layers and learning weights using gradient descent or other
    optimization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: CNN in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A CNN is composed of convolution layers (defined by `tf.nn.conv2d`), a non-linearity
    layer (`tf.nn.relu`), a max pool (`tf.nn.max_pool`), and fully connected layers
    (`tf.matmul`). The following image shows typical CNN layers and their corresponding
    implementations in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7949c19-8b34-4a86-b6e7-5aa9746d84a4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mapping CNN layers to TensorFlow functions
  prefs: []
  type: TYPE_NORMAL
- en: Image loading in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s look at how TensorFlow loads images. Let''s define a constant with
    a small array of three images and load them into a session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding listing shows the shape of the tensor and the first
    pixel of the first image. In this example code, an array of images is created
    that includes three images. Each image has a height of two pixels and a width
    of three pixels with an RGB color space. The output from the example code shows
    the number of images as the size of the first set of dimensions, Dimension(1).
    The height of each image is the size of the second set, Dimension(2), the width
    of each image comprises the third set, Dimension(3), and the array size of the
    color channel is the final set, Dimension(3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Convolution operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution operations are key components of a CNN; these operations use an
    input tensor and a filter to compute the output. The key is deciding the parameters
    available to tune them.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are tracking the location of an object. Its output is a single *x(t)*,
    which is the position of the object at time *t*. Both *x* and *t* are real-valued,
    that is, we can get a different reading at any instant in time. Suppose that our
    measurement is noisy. To obtain a less noisy estimate of the object's position,
    we would like to average together measurements. More recent measurements are more
    relevant for us; we want this to be a weighted average giving higher weight to
    recent measurements. We can compute this using a weighting function *w(a)*, where
    *a* is the age of a measurement (when the measurement was taken)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply a weighted average operation at every moment, we obtain a new function
    providing a smoothed estimate of the position of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/136f8c61-0fbb-4ab6-ad49-73cef018deee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This operation is called **convolution**. A convolution operation is denoted
    with an asterisk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4e11c31-065e-44d8-869c-7b8993d9072c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here,
  prefs: []
  type: TYPE_NORMAL
- en: '*w* is the kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is the output, also called a **feature map**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution on an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we use a 2D image *I* as our input, we probably also want to use a 2D kernel
    *K*. The preceding equation will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c8fcc55-c730-4e45-9931-07a02f67d6d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the convolution function is commutative, we can write the preceding equation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f2da051-543d-4049-81b0-4b8a86ce60a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Changing *i - m* and *j -n* to additions is referred to as cross-correlation,
    as that is what is implemented by TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a918398f-37c2-4aa0-bdef-076bd0417146.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s define a simple input and a kernel and run the `conv2d` operation in
    TensorFlow. Let''s take a look at a simple image input and a kernel input. The
    following diagram shows a basic image, a kernel, and the expected output by applying
    the convolution operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c93289c-7713-40b6-8a5f-2bd2d8ac2efb.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Example of basic image and kernel applied to it
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at how the output is achieved with a stride of 1, 1, 1, 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be47e774-a84e-4442-9085-9f259093ba78.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Calculating output by applying kernel to the input
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will implement the same in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding listing is as follows--this is the same as the
    one we calculated manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Strides
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary purpose of convolutions is to reduce the dimensions of an image
    (width, height, and number of channels). The larger the image, the more processing
    time is required.
  prefs: []
  type: TYPE_NORMAL
- en: The `strides` parameter causes a kernel to skip over pixels in an image and
    not include them in the output. The `strides` parameter determines how a convolution
    operation works with a kernel when a larger image and more complex kernel are
    used. As a convolution is sliding the kernel over the input, it is using the `strides`
    parameter to determine how it walks over the input, instead of going over every
    element of an input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following example, where we are moving a 3 x 3 x
    1 kernel over a 6 x 6 x 1 image with a stride of 1, 3, 3, 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88babb37-5210-499f-8ed8-267661d6b995.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 1 as kernel slides with stride of 1,3,3,1
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel strides over the following elements in steps 3 and 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d37b361e-768b-4670-a7c8-a5dc10beef9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 3 and 4 of kernel stride over input
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement this in TensorFlow; the output will be a 4 x 4 x 1 tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to the following listing, in which 1, 3, 3, 1 stride
    leaders to four red boxes in the preceding image are being multiplied with the
    kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling layers help with overfitting and improve performance by reducing the
    size of the input tensor. Typically, they are used to scale down the input, keeping
    important information. Pooling is a much faster mechanism for input size reduction
    compared with `tf.nn.conv2d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pooling mechanisms are supported by TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: Average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max with argmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pooling operation uses rectangular windows of size `ksize` separated by
    offset `strides`. If `strides` are all ones (1, 1, 1, 1), every window is used;
    if `strides` are all twos (1, 2, 2, 1), every other window is used in each dimension;
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Max pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following defined function provides max pooling for the input 4D tensor
    `tf.nn.max_pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding arguments are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`value`: This is the 4D tensor with shape [batch, height, width, channels],
    type `tf.float32` on which max pooling needs to be done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ksize`: This is the list of ints that has `length >= 4`. The size of the window
    for each dimension of the input tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides`: This is the list of ints, `length >= 4`. A stride of the sliding
    window for each dimension of the input tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding`: This is a string, either `VALID` or `SAME`. The padding algorithm.
    The following section explains `VALID` and `SAME` padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/12b26b0d-fafe-45c2-92ff-4701475b58a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reference: [https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t](https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t)'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_format`: This is a string. `NHWC` and `NCHW` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This is the optional name for the operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code demonstrates max pooling on a tensor using a `VALID` padding
    scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding listing will give the maximum values in the window
    3 x 3 x 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains how max pool logic works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f500a11c-b10d-4cd6-b4bf-38f16e45446f.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, max pool selected the maximum value from the window based on
    a stride of 1, 1, 1.
  prefs: []
  type: TYPE_NORMAL
- en: Average pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It performs the average pooling on the input tensor. Each entry in the output
    is the mean of the corresponding size `ksize` window in value. It is defined using
    the `tf.nn.avg_pool` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the code example where `avg_pool` is used in a simple 2D tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding listing is the average of all the values in the
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '*Average = (1.0 + 0.2 + 2.0 + 0.1 + 1.2 + 1.4 + 1.1 + 0.4 + 0\. 4) / 9 = 0.86666*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Image classification with convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at a more realistic case for using CNNs; we will use the Stanford
    Dogs versus Cats dataset. This dataset has 100+ images of dogs and cats.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download this dataset (100 images each) from the following location:
    [https://s3.amazonaws.com/neural-networking-book/ch04/dogs_vs_cats.tar.gz](https://s3.amazonaws.com/neural-networking-book/ch04/dogs_vs_cats.tar.gz)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant functions and Python classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the parameters for the convolution layers. There are three convolution
    layers with the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Layer number** | **Layer type** | **Number of filters/neurons** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Convolution | 32 filters |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Convolution | 32 filters |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Convolution | 64 filters |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Fully connected | 128 neurons |'
  prefs: []
  type: TYPE_TB
- en: 'The Network topolgy can be represented as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04579c56-ba76-4fc4-9c9d-c1282aa91ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code should be helpful for understanding the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the constant for the number of classes (two, in this case) and other
    variables. We have taken the Stanford dataset and reduced it to 100 images each
    of dogs and cats for easier processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first read the dataset into a tensor. The logic for the reading is defined
    in the `dataset` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `train_path`, `image_size`, `classes`, and `validation_size` are defined.
    Let''s look at the implementation of `read_train_sets(..)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This method, in turn, calls `load_train(...)` to return a `numpy.array` of
    the data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The data loaded into training is a function of `validation_set`; it is calculated
    from the images array''s first dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19940b2c-a831-4b0e-a3e1-515a1fb652d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We calculate `validation_size as` shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have kept validation size as `0.2`, it comes out to `58.2` rounded off
    to `58`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4686ba14-d906-429d-a911-2c5841fedc0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we create the test dataset, `test_images` and `test_ids`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `read_test_set(...)` is a function called internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`read_test_set(test_path, image_size)` in turn calls `load_test(test_path,
    image_size)`, for which the listing is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the sizes of the various `numpy` arrays created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot nine random images in a grid of 3 x 3 with the appropriate classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `plot_images` function is defined in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0baca6e1-e7d0-4d90-8a99-54c0f13843d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Nine random images from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Defining a tensor for input images and the first convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will define a tensor for input images and the first convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Input tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a placeholder with `shape[None, img_size_flat]` and reshape it into
    `[-1, img_size, img_size, num_channels]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the parameters `img_size` and `num_channels` have the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`img_size` = 128'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After reshaping the input tensor into `x_image`, we will create the first convolution
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `new_conv_layer(...)` function is defined here. Let''s look at the value
    of each variable being sent to this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8b1af64-88cf-40e5-aba0-c1b7f4704291.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables have the following values at runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94fc1581-7e56-4eea-8f6a-462173e18a4c.png)![](img/b86f9480-4130-4e9b-a42c-56e921259837.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we run this, the output of the `print(..)` statement will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the shape of the output tensor coming out of input layer 1.
  prefs: []
  type: TYPE_NORMAL
- en: Second convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the second convolution layer, we start with the first layer''s output as
    input and build a new layer with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe16178f-69ae-4b98-a2cd-c6db7d000c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we define a placeholder for real `y` and the class of real `y` (the
    label of the class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of these two variables is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21d8c384-836d-44e2-84ea-c905f32cf6a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'where following are the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_input_channels` = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_size` = 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_filters` = 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the output of the printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Third convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This layer takes the output of the second layer as the input. Let''s look at
    the inputs going into the creation of this layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d0e1178-13c2-41d2-b055-3e003015ed40.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of `layer_conv3` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Flatten the layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we flatten the layer to the `num` of images and the `num` of features,
    which is 16,384 in this case. If you notice for the last layer''s output, we have
    flattened it with the following logic, 16 x 16 x 64 = 16,384:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print these values, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Fully connected layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the fourth and fifth layers, we define fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '`layer_flat`: the last layer flattened'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_features`: number of features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fc_size`: number of outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows the values that are passed to `new_fc_layer()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd3e30ec-b8fb-4c02-ac90-428f32e4a762.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of the print is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is the fully connected layer 2, where the function takes the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`layer_fc1`: the output from the first fully connected layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inputs`: 128'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inputs`: `num_classes`, 2 in this case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_relu`: a Boolean function specifying whether to use `relu` or not; `False`
    in this case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the output of the second fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Defining cost and optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply Softmax on the output from `layer_fc2` (the fully connected second layer).
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematics, the `softmax` function, or normalized exponential function,^([[1]](https://en.wikipedia.org/wiki/Softmax_function#cite_note-bishop-1))^(:198)
    is a generalization of the [logistic function](https://en.wikipedia.org/wiki/Logistic_function)
    that *squashes* a K-dimensional vector Z of arbitrary real values to a K-dimensional
    vector *σ(z)* of real values in the range [*0*, *1*] that add up to *1*. The function
    is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be5e10c9-6ffd-4ff8-bcf3-35b80c8c2454.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the cross entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we define the optimizer, which is based on the Adam optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Adam is different to the stochastic gradient descent algorithm. Stochastic gradient
    descent maintains a single learning rate (called **alpha**) for all weight updates
    and the learning rate does not change during training.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm maintains a learning rate for each network weight (parameter)
    and separately adapts as learning unfolds. It computes individual adaptive learning
    rates for different parameters from the estimates of the first and second moments
    of the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Adam combines the advantages of two other extensions of stochastic gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: The **adaptive gradient algorithm** (**AdaGrad**) maintains a per-parameter
    learning rate that improves performance for ML problems with sparse gradients
    (for example, natural language and computer vision problems). **Root mean square
    propagation** (**RMSProp**) maintains learning rates for each parameter; these
    are adapted based on the average of recent values of the gradients for the weight
    (how quickly it is changing).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We also calculate the variables for `correct_prediction` and `accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: First epoch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initialize the session and call the `optimize()` function for `num_iterations=1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `optimize()` function is defined in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that prints the training, validation accuracy, and validation loss
    is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the accuracy of `Test-Set`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s optimize the model for `100` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also shows false positives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0701d3e7-93a0-4d1f-a3a1-b60938860ec7.png)'
  prefs: []
  type: TYPE_IMG
- en: Output showing false positives
  prefs: []
  type: TYPE_NORMAL
- en: Plotting filters and their effects on an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s apply filters in two layers to two test images and see how that affects
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `plot_image(image1)` function is shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e73c27ed-677f-4ced-a229-e7ae1b1f0828.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `image2` with filters applied is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a69fdcd-2edf-412c-8c87-2f3e61a606f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Convolution layer 1**: The following is the plot for weights for layer 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/148d381d-ae2f-431f-9f59-c5777a7ffdd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Filters from layer 1 applied to i mage 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0a7fcbe4-27aa-4810-8426-3dcfc2b29895.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Filters from Layer 1 applied to Image 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c87f3e63-b355-43f2-a591-73c5adbff0c9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Convolution layer 2**: Now plot the filter-weights for the second convolutional
    layer. There are 16 output channels from the first conv-layer, which means there
    are 16 input channels to the second conv-layer. The second Conv layer has a set
    of filter-weights for each of its input channels. We start by plotting the filter-weights
    for the first channel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 2 weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1fef67e7-1d91-4d12-8472-88ba49458be4.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights for Conv2, input channel 0\. Positive weights are red and negative weights
    are blue
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 16 input channels to the second convolutional layer, so we can make
    another 15 plots of filter-weights like this. We just make one more with the filter-weights
    for the second channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3940424f-1b26-4655-a8e6-33b1f835132c.png)'
  prefs: []
  type: TYPE_IMG
- en: Positive weights are red and negative weights are blue
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot Images 1 and 2 with filters from convolution layer 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/32a3fb47-36fc-4c28-a134-dbe122e429f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights for conv2, input channel 1\. Image displaying image1 filtered through
    a layer 2 filter
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf75f4f8-7094-42de-93ae-a53ef5def00c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image displaying image 2 filtered through a layer 2 filter
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolution Layer 3**: Let''s print the layer 3 weights; this layer has 64
    filters. This is how images 1 and 2 look passed through each of these filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/75b0fedd-b2fc-4fd5-b618-1bfad5389d36.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights for Conv2, Input Channel 0, Positive weights are red and negative weights
    are blue
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ee6baebb-0628-4681-b538-6049633ac2e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights for Conv2, input channel 1\. Positive weights are red and negative weights
    are blue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plotting an image passed through layer 3 filters**: Execute the following
    statements to plot images 1 and 2 being passed from 64 filters of convolution
    layer 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0c6fbc09-09e5-4a7d-ae8b-9f7c14c9a579.png)'
  prefs: []
  type: TYPE_IMG
- en: Image 1, plotted with convolution filters from conv3
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the image with convolution filters from conv3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8b2d685-5888-4864-b36a-f979f377d1ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Image 2, plotted with convolution filters from conv3
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have completed the analysis of the Cats versus Dogs dataset, where
    we used a five-layer CNN with three hidden layers and two fully connected layers
    to build our model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the basics of convolution and why it is an effective
    mechanism for image label prediction. You learned about basic concepts such as
    `strides` and padding. This was followed by an example based on the Stanford dataset
    of Cats versus Dogs. We used three convolution layers to build the neural network
    and two fully connected layers to showcase how it is used to classify the images.
    We also plotted the weights for three layers and saw how filters modify the image.
    We also looked at concepts such as image pooling and how it helps make CNN more
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we look at a different kind of neural network called a **Recurrent
    Neural Network** (**RNN**), which processes time series data or is used for **natural
    language processing** (**NLP**) to predict next word in a sequence
  prefs: []
  type: TYPE_NORMAL
