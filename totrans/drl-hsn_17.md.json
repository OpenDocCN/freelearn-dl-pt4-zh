["```py\nimport gymnasium as gym \nimport time \nimport numpy as np \nimport typing as tt \n\nimport torch \nimport torch.nn as nn \n\nfrom torch.utils.tensorboard.writer import SummaryWriter\n```", "```py\nMAX_BATCH_EPISODES = 100 \nMAX_BATCH_STEPS = 10000 \nNOISE_STD = 0.001 \nLEARNING_RATE = 0.001 \n\nTNoise = tt.List[torch.Tensor]\n```", "```py\nclass Net(nn.Module): \n    def __init__(self, obs_size: int, action_size: int): \n        super(Net, self).__init__() \n        self.net = nn.Sequential( \n            nn.Linear(obs_size, 32), \n            nn.ReLU(), \n            nn.Linear(32, action_size), \n            nn.Softmax(dim=1) \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        return self.net(x)\n```", "```py\ndef evaluate(env: gym.Env, net: Net) -> tt.Tuple[float, int]: \n    obs, _ = env.reset() \n    reward = 0.0 \n    steps = 0 \n    while True: \n        obs_v = torch.FloatTensor(np.expand_dims(obs, 0)) \n        act_prob = net(obs_v) \n        acts = act_prob.max(dim=1)[1] \n        obs, r, done, is_tr, _ = env.step(acts.data.numpy()[0]) \n        reward += r \n        steps += 1 \n        if done or is_tr: \n            break \n    return reward, steps\n```", "```py\ndef sample_noise(net: Net) -> tt.Tuple[TNoise, TNoise]: \n    pos = [] \n    neg = [] \n    for p in net.parameters(): \n        noise = np.random.normal(size=p.data.size()) \n        noise_t = torch.FloatTensor(noise) \n        pos.append(noise_t) \n        neg.append(-noise_t) \n    return pos, neg\n```", "```py\ndef eval_with_noise(env: gym.Env, net: nn.Module, noise: TNoise, noise_std: float, \n        get_max_action: bool = True, device: torch.device = torch.device(\"cpu\") \n    old_params = net.state_dict() \n    for p, p_n in zip(net.parameters(), noise): \n        p.data += NOISE_STD * p_n \n    r, s = evaluate(env, net) \n    net.load_state_dict(old_params) \n    return r, s\n```", "```py\ndef train_step(net: Net, batch_noise: tt.List[common.TNoise], batch_reward: tt.List[float], \n               writer: SummaryWriter, step_idx: int): \n    weighted_noise = None \n    norm_reward = np.array(batch_reward) \n    norm_reward -= np.mean(norm_reward) \n    s = np.std(norm_reward) \n    if abs(s) > 1e-6: \n        norm_reward /= s\n```", "```py\n for noise, reward in zip(batch_noise, norm_reward): \n        if weighted_noise is None: \n            weighted_noise = [reward * p_n for p_n in noise] \n        else: \n            for w_n, p_n in zip(weighted_noise, noise): \n                w_n += reward * p_n\n```", "```py\n m_updates = [] \n    for p, p_update in zip(net.parameters(), weighted_noise): \n        update = p_update / (len(batch_reward) * NOISE_STD) \n        p.data += LEARNING_RATE * update \n        m_updates.append(torch.norm(update)) \n    writer.add_scalar(\"update_l2\", np.mean(m_updates), step_idx)\n```", "```py\nif __name__ == \"__main__\": \n    writer = SummaryWriter(comment=\"-cartpole-es\") \n    env = gym.make(\"CartPole-v1\") \n\n    net = Net(env.observation_space.shape[0], env.action_space.n) \n    print(net)\n```", "```py\n step_idx = 0 \n    while True: \n        t_start = time.time() \n        batch_noise = [] \n        batch_reward = [] \n        batch_steps = 0 \n        for _ in range(MAX_BATCH_EPISODES): \n            noise, neg_noise = sample_noise(net) \n            batch_noise.append(noise) \n            batch_noise.append(neg_noise) \n            reward, steps = eval_with_noise(env, net, noise) \n            batch_reward.append(reward) \n            batch_steps += steps \n            reward, steps = eval_with_noise(env, net, neg_noise) \n            batch_reward.append(reward) \n            batch_steps += steps \n            if batch_steps > MAX_BATCH_STEPS: \n                break\n```", "```py\n step_idx += 1 \n        m_reward = float(np.mean(batch_reward)) \n        if m_reward > 199: \n            print(\"Solved in %d steps\" % step_idx) \n            break \n\n        train_step(net, batch_noise, batch_reward, writer, step_idx)\n```", "```py\n writer.add_scalar(\"reward_mean\", m_reward, step_idx) \n        writer.add_scalar(\"reward_std\", np.std(batch_reward), step_idx) \n        writer.add_scalar(\"reward_max\", np.max(batch_reward), step_idx) \n        writer.add_scalar(\"batch_episodes\", len(batch_reward), step_idx) \n        writer.add_scalar(\"batch_steps\", batch_steps, step_idx) \n        speed = batch_steps / (time.time() - t_start) \n        writer.add_scalar(\"speed\", speed, step_idx) \n        print(\"%d: reward=%.2f, speed=%.2f f/s\" % ( \n            step_idx, m_reward, speed))\n```", "```py\nChapter17$ ./01_cartpole_es.py \nNet( \n  (net): Sequential( \n   (0): Linear(in_features=4, out_features=32, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=32, out_features=2, bias=True) \n   (3): Softmax(dim=1) \n  ) \n) \n1: reward=10.00, speed=7458.03 f/s \n2: reward=11.93, speed=8454.54 f/s \n3: reward=13.71, speed=8677.55 f/s \n4: reward=15.96, speed=8905.25 f/s \n5: reward=18.75, speed=9098.71 f/s \n6: reward=22.08, speed=9220.68 f/s \n7: reward=23.57, speed=9272.45 f/s \n...\n```", "```py\n@dataclass(frozen=True) \nclass RewardsItem: \n    seed: int \n    pos_reward: float \n    neg_reward: float \n    steps: int\n```", "```py\ndef worker_func(params_queue: mp.Queue, rewards_queue: mp.Queue, \n                device: torch.device, noise_std: float): \n    env = make_env() \n    net = Net(env.observation_space.shape[0], env.action_space.shape[0]).to(device) \n    net.eval() \n\n    while True: \n        params = params_queue.get() \n        if params is None: \n            break \n        net.load_state_dict(params)\n```", "```py\n for _ in range(ITERS_PER_UPDATE): \n            seed = np.random.randint(low=0, high=65535) \n            np.random.seed(seed) \n            noise, neg_noise = common.sample_noise(net, device=device) \n            pos_reward, pos_steps = common.eval_with_noise(env, net, noise, noise_std, \n                get_max_action=False, device=device) \n            neg_reward, neg_steps = common.eval_with_noise(env, net, neg_noise, noise_std, \n                get_max_action=False, device=device) \n            rewards_queue.put(RewardsItem(seed=seed, pos_reward=pos_reward, \n                neg_reward=neg_reward, steps=pos_steps+neg_steps))\n```", "```py\ndef train_step(optimizer: optim.Optimizer, net: Net, batch_noise: tt.List[common.TNoise], \n               batch_reward: tt.List[float], writer: SummaryWriter, step_idx: int, \n               noise_std: float): \n    weighted_noise = None \n    norm_reward = compute_centered_ranks(np.array(batch_reward))\n```", "```py\n for noise, reward in zip(batch_noise, norm_reward): \n        if weighted_noise is None: \n            weighted_noise = [reward * p_n for p_n in noise] \n        else: \n            for w_n, p_n in zip(weighted_noise, noise): \n                w_n += reward * p_n \n    m_updates = [] \n    optimizer.zero_grad() \n    for p, p_update in zip(net.parameters(), weighted_noise): \n        update = p_update / (len(batch_reward) * noise_std) \n        p.grad = -update \n        m_updates.append(torch.norm(update)) \n    writer.add_scalar(\"update_l2\", np.mean(m_updates), step_idx) \n    optimizer.step()\n```", "```py\n params_queues = [mp.Queue(maxsize=1) for _ in range(PROCESSES_COUNT)] \n    rewards_queue = mp.Queue(maxsize=ITERS_PER_UPDATE) \n    workers = [] \n\n    for params_queue in params_queues: \n        p_args = (params_queue, rewards_queue, device, args.noise_std) \n        proc = mp.Process(target=worker_func, args=p_args) \n        proc.start() \n        workers.append(proc) \n\n    print(\"All started!\") \n    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n```", "```py\n for step_idx in range(args.iters): \n        params = net.state_dict() \n        for q in params_queues: \n            q.put(params)\n```", "```py\n t_start = time.time() \n        batch_noise = [] \n        batch_reward = [] \n        results = 0 \n        batch_steps = 0 \n        while True: \n            while not rewards_queue.empty(): \n                reward = rewards_queue.get_nowait() \n                np.random.seed(reward.seed) \n                noise, neg_noise = common.sample_noise(net) \n                batch_noise.append(noise) \n                batch_reward.append(reward.pos_reward) \n                batch_noise.append(neg_noise) \n                batch_reward.append(reward.neg_reward) \n                results += 1 \n                batch_steps += reward.steps \n\n            if results == PROCESSES_COUNT * ITERS_PER_UPDATE: \n                break \n            time.sleep(0.01)\n```", "```py\n train_step(optimizer, net, batch_noise, batch_reward, \n                   writer, step_idx, args.noise_std)\n```", "```py\n$ ./02_cheetah_es.py \nNet( \n  (mu): Sequential( \n   (0): Linear(in_features=17, out_features=64, bias=True) \n   (1): Tanh() \n   (2): Linear(in_features=64, out_features=6, bias=True) \n   (3): Tanh() \n  ) \n) \nAll started! \n0: reward=-505.09, speed=17621.60 f/s, data_gather=6.792, train=0.018 \n1: reward=-440.50, speed=20609.56 f/s, data_gather=5.815, train=0.007 \n2: reward=-383.76, speed=20568.74 f/s, data_gather=5.827, train=0.007 \n3: reward=-326.02, speed=20413.63 f/s, data_gather=5.871, train=0.007 \n4: reward=-259.58, speed=20181.74 f/s, data_gather=5.939, train=0.007 \n5: reward=-198.80, speed=20496.81 f/s, data_gather=5.848, train=0.007 \n6: reward=-113.22, speed=20467.71 f/s, data_gather=5.856, train=0.007\n```", "```py\ndef mutate_parent(net: Net) -> Net: \n    new_net = copy.deepcopy(net) \n    for p in new_net.parameters(): \n        noise = np.random.normal(size=p.data.size()) \n        noise_t = torch.FloatTensor(noise) \n        p.data += NOISE_STD * noise_t \n    return new_net\n```", "```py\nNOISE_STD = 0.01 \nPOPULATION_SIZE = 50 \nPARENTS_COUNT = 10\n```", "```py\nif __name__ == \"__main__\": \n    env = gym.make(\"CartPole-v1\") \n    writer = SummaryWriter(comment=\"-cartpole-ga\") \n\n    gen_idx = 0 \n    nets = [ \n        Net(env.observation_space.shape[0], env.action_space.n) \n        for _ in range(POPULATION_SIZE) \n    ] \n    population = [ \n        (net, common.evaluate(env, net)) \n        for net in nets \n    ]\n```", "```py\n while True: \n        population.sort(key=lambda p: p[1], reverse=True) \n        rewards = [p[1] for p in population[:PARENTS_COUNT]] \n        reward_mean = np.mean(rewards) \n        reward_max = np.max(rewards) \n        reward_std = np.std(rewards) \n\n        writer.add_scalar(\"reward_mean\", reward_mean, gen_idx) \n        writer.add_scalar(\"reward_std\", reward_std, gen_idx) \n        writer.add_scalar(\"reward_max\", reward_max, gen_idx) \n        print(\"%d: reward_mean=%.2f, reward_max=%.2f, reward_std=%.2f\" % ( \n            gen_idx, reward_mean, reward_max, reward_std)) \n        if reward_mean > 199: \n            print(\"Solved in %d steps\" % gen_idx) \n            break\n```", "```py\n prev_population = population \n        population = [population[0]] \n        for _ in range(POPULATION_SIZE-1): \n            parent_idx = np.random.randint(0, PARENTS_COUNT) \n            parent = prev_population[parent_idx][0] \n            net = mutate_parent(parent) \n            fitness = common.evaluate(env, net) \n            population.append((net, fitness)) \n        gen_idx += 1\n```", "```py\nChapter17$ ./03_cartpole_ga.py \n0: reward_mean=29.50, reward_max=109.00, reward_std=27.86 \n1: reward_mean=65.50, reward_max=111.00, reward_std=27.61 \n2: reward_mean=149.10, reward_max=305.00, reward_std=57.76 \n3: reward_mean=175.00, reward_max=305.00, reward_std=47.35 \n4: reward_mean=200.50, reward_max=305.00, reward_std=39.98 \nSolved in 4 steps\n```", "```py\nNOISE_STD = 0.01 \nPOPULATION_SIZE = 2000 \nPARENTS_COUNT = 10 \nWORKERS_COUNT = 6 \nSEEDS_PER_WORKER = POPULATION_SIZE // WORKERS_COUNT \nMAX_SEED = 2**32 - 1\n```", "```py\ndef mutate_net(net: Net, seed: int, copy_net: bool = True) -> Net: \n    new_net = copy.deepcopy(net) if copy_net else net \n    np.random.seed(seed) \n    for p in new_net.parameters(): \n        noise = np.random.normal(size=p.data.size()) \n        noise_t = torch.FloatTensor(noise) \n        p.data += NOISE_STD * noise_t \n    return new_net\n```", "```py\ndef build_net(env: gym.Env, seeds: tt.List[int]) -> Net: \n    torch.manual_seed(seeds[0]) \n    net = Net(env.observation_space.shape[0], env.action_space.shape[0]) \n    for seed in seeds[1:]: \n        net = mutate_net(net, seed, copy_net=False) \n    return net\n```", "```py\n@dataclass \nclass OutputItem: \n    seeds: tt.List[int] \n    reward: float \n    steps: int \n\ndef worker_func(input_queue: mp.Queue, output_queue: mp.Queue): \n    env = gym.make(\"HalfCheetah-v4\") \n    cache = {} \n\n    while True: \n        parents = input_queue.get() \n        if parents is None: \n            break \n        new_cache = {} \n        for net_seeds in parents: \n            if len(net_seeds) > 1: \n                net = cache.get(net_seeds[:-1]) \n                if net is not None: \n                    net = mutate_net(net, net_seeds[-1]) \n                else: \n                    net = build_net(env, net_seeds) \n            else: \n                net = build_net(env, net_seeds) \n            new_cache[net_seeds] = net \n            reward, steps = common.evaluate(env, net, get_max_action=False) \n            output_queue.put(OutputItem(seeds=net_seeds, reward=reward, steps=steps)) \n        cache = new_cache\n```", "```py\n batch_steps = 0 \n        population = [] \n        while len(population) < SEEDS_PER_WORKER * WORKERS_COUNT: \n            out_item = output_queue.get() \n            population.append((out_item.seeds, out_item.reward)) \n            batch_steps += out_item.steps \n        if elite is not None: \n            population.append(elite) \n        population.sort(key=lambda p: p[1], reverse=True) \n        elite = population[0] \n        for worker_queue in input_queues: \n            seeds = [] \n            for _ in range(SEEDS_PER_WORKER): \n                parent = np.random.randint(PARENTS_COUNT) \n                next_seed = np.random.randint(MAX_SEED) \n                s = list(population[parent][0]) + [next_seed] \n                seeds.append(tuple(s)\n```"]