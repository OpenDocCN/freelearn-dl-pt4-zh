<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Future Stock Prices</h1>
                </header>
            
            <article>
                
<p class="p5"><span class="s2">The financial market is a very important part of any economy. For an economy to thrive, its financial market must be solid. Since the advent of machine learning, companies have begun to adopt algorithmic trading in the purchase of stocks and other financial assets. There has been proven successful with this method, and it has risen in prominence over time. Given its rise, several machine models have been developed and adopted for algorithmic trading. One popular machine learning model for trading is the time series analysis. You have already learned about reinforcement learning and Keras, and in this chapter, they will be used to develop a model that can predict stock prices.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Background problem</h1>
                </header>
            
            <article>
                
<p class="p5"><span class="s2">Automation is taking over in almost every sector, and the financial market is no exception. Creating automated algorithmic trading models will provide for a faster and more accurate analysis of stocks before purchase. Multiple indicators can be analyzed at a speed that humans are incapable of. Also, in trading, it is dangerous to operate with emotions. Machine learning models can solve that problem. There is also a reduction in transaction costs, as there is no need for continuous supervision.</span></p>
<p class="p6"><span class="s2">In this tutorial, you will learn how to combine reinforcement learning with time series modeling, in order to predict the prices of stocks, based on real-life data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data used</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">The data that we will use will be the standard and poor's 500. According to Wikipedia, it is <em>An American stock market index based on the market capitalizations of 500 large companies having common stock listed on the NYSE or NASDAQ.</em> Here is <span class="s3">a link</span> to the data <span class="s3">(<a href="https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC">https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC</a>)</span>.</span></p>
<p class="p6"><span class="s2">The data has the following columns:</span></p>
<ol class="ol1">
<li class="li6"><span class="s2"><strong>Date</strong>: This indicates the date under consideration</span></li>
<li class="li6"><span class="s2"><strong>Open</strong>: This indicates the price at which the market opens on the date</span></li>
<li class="li6"><span class="s2"><strong>High</strong>: This indicates the highest market price on the date</span></li>
<li class="li6"><span class="s2"><strong>Low</strong>: This indicates the lowest market price on the date</span></li>
<li class="li6"><span class="s2"><strong>Close</strong>: This indicates the price at which the market closes on the date, adjusted for the split</span></li>
<li class="li6"><span class="s2"><strong>Adj Close</strong>: This indicates the adjusted closing price for both the split and dividends</span></li>
<li class="li6"><span class="s2"><strong>Volume</strong>: This indicates the total volume of shares available</span></li>
</ol>
<p class="p6"><span class="s2">The date under consideration for training the data is as follows:</span></p>
<pre class="p6"><span class="s2">Start: 14 August 2006<br/></span><span class="s2">End: 13th August 2015</span></pre>
<p>On the website, filter the date as follows, and download the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-605 image-border" src="assets/cd47755a-4b1b-423b-802d-a4121658a46e.png" style="width:150.00em;height:27.50em;"/></p>
<p class="p6"><span class="s2">For testing, we will use the following date range:</span></p>
<pre class="p6"><span class="s2">Start: 14 August 2015<br/></span><span class="s2">End: 14 August 2018</span></pre>
<p>Change the dates on the website accordingly, and download the dataset for testing, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-606 image-border" src="assets/f9367771-52bb-4db0-8eda-5a61cf78a297.png" style="width:149.08em;height:28.17em;"/></p>
<p class="mce-root"/>
<p>In the next section, we will define some possible actions that the agent can carry out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step-by-step guide</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">Our solution uses an actor-critic reinforcement learning model, along with an infused time series, to help us predict the best action, based on the stock prices. The possible actions are as follows:</span></p>
<ol class="ol1">
<li class="li6"><span class="s2"><strong>Hold</strong>: This means that based on the price and projected profit, the trader should hold a stock</span></li>
<li class="li6"><span class="s2"><strong>Sell</strong>: This means that based on the price and projected profit, the trader should sell a stock</span></li>
<li class="li6"><span class="s2"><strong>Buy</strong>: This means that based on the price and projected profit, the trader should buy a stock</span></li>
</ol>
<p class="p6"><span class="s2">The actor-critic network is a family of reinforcement learning methods premised on two interacting network models. These models have two components: the actor and the critic. In our case, the network models that we will use will be neural networks. We will use the Keras package, which you have already learned about, to create the neural networks. The reward function that we are looking to improve is the profit.</span></p>
<p class="p6"><span class="s2">The actor takes in the state of the environment, then returns the best action, or a policy that refers to a probability distribution over actions. This seems like a natural way to perform reinforcement learning, as policies are directly returned as a function of the state.</span></p>
<p class="p6"><span class="s2">The critic evaluates the actions returned by the actor-network. This is similar to the traditional deep Q network; in the environment state and an action to return a score representing the value of taking that action given the state. The job of the critic is to compute an approximation, which is then used to update the actor in the direction of its gradient. The critic is trained itself temporal difference algorithm.</span></p>
<p class="p6"><span class="s2">These two networks are trained simultaneously. With time, the critic network is able to improve its <kbd>Q_value</kbd> prediction, and the actor also learns how to make better decisions, given the state.</span></p>
<p class="p5"><span class="s2">There are five scripts that make up this solution, and they will be described in the next sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Actor script</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">The actor script is where the policy model is defined. We begin by importing certain modules from Keras: layers, optimizers, models, and the backend. These modules will help us to construct our neural network: Let's start by importing the required functions from Keras.</span></p>
<pre><span>from </span>keras <span>import </span>layers<span>, </span>models<span>, </span>optimizers<br/><span>from </span>keras <span>import </span>backend <span>as </span>K<br/><br/></pre>
<ol>
<li><span class="s2">We create a class called <kbd>Actor</kbd>, whose object takes in the parameters of the <kbd>state</kbd> and <kbd>action</kbd> size:</span></li>
</ol>
<pre style="padding-left: 60px"><span>class </span>Actor:<br/>    <br/>    <br/>  <span># """Actor (policy) Model. """<br/></span><span><br/></span><span>    </span><span>def </span><span>__init__</span>(<span>self</span><span>, </span>state_size<span>, </span>action_size):<br/><span>        </span><span>self</span>.state_size = state_size<br/>        <span>self</span>.action_size = action_size<br/><br/></pre>
<ol start="2">
<li class="p8"><span class="s2">The preceding code shows the state size,</span> <span class="s2">which represents the dimension of each state, and the a</span><span class="s2">ction size, which represents the dimensions of</span> the <span class="s2">actions. Next, call a function to build the model, as follows:</span></li>
</ol>
<pre>        <span>self</span>.build_model()</pre>
<ol start="3">
<li>Build a policy model that maps the states to actions, and start by defining the input layer, as follows:</li>
</ol>
<pre>    <span>def </span><span>build_model</span>(<span>self</span>):<span><br/></span><span>        </span>states = layers.Input(<span>shape</span>=(<span>self</span>.state_size<span>,</span>)<span>, </span><span>name</span>=<span>'states'</span>)         </pre>
<ol start="4">
<li>Add hidden layers to the model. There are two dense layers, each one followed by a batch normalization and an activation layer. The dense layers are regularized. The two layers have 16 and 32 hidden units, respectively:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>net = layers.Dense(<span>units</span>=<span>16</span><span>,</span><span>kernel_regularizer</span>=layers.regularizers.l2(<span>1e-6</span>))(states)<br/>        net = layers.BatchNormalization()(net)<br/>        net = layers.Activation(<span>"relu"</span>)(net)<br/>        net = layers.Dense(<span>units</span>=<span>32</span><span>,</span><span>kernel_regularizer</span>=layers.regularizers.l2(<span>1e-6</span>))(net)<br/>        net = layers.BatchNormalization()(net)<br/>        net = layers.Activation(<span>"relu"</span>)(net)</pre>
<p class="mce-root"/>
<ol start="5">
<li>The final output layer will predict the action probabilities that have an activation of <kbd>softmax</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>actions = layers.Dense(<span>units</span>=<span>self</span>.action_size<span>, </span><span>activation</span>=<span>'softmax'</span><span>, </span><span>name </span>= <span>'actions'</span>)(net)<br/>        <br/>        <span>self</span>.model = models.Model(<span>inputs</span>=states<span>, </span><span>outputs</span>=actions)<br/>        </pre>
<ol start="6">
<li>Define the loss function by using the action value (<kbd>Q_value</kbd>) gradients, as follows:</li>
</ol>
<pre><span>        </span>action_gradients = layers.Input(<span>shape</span>=(<span>self</span>.action_size<span>,</span>))<br/>        loss = K.mean(-action_gradients * actions)</pre>
<ol start="7">
<li>Define the <kbd>optimizer</kbd> and training function, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>optimizer = optimizers.Adam(<span>lr</span>=<span>.00001</span>)<br/>        updates_op = optimizer.get_updates(<span>params</span>=<span>self</span>.model.trainable_weights<span>, </span><span>loss</span>=loss)<br/>        <span>self</span>.train_fn = K.function(<br/>            <span>inputs</span>=[<span>self</span>.model.input<span>, </span>action_gradients<span>, </span>K.learning_phase()]<span>,<br/></span><span>            </span><span>outputs</span>=[]<span>,<br/></span><span>            </span><span>updates</span>=updates_op)</pre>
<p><span>The custom training function for the actor-network that makes use of the Q gradients with respect to the action probabilities. With this custom function, the training aims to maximize the profits (in other words, minimize the negatives of the <kbd>Q_values</kbd>).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Critic script</h1>
                </header>
            
            <article>
                
<p><span class="s2">We begin by importing certain modules from Keras: layers, optimizers, models, and the backend. These modules will help us to construct our neural network:</span></p>
<pre><span>from </span>keras <span>import </span>layers<span>, </span>models<span>, </span>optimizers<br/><span>from </span>keras <span>import </span>backend <span>as </span>K</pre>
<ol>
<li><span>We create a class called <kbd>Critic</kbd>, whose object takes in the following parameters:</span></li>
</ol>
<pre style="padding-left: 60px"><span>class </span>Critic:<br/>    <span>"""Critic (Value) Model."""<br/></span><span><br/></span><span>    </span><span>def </span><span>__init__</span>(<span>self</span><span>, </span>state_size<span>, </span>action_size):<br/>        <span>"""Initialize parameters and build model.<br/></span><span>        Params<br/></span><span>        ======<br/></span><span>            state_size (int): Dimension of each state<br/></span><span>            action_size (int): Dimension of each action<br/></span><span>        """<br/></span><span>        </span><span>self</span>.state_size = state_size<br/>        <span>self</span>.action_size = action_size<br/><br/>        <span>self</span>.build_model()</pre>
<ol start="2">
<li>Build a critic (value) network that maps <kbd>state</kbd> and <kbd>action</kbd> pairs (<kbd>Q_values</kbd>), and define input layers, as follows:</li>
</ol>
<pre style="padding-left: 60px">    <span>def </span><span>build_model</span>(<span>self</span>):<span><br/></span><span>        </span>states = layers.Input(<span>shape</span>=(<span>self</span>.state_size<span>,</span>)<span>, </span><span>name</span>=<span>'states'</span>)<br/>        actions = layers.Input(<span>shape</span>=(<span>self</span>.action_size<span>,</span>)<span>, </span><span>name</span>=<span>'actions'</span>)</pre>
<ol start="3">
<li>Add the hidden layers for the state pathway, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>net_states = layers.Dense(<span>units</span>=<span>16</span><span>,</span><span>kernel_regularizer</span>=layers.regularizers.l2(<span>1e-6</span>))(states)<br/>        net_states = layers.BatchNormalization()(net_states)<br/>        net_states = layers.Activation(<span>"relu"</span>)(net_states)<br/><br/>        net_states = layers.Dense(<span>units</span>=<span>32</span><span>, </span><span>kernel_regularizer</span>=layers.regularizers.l2(<span>1e-6</span>))(net_states)</pre>
<ol start="4">
<li>Add the hidden layers for the action pathway, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>net_actions = layers.Dense(<span>units</span>=<span>32</span><span>,</span><span>kernel_regularizer</span>=layers.regularizers.l2(<span>1e-6</span>))(actions)</pre>
<ol start="5">
<li>Combine the state and action pathways, as follows:</li>
</ol>
<pre><span>        </span>net = layers.Add()([net_states<span>, </span>net_actions])<br/>        net = layers.Activation(<span>'relu'</span>)(net)</pre>
<ol start="6">
<li>Add the final output layer to produce the action values (<kbd>Q_values</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><span>        </span>Q_values = layers.Dense(<span>units</span>=<span>1</span><span>, </span><span>name</span>=<span>'q_values'</span><span>,</span><span>kernel_initializer</span>=layers.initializers.RandomUniform(<span>minval</span>=-<span>0.003</span><span>, </span><span>maxval</span>=<span>0.003</span>))(net)</pre>
<ol start="7">
<li>Create the Keras model, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>        </span><span>self</span>.model = models.Model(<span>inputs</span>=[states<span>, </span>actions]<span>, </span><span>outputs</span>=Q_values)</pre>
<ol start="8">
<li>Define the <kbd>optimizer</kbd> and compile a model for training with the built-in loss function:</li>
</ol>
<pre><span>        </span>optimizer = optimizers.Adam(<span>lr</span>=<span>0.001</span>)<br/>        <span>self</span>.model.compile(<span>optimizer</span>=optimizer<span>, </span><span>loss</span>=<span>'mse'</span>)</pre>
<ol start="9">
<li>Compute the action gradients (the derivative of <kbd>Q_values</kbd>, with respect to <kbd>actions</kbd>):</li>
</ol>
<pre><span>        </span>action_gradients = K.gradients(Q_values<span>, </span>actions)</pre>
<ol start="10">
<li><span>D</span>efine an additional function to fetch the action gradients (to be used by the actor mo<span>del), as follows:</span></li>
</ol>
<pre><span>        </span><span>self</span>.get_action_gradients = K.function(<br/>            <span>inputs</span>=[*self.model.input<span>, </span>K.learning_phase()]<span>,<br/></span><span>            </span><span>outputs</span>=action_gradients)</pre>
<p>This concludes the critic script.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agent script</h1>
                </header>
            
            <article>
                
<p class="p11"><span class="s2">In this section, we will train an agent that will perform reinforcement learning based on the actor and critic networks. We will perform the following steps to achieve this:</span></p>
<ol class="ol1">
<li class="li6"><span class="s2">Create an agent class whose initial function takes in the batch size, state size, and an evaluation Boolean function, to check whether the training is ongoing.</span></li>
<li class="li6"><span class="s2">In the agent class, create the following methods:</span></li>
<li class="li6">Import the <kbd>actor</kbd> and <kbd>critic</kbd> scripts:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>actor <span>import </span>Actor<br/><span>from </span>critic <span>import </span>Critic</pre>
<ol start="4">
<li><span class="s2">Import<span> </span><kbd>numpy</kbd>,</span><span class="s2"> <kbd>random</kbd>, </span><span class="s2"><kbd>namedtuple</kbd>,<span> </span>and <kbd>deque</kbd> from the <kbd>collections</kbd> package</span><span class="s2">:</span></li>
</ol>
<pre style="padding-left: 60px"><span>import </span>numpy <span>as </span>np<br/><span>from </span>numpy.random <span>import </span>choice<br/><span>import </span>random<br/><br/><span>from </span>collections <span>import </span>namedtuple<span>, </span>deque</pre>
<ol start="5">
<li>Create a <kbd>ReplayBuffer</kbd> class that adds, samples, and evaluates a buffer<span>:</span></li>
</ol>
<pre style="padding-left: 60px"><span>class </span>ReplayBuffer:<br/>    <span>#Fixed sized buffer to stay experience tuples<br/></span><span>    <br/></span><span>    </span><span>def </span><span>__init__</span>(<span>self</span><span>, </span>buffer_size<span>, </span>batch_size):<br/>        <br/>    <br/>    <span>#Initialize a replay buffer object.<br/></span><span>    <br/></span><span>    #parameters<br/></span><span>    <br/></span><span>    #buffer_size: maximum size of buffer. Batch size: size of each batch<br/></span><span>    <br/></span><span>        </span><span>self</span>.memory = deque(<span>maxlen </span>= buffer_size)  <span>#memory size of replay buffer<br/></span><span>        </span><span>self</span>.batch_size = batch_size               <span>#Training batch size for Neural nets<br/></span><span>        </span><span>self</span>.experience = namedtuple(<span>"Experience"</span><span>, </span><span>field_names </span>= [<span>"state"</span><span>, </span><span>"action"</span><span>, </span><span>"reward"</span><span>, </span><span>"next_state"</span><span>, </span><span>"done"</span>])                                           <span>#Tuple containing experienced replay<br/></span><span>    <br/></span></pre>
<ol start="6">
<li>Add a new experience to the replay buffer memory:</li>
</ol>
<pre style="padding-left: 60px"><span>    </span><span>def </span><span>add</span>(<span>self</span><span>, </span>state<span>, </span>action<span>, </span>reward<span>, </span>next_state<span>, </span>done):<span><br/></span><span>        </span>e = <span>self</span>.experience(state<span>, </span>action<span>, </span>reward<span>, </span>next_state<span>, </span>done)<br/>        <span>self</span>.memory.append(e)</pre>
<ol start="7">
<li>Randomly sample a batch of experienced tuples from the memory. In the following function, we randomly sample states from a memory buffer. We do this so that the states that we feed to the model are not temporally correlated. Thi<span>s will reduce overfitting:</span></li>
</ol>
<pre style="padding-left: 60px">    <span>def </span><span>sample</span>(<span>self</span><span>, </span><span>batch_size = 32</span>):<span><br/></span><span>        </span><span>return </span>random.sample(<span>self</span>.memory<span>, </span><span>k</span>=<span>self</span>.batch_size)</pre>
<ol start="8">
<li>Return the current size of the buffer memory, as follows:</li>
</ol>
<pre style="padding-left: 60px">    <span>def </span><span>__len__</span>(<span>self</span>):<span><br/></span><span>        </span><span>return </span><span>len</span>(<span>self</span>.memory)</pre>
<p class="mce-root"/>
<ol start="9">
<li>The reinforcement learning agent that learns using the actor-critic network is <span>as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>class </span>Agent:<span><br/></span><span>    </span><span>def </span><span>__init__</span>(<span>self</span><span>, </span>state_size<span>, </span>batch_size<span>, </span>is_eval = <span>False</span>):<br/>        <span>self</span>.state_size = state_size <span>#</span></pre>
<ol start="10">
<li>The number of actions are defined as 3: <span>sit, buy, sell</span></li>
</ol>
<pre><span>        </span><span>self</span>.action_size = <span>3 </span></pre>
<ol start="11">
<li>Define the replay memory <span>size</span></li>
</ol>
<pre><span>        </span><span>self</span>.buffer_size = <span>1000000</span><span><br/></span><span>        </span><span>self</span>.batch_size = batch_size<br/>        <span>self</span>.memory = ReplayBuffer(<span>self</span>.buffer_size<span>, </span><span>self</span>.batch_size)<br/>        <span>self</span>.inventory = []</pre>
<ol start="12">
<li>Define whether or not training is <span>ongoing. This variable will be changed during the training and evaluation phase:</span></li>
</ol>
<pre>        <span>self</span>.is_eval = is_eval    </pre>
<ol start="13">
<li>Discount factor in Bellman <span>equation:</span></li>
</ol>
<pre><span>        </span><span>self</span>.gamma = <span>0.99        </span></pre>
<ol start="14">
<li>A soft update of the actor and critic networks can be done as follows<span>:</span></li>
</ol>
<pre><span>        </span><span>self</span>.tau = <span>0.001   </span></pre>
<ol start="15">
<li>The actor policy model maps states to actions and instantiates the actor networks (local and target models, for soft updates of parameters)<span>:</span></li>
</ol>
<pre><span>        </span><span>self</span>.actor_local = Actor(<span>self</span>.state_size<span>, </span><span>self</span>.action_size) <span><br/></span><span>        </span><span>self</span>.actor_target = Actor(<span>self</span>.state_size<span>, </span><span>self</span>.action_size)    </pre>
<ol start="16">
<li>The critic (value) model that maps the state-action pairs to <kbd>Q_values</kbd> is as follows<span>:</span></li>
</ol>
<pre><span>        </span><span>self</span>.critic_local = Critic(<span>self</span>.state_size<span>, </span><span>self</span>.action_size)</pre>
<ol start="17">
<li>Instantiate the critic model (the local and target models are utilized to allow for soft updates), as follows:</li>
</ol>
<pre style="padding-left: 60px">        <span>self</span>.critic_target = Critic(<span>self</span>.state_size<span>, </span><span>self</span>.action_size)    <span><br/></span><span>        </span><span>self</span>.critic_target.model.set_weights(<span>self</span>.critic_local.model.get_weights()) </pre>
<ol start="18">
<li>The following code sets the target model parameters to local model parameters<span>:</span></li>
</ol>
<pre style="padding-left: 60px">      <span>self</span>.actor_target.model.set_weights(<span>self</span>.actor_local.model.get_weights()</pre>
<ol start="19">
<li><span>Returns</span> <span>an action, given a state, using the actor (policy network) and the </span><span>output of the</span> <kbd>softmax</kbd> layer of the actor-network, returning the probability for each action. An action method that returns an action, given a state, using the actor (policy network) is as follo<span class="s2">ws:</span></li>
</ol>
<pre style="padding-left: 60px"><span> </span><span>def </span><span>act</span>(<span>self</span><span>, </span>state):<br/>        options = <span>self</span>.actor_local.model.predict(state) <span><br/></span><span>        </span><span>self</span>.last_state = state<br/>        <span>if not </span><span>self</span>.is_eval:<br/>            <span>return </span>choice(<span>range</span>(<span>3</span>)<span>, </span><span>p </span>= options[<span>0</span>])     <span><br/></span><span>        </span><span>return </span>np.argmax(options[<span>0</span>])</pre>
<ol start="20">
<li><span>Returns a s</span>tochastic policy, based on the action probabilities in the training model and a deterministic action corresponding to the maximum probability during testing. There is a set of actions to be carried out by the agent at every step of the episode. A method (step) that returns the set of act<span class="s2">ions to be carried out by the agent at every step of the episode is as follows:</span></li>
</ol>
<pre><span>    </span><span>def </span><span>step</span>(<span>self</span><span>, </span>action<span>, </span>reward<span>, </span>next_state<span>, </span>done):</pre>
<ol start="21">
<li><span>T</span>he following code adds a new experience to the memor<span>y:</span></li>
</ol>
<pre>        <span>self</span>.memory.add(<span>self</span>.last_state<span>, </span>action<span>, </span>reward<span>, </span>next_state<span>, <br/></span>          done) </pre>
<ol start="22">
<li><span>T</span>he following code asserts that enough experiences are present in the memor<span>y to train:</span></li>
</ol>
<pre><span>        </span><span>if </span><span>len</span>(<span>self</span>.memory) &gt; <span>self</span>.batch_size:               </pre>
<ol start="23">
<li>The following code samples a random batch from the memory to train:</li>
</ol>
<pre><span>     </span><span>  </span>experiences = <span>self</span>.memory.sample(<span>self</span>.batch_size)</pre>
<ol start="24">
<li>Learn from the sampled experiences, <span>as follows:</span></li>
</ol>
<pre><span>  </span><span>      </span><span>self</span>.learn(experiences)                                 </pre>
<ol start="25">
<li>The following code updates the state to the ne<span>xt state:</span></li>
</ol>
<pre><span>        </span><span>self</span>.last_state = next_state                   </pre>
<ol start="26">
<li><span>Learning from the sampled experiences through the actor and the critic. </span>Create a method to learn from the sampled experiences through the actor and the critic, a<span class="s2">s follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    </span><span>def </span><span>learn</span>(<span>self</span><span>, </span>experiences):               <br/>        states = np.vstack([e.state <span>for </span>e <span>in </span>experiences <span>if </span>e <span>is not </span><span>None</span>]).astype(np.float32).reshape(-<span>1</span><span>,</span><span>self</span>.state_size)    <br/>        actions = np.vstack([e.action <span>for </span>e <span>in </span>experiences <span>if </span>e <span>is not </span><span>None</span>]).astype(np.float32).reshape(-<span>1</span><span>,</span><span>self</span>.action_size)<br/>        rewards = np.array([e.reward <span>for </span>e <span>in </span>experiences <span>if </span>e <span>is not </span><span>None</span>]).astype(np.float32).reshape(-<span>1</span><span>,</span><span>1</span>)<br/>        dones = np.array([e.done <span>for </span>e <span>in </span>experiences <span>if </span>e <span>is not </span><span>None</span>]).astype(np.float32).reshape(-<span>1</span><span>,</span><span>1</span>)<br/>        next_states = np.vstack([e.next_state <span>for </span>e <span>in </span>experiences <span>if </span>e <span>is not </span><span>None</span>]).astype(np.float32).reshape(-<span>1</span><span>,</span><span>self</span>.state_size) <span><br/></span><span>        </span></pre>
<ol start="27">
<li class="mce-root"><span>Re</span>turn a separate array for each experience in the replay component and predict actions based on the next states, <span>as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>        </span>actions_next = <span>self</span>.actor_target.model.predict_on_batch(next_states)    </pre>
<ol start="28">
<li><span>P</span>redict the <kbd>Q_value</kbd> of the actor output for the next <span>state, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>        </span>Q_targets_next = <span>self</span>.critic_target.model.predict_on_batch([next_states<span>, </span>actions_next])  </pre>
<ol start="29">
<li><span>Target the <kbd>Q_value</kbd> to</span> serve as a label for the critic network, based on the temporal difference, as follows<span>:</span></li>
</ol>
<pre><span>        </span>Q_targets = rewards + <span>self</span>.gamma * Q_targets_next * (<span>1 </span>- dones)   </pre>
<ol start="30">
<li><span>Fit</span> the critic model to the time difference of the target, <span>as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>        </span><span>self</span>.critic_local.model.train_on_batch(<span>x </span>= [states<span>, </span>actions]<span>, </span><span>y </span>= Q_targets) <span><br/></span></pre>
<p class="mce-root"/>
<ol start="31">
<li>Train the actor model (local) using the gradient of the critic network output with respect to the action probabilities fed from the actor-network<span>:</span></li>
</ol>
<pre style="padding-left: 60px"><span>        </span>action_gradients = np.reshape(<span>self</span>.critic_local.get_action_gradients([states<span>, </span>actions<span>, </span><span>0</span>])<span>,</span>(-<span>1</span><span>, </span><span>self</span>.action_size))</pre>
<ol start="32">
<li>Next, define a custom training function, as fol<span>lows:</span></li>
</ol>
<pre><span>        </span><span>self</span>.actor_local.train_fn([states<span>, </span>action_gradients<span>, </span><span>1</span>])  </pre>
<ol start="33">
<li><span>Nex</span>t, initiate a soft update of the parameters of both networks, <span>as follows:</span></li>
</ol>
<pre style="padding-left: 60px">        <span>self</span>.soft_update(<span>self</span>.actor_local.model<span>, </span><span>self</span>.actor_target.model)</pre>
<ol start="34">
<li><span>T</span>his performs soft updates on the model parameters, bas<span>ed on the parameter <kbd>tau</kbd> to avoid drastic model changes. </span><span class="s2">A method that updates the model by performing soft updates on the model parameters, based on the parameter <kbd>tau</kbd> (to avoid drastic model changes), is as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    </span><span>def </span><span>soft_update</span>(<span>self</span><span>, </span>local_model<span>, </span>target_model):<br/>        local_weights = np.array(local_model.get_weights())<br/>        target_weights = np.array(target_model.get_weights())<br/>        <span>assert </span><span>len</span>(local_weights) == <span>len</span>(target_weights)<br/>        new_weights = <span>self</span>.tau * local_weights + (<span>1 </span>- <span>self</span>.tau) * target_weights<br/>        target_model.set_weights(new_weights)</pre>
<p>This concludes the agent script. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Helper script</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">In this script, we will create functions that will be helpful for training, via the following steps:</span></p>
<ol>
<li class="p6">Import the <kbd>numpy</kbd> and <kbd>math</kbd> modules, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>numpy <span>as </span>np<br/><span>import </span>math</pre>
<ol start="2">
<li>Next, define a function to format the price to two decimal places, to reduce the ambiguity of the data:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>formatPrice</span>(n):<br/>    <br/>    <span>if </span>n&gt;=<span>0</span>:<br/>        curr = <span>"$"<br/></span><span>    </span><span>else</span>:<br/>        curr = <span>"-$"<br/></span><span>    </span><span>return </span>(curr +<span>"{0:.2f}"</span>.format(<span>abs</span>(n)))</pre>
<ol start="3">
<li>Return a vector of stock data from the CSV file. Convert the closing stock prices from the data to vectors, and return a vector of all stock prices, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>getStockData</span>(key):<br/>    datavec = []<br/>    lines = <span>open</span>(<span>"data/" </span>+ key + <span>".csv"</span><span>, </span><span>"r"</span>).read().splitlines()<br/>    <br/>    <span>for </span>line <span>in </span>lines[<span>1</span>:]:<br/>        datavec.append(<span>float</span>(line.split(<span>","</span>)[<span>4</span>]))<br/>    <br/>    <span>return </span>datavec</pre>
<ol start="4">
<li><span>Next, define a function to generate states from the input vector. </span><span class="s2">Create the time series by generating the states from the vectors created in the previous step. The function for this takes three parameters: the data; a time, <em>t</em> (the day that you want to predict); and a window (how many days to go b</span><span class="s2">ack in time). The rate of change between these vectors will then be measured and based on the sigmoid function:</span></li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>getState</span>(data<span>, </span>t<span>, </span>window):    <br/>    <span>if </span>t - window &gt;= -<span>1</span>:<br/>        vec = data[t - window+ <span>1</span>:t+ <span>1</span>]<br/>    <span>else</span>: <br/>        vec = -(t-window+<span>1</span>)*[data[<span>0</span>]]+data[<span>0</span>: t + <span>1</span>]<br/>    scaled_state = []<br/>    <span>for </span>i <span>in </span><span>range</span>(window - <span>1</span>):</pre>
<ol start="5">
<li>Next, scale the state vector from 0 to 1 with a sigmoid function. The sigmoid function can map any input value, from 0 to 1. This helps to normalize the valu<span>es to probabilities:</span></li>
</ol>
<pre style="padding-left: 60px">        scaled_state.append(<span>1</span>/(<span>1 </span>+ math.exp(vec[i] - vec[i+<span>1</span>])))  <span><br/></span><span>    </span><span>return </span>np.array([scaled_state])</pre>
<p>All of the necessary functions and classes are now defined, so we can start the training process. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the data</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">We will proceed to train the data, based on our agent and helper methods. This will provide us with one of three actions, based on the states of the stock prices at the end of the day. These states can be to buy, sell, or hold. </span><span class="s2">During training, the prescribed action for each day is predicted, and the price (profit, loss, or unchanged) of the action is calculated. The cumulative sum will be calculated at the end of the training period, and we will see whether there has been a profit or a loss. The aim is to maximize the total profit.</span></p>
<p class="p6"><span class="s2">Let's start with the imports, as follows:</span></p>
<pre><span>from </span>agent <span>import </span>Agent<br/><span>from </span>helper <span>import </span>getStockData, getState<br/><span>import </span>sys</pre>
<ol>
<li><span>Next, define the number of market days to consider as the window size, and define the batch size with which the neural network will be trained, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">window_size = 100<span>                         </span><span><br/></span>batch_size = <span>32</span></pre>
<ol start="2">
<li>Instantiate the stock agent with the window size and batch size, as follows:</li>
</ol>
<pre style="padding-left: 60px">agent = Agent(window_size<span>, </span>batch_size)  </pre>
<ol start="3">
<li>Next, read the training data from the CSV file, using the helper function:</li>
</ol>
<pre style="padding-left: 60px">data = getStockData(<span>"^GSPC"</span>)<br/>l = <span>len</span>(data) - <span>1</span></pre>
<ol start="4">
<li>Next, the episode count is defined as <kbd>300</kbd>. The agent will look at the data for so many numbers of times. An episode represents a complete pass over the data:</li>
</ol>
<pre style="padding-left: 60px">episode_count = <span>300</span></pre>
<ol start="5">
<li>Next, we can start to iterate through the episodes, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>for </span>e <span>in </span><span>range</span>(episode_count):<span><br/></span><span>    </span><span>print</span>(<span>"Episode " </span>+ <span>str</span>(e) + <span>"/" </span>+ <span>str</span>(episode_count))</pre>
<ol start="6">
<li>Each episode has to be started with a state based on the data and window size. The inventory of stocks is initialized before going through the data:</li>
</ol>
<pre style="padding-left: 60px">    state = getState(data<span>, </span><span>0</span><span>, </span>window_size + <span>1</span>)<br/>    agent.inventory = []<br/>    total_profit = <span>0<br/></span><span>    </span>done = <span>False</span></pre>
<ol start="7">
<li>Next, start to iterate over every day of the stock data. The action probability is predicted by the agent, based on the <kbd>state</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>    </span><span>for </span>t <span>in </span><span>range</span>(l):<br/>        action = agent.act(state)<br/>        action_prob = agent.actor_local.model.predict(state)<br/><br/>        next_state = getState(data<span>, </span>t + <span>1</span><span>, </span>window_size + <span>1</span>)<br/>        reward = <span>0<br/></span></pre>
<ol start="8">
<li>The <kbd>action</kbd> can be held, if the agent decides not to do anything with the stock. Another possible action is to buy (hence, the stock will be added to the inventory), as follows:</li>
</ol>
<pre><span>        </span><span>if </span>action == <span>1</span>:<br/>            agent.inventory.append(data[t])<br/>            <span>print</span>(<span>"Buy:" </span>+ formatPrice(data[t]))</pre>
<ol start="9">
<li>If the <kbd>action</kbd> is <kbd>2</kbd>, the agent sells the stocks and removes it from the inventory. Based on the sale, the profit (or loss) is calculated:</li>
</ol>
<pre style="padding-left: 30px">        <span>elif </span>action == <span>2 </span><span>and </span><span>len</span>(agent.inventory) &gt; <span>0</span>:  <span># sell<br/></span><span>            </span>bought_price = agent.inventory.pop(<span>0</span>)<br/>            reward = <span>max</span>(data[t] - bought_price<span>, </span><span>0</span>)<br/>            total_profit += data[t] - bought_price<br/>            <span>print</span>(<span>"sell: " </span>+ formatPrice(data[t]) + <span>"| profit: " </span>+ <br/>              formatPrice(data[t] - bought_price))<br/><br/>        <span>if </span>t == l - <span>1</span>:<br/>            done = <span>True<br/></span><span>        </span>agent.step(action_prob<span>, </span>reward<span>, </span>next_state<span>, </span>done)<br/>        state = next_state<br/><br/>        <span>if </span>done:<br/>            <span>print</span>(<span>"------------------------------------------"</span>)<br/>            <span>print</span>(<span>"Total Profit: " </span>+ formatPrice(total_profit))<br/>            <span>print</span>(<span>"------------------------------------------"</span>)</pre>
<ol start="10">
<li>You can see logs similar to those that follow during the training process. The stocks are bought and sold at certain prices:</li>
</ol>
<pre style="padding-left: 60px"><strong>sell: $2102.15| profit: $119.30
sell: $2079.65| profit: $107.36
Buy:$2067.64
sell: $2108.57| profit: $143.75
Buy:$2108.63
Buy:$2093.32
Buy:$2099.84
Buy:$2083.56
Buy:$2077.57
Buy:$2104.18
sell: $2084.07| profit: $115.18
sell: $2086.05| profit: $179.92
------------------------------------------
Total Profit: $57473.53</strong></pre>
<ol start="11">
<li>Next, the test data is read from the CSV file. The initial state is inferred from the data. The steps are very similar to a single episode of the training process:</li>
</ol>
<pre style="padding-left: 60px">test_data = getStockData(<span>"^GSPC Test"</span>)<br/>l_test = <span>len</span>(test_data) - <span>1<br/></span>state = getState(test_data<span>, </span><span>0</span><span>, </span>window_size + <span>1</span>)</pre>
<ol start="12">
<li>The profit starts at <kbd>0</kbd>. The agent is initialized with a zero inventory and in test mode:</li>
</ol>
<pre style="padding-left: 60px">total_profit = <span>0<br/></span>agent.inventory = []<br/>agent.is_eval = <span>False<br/></span>done = <span>False</span></pre>
<ol start="13">
<li>Next, every day of trading is iterated, and the agent can act upon the data. Every day, the agent decides an action. Based on the action, the stock is held, sold, or bought:</li>
</ol>
<pre style="padding-left: 60px"><span>for </span>t <span>in </span><span>range</span>(l_test):<br/>    action = agent.act(state)</pre>
<ol start="14">
<li>If the action is <kbd>0</kbd>, then there is no trade. The state can be called <strong>holding</strong> during that period:</li>
</ol>
<pre style="padding-left: 60px"><span>    </span>next_state = getState(test_data<span>, </span>t + <span>1</span><span>, </span>window_size + <span>1</span>)<br/>    reward = <span>0<br/></span></pre>
<ol start="15">
<li>If the action is <kbd>1</kbd>, buy the stock by adding it to the inventory, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>    </span><span>if </span>action == <span>1</span>:  <span><br/></span><span><br/></span><span>        </span>agent.inventory.append(test_data[t])<br/>        <span>print</span>(<span>"Buy: " </span>+ formatPrice(test_data[t]))</pre>
<ol start="16">
<li>If the action is <kbd>2</kbd>, the agent sells the stock by removing it from the inventory. The difference in price is recorded as a profit or a loss:</li>
</ol>
<pre style="padding-left: 60px">    <span>elif </span>action == <span>2 </span><span>and </span><span>len</span>(agent.inventory) &gt; <span>0</span>: <span><br/></span><span>        </span>bought_price = agent.inventory.pop(<span>0</span>)<br/>        reward = <span>max</span>(test_data[t] - bought_price<span>, </span><span>0</span>)<br/>        total_profit += test_data[t] - bought_price<br/>        <span>print</span>(<span>"Sell: " </span>+ formatPrice(test_data[t]) + <span>" | profit: " </span>+ formatPrice(test_data[t] - bought_price))<br/><br/>    <span>if </span>t == l_test - <span>1</span>:<br/>        done = <span>True<br/></span><span>    </span>agent.step(action_prob<span>, </span>reward<span>, </span>next_state<span>, </span>done)<br/>    state = next_state<br/><br/>    <span>if </span>done:<br/>        <span>print</span>(<span>"------------------------------------------"</span>)<br/>        <span>print</span>(<span>"Total Profit: " </span>+ formatPrice(total_profit))<br/>        <span>print</span>(<span>"------------------------------------------"</span>)</pre>
<ol start="17">
<li>Once the script starts to run, the model will get better over time through training. You can see the logs, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>Sell: $2818.82 | profit: $44.80
Sell: $2802.60 | profit: $4.31
Buy: $2816.29
Sell: $2827.22 | profit: $28.79
Buy: $2850.40
Sell: $2857.70 | profit: $53.21
Buy: $2853.58
Buy: $2833.28
------------------------------------------
Total Profit: $10427.24</strong></pre>
<p>The model has traded and made a total profit of $10,427. Please note that this style of trading is not suitable for the real world, as trading involves more costs and uncertainty; hence, this trading style could have adverse effects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Final result</h1>
                </header>
            
            <article>
                
<p class="p6"><span class="s2">After training the data, we tested it against the <kbd>test</kbd> dataset. Our model resulted in a total profit of <kbd>$10427.24</kbd>. The best thing about the model was that the profits kept improving over time, indicating that it was learning well and taking better actions.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="p5"><span class="s2">In conclusion, machine learning can be applied to several industries and can be applied very efficiently in financial markets, as you saw in this chapter. We can combine different models, as we did with reinforcement learning and time series, to produce stronger models that suit our use cases. </span><span class="s2">We discussed the use of reinforcement learning and time series to predict the stock market. We worked with an actor-critic model that determined the best action, based on the state of the stock prices, with the aim of maximizing profits. In the end, we obtained a result that boasted an overall profit and included increasing profits over time, indicating that the agent learned more with each state.</span></p>
<p class="p5"><span class="s2">In the next chapter, you will learn about the future areas of work. </span></p>


            </article>

            
        </section>
    </body></html>