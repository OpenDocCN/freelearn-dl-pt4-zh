- en: 2D HMM for Image Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce the application of HMM in the case of image
    segmentation. For image segmentation, we usually split up the given image into
    multiple blocks of equal size and then perform an estimation for each of these
    blocks. However, these algorithms usually ignore the contextual information from
    the neighboring blocks. To deal with that issue, 2D HMMs were introduced, which
    consider feature vectors to be dependent through an underlying 2D Markovian mesh.
    In this chapter, we will discuss how these 2D HMMs work and will derive parameter
    estimation algorithms for them. In this chapter, we will discuss the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo 2D HMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to 2D HMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter learning in 2D HMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recap of 1D HMM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's recap how 1D HMMs work, which we discussed in the previous chapters of
    this book. We have seen that HMM is a just a process over Markov chains. At any
    point in time, an HMM is in one of the possible states, and the next state that
    the model will transition to depends on the current state and the transition probability
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that there are *M = {1, 2, ..., M}* possible states for HMM, and the
    transition probability of going from some state *i* to state *j* is given by *a[i,j]*. For
    such a model, if at time *t-1* the model is at state *i*, then at time *t* it
    would be in state *j* with a probability of *a[i,j]*. This probability is known
    as the **transition probability**. Also, we have defined the observed variable
    in the model, which only depends on the current state of our hidden variable.
    We can define the observed variable at time *t* as *u[t]*, so let's say the emission
    distribution for the state *i* for the variable *u[t]* is given by *b[i](u[t])*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to define the initial probability, *π[i]*, as the probability
    of being in state *i* at time *t = 1*. With all these given values, we can determine
    the likelihood of observing any given sequence, ![](img/3521f387-45e1-4ef5-9b5a-c4278bcc9e0c.png), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/042c0543-12c7-41ed-80d4-b33dffcb2b12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In most situations, we assume the states to be a Gaussian mixture model; in
    which case, the previous equation can be generalized further, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65242904-b50a-4f5f-9b5f-3b7cec0002a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *u[i]* is the mean, *∑[i]* is the covariance matrix, and *k* is the dimensionality
    of the observed variable *u[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have our model defined, we can move on to the estimation method. Estimation
    is usually performed using the Baum-Welch algorithm that we saw in [Chapter 4](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml),
    *Parameter Inference using Maximum Likelihood*, which performs a maximum-likelihood
    estimation. Let *L[i](t)* be the conditional distribution of being in state *i* at
    time *t,* given the observations, and *H[i,j](t)* be the conditional probability
    of transitioning to state *j* from state *i* at time *t + 1,* again, given the
    observations. Then, we can re-estimate the mean, covariance, and transition probability
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb175f4f-5958-4b48-8001-967193bace35.png)'
  prefs: []
  type: TYPE_IMG
- en: To compute the values of *L[i](t)* and *H[i,j](t),* we use the forward-backward
    algorithm. The forward algorithm gives us the probability, *α[i](t),* of observing
    the first *t* outcomes, ![](img/c6ef61da-9aa5-474e-9394-ba29bcd71567.png), and
    being in state *i* at time *t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This probability can be evaluated using the following set of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac5f17a1-566c-42f1-bda1-226ceed116ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also define the backward probability, *β[i](t),* as the conditional probability
    of having the observations *{u[r]}r=t + 1,...,T*, given that the model is in state
    *i* at time *t*. The conditional probability *β[i](t)* can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5182d06-65fc-4a97-835b-a47b2e57898b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With these values to hand, we can compute that *L[i](t)* and *H[i,j](t)* can
    be solved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f83be13-899c-4c40-92dc-5896c50fa924.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can approximate this algorithm by assuming each observation to have resulted
    from the most likely hidden state. This allows us to simplify the Baum-Welch algorithm;
    this is commonly also known as the **Viterbi training algorithm**. Given the observed
    states, and assuming the state sequence to be ![](img/38ab31a3-1fcc-4aa2-8f9b-e47e836b3667.png) with
    the maximum conditional probability, this can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f80bb564-e3ab-40e5-bf45-04e98b052cd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/b7a2aeba-0caf-497b-886c-33f9030cc85c.png) can be computed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/351bc225-75ac-4ef2-8e3a-02286779feef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With these values, we can then compute the model parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e6edb61-4de7-4ae6-8240-9c2570a72d84.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I* is the indicator function, which returns 1 if the function's argument
    is true; otherwise, it returns 0.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have quickly reviewed the basic concepts of 1D HMMs when
    the states are parameterized using Gaussian mixture models, so we can now move
    on to 2D HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2D HMMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of work has been done regarding 2D HMMs, but the most recent work and
    well-received work has been done by Jia Li, Amir Najmi, and Robert Gray in their
    paper, *Image Classification by a Two Dimensional Hidden Markov Model.*This section
    has been written based on their work. We will start by giving the general algorithm
    they have introduced, and then, in further subsections, we will see how the algorithm
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for image classification is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide the training images into non-overlapping blocks with equal size and extract
    a feature vector for each block
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the number of states for the 2D HMM
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the model parameters based on the feature vectors and the training
    labels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Testing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to training, generate feature vectors for the testing images
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Search for the set of classes with the maximum a posteriori probability, given
    the feature vectors, according to the training model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions for the 2D HMM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will quickly go through the assumptions for the 2D HMM model and
    the derivation of how these assumptions simplify our equations. For a more detailed
    derivation, please refer to the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: We start by dividing the image into smaller blocks, from which we evaluate the
    feature vectors, and, using these feature vectors, we classify the image. In the
    case of a 2D HMM, we make the assumption that the feature vectors are generated
    by a Markov model with a state change happening once every block. We also define
    the relationship between the blocks based on which block comes before or after
    which block. A block at position *(i', j')* is said to come before the block at
    position *(i, j)* if *i'* or *i' = i* and *j' < j*. Assuming that there are *M
    = {1, 2, ... M}* states, the state of any given block at position *(i, j)* is
    denoted by *S[i,j]*, the feature vector is denoted by *u[i,j]*, and the class
    is denoted by *c[i,j]*. Another point to keep in mind is that the order of the
    blocks has been introduced just to explain the assumptions of the model, and the
    algorithm doesn't consider any order of blocks while doing the classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first assumption made by the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c3eb9bf-477f-4321-8d16-2db25feaa3c7.png)'
  prefs: []
  type: TYPE_IMG
- en: This assumption states that knowing the current state is a sufficient statistic
    for estimating the transition probabilities, which means that *u* is conditionally
    independent. Also, according to the assumption, the state transition is a Markov
    process in two dimensions, and the probability of the system entering any particular
    state depends on the state of the model in both the horizontal and vertical directions
    in the previous time and observation instance. We also assume that there is one-to-one
    mapping from state to class, so that once we know the state, the class can be
    directly computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second assumption is that the feature vectors are a Gaussian mixture distribution
    for each state. We know that any M-component Gaussian mixture can be split into
    M substates with single Gaussian distributions; therefore, for a block with state
    *s* and feature vector *u*, the probability of the distribution is given by the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58b278e3-5a42-4ffa-bcf7-4a412c916567.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *∑[s]* is the covariance matrix and *μ[s]* is the mean vector of the Gaussian
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the Markovian assumptions to simplify the evaluation of the
    probability of the states. The probability of the states for all the blocks in
    the image is denoted by *P{s[i,j] : (i,j) ∈ N}*, where *N = {(i,j) : 0 ≤ i < w,
    0 ≤ j < z}*. But before we use the Markovian assumptions to efficiently expand
    the probability, we need to prove that, given the two previous assumptions, a
    rotated form of two-dimensional Markovian property holds for the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a rotated relation of ![](img/3c09049a-47b5-4f1c-879e-b62c5ae7c047.png),
    denoted by ![](img/bf1768a6-d1d0-4807-8510-e7e986aa189e.png), which specifies ![](img/596b1745-78e3-4d41-be38-213c8116f383.png),
    if *j'' < j*, or *j'' = j* and *i'' < i*. We need to prove the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79ff27fe-7ec0-4bad-b171-b7e7136b11a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we use the previous definition of ![](img/8c06ffa1-e05e-495c-b5b2-86669d53cdd2.png) and
    introduce the following new notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d933c53-807c-49d6-977f-bd810ca54879.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding equations, we can also see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeddc6c8-43d8-4b44-8e17-34f7ac4a7e4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, to simplify notation, let''s introduce ![](img/b68105e2-f519-4505-ba80-a315a1478499.png) and ![](img/06f8b9c4-4aed-45be-8a0a-8eeb4f8e6ae9.png).
    From these we can do the following derivation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58cb7618-9a25-4eb7-b3d8-d5e79b4f6cb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expanding the conditional probability, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7250fd03-5f94-47b8-95ec-18a1dc043e5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the Markovian assumption, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fe48d95-9c3e-450e-8eba-734330c19647.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, finally, using the Markovian assumption and the assumption that the feature
    vector of a block is conditionally independent of other blocks given its state,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b01f2c6-4fda-49dc-ab90-eb64650b4f0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *m = s[i-1,j]*, *n = s[i,j-1]*, and *l = s[i,j]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we replace ![](img/4c8ad46c-f557-4b7f-a6ea-4f8d6560d501.png) with ![](img/19fe49ce-a212-4450-a337-3b8006b7747c.png),
    and replace ![](img/7eed13f5-8e55-464b-9c0e-1d43f4ca917d.png) with ![](img/90b91eb4-b566-4bf1-827e-df853411dbfb.png) in
    the derivation, all the equations will still hold. This gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dea74b78-e673-4d7c-9ce0-ef8dce4f395c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the preceding equation implies the original Markovian assumption and
    its rotated version, we can show the equivalent end of the two assumptions as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c7cdb97-5dd2-4d69-be8d-98c616b266cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can simplify the expansion of *P{s[i,j] : (i,j) ∈ N}*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5b60120-a1df-4be4-ab1a-e8b9b72d6821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w* and *z* are the numbers of rows and columns in the image, respectively,
    and *T[i]* is the sequence of states for blocks on the diagonal *i*. Next, we
    need to prove that *P(T[i]|T[i-1],...,T[0]) = P(T[i]|T[i-1])*. Assuming *T[i]
    = {s[i,0], s[i-1,1], ..., s[0,i]}*, this means *T[i-1] = {s[i-1,0], s[i-2,1],
    ..., s[0,i-1]}* and hence we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6f12b4f-77fd-4c19-a1d3-a88d2728c541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can conclude:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df39c7ac-122a-4ffa-9716-2eb4325006d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this, we get the following simplified equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3f98132-ebce-4291-8343-be8ab1f9a950.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameter estimation using EM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have the model ready, we now need to estimate the parameters of the
    model. We need to estimate the mean vectors* μ[m]*; the covariance matrices *∑[m]*;
    and the transition probabilities *a[m,n,l]*, where *m,n,l = 1,..., M*, and *M*
    is the total number of states. We will use the **expectation maximization** (**EM**)
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in previous chapters, EM is an iterative algorithm that can
    learn the maximum likelihood estimates in the case of missing data; that is, when
    we have unobserved variables in our data. Let''s say that our unobserved variable *x* is
    in the sample space *x*, and the observed variable *y is* in the sample space
    *y*. If we postulate a family of distribution *f(x|Φ)*, with parameters *Φ ∈ Ω*,
    then the distribution over *y* is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0df3b3b-5947-49b7-a79b-54219b731b50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The EM algorithm will then try to find the value of ![](img/754282b8-979e-4614-b427-d7a6abe56fad.png) that
    would maximize the value of *g(y|Φ)* given the observed *y*. The EM iteration *Φ^((p)) → Φ^((p+1))* is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E-step**: Compute *Q*(*Φ|**Φ^((p))*) where *Q(Φ''|Φ)* is the expected value
    of *log f(x|Φ'')*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M-step**: Choose *Φ^((p+1))* to be a value of *Φ ∈ Ω* that maximizes *Q*(*Φ|**Φ^((p))*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now define the terms needed for 2D HMMs:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of observed feature vectors for the entire image is ![](img/5baf8d28-1c1c-43e3-b639-1916db29e96b.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of states for the image is ![](img/36684bb8-35ff-45fc-8c9a-e5befbb557c7.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of classes for the image is ![](img/8fb5bae3-a5db-426e-b073-c21269e90748.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mapping from a state *s[i,j]* to its class is C(*s[i,j]*), and the set of
    classes mapped from states *s*, is denoted by *C(s)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s define the distribution over *x* for a 2D HMM, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7bb7c16-b1fc-4f19-a125-347c08dc501e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this, we can compute the following *log f(x|Φ'')*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfc8e4d5-0475-46bf-ad04-c6ad42bf944b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now know that, given *y*, *x* can only have a finite number of values corresponding
    to states that are consistent with the value of *y*. Therefore, the distribution
    over *x* is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d343cd48-215c-40bb-bc1b-6b457581740f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *α* is the normalizing constant and *I* is the indicator function. Now,
    for the M-step, we need to set the value of *Φ^((p+1))* to *Φ''*, which will maximize
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f08f7a9-b0b0-47e3-827d-eedceb60bd93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the previous term has two parts with an addition between them, we can
    deal with each term separately, since we are trying to maximize the total. Consider
    the first term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e4cdb78-25e4-424a-9d1a-7d3f0dba5af2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By defining ![](img/3e673b47-f5a0-4471-b1eb-e3e60933e5f0.png), the preceding
    equation can be reduced to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06a06dbc-eea3-4365-8b99-d67e16aca866.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This term is concave in ![](img/16ae97f5-a5a0-4366-8677-b50ddae3bb22.png);
    therefore, using the Lagrangian multiplier and taking the derivative we get the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d976faaa-5f1f-4853-9b72-8ab8a8016870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Coming back to maximizing the second term of ![](img/51b62008-0348-43ac-ac06-db11ff691a62.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d04d67ed-25f4-426c-9100-f30ef17b88f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, to simplify the preceding equation, we define ![](img/0bdaf41a-7daa-4727-a14e-b46a5593304a.png),
    and the preceding term becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/264efbef-64f4-4c7c-a4b4-0928a4a1f95e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, our ML estimates for our Gaussian distribution are given by the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94e685d6-6ba3-4b77-abc9-2fae6b4ef39c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To summarize the whole derivation, we can write the EM algorithm in the following
    two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the model estimation *Φ^((p))*, the parameters are updated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/31e11a3a-7e50-48eb-a615-c011fe7eadfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the term ![](img/426b5f7c-b95f-43fc-9c79-7e443ca525bf.png) can be computed
    as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/854f7887-920d-4a60-97d7-49950b0712b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The transition probability is updated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8e8ecf5-7202-414f-ab0d-a21ba88ebe43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, here, ![](img/4e68c128-e7df-4413-abe4-42050f931fdc.png) is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7250355-067f-4f1c-9d82-d2c4e2e13eff.png)'
  prefs: []
  type: TYPE_IMG
- en: By applying the preceding two equations iteratively, our algorithm will converge
    to the maximum likelihood estimation of the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter  we started with a short recap of 1D HMMs which we introduced in
    the previous chapter. Later we introduced the concepts of 2D HMMs and derived
    the various assumptions that we make for 2D HMMs to simplify our computations
    and it can be applied in image processing tasks. We then introduce a generic EM
    algorithm for learning the parameters in the case of 2D-HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at another application of HMMs in the field
    of reinforcement learning and will introduce MDP.
  prefs: []
  type: TYPE_NORMAL
