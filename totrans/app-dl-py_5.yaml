- en: Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building on fundamental concepts from C*hapter 4*, Introduction to Neural Networks
    and Deep Learning, we now move into a practical problem: can we predict Bitcoin
    prices using a deep learning model? In this chapter, we will learn how to build
    a deep learning model that attempts to do that. We will conclude this chapter
    by putting all of these components together and building a bare-bones yet complete
    fist version of a deep learning application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare data for a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the right model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Keras, a TensorFlow abstraction library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make predictions with a trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the Right Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a filled undergoing intense research activity. Among other
    things, researchers are devoted to inventing new neural network architectures
    that can either tackle new problems or increase the performance of previously
    implemented architectures. In this section, we study both old and new architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Older architectures have been used to solve a large array of problems and are
    generally considered the right choice when starting a new project. Newer architectures
    have shown great successes in specific problems, but are harder to generalize.
    The latter are interesting as references of what to explore next, but are hardly
    a good choice when starting a project.
  prefs: []
  type: TYPE_NORMAL
- en: Common Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Considering the many architecture possibilities, there are two popular architectures
    that have often been used as starting points for a number of applications: **convolutional
    neural networks** (**CNNs**) and **recurrent neural networks** (**RNNs**). These
    are foundational networks and should be considered starting points for most projects.
    We also include descriptions of another three networks, due to their relevance
    in the field: **Long-short term memory** (**LSTM**) networks, an RNN variant;
    **generative adversarial networks** (**GANs**); and deep reinforcement learning.
    These latter architectures have shown great successes in solving contemporary
    problems, but are somewhat more difficult to use.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional neural networks have gained notoriety for working with problems
    that have a grid-like structure. They were originally created to classify images,
    but have been used in a number of other areas, ranging from speech recognition
    to self-driving vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: CNN's essential insight is to use closely related data as an element of the
    training process, instead of only individual data inputs. This idea is particularly
    effective in the context of images, where a pixel located to the right of another
    pixel is related to that pixel as well, given that they form part of a larger
    composition. In this case, that composition is what the network is training to
    predict. Hence, combining a few pixels together is better than using an individual
    pixel on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name **convolution** is given to the mathematical representation of this
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8de2852a-e55b-420b-aed1-cbcdfa685677.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the convolution process Image source: Volodymyr Mnih,
    et al.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Human-level control through deep reinforcement
    learning. February 2015, Nature. Available at: [https://storage. googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional neural networks work with a set of inputs that keep altering the
    weights and biases of the networks' respective layers and nodes. A known limitation
    of this approach is that its architecture ignores the sequence of these inputs
    when determining how to change the networks' weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks were created precisely to address that problem. RNNs
    are designed to work with sequential data. This means that at every epoch, layers
    can be influenced by the output of previous layers. The memory of previous observations
    in a given sequence plays a role in the evaluation of posterior observations.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have had successful applications in speech recognition due to the sequential
    nature of that problem. Also, they are used for translation problems. Google Translate's
    current algorithm—called **Transformer**—uses an RNN to translate text from one
    language to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Transformer: A Novel Neural Network Architecture
    for Language Understanding, by Jakob Uszkoreit, Google Research Blog, August 2017\.
    Available at: [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb6536c7-db6a-4a9a-8bdd-c24d002b1c65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration from distill.pub (https://distill.pub/2016/augmented-rnns/)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2 shows that words in English are related to words in French, based on
    where they appear in a sentence. RNNs are very popular in language translation
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Long-short term memory networks are RNN variants created to address the vanishing
    gradient problem. The vanishing gradient problem is caused by memory components
    that are too distant from the current step and would receive lower weights due
    to their distance. LSTMs are a variant of RNNs that contain a memory component—called
    **forget gate**. That component can be used to evaluate how both recent and old
    elements affect the weights and biases, depending on where the observation is
    placed in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: For more details refer, The LSTM architecture was fist introduced by Sepp Hochreiter
    and Jürgen Schmidhuber in 1997\. Current implementations have had several modifiations.
    For a detailed mathematical explanation of how each component of an LSTM works,
    we suggest the article *Understanding LSTM Networks* by Christopher Olah, August
    2015, available at [http://colah.github.io/posts/2015- 08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**) were invented in 2014 by Ian
    Goodfellow and his colleagues at the University of Montreal. GANs suggest that,
    instead of having one neural network that optimizes weights and biases with the
    objective to minimize its errors, there should be two neural networks that compete
    against each other for that purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details refer, Generative Adversarial Networks by Ian Goodfellow,et
    al, arXiv. June 10, 2014\. Available at: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).'
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have a network that generates new data (that is, "fake" data) and a network
    that evaluates the likelihood of the data generated by the fist network to be
    real or "fake". They compete because both learn: one learns how to better generate
    "fake" data, and the other learns how to distinguish if the data it is presented
    with is real or not. They iterate on every epoch until they both converge. That
    is the point when the network that evaluates generated data cannot distinguish
    between "fake" and real data any longer.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs have been successfully used in fields where data has a clear topological
    structure. Its original implementation used a GAN to create synthetic images of
    objects, people's faces, and animals that were similar to real images of those
    things. This domain of image creation is where GANs are used the most frequently,
    but applications in other domains occasionally appear in research papers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a256fdab-16a0-4f18-b2ab-907ddbecd12b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Image that shows the result of different GAN algorithms in changing
    people''s faces based on a given emotion. Source: StarGAN Project. Available at
    [https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN).'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original DRL architecture was championed by DeepMind, a Google-owned artificial
    intelligence research organization based in the UK.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of DRL networks is that they are unsupervised in nature and that
    they learn from trial-and-error, only optimizing for a reward function. That is,
    different than other networks (which use supervised approaches to optimize for
    how wrong the predictions are, compared to what is known to be right), DRL networks
    do not know of a correct way of approaching a problem. They are simply given the
    rules of a system and are then rewarded every time they perform a function correctly.
    This process, which takes a very large number of iterations, eventually trains
    networks to excel in a number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Human-level control through deep reinforcement
    learning, by Volodymyr Mnih et al., February 2015, Nature. Available at: [https://storage.googleapis.com/deepmind-media/dqn/
    DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Reinforcement Learning models gained popularity after DeepMind created
    AlphaGo, a system that plays the game Go better than professional players. DeepMind
    also created DRL networks that learn how to play video games at a superhuman level,
    entirely on their own:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42cf84e8-ec2b-454b-9198-5e47b6bb8c13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Image that represents how the DQN algorithm works'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, DQN was created by DeepMind to beat Atari games.
    The algorithm uses a deep reinforcement learning solution to continuously increase
    its reward. Image source: [https://keon.io/ deep-q-learning/](https://keon.io/deep-q-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture**  | **Data Structure** | **Successful Applications**  |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional neural networks (CNNs)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Grid-like topological structure (that is, images)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image recognition and classification  |'
  prefs: []
  type: TYPE_TB
- en: '| Recurrent neural network (RNN) and long-short term memory (LSTM) networks  |
    Sequential data (that is, time-series data)  | Speech recognition, text generation,
    and translation  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative adversarial networks (GANs)  | Grid-like topological structure (that
    is, images)  | Image generation  |'
  prefs: []
  type: TYPE_TB
- en: '| Deep reinforcement learning (DRL)  | System with clear rules and a clearly
    defined reward function  | Playing video games and selfdriving vehicles  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Different neural network architectures have shown success in different
    filelds. The networks'' architecture is typically related to the structure of
    the problem at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before building a deep learning model, one more step is necessary: data normalization.'
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization is a common practice in machine learning systems. Particularly
    regarding neural networks, researchers have proposed that normalization is an
    essential technique for training RNNs (and LSTMs), mainly because it decreases
    the network's training time and increases the network's overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, *Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift* by Sergey Ioffe et. al., arXiv,March 2015\.
    Available at: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  prefs: []
  type: TYPE_NORMAL
- en: Deciding on a normalization technique varies, depending on the data and the
    problem at hand. The following techniques are commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When data is normally distributed (that is, Gaussian), one can compute the distance
    between each observation as a standard deviation from its mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'This normalization is useful when identifying how distant data points are from
    more likely occurrences in the distribution. The Z-score is defiled by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dea0c619-724b-4811-89d7-7b94ad031f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/71b9e4fd-3854-4202-8792-5872dca200f2.png) is the ![](img/88fdc3b7-222c-40df-90fd-19de4ef61ac3.png)observation,
    ![](img/64f15854-2c09-4dc7-8ae2-66cbba67e794.png)the mean, and ![](img/e519bc15-4a32-4535-ad1a-f4803d95c3ea.png)the
    standard deviation of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Standard score article (Z-score). Wikipedia.Available
    at: [https://en.wikipedia.org/wiki/Standard_score](https://en.wikipedia.org/wiki/Standard_score).'
  prefs: []
  type: TYPE_NORMAL
- en: Point-Relative Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This normalization computes the difference of a given observation in relation
    to the fist observation of the series. This kind of normalization is useful to
    identify trends in relation to a starting point. The point-relative normalization
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77bd28e6-aa6b-44ad-94b6-d4a342a0d074.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/9d1a43fa-2b2e-4f4c-8b23-362390dae545.png)is the ![](img/ad642e62-6059-4768-b872-19f70caa4a88.png)observation
    and![](img/77b76bcb-3fbd-43e7-8c5d-ed149d7e7afb.png) is the fist observation of
    the series.
  prefs: []
  type: TYPE_NORMAL
- en: As suggested by Siraj Raval in his video, *How to Predict Stock Prices Easily
    Intro to Deep Learning* *#7*, available on YouTube at: [https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w).
  prefs: []
  type: TYPE_NORMAL
- en: Maximum and Minimum Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This normalization computes the distance between a given observation and the
    maximum and minimum values of the series. This normalization is useful when working
    with series in which the maximum and minimum values are not outliers and are important
    for future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This normalization technique can be applied with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/553451e1-0d79-4ddb-a484-4501ea4298dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/0d60a1c3-779b-4517-a09f-5fc81d3f41d0.png)is the ![](img/4c8b2958-c7c3-4b0e-a6e9-88d3aeee18ca.png)observation,
    *O* represents a vector with all O values, and the functions min (O) and max (O)
    represent the minimum and maximum values of the series, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: During next Activity, Exploring the Bitcoin Dataset and Preparing Data for Model,
    we will prepare available Bitcoin data to be used in our LSTM mode. That includes
    selecting variables of interest, selecting a relevant period, and applying the
    preceding point-relative normalization technique.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring Your Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to researchers, practitioners spend much less time determining which
    architecture to choose when starting a new deep learning project. Acquiring data
    that represents a given problem correctly is the most important factor to consider
    when developing these systems, followed by the understanding of the datasets inherent
    biases and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When starting to develop a deep learning system, consider the following questions
    for reflection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Do I have the right data?** This is the hardest challenge when training a
    deep learning model. First, define your problem with mathematical rules. Use precise
    definitions and organize the problem in either categories (classification problems)
    or a continuous scale (regression problems). Now, how can you collect data about
    those metrics?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do I have enough data?** Typically, deep learning algorithms have shown to
    perform much better in large datasets than in smaller ones. Knowing how much data
    is necessary to train a high-performance algorithm depends on the kind of problem
    you are trying to address, but aim to collect as much data as you can.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Can I use a pre-trained model?** If you are working on a problem that is
    a subset of a more general application—but within the same domain—consider using
    a pretrained model. Pretrained models can give you a head start on tackling the
    specific patterns of your problem, instead of the more general characteristics
    of the domain at large. A good place to start is the official TensorFlow repository 
    ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/40b2f22f-501b-4608-b702-668971502661.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Decision-tree of key reflection questions to be made at the beginning
    of a deep learning project'
  prefs: []
  type: TYPE_NORMAL
- en: In certain circumstances, data may simply not be available. Depending on the
    case, it may be possible to use a series of techniques to effectively create more
    data from your input data. This process is known as **data augmentation** and
    has successful application when working with image recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: A good reference is the article Classifying plankton with deep neural networks
    available at [http://benanne.github.io/2015/03/17/plankton.html](http://benanne.github.io/2015/03/17/plankton.html).
    The authors show a series of techniques for augmenting a small set of image data
    in order to increase the number of training samples the model has.
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Exploring the Bitcoin Dataset and Preparing Data for Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using a public dataset originally retrieved from CoinMarketCap, a
    popular website that tracks different cryptocurrency statistics. The dataset has
    been provided alongside this chapter and will be used.
  prefs: []
  type: TYPE_NORMAL
- en: We will be exploring the dataset using Jupyter Notebooks. Jupyter Notebooks
    provide Python sessions via a web-browser that allows you to work with data interactively.
    They are a popular tool for exploring datasets. They will be used in activities
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your terminal, navigate to the directory `Chapter_5/activity_3` and execute
    the following command to start a Jupyter Notebook instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open the URL provided by the application in your browser. You should be
    able to see a Jupyter Notebook page with a number of directories from your file
    system. You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f2a7e42-7770-466a-b1bd-4d1232d1d6e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Terminal image after starting a Jupyter Notebook instance. Navigate
    to the URL show in a browser, and you should be able to see the Jupyter Notebook
    landing page.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, navigate to the directories and click on the file Activity `Exploring_Bitcoin_
    Dataset.ipynb`. This is a Jupyter Notebook file that will be opened in a new browser
    tab. The application will automatically start a new Python interactive session
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d520e1b2-f642-4877-9885-3a6d2bf9ab1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Landing page of your Jupyter Notebook instance'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0eeb900e-4592-4edd-b97b-2f3516e4b4c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Image of the Notebook Activity_Exploring_Bitcoin_Dataset.ipynb. You
    can now interact with that Notebook and make modifications.'
  prefs: []
  type: TYPE_NORMAL
- en: After opening our Jupyter Notebook, let's now explore the Bitcoin data made
    available with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset `data/bitcoin_historical_prices.csv` contains measurements of Bitcoin
    prices since early 2013\. The most recent observation is on November 2017—the
    dataset comes from `CoinMarketCap`, an online service that is updated daily. It
    contains eight variables, two of which (date and week) describe a time period
    of the data—these can be used as indices—and six others (`open`, `high`, `low`,
    `close`, `volume`, and `market_ capitalization`) that can be used to understand
    how the price and value of Bitcoin has changed over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable**  | **Description**  |'
  prefs: []
  type: TYPE_TB
- en: '| `date`  | Date of the observation.  |'
  prefs: []
  type: TYPE_TB
- en: '| `iso_week`  | Week number for a given year.  |'
  prefs: []
  type: TYPE_TB
- en: '| `open`  | Open value for a single Bitcoin coin.  |'
  prefs: []
  type: TYPE_TB
- en: '| `high`  | Highest value achieved during a given day period.  |'
  prefs: []
  type: TYPE_TB
- en: '| `low`  | Lowest value achieved during a given day period.  |'
  prefs: []
  type: TYPE_TB
- en: '| `close`  | Value at the close of the transaction day.  |'
  prefs: []
  type: TYPE_TB
- en: '| `volume`  | The total volume of Bitcoin that was exchanged during that day.  |'
  prefs: []
  type: TYPE_TB
- en: '| `market_capitalization`  | Market capitalization, which is explained by Market
    Cap = Price *Circulating Supply.  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Available variables (that is, columns) in the Bitcoin historical prices
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the open Jupyter Notebook instance, let''s now explore the time-series
    of two of those variables: `close` and `volume`. We will start with those time-series
    to explore price-fluctuation patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the open instance of the Jupyter Notebook Activity `Exploring_Bitcoin_
    Dataset.ipynb`. Now, execute all cells under the header Introduction. This will
    import the required libraries and import the dataset into memory.
  prefs: []
  type: TYPE_NORMAL
- en: After the dataset has been imported into memory, move to the Exploration section.
    You will find a snippet of code that generates a time-series plot for the close
    variable. Can you generate the same plot for the `volume` variable?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f68fa8a7-9123-44f9-93fd-afbe5ac1a1b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Time-series plot of the closing price for Bitcoin from the close
    variable. Reproduce this plot,but using the volume variable in a new cell below
    this one.'
  prefs: []
  type: TYPE_NORMAL
- en: You will have most certainly noticed that both variables surge in 2017\. This
    reflects the current phenomenon that both the prices and value of Bitcoin have
    been continuously growing since the beginning of that year.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7278be-375b-4533-b4b7-22f1cd2daf27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Closing price of Bitcoin coins in USD. Notice an early spike by
    late 2013 and early 2014\. Also, notice how the recent prices have skyrocketed
    since the beginning of 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c239d72f-ae1c-4050-b983-c461063fde59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The volume of transactions of Bitcoin coins (in USD) shows that
    starting in 2017, a trend starts in which a significantly larger amount of Bitcoin
    is being transacted in the market. The total daily volume varies much more than
    daily closing prices.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, we notice that for many years, Bitcoin prices did not fluctuate as much
    as in recent years. While those periods can be used by a neural network to understand
    certain patterns, we will be excluding older observations, given that we are interested
    in predicting future prices for not-too-distant periods. Let's filter the data
    for 2016 and 2017 only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the Preparing Dataset for Model section. We will use the pandas
    API for filtering the data for the years 2016 and 2017\. Pandas provides an intuitive
    API for performing this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The variable `bitcoin_recent` now has a copy of our original bitcoin dataset,
    but filtered to the observations that are newer or equal to January 1, 2016.
  prefs: []
  type: TYPE_NORMAL
- en: As our final step, we now normalize our data using the point-relative normalization
    technique described in the *Data Normalization* section. We will only normalize
    two variables (close and volume), because those are the variables that we are
    working to predict.
  prefs: []
  type: TYPE_NORMAL
- en: In the same directory containing this chapter, we have placed a script called
    `normalizations.py`. That script contains the three normalization techniques described
    in this chapter. We import that script into our Jupyter Notebook and apply the
    functions to our series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the Preparing Dataset for Model section. Now, use the `iso_week`
    variable to group all the day observations from a given week using the pandas
    method `groupby()` . We can now apply the normalization function `normalizations.point_relative_
    normalization()` directly to the series within that week. We store the output
    of that normalization as a new variable in the same pandas dataframe using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable `close_point_relative_normalization` now contains the normalized
    data for the variable `close`. Do the same with the variable `volume`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8daa1c0-9449-464b-abeb-05250c0ac72c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Image of Jupyter Notebook focusing on section where the normalization
    function is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: The normalized close variable contains an interesting variance pattern every
    week. We will be using that variable to train our LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8412f325-783f-478b-8862-adc14683b416.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Plot that displays the series from the normalized variable close_point_relative_normalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to evaluate how well our model performs, we need to test its accuracy
    versus some other data. We do that by creating two datasets: a training set and
    a test set. In this activity, we will use 80 percent of the dataset to train our
    LSTM model and 20 percent to evaluate its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the data is continuous and in the form of a time series, we use
    the last 20 percent of available weeks as a test set and the fist 80 percent as
    a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad7ee5d6-370e-49b2-bad4-3871d4146d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Using weeks to create a training and a test set'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, navigate to the Storing Output section and save the filtered variable
    to disk, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we explored the Bitcoin dataset and prepared it for a deep
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that during the year 2017, the prices of Bitcoin skyrocketed. This
    phenomenon takes a long time to take place—and may be influenced by a number of
    external factors that this data alone doesn't explain (for instance, the emergence
    of other cryptocurrencies). We also used the point-relative normalization technique
    to process the Bitcoin dataset in weekly chunks. We do this to train an LSTM network
    to learn the weekly patterns of Bitcoin price changes so that it can predict a
    full week into the future. However, Bitcoin statistics show significant fluctuations
    on a weekly basis. Can we predict the price of Bitcoin in the future?
  prefs: []
  type: TYPE_NORMAL
- en: What will those prices be seven days from now? We will be building a deep learning
    model to explore that question in our next section using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Using Keras as a TensorFlow Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses on Keras. We are using Keras because it simplifies the
    TensorFlow interface into general abstractions. In the backend, the computations
    are still performed in TensorFlow—and the graph is still built using TensorFlow
    components—but the interface is much simpler. We spend less time worrying about
    individual components, such as variables and operations, and spend more time building
    the network as a computational unit. Keras makes it easy to experiment with different
    architectures and hyperparameters, moving more quickly towards a performant solution.
  prefs: []
  type: TYPE_NORMAL
- en: As of TensorFlow 1.4.0 (November 2017), Keras is now officially distributed
    with TensorFlow as `tf.keras`. This suggests that Keras is now tightly integrated
    with TensorFlow and that it will likely continue to be developed as an open source
    tool for a long period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Model Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in C*hapter 4*, *Introduction to Neural Networks and Deep Learning*,
    LSTM networks also have input, hidden, and output layers. Each hidden layer has
    an activation function which evaluates that layer's associated weights and biases.
    As expected, the network moves data sequentially from one layer to another and
    evaluates the results by the output at every iteration (that is, an epoch).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides intuitive classes that represent each one of those components:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component**  | **Keras Class**  |'
  prefs: []
  type: TYPE_TB
- en: '| High-level abstraction of a complete sequential neural network.  | `keras.models.Sequential()`  |'
  prefs: []
  type: TYPE_TB
- en: '| Dense, fully-connected layer.  | `keras.layers.core.Dense()`  |'
  prefs: []
  type: TYPE_TB
- en: '| Activation function.  | `keras.layers.core.Activation()`  |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM recurrent neural network. This class contains components that are exclusive
    to this architecture, most of which are abstracted by Keras.  | `keras.layers.recurrent.LSTM()`  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Description of key components from the Keras API. We will be using
    these components to build a deep learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: Keras' `keras.models.Sequential()` component represents a whole sequential neural
    network. That Python class can be instantiated on its own, then have other components
    added to it subsequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are interested in building an LSTM network because those networks perform
    well with sequential data—and time-series is a kind of sequential data. Using
    Keras, the complete LSTM network would be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 1*: LSTM implementation using Keras'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation will be further optimized in *Chapter 6*, *Model Evaluation
    and Optimization*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras abstraction allows for one to focus on the key elements that make a deep
    learning system more performant: what the right sequence of components is, how
    many layers and nodes to include, and which activation function to use. All of
    these choices are determined by either the order in which components are added
    to the instantiated `keras.models. Sequential()` class or by parameters passed
    to each component instantiation (that is, `Activation("linear")` ). The final
    model`.compile()` step builds the neural network using TensorFlow components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the network is built, we train our network using the `model.fit()` method.
    This will yield a trained model that can be used to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet* 2.1: Usage of `model.fit()`'
  prefs: []
  type: TYPE_NORMAL
- en: The variables `X_train` and `Y_train` are, respectively, a set used for training
    and a smaller set used for evaluating the loss function (that is, testing how
    well the network predicts data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can make predictions using the `model.predict()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet* 2.2: Usage of `model.predict()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous steps cover the Keras paradigm for working with neural networks.
    Despite the fact that different architectures can be dealt with in very different
    ways, Keras simplifies the interface for working with different architectures
    by using three components - network architecture, fit, and predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5e7c37a-bb2b-4af7-91f9-5ad1c648a012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The Keras neural network paradigm: A. design a neural network architecture,
    B. Train a neural network (or Fit), and C. Make predictions'
  prefs: []
  type: TYPE_NORMAL
- en: Keras allows for much greater control within each of those steps. However, its
    focus is to make it as easy as possible for users to create neural networks in
    as little time as possible. That means that we can start with a simple model,
    then add complexity to each one of the steps above to make that initial model
    perform better.
  prefs: []
  type: TYPE_NORMAL
- en: We will take advantage of that paradigm during our upcoming activity and chapters.
    In the next activity, we will create the simplest LSTM network possible. Then,
    in C*hapter 6*, *Model Evaluation and Optimization*, we will continuously evaluate
    and alter that network to make it more robust and performant.
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Creating a TensorFlow Model Using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this activity, we will create an LSTM model using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Keras serves as an interface for lower-level programs; in this case, TensorFlow.
    When we use Keras to design our neural network, that neural network is *compiled*
    as a TensorFlow computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the open instance of the Jupyter Notebook `Activity_4_Creating_a_
    TensorFlow_Model_Using_Keras.ipynb`. Now, execute all cells under the header **Building
    a Model**. In that section, we build our fist LSTM model parametrizing two values:
    the input size of the training observation (1 equivalent for a single day) and
    the output size for the predicted period—in our case, seven days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cc954a0-193b-4adb-9fa2-fc4ccf80e5fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Use the Jupyter Notebook `Activity_4_Creating_a_TensorFlow_Model_Using_Keras.ipynb`
    to build the same model from the *Model Components* section, parametrizing the
    period length of input and of output to allow for experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: After the model is compiled, we proceed to storing it as an `h5 file` on disk.
    It is a good practice to store versions of your model on disk occasionally, so
    that you keep a version of the model architecture alongside its predictive capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still on the same Jupyter Notebook, navigate to the header **Saving Model**.
    In that section, we will store the model as a file on disk with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The model ''`bitcoin_lstm_v0.h5`'' hasn''t been trained yet. When saving a
    model without prior training, one effectively only saves the architecture of the
    model. That same model can later be loaded by using Keras'' `load_model()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You may encounter the following warning when loading the Keras library: Using
    TensorFlow backend. Keras can be configured to use an other backend instead of
    TensorFlow (that is, Theano). In order to avoid this message, you can create a
    file called `keras.json` and configure its backend there. The correct configuration
    of that file depends on your system. Hence, it is recommended that you visit Keras''
    official documentation on the topic at [https://keras.io/backend/](https://keras.io/backend/)[.](https://keras.io/backend/)'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to build a deep learning model using Keras,
    an interface for TensorFlow. We studied core components from Keras and used those
    components to build the fist version of our Bitcoin price-predicting system based
    on an LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will discuss how to put all the components from this
    chapter together into a (nearly complete) deep learning system. That system will
    yield our very fist predictions, serving as a starting point for future improvements.
  prefs: []
  type: TYPE_NORMAL
- en: From Data Preparation to Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses on the implementation aspects of a Deep Learning system.
    We will use the Bitcoin data from, *Choosing the Right Model Architecture* and
    the Keras knowledge from, *Using Keras as a TensorFlow Interface* to put both
    of these components together. This section concludes the chapter by building a
    system that reads data from a disk and feeds it into a model as a single piece
    of software.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks can take long periods of time to train. Many factors affect
    how long that process may take. Among them, three factors are commonly considered
    the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: The network's architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many layers and neurons the network has
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much data there is to be used in the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other factors may also greatly impact how long a network takes to train, but
    most of the optimization that a neural network can have when addressing a business
    problem comes from exploring those three.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the normalized data from our previous section. Recall that
    we have stored the training data in a file called `train_dataset.csv`. We will
    load that dataset into memory using pandas for easy exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a32c3f11-9d76-4325-8350-37b8eefd48eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Table showing the first five rows of the training dataset loaded
    from the `train_d–ataset.csv` file'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the series from the variable `close_point_relative_normalization`,
    which is a normalized series of the Bitcoin closing prices—from the variable close—since
    the beginning of 2016.
  prefs: []
  type: TYPE_NORMAL
- en: The variable `close_point_relative_normalization` has been normalized on a weekly
    basis. Each observation from the week's period is made relative to the difference
    from the closing prices on the fist day of the period. This normalization step
    is important and will help our network train faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3c97bde-f62a-43c2-8f94-5377459c1fb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Plot that displays the series from the normalized variable close_point_relative_normalization.
    This variable will be used to train our LSTM model.'
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping Time-Series Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks typically work with vectors and tensors, both mathematical objects
    that organize data in a number of dimensions. Each neural network implemented
    in Keras will have either a vector or a tensor that is organized according to
    a specification as input. At fist, understanding how to reshape the data into
    the format expected by a given layer can be confusing. To avoid confusion, it
    is advised to start with a network with as little components as possible, then
    add components gradually. Keras' official documentation (under the section **Layers**)
    is essential for learning about the requirements for each kind of layer.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras official documentation is available at [https://keras.io/ layers/core/](https://keras.io/layers/core/).
    That link takes you directly to the Layers section.
  prefs: []
  type: TYPE_NORMAL
- en: '`NumPy` is a popular Python library used for performing numerical computations.
    It is used by the deep learning community to manipulate vectors and tensors and
    prepare them for deep learning systems. In particular, the `numpy.reshape()` method
    is very important when adapting data for deep learning models. That model allows
    for the manipulation of NumPy arrays, which are Python objects analogous to vectors
    and tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: We now organize the prices from the variable `close_point_relative_normalization`
    using the weeks of both 2016 and 2017\. We create distinct groups containing seven
    observations each (one for each day of the week) for a total of 77 complete weeks.
  prefs: []
  type: TYPE_NORMAL
- en: We do that because we are interested in predicting the prices of a week's worth
    of trading.
  prefs: []
  type: TYPE_NORMAL
- en: We use the ISO standard to determine the beginning and the end of a week. Other
    kinds of organizations are entirely possible. This one is simple and intuitive
    to follow, but there is room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM networks work with three-dimensional tensors. Each one of those dimensions
    represents an important property for the network. These dimensions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Period length**: The period length, that is, how many observations there
    are on a period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of periods**: How many periods are available in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of features**: Number of features available in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our data from the variable `close_point_relative_normalization` is currently
    a one dimensional vector—we need to reshape it to match those three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a week's period. Hence, our period length is seven days (period
    length = 7). We have 77 complete weeks available in our data. We will be using
    the very last of those weeks to test our model against during its training period.
    That leaves us with 76 distinct weeks (number of periods = 76). Finally, we will
    be using a single feature in this network (number of features = 1)—we will include
    more features in future versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we reshape the data to match those dimensions? We will be using a combination
    of base Python properties and the `reshape()` from the `numpy library`. First,
    we create the 76 distinct week groups with seven days each using pure Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 3: Python code snippet that creates distinct week groups'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting variable data is a variable that contains all the right dimensions.
    The Keras LSTM layer expects these dimensions to be organized in a specific order:
    number of features, number of observations, and period length.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s reshape our dataset to match that format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4: Python code snippet that creates distinct week groups'
  prefs: []
  type: TYPE_NORMAL
- en: Each Keras layer will expect its input to be organized in specific ways. However,
    Keras will reshape data accordingly in most cases. Always refer to the Keras documentation
    on layers ([https://keras.io/layers/ core/](https://keras.io/layers/core/)) before
    adding a new layer or if you encounter issues with the shape layers expect.
  prefs: []
  type: TYPE_NORMAL
- en: '*Snippet 4* also selects the very last week of our set as a validation set
    (`via data[-1]` ). We will be attempting to predict the very last week in our
    dataset by using the preceding 76 weeks. The next step is to use those variables
    to fit our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 5*: Snippet shows how to train our model'
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs are computationally expensive models. They may take up to file minutes
    to train with our dataset in a modern computer. Most of that time is spent at
    the beginning of the computation, when the algorithm creates the full computation
    graph. The process gains speed after it starts training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01635e04-55ec-49fd-9de8-911f20ea04f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Graph that shows the results of the loss function evaluated at each
    epoch'
  prefs: []
  type: TYPE_NORMAL
- en: This compares what the model predicted at each epoch, then compares with the
    real data using a technique called mean-squared error. This plot shows those results.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a glance, our network seems to perform very well: it starts with a very
    small error rate that continuously decreases. Now, what do our predictions tell
    us?'
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After our network has been trained, we can now proceed to making predictions.
    We will be making predictions for a future week beyond our time period.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have trained our model with model.fit(), making predictions is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 6*: Making a prediction using the same data that we previously used
    for training'
  prefs: []
  type: TYPE_NORMAL
- en: We use the same data for making predictions as the data used for training (`the
    X_train variable`). If we have more data available, we can use that instead—given
    that we reshape it to the format the LSTM requires.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a neural network overfits to a validation set, it means that it learns
    patterns present in the training set, but is unable to generalize it to unseen
    data (for instance, the test set). During our next chapter, we will learn how
    to avoid over-fitting and create a system for both evaluating our network and
    increasing its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4b1d78c-54be-448e-aa90-9d0cb95d865a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: After de-normalization, our LSTM model predicted that in late July
    2017, the prices of Bitcoin would increase from $2,200 to roughly $2,800, a 30
    percent increase in a single week'
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Assembling a Deep Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this activity, we bring together all the essential pieces for building a
    basic deep learning system: data, model, and prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to use Jupyter Notebooks, and will use the data prepared in
    previous exercises (`data/train_dataset.csv`) as well as the model that we stored
    locally (`bitcoin_ lstm_v0.h5`).
  prefs: []
  type: TYPE_NORMAL
- en: 'After starting a Jupyter Notebook instance, navigate to the Notebook called
    `Activity_5_Assembling_a_Deep_Learning_System.ipynb` and open it. Execute the
    cells from the header to load the required components and then navigate to the
    header **Shaping Data**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bc02ce10-84f8-40b3-886a-1720b68227ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Plot that displays the series from the normalized variable close_point_relative_normalization'
  prefs: []
  type: TYPE_NORMAL
- en: The `close_point_relative_normalization` variable will be used to train our
    LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by loading the dataset we prepared during our previous activities.
    We use pandas to load that dataset into memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the training dataset into memory using pandas, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, quickly inspect the dataset by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As explained in this chapter, LSTM networks require tensors with three dimensions.
    These dimensions are: period length, number of periods, and number of features.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, proceed to creating weekly groups, then rearrange the resulting array to
    match those dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to use the provided function `create_groups()` to perform this operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The default values for that function are 7 days. What would happen if you changed
    that number to a different value, for instance, 10?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, make sure to the data into two sets: training and validation. We do this
    by assigning the last week from the Bitcoin prices dataset to the evaluation set.
    We then train the network to evaluate that last week.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Separate the last week of the training data and reshape it using `numpy.reshape()`
    . Reshaping is important, as the LSTM model only accepts data organized in this
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our data is now ready to be used in training. Now we load our previously saved
    model and train it with a given number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the header **Load Our Model** and load our previously trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, train that model with our training data `X_train` and `Y_validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we store the logs of the model in a variable called history. The
    model logs are useful for exploring specific variations in its training accuracy
    and to understand how well the loss function is performing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/019c6080-5184-4600-a077-cc5315e19925.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Section of Jupyter Notebook where we load our earlier model and
    train it with new data'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's make a prediction with our trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same `data X_train`, call the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The model immediately returns a list of normalized values with the prediction
    for the next seven days. Use the `denormalize()` function to turn the data into
    US Dollar values. Use the latest values available as a reference for scaling the
    predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/020c46f7-38af-4919-a8b8-d7ce2ede143a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Section of Jupyter Notebook where we predict the prices of Bitcoin
    for the next seven days.'
  prefs: []
  type: TYPE_NORMAL
- en: Our predictions suggest a great price surge of about 30 percent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b15b2b04-3e9c-4349-8c52-f6b4a8660e64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: Projection of Bitcoin prices for seven days in the future using
    the LSTM model we just built'
  prefs: []
  type: TYPE_NORMAL
- en: 'We combine both time-series in this graph: the real data (before the line)
    and the predicted data (after the line). The model shows variance similar to the
    patterns seen before and it suggests a price increase during the following seven
    days period.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are done experimenting, save your model with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We will save this trained network for future reference and compare its performance
    with other models.
  prefs: []
  type: TYPE_NORMAL
- en: The network may have learned patterns from our data, but how can it do that
    with such a simple architecture and so little data? LSTMs are powerful tools for
    learning patterns from data. However, we will learn in our next sessions that
    they can also suffer from *overfitting*, a phenomenon common in neural networks
    in which they learn patterns from the training data that are useless when predicting
    real-world patterns. We will learn how to deal with that and how to improve our
    network to make useful predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have assembled a complete deep learning system: from data
    to prediction. The model created in this activity needs a number of improvements
    before it can be considered useful. However, it serves as a great starting point
    from which we will continuously improve.'
  prefs: []
  type: TYPE_NORMAL
- en: Our next chapter will explore techniques for measuring the performance of our
    model and will continue to make modifications until we reach a model that is both
    useful and robust.
  prefs: []
  type: TYPE_NORMAL
