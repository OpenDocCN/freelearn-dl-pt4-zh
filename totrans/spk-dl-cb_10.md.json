["```py\n%matplotlib inline\nfrom os import listdir\nfrom os.path import isfile, join\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D\nfrom keras.optimizers import Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\n```", "```py\ny =np.empty([3240,1],dtype=int)\nfor x in range(0, len(onlyfiles)):\n    if onlyfiles[x][3]=='0': y[x]=0\n    elif onlyfiles[x][3]=='1': y[x]=1\n    elif onlyfiles[x][3]=='2': y[x]=2\n    elif onlyfiles[x][3]=='3': y[x]=3\n    elif onlyfiles[x][3]=='4': y[x]=4\n    elif onlyfiles[x][3]=='5': y[x]=5\n    elif onlyfiles[x][3]=='6': y[x]=6\n    elif onlyfiles[x][3]=='7': y[x]=7\n    elif onlyfiles[x][3]=='8': y[x]=8\n    elif onlyfiles[x][3]=='9': y[x]=9\n```", "```py\n #function for cropping images to obtain only the significant part\n def crop(img):\n      a=28*np.ones(len(img)) \n      b=np.where((img== a).all(axis=1)) \n      img=np.delete(img,(b),0) \n      plt.imshow(img)\n      img=img.transpose()\n      d=28*np.ones(len(img[0]))\n      e=np.where((img== d).all(axis=1))\n      img=np.delete(img,e,0) \n      img=img.transpose()\n      print(img.shape) \n      super_threshold_indices = img < 29 \n      img[super_threshold_indices] = 0\n      plt.imshow (img)\n      return img[0:150, 0:128]\n```", "```py\n#cropping all the images\n image = np.empty([3240,150,128],dtype=int)\n for n in range(0, len(images)):\n     image[n]=crop(images[n])\n```", "```py\n print (image[22])\n print (image[22].shape)\n```", "```py\n# Split data into 80/20 split for testing and training\ntest_ind=np.random.choice(range(3240), 648, replace=False) train_ind=np.delete(range(0,len(onlyfiles)),test_ind)\n```", "```py\n # slicing the training and test images \n y1_train=y[train_ind]\n x_test=image[test_ind]\n y1_test=y[test_ind]\n```", "```py\n#reshaping the input images\n x_train = x_train.reshape(x_train.shape[0], 128, 150, 1)\n x_test = x_test.reshape(x_test.shape[0], 128, 150, 1)\n```", "```py\n #converting data to float32\n x_train = x_train.astype('float32')\n x_test = x_test.astype('float32')\n```", "```py\n #normalizing data\n x_train/=255\n x_test/=255\n #10 digits represent the 10 classes\n number_of_persons = 10\n```", "```py\n #convert data to vectors\n y_train = np_utils.to_categorical(y1_train, number_of_persons)\n y_test = np_utils.to_categorical(y1_test, number_of_persons)\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(16, (3, 3), input_shape=(128,150,1)))  \nmodel.add(Activation('relu')) \nmodel.add(Conv2D(16, (3, 3))) \nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2))) \nmodel.add(Conv2D(16,(3, 3))) \nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2))) \nmodel.add(Flatten()) \n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25)) \nmodel.add(Dense(10))\n\nmodel.add(Activation('softmax')) \n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=        ['accuracy'])\n```", "```py\n# data augmentation to minimize overfitting\ngen = ImageDataGenerator(rotation_range=8, \n        width_shift_range=0.08, shear_range=0.3,\n        height_shift_range=0.08,zoom_range=0.08)\ntest_gen = ImageDataGenerator()\ntrain_generator = gen.flow(x_train, y_train, batch_size=16) \ntest_generator = test_gen.flow(x_test, y_test, batch_size=16)\n```", "```py\nmodel.fit_generator(train_generator, epochs=5, validation_data=test_generator)\n\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Recognition Error: %.2f%%\" % (100-scores[1]*100))\n```"]