- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unleashing the Power of LLMs for Coding: A Paradigm Shift'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll be unveiling the advantages of coding with LLMs and looking
    at what is possible with code generation with LLMs such as ChatGPT and Bard (now
    Gemini). Here, you’ll find out how to start getting good results, recognize the
    limitations of code taken from LLMs, and discover practical examples of LLM applications
    in coding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This should help you to get into some coding and prompt engineering. You’ll
    get some results, then get better results, and you’ll start to make more stable
    code.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be looking at some example prompts for the LLMs and the code they produce,
    progressing to better prompts to get better code. However, this will take multiple
    prompts. Then, in [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073) , we’ll look
    more at the whole process of correcting and debugging your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling the advantages of coding with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan your LLM-powered coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting into LLM-powered coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making it work for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to an LLM/chatbot such as GPT-4 or Gemini; each requires a login. For
    GPT-4, you’ll need an OpenAI account, and for Gemini, you’ll need a Google account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python interpreter or **integrated development environment** ( **IDE** ) such
    as Spyder, PyCharm, Visual Studio, or Eclipse, or an online interpreter such as
    [https://www.online-python.com/](https://www.online-python.com/) or [https://www.w3schools.com/python/python_compiler.asp](https://www.w3schools.com/python/python_compiler.asp)
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HTML interpreter such as [https://htmledit.squarefree.com/](https://htmledit.squarefree.com/)
    , [https://onecompiler.com/html](https://onecompiler.com/html) , or [https://www.w3schools.com/tryit/](https://www.w3schools.com/tryit/)
    , or a website editor such as Wix, GoDaddy, One.com, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get the code in this book here: [https://github.com/PacktPublishing/Coding-with-ChatGPT-and-other-LLMs/tree/main/Chapter2](https://github.com/PacktPublishing/Coding-with-ChatGPT-and-other-LLMs/tree/main/Chapter2)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we’ll get into the chapter, starting with the advantages of coding with
    LLMs, and why you’d want to do this at all.
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling the advantages of coding with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why would you want to use LLMs such as Gemini or GPT-4 to give you code?
  prefs: []
  type: TYPE_NORMAL
- en: The short version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using LLMs to code is quick and makes it easy to get the code you need, translate
    between code languages, document and explain code, and share with programmers
    of different abilities.
  prefs: []
  type: TYPE_NORMAL
- en: The longer version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs boost your ability in more ways than one, including increased productivity,
    faster innovation, reduced barriers to entry, and better collaboration. We’ll
    cover the most useful advantages next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Greatly** **increased productivity** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can save a lot of time, and almost immediately get examples of relevant
    code for what you’re trying to do. The LLM will most likely give you helpful comments
    too.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can also translate or convert code from one coding language to another
    (Python to C++, for example).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can even automatically generate documentation for code, improving communication
    and understanding for other coders working on your code, or you, if you’ve forgotten
    what you were attempting to create and why.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boilerplate reduction** : LLMs can automatically generate repetitive code
    snippets, saving developers time and effort on routine tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This all means greater productivity. What about how it improves the speed of
    innovation?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Innovation acceleration** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimentation with new ideas** : LLMs can generate alternative code approaches
    and solutions, encouraging exploration and innovation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration of niche and complex algorithms** : LLMs can efficiently search
    through vast code libraries and suggest relevant algorithms for specific needs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid prototyping** : LLMs can quickly generate functional prototypes, speeding
    up the development cycle and testing new ideas'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized optimization** : LLMs can suggest code optimization based on
    specific use cases and performance requirements'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced barriers** **to entry** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning assistance** : LLMs can explain complex code concepts and answer
    questions in real time, making coding more accessible for beginners'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code completion suggestions** : LLMs can auto-complete code, providing helpful
    prompts and reducing syntax errors'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized learning materials** : LLMs can generate tailored learning materials
    based on individual needs and skill levels'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Democratization of data** : LLMs can help analyze and process large datasets,
    making data-driven development more accessible to all'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced collaboration** :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved communication** : LLMs can simplify technical explanations and generate
    code comments, enabling clearer communication between developers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility for diverse skill sets** : LLMs can bridge the gap between
    developers with different levels of expertise, facilitating knowledge sharing
    and collaboration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Team augmentation** : LLMs can act as virtual assistants, assisting developers
    with tasks and freeing them to focus on complex problems'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control assistance** : LLMs can help track changes and suggest improvements
    to code over time'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'You will still need to know something about writing code and how to do other
    tasks associated with making a program: programming. You’ll need to debug the
    code you get from LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can suggest possible fixes for identified errors, accelerating the debugging
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, those are the advantages; there are reasons why this might not be straightforward
    though and there are dangers. We’ll get into those things in later chapters: code
    refactoring, debugging, and optimization ( [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073)
    ), addressing biases and ethical concerns ( [*Chapter 5*](B21009_05.xhtml#_idTextAnchor115)
    ), navigating the legal landscape of LLM-generated code ( [*Chapter 6*](B21009_06.xhtml#_idTextAnchor137)
    ), security considerations and measures ( [*Chapter 7*](B21009_07.xhtml#_idTextAnchor180)
    ), and limitations of coding with LLMs ( [*Chapter 8*](B21009_08.xhtml#_idTextAnchor203)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: To generate good, workable code, like most things, it’s important to plan.
  prefs: []
  type: TYPE_NORMAL
- en: These are some powerful reasons why using LLMs to assist with your coding can
    be advantageous. In the next section, you can learn about how to actually implement
    LLM tools in coding, starting with the planning phase.
  prefs: []
  type: TYPE_NORMAL
- en: Planning your LLM-powered coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make sure you have good code, you’re going to need to plan what you will
    need well.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may need to answer questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the purpose of the code/software?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who will use your software?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where will the software be used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will the user interaction work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data sources will be needed, if any?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data format will you use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will you plumb in the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will your software look like? GUI or command-line interface?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before leaping into the exciting world of LLM-powered coding, taking a moment
    to meticulously plan your project is crucial for ensuring the creation of robust,
    effective code. It’s like sketching out a blueprint before building a house –
    you prioritize stability and functionality before adding the finishing touches.
    Here are some key questions to ponder.
  prefs: []
  type: TYPE_NORMAL
- en: 1. Understanding your purpose – unveiling the why
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every great creation starts with a purpose. Your LLM-powered code is no exception.
    Before diving into technical nuances, pause and truly define its raison d’être.
    What problem are you solving? What specific task will it automate? Is it about
    streamlining an existing process, creating a completely new solution, or exploring
    innovative possibilities?
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly articulating your goals is akin to giving the LLM a roadmap. The clearer
    your vision, the more effectively it can focus its generation capabilities on
    relevant code aligned with your aspirations. Think of it as a conversation: “
    *This is what I want to achieve; LLM, guide me to the code that makes* *it happen*
    .”'
  prefs: []
  type: TYPE_NORMAL
- en: 2. Identifying your audience – tailoring the experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your software being used. Who is interacting with it? Are they tech-savvy
    individuals familiar with complex interfaces, or everyday users seeking a familiar
    and intuitive experience? Identifying your target audience is crucial, as it shapes
    the LLM’s output in significant ways.
  prefs: []
  type: TYPE_NORMAL
- en: For tech-savvy audiences, the LLM might focus on generating efficient and powerful
    code, even if it involves more technical elements. But when catering to everyday
    users, the emphasis shifts toward creating a user-friendly interface, prioritizing
    clarity and ease of use. Think of it as designing a building – you consider your
    audience’s needs and preferences to create a welcoming and accessible space.
  prefs: []
  type: TYPE_NORMAL
- en: 3. Defining the environment – where your code calls home
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your code having a physical address. Where will it reside? Will it be
    accessible online as a web application, readily available on devices as a mobile
    app, or tucked away on specific hardware as local software? Understanding the
    deployment environment is vital for the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Different environments have distinct requirements. Web applications need optimization
    for online accessibility, mobile apps demand portability and resource efficiency,
    and local software might prioritize offline functionality. Each context influences
    the LLM’s code generation, ensuring it meets the specific needs of its deployment
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 4. Mapping user interaction – charting the navigation flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think about how users will interact with your software. Will they navigate through
    graphical elements such as buttons and menus, or will they rely on text commands
    in a terminal-like interface? Perhaps even voice commands are in the mix! Defining
    the interaction style significantly impacts the LLM’s output.
  prefs: []
  type: TYPE_NORMAL
- en: For **graphical user interfaces** ( **GUIs** ), the LLM prioritizes code for
    element placement, responsiveness, and intuitive interaction handling. Think of
    it as drawing a map – you guide the LLM in creating a clear and efficient navigational
    flow for users. In contrast, for **command-line interfaces** ( **CLIs** ), the
    focus shifts toward text input, output formatting, and command parsing, helping
    users interact with the software through clear and concise textual commands.
  prefs: []
  type: TYPE_NORMAL
- en: 5. Identifying data sources – feeding the machine learning beast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Does your software rely on external data to function? Perhaps it uses user input,
    retrieves information from databases, or processes sensor readings. If so, pinpointing
    the data sources, their format (CSV, JSON, etc.), and how they will be integrated
    into the code becomes crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Think of data as the fuel that powers your LLM. By providing specific details
    about the data sources, you equip the LLM with the knowledge it needs to access
    and process this fuel effectively. This ensures the generated code can correctly
    handle data interactions, leading to accurate and robust functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 6. What data format?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Choosing the right data format is crucial for seamless integration with your
    LLM-powered code. Consider these factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compatibility** : Will your LLM natively understand the format, or will you
    need conversion steps? Popular options such as CSV, JSON, and XML are widely supported,
    while less common formats might require additional processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency** : For large datasets, consider space-efficient formats such
    as Parquet or Apache DataFrames. For structured data with frequent reads/writes,
    relational databases such as MySQL might be suitable. Analyze your data size,
    access patterns, and LLM compatibility to find the optimal balance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human readability** : If you need to understand the data manually, opt for
    human-readable formats such as CSV or JSON. However, for purely machine processing,
    more compact formats may be preferable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future needs** : Think beyond your current requirements. Will the data format
    scale well with your project’s growth? Can you easily add new data types later?
    Choosing a flexible and widely adopted format can save you headaches down the
    line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By carefully considering these factors, you’ll select a data format that enables
    smooth communication between your LLM and the data, ensuring it fuels the code
    generation process effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 7. How will you plumb in the data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Plumbing** refers to how your code retrieves, processes, and integrates data.
    With LLMs, this takes on a special importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data access mechanism** : Will your LLM directly access the data source (e.g.,
    database API)? Or will your code act as an intermediary, fetching and preprocessing
    data before feeding it to the LLM? Choose based on complexity, security needs,
    and desired control over data handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation** : Does the LLM expect specific data structures or formats?
    You might need to transform or preprocess the data (e.g., cleaning and normalization)
    before passing it to the LLM. Consider using libraries or tools specializing in
    data preparation for AI models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling** : Anticipate potential data issues such as missing values,
    inconsistencies, or errors. Design your code to handle these gracefully, potentially
    providing alternative data sources or fallback mechanisms to prevent the LLM from
    generating faulty code due to bad data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility and scalability** : As your project evolves, the data access
    and transformation might need adjustments. Ensure your code is modular and adaptable,
    allowing for easy updates and scaling to accommodate changes in data sources,
    formats, or processing requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By effectively “plumbing” the data, you provide the LLM with the clean and organized
    information it needs to generate robust and accurate code, empowering your LLM-powered
    development journey.
  prefs: []
  type: TYPE_NORMAL
- en: 8. Visualizing the interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Will your software have a user-friendly GUI with buttons, menus, and visual
    elements, or will it be a CLI driven by text commands? Choosing the interface
    is like deciding on the exterior design of your building – it determines how users
    interact and experience your creation. Here’s how your choice impacts the LLM’s
    code generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GUI** : If you choose a GUI, the LLM will generate code that adheres to UI
    design principles. This includes code for element placement, responsiveness, and
    interaction handling. Consider providing the LLM with examples of similar GUIs
    or design mockups to guide its output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CLI** : For a CLI, the LLM will focus on code for text input, output formatting,
    and command parsing. Providing examples of existing CLI tools or command structures
    can help the LLM understand your desired user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, the interface also shapes user expectations and perceptions. Choose
    wisely and ensure that the LLM generates code that aligns with your vision for
    user interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'By meticulously addressing these foundational elements, you lay a solid groundwork
    for your LLM-powered coding journey. Take note: a clear purpose, defined audience,
    appropriate environment, intuitive interaction, and accessible data sources are
    the pillars upon which successful LLM-driven code is built.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you’re ready to delve deeper into the technical aspects, confident that
    your LLM is equipped with the right map and tools to guide you toward achieving
    your coding goals.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s actually get that code!
  prefs: []
  type: TYPE_NORMAL
- en: Getting into LLM-powered coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, you get to use an LLM to produce code for you! It can just reel off a
    load of code for you from some English. If you’re lucky or the code is simple
    and commonly used enough, the code will just work straight away! This is going
    to save you a lot of time and effort!
  prefs: []
  type: TYPE_NORMAL
- en: Here is a series of prompts and their resulting code responses by Gemini. From
    these, we can start to see the sorts of things that can be done easily by chatbots
    and what is more difficult and error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: We can then analyze and see how to improve our prompts and how to debug when
    we get errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is *Prompt 1* : **Please give me an example of python code to make a button
    for a webpage where if you click the button it** **says "hello".**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is HTML, so should be put into a web page on a website editor or it can
    be tested and played with in an HTML interpreter, such as [https://htmledit.squarefree.com/](https://htmledit.squarefree.com/)
    , [https://onecompiler.com/html](https://onecompiler.com/html) , or [https://www.w3schools.com/tryit/](https://www.w3schools.com/tryit/)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In an online HTML interpreter, this is what the code looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Here is the code pasted into an HTML interpreter](img/B21009_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Here is the code pasted into an HTML interpreter'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll get a button similar to the one shown in the following figure. It’s pretty
    simple!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: The resulting button](img/B21009_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: The resulting button'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2* *.3* , we can see the output from clicking the button.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Output from clicking the button from the HTML code I got from
    Gemini](img/B21009_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Output from clicking the button from the HTML code I got from Gemini'
  prefs: []
  type: TYPE_NORMAL
- en: So, that first bit of code worked fine, with no debugging needed, apparently.
    The user clicks the **OK** button, and the window disappears. You’d have to click
    the first button, **Click me** , to get the pop-up window again. That part works
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, we’ve learned (yes, me too) that LLM chatbots can give some nice
    code to do simple things and it can work if you place it in the correct place
    to execute that code.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t normally put all the code on the page like this, as it can get unwieldy
    and too verbose for a book.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the code from Packt’s GitHub repository for this book: [https://github.com/PacktPublishing/Coding-with-ChatGPT-and-Other-LLMs/](https://github.com/PacktPublishing/Coding-with-ChatGPT-and-Other-LLMs/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code combines HTML and JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: The HTML creates a button with the text **Click me**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **sayHello()** JavaScript function displays an alert box with the message
    **Hello!** when the button is clicked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have the second prompt for Bard (now replaced by Gemini), and you’ll
    see a little bit of mathematics is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is *Prompt 2* : **Please give me an example of python code to make a simple
    script where if you put in Fahrenheit it gives Celsius but if you put in Celsius
    temperature it gives** **you Fahrenheit.**'
  prefs: []
  type: TYPE_NORMAL
- en: This will need a Python interpreter or IDE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the code from the following block, run it, and see how well it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 2* *.4* , we can see the code in the free, online interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: The Fahrenheit and Celsius converter code](img/B21009_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The Fahrenheit and Celsius converter code'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in *Figure 2* *.5* , we can see the code executing once:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: The F to C and C to F temperature conversion script; this is
    to the right on the web page in Figure 2.4](img/B21009_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: The F to C and C to F temperature conversion script; this is to
    the right on the web page in Figure 2.4'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the Celsius to Fahrenheit conversion running in
    the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Celsius to Fahrenheit conversion code in the command line](img/B21009_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Celsius to Fahrenheit conversion code in the command line'
  prefs: []
  type: TYPE_NORMAL
- en: What can we learn here? Again, simple code snippets from an LLM chatbot can
    work fine the first time! This time, the code is Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll want to improve this code later; it’s very simple but we might be able
    to make it more user-friendly: easy to use and prettier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is *Prompt 3* : **A block of code for Jupiter notebook that will if given
    a column of data display a plot** **a graph.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is for a Jupyter notebook, formerly called an IPython notebook, so the
    files from these have the **.ipynb** file format. You can get a Jupyter notebook
    in the Anaconda Python distribution. If you would like to test this code much
    quicker, you could use Google Colab ( [https://colab.research.google.com](https://colab.research.google.com)
    ) or Jupyter Notebook itself: [https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: These notebooks work by putting the Python code into cells; see *Figure 2* *.7*
    .
  prefs: []
  type: TYPE_NORMAL
- en: It’s Python code; if you don’t know how to use these notebooks, search the internet
    for Jupyter Notebook or Google Colaboratory, or use an LLM such as GPT-4 or Gemini,
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 2* *.7* , you’ll see the code in the notebook, along with a title
    and extra text. It’s always good to introduce your code block or cell.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Python code in Jupyter Notebook, which, given a column of data,
    should plot the data](img/B21009_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Python code in Jupyter Notebook, which, given a column of data,
    should plot the data'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2* *.8* shows the output plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: The plot resulting from the code from Prompt 3](img/B21009_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: The plot resulting from the code from Prompt 3'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, this plot is pretty boring, but now you know how to make a plot in Python
    or Jupyter notebooks. If you didn’t know that before, the chapter and the LLM
    have given some pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is *Prompt 4* : **A script where to ask you to load bench then it will
    help you browse for the image finally** **displayed image.**'
  prefs: []
  type: TYPE_NORMAL
- en: Load bench? What does that mean?
  prefs: []
  type: TYPE_NORMAL
- en: I got the prompt wrong! I spoke badly or the AI didn’t listen properly, but
    it still figured out what I wanted and gave me the code! Haha!
  prefs: []
  type: TYPE_NORMAL
- en: This is Python code, meant for putting in an IDE that can run Python and also
    load code libraries such as tkinter. So, that code library needs to be installed
    on whatever code environment you’re running.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see whether it works in one of those online Python interpreters… This
    is continued in [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073) , in prompt 4.
    There was a fair amount of debugging needed, and that chapter is on debugging
    and other things, so the process is there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the final code, which came from Gemini and Stack Overflow,
    and me making sure I’d installed the relevant code libraries. Google replaced
    the Bard LLM with Gemini in February 2024. This code began with Bard and finished
    with Gemini:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This should work well. What I hope you learn here, and from the relevant section
    of [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073) ( *Prompt 4 debugging* ), is
    that these LLMs can produce pretty good code but as soon as it gets a little bit
    complicated, such as code libraries and loading files, the code often needs work:
    debugging, checking online forums such as Stack Overflow, thinking about the code,
    testing it, and iterating.'
  prefs: []
  type: TYPE_NORMAL
- en: So, you still need programming skills. LLMs are not ready for complex code in
    early 2024.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is *Prompt 5* : **HTML code for a web page that will display a graphical
    user interface. However, calculator a calculator that actually functions if you
    click** **the buttons**'
  prefs: []
  type: TYPE_NORMAL
- en: Again, I got the prompt slightly wrong, but the LLM still got the idea.
  prefs: []
  type: TYPE_NORMAL
- en: It’s reassuring to understand that LLMs are somewhat capable of managing and
    understanding errors in user input.
  prefs: []
  type: TYPE_NORMAL
- en: Bard sometimes gives multiple versions for your consideration. Here, it gives
    HTML and Flask (Python). An HTML and JavaScript script worked well, so we didn’t
    need the flask version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put this into a website builder or HTML interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 2* *.9* shows the calculator GUI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: The interface looks like this](img/B21009_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The interface looks like this'
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculator is very simple; you can type in numbers or click the little
    up/down arrows that appear. You can also select the operators from the drop-down
    menu ( **+** , **-** , ***** , and **/** ). This is not a scientific calculator,
    just very simple operators here: an arithmetic calculator.'
  prefs: []
  type: TYPE_NORMAL
- en: This code is HTML and calls JavaScript embedded within a **<script>** tag that
    does the calculations. So, it’s not pure HTML code. HTML is there to do the GUI
    side of things.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that this worked the second time around. There was a minor
    bug. I copied the error message to the LLM, and it gave me the solution. That
    was a very quick debugging process and Gemini gave me usable code. [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073)
    is the debugging chapter; you can see the debugging of all the [*Chapter 2*](B21009_02.xhtml#_idTextAnchor051)
    code in that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have *Prompt 5* as a Flask app ( **app.py** ) with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code should be run in a Python-friendly IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the HTML code ( **templates/index.html** ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this code, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Save the Flask code as **app.py** in a simple, short file path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a **templates** folder and save the HTML code as **index.html** inside
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Flask app with **python app.py** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your browser and navigate to **http://localhost:5000** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should now see a functional calculator GUI on the web page. When you click
    the buttons, the corresponding values will be appended to the display input field.
    Clicking the **=** button will evaluate the expression and display the result.
    The **C** button clears the display.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This implementation uses the **eval** function in JavaScript to evaluate the
    expression, which can be a security risk if you’re accepting user input directly.
    In a production environment, you should use a safer method or a dedicated library
    for expression evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Back to the HTML code for Prompt 5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This HTML code did not give functioning code, so it needed debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Wow, this has lots of **if** , **else** , and **elif** statements!
  prefs: []
  type: TYPE_NORMAL
- en: For the debugging of this code, see [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073)
    , in the *Prompt 5* *debugging* section.
  prefs: []
  type: TYPE_NORMAL
- en: Back to the Flask code for Prompt 5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With help from Claude 3 Sonnet, this code was debugged and started to work.
    Where is the code?
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, you can get the code from Packt’s GitHub repository for this book, here:
    [https://github.com/PacktPublishing/Coding-with-ChatGPT-and-Other-LLMs/tree/main/Chapter2](https://github.com/PacktPublishing/Coding-with-ChatGPT-and-Other-LLMs/tree/main/Chapter2)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: This is okay for starters, but this is not brilliant. We need more quality,
    and this will not work straight away. There will be bugs that the LLM hasn’t foreseen.
  prefs: []
  type: TYPE_NORMAL
- en: As Bard says at the bottom of every code snippet it provides, “ *Use code with
    caution.* *Learn more* .”
  prefs: []
  type: TYPE_NORMAL
- en: That then leads to an FAQ page about the current engine and what to expect from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: How can this process and this code be made better? Let’s talk about that in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making it work for you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some tips to make coding with LLMs work better for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Be specific about what you want the code to do and look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master prompt engineering. Craft your prompts meticulously. The clearer and
    more detailed your instructions, the better the LLM understands your intent and
    generates relevant code. Think of prompts as precise questions guiding the LLM
    toward creating your desired solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tell the LLM which version of which language you’re using and which IDE/environment
    you’re using for the code. For example, I have been using Python 3.10 in a Spyder
    IDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break down tasks into smaller chunks. Don’t overwhelm the LLM with complex,
    multi-step tasks. Instead, break them down into smaller, manageable subtasks.
    This allows for more focused prompts and facilitates easier troubleshooting and
    refinement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go through iterations, improving all the time. Use the LLM to check how you
    can improve, and get it to ask you questions about what you want, like a good
    consultant. Don’t simply accept the first generated code. Treat it as a starting
    point and provide constructive feedback to the LLM. Explain what aspects worked
    well and what needs improvement. This feedback loop guides the LLM toward better
    understanding your needs and refining the code output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize code examples and documentation. Provide the LLM with relevant code
    examples, libraries, or documentation references when possible. This serves as
    a roadmap, influencing the LLM’s code generation within the context you desire.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine LLM power with your expertise. Remember, LLMs are powerful tools, but
    they don’t replace your coding knowledge. Use your expertise to curate prompts,
    evaluate outputs, and integrate the LLM’s suggestions into your existing coding
    practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore different LLMs and tools. Not all LLMs are created equal. Experiment
    with different options and explore available tools specifically designed for LLM-assisted
    coding. Find the combination that best suits your needs and coding style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embrace the learning process. Remember, LLM-powered coding is still evolving.
    Embrace the learning journey, experiment, and don’t be afraid to make mistakes.
    Every interaction with the LLM contributes to your understanding and helps you
    unlock its full potential.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the code and see whether it does what you want it to; this is the most
    important stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like with any code, rigorous testing is essential. Don't assume that the
    LLM-generated code is flawless. Put it through its paces, identify and address
    any errors, and verify its functionality before deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll get into debugging in [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has introduced you to how to code with LLMs. You’ve explored the
    potential benefits, learned practical steps to get started (planning and iterating),
    and gained valuable insights into using LLMs effectively. Remember, these are
    just the most simple examples of what can be done along with some advice.
  prefs: []
  type: TYPE_NORMAL
- en: There’s plenty more to learn to do this well, but this can help you get started.
    We need to understand how to deal with code that has bugs and doesn’t work how
    we intend.
  prefs: []
  type: TYPE_NORMAL
- en: We might want to improve upon the speed or the memory usage of the code.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4, Gemini, and so on don’t innately know what you want to achieve or which
    resources you want to save or fully exploit.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B21009_03.xhtml#_idTextAnchor073) , we’ll delve into debugging,
    code refactoring, and optimization.
  prefs: []
  type: TYPE_NORMAL
