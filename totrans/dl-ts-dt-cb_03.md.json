["```py\npip install -U scikit-learn\n```", "```py\nimport pandas as pd\nserie = pd.read_csv(\n    \"assets/datasets/time_series_solar.csv\",\n    parse_dates=[\"Datetime\"],\n    index_col=\"Datetime\",\n)['Incoming Solar']\n```", "```py\n    series.shift(1)\n    ```", "```py\n    m = 12\n    series.shift(m)\n    ```", "```py\n    series.expanding().mean()\n    ```", "```py\npip install -U statsmodels\n```", "```py\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\nseries = pd.read_csv(\n    \"assets/datasets/time_series_solar.csv\",\n    parse_dates=[\"Datetime\"],\n    index_col=\"Datetime\",\n)['Incoming Solar']\nmodel = ARIMA(series, order=(1, 1, 1), freq='H')\nmodel_fit = model.fit()\nforecasts = model_fit.predict(start=0, end=5, typ='levels')\n```", "```py\npip install -U pandas numpy\n```", "```py\nseries = pd.read_csv(\n    \"assets/datasets/time_series_solar.csv\",\n    parse_dates=[\"Datetime\"],\n    index_col=\"Datetime\",\n)['Incoming Solar']\n```", "```py\nimport pandas as pd\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if len(data.shape) == 1 else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n     for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    if dropnan:\n        agg.dropna(inplace=True)\n     return agg\ndata = series_to_supervised(series, 3)\nprint(data)\n```", "```py\nseries = series.resample('D').sum()\n```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    train, test = train_test_split(data, test_size=0.2, \n        shuffle=False)\n    train = scaler.fit_transform(train)\n    test = scaler.transform(test)\n    X_train, y_train = train[:, :-1], train[:, -1]\n    X_test, y_test = test[:, :-1], test[:, -1]\n    X_train = torch.from_numpy(X_train).type(torch.Tensor)\n    X_test = torch.from_numpy(X_test).type(torch.Tensor)\n    y_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1)\n    y_test = torch.from_numpy(y_test).type(torch.Tensor).view(-1)\n    ```", "```py\n    class FeedForwardNN(nn.Module):\n        def __init__(self, input_dim, hidden_dim, output_dim):\n            super(FeedForwardNN, self).__init__()\n            self.fc1 = nn.Linear(input_dim, hidden_dim)\n            self.fc2 = nn.Linear(hidden_dim, output_dim)\n            self.activation = nn.ReLU()\n        def forward(self, x):\n            out = self.activation(self.fc1(x))\n            out = self.fc2(out)\n            return out\n    model = FeedForwardNN(input_dim=X_train.shape[1],\n                          hidden_dim=32,\n                          output_dim=1)\n    ```", "```py\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    epochs = 200\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        out = model(X_train).reshape(-1,)\n        loss = loss_fn(out, y_train)\n        loss.backward()\n        optimizer.step()\n        if epoch % 10 == 0:\n            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n    ```", "```py\n    model.eval()\n    y_pred = model(X_test).reshape(-1,)\n    test_loss = loss_fn(y_pred, y_test)\n    print(f\"Test Loss: {test_loss.item()}\")\n    ```", "```py\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\n```", "```py\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n            batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0),\n            self.hidden_dim).requires_grad_()\n        c0 = torch.zeros(self.num_layers, x.size(0),\n            self.hidden_dim).requires_grad_()\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:, -1, :])\n        return out\nmodel = LSTM(input_dim=1,\n             hidden_dim=32,\n             output_dim=1,\n             num_layers=1)\n```", "```py\nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1,)\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n```", "```py\nmodel.eval()\ny_pred = model(X_test)\ntest_loss = loss_fn(y_pred, y_test)\nprint(f\"Test Loss: {test_loss.item()}\")\n```", "```py\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\n```", "```py\n    class GRUNet(nn.Module):\n        def init(self, input_dim, hidden_dim, output_dim=1, \n            num_layers=2):\n            super(GRUNet, self).init()\n            self.hidden_dim = hidden_dim\n            self.num_layers = num_layers\n            self.gru = nn.GRU(input_dim, hidden_dim, num_layers, \n                batch_first=True)\n            self.fc = nn.Linear(hidden_dim, output_dim)\n            def forward(self, x):\n            h0 = torch.zeros(self.num_layers, x.size(0), \n                self.hidden_dim).to(x.device)\n            out, _ = self.gru(x, h0)\n            out = self.fc(out[:, -1, :])\n            return out\n    model = GRUNet(input_dim=1,\n                   hidden_dim=32,\n                   output_dim=1,\n                   num_layers=1)\n    ```", "```py\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    ```", "```py\n    epochs = 200\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        out = model(X_train).reshape(-1,)\n        loss = loss_fn(out, y_train)\n        loss.backward()\n        optimizer.step()\n        if epoch % 10 == 0:\n            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n    ```", "```py\n    model.eval()\n    y_pred = model(X_test).reshape(-1,)\n    test_loss = loss_fn(y_pred, y_test)\n    print(f\"Test Loss: {test_loss.item()}\")\n    ```", "```py\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\n```", "```py\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n            batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0),\n            self.hidden_dim).requires_grad_()\n        c0 = torch.zeros(self.num_layers, x.size(0),\n            self.hidden_dim).requires_grad_()\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:, -1, :])\n        return out\n```", "```py\nmodel = LSTM(input_dim=1, hidden_dim=32, output_dim=1, num_layers=2)\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```", "```py\nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1,)\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n```", "```py\nmodel.eval()\ny_pred = model(X_test).reshape(-1,)\ntest_loss = loss_fn(y_pred, y_test)\nprint(f\"Test Loss: {test_loss.item()}\")\n```", "```py\nclass HybridLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, \n        output_dim=1, num_layers=1):\n        super(HybridLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim,\n            num_layers, batch_first=True)\n        self.fc1 = nn.Linear(hidden_dim, 50)\n        self.fc2 = nn.Linear(50, output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0),\n            self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers,x.size(0),\n            self.hidden_dim).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = F.relu(self.fc1(out[:, -1, :]))\n        out = self.fc2(out)\n        return out\nmodel = HybridLSTM(input_dim=1, hidden_dim=32, output_dim=1, \n    num_layers=1)\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n```", "```py\nclass CNNTimeseries(nn.Module):\n    def __init__(self, input_dim, output_dim=1):\n        super(CNNTimeseries, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_dim,\n                               out_channels=64,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1)\n        self.fc = nn.Linear(in_features=64,\n                            out_features=output_dim)\n     def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\nmodel = CNNTimeseries(input_dim=3, output_dim=1)\n```", "```py\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(series, test_size=0.2, shuffle=False)\n```", "```py\n    train.diff(periods=1)\n    test.diff(periods=1)\n    ```", "```py\n    train_shifted = train.shift(periods=1)\n    train_diff = train - train_shifted\n    test_shifted = test.shift(periods=1)\n    test_diff = test - test_shifted\n    ```", "```py\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    train_diffnorm = scaler.fit_transform(\n        train_diff.values.reshape(-1, 1))\n    test_diffnorm = scaler.transform(test_diff.values.reshape(-1,1))\n    ```", "```py\n    train_df = series_to_supervised(train_diffnorm, n_in=3).values\n    test_df = series_to_supervised(test_diffnorm, n_in=3).values\n    ```", "```py\n    X_train, y_train = train_df[:, :-1], train_df[:, -1] \n    X_test, y_test = test_df[:, :-1], test_df[:, -1]\n    X_train = torch.from_numpy(X_train).type(torch.Tensor)\n    X_test = torch.from_numpy(X_test).type(torch.Tensor) \n    y_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1) \n    y_test = torch.from_numpy(y_test).type(torch.Tensor).view(-1) \n    X_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\n    X_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\n    model = LSTM(input_dim=1, hidden_dim=32, output_dim=1, \n        num_layers=2)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    epochs = 200\n    for epoch in range(epochs):\n        model.train()\n        optimizer.zero_grad()\n        out = model(X_train).reshape(-1, )\n        loss = loss_fn(out, y_train)\n        loss.backward()\n        optimizer.step()\n        if epoch % 10 == 0:\n            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n    ```", "```py\n    model.eval()\n    y_pred = model(X_test).reshape(-1, )\n    ```", "```py\n    y_pred_np = y_pred.detach().numpy().reshape(-1, 1)\n    y_diff = scaler.inverse_transform(y_pred_np).flatten()\n    ```", "```py\n    y_orig_scale = y_diff + test_shifted.values[4:]\n    ```", "```py\ntrain, test = train_test_split(series, test_size=0.2, shuffle=False)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_norm = scaler.fit_transform(\n    train.values.reshape(-1, 1)).flatten()\ntrain_norm = pd.Series(train_norm, index=train.index)\ntest_norm = scaler.transform(test.values.reshape(-1, 1)).flatten()\ntest_norm = pd.Series(test_norm, index=test.index)\ntrain_df = series_to_supervised(train_norm, 3)\ntest_df = series_to_supervised(test_norm, 3)\n```", "```py\nfrom sktime.transformations.series.date import DateTimeFeatures\ndate_features = DateTimeFeatures(ts_freq='D', \n    keep_original_columns=False, feature_scope='efficient')\ntrain_dates = date_features.fit_transform(train_df.iloc[:, -1])\n```", "```py\ntrain_dates = train_dates[['month_of_year', 'day_of_week']]\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(drop='first', sparse=False) \nencoded_train = encoder.fit_transform(train_dates) \ntrain_dummies = pd.DataFrame(encoded_train, \n    columns=encoder.get_feature_names_out(),dtype=int)\n```", "```py\ntest_dates = date_features.transform(test_df.iloc[:, -1]) \ntest_dates = test_dates[['month_of_year', 'day_of_week']]\ntest_encoded_feats = encoder.transform(test_dates)\ntest_dummies = pd.DataFrame(test_encoded_feats,\n                            columns=encoder.get_feature_names_out(),\n                            dtype=int)\n```", "```py\nfrom sktime.transformations.series.fourier import FourierFeatures\nfourier = FourierFeatures(sp_list=[365.25],\n                          fourier_terms_list=[2],\n                          keep_original_columns=False)\ntrain_fourier = fourier.fit_transform(train_df.iloc[:, -1]) \ntest_fourier = fourier.transform(test_df.iloc[:, -1])\n```", "```py\nX_train = np.hstack([X_train, train_dummies, train_fourier])\nX_test = np.hstack([X_test, test_dummies, test_fourier])\n```", "```py\nX_train = torch.from_numpy(X_train).type(torch.Tensor) \nX_test = torch.from_numpy(X_test).type(torch.Tensor) \ny_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1) \ny_test = torch.from_numpy(y_test).type(torch.Tensor).view(-1)\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\nmodel = LSTM(input_dim=1, hidden_dim=32, output_dim=1, num_layers=2)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001) \nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1, )\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\nmodel.eval()\ny_pred = model(X_test).reshape(-1, )\ntest_loss = loss_fn(y_pred, y_test)\ny_pred_np = y_pred.detach().numpy().reshape(-1, 1)\ny_pred_orig = scaler.inverse_transform(y_pred_np).flatten()\n```", "```py\nfourier = FourierFeatures(sp_list=[7, 365.25],\n                          fourier_terms_list=[2, 2],\n                          keep_original_columns=False)\n```", "```py\ntime_series = df[\"Incoming Solar\"]\ntrain, test = train_test_split(time_series, test_size=0.2, shuffle=False)\n```", "```py\nperiods = 365\ntrain_shifted = train.shift(periods=periods)\ntrain_diff = train - train_shifted\ntest_shifted = test.shift(periods=periods)\ntest_diff = test - test_shifted\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_diffnorm = scaler.fit_transform(train_diff.values.reshape(-1,1))\ntest_diffnorm = scaler.transform(test_diff.values.reshape(-1, 1))\ntrain_df = series_to_supervised(train_diffnorm, 3).values\ntest_df = series_to_supervised(test_diffnorm, 3).values\n```", "```py\nX_train, y_train = train_df[:, :-1], train_df[:, -1] \nX_test, y_test = test_df[:, :-1], test_df[:, -1]\nX_train = torch.from_numpy(X_train).type(torch.Tensor) \nX_test = torch.from_numpy(X_test).type(torch.Tensor) \ny_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1) \ny_test = torch.from_numpy(y_test).type(torch.Tensor).view(-1)\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\nmodel = LSTM(input_dim=1, hidden_dim=32, output_dim=1, num_layers=2)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1, )\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n```", "```py\nmodel.eval()\ny_pred = model(X_test).reshape(-1, )\ny_diff = scaler.inverse_transform(\n    y_pred.detach().numpy().reshape(-1, 1)).flatten()\ny_original = y_diff + test_shifted.values[(periods+3):]\n```", "```py\nfrom statsmodels.tsa.api import STL\nseries_decomp = STL(series, period=365).fit()\nseas_adj = series – series_decomp.seasonal\n```", "```py\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_norm = scaler.fit_transform(\n    seas_adj.values.reshape(-1, 1)).flatten()\ntrain_norm = pd.Series(train_norm, index=time_series.index)\ntrain_df = series_to_supervised(train_norm, 3)\nX_train, y_train = train_df.values[:, :-1], train_df.values[:, -1]\nX_train = torch.from_numpy(X_train).type(torch.Tensor)\ny_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1)\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nmodel = LSTM(input_dim=1, hidden_dim=32, output_dim=1, num_layers=2)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1, )\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n```", "```py\nlatest_obs = train_norm.tail(3)\nlatest_obs = latest_obs.values.reshape(1, 3, -1)\nlatest_obs_t = torch.from_numpy(latest_obs).type(torch.Tensor)\nmodel.eval()\ny_pred = model(latest_obs_t).reshape(-1, ).detach().numpy()\ny_denorm = scaler.inverse_transform(y_pred.reshape(-1,1)).flatten()\n```", "```py\nfrom sktime.forecasting.naive import NaiveForecaster\nseas_forecaster = NaiveForecaster(stra'egy='last', sp=365)\nseas_forecaster.fit(series_decomp.seasonal)\nseas_preds = seas_forecaster.predict(fh=[1])\n```", "```py\npreds = y_denorm + seas_preds\n```", "```py\ntrain, test = train_test_split(time_series, test_size=0.2, \n    shuffle=False)\n```", "```py\nimport numpy as np\nclass LogTransformation:\n    @staticmethod\n    def transform(x):\n        xt = np.sign(x) * np.log(np.abs(x) + 1)\n        return xt\n    @staticmethod\n    def inverse_transform(xt):\n        x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n        return x\n```", "```py\ntrain_log = LogTransformation.transform(train)\ntest_log = LogTransformation.transform(test)\n```", "```py\nfrom scipy import stats\ntrain_bc, bc_lambda = stats.boxcox(train)\ntrain_bc = pd.Series(train_bc, index=train.index)\n```", "```py\ntest_bc = stats.boxcox(test, lmbda=bc_lambda)\ntest_bc = pd.Series(test_bc, index=test.index)\n```", "```py\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_norm = scaler.fit_transform(train_log.values.reshape(-1, 1))\ntest_norm = scaler.transform(test_log.values.reshape(-1, 1))\ntrain_df = series_to_supervised(train_norm, 3).values\ntest_df = series_to_supervised(test_norm, 3).values\nX_train, y_train = train_df[:, :-1], train_df[:, -1]\nX_test, y_test = test_df[:, :-1], test_df[:, -1]\nX_train = torch.from_numpy(X_train).type(torch.Tensor)\nX_test = torch.from_numpy(X_test).type(torch.Tensor)\ny_train = torch.from_numpy(y_train).type(torch.Tensor).view(-1)\ny_test = torch.from_numpy(y_test).type(torch.Tensor).view(-1)\nX_train = X_train.view([X_train.shape[0], X_train.shape[1], 1])\nX_test = X_test.view([X_test.shape[0], X_test.shape[1], 1])\nmodel = LSTM(input_dim=1, hidden_dim=32, output_dim=1, num_layers=2)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nepochs = 200\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    out = model(X_train).reshape(-1, )\n    loss = loss_fn(out, y_train)\n    loss.backward()\n    optimizer.step()\n```", "```py\nmodel.eval()\ny_pred = model(X_test).reshape(-1, )\ny_pred_np = y_pred.detach().numpy().reshape(-1, 1)\ny_pred_denorm = scaler.inverse_transform(y_pred_np).flatten()\ny_pred_orig = LogTransformation.inverse_transform(y_pred_denorm)\n```", "```py\nfrom scipy.special import inv_boxcox\ny_pred_orig = inv_boxcox(y_pred_denorm, bc_lambda)\n```"]