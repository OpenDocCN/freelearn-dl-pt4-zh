<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generating Images Based on Label Information</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we got the first taste of the potential of GANs to learn the connections between latent vectors and generated images and made a vague observation that latent vectors somehow manipulate the attributes of images. In this chapter, we will officially make use of the label and attribute information commonly seen in open datasets to properly establish the bridge between latent vectors and image attributes.</p>
<p>In this chapter, you will learn how to use <strong>conditional GANs</strong> (<strong>CGANs</strong>) to generate images based on a given label and how to implement adversarial learning with <span>autoencoders and age human faces from young to old. Following this, you will be shown how to e</span><span>fficiently organize your source code for easy adjustments and extensions.</span></p>
<p>After reading this chapter, you will have learned both supervised and unsupervised approaches to improve the quality of the <span>images</span><span> </span><span>generated by GANs with label and attribute information. This chapter also introduces the basic source code hierarchy throughout this book, which can be very useful for your own projects.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>CGANs – how are labels used?</li>
<li>Generating images from labels with CGANs</li>
<li>Working with Fashion-MNIST</li>
<li>InfoGAN <span>–</span> unsupervised attribute extraction</li>
<li>References and useful reading list</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CGANs – how are labels used?</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned that a relation between the latent vector and the generated images can be established by the training process of GANs and certain manipulation of the latent vectors is reflected by the changes in the generated images. But we have no control over what part or what kinds of latent vectors would give us images with the attributes we want. To address this issue, we will use a CGAN to add label information in the training process so that we can have a say in what kinds of images the model will generate.</p>
<p>The idea of CGANs was proposed by Mehdi Mirza and Simon Osindero in their paper, <em>Conditional Generative Adversarial Nets</em>. The core idea was to integrate the label information into both generator and discriminator networks so that the label vector would alter the distribution of latent vectors, which leads to images with different attributes.</p>
<p>Compared to the vanilla GAN model, CGAN makes a small change to the objective function to make it possible to include extra information by replacing the real data, <img class="fm-editor-equation" src="assets/eaca6776-3f02-440c-9a80-4356f4487348.png" style="width:0.92em;height:0.92em;"/>, and generated data, <img class="fm-editor-equation" src="assets/6326aae7-0bc5-4054-903b-0b2495f591cf.png" style="width:1.58em;height:1.42em;"/>, with <img class="fm-editor-equation" src="assets/c4248bfa-8bfe-4af4-ad14-746e0d5d2dd5.png" style="width:2.17em;height:1.75em;"/> and <img class="fm-editor-equation" src="assets/98915d08-f8bc-4f40-af11-71b0a59b2cb0.png" style="width:2.83em;height:1.75em;"/>, respectively, in which <img class="fm-editor-equation" src="assets/a6657c7b-fa05-41b0-a223-a17d531ff638.png" style="width:0.83em;height:1.25em;"/> represents auxiliary information such as label and attribute:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/32861aa6-4c2c-4fcc-ba65-145237f1815b.png" style="width:39.50em;height:2.58em;"/></p>
<p>In this equation, <img class="fm-editor-equation" src="assets/61b7d781-89a5-497d-8fbc-3a376b09e790.png" style="width:3.75em;height:1.42em;"/> borrows the form of conditional probability that describes how data <img class="fm-editor-equation" src="assets/1f85531f-4c3a-4308-ae51-fa820cb8a117.png" style="width:0.92em;height:0.92em;"/> is distributed under the condition of <img class="fm-editor-equation" src="assets/61e3a5c4-e687-4d28-be72-5bb31c3fc309.png" style="width:0.83em;height:1.25em;"/>. To calculate the new object function, we need the generator network to be able to generate data given certain conditions and the discriminator network to tell whether the input image obeys the given condition. Therefore, in this section, we will talk about how to design the generator and discriminator to achieve this purpose.</p>
<p>We will create two different models in this chapter and, in order to write reusable code, we will put our source codes in separate files instead of putting all the code in to one single file as we did in previous chapters.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining labels with the generator</h1>
                </header>
            
            <article>
                
<p>The architecture of the generator network of the CGAN is illustrated as follows. <span>As described in the original paper, all data is generated through an MLP-like network. Unlike in the original paper, however, we use a much deeper structure and techniques such as batch normalization and LeakyReLU to ensure better-looking results:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/360f39cd-eb70-4a81-af55-0f39dec63c23.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The generator network architecture of the CGAN</div>
<p>The label value is transformed into a vector with a length of 10, which is concatenated with the latent vector <em>z</em>. All data in the generator network is stored in the form of a vector. The length of the output vector equals the multiplication of width and height of the generated image, which is <img class="fm-editor-equation" src="assets/c674568b-845e-4ceb-8acd-3dc0a733c1da.png" style="width:8.25em;height:1.17em;"/> for the MNIST dataset. We can, of course, change the size of the output image to other values we want (we will set the image size to 64 x 64 later in the source code).</p>
<p>Let's organize the codes differently from previous chapters and create a <kbd>cgan.py</kbd> file for model definition.</p>
<p>First, we import the PyTorch and NumPy modules at the beginning of the source file:</p>
<pre>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import numpy as np</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Then, we define the <kbd>Generator</kbd> network:</p>
<pre>class Generator(nn.Module):<br/>    def __init__(self, classes, channels, img_size, latent_dim):<br/>        super(Generator, self).__init__()<br/>        self.classes = classes<br/>        self.channels = channels<br/>        self.img_size = img_size<br/>        self.latent_dim = latent_dim<br/>        self.img_shape = (self.channels, self.img_size, self.img_size)<br/>        self.label_embedding = nn.Embedding(self.classes, self.classes)<br/><br/>        self.model = nn.Sequential(<br/>            *self._create_layer(self.latent_dim + self.classes, 128, False),<br/>            *self._create_layer(128, 256),<br/>            *self._create_layer(256, 512),<br/>            *self._create_layer(512, 1024),<br/>            nn.Linear(1024, int(np.prod(self.img_shape))),<br/>            nn.Tanh()<br/>        )<br/><br/>    def _create_layer(self, size_in, size_out, normalize=True):<br/>        layers = [nn.Linear(size_in, size_out)]<br/>        if normalize:<br/>            layers.append(nn.BatchNorm1d(size_out))<br/>        layers.append(nn.LeakyReLU(0.2, inplace=True))<br/>        return layers<br/><br/>    def forward(self, noise, labels):<br/>        z = torch.cat((self.label_embedding(labels), noise), -1)<br/>        x = self.model(z)<br/>        x = x.view(x.size(0), *self.img_shape)<br/>        return x</pre>
<p>The generator network consists of 5 linear layers, 3 of which are connected to batch normalization layers, and the first 4 linear layers have <kbd>LeakyReLU</kbd> activation functions while the last has a <kbd>Tanh</kbd> activation function. The label information is processed by the <kbd>nn.Embedding</kbd> module, which behaves as a lookup table. Say we have 10 labels at hand for training samples. The embedding layer transforms the 10 different labels into 10 pre-defined embedding vectors, which are initialized based on normal distribution by default. The embedding vector of labels is then concatenated with the random latent vector to serve as the input vector of the first layer. Finally<span>, we need to reshape the output vector into 2D images as the final results.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating labels into the discriminator</h1>
                </header>
            
            <article>
                
<p>The architecture of the discriminator network of the CGAN is illustrated as follows. Again, the discriminator architecture is different from the one used in the original paper. You are, of course, more than welcome to make adjustments to the networks and see whether your models can generate better results:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img src="assets/65c76439-518e-45bd-b9b8-f83c697cc7ba.png" style="width:38.75em;height:16.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Discriminator network architecture of CGAN</div>
<p>Similar to the generator network, the label value is also part of the input of the discriminator network of the CGAN. The input image (with a size of 28 x 28) is transformed into a vector with a length of 784, therefore, the total length of the input vector of the discriminator network is 794. There are 4 hidden layers in the discriminator network. Unlike common CNN models for image classification, <span>the discriminator network</span> outputs a single value instead of a vector with the length as the number of classes. It is because <span>we already include the label information in the network input and </span>we only want the <span>discriminator network to tell us how</span><span> </span><span>close</span><span> an image is to the real images, given the label condition.</span></p>
<p>Now, let's define the discriminator network in the <kbd>cgan.py</kbd> file:</p>
<pre>class Discriminator(nn.Module):<br/>    def __init__(self, classes, channels, img_size, latent_dim):<br/>        super(Discriminator, self).__init__()<br/>        self.classes = classes<br/>        self.channels = channels<br/>        self.img_size = img_size<br/>        self.latent_dim = latent_dim<br/>        self.img_shape = (self.channels, self.img_size, self.img_size)<br/>        self.label_embedding = nn.Embedding(self.classes, self.classes)<br/>        self.adv_loss = torch.nn.BCELoss()<br/><br/>        self.model = nn.Sequential(<br/>            *self._create_layer(self.classes + int(np.prod(self.img_shape)), 1024, False, True),<br/>            *self._create_layer(1024, 512, True, True),<br/>            *self._create_layer(512, 256, True, True),<br/>            *self._create_layer(256, 128, False, False),<br/>            *self._create_layer(128, 1, False, False),<br/>            nn.Sigmoid()<br/>        )<br/><br/>    def _create_layer(self, size_in, size_out, drop_out=True, act_func=True):<br/>        layers = [nn.Linear(size_in, size_out)]<br/>        if drop_out:<br/>            layers.append(nn.Dropout(0.4))<br/>        if act_func:<br/>            layers.append(nn.LeakyReLU(0.2, inplace=True))<br/>        return layers<br/><br/>    def forward(self, image, labels):<br/>        x = torch.cat((image.view(image.size(0), -1), self.label_embedding(labels)), -1)<br/>        return self.model(x)<br/><br/>    def loss(self, output, label):<br/>        return self.adv_loss(output, label)</pre>
<p>Similarly, the labels are passed by another <kbd>nn.Embedding</kbd> module before being concatenated with the image vector. The discriminator network consists of 5 linear layers, 2 of which are connected to <kbd>Dropout</kbd> layers to enhance the generalization capability. Since we cannot always guarantee that the output values of the last layer lie within a range of [0, 1], we need a <kbd>Sigmoid</kbd> activation function to make sure of that.</p>
<div class="packt_tip">A <kbd>Dropout</kbd> layer with a dropout rate of 0.4 means that, at each iteration during training, each neuron has a probability of 0.4 of not participating in the calculation of the final results. Therefore, different submodels are trained at different training steps, which makes it harder for the whole model to overfit the training data compared to the one without <kbd>Dropout</kbd> layers. <kbd>Dropout</kbd> layers are often deactivated during evaluation.</div>
<p>The choice of which layer has a <kbd>Dropout</kbd> or <kbd>LeakyReLU</kbd> activation function is rather subjective. You can try out other combinations and find out which configuration yields the best results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating images from labels with the CGAN</h1>
                </header>
            
            <article>
                
<p>In the previous section, we defined the architecture of both generator and discriminator networks of the CGAN. Now, let's write the code for model training. In order to make it easy for you to reproduce the results, we will use MNIST as the training set to see how the CGAN performs in image generation. What we want to accomplish here is that, after the model is trained, it can generate the correct digit image we tell it to, with extensive variety.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-stop model training API</h1>
                </header>
            
            <article>
                
<p>First, let's create a new <kbd>Model</kbd> class that serves as a wrapper for different models and provides the one-stop training API. Create a new file named <kbd>build_gan.py</kbd> and import the necessary modules:</p>
<pre>import os<br/><br/>import numpy as np<br/>import torch<br/>import torchvision.utils as vutils<br/><br/>from cgan import Generator as cganG<br/>from cgan import Discriminator as cganD</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Then, let's create the <kbd>Model</kbd> class. In this class, we will initialize the <kbd>Generator</kbd> and <kbd>Discriminator</kbd> modules and provide <kbd>train</kbd> and <kbd>eval</kbd> methods so that users can simply call <kbd>Model.train()</kbd> (or <kbd>Model.eval()</kbd>) somewhere else to complete the model training (or evaluation):</p>
<pre>class Model(object):<br/>    def __init__(self,<br/>                 name,<br/>                 device,<br/>                 data_loader,<br/>                 classes,<br/>                 channels,<br/>                 img_size,<br/>                 latent_dim):<br/>        self.name = name<br/>        self.device = device<br/>        self.data_loader = data_loader<br/>        self.classes = classes<br/>        self.channels = channels<br/>        self.img_size = img_size<br/>        self.latent_dim = latent_dim<br/>        if self.name == 'cgan':<br/>            self.netG = cganG(self.classes, self.channels, <br/>              self.img_size, self.latent_dim)<br/>        self.netG.to(self.device)<br/>        if self.name == 'cgan':<br/>            self.netD = cganD(self.classes, self.channels, <br/>              self.img_size, self.latent_dim)<br/>        self.netD.to(self.device)<br/>        self.optim_G = None<br/>        self.optim_D = None</pre>
<p>Here, the generator network, <kbd>netG</kbd>, and the discriminator network, <kbd>netD</kbd>, are initialized based on the class number (<kbd>classes</kbd>), image channel (<kbd>channels</kbd>), image size (<kbd>img_size</kbd>), and length of the latent vector (<kbd>latent_dim</kbd>). These arguments will be given later. For now, let's assume that these values are already known. Since we need to initialize all tensors and functions in this class, we need to define the <kbd>device</kbd> our model is running on (<kbd>self.device</kbd>). The <kbd>optim_G</kbd> and <kbd>optim_D</kbd> objects are optimizers for the two networks. They are initialized with the following:</p>
<pre>    def create_optim(self, lr, alpha=0.5, beta=0.999):<br/>        self.optim_G = torch.optim.Adam(filter(lambda p: p.requires_grad,<br/>                                        self.netG.parameters()),<br/>                                        lr=lr,<br/>                                        betas=(alpha, beta))<br/>        self.optim_D = torch.optim.Adam(filter(lambda p: p.requires_grad,<br/>                                        self.netD.parameters()),<br/>                                        lr=lr,<br/>                                        betas=(alpha, beta))</pre>
<div class="packt_tip">The first argument of the <kbd>Adam</kbd> optimizer, <kbd>filter(lambda p: p.requires_grad, self.netG.parameters())</kbd>, is to grab all <kbd>Tensor</kbd> whose <span><kbd>requires_grad</kbd> flag is set to <kbd>True</kbd>. It is pretty useful when part of the model is untrained (for example, fine-tuning the last layer after transferring a trained model to a new dataset), even though it's not necessary in our case.</span></div>
<p>Next, let's define a method called <kbd>train</kbd> for model training. Arguments of <kbd>train</kbd> include the number of training epochs (<kbd>epochs</kbd>), the iteration interval between logging messages (<kbd>log_interval</kbd>), the output directory for results (<kbd>out_dir</kbd>), and whether to print training messages to the Terminal (<kbd>verbose</kbd>):</p>
<pre>    def train(self,<br/>              epochs,<br/>              log_interval=100,<br/>              out_dir='',<br/>              verbose=True):<br/>        self.netG.train()<br/>        self.netD.train()<br/>        viz_noise = torch.randn(self.data_loader.batch_size, self.latent_dim, device=self.device)<br/>        viz_label = torch.LongTensor(np.array([num for _ in range(nrows) for num in range(8)])).to(self.device)<br/>        for epoch in range(epochs):<br/>            for batch_idx, (data, target) in enumerate(self.data_loader):<br/>                data, target = data.to(self.device), target.to(self.device)<br/>                batch_size = data.size(0)<br/>                real_label = torch.full((batch_size, 1), 1., device=self.device)<br/>                fake_label = torch.full((batch_size, 1), 0., device=self.device)<br/><br/>                # Train G<br/>                self.netG.zero_grad()<br/>                z_noise = torch.randn(batch_size, self.latent_dim, device=self.device)<br/>                x_fake_labels = torch.randint(0, self.classes, (batch_size,), device=self.device)<br/>                x_fake = self.netG(z_noise, x_fake_labels)<br/>                y_fake_g = self.netD(x_fake, x_fake_labels)<br/>                g_loss = self.netD.loss(y_fake_g, real_label)<br/>                g_loss.backward()<br/>                self.optim_G.step()<br/><br/>                # Train D<br/>                self.netD.zero_grad()<br/>                y_real = self.netD(data, target)<br/>                d_real_loss = self.netD.loss(y_real, real_label)<br/><br/>                y_fake_d = self.netD(x_fake.detach(), x_fake_labels)<br/>                d_fake_loss = self.netD.loss(y_fake_d, fake_label)<br/>                d_loss = (d_real_loss + d_fake_loss) / 2<br/>                d_loss.backward()<br/>                self.optim_D.step()</pre>
<p>In <kbd>train</kbd>, we first switch the networks to train mode (for example, <kbd>self.netG.train()</kbd>). It mostly affects the behaviors of <kbd>Dropout</kbd> and  the batch normalization layers. Then, we define a set of fixed latent vectors (<kbd>viz_noise</kbd>) and labels (<kbd>viz_label</kbd>). They are used to occasionally produce images during training so that we can track how the model is trained, otherwise, we may only realize the training has gone south after the training is done:</p>
<pre>                if verbose and batch_idx % log_interval == 0 and batch_idx &gt; 0:<br/>                    print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f}'.format(<br/>                          epoch, batch_idx, len(self.data_loader),<br/>                          d_loss.mean().item(),<br/>                          g_loss.mean().item()))<br/>                    vutils.save_image(data, os.path.join(out_dir, 'real_samples.png'), normalize=True)<br/>                    with torch.no_grad():<br/>                        viz_sample = self.netG(viz_noise, viz_label)<br/>                        vutils.save_image(viz_sample, os.path.join(out_dir, 'fake_samples_{}.png'.format(epoch)), nrow=8, normalize=True)<br/>            self.save_to(path=out_dir, name=self.name, verbose=False)</pre>
<p><span>Here, we omitted some parts of the code (including the evaluation API and model exporting and loading). You can get the full source code from the code repository for this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Argument parsing and model training</h1>
                </header>
            
            <article>
                
<p>Now, the only thing left for us to do is to create and define the main entry for the project. In this file, we will need to define the arguments we previously have assumed to be known. These hyper-parameters are essential when we create any network, and we will elegantly parse these values. Let's create a new file called <kbd>main.py</kbd> and import the necessary modules:</p>
<p class="mce-root"/>
<pre>import argparse<br/>import os<br/>import sys<br/><br/>import numpy as np<br/>import torch<br/>import torch.backends.cudnn as cudnn<br/>import torch.utils.data<br/>import torchvision.datasets as dset<br/>import torchvision.transforms as transforms<br/><br/>import utils<br/><br/>from build_gan import Model</pre>
<p>Have you noticed that the only Python module that's related to our model is <kbd>build_gan.Model</kbd>? We can easily create another project and copy most of the content in this file without major revisions.</p>
<p>Then, let's define the <kbd>main</kbd> function:</p>
<pre>FLAGS = None<br/><br/>def main():<br/>    device = torch.device("cuda:0" if FLAGS.cuda else "cpu")<br/><br/>    if FLAGS.train:<br/>        print('Loading data...\n')<br/>        dataset = dset.MNIST(root=FLAGS.data_dir, download=True,<br/>                             transform=transforms.Compose([<br/>                             transforms.Resize(FLAGS.img_size),<br/>                             transforms.ToTensor(),<br/>                             transforms.Normalize((0.5,), (0.5,))<br/>                             ]))<br/>        assert dataset<br/>        dataloader = torch.utils.data.DataLoader(dataset, batch_size=FLAGS.batch_size,<br/>                                                 shuffle=True, num_workers=4, pin_memory=True)<br/>        print('Creating model...\n')<br/>        model = Model(FLAGS.model, device, dataloader, FLAGS.classes, FLAGS.channels, FLAGS.img_size, FLAGS.latent_dim)<br/>        model.create_optim(FLAGS.lr)<br/><br/>        # Train<br/>        model.train(FLAGS.epochs, FLAGS.log_interval, FLAGS.out_dir, True)<br/><br/>        model.save_to('')<br/>    else:<br/>        model = Model(FLAGS.model, device, None, FLAGS.classes, FLAGS.channels, FLAGS.img_size, FLAGS.latent_dim)<br/>        model.load_from(FLAGS.out_dir)<br/>        model.eval(mode=0, batch_size=FLAGS.batch_size)</pre>
<p>Since we have already defined the networks and training schedule in separate files, the initialization and training <span>of the model are accomplished with only 3 lines of codes: <kbd>model = Model()</kbd>, <kbd>model.create_optim()</kbd></span>, <span>and <kbd>model.train()</kbd>. This way, our code is easy to read, modify, and maintain, and we can easily use most of the code in other projects.</span></p>
<p>The <kbd>FLAGS</kbd> object stores all the arguments and hyper-parameters needed for model definition and training. To make the configuration of the arguments more user-friendly, we will use the <kbd>argparse</kbd> module provided by Python.</p>
<p>Note that if you would like to use a different dataset, you can change the definition of the <kbd>dataset</kbd> object in the same way as in the previous chapter.</p>
<p>The <kbd>main</kbd> entry of the source code and the definitions of arguments are as follows:</p>
<pre>if __name__ == '__main__':<br/>    from utils import boolean_string<br/>    parser = argparse.ArgumentParser(description='Hands-On GANs - Chapter 5')<br/>    parser.add_argument('--model', type=str, default='cgan', help='one of `cgan` and `infogan`.')<br/>    parser.add_argument('--cuda', type=boolean_string, default=True, help='enable CUDA.')<br/>    parser.add_argument('--train', type=boolean_string, default=True, help='train mode or eval mode.')<br/>    parser.add_argument('--data_dir', type=str, default='~/Data/mnist', help='Directory for dataset.')<br/>    parser.add_argument('--out_dir', type=str, default='output', help='Directory for output.')<br/>    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')<br/>    parser.add_argument('--batch_size', type=int, default=128, help='size of batches')<br/>    parser.add_argument('--lr', type=float, default=0.0002, help='learning rate')<br/>    parser.add_argument('--latent_dim', type=int, default=100, help='latent space dimension')<br/>    parser.add_argument('--classes', type=int, default=10, help='number of classes')<br/>    parser.add_argument('--img_size', type=int, default=64, help='size of images')<br/>    parser.add_argument('--channels', type=int, default=1, help='number of image channels')<br/>    parser.add_argument('--log_interval', type=int, default=100, help='interval between logging and image sampling')<br/>    parser.add_argument('--seed', type=int, default=1, help='random seed')<br/><br/>    FLAGS = parser.parse_args()</pre>
<p>A new argument is created by <kbd>parser.add_argument(ARG_NAME, ARG_TYPE, DEFAULT_VALUE, HELP_MSG)</kbd>, in which <span><kbd>ARG_NAME</kbd> is the argument name, <kbd>ARG_TYPE</kbd> is the value type of argument (for example, <kbd>int</kbd>, <kbd>float</kbd>, <kbd>bool</kbd>, or <kbd>str</kbd>), <kbd>DEFAULT_VALUE</kbd> is the default argument value when none is given, and <kbd>HELP_MSG</kbd> is the message printed when running <kbd>python main.py --help</kbd> in the Terminal. The argument value is specified by <kbd>python main.py --ARG_NAME ARG_VALUE</kbd>, or you can change the default value in the source code and simply run <kbd>pythin main.py</kbd>. Here, our model is to be trained for 200 epochs with a batch size of 128. The learning rate is set to 0.0002,  because a small learning rate value is suitable for the <kbd>Adam</kbd> method. The length of the latent vector is 100 and the size of the generated image is set to 64. We also set the random seed to 1 so that you can produce the exact same results as in this book.</span></p>
<p><kbd>boolean_string</kbd> is defined in the <kbd>utils.py</kbd> file, which is as follows <span>(reference visit </span><a href="https://stackoverflow.com/a/44561739/3829845">https://stackoverflow.com/a/44561739/3829845</a> for more information<span>)</span><span>. Otherwise, passing</span> <kbd>--train False</kbd> <span>in the Terminal will not affect the script:</span></p>
<pre>def boolean_string(s):<br/>    if s not in {'False', 'True'}:<br/>        raise ValueError('Not a valid boolean string')<br/>    return s == 'True'</pre>
<p>We still need to do some preprocessing on the arguments:</p>
<pre>    FLAGS.cuda = FLAGS.cuda and torch.cuda.is_available()<br/><br/>    if FLAGS.seed is not None:<br/>        torch.manual_seed(FLAGS.seed)<br/>        if FLAGS.cuda:<br/>            torch.cuda.manual_seed(FLAGS.seed)<br/>        np.random.seed(FLAGS.seed)<br/><br/>    cudnn.benchmark = True<br/><br/>    if FLAGS.train:<br/>        utils.clear_folder(FLAGS.out_dir)<br/><br/>    log_file = os.path.join(FLAGS.out_dir, 'log.txt')<br/>    print("Logging to {}\n".format(log_file))<br/>    sys.stdout = utils.StdOut(log_file)<br/><br/>    print("PyTorch version: {}".format(torch.__version__))<br/>    print("CUDA version: {}\n".format(torch.version.cuda))<br/><br/>    print(" " * 9 + "Args" + " " * 9 + "| " + "Type" + \<br/>          " | " + "Value")<br/>    print("-" * 50)<br/>    for arg in vars(FLAGS):<br/>        arg_str = str(arg)<br/>        var_str = str(getattr(FLAGS, arg))<br/>        type_str = str(type(getattr(FLAGS, arg)).__name__)<br/>        print(" " + arg_str + " " * (20-len(arg_str)) + "|" + \<br/>              " " + type_str + " " * (10-len(type_str)) + "|" + \<br/>              " " + var_str)<br/><br/>    main()</pre>
<p>Here, we first make sure that CUDA is indeed available to PyTorch. Then, we manually set the random seed to the NumPy, PyTorch, and CUDA backend. We need to clear the output directory each time we retrain the model and all output messages are redirected to an external file, <kbd>log.txt</kbd>. Finally, we print all of the arguments taken before running the <kbd>main</kbd> function so that we may have a chance to check whether we have configured the model correctly.</p>
<p>Now, open a Terminal and run the following script. Remember to change <kbd>DATA_DIRECTORY</kbd> to the path of the MNIST dataset on your machine:</p>
<pre>       $ conda activate torch<br/>(torch)$ python main.py --model cgan --train True --data_dir DATA_DIRECTORY</pre>
<p>The output message may look like this (the order of the arguments might be different):</p>
<pre>Logging to output/log.txt<br/><br/>PyTorch version: 1.0.1.post2<br/>CUDA version: 10.0.130<br/><br/>         Args         |   Type    |   Value<br/>--------------------------------------------------<br/>  model               | str       | cgan<br/>  cuda                | bool      | True<br/>  train               | bool      | True<br/>  data_dir            | str       | ~/Data/mnist<br/>  out_dir             | str       | output<br/>  epochs              | int       | 200<br/>  batch_size          | int       | 128<br/>  lr                  | float     | 0.0002<br/>  latent_dim          | int       | 100<br/>  classes             | int       | 10<br/>  img_size            | int       | 64<br/>  channels            | int       | 1<br/>  log_interval        | int       | 100<br/>  seed                | int       | 1<br/>Loading data...<br/><br/>Creating model...<br/><br/>Epoch 0 [100/469] loss_D: 0.6747 loss_G: 0.6119<br/>Epoch 0 [200/469] loss_D: 0.4745 loss_G: 0.8135<br/>...</pre>
<p>It takes about 22 minutes to train for 200 epochs on a GTX 1080Ti graphics card and costs about 729 MB of GPU memory. The generated images from the MNIST dataset are shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/02600168-55c5-4106-b548-bce59d22737c.png" style="width:10.83em;height:10.83em;"/><img src="assets/2532795f-15e5-4362-acd0-3f62db82621f.png" style="width:10.83em;height:10.83em;"/><img src="assets/9a207fa4-3801-43a6-81ae-105e4e27de51.png" style="width:10.83em;height:10.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch; right: 200th epoch)</div>
<p>We can see that the digit images are correctly generated for the corresponding labels while maintaining realistic variety in appearance. Because we treat the images as very long vectors in the model, it is hard to generate smoothness in both vertical and horizontal directions and it is easy to spot speckle noise in the generated images after only 25 epochs of training. However, the quality of the images gets a lot better after 200 epochs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Fashion-MNIST</h1>
                </header>
            
            <article>
                
<p>So you know by now that the MNIST dataset is comprised of a bunch of handwritten numbers. It is the defacto standard for the Machine Learning community, and it is often used to validate processes. Another group has decided to create another dataset that could be a better replacement. This project is named <strong>Fashion-MNIST</strong> and is designed to be a simple drop-in replacement. You can get a deeper understanding of the project at <a href="https://www.kaggle.com/zalando-research/fashionmnist/data#">https://www.kaggle.com/zalando-research/fashionmnist/data#</a>.</p>
<p class="mce-root"><span><strong>Fashion-MNIST</strong> consists of a training set of 60,000 images and labels and a test set of 10,000 images and labels. All images are grayscale and set to 28x28 pixels, and there are 10 classes of images, namely: T-shirt/top, Trouser, Pullover, Dress, Coat, Saldal, Shirt, Sneaker, Bag, and Ankle boot. You can already begin to see that this replacement dataset should work the algorithms harder.<br/>
To demonstrate the use of the dataset, we will use the program that we just created for the standard MNIST dataset, and make a few changes.</span></p>
<ol>
<li>Copy the main.py file to <kbd>fashion-main.py</kbd> to keep the original safe. Now in the <kbd>fashion-main.py</kbd> file find the following portion of code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">dataset = dset.MNIST(root=FLAGS.data_dir, download=True,<br/>                     transform=transforms.Compose([<br/>                     transforms.Resize(FLAGS.img_size),<br/>                     transforms.ToTensor(),<br/>                     transforms.Normalize((0.5,), (0.5,))<br/>                     ]))</pre>
<p class="mce-root" style="padding-left: 60px">It's the fourth line in the <kbd>main()</kbd> function.</p>
<ol start="2">
<li class="mce-root">Now, all you need to change is the the <kbd>dset.MNIST(</kbd> to <kbd>dset.FashionMNIST(</kbd> like this:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">dataset = dset.FashionMNIST(root=FLAGS.data_dir, download=True,<br/>                            transform=transforms.Compose([<br/>                            transforms.Resize(FLAGS.img_size),<br/>                            transforms.ToTensor(),<br/>                            transforms.Normalize((0.5,), (0.5,))<br/>                            ]))</pre>
<p style="padding-left: 60px">Luckily, torchvision already has a built-in class for Fashion-MNIST. We'll point out a few others in a few minutes.</p>
<ol start="3">
<li>Now save your source file.</li>
<li>Now, to make sure that your dataset from the first example is safe, rename the Data folder that was used last time. The new dataset will automatically be downloaded for you. One other thing you should do is to rename your output folder, again to keep that safe.</li>
<li>As we did with the last program, we'll start it with a command line entry:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>(torch)$ python fashion_main.py --model cgan --train True --data_dir DATA_DIRECTORY</strong></pre>
<p class="mce-root"/>
<p class="mce-root">The output in the Terminal will look pretty much like that of the last program, except for the lines showing the download of the new dataset information:</p>
<pre class="mce-root"><strong>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./Data/fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz</strong><br/><strong>26427392it [00:06, 4212673.42it/s] </strong><br/><strong>Extracting ./Data/fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./Data/fashion/FashionMNIST/raw</strong><br/><strong>...</strong></pre>
<p class="mce-root">Here is an example of the output images you can expect:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b484efe-7cf1-4bbb-adcf-9cf7b26bd6f1.png"/></p>
<p>On the left is the real sample data, in the middle is the result from epoch 0, and finally on the right is the result from epoch 199.  While not perfect, you can see that the output is getting quite good.</p>
<p>Earlier, I said that we would look at other classes that torchvision supports. There are too many to discuss here, but if you go to: <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">https://pytorch.org/docs/stable/torchvision/datasets.html</a>, you can see the large list of each supported class and the API parameters. Many of them can be used as-is with our code, with the exception of modifying the dataset line in your code, and even allow the program to download the dataset for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">InfoGAN – unsupervised attribute extraction</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we have learned how to use auxiliary information such as the labels of data to improve the image quality generated by GANs. However, it is not always possible to prepare accurate labels of training samples beforehand. Sometimes, it is even difficult for us to accurately describe the labels of extremely complex data. In this section, we will introduce another excellent model from the GAN family, <strong>InfoGAN</strong>, which is capable of extracting data attributes during training in an unsupervised manner. InfoGAN was proposed by Xi Chen, Yan Duan, Rein Houthooft, et. al. in their paper, <em>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em>. It showed that GANs could not only learn to generate realistic samples but also learn semantic features, which are essential to sample generation.</p>
<p>Similar to CGANs, InfoGAN also replaces the original distribution of data with conditional distribution (with auxiliary information as conditions). The main difference is that InfoGAN does not need to feed label and attribute information into the discriminator network; instead, it uses another classifier, Q, to measure how auxiliary features are learned. The objective function of InfoGAN is as follows. You may notice that it adds another objective, <img class="fm-editor-equation" src="assets/79d47314-c5a5-43d0-aba0-ba44df6de818.png" style="width:3.83em;height:1.50em;"/>, at the end of the formula:</p>
<p><img class="alignnone size-full wp-image-797 image-border" src="assets/bcb9dc84-25a6-41b6-8ebe-bdcfa1679a8f.png" style="width:584.42em;height:47.50em;"/></p>
<p>In this formula, <img class="fm-editor-equation" src="assets/cbf2563c-2567-41b1-8fb9-d9378d5acbb3.png" style="width:8.33em;height:1.75em;"/> is the generated sample, <img class="fm-editor-equation" src="assets/9cfdd18b-8977-4117-85a1-cc2a0c46c587.png" style="width:0.75em;height:0.92em;"/> is the latent vector, and <img class="fm-editor-equation" src="assets/654bfdde-2fef-4d3e-91a0-4f17c16d495b.png" style="width:0.83em;height:1.25em;"/> represent auxiliary information. <img class="fm-editor-equation" src="assets/a35772fe-0a4e-4ad1-8b0b-be86133e22e5.png" style="width:2.17em;height:1.17em;"/> describes the actual distribution of <img class="fm-editor-equation" src="assets/509e60c1-6cca-4ace-a755-73b3061c0b63.png" style="width:0.83em;height:1.25em;"/>, which is rather hard to find. Therefore, we use the posterior distribution, <img class="fm-editor-equation" src="assets/475265f4-181b-4049-81ec-0b07128db5a0.png" style="width:3.75em;height:1.42em;"/>, to estimate <img class="fm-editor-equation" src="assets/6bc1c28f-753d-4dfd-940b-a0228b0ad023.png" style="width:2.75em;height:1.50em;"/>, and this process is done with a neural network classifier.</p>
<div class="packt_infobox">In the preceding formula, <img class="fm-editor-equation" src="assets/8136ddf4-79c1-4755-805c-944529966d0b.png" style="width:4.08em;height:1.17em;"/> is, in fact, an approximation of <strong>mutual information,</strong> <img class="fm-editor-equation" src="assets/1d81edb9-a9f6-437e-bddb-d2e0a675c288.png" style="width:6.67em;height:1.50em;"/>, between the auxiliary vector and generated sample. Mutual information, <img class="fm-editor-equation" src="assets/ec123496-34f5-4261-bba5-46cc2d607f0c.png" style="width:3.92em;height:1.25em;"/>, describes how much we know about random variable <em>X</em> based on knowledge of <em>Y</em>—<img class="fm-editor-equation" src="assets/3afa941c-e215-4e7c-aab4-eb1c3da11316.png" style="width:12.33em;height:1.17em;"/>, in which <img class="fm-editor-equation" src="assets/fb07f42f-3d25-44b5-9082-949730ee7887.png" style="width:2.67em;height:1.25em;"/> is <strong>entropy</strong> and <img class="fm-editor-equation" src="assets/6eae0ac6-7c94-429d-b139-c01f56f40ef6.png" style="width:3.67em;height:1.17em;"/> is <strong>conditional entropy</strong>. It can also be described by the <strong>Kullback–Leibler divergence</strong>, <img class="fm-editor-equation" src="assets/410d5388-e62a-4b72-906d-0d136f5d3614.png" style="width:12.25em;height:1.42em;"/>, which describes the information loss when we use marginal distributions to approximate the joint distribution of <em>X</em> and <em>Y</em>. You can refer to the original InfoGAN paper for detailed mathematical derivation. For now, you only need to know that <img class="fm-editor-equation" src="assets/ffd1b3f6-526a-4e87-9b42-7a447788c35c.png" style="width:1.25em;height:1.08em;"/> tells us whether the generation of <img class="fm-editor-equation" src="assets/88503b5b-daaa-443d-899c-9b1e282c37b3.png" style="width:1.58em;height:1.42em;"/> based on <img class="fm-editor-equation" src="assets/ad5450c3-c36f-43c1-9a2b-bd936e9b1e73.png" style="width:0.83em;height:1.25em;"/> goes as desired.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network definitions of InfoGAN</h1>
                </header>
            
            <article>
                
<p>The architecture of the generator network of InfoGAN is illustrated as follows. The reproduction of results from the original paper is rather tricky to handle. Therefore, we present a model architecture based on this GitHub repository, <a href="https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py">https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py</a>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/96400795-ee88-4aea-8602-ff8f394dcf9c.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Generator architecture of InfoGAN</div>
<p>The generator network of InfoGAN consists of 4 hidden layers. The first hidden layer transforms the input vector with a length of 74 (<em>62+10+2</em>) into a length of 8,192 (<em>128*8*8</em>), which is then directly turned into tensor with the dimensionality of <em>128*8*8.</em> The feature maps are then gradually up-scaled to 32*32 images. The scaling of feature maps is done with <kbd>torch.nn.functional.interpolate</kbd>. We need to define a derived <kbd>Module</kbd> class for upsampling so that we can treat it as other <kbd>torch.nn</kbd> layers.</p>
<p>Let's create a new source file called <kbd>infogan.py</kbd> and import the same Python modules as in <kbd>cgan.py</kbd> and define the <kbd>Upsample</kbd> class as follows:</p>
<pre>class Upsample(nn.Module):<br/>    def __init__(self, scale_factor):<br/>        super(Upsample, self).__init__()<br/>        self.scale_factor = scale_factor<br/><br/>    def forward(self, x):<br/>        return F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)</pre>
<p>We use the <kbd>bilinear</kbd> method to up-scale the images because it's the best fit compared to other choices. Since we derive this class from <kbd>torch.nn.Module</kbd> and only use the functions from <kbd>torch</kbd> to perform the calculation in the forward pass, our custom class will have no trouble performing the gradient back-propagation in training.</p>
<div class="packt_infobox">In PyTorch 1.0, calling <kbd>nn.Upsample</kbd> will give a deprecated warning and it is now, in fact, implemented with <kbd>torch.nn.functional.interpolate</kbd>. Therefore, our custom <kbd>Upsample</kbd> layer is the same as <kbd>nn.Upsample</kbd>, but without warning message.</div>
<p>The generator network is defined as follows:</p>
<pre>class Generator(nn.Module):<br/>    def __init__(self, classes, channels, img_size, latent_dim, code_dim):<br/>        super(Generator, self).__init__()<br/>        self.classes = classes<br/>        self.channels = channels<br/>        self.img_size = img_size<br/>        self.img_init_size = self.img_size // 4<br/>        self.latent_dim = latent_dim<br/>        self.code_dim = code_dim<br/>        self.img_init_shape = (128, self.img_init_size, self.img_init_size)<br/>        self.img_shape = (self.channels, self.img_size, self.img_size)<br/>        self.stem_linear = nn.Sequential(<br/>            nn.Linear(latent_dim + classes + code_dim,<br/>                      int(np.prod(self.img_init_shape)))<br/>        )<br/>        self.model = nn.Sequential(<br/>            nn.BatchNorm2d(128),<br/>            *self._create_deconv_layer(128, 128, upsample=True),<br/>            *self._create_deconv_layer(128, 64, upsample=True),<br/>            *self._create_deconv_layer(64, self.channels, upsample=False, normalize=False),<br/>            nn.Tanh()<br/>        )<br/><br/>    def _create_deconv_layer(self, size_in, size_out, upsample=True,  <br/>      normalize=True):<br/>        layers = []<br/>        if upsample:<br/>            layers.append(Upsample(scale_factor=2))<br/>        layers.append(nn.Conv2d(size_in, size_out, 3, stride=1, <br/>          padding=1))<br/>        if normalize:<br/>            layers.append(nn.BatchNorm2d(size_out, 0.8))<br/>            layers.append(nn.LeakyReLU(0.2, inplace=True))<br/>        return layers<br/><br/>    def forward(self, noise, labels, code):<br/>        z = torch.cat((noise, labels, code), -1)<br/>        z_vec = self.stem_linear(z)<br/>        z_img = z_vec.view(z_vec.shape[0], *self.img_init_shape)<br/>        x = self.model(z_img)<br/>        return x</pre>
<p>In this class, we use a helper function, <kbd>_create_deconv_layer</kbd>, to create the convolutional hidden layers. Since we will use the custom <kbd>Upsample</kbd> layer to increase the size of feature maps, we only need to use <kbd>nn.Conv2d</kbd>, whose input size equals, output size, rather than <kbd>nn.ConvTranspose2d</kbd> as in the DCGAN in the last chapter.</p>
<div class="packt_tip">In our configuration of InfoGAN, <kbd>torch.nn.functional.interpolate</kbd> <span>combined with <kbd>nn.Conv2d</kbd> performs better than <kbd>nn.ConvTranspose2d</kbd> with stride. Although you are welcome to try out different configurations and see whether they produce better results.</span></div>
<p>The architecture of the discriminator network of InfoGAN is illustrated as follows. Again, we use a different structure than in the original paper:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/780dbcbf-9c2c-443f-b87e-1c14b91e065e.png" style="width:38.92em;height:15.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Discriminator architecture of InfoGAN</div>
<p>The discriminator network consists of 4 hidden layers. As explained before, InfoGAN uses an additional classifier network to measure the validity of the auxiliary vector. In fact, this additional classifier shares most of its weights (the first 4 hidden layers) with the discriminator. Therefore, the quality measure on the image is represented by a 1 x 1 tensor, which is a result of a linear layer at the end of the 4 hidden layers. The measurement of auxiliary information, which includes class fidelity and style fidelity, is obtained from two different groups of linear layers, in which the <em>128*2*2</em> feature maps are first mapped to 128-length vectors then mapped to output vectors with lengths of 10 and 2, respectively.</p>
<p>The definition of the discriminator in PyTorch code is as follows:</p>
<pre>class Discriminator(nn.Module):<br/> def __init__(self, classes, channels, img_size, latent_dim, code_dim):<br/> super(Discriminator, self).__init__()<br/> self.classes = classes<br/> self.channels = channels<br/> self.img_size = img_size<br/> self.latent_dim = latent_dim<br/> self.code_dim = code_dim<br/> self.img_shape = (self.channels, self.img_size, self.img_size)<br/> self.model = nn.Sequential(<br/> *self._create_conv_layer(self.channels, 16, True, False),<br/> *self._create_conv_layer(16, 32, True, True),<br/> *self._create_conv_layer(32, 64, True, True),<br/> *self._create_conv_layer(64, 128, True, True),<br/> )<br/> out_linear_dim = 128 * (self.img_size // 16) * (self.img_size // 16)<br/> self.adv_linear = nn.Linear(out_linear_dim, 1)<br/> self.class_linear = nn.Sequential(<br/> nn.Linear(out_linear_dim, 128),<br/> nn.BatchNorm1d(128),<br/> nn.LeakyReLU(0.2, inplace=True),<br/> nn.Linear(128, self.classes)<br/> )<br/> self.code_linear = nn.Sequential(<br/> nn.Linear(out_linear_dim, 128),<br/> nn.BatchNorm1d(128),<br/> nn.LeakyReLU(0.2, inplace=True),<br/> nn.Linear(128, self.code_dim)<br/> )<br/> self.adv_loss = torch.nn.MSELoss()<br/> self.class_loss = torch.nn.CrossEntropyLoss()<br/> self.style_loss = torch.nn.MSELoss()<br/><br/> def _create_conv_layer(self, size_in, size_out, drop_out=True, normalize=True):<br/> layers = [nn.Conv2d(size_in, size_out, 3, 2, 1)]<br/> if drop_out:<br/> layers.append(nn.LeakyReLU(0.2, inplace=True))<br/> layers.append(nn.Dropout(0.4))<br/> if normalize:<br/> layers.append(nn.BatchNorm2d(size_out, 0.8))<br/> return layers<br/><br/> def forward(self, image):<br/> y_img = self.model(image)<br/> y_vec = y_img.view(y_img.shape[0], -1)<br/> y = self.adv_linear(y_vec)<br/> label = F.softmax(self.class_linear(y_vec), dim=1)<br/> code = self.code_linear(y_vec)<br/> return y, label, code</pre>
<p>Here, we treat the quality fidelity (<kbd>self.adv_loss</kbd>) as in an ordinary GAN model, the class fidelity (<kbd>self.class_loss</kbd>) as a classification problem (because label values are hard-coded, often in one-hot codes) and the style fidelity (<kbd>self.style_loss</kbd>) as an expectation maximization problem (because we want style vectors to obey certain random distribution). Therefore, cross-entropy (<kbd>torch.nn.CrossEntropyLoss</kbd>) and mean squared (<kbd>torch.nn.MSELoss</kbd>) loss functions are used for them.</p>
<div class="packt_tip">We'd like explain why mean squared error is used for expectation maximization. We assume that the style vectors obey a normal distribution with a mean of 0 and a standard deviation of 1. In the calculation of entropy, the logarithm of the probability of random variable is taken. The <span>logarithm of </span><strong>probability density function</strong> (<strong>pdf</strong>) of the normal distribution <img class="fm-editor-equation" src="assets/47a22187-f8e0-4124-a3e9-9f5ef9993390.png" style="width:20.42em;height:1.92em;"/> is deduced to <img class="fm-editor-equation" src="assets/1d1d3b79-700b-48b4-9935-48c9ffb365ed.png" style="width:10.00em;height:1.75em;"/>. Therefore, <span>mean squared error is suited for such a purpose.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and evaluation of InfoGAN</h1>
                </header>
            
            <article>
                
<p>We need to make some adjustments to the training API so that we can make use of the class and style vectors for attribute extraction and image generation.</p>
<p>First, we add several imported modules in the <kbd>build_gan.py</kbd> file:</p>
<pre>import itertools<br/><br/>from infogan import Generator as infoganG<br/>from infogan import Discriminator as infoganD</pre>
<p>The default weight initialization provided by PyTorch easily leads to saturation, so we need a custom <kbd>weight</kbd> initializer:</p>
<pre>def _weights_init(m):<br/>    classname = m.__class__.__name__<br/>    if classname.find('Conv') != -1:<br/>        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)<br/>    elif classname.find('BatchNorm') != -1:<br/>        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)<br/>        torch.nn.init.constant_(m.bias.data, 0.0)</pre>
<p>Let's add the following lines in the definition of the <kbd>Model</kbd> class:</p>
<pre>        self.style_dim = 2<br/>        self.infogan = self.name == 'infogan'<br/>        self.optim_info = None</pre>
<p class="mce-root"/>
<p>And we need to change the definitions of <kbd>self.netG</kbd> and <kbd>self.netD</kbd>:</p>
<pre>        if self.name == 'cgan':<br/>            self.netG = cganG(self.classes, self.channels, self.img_size, self.latent_dim)<br/>        elif self.name == 'infogan':<br/>            self.netG = infoganG(self.classes, self.channels, self.img_size, self.latent_dim, self.style_dim)<br/>            self.netG.apply(_weights_init)<br/>        self.netG.to(self.device)<br/>        if self.name == 'cgan':<br/>            self.netD = cganD(self.classes, self.channels, self.img_size, self.latent_dim)<br/>        elif self.name == 'infogan':<br/>            self.netD = infoganD(self.classes, self.channels, self.img_size, self.latent_dim, self.style_dim)<br/>            self.netD.apply(_weights_init)<br/>        self.netD.to(self.device)</pre>
<p>Then, we add an optimizer for mutual information at the end of the <kbd>create_optim</kbd> method:</p>
<pre>        if self.infogan:<br/>            self.optim_info = torch.optim.Adam(itertools.chain(self.netG.parameters(), self.netD.parameters()),<br/>                                               lr=lr, betas=(alpha, beta))</pre>
<p>Next, we need to make some adjustments to <kbd>train</kbd> method, in which we first train the generator and discriminator networks and update the two networks again based on auxiliary information. Here, we omit all of the <kbd>if self.infogan</kbd> statements and only show the training procedure for InfoGAN. Full source code can be referred to in the code repository for this chapter.</p>
<p>Initialize the fixed latent vectors for result visualization:</p>
<pre>        viz_noise = torch.randn(self.data_loader.batch_size, self.latent_dim, device=self.device)<br/>        nrows = self.data_loader.batch_size // 8<br/>        viz_label = torch.LongTensor(np.array([num for _ in range(nrows) for num in range(8)])).to(self.device)<br/>        viz_onehot = self._to_onehot(viz_label, dim=self.classes)<br/>        viz_style = torch.zeros((self.data_loader.batch_size, self.style_dim), device=self.device)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here, the <kbd>self._to_onehot</kbd> method is responsible for transforming the label values to one-hot coding:</p>
<pre>    def _to_onehot(self, var, dim):<br/>        res = torch.zeros((var.shape[0], dim), device=self.device)<br/>        res[range(var.shape[0]), var] = 1.<br/>        return res</pre>
<p>A training iteration for InfoGAN includes the following:</p>
<ul>
<li>Training the generator with fake data and letting the discriminator see them as real ones</li>
<li>Training the discriminator with both real and fake data to increase its ability to distinguish them</li>
<li>Training both generator and discriminator so that the generator can produce samples with good quality based on given auxiliary information and the discriminator can tell whether the generated samples obey the distribution of given auxiliary information:</li>
</ul>
<pre>        for epoch in range(epochs):<br/>            for batch_idx, (data, target) in enumerate(self.data_loader):<br/>                data, target = data.to(self.device), target.to(self.device)<br/>                batch_size = data.size(0)<br/>                real_label = torch.full((batch_size, 1), 1., <br/>                  device=self.device)<br/>                fake_label = torch.full((batch_size, 1), 0.,  <br/>                  device=self.device)<br/><br/>                # Train G<br/>                self.netG.zero_grad()<br/>                z_noise = torch.randn(batch_size, self.latent_dim,  <br/>                  device=self.device)<br/>                x_fake_labels = torch.randint(0, self.classes, <br/>                  (batch_size,), device=self.device)<br/>                labels_onehot = self._to_onehot(x_fake_labels, <br/>                  dim=self.classes)<br/>                z_style = torch.zeros((batch_size, self.style_dim), <br/>                  device=self.device).normal_()<br/>                x_fake = self.netG(z_noise, labels_onehot, z_style)<br/>                y_fake_g, _, _ = self.netD(x_fake)<br/>                g_loss = self.netD.adv_loss(y_fake_g, real_label)<br/>                g_loss.backward()<br/>                self.optim_G.step()<br/><br/>                # Train D<br/>                self.netD.zero_grad()<br/>                y_real, _, _ = self.netD(data)<br/>                d_real_loss = self.netD.adv_loss(y_real, real_label)<br/>                y_fake_d, _, _ = self.netD(x_fake.detach())<br/>                d_fake_loss = self.netD.adv_loss(y_fake_d, fake_label)<br/>                d_loss = (d_real_loss + d_fake_loss) / 2<br/>                d_loss.backward()<br/>                self.optim_D.step()<br/><br/>                # Update mutual information<br/>                self.optim_info.zero_grad()<br/>                z_noise.normal_()<br/>                x_fake_labels = torch.randint(0, self.classes,  <br/>                  (batch_size,), device=self.device)<br/>                labels_onehot = self._to_onehot(x_fake_labels, <br/>                  dim=self.classes)<br/>                z_style.normal_()<br/>                x_fake = self.netG(z_noise, labels_onehot, z_style)<br/>                _, label_fake, style_fake = self.netD(x_fake)<br/>                info_loss = self.netD.class_loss(label_fake, <br/>                  x_fake_labels) +\<br/>                            self.netD.style_loss(style_fake, z_style)<br/>                info_loss.backward()<br/>                self.optim_info.step()</pre>
<p>We don't need to change anything in the <kbd>main.py</kbd> file at all, and we can simply run the following script in the Terminal:</p>
<pre><strong>(torch)$ python main.py --model infogan --latent_dim 62 --img_size 32 --batch_size 64 --data_dir DATA_DIRECTORY</strong></pre>
<p>It takes about 2 hours to finish 200 epochs of training and costs about 833 MB GPU memory <span>on a GTX 1080Ti graphics card</span>. The results produced during training are shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/8fa92731-6e17-4dfe-83a9-b2aa5fd986bb.png" style="width:13.42em;height:13.42em;"/><img src="assets/d9b0754d-2e98-42c8-b71e-f8e6c754a753.png" style="width:13.50em;height:13.50em;"/><img src="assets/dae5ea62-3aff-4a3e-8c19-b32c49227292.png" style="width:13.58em;height:13.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch; right: 200th epoch)</div>
<p>After the training is done, run the following script to perform the model evaluation:</p>
<pre><strong>(torch)$ python main.py --model infogan --latent_dim 62 --img_size 32 --batch_size 64 --train False</strong></pre>
<p>Calling <kbd>model.eval()</kbd> with <kbd>mode=0</kbd> or <kbd>mode=1</kbd> will tell us what the two values in the style vector are responsible for, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7d1c962e-7cd5-43de-bf24-47691f80c5c6.png" style="width:10.17em;height:10.17em;"/><img src="assets/25924586-1b46-4b84-b7b2-d4439f2f37a5.png" style="width:10.25em;height:10.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The first style bit (mode=0) controls the digit angle, and the second style bit <span>(</span>mode=1) controls the width of strokes.</div>
<p>One of the style vector values is responsible for the angle of digits, and the other is responsible for the width of strokes, just as the original InfoGAN paper proclaims. Imagine what this technique is capable of on complex datasets and an elaborate training configuration.</p>
<p>We can do a lot more with CGANs and similar. For example, the labels can be more than for images. An individual pixel in the image can certainly have its own label. In the next chapter, we will look into how GANs perform on pixel-wise labels and we can do interesting things, more than hand-written digits and human faces.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References and useful reading list</h1>
                </header>
            
            <article>
                
<ol>
<li>Mirza M and Osindero S. (2014). <em>Conditional Generative Adversarial Nets</em>. <span>arXiv preprint arXiv:</span>1411.1784.</li>
<li>Hui J. (Jun 3, 2018). <em>GAN — CGAN &amp; InfoGAN (using labels to improve GAN)</em>. Retrieved from <a href="https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d">https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d</a>.</li>
<li>Zhang Z and Song Y and Qi H. (2017). <em>Age Progression/Regression by Conditional Adversarial Autoencoder</em>. CVPR.</li>
<li>Chen X, Duan Y, Houthooft R. (2016). <em>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em>. <span>arXiv preprint arXiv:</span>1606.03657.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discovered <strong>Conditional Generative Adversarial Networks</strong> (<strong><span>C</span><span>GANs</span></strong>), which worked with MNIST and <span>Fashion-MNIST</span>, and we learned about using the InfoGAN model, which again, worked with MNIST.</p>
<p>In our next chapter, we will learn about image-to-image translation, which I truly believe you will find exciting and very relevant in today's world.</p>


            </article>

            
        </section>
    </body></html>