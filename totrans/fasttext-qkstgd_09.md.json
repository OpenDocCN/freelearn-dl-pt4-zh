["```py\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, ft_wv):\n        self.ft_wv = ft_wv\n        if len(ft_wv)>0:\n            self.dim = ft_wv[next(iter(all_words))].shape[0] \n        else:\n            self.dim=0\n\n    def fit(self, X, y):\n        return self \n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.ft_wv[w] for w in words if w in self.ft_wv] \n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n```", "```py\nf = load_model(FT_MODEL)\n\nall_words = set([x for tokens in data['tokens'].values for x in tokens])\n\nwv_dictionary = {w: f.get_word_vector(w) for w in all_words}\n```", "```py\netree_w2v = Pipeline([(\"fasttext word-vector vectorizer\",                                 MeanEmbeddingVectorizer(wv_dictionary)), \n                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n```", "```py\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM, \n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n```", "```py\n>>> df = pd.read_csv('yelp_review.csv')\n>>> texts = df.text.values\n>>> labels = df.stars.values\n>>> texts = texts[:20000]\n>>> labels = labels[:20000]\n>>> print('Found %s texts.' % len(texts))\nFound 20000 texts.\n```", "```py\n>>> # finally, vectorize the text samples into a 2D integer tensor\n>>> tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n>>> tokenizer.fit_on_texts(texts)\n>>> sequences = tokenizer.texts_to_sequences(texts)\n>>> word_index = tokenizer.word_index\n>>> print('Found %s unique tokens.' % len(word_index))\nFound 45611 unique tokens.\n>>> data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n>>> labels = to_categorical(np.asarray(labels))\n>>> print('Shape of data tensor:', data.shape)\n>>> print('Shape of label tensor:', labels.shape)\nShape of data tensor: (20000, 1000)\nShape of label tensor: (20000, 6)\n>>> # split the data into a training set and a validation set\n>>> indices = np.arange(data.shape[0])\n>>> np.random.shuffle(indices)\n>>> data = data[indices]\n>>> labels = labels[indices]\n>>> num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n>>> x_train = data[:-num_validation_samples]\n>>> y_train = labels[:-num_validation_samples]\n>>> x_val = data[-num_validation_samples:]\n>>> y_val = labels[-num_validation_samples:]\n```", "```py\n>>> print('Preparing embedding matrix.')\n\n>>># load the fasttext model\n>>> f = load_model(FT_MODEL)\n\n>>> # prepare embedding matrix\n>>> num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n>>> embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n>>> for word, i in word_index.items():\n...     if i >= MAX_NUM_WORDS:\n...         continue\n...     embedding_vector = f.get_word_vector(word)\n...     if embedding_vector is not None:\n...         # words not found in embedding index will be all-zeros.\n...         embedding_matrix[i] = embedding_vector\n```", "```py\n>>> # load pre-trained word embeddings into an Embedding layer\n>>> embedding_layer = Embedding(num_words,\n...                             EMBEDDING_DIM,\n...                             weights=[embedding_matrix],\n...                             input_length=MAX_SEQUENCE_LENGTH,\n...                             trainable=False)\n```", "```py\n>>> # train a 1D convnet with global maxpooling\n>>> sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n>>> embedded_sequences = embedding_layer(sequence_input)\n>>> x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n>>> x = MaxPooling1D(5)(x)\n>>> x = Conv1D(128, 5, activation='relu')(x)\n>>> x = MaxPooling1D(5)(x)\n>>> x = Conv1D(128, 5, activation='relu')(x)\n>>> x = GlobalMaxPooling1D()(x)\n>>> x = Dense(128, activation='relu')(x)\n>>> preds = Dense(6, activation='softmax')(x)\n```", "```py\n_________________________________________________________________ \nLayer (type) Output Shape Param #\n=================================================================\ninput_1 (InputLayer) (None, 1000) 0\n_______________________________________________________________\nembedding_1 (Embedding) (None, 1000, 300) 6000000 _______________________________________________________________\nconv1d_1 (Conv1D) (None, 996, 128) 192128\n_______________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 199, 128) 0 _______________________________________________________________\nconv1d_2 (Conv1D) (None, 195, 128) 82048\n_______________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 39, 128) 0 _______________________________________________________________\nconv1d_3 (Conv1D) (None, 35, 128) 82048\n_______________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 128) 0\n_______________________________________________________________\ndense_1 (Dense) (None, 128) 16512\n_______________________________________________________________\ndense_2 (Dense) (None, 6) 774\n=================================================================\nTotal params: 6,373,510 \nTrainable params: 373,510 \nNon-trainable params: 6,000,000\n_________________________________________________________________\n```", "```py\nword_embeddings = tf.get_variable(“word_embeddings”,\n                                  [vocabulary_size, embedding_size])\nembedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)\n```", "```py\nwith tf.name_scope(\"embedding\"):\n    W = tf.Variable(tf.constant(0.0,\n                                shape=[doc_vocab_size,\n                                       embedding_dim]),\n                    trainable=False, \n                    name=\"W\")\n embedding_placeholder = tf.placeholder(tf.float32,\n    [doc_vocab_size, embedding_dim])\n embedding_init = W.assign(embedding_placeholder)\n embedded_chars = tf.nn.embedding_lookup(W,x)\n```", "```py\nsess = tf.Session()\nsess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n```", "```py\n#Load fasttext vectors\nfilepath_glove = 'crawl-300d-2M.vec'\nglove_vocab = []\nglove_embd=[]\nembedding_dict = {}\n\nwith open(filepath_glove) as file:\n    for index, line in enumerate(file):\n        values = line.strip().split() # Word and weights separated by space\n        if index == 0:\n            glove_vocab_size = int(values[0])\n            embedding_dim = int(values[1])\n        else:\n            row = line.strip().split(' ')\n            vocab_word = row[0]\n            glove_vocab.append(vocab_word)\n            embed_vector = [float(i) for i in row[1:]] # convert to list of float\n            embedding_dict[vocab_word]=embed_vector\n```", "```py\n#Create dictionary and reverse dictionary with word ids\n\ndef build_dictionaries(words):\n    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return dictionary, reverse_dictionary\n\ndictionary, reverse_dictionary = build_dictionaries(training_data)\n```", "```py\n#Create embedding array\n\ndoc_vocab_size = len(dictionary)\ndict_as_list = sorted(dictionary.items(), key = lambda x : x[1])\n\nembeddings_tmp=[]\n\nfor i in range(doc_vocab_size):\n    item = dict_as_list[i][0]\n    if item in glove_vocab:\n        embeddings_tmp.append(embedding_dict[item])\n    else:\n        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n        embeddings_tmp.append(rand_num)\n\n# final embedding array corresponds to dictionary of words in the document\nembedding = np.asarray(embeddings_tmp)\n\n# create tree so that we can later search for closest vector to prediction\ntree = spatial.KDTree(embedding)\n```", "```py\n# create input placeholders\nx = tf.placeholder(tf.int32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, embedding_dim])\n\n# RNN output node weights and biases\nweights = { 'out': tf.Variable(tf.random_normal([n_hidden, embedding_dim])) }\nbiases = { 'out': tf.Variable(tf.random_normal([embedding_dim])) }\n\nwith tf.name_scope(\"embedding\"):\n    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=False, name=\"W\")\n    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n    embedding_init = W.assign(embedding_placeholder)\n    embedded_chars = tf.nn.embedding_lookup(W,x)\n\n# reshape input data\nx_unstack = tf.unstack(embedded_chars, n_input, 1)\n\n# create RNN cells\nrnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\noutputs, states = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n\n# capture only the last output\npred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n```", "```py\ncost = tf.reduce_mean(tf.nn.l2_loss(pred-y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\nchapter6 folder: https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter6/TensorFlow%20rnn.ipynb.\n```", "```py\nfrom torchtext import data\nimport spacy\n...\n```", "```py\nfrom torchtext.vocab import FastText\nvectors = FastText('simple')\n```", "```py\ndef clean_str(string):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n\n    return string.strip().lower()\n\ndef prepare_csv(df, seed=999):\n    df['text'] = df['text'].apply(clean_str)\n    df_train, df_test = train_test_split(df, test_size=0.2)\n    df_train.to_csv(\"yelp_tmp/dataset_train.csv\", index=False)\n    df_test.to_csv(\"yelp_tmp/dataset_val.csv\", index=False)\n```", "```py\n# Define all the types of fields\n# pip install spacy for the tokenizer to work (or remove to use default)\nTEXT = data.Field(lower=True, include_lengths=True, fix_length=150, tokenize='spacy')\nLABEL = data.Field(sequential=True, use_vocab=False)\n\n# we use the index field to re-sort test data after processing\nINDEX = data.Field(sequential=False)\n\ntrain_fields=[\n    (text_label, TEXT),\n    (stars_label, LABEL)\n]\n\ntrain_fields=[\n    (text_label, TEXT),\n    (stars_label, LABEL)\n]\n\ntrain = data.TabularDataset(\n    path='yelp_tmp/dataset_train.csv', format='csv', skip_header=True,\n    fields=train_fields)\n\ntest_fields=[\n    (id_label, INDEX),\n    (text_label, TEXT),\n    (stars_label, LABEL)\n]\ntest = data.TabularDataset(\n        path='yelp_tmp/dataset_val.csv', format='csv', skip_header=True,\n        fields=test_fields)\n```", "```py\nmax_size = 30000\nTEXT.build_vocab(train, test, vectors=vectors, max_size=max_size)\nINDEX.build_vocab(test)\n```", "```py\ntrain = data.BucketIterator(train, batch_size=32,\n                            sort_key=lambda x: len(x.text),\n                            sort_within_batch=True, repeat=False)\ntest = data.BucketIterator(test, batch_size=128,\n                           sort_key=lambda x: len(x.text),\n                           sort_within_batch=True, train=False,\n                           repeat=False)\n```", "```py\nmodel = RNNModel('GRU', ntokens, emsize, nhidden, 6,\n                  nlayers, dropemb=dropemb, droprnn=droprnn, \n                  bidirectional=True)\nmodel.encoder.weight.data.copy_(TEXT.vocab.vectors)\n```"]