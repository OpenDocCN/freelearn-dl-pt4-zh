- en: Summary and Next Steps in Your Deep Learning Career
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been a fantastic journey and you've been quite productive as a member
    of the team! We hope that you've enjoyed our practical approach to teaching *Python
    Deep Learning Projects*. Furthermore, it was our intention to provide you with
    thought-provoking and exciting experiences that will further your intuition and
    form the technical foundation for your career in deep learning engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each chapter was structured similarly to participating as a member of our Intelligence
    Factory team, where, by going through the material, we achieved the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Saw the big picture of the real-world use case and identified the success criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Got focused and into the code, loaded dependencies and data, and built, trained,
    and evaluated our models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expanded back out to the big picture to confirm that we achieved our goal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We love solving problems and building smart solutions, insights, and people!
    Let's review some key learning, summarize some of our intuition, and look at what
    could be next in your deep learning career.
  prefs: []
  type: TYPE_NORMAL
- en: Python deep learning – building the foundation – two projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The foundation of a common working environment enables us to work together,
    and empowers our learning of cool and powerful deep learning technologies in the
    fields of **computer vision** (**CV**) and **natural language processing** (**NLP**).
    The first two chapters in this book provide the establishing experience that you
    will use time and again in your professional career as a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 – Building the Deep Learning Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal in this chapter was to standardize the toolset for our work together
    to achieve consistently accurate results. We want to establish a process for building
    applications using deep learning algorithms that can scale for production. Towards
    the end, we identified the components of our common deep learning environment,
    and initially set up a local deep learning environment, which expanded to a cloud-based
    environment. Throughout the projects that followed, you gained experience with
    Ubuntu, Anaconda, Python, TensorFlow, Keras, and **Google Cloud Platform** (**GCP**),
    to highlight but a few core technologies. These will continue to be of value to
    you in your deep learning engineering career!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2 – Training NN for Prediction Using Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training NN for
    Prediction Using Regression*, we identified our first business use case—one that
    would become a theme for a number of projects: that of a restaurant chain seeking
    to automate some of its processes. Specifically, in this chapter, the business
    use case was to build a deep learning classifier using a **multi-layer perceptron**
    (**MLP**), the basic building block in deep learning, to accurately classify handwritten
    digits of a customer''s phone number. If you recall, the goal was to accurately
    classify (digitize) the handwritten phone number on an iPad so that the patron
    could receive a text that their table was ready.'
  prefs: []
  type: TYPE_NORMAL
- en: We built a two-layer (minimally deep) neural network in TensorFlow and trained
    it on the classic MNIST dataset. This project provided us the opportunity to address
    overfitting, underfitting, hyperparameter tuning, and activation functions in
    our exploration of the model's performance. What we found particularly interesting
    was the impact of the business use case on interpreting the utility of the model's
    performance. Our accuracy with this simple model initially seemed adequate, until
    we thought about what a single digit error in a phone number would mean to the
    accurate delivery of a text to the right patron. In this context, we quickly understood
    we would need to do much better. Fortunately, we had an opportunity later in the
    book to take a second run at the problem, and in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml), *Handwritten
    Digits Classification Using ConvNets*, we employed a more complex deep learning model
    that performed much better!
  prefs: []
  type: TYPE_NORMAL
- en: Python deep learning – NLP – 5 projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A third of our *Python Deep Learning Projects* are in the field of computational
    linguistics. Unstructured text data is everywhere, and is being generated at an
    astonishing rate. We split up the approaches and technologies employed into five
    parts, to adequately handle the breadth of information. Let's review how the projects
    in [Chapters 3](4dcd4b65-934b-4a8a-a252-9af7513a4787.xhtml), *Word Representation
    Using word2vec*, through [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml), *Handwritten
    Digits Classification Using ConvNets*, relate and build on one another.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3 – Word Representations Using word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Core to computational linguistics is an effective representation of words and
    the features they embody. word2vec was used to transform the words into dense
    vectors (that is, tensors), creating embedding representations for the corpus.
    We then created a **convolutional neural network** (**CNN**) to build a language
    model for sentiment analysis. To help us frame this task we envisioned the hypothetical
    use case of our restaurant chain client asking us to make sense of the response
    texts that they were receiving from their patrons getting the notification that
    their table was ready. Particularly interesting was the realization that CNNs
    can be applied to more than just image data! We also took this project as an opportunity
    to explore data visualization with **t-distributed stochastic neighbor embedding**
    (**t-SNE**) and TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4 – Build an NLP Pipeline for Building Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We dive deeper into computational linguistics in this project by exploring the
    deep learning techniques (building blocks) for language models. word2vec models
    like ours in [Chapter 3](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=39&action=edit#post_26), *Word
    Representation Using word2vec*, are made possible by NLP pipelines. Our task was
    to create a natural language pipeline that would power a chatbot for open-domain
    question-answering. We pictured our (hypothetical) restaurant chain as having
    a website with their menu, history, location, hours, and other information, and
    that they would like the added ability for a website visitor to ask a question
    in a query box, and for our deep-learning NLP chatbot to find the relevant information
    and present that back.
  prefs: []
  type: TYPE_NORMAL
- en: The NLP pipeline tokenized the corpus, tagged parts of speech, determined the
    relationship between words with dependency parsing, and conducted **named entity
    recognition** (**NER**). This prepared us to use TF-IDF to vectorize the features
    in the document to create a simple FAQ-type chatbot. We enhanced this with NER
    and the implementation of Rasa NLU. We were then able to build a bot that understood
    the context (intent) of a piece of text, and could also extract the entities,
    because we created an NLP pipeline that could perform intent classification, along
    with NER extraction, to allow it to provide an accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5 – Sequence-to-Sequence Models for Building Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter builds directly on [Chapter 4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml), *Build
    NLP Pipeline for Building Chatbots* to build a more advanced chatbot for our hypothetical
    restaurant chain to automate the process of fielding call in orders. We combined
    our learning on a number of technologies to make a chatbot that is more contextually
    aware and robust. We avoided some of the limitations of CNNs in chatbots by building
    a **recurrent neural network** (**RNN**) model with **long short-term memory**
    (**LSTM**) units, specifically designed to capture the signal represented in sequences
    of characters or words.
  prefs: []
  type: TYPE_NORMAL
- en: We implemented a language model, with an encoder-decoder RNN based on the LSTM
    unit, for a simple sequence-to-sequence question-answer task. This model was able
    to handle inputs and outputs of different sizes, preserve the state of information,
    and adequately handle complex context. An additional learning of ours was that
    of the importance of obtaining a sufficient amount of the right training data
    as the outputs of the model are put up against a very high standard for speech
    interpretability. However, with the right training data, it would be possible
    to use this model to achieve the hypothetical restaurant chain's goal of building
    a robust chatbot (in combination with other computational linguistic technologies
    we've explored) that could automate the over-the-phone process of ordering food.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 – Generative Language Model for Content Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this project, we not only take the next step in our computational linguistics
    journey; we take a profound leap to generate new content! We defined the business
    use case goal of providing a deep learning solution that generates new content
    that can be used in movie scripts, song lyrics, and music. We asked ourselves:
    how can we leverage our experience in solving problems for restaurant chains and
    apply it to different industries? Upon reflection on what we learned in past projects
    regarding the inputs and outputs of the models, we gained confidence that novel
    content was just another type of output. We demonstrated that we could take an
    image as input, and output a class label ([Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression*). We trained a model to take inputs of text
    and output sentiment classifications ([Chapter 3](4dcd4b65-934b-4a8a-a252-9af7513a4787.xhtml), *Word
    Representation Using word2vec*), and we built a NLP pipeline for an open-domain
    question-answering chatbot, where we took text as input, and identified text in
    a corpus to present the appropriate output ([Chapter 4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml), *Build
    NLP Pipeline for Building Chatbots*). We then expanded that chatbot functionality
    to be able to serve a restaurant with an automated ordering system ([Chapter 5](856ccfef-cfe1-462f-9998-73f2b5168ae7.xhtml), *Sequence-to-Sequence
    Models for Building Chatbots*).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we implemented a generative model to generate content using
    the **long short-term memory** (**LSTM**), variational autoencoders, and **g****enerative
    adversarial networks** (**GANs**). We effectively implemented models, for both
    text and music, that can generate song lyrics, scripts, and music for artists
    and various creative businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7 – Building Speech Recognition with DeepSpeech2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project on building speech recognition with DeepSpeech2 is the capstone
    in the *Natural Language Processing* section of the *Python Deep Learning Projects*
    book. So far, we've explored chatbots, natural language processing, and speech
    recognition with RNNs (both uni- and bi-directional, with and without LSTM components)
    and CNNs. We've seen the power of these technologies to provide intelligence to
    existing business processes, as well as to create entirely new and smart systems.
    This is exciting work at the cutting edge of applied AI using deep learning!
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this project was to build and train an **automatic speech recognition** (**ASR**)
    system to take in and convert an audio call to text, which could then be used
    as the input for a text-based chatbot that was capable of parsing the input and
    responding appropriately. We made a deep dive into speech data, performing feature
    engineering to allow us to extract various kinds of features from the data, to
    then build a speech recognition system which can detect a users voice. In the
    end, we demonstrated mastery by building a system, using the DeepSpeech2 model,
    that recognizes English speech. We worked with speech and spectograms to build an end-to-end
    speech recognition system using the **c****onnectionist temporal classification**
    (**CTC**) loss function, batch normalization, and SortaGrad for the RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning – computer vision – 6 projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following six Python deep learning projects, focusing on CV, represent the
    largest portion of the content of this book. We've already seen how some of the
    deep learning technologies we explore in detail, with reference to CV, have some
    applicability to other types of data, and in particular, to text-based data. In
    no small part, that is because of the enormous utility of CNNs in feature extraction
    and hierarchical representation. There is no magic tool that is perfect for all
    jobs—being a deep learning engineer in data science is no exception. But you should
    not underestimate the familiarity you'll get with CNNs, as you'll find yourself
    using them time and again, across many different datasets and business use cases.
    Being a data scientist without CNN skills is like being a carpenter without a
    hammer. The obvious caveat is that not everything in data science is the equivalent
    of a nail!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8 – Handwritten Digit Classification Using ConvNets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter reminds us of the first deep neural net we created in [Chapter
    2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training NN for Prediction Using
    Regression*, and the business use case to which it was applied. The purpose of
    that chapter was to provide a foundation for our understanding of deep neural
    networks and how they operate. The complexity of the math underlying deep learning
    was highlighted when we compared the model architecture with the more advanced
    techniques afforded when we build deeper and more robust models. Complexity isn't
    cool just because it's complex; in this case, it's cool because of the improvement
    in realized performance utility that it provides.
  prefs: []
  type: TYPE_NORMAL
- en: We spent a considerable amount of time examining the convolution operation,
    pooling, and dropout regularization. These are the levers you'll adjust in tuning
    your models in your career, so getting a solid understanding of them early is
    essential. In reference to the business use case, we see the value of deploying
    a more complex model, in that the performance gain supports the parent product
    implementation. The error rate obtained in [Chapter 2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=39&action=edit#post_25), *Training
    NN for Prediction Using Regression*, was such that, in the worse case, not a single
    text would have been appropriately delivered to the right patron at the hypothetical
    restaurant chain (and in the best case, it was still dismal and effectively not
    functional). The CNN model on the same dataset produces results that mean that,
    in the new worst-case scenario, 90% of the patrons would receive the text notifications,
    and in the best case, 99% would get the text!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9 – Object Detection Using OpenCV and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's think about what we accomplished in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml), *Handwritten
    Digits Classification Using ConvNets*, where we were able to train an image classifier,
    with a CNN, to accurately classify handwritten digits in an image. The data was
    less complicated than it could have been, because each image only had one handwritten
    digit in it, and our goal was to accurately assign a class label to the image.
    What would have happened if each image had multiple handwritten digits in it,
    or different types of objects? What if we had a video? What if we want to identify
    *where* the digits are in the image? These questions represent the challenges
    that real-world data embodies, and drives our data science innovation toward new
    models and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and classification is no trivial task for a computer, particularly
    at scale and hitting speed requirements. We employed data inputs in this project
    that were much more informationally complex than what we've had in previous projects,
    and the outcomes, when we got them right, were that much more impressive. We found
    that the deep learning package YOLOv2 performed very well, and saw our model architecture
    get deeper and more complex with good results.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10 – Building Facial Recognition Using OpenFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 9](d3a16c16-a745-4cd4-ae4b-4afcec887ada.xhtml), *Object Detection
    Using OpenCV and TensorFlow*, we demonstrated mastery in the skills needed to
    build a deep learning object detection and classification model. Building on that,
    we set our objective at a refinement of that classification operation: is the
    object identical to another? In our case, we were looking to build a facial recognition
    system of the kind that we see in spy movies, and now in high tech security systems. Facial
    recognition is a combination of two major operations: face detection, followed
    by face classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenFace in this project, we built a model that looked at a picture and
    identified all the possible faces in it, then performed face extraction to understand
    the quality of the part of the image containing faces. We then performed feature
    extraction on the face, identifying parts of the image that gave us the basis
    for comparison with another data point (a labeled image of the person's face). This
    Python deep learning project demonstrates the exciting potential for this technology,
    and the future for the engineers that excel at working on these applications.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 11 – Automated Image Captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](d3a16c16-a745-4cd4-ae4b-4afcec887ada.xhtml), *Object Detection
    Using OpenCV and TensorFlow*, we learned how to detect and classify objects in
    an image, and in [Chapter 10](78e388c6-e8f4-49f4-84d6-ab7c20fd6b71.xhtml), *Building
    Face Recognition Using FaceNet*, we learned how to detect, classify, and identify
    objects as being the same thing (for example, identifying the same person from
    two different facial images). In this project, we did something even more complicated
    and cool! We combined the current state-of-the-art techniques that we've learned
    so far in our Python deep learning projects, in both CV andNLP, to form a complete
    image description approach. This model was capable of constructing computer-generated
    natural language descriptions of any image provided.
  prefs: []
  type: TYPE_NORMAL
- en: The clever idea that made this possible was to replace the encoder (the RNN
    layer) in an encoder-decoder architecture with a deep CNN, trained to classify
    objects in images. Normally, the CNN's last layer is the softmax layer, which
    assigns the probability that each object might be in the image. But when we remove
    that softmax layer from the CNN, we can feed the CNN's rich encoding of the image
    into the decoder (the language generation component of the RNN) designed to produce
    phrases. We can then train the whole system directly on images and their captions,
    maximizing the likelihood that the descriptions it produces best match the training
    descriptions for each image. This deep learning technology is the backbone of
    many intelligence factory solutions!
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 12 – Pose Estimation on 3D Models Using ConvNets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data that we apply to our models are representations of the real world. This
    is the fundamental truth that unites computational linguistics and CV. With respect
    to CV, we need to remember that 2D images represent a 3D world, in the same way
    that video represents 4D, with the added aspects of time and movement. Recalling
    this obvious fact lets us ask ever more interesting questions and develop deep
    learning technologies with increasing utility. Our hypothetical use case was to
    enable visual effects specialists to easily estimate the pose of actors (particularly
    the shoulders, neck, and head) on frames of a video. Our task was to build the
    intelligence for this application.
  prefs: []
  type: TYPE_NORMAL
- en: We successfully built a deep CNN/VGG16 model in Keras on **frames labeled in
    cinema** (**FLIC**) images. We got hands-on experience in preparing the images
    for modeling. We successfully implemented transfer learning, and understood that
    doing so will save us a lot of time. We defined some key hyperparameters, as well
    as understanding why we did what we did. Finally, we tested the modified VGG16
    model performance on unseen data, and determined that we succeeded in achieving
    our goals.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 13 – Image Translation Using GANs for Style Transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GANs are just downright cool. When we look back at the skills and intuition
    we''ve built throughout these projects, we had an interesting idea. Could we predict
    missing information? Or, stated in a different way: can we create data that should
    be in an image, but that''s not there? If we can take text input and generate
    novel text output, and if we can take a 2D image and generate or predict a 3D
    positional output, then it would seem possible that, if we have a 2D image that''s
    missing some information, maybe we ought to be able to generate the missing information?
    So, in this chapter, we built a neural network that fills in the missing part
    of a handwritten digit. We previously built a digit classifier for a hypothetical
    restaurant chain client. Error rates could be attributable to the digits not being
    accurately captured, and the resulting image having incompletely drawn digits. We focused
    our efforts on the new part of the model creation—the generation/reconstruction
    of the missing sections of a digit with the help of neural inpainting with GANs.
    We then reconstructed the missing parts of the handwritten numbers, so that the
    classifier received clear handwritten numbers for conversion into digits. With
    this, the classifier was able to do a much more accurate job of classifying the
    handwritten digits (and our mythical restaurant patrons were able to receive their
    notification texts and get seated promptly).'
  prefs: []
  type: TYPE_NORMAL
- en: Python deep learning – autonomous agents – 1 project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final project in our book is unlike anything we've done so far, and deserves
    its own treatment. Robotic process automation and optimization, and autonomous
    agents, such as drones and vehicles, require our deep learning models to learn
    from environmental cues in a reinforcement learning paradigm. Unlike previous
    projects, where we've been primarily focused on solving supervised learning problems, in
    this chapter, we learned to build and train a deep reinforcement learning model
    capable of playing games.
  prefs: []
  type: TYPE_NORMAL
- en: We employed a deep Q-learning and deep **state-action-reward-state-action** (**SARSA**)
    learning model. Unlike programming simple models by defining heuristics, deep
    learning models mapping A-B in a supervised learning environment, or determining
    decision boundaries in cluster analysis in unsupervised learning, it is the rules
    of the game or environment (as expressed in the delivery of reinforcement) that
    provide the feedback for training in reinforcement learning. The deep learning
    model, also called the agent in reinforcement-learning terms, interacts with the
    game environment and learns how to play the game, seeking to maximize rewards
    after several attempts at playing.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 14 – Develop an Autonomous Agent with Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we built a deep reinforcement learning model to successfully
    play the game of CartPole-v1, from OpenAI Gym. Demonstrating mastery here first,
    we could then extend it to other complex games, such as those by Atari.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to interact with the Gym toolkit, Q-learning, and SARSA learning;
    how to code the reinforcement learning model and define hyperparameters; and how
    to build the training loop and test the model. We found that our SARSA model performed
    quite a bit better than the Q-learning model. Further training and tuning of hyperparameters,
    and our own capture of reinforcement units (better scores by our models), should
    shape our behavior to build better models that ultimately result in the nearly
    perfect performance of our agent!
  prefs: []
  type: TYPE_NORMAL
- en: Next steps – AI strategy and platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, you've gained experiences that form the technical foundations
    for professional work in deep learning projects. However, the scope of the book
    was such that our focus could only be on a subset of the entire production-scale
    data science pipeline. We spent our time in the context of a business use case
    to ground our thinking on the domain and success criteria, but quickly dove into
    deep-learning model training, evaluation, and validation. These components, comprising
    the bulk of the training in our projects, are certainly the core of a data science
    pipeline for an enterprise, but cannot function in a vacuum. Additional considerations
    and training in AI strategy and data science platforms are the natural next steps
    in your education and career.
  prefs: []
  type: TYPE_NORMAL
- en: AI strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI strategy is about gaining knowledge from the client that empowers you to
    determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The client's grand vision for an intelligence-based competitive advantage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to translate that vision into an effective production-scale data science
    pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take into account the current and near-term digital maturity of the client
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes of data ingestion, analysis, and transformation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Technology and engineering resources and constraints
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The analytics team's current capabilities
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection, customization, training, evaluation, validation, and serving
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Achievement of KPIs and ROI that meets the objectives of the client's leadership
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI strategy consulting uncovers goals and expectations, while aligning outcomes
    with machine learning and deep learning technologies. Building an AI solution
    architecture must take all of this into account to be successful. You should look
    to mentors in the industry, read available case studies, and keep this in mind
    as your career advances, and you are called in to provide guidance and opinions
    earlier and earlier in the solution-building process.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning platforms – TensorFlow Extended (TFX)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science platforms, designed to meet the demands for production-scale deployment,
    require significant engineering support. At the Intelligence Factory and Skejul,
    we've built deep learning platforms that take in live feeds of constantly updating
    data to produce intelligence-based outputs within milliseconds, to be delivered
    via a cloud-based web application using API gateways. It's extraordinarily complex
    and rewarding, once you get all the pieces to come together!
  prefs: []
  type: TYPE_NORMAL
- en: One technology that will aid in your deep learning and data science career is
    TFX. This is Google's TensorFlow-based production-scale machine learning platform.
    The first few lines from their abstract from the article
  prefs: []
  type: TYPE_NORMAL
- en: '*TFX: A TensorFlow-Based Production-Scale Machine Learning Platform* ([https://ai.google/research/pubs/pub46484](https://ai.google/research/pubs/pub46484))
    summarize the potential of the TFX and similar platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Creating and maintaining a platform for reliably producing and deploying machine
    learning models requires careful orchestration of many components—a learner for
    generating models based on training data, modules for analyzing and validating
    both data as well as models, and finally infrastructure for serving models in
    production. This becomes particularly challenging when data changes over time
    and fresh models need to be produced continuously."'
  prefs: []
  type: TYPE_NORMAL
- en: Data science platform engineering that's based on a smartly crafted AI strategy
    is our next step in training, and we look forward to the opportunity to share
    those experiences with you too!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and thank you!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to thank you for choosing our book, *Python Deep Learning Projects*,
    as part of your data science education! It's our hope that you found the projects
    and the business use cases intriguing and informative, and that you feel more
    professionally prepared than when you started. We look forward to the opportunity
    to engage with you via our respective blogs, on social media, and possibly even
    at conferences or on working together on delivering AI-based solutions to clients
    around the world.
  prefs: []
  type: TYPE_NORMAL
- en: We've been happy to have you in our weekly AI team meetings in these projects. Now
    that we've learned a bunch of stuff, and had some fun with really cool and powerful
    data science technologies, let's go out to do great work based on these Python
    deep learning projects!
  prefs: []
  type: TYPE_NORMAL
