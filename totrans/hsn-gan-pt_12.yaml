- en: Sequence Synthesis with GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN进行序列合成
- en: In this chapter, we will work on GANs that directly generate sequential data,
    such as text and audio. While doing so, we will go back to the previous image-synthesizing
    models we've looked at so that you can become familiar with NLP models quickly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究能够直接生成序列数据（如文本和音频）的GAN。与此同时，我们将回顾之前所学的图像生成模型，以便让你更快地熟悉NLP模型。
- en: Throughout this chapter, you will get to know the commonly used techniques of
    the NLP field, such as RNN and LSTM. You will also get to know some of the basic
    concepts of **reinforcement learning** (**RL**) and how it differs from supervised
    learning (such as SGD-based CNNs). Later on, we will learn how to build a custom
    vocabulary from a collection of text so that we can train our own NLP models and
    learn how to train SeqGAN so that it can generate short English jokes. You will
    also learn how to use SEGAN to remove background noise and enhance the quality
    of speech audio.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解NLP领域常用的技术，如RNN和LSTM。你还将了解**强化学习**（**RL**）的一些基本概念，以及它与监督学习（如基于SGD的CNN）的区别。接下来，我们将学习如何从文本集合中构建自定义词汇表，以便训练自己的NLP模型，并学习如何训练SeqGAN，使其能够生成简短的英语笑话。你还将学习如何使用SEGAN去除背景噪音并增强语音音频的质量。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Text generation via SeqGAN – teaching GANs how to tell jokes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SeqGAN进行文本生成 – 教授GAN如何讲笑话
- en: Speech quality enhancement with SEGAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SEGAN进行语音质量增强
- en: Text generation via SeqGAN – teaching GANs how to tell jokes
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过SeqGAN进行文本生成 – 教授GAN如何讲笑话
- en: In the previous chapter, we learned how to generate high-quality images based
    on description text with GANs. Now, we will move on and look at sequential data
    synthesis, such as text and audio, using various GAN models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何通过GAN根据描述文本生成高质量图像。现在，我们将继续研究如何使用各种GAN模型进行序列数据合成，如文本和音频。
- en: When it comes to the generation of text, the biggest difference in terms of
    image generation is that text data is discrete while image pixel values are more
    continuous, though digital images and text are both essentially discrete. A pixel
    typically has 256 values and slight changes in the pixels won't necessarily affect
    the image's meaning to us. However, a slight change in the sentence – even a single
    letter (for example, turning *we* into *he*) – may change the whole meaning of
    the sentence. Also, we tend to have a higher tolerance bar for synthesized images
    compared to text. For example, if 90% of the pixels in the generated image of
    a dog are nearly perfect, we may have little trouble recognizing the dog because
    our brains are smart enough to automatically fill in the missing pixels. However,
    if you are reading a piece of news in which every one out of 10 words doesn't
    make any sense, you will definitely find it hard to enjoy reading it. This is
    why text generation is hard and there's less remarkable progress in text generation
    than image synthesis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成方面，与图像生成的最大区别在于，文本数据是离散的，而图像像素值则更为连续，尽管数字图像和文本本质上都是离散的。一个像素通常有256个值，而像素的微小变化通常不会影响我们对图像的理解。然而，句子中的微小变化——即使是一个字母（例如，将*we*改成*he*）——也可能改变整个句子的意思。而且，我们通常对合成图像的容忍度要高于文本。例如，如果生成的狗的图像中有90%的像素几乎完美无缺，我们通常能轻松识别出狗，因为我们的大脑足够聪明，能自动填补缺失的像素。然而，如果你阅读的新闻中每10个单词中就有一个不合逻辑，你肯定会觉得很难享受阅读。这就是为什么文本生成很困难，而且相较于图像合成，文本生成的进展较慢的原因。
- en: 'SeqGAN was one of the first successful attempts of text generation with adversarial
    learning. It was proposed by Lantao Yu, Weinan Zhang, and Jun Wang, et. al. in
    their paper, *SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient*.
    In this section, we will walk you through the design of SeqGAN, how to create
    your own vocabulary for NLP tasks, and how to train SeqGAN so that it can generate
    short jokes.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'SeqGAN是首批成功尝试使用对抗学习生成文本的模型之一。它由Lantao Yu、Weinan Zhang、Jun Wang等人在他们的论文《*SeqGAN:
    Sequence Generative Adversarial Nets with Policy Gradient*》中提出。在这一节中，我们将带你了解SeqGAN的设计，如何为NLP任务创建自己的词汇表，以及如何训练SeqGAN，使其能够生成简短的笑话。'
- en: Design of SeqGAN – GAN, LSTM, and RL
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SeqGAN的设计 – GAN、LSTM和RL
- en: Like other GAN models, SeqGAN is built upon the idea of adversarial learning.
    Some major changes have to be made so that it can accommodate NLP tasks. For example,
    the generation network is built with LSTM instead of CNNs, similar to some of
    the other GANs we looked at in the previous chapters. Also, reinforcement learning
    is used to optimize discrete objectives, unlike the SGD-family methods that were
    used in previous GAN models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will provide a quick introduction to LSTM and RL. However, we won't
    go too deep into these topics since we want to focus on the adversarial learning
    part of the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A quick introduction to RNN and LSTM
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) are designed to process sequential
    data such as text and audio. Their biggest difference to CNNs is that the weights
    in the hidden layers (that is, certain functions) are used repeatedly on multiple
    inputs and the order of the inputs affects the final results of the functions.
    The typical design of an RNN can be seen in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59435150-acdf-4e3b-99c8-68dd0f502411.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 Basic computational units of a recurrent neural network
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the most distinctive characteristic of an RNN unit is that the
    hidden state, ![](img/54184b1f-1edb-4bf2-a92f-130f4bfc14b4.png), has an outgoing
    connection pointing to itself. This self-loop is where the name "recurrent" comes
    from. Let''s say the self-loop is performed three times. The extended version
    of this computational unit is shown on the right in the preceding diagram. The
    computational process is expressed as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ffcd1a0-c001-48f7-a31b-da950dd05d5a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: '![](img/bd4676e8-94ff-4550-9319-f85aad178b67.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Thus, after proper training, this RNN unit is capable of handling sequential
    data with a maximum length of 3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are widely used in voice recognition, natural language translation, language
    modeling, and image captioning. However, a critical flaw remains in RNN that we
    need to address with LSTM.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: An RNN model assumes that a strong connection only exists between the neighboring
    inputs (for example, ![](img/3b929bf1-1c43-45d5-96f2-0c8b330bdc0c.png) and ![](img/2e5776b3-de79-4395-b77d-e941cba14a75.png), as
    shown in the preceding diagram) and that the connections between the inputs that
    are far apart from each other are ignored (for example, ![](img/f60506e1-eeb9-4c81-9a89-d56b18043f1c.png) and
    ![](img/8baa2dde-5c82-4cf0-afb4-4b797035ef16.png)). This becomes troublesome when
    we try to translate a long sentence into another language that has totally different grammatical
    rules and we need to look through all the parts of the sentence to make sense
    of it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**LSTM** (**Long Short-Term Memory**) was proposed by Sepp Hochreiter and Jürgen
    Schmidhuber in 1997 to preserve the long-term memory of sequential data and address
    the gradient explosion and vanishing issues in RNNs. Its computational process
    is illustrated in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7025e9b-c99f-41e3-b7ba-93443626748e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Computational process of LSTM
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 LSTM 的计算过程
- en: 'As we can see, an addition term, ![](img/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png),
    is included to help us choose what long-term information should be memorized.
    The detailed computational process is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，一个附加项 ![](img/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png) 被引入，以帮助我们选择应该记住的长期信息。详细的计算过程如下：
- en: '![](img/2dfda1e3-d735-4bf0-9784-8bea0de02215.png) and ![](img/15c720a7-08a8-45e7-8ba8-ef713ce6eca4.png) are
    passed through the Forget Gate to decide what information should be forgotten:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/2dfda1e3-d735-4bf0-9784-8bea0de02215.png) 和 ![](img/15c720a7-08a8-45e7-8ba8-ef713ce6eca4.png)
    被传递通过忘记门，用来决定应该忘记哪些信息：'
- en: '![](img/c75f9a60-d34c-48f0-add0-b9b3c60d463c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c75f9a60-d34c-48f0-add0-b9b3c60d463c.png)'
- en: 'The same inputs are also passed through the Input Gate so that we can calculate
    the updated ![](img/2c9501bf-f00a-4967-a448-ff5210863135.png) at the next step:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相同的输入也通过输入门，以便我们在下一步计算更新后的 ![](img/2c9501bf-f00a-4967-a448-ff5210863135.png)：
- en: '![](img/8ad3118d-ae7c-4b58-bd9d-dab83283b4b3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ad3118d-ae7c-4b58-bd9d-dab83283b4b3.png)'
- en: 'The updated ![](img/f6adc698-8d81-4745-ba20-f770ee4c1205.png) and ![](img/38a54135-0454-41f2-92c6-242b2ad566be.png) are
    calculated by the Output Gate:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新后的 ![](img/f6adc698-8d81-4745-ba20-f770ee4c1205.png) 和 ![](img/38a54135-0454-41f2-92c6-242b2ad566be.png)
    通过输出门计算得出：
- en: '![](img/5991df60-f54b-4c6b-a98f-94a18ed6137f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5991df60-f54b-4c6b-a98f-94a18ed6137f.png)'
- en: Then, the new ![](img/2d0aaa6b-e7ba-4a49-a362-8018613c4751.png) and ![](img/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png) are
    used to calculate the next pair of ![](img/d77ae7d9-a5b5-43f0-872e-da13bf39d901.png) and
    ![](img/7555bd34-4191-4e83-8ddc-4a3ae8084bcc.png). Although the structure of an
    LSTM cell is much more complicated than the vanilla RNN cell, thanks to the delicate
    design of the three gates (Forget, Input, and Output), LSTM can be seen in almost
    every milestone NLP model in the past few years. If you want to find out more
    about LSTM and its variants, check out [https://colah.github.io/posts/2015-08-Understanding-LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs)
    and [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，新的 ![](img/2d0aaa6b-e7ba-4a49-a362-8018613c4751.png) 和 ![](img/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png)
    被用来计算下一对 ![](img/d77ae7d9-a5b5-43f0-872e-da13bf39d901.png) 和 ![](img/7555bd34-4191-4e83-8ddc-4a3ae8084bcc.png)。虽然
    LSTM 单元的结构比普通的 RNN 单元复杂得多，但由于三大门（忘记门、输入门和输出门）的精妙设计，LSTM 可以在过去几年几乎所有的里程碑式 NLP 模型中看到。如果你想深入了解
    LSTM 及其变体，可以查看 [https://colah.github.io/posts/2015-08-Understanding-LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs)
    和 [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)。
- en: Reinforcement learning versus supervised learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与监督学习
- en: Reinforcement learning is another optimization method family in machine learning.
    It is often used when it's hard to provide standard correct answers for the tasks
    that the model is trying to solve, especially when the solution involves *free
    exploration* and the end goal of the task is somewhat *vague* compared to the
    specific decisions the model needs to make.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习 是机器学习中的另一种优化方法。它通常用于模型试图解决的任务很难提供标准的正确答案，特别是当解决方案涉及 *自由探索* 并且任务的最终目标相比模型需要做出的具体决策更为
    *模糊* 时。
- en: For example, if we want to teach a robot to walk, we can use reinforcement learning
    to let the robot teach itself to walk. We don't need to tell the robot how to
    move which body part at what time. We only tell it that its final goal is to *take
    yourself to that location 10 meters in front of you* and let it randomly move
    its limbs. At some point, a certain combination of movements for the robot's legs
    will bring the robot a step forward and a certain combination of movements for
    the robot's arms makes sure it won't fall out of balance. Similarly, reinforcement
    learning is also used to teach machines to play Go ([https://www.alphago-games.com](https://www.alphago-games.com))
    and video games ([https://openai.com/blog/openai-five](https://openai.com/blog/openai-five)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想教一个机器人走路，我们可以使用强化学习让机器人自己学会走路。我们不需要告诉机器人在什么时间如何移动哪个身体部位。我们只告诉它，最终目标是*把自己带到前方10米的那个位置*，然后让它随机地移动四肢。某个时刻，机器人的腿部某种组合的动作会让机器人前进一步，而机器人手臂的某种动作组合则确保它不会失去平衡。同样，强化学习也被用来教机器玩围棋（[https://www.alphago-games.com](https://www.alphago-games.com)）和视频游戏（[https://openai.com/blog/openai-five](https://openai.com/blog/openai-five)）。
- en: SGD-based optimization methods are often used in supervised learning (they were
    used in the models in the previous chapters where real data is always used to
    measure the quality of synthesized data), whereas, in unsupervised learning, the
    optimization strategies are totally different.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SGD的优化方法通常用于监督学习（它们曾用于前几章的模型，在这些模型中总是使用真实数据来衡量合成数据的质量），而在无监督学习中，优化策略则完全不同。
- en: 'Currently, Policy Gradients and Q-Learning are two of the most commonly used
    methods in RL. Let''s explain them in brief:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，策略梯度（Policy Gradients）和Q学习（Q-Learning）是强化学习（RL）中最常用的两种方法。我们简要解释一下它们：
- en: '**Policy Gradient** is a policy-based method. The model directly gives actions
    (output) based on the current states (input). It alternates between evaluating
    the policy (takes actions based on states) and updating the policy (updates the
    mappings between states and actions). It is often used in large and continuous
    action spaces.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略梯度（Policy Gradient）**是一种基于策略的方法。模型直接根据当前状态（输入）给出动作（输出）。它在评估策略（基于状态采取行动）和更新策略（更新状态和动作之间的映射）之间交替进行。它通常用于大的连续动作空间。'
- en: '**Q-Learning** is a value-based method. It maintains a Q-table that keeps track
    of the rewards of various actions. It chooses the action that leads to the maximum
    reward value and then updates the Q-table, based on the new environment as a result
    of the action. It can be trained faster than the Policy Gradient method and is
    often used for simple tasks with small action spaces.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Q学习（Q-Learning）**是一种基于价值的方法。它维护一个Q表，记录各种动作的奖励。它选择导致最大奖励值的动作，然后根据该动作带来的新环境更新Q表。与策略梯度方法相比，它的训练速度较快，通常用于简单任务和小的动作空间。'
- en: So, how can we choose between reinforcement learning and supervised learning
    (such as SGD methods in CNNs) when both of them are available? A simple rule of
    thumb is the **continuity** of the search space and the **differentiability**
    of the objective function. If the objective function is differentiable and the
    search space is continuous, it's better to use SGD methods. If the search space
    is discrete or the objective function is nondifferentiable, we need to stick to
    reinforcement learning. However, if the search space isn't very large and you
    have extra computing power to spare, **Evolutionary Search** (**ES**) methods
    are also a good option. When your variables are assumed to obey Gaussian distribution,
    you can always give the CMA-ES ([http://cma.gforge.inria.fr](http://cma.gforge.inria.fr))
    method a try.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当强化学习和监督学习（如CNN中的SGD方法）都可用时，我们该如何选择呢？一个简单的经验法则是**搜索空间的连续性**和目标函数的**可微性**。如果目标函数是可微的，并且搜索空间是连续的，那么最好使用SGD方法。如果搜索空间是离散的，或者目标函数是不可微的，我们需要坚持使用强化学习。然而，如果搜索空间不是特别大，并且你有多余的计算能力，那么**进化搜索（Evolutionary
    Search，ES）**方法也是一个不错的选择。当你的变量假定服从高斯分布时，你可以尝试CMA-ES方法（[http://cma.gforge.inria.fr](http://cma.gforge.inria.fr)）。
- en: 'Here are two extra reading materials if you want to learn more about Policy
    Gradients:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解策略梯度，这里有两篇额外的阅读材料：
- en: '[https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146)'
- en: '[https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of SeqGAN
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind SeqGAN is to get it to solve problems that vanilla GANs can't,
    since they are good at synthesizing discrete data, and discriminator networks
    can't, since they can't evaluate sequential data with various lengths. To solve
    the first problem, Policy Gradients are used for updating the generator network.
    The second problem is addressed by generating the remaining data with the **Monte
    Carlo Tree Search** (**MCTS**) method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The reinforcement learning strategy in SeqGAN is designed as follows. Let''s
    assume that at time ![](img/968d471c-2b1b-42a8-9c28-ad6358aa650f.png), the generated
    sequence is denoted as ![](img/6cc4ad05-2814-4019-827b-9c763601285a.png) and that
    the current action, ![](img/fecafdf9-7338-4c4b-9d12-8cb53b5d2d35.png), needs to
    be given by the generator network, ![](img/d3cc5c60-c47a-47a8-aacd-68b41d882508.png),
    in which ![](img/555b91db-e108-404f-a68b-5e0b61d13df5.png) is the initial state.
    The generation of ![](img/43f5aadd-a184-4c99-a92f-3951ac78e84a.png) based on ![](img/fef5a57f-bb22-48a5-9a1b-92290afb70e9.png) is
    done by LSTM (or any of its variants). The objective of the generator is to maximize
    the cumulative rewards:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b2631cf-8640-4697-9e51-9bc966795221.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/e6c5ed41-a6a3-4921-95c1-a83888146822.png) is the cumulative rewards, ![](img/bc008ce8-371b-4d4b-9eca-189aad473605.png) is
    the parameters to be optimized (that is, parameters in ![](img/66c2b854-dc58-45e0-970c-e555c8362e9a.png)),
    and ![](img/db34895b-1687-40ee-ad8c-b8090906a2c1.png) is called the **action-value
    function**. The action-value function, ![](img/26713f87-4914-4848-a523-a54070d61713.png), gives
    us the reward of taking the action, ![](img/1e773778-6365-4fe5-abe8-6146c0cd9943.png), by
    following policy, ![](img/c826ad81-ef12-430c-aedc-59d6b1608688.png), starting
    from the initial state, ![](img/e5f6cb1c-7e78-41e1-bc98-c7aaeabcceff.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Normally, we would expect to use the discriminator network to give us reward
    values. However, the discriminator cannot be used directly to calculate the cumulative
    rewards because it can only evaluate a full-length sequence, ![](img/be89d748-38d2-4897-b0ec-506ae4cd6f31.png).
    At time ![](img/9e8f42b8-3dfc-4277-b2a6-c46e8e71f07b.png), all we have is ![](img/b50d186b-900f-4e29-bdad-8ecb9903eddf.png).
    How do we get the rest of the sequence?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'In SeqGAN, the remaining sequence, ![](img/2aec922b-2d4a-45d9-a159-7fa86af0acb2.png), is
    generated by the MCTS method. MCTS is a tree-based search method and widely used
    in chess- and poker-playing programs and video game AI algorithms. All the actions
    that can be made are represented by nodes in a very large tree. It takes four
    steps to do a complete search in the Monte Carlo tree, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection**, which is where you select a path from the root node to a leaf
    node. Normally, the selection of the existing nodes is based on **Upper Confidence
    Bounds** (**UCB**). Nodes with high scores are more likely to be chosen and nodes
    that haven''t been chosen that many times before are more likely to be selected.
    It is a balance between **exploration and exploitation**.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择**，即从根节点到叶节点选择一条路径。通常，现有节点的选择是基于**上置信界限**（**UCB**）。得分较高的节点更有可能被选择，而那些之前没有被选择过很多次的节点更有可能被选中。这是**探索与利用**之间的平衡。'
- en: '**Expansion**, which is where you add new child nodes to the selected leaf
    node.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展**，即在选定的叶节点上添加新的子节点。'
- en: '**Simulation**, which is where you evaluate the newly added nodes and get the
    final results (rewards).'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模拟**，即评估新添加的节点并获得最终结果（奖励）。'
- en: '**Backpropagation**, which is where you update the scores and counts statistics
    of all the nodes on the selected path.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向传播**，即更新所选路径上所有节点的得分和计数统计。'
- en: In fact, only the third step, simulation, is used to generate the remaining
    sequence, where it performs the simulation (generating the remaining sequence
    with ![](img/d8b164aa-836e-4c56-898c-e5ba4f5d43fe.png)) multiple times generate
    and get the averaged reward.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，只有第三步——模拟，才是用来生成剩余序列的，它通过多次执行模拟（生成剩余序列，使用 ![](img/d8b164aa-836e-4c56-898c-e5ba4f5d43fe.png)）来生成并获得平均奖励。
- en: 'Therefore, the definition of ![](img/68e90681-3d82-4476-84d9-ec342c6a7d4f.png) is
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/68e90681-3d82-4476-84d9-ec342c6a7d4f.png) 的定义如下：
- en: '![](img/5716dd43-c145-4ce2-9891-57ac1383612d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5716dd43-c145-4ce2-9891-57ac1383612d.png)'
- en: 'The generator network is an LSTM network with an embedding layer as an input
    layer and a linear layer as an output layer. The discriminator network consists
    of an embedding layer, a convolution layer, a max-pooling layer, and a softmax
    layer. The code that was published by the authors of this paper was written for
    TensorFlow. Luckily, a PyTorch version can be found on GitHub at [https://github.com/suragnair/seqGAN](https://github.com/suragnair/seqGAN).
    In this version, two differences should be noted: first, the Monte Carlo simulation
    is only performed once, and second, the discriminator network is also a recurrent
    network and a variant of LSTM called **Gated Recurrent Unit** (**GRU**) is used
    in both networks. Feel free to adjust the network architectures and try out the
    tricks and techniques we have learned in the previous chapters of this book. Our
    modified code is also available under the `seqgan` folder in the code repository
    for this chapter.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络是一个 LSTM 网络，输入层是一个嵌入层，输出层是一个线性层。判别器网络由一个嵌入层、一个卷积层、一个最大池化层和一个 softmax 层组成。本文作者发布的代码是为
    TensorFlow 编写的。幸运的是，在 GitHub 上可以找到 PyTorch 版本，链接为 [https://github.com/suragnair/seqGAN](https://github.com/suragnair/seqGAN)。在这个版本中，有两个需要注意的区别：首先，蒙特卡洛模拟只执行一次；其次，判别器网络也是一个递归网络，并且在两个网络中使用了一种叫做
    **门控递归单元**（**GRU**）的 LSTM 变体。您可以自由地调整网络架构，尝试我们在本书前几章中学到的技巧和方法。我们修改后的代码也可以在本章的代码库中的
    `seqgan` 文件夹下找到。
- en: Creating your own vocabulary for training
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您自己的词汇表用于训练
- en: Reading code that's been written by someone else in GitHub is easy. The most
    important thing we need to do is apply the models we know to new applications
    and create our own samples. Here, we will walk through the basic steps of creating
    a vocabulary from a huge collection of text and use it to train our NLP models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读别人写的 GitHub 上的代码是很容易的。我们需要做的最重要的事情是将我们已知的模型应用到新的应用中，并创建我们自己的样本。在这里，我们将通过一些基本步骤来创建一个从大量文本中提取的词汇表，并用它来训练我们的
    NLP 模型。
- en: In the NLP model, a vocabulary set is normally a table that maps each word or
    symbol to a unique token (typically, an `int` value) so that any sentence can
    be represented by a vector of `int`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 模型中，词汇集通常是一个表，将每个单词或符号映射到一个唯一的标记（通常是 `int` 值），这样任何句子都可以通过一个 `int` 向量来表示。
- en: First, let's find some data to play with. To get started, here's a list of NLP
    datasets available on GitHub: [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets).
    From this list, you will find an English joke dataset ([https://github.com/taivop/joke-dataset](https://github.com/taivop/joke-dataset))
    that contains more than 200,000 jokes parsed from Reddit ([https://www.reddit.com/r/jokes](https://www.reddit.com/r/jokes)),
    Stupid Stuff ([stupidstuff.org](http://stupidstuff.org/)), and Wocka ([wocka.com](http://wocka.com/)). 
    The joke text will be in three different files (`reddit_jokes.json`, `stupidstuff.json`,
    and `wocka.json`). Please don't hold us responsible for the content of these jokes!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's create our vocabulary. First, create a folder named `data` in the
    project code folder and copy the aforementioned files into it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a small program so that we can parse the JSON files and
    put them in CSV format. Let''s call it `parse_jokes.py`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I'm sure that the import section entries are obvious. The definitions of the
    constants should be fairly obvious as well. The headers variable is simply a list
    of the column names that will be used when we create the CSV file.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'We want all of the jokes that will be stored in our files to be in plain text.
    To do this, get rid of all the non-letter symbols. This is done by cleaning the
    text using `clean_str()`, which uses Python''s `str_translate` parameter, as shown
    here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Feel free to tweak the `filters` string so that you can add or remove any special
    characters. The next function will read one of our three JSON files and return
    it as a JSON object. I''ve made it rather generic, so that the only thing it needs
    to know about is the filename to deal with:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we''ll create three functions that will handle converting the three JSON
    objects into CSV files. It is important to remember that none of the three JSON
    files have the same structure. Due to this, we''ll make all three handler functions
    fairly similar and handle the differences between them at the same time. Each
    of the functions will take the JSON object that was created by the `get_data`
    function, as well as an integer value called `startcount`. This will provide a
    row number for the CSV file.  This value will be incremented for each line in
    the JSON object. Then, we will create a dictionary out of each piece of data and
    write it to the CSV file.  Finally, we will return our counter so that the next
    function knows what the row value should be. This is the function that will handle
    the Reddit file:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we have the other two functions: one for the `StupidStuff` file and the
    other for the `Wocka` file:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The second to last function will create the actual CSV file and write the header:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we have the main function and the entry point for the program. Here,
    we will call the preceding functions in any order we like:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, all we have to do is run the script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When we're finished, the joke text will be stored in the `jokes.csv` file. Now,
    we need to use TorchText to build the vocabulary. TorchText ([https://github.com/pytorch/text](https://github.com/pytorch/text))
    is a data loading tool for NLP that works directly with PyTorch.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '**Note for Windows 10 users**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, there appears to be an issue in `torchtext\utils.py`.
    If you install the `torchtext` package directly from PyPi, you could run into
    an error while trying to execute some of the code.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way around this is to head over to the GitHub source repository ([https://github.com/pytorch/text](https://github.com/pytorch/text))
    and download the source code. Then, unpack the code into a safe folder. In Command
    Prompt, navigate to the folder that contains the source code and enter the following
    command to install the library:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install -e .`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: This will install torchtext directly from the source code.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'For other OS, you can install it with the following command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Please make sure that you have installed the latest version of `torchtext` (0.4.0,
    at the time of writing this book); otherwise, the code we will use later may not
    work for you. If `pip` doesn't install the latest version for you, you can find
    the `whl` file at [https://pypi.org/project/torchtext/#files](https://pypi.org/project/torchtext/#files)
    and install it manually.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the default vocab tool provided by `torchtext` for this. You can
    also try using `spaCy` ([https://spacy.io](https://spacy.io)) if you want to build
    vocab for more complex NLP tasks. Create a new file and call it `mymain.py`. Start
    by adding the following code to it:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `datafields` structure describes the CSV file we just created. Each column
    in the file is described and the only column we want the `torchtext` library to
    be concerned with is the `'Joke'` column, so we mark that as `'src'` and all the
    other columns as `'None'`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create the dataset object and start to build a vocabulary object:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll use the `torchtext` library''s `BucketIterator` to go through the data
    in the dataset and create sequences of equal length:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we''ve built our vocabulary, we need to build a small data loader
    that will feed the batch data into SeqGAN during training:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also need a mapping from tokens back to words so that we can see the generated
    text when the training process is complete:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, our vocabulary is stored in `src.vocab`. `src.vocab.stoi` is a Python
    `defaultdict` that maps words to `int` values. The last line in the preceding
    code snippet inverses the dictionary and stores the mappings from the `int` values
    as words in `inv_vocab`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the vocabulary with the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you''re curious, you can view the contents of `inv_vocab` by adding the
    following code after the preceding code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'However, remember that around 5,000 lines will be printed, so it will be a
    long list:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, we need to work on the rest of the SeqGAN program. This includes the generator
    and the discriminator. As we mentioned in the *Architecture of SeqGAN* section,
    these modules can be found at [https://github.com/suragnair/seqGAN](https://github.com/suragnair/seqGAN).
    Download the source code and unpack it into a folder in your working directory.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'To train SeqGAN, run the following script under the code folder:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The generator network is pretrained with **Maximum Likelihood Estimation** (**MLE**)
    against the real data for 100 epochs so that it will be trained faster later.
    Then, the discriminator network is pretrained against real data and some generated
    data for 150 epochs, in which the generated data is kept the same for every three
    epochs so that the discriminator becomes familiar with fake data. Finally, both
    networks are trained together in an adversarial fashion for 50 epochs, in which
    the discriminator network is trained 15 times more than the generator network.
    On a single GTX 1080Ti graphics card, the pretraining process takes about **33
    hours**, and 17 epochs of the final training can take long as **48 hours** to
    complete. GPU memory consumption is about 4,143 MB.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The following are some of the jokes that were generated by SeqGAN. Unfortunately,
    most of the sentences don't make sense due to mode collapse (which means that
    the same random word will appear anywhere in the sentences in one batch).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, let''s take a look. Note that sentences shorter than `MAX_SEQ_LEN` are
    filled with `<pad>` at the end and have been omitted here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '"have you ever make like a tomato of jokes ? . there d call out of vegetables
    !"'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"the patriots weren''t invited camping ! . because i can rather have been born
    in tents ."'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"trainees. it is a train for christmas pockets"'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"what do you get when you cross a kangaroo and a rhino ? . spanish"'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following sentences were generated by the model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '"i can''t stop a joke . . . . it''s all ."'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"i can''t see a new joke ."'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our model also created some jokes that were too inappropriate to print, which
    is an interesting demonstration of its attempt to emulate human humor!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Speech quality enhancement with SEGAN
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml), *Image Restoration
    with GANs*, we explored how GANs can restore some of the pixels in images. Researchers
    have found a similar application in NLP where GANs can be trained to get rid of
    the noises in audio in order to enhance the quality of the recorded speeches.
    In this section, we will learn how to use SEGAN to reduce background noise in
    the audio and make the human voice in the noisy audio more audible.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: SEGAN architecture
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Speech Enhancement GAN** (**SEGAN**) was proposed by Santiago Pascual, Antonio
    Bonafonte, and Joan Serrà in their paper, *SEGAN: Speech Enhancement Generative
    Adversarial Network*. It uses 1D convolutions to successfully remove noise from
    speech audio. You can check out the noise removal results compared to other methods
    here at [http://veu.talp.cat/segan](http://veu.talp.cat/segan). There''s also
    an upgraded version, which can be found at [http://veu.talp.cat/seganp](http://veu.talp.cat/seganp).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Images are two-dimensional, while sounds are one-dimensional. Considering GANs
    are so good at synthesizing 2D images, it is rather obvious to consider using
    1D convolution layers instead of 2D convolutions in order to harness the power
    of GANs when it comes to synthesizing audio data. This is exactly how SEGAN is
    built.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network in SEGAN employs an architecture of **Encoder-Decoder** with
    skip connections, which you may be familiar with since we have already met other
    GANs that use a similar architecture (such as `pix2pixHD`). The architecture of
    the generator network is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/193453c0-4b63-4017-9e2a-53f9ef467039.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Architecture of the generator network in SEGAN
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: First, the audio samples are cropped to a fixed length of 16,384 and are passed
    through five of the layers of the 1D convolution with a kernel size of 31 and
    a stride size of 4\. The compressed 1,024 x 16 vector (ignoring the batch channel)
    is concatenated with the latent vector (that's 1,024 x 16 in size) so that it
    can be fed through another five transposed convolution layers. The feature maps
    with the same shape in the mirrored convolution and transposed convolution layers
    are connected with skip connections. This is because the basic structures of noisy
    and clean audio are pretty much the same and skip connections help the generator
    reconstruct the structure of enhanced audio a lot faster. Finally, a denoised
    audio sample with a length of 16,384 is generated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the discriminator network of SEGAN is a single encoder network since
    all we need from the discriminator is the fidelity score of the input audio. The
    architecture of the discriminator network is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6800dd57-6543-465b-ad22-4184f8b5544f.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Architecture of the discriminator network in SEGAN
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The noisy audio and the clean (real data or synthesized data) audio are concatenated
    together to form a 2 x 16,384 tensor, which is passed through five convolution
    layers and three fully-connected layers to get the final output, which indicates
    whether the clean audio is real or synthesized. In both networks, **Parametric
    ReLU** (**PReLU**) is used as an activation function in hidden layers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Training SEGAN to enhance speech quality
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training SEGAN isn''t much different from training a normal image-synthesizing
    GAN. The training process of SEGAN is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b81b6510-8f7b-4cae-8bb6-8a469e0d2c59.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 The training process of SEGAN. Networks that are updated in each
    stage are marked with red boundaries. Here, `c*` denotes real clean audio, `n`
    denotes noisy audio, and `c` denotes synthesized clean audio.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: First, the clean audio and the noisy audio from the training data are fed into
    the discriminator network to calculate MSE loss. The synthesized audio that's
    generated by the generator, as well as the noisy audio, are also fed into the
    discriminator network. In this stage, the discriminator network is trained to
    be better at knowing the difference between real and synthesized clean audio.
    Then, the generated audio is used to fool the discriminator (by minimizing the
    MSE loss against 1) so that our generator network will get better at synthesizing
    realistic clean audio. Also, the L1 loss between the synthesized audio (`c*`)
    and real audio is calculated (with a scale factor of 100) to force the two to
    have similar basic structures. RMSprop is used as an optimization method in which
    the learning rate is set to a very small value (for example, ![](img/6e06c8fc-847e-4e9f-ad07-156b2dd74463.png)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get some audio data and see what SEGAN can do. A paired clean-noisy
    audio dataset is available here: [https://datashare.is.ed.ac.uk/handle/10283/1942](https://datashare.is.ed.ac.uk/handle/10283/1942).
    We need to download both the clean and noisy 48 kHz speech training sets. The
    `clean` dataset is about 822 MB in size while the `noisy` dataset is about 913
    MB in size. There are 11,572 pieces of speech inside both sets, most of which
    are single lines of English spoken by humans. The `noisy` audio is contaminated
    by several people speaking simultaneously.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code of SEGAN for PyTorch has been kindly provided by the authors
    of the paper: [https://github.com/santi-pdp/segan_pytorch](https://github.com/santi-pdp/segan_pytorch).
    Follow these steps to prepare your code and start training SEGAN:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following script to get the code and install the prerequisites:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'An additional tool called `ahoproc_tools` ([https://github.com/santi-pdp/ahoproc_tools](https://github.com/santi-pdp/ahoproc_tools))
    is also required. We need to download the source code of `ahoproc_tools` and copy
    the `ahoproc_tools` inside it into the root folder of `segan_pytorch`. Alternatively,
    you can access the full source code inside the code repository for this chapter
    directly. You need to run the following script to make sure that all the submodules
    have been downloaded:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Extract the `.wav` files from the downloaded `.zip` dataset files and move them
    into the `data/clean_trainset_wav` and `data/noisy_trainset_wav` folders, respectively.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, run the following script to start the training process:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, the training script will create a cache folder (`data/cache`) where it
    will temporarily store the slicing results of the audio files (because we want
    the inputs of both networks to be 16,384 in length).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: With a batch size of 300, it takes about 10.7 hours to finish 100 epochs of
    training on a single GTX 1080Ti graphics card and costs about 10,137 MB of GPU
    memory.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training process has finished, run the following script to test the
    trained model and remove the background noises from any audio file that''s put
    inside the `data/noisy_testset` folder:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to generate plain text with SeqGAN and remove
    background noises in speech audio with SEGAN. We also experimented with how to
    build a custom vocabulary from a collection of sentences for NLP tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train GANs so that we can directly
    generate 3D models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yu L, Zhang W, Wang J. (2017). *SeqGAN: Sequence Generative Adversarial Nets
    with Policy Gradient*. AAAI.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hochreiter S and Schmidhuber J. (1997). *Long Short-Term Memory. Neural computation*.
    9\. 1735-80\. 10.1162/neco.1997.9.8.1735.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Olah C. (Aug 27, 2015). *Understanding LSTM Networks*. Retrieved from [https://colah.github.io/posts/2015-08-Understanding-LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs).
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Nguyen M. (Sep 25, 2018). *Illustrated Guide to LSTMs and GRUs: A step by step
    explanation*. Retrieved from [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hui J. (Sep 12, 2018). *RL – Policy Gradient Explained*. Retrieved from [https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146).
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weng L. (Apr 8, 2018). *Policy Gradient Algorithms*. Retrieved from [https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pascual S, Bonafonte A and Serrà J. (2017). *SEGAN: Speech Enhancement Generative
    Adversarial Network*. INTERSPEECH.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
