<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Cleaning and Advanced Machine Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">The goal of data analytics in general is to uncover actionable insights that result in positive business outcomes. In the case of predictive analytics, the aim is to do this by determining the most likely future outcome of a target, based on previous trends and patterns.</p>
<p class="mce-root">The benefits of predictive analytics are not restricted to big technology companies. Any business can find ways to benefit from machine learning, given the right data.</p>
<p class="mce-root">Companies all around the world are collecting massive amounts of data and using predictive analytics to cut costs and increase profits. Some of the most prevalent examples of this are from the technology giants Google, Facebook, and Amazon, who utilize big data on a huge scale. For example, Google and Facebook serve you personalized ads based on predictive algorithms that guess what you are most likely to click on. Similarly, Amazon recommends personalized products that you are most likely to buy, given your previous purchases.</p>
<p class="mce-root">Modern predictive analytics is done with machine learning, where computer models are trained to learn patterns from data. As we saw briefly in the previous chapter, software such as scikit-learn can be used with Jupyter Notebooks to efficiently build and test machine learning models. As we will continue to see, Jupyter Notebooks are an ideal environment for doing this type of work, as we can perform adhoc testing and analysis, and easily save the results for reference later.</p>
<p><span>In this chapter, we will again take a hands-on approach by running through various examples and activities in a Jupyter Notebook. Where we saw a couple of examples of machine learning in the previous chapter, here we'll take a much slower and more thoughtful approach. Using an employee retention problem as our overarching example for the chapter, we will discuss how to approach predictive analytics, what things to consider when preparing the data for modeling, and how to implement and compare a variety of models using Jupyter Notebooks.</span></p>
<p><span>By the end of this chapter, you will be able to:<br/></span></p>
<ul>
<li><span>Plan a machine learning classification strategy</span></li>
<li><span>Preprocess data to prepare it for machine learning</span></li>
<li><span>Train classification models</span></li>
<li><span>Use validation curves to tune model parameters</span></li>
<li><span>Use dimensionality reduction to enhance model performance<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing to Train a Predictive Model</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Here, we will cover the preparation required to train a predictive model. Although not as technically glamorous as training the models themselves, this step should not be taken lightly. It's very important to ensure you have a good plan before proceeding with the details of building and training a reliable model. Furthermore, once you've decided on the right plan, there are technical steps in preparing the data for modeling that should not be overlooked. </span></p>
<div class="packt_infobox"><span>We must be careful not to go so deep into the weeds of technical tasks that we lose sight of the goal.Technical tasks include things that require programming skills, for example, constructing visualizations, querying databases, and validating predictive models. It's easy to spend hours trying to implement a specific feature or get the plots looking just right. Doing this sort of thing is certainly beneficial to our programming skills, but we should not forget to ask ourselves if it's really worth our time with respect to the current project.<br/></span></div>
<p>Also, keep in mind that Jupyter Notebooks are particularly well-suited for this step, as we can use them to document our plan, for example, by writing rough notes about the data or a list of models we are interested in training. Before starting to train models, it's good practice to even take this a step further and write out a well-structured plan to follow. Not only will this help you stay on track as you build and test the models, but it will allow others to understand what you're doing when they see your work.</p>
<p>After discussing the preparation, we will also cover another step in preparing to train the predictive model, which is cleaning the dataset. This is another thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing ground for performing dataset transformations and keeping track of the exact changes. The data transformations required for cleaning raw data can quickly become intricate and convoluted; therefore, it's important to keep track of your work. As discussed in the fist chapter, tools other than Jupyter Notebooks just don't offer very good options for doing this efficiently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Determining a Plan for Predictive Analytics</h1>
                </header>
            
            <article>
                
<p>When formulating a plan for doing predictive modeling, one should start by considering stakeholder needs. A perfect model will be useless if it doesn't solve a relevant problem. Planning a strategy around business needs ensures that a successful model will lead to actionable insights.</p>
<p>Although it may be possible in principle to solve many business problems, the ability to deliver the solution will always depend on the availability of the necessary data. Therefore, it's important to consider the business needs in the context of the available data sources. When data is plentiful, this will have little effect, but as the amount of available data becomes smaller, so too does the scope of problems that can be solved.</p>
<p><span>These ideas can be formed into a standard process for determining a predictive analytics plan, which goes as follows:<br/></span></p>
<ol>
<li><span>Look at the available data to understand the range of realistically solvable business problems. At this stage, it might be too early to think about the exact problems that can be solved. Make sure you understand the data fields available and the<br/>
time frames they apply to.</span></li>
<li><span>Determine the business needs by speaking with key stakeholders. Seek out a problem where the solution will lead to actionable business decisions.<br/></span></li>
<li><span>Assess the data for suitability by considering the availability of a sufficiently diverse and large feature space. Also, take into account the condition of the data: are there large chunks of missing values for certain variables or time ranges?<br/></span></li>
</ol>
<p><span>Steps 2 and 3 should be repeated until a realistic plan has taken shape. At this point, you will already have a good idea of what the model input will be and what you might expect as output.<br/></span></p>
<p>Once we've identified a problem that can be solved with machine learning, along with the appropriate data sources, we should answer the following questions to lay a framework for the project. Doing this will help us determine which types of machine learning models we can use to solve the problem:</p>
<ul>
<li>Is the training data labeled with the target variable we want to predict?</li>
</ul>
<p style="padding-left: 60px">If the answer is yes, then we will be doing supervised machine learning. Supervised learning has many real-world use cases, whereas it's much rarer to find business cases for doing predictive analytics on unlabeled data.</p>
<p style="padding-left: 60px">If the answer is no, then you are using unlabeled data and hence doing unsupervised machine learning. An example of an unsupervised learning method is cluster analysis, where labels are assigned to the nearest cluster for each sample. </p>
<ul>
<li>If the data is labeled, then are we solving a regression or classification problem?</li>
</ul>
<p style="padding-left: 60px">In a regression problem, the target variable is continuous, for example, predicting the amount of rain tomorrow in centimeters. In a classification problem, the target variable is discrete and we are predicting class labels. The simplest type of classification problem is binary, where each sample is grouped into one of two classes. For example, will it rain tomorrow or not?</p>
<ul>
<li>What does the data look like? How many distinct sources?</li>
</ul>
<p style="padding-left: 60px">Consider the size of the data in terms of width and height, where width refers to the number of columns (features) and height refers to the number of rows. Certain algorithms are more effective at handling large numbers of features than others. Generally, the bigger the dataset, the better in terms of accuracy. However, training can be very slow and memory intensive for large datasets. This can always be reduced by performing aggregations on the data or using dimensionality reduction techniques.</p>
<p>If there are different data sources, can they be merged into a single table? If not, then we may want to train models for each and take an ensemble average for the final prediction model. An example where we may want to do this is with various sets of times series data on different scales. Consider we have the following data sources: a table with the AAPL stock closing prices on a daily time scale and iPhone sales data on a monthly time scale.</p>
<p>We could merge the data by adding the monthly sales data to each sample in the daily time scale table, or grouping the daily data by month, but it might be better to build two models, one for each dataset, and use a combination of the results from each in the final prediction model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing Data for Machine Learning</h1>
                </header>
            
            <article>
                
<p>Data preprocessing has a huge impact on machine learning. Like the saying "you are what you eat," the model's performance is a direct reflection of the data it's trained on. Many models depend on the data being transformed so that the continuous feature values have comparable limits. Similarly, categorical features should be encoded into numerical values. Although important, these steps are relatively simple and do not take very long.</p>
<div class="packt_infobox">The aspect of preprocessing that usually takes the longest is cleaning up messy data. Just take a look at this pie plot showing what data scientists from a particular survey spent most of their time doing.</div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/32567c8e-b535-47f7-926b-58915f4cb7cf.png" style="width:69.92em;height:29.92em;"/></p>
<p>Another thing to consider is the size of the datasets being used by many data scientists. As the dataset size increases, the prevalence of messy data increases as well, along with the difficulty in cleaning it.</p>
<p>Simply dropping the missing data is usually not the best option, because it's hard to justify throwing away samples where most of the fields have values. In doing so, we could lose valuable information that may hurt final model performance.</p>
<p><span>The steps involved in data preprocessing can be grouped as follows:</span></p>
<ul>
<li><span>Merging data sets on common fields to bring all data into a single table</span></li>
<li><span>Feature engineering to improve the quality of data, for example, the use of dimensionality reduction techniques to build new features</span></li>
<li><span>Cleaning the data by dealing with duplicate rows, incorrect or missing values, and other issues that arise</span></li>
<li><span>Building the training data sets by standardizing or normalizing the required data and splitting it into training and testing sets<br/></span></li>
</ul>
<p><span>Let's explore some of the tools and methods for doing the preprocessing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring data preprocessing tools and methods</h1>
                </header>
            
            <article>
                
<ol>
<li><span>Start the <kbd>NotebookApp</kbd> from the project directory by executing <kbd>jupyter notebook</kbd>.Navigate to the <kbd>chapter-2</kbd> directory and open up the <kbd>chapter-2-workbook.ipynb</kbd> file. Find the cell near the top where the packages are loaded, and run it.</span></li>
</ol>
<p style="padding-left: 60px"><span>We are going to start by showing off some basic tools from Pandas and scikit-learn. Then, we'll take a deeper dive into methods for rebuilding missing data.<br/></span></p>
<ol start="2">
<li><span>Scroll down to Subtopic <kbd>Preprocessing data for machine learning</kbd> and run the cell containing <kbd>pd.merge</kbd>? to display the docstring for the merge function in the notebook:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6f724102-2815-42ef-869b-f59d448858ec.png" style="width:51.25em;height:36.42em;"/></p>
<p style="padding-left: 60px"><span>As we can see, the function accepts a left and right DataFrame to merge. You can specify one or more columns to group on as well as how they are grouped, that is,to use the left, right, outer, or inner sets of values. Let's see an example of this in use.</span></p>
<ol start="3">
<li><span>Exit the help popup and run the cell containing the following sample DataFrames:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    </span>df_1 = pd.DataFrame({'product': ['red shirt', 'red shirt', 'red shirt',<br/>                                 'white dress'],<br/>                     'price': [49.33, 49.33, 32.49, 199.99]}),<br/>    df_2 = pd.DataFrame({'product': ['red shirt', 'blue pants',<br/>                                 'white tuxedo', 'white dress'],<br/>                     'in_stock': [True, True, False, False]})<span>    </span></pre>
<p style="padding-left: 60px">Here, we will build two simple DataFrames from scratch. As can be seen, they contain a <kbd>product</kbd> column with some shared entries.</p>
<p style="padding-left: 60px">Now, we are going to perform an inner merge on the <kbd>product</kbd> shared column and print the result.</p>
<ol start="4">
<li><span>Run the next cell to perform the inner merge:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9c21a6e7-9029-4b81-a4a2-b0ded6813e94.png" style="width:35.83em;height:16.00em;"/></p>
<p style="padding-left: 60px"><span>Note how only the shared items, <strong>red shirt</strong> and white dress, are included. To include all entries from both tables, we can do an outer merge instead. Let's do this now.</span></p>
<ol start="5">
<li><span>Run the next cell to perform an outer merge:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png" style="width:35.58em;height:20.17em;"/></p>
<p style="padding-left: 60px"><span>This returns all of the data from each table where missing values have been labeled with <kbd>NaN</kbd>.<br/></span></p>
<ol start="6">
<li><span>Run the next cell to perform an outer merge:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png" style="width:34.58em;height:19.58em;"/></p>
<p style="padding-left: 60px"><span>This returns all of the data from each table where missing values have been labeled with <kbd>NaN</kbd>.<br/></span></p>
<p>Since this is our fist time encountering an <kbd>NaN</kbd> value in this book, now is a good time to discuss how these work in Python.</p>
<p><span>First of all, you can define an <kbd>NaN</kbd> variable by doing, for example, <kbd>a = float('nan')</kbd>.</span></p>
<p><span>However, if you want to test for equality, you cannot simply use standard comparison methods.</span></p>
<p><span>It's best to do this instead with a high-level function from a library such as <kbd>NumPy</kbd>. This is illustrated with the following code:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/4e351060-c60e-41e8-98fb-de8f72b9e064.png" style="width:37.17em;height:19.25em;"/><br/></span></p>
<p><span>Some of these results may seem counter intuitive. There is logic behind this behavior, however, and for a deeper understanding of the fundamental reasons for standard comparisons returning False, check out this excellent Stack Overflow thread: <a href="https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values">https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values.</a></span></p>
<ol>
<li><span>You may have noticed that our most recently merged table has duplicated data in the fist few rows. Let's see how to handle this.<br/></span></li>
</ol>
<p style="padding-left: 60px"><span>Run the cell containing <kbd>df.drop_duplicates()</kbd> to return a version of the DataFrame with no duplicate rows:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a6eb2f78-014e-415c-9f05-60aa2a799270.png" style="width:15.50em;height:14.17em;"/></p>
<p style="padding-left: 60px"><span>This is the easiest and "standard" way to drop duplicate rows. To apply these changes to df, we can either <kbd>set inplace=True</kbd> or do something like <kbd>df = df.drop_duplicated()</kbd> . Let's see another method, which uses masking to select or drop duplicate rows.<br/></span></p>
<ol start="2">
<li>Run the cell containing <kbd>df.duplicated()</kbd> to print the True/False series, marking duplicate rows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/76823424-ab27-4c20-b56a-053833baa96f.png" style="width:15.83em;height:14.08em;"/></p>
<p style="padding-left: 60px">We can take the sum of this result to determine how many rows have duplicates, or it can be used as a mask to select the duplicated rows.</p>
<ol start="3">
<li><span>Do this by running the next two cells:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3c292a6c-ab36-4293-9e9b-4288b5cba555.png" style="width:16.83em;height:14.83em;"/></p>
<ol start="4">
<li><span>We can compute the opposite of the mask with a simple tilde (<kbd>~</kbd>) to extract the deduplicated DataFrame. Run the following code and convince yourself the output is the same as that from <kbd>df.drop_duplicates()</kbd> :</span></li>
</ol>
<p style="padding-left: 60px"><span><kbd>df[~df.duplicated()]</kbd><br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9a3e75fd-9f5c-42ce-8b71-cb95595583d6.png" style="width:18.00em;height:16.50em;"/></p>
<ol start="5">
<li><span>This can also be used to drop duplicates from a subset of the full DataFrame. For example, run the cell containing the following code:<br/></span></li>
</ol>
<p style="padding-left: 60px"><span><kbd>df[~df['product'].duplicated()]</kbd><br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cac8d491-5f80-4fb4-b09f-440e34f9453c.png" style="width:20.25em;height:13.67em;"/></p>
<p style="padding-left: 60px">Here, we are doing the following things:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Creating a mask (a <kbd>True</kbd>/<kbd>False</kbd> series) for the product row, where duplicates are marked with <kbd>e</kbd></li>
<li> <span>Using the tilde (<kbd>~</kbd>) to take the opposite of that mask, so that duplicates are instead marked with False and everything else is</span> <kbd>True</kbd><span> </span></li>
</ul>
<ul>
<li>Using that mask to filter out the <kbd>False</kbd> rows of <kbd>df</kbd>, which correspond to the duplicated products</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">As expected, we now see that only the first red shirt row remains, as the duplicate product rows have been removed.</p>
<p style="padding-left: 60px">In order to proceed with the steps, let's replace <kbd>df</kbd> with a deduplicated version of itself. This can be done by running <kbd>drop_duplicates</kbd> and passing the parameter <kbd>inplace=True</kbd>.</p>
<ol start="6">
<li><span>Deduplicate the DataFrame and save the result by running the cell containing the following code:<br/>
<kbd>df.drop_duplicates(inplace=True)</kbd><br/></span></li>
</ol>
<p style="padding-left: 60px"><span>Continuing on to other preprocessing methods, let's ignore the duplicated rows and first deal with the missing data. This is necessary because models cannot be trained on incomplete samples. Using the missing price data for blue pants and white tuxedo as an example, let's show some different options for handling <kbd>NaN</kbd> values.<br/></span></p>
<ol start="7">
<li><span>One option is to drop the rows, which might be a good idea if your NaN samples are missing the majority of their values. Do this by running the cell containing <kbd>df.dropna()</kbd>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aeae3f40-b4db-4577-a076-173d33862f59.png" style="width:21.75em;height:17.08em;"/></p>
<ol start="8">
<li><span>If most of the values are missing for a feature, it may be best to drop that column entirely. Do this by running the cell containing the same method as before, but this time with the axes parameter passed to indicate columns instead of rows:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/98ef084c-27d4-496d-8fea-db589f8eb018.png" style="width:21.08em;height:19.25em;"/></p>
<p style="padding-left: 60px"><span>Simply dropping the <kbd>NaN</kbd> values is usually not the best option, because losing data is never good, especially if only a small fraction of the sample values is missing.Pandas offers a method for filling in <kbd>NaN</kbd> entries in a variety of different ways, some of which we'll illustrate now.</span></p>
<ol start="9">
<li><span>Run the cell containing</span> <kbd>df.fillna?</kbd> <span>to print the docstring for the Pandas</span> <kbd>NaN-fill</kbd><span> method:</span><span><br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/4fb8a796-95f2-4d38-a0db-f9624e3b14a5.png" style="width:47.17em;height:23.00em;"/></span></p>
<p style="padding-left: 60px"><span>Note the options for the value parameter; this could be, for example, a single value or a dictionary/series type map based on index. Alternatively, we can leave the value as None and pass a fill method instead. We'll see examples of each in this chapter.<br/></span></p>
<ol start="10">
<li><span>Fill in the missing data with the average product price by running the cell containing the following code:<br/>
<kbd>df.fillna(value=df.price.mean())</kbd><br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d2508e74-e8e6-49b6-ab28-f45226336411.png" style="width:20.58em;height:17.33em;"/></p>
<ol start="11">
<li><span>Now, fill in the missing data using the pad method by running the cell containing the following code instead:<br/>
<kbd>df.fillna(method='pad')</kbd><br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/06574946-eeaf-44c8-8d7c-ff33db9d53e6.png" style="width:27.67em;height:18.67em;"/></p>
<p style="padding-left: 60px">Notice how the <strong>white dress</strong> price was used to pad the missing values below it.</p>
<p style="padding-left: 60px">To conclude this section, we will prepare our simple table to be used for training a machine learning algorithm. Don't worry, we won't actually try to train any models on such a small dataset! We start this process by encoding the class labels for the categorical data.</p>
<ol start="12">
<li>Before encoding the labels, run the fist cell in the <kbd>Building training data sets</kbd> section to add another column of data representing the average product ratings:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/780a265b-e2e9-4999-94c9-bebd4b545d11.png" style="width:29.25em;height:19.75em;"/></p>
<p style="padding-left: 60px">Imagining we want to use this table to train a predictive model, we should first think about changing all the variables to numeric types.</p>
<ol start="13">
<li>The simplest column to handle is the Boolean list:<kbd>in_stock</kbd>. This should be changed to numeric values, for example, 0 and 1, before using it to train a predictive model. This can be done in many ways, for example, by running the cell containing the following code: <kbd>df.in_stock = df.in_stock.map({False: 0, True: 1})</kbd></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/870248af-69ea-4dec-b052-fca89b2e7dbe.png" style="width:28.83em;height:17.92em;"/></p>
<ol start="14">
<li><span>Another option for encoding features is scikit-learn's LabelEncoder, which can be used to map the class labels to integers at a higher level. Let's test this by running the cell containing the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.preprocessing import LabelEncoder<br/>    rating_encoder = LabelEncoder()<br/>    _df = df.copy()<br/>    _df.rating = rating_encoder.fit_transform(df.rating)<br/>    _df </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c6e2ce61-258a-4a0a-bfe5-743d9a90ce04.png" style="width:32.42em;height:22.17em;"/></p>
<p style="padding-left: 60px">This might bring to mind the preprocessing we did in the previous chapter, when building the polynomial model. Here, we instantiate a label encoder and then "train" it and "transform" our data using the <kbd>fit_transform</kbd> method. We apply the result to a copy of our DataFrame, <kbd>_df</kbd>.</p>
<ol start="15">
<li><span>The features can then be converted back using the class we reference with the variable <kbd>rating_encoder, by running rating_encoder.inverse_transform(df.rating)</kbd>:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d0c3289c-bfb4-4b4f-9566-163d2acb87d5.png" style="width:35.58em;height:6.50em;"/></p>
<p style="padding-left: 60px">You may notice a problem here. We are working with a so-called "ordinal" feature, where there's an inherent order to the labels. In this case, we should expect that a rating of "low" would be encoded with a 0 and a rating of "high" would be encoded with a 2. However, this is not the result we see. In order to achieve proper ordinal label encoding, we should again use map, and build the dictionary ourselves.</p>
<ol start="16">
<li><span>Encode the ordinal labels properly by running the cell containing the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    ordinal_map = {rating: index for index, rating in enumerate<br/>    (['low','medium', 'high'])}<br/>    print(ordinal_map)<br/>    df.rating = df.rating.map(ordinal_map) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/001e575c-a915-4803-9378-acd1388b0269.png" style="width:44.67em;height:21.25em;"/></p>
<p style="padding-left: 60px">We first create the mapping dictionary. This is done using a dictionary comprehension and enumeration, but looking at the result, we see that it could just as easily be defined <em>manually</em> instead. Then, as done earlier for the <kbd>in_stock</kbd> column, we apply the dictionary mapping to the feature. Looking at the result, we see that rating now makes more sense than before, where <kbd>low</kbd> is labeled with 0, <kbd>medium</kbd> with 1, and <kbd>high</kbd> with 2.</p>
<p style="padding-left: 60px">Now that we've discussed ordinal features, let's touch on another type called nominal features. These are fields with no inherent order, and in our case, we see that <kbd>product</kbd> is a perfect example.</p>
<p style="padding-left: 60px">Most scikit-learn models can be trained on data like this, where we have strings instead of integer-encoded labels. In this situation, the necessary conversions are done under the hood. However, this may not be the case for all models in scikit learn, or other machine learning and deep learning libraries. Therefore, it's good practice to encode these ourselves during preprocessing</p>
<ol start="17">
<li>A commonly used technique to convert class labels from strings to numerical values is called one-hot encoding. This splits the distinct classes out into separate features. It can be accomplished elegantly with <kbd>pd.get_dummies()</kbd> . Do this by running the cell containing the following code: <kbd>df = pd.get_dummies(df)</kbd></li>
</ol>
<p style="padding-left: 60px"><span>The final DataFrame then looks as follows:<br/></span></p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 60px"><img src="assets/96dcf83e-e7c7-47ed-b2f4-3c92f4714a67.png" style="width:58.75em;height:21.08em;"/></p>
<p style="padding-left: 60px"><span>Here, we see the result of one-hot encoding: the product column has been split into 4, one for each unique value. Within each column, we find either a 1 or 0 representing whether that row contains the particular value or product.</span></p>
<p style="padding-left: 60px"><span>Moving on and ignoring any data scaling (which should usually be done), the final step is to split the data into training and test sets to use for machine learning. This can be done using scikit-learn's train_test_split. Let's assume we are going to try to predict whether an item is in stock, given the other feature values.<br/></span></p>
<ol start="18">
<li><span>Split the data into training and test sets by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    features = ['price', 'rating', 'product_blue pants',<br/>    'product_red shirt', 'product_white dress',<br/>    'product_white tuxedo']<br/>    X = df[features].values<br/>    target = 'in_stock'<br/>    y = df[target].values<br/>    from sklearn.model_selection import train_test_split<br/>    X_train, X_test, y_train, y_test = \<br/>    train_test_split(X, y, test_size=0.3) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/086e1b46-616e-436b-8560-0681c6cf3ce2.png" style="width:23.75em;height:17.67em;"/></p>
<p style="padding-left: 60px">Here, we are selecting subsets of the data and feeding them into the <kbd>train_test_ split</kbd> function. This function has four outputs, which are unpacked into the training and testing splits for features (X) and the target (y).</p>
<p style="padding-left: 60px">Observe the shape of the output data, where the test set has roughly 30% of the samples and the training set has roughly 70%.</p>
<p style="padding-left: 60px">We'll see similar code blocks later, when preparing real data to use for training predictive models.</p>
<p>This concludes the section on cleaning data for use in machine learning applications. Let's take a minute to note how effective our Jupyter Notebook was for testing various methods of transforming the data, and ultimately documenting the pipeline we decided upon. This could easily be applied to an updated version of the data by altering only specific cells of code, prior to processing. Also, should we desire any changes to the processing, these can easily be tested in the notebook, and specific cells may be changed to accommodate the alterations. The best way to achieve this would probably be to copy the notebook over to a new file, so that we can always keep a copy of the original analysis for reference.</p>
<p>Moving on to an activity, we'll now apply the concepts from this section to a large dataset as we prepare it for use in training predictive models.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Preparing to Train a Predictive Model for the Employee-Retention Problem</h1>
                </header>
            
            <article>
                
<p>Suppose you are hired to do freelance work for a company who wants to find insights into why their employees are leaving. They have compiled a set of data they think will be helpful in this respect. It includes details on employee satisfaction levels, evaluations, time spent at work, department, and salary.</p>
<p>The company shares their data with you by sending you a file called <kbd>hr_data.csv</kbd> and asking what you think can be done to help stop employees from leaving. To apply the concepts we've learned thus far to a real-life problem. In particular, we seek to:</p>
<ul>
<li><span>Determine a plan for using predictive analytics to provide impactful business insights, given the available data.</span></li>
<li><span>Prepare the data for use in machine learning models.<br/></span></li>
</ul>
<div class="packt_infobox"><span>Starting with this activity and continuing through the remainder of this chapter, we'll be using <em>Human Resources Analytics</em>, which is a Kaggle dataset. There is a small difference between the dataset we use in this book and the online version. Our human resource analytics data contains some <kbd>NaN</kbd> values. These were manually removed from the online version of the<br/>
dataset, for the purposes of illustrating data cleaning techniques. We have also added a column of data called <kbd>is_smoker</kbd>, for the same purposes.<br/></span></div>
<ol>
<li>With the <kbd>chapter-2-workbook.ipynb</kbd> notebook file open, scroll to the Activity section.</li>
<li><span>Check the head of the table by running the following code:<br/></span></li>
</ol>
<pre><span>      %%bash<br/>      head ../data/hr-analytics/hr_data.csv </span></pre>
<p style="padding-left: 60px"><span>Judging by the output, convince yourself that it looks to be in standard CSV format. For CSV files, we should be able to simply load the data with <kbd>pd.read_csv</kbd>.<br/></span></p>
<ol start="3">
<li><span><span>Load the data with Pandas by running <kbd>df = pd.read_csv('../data/hranalytics/hr_data.csv')</kbd> . Use tab completion to help type the file path.</span></span></li>
<li><span>Inspect the columns by printing df.columns and make sure the data has loaded as expected by printing the DataFrame head and tail with <kbd>df.head()</kbd> and <kbd>df.tail()</kbd> :<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a181777-dd12-4549-b10e-63fb4fcfe683.png" style="width:73.83em;height:52.42em;"/></p>
<p style="padding-left: 60px"><span>We can see that it appears to have loaded correctly. Based on the tail index values,there are nearly 15,000 rows; let's make sure we didn't miss any.</span></p>
<ol start="5">
<li><span>Check the number of rows (including the header) in the <kbd>CSV file</kbd> with the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    with open('../data/hr-analytics/hr_data.csv') as f:<br/>    print(len(f.read().splitlines())) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/18be9678-2c67-40ca-a02d-0ae482179cfc.png" style="width:41.00em;height:10.08em;"/></p>
<ol start="6">
<li><span>Compare this result to <kbd>len(df)</kbd> to make sure we've loaded all the data:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b9116ca7-4323-4383-baf0-3067742ecd08.png" style="width:35.17em;height:8.33em;"/></p>
<p style="padding-left: 60px"><span>Now that our client's data has been properly loaded, let's think about how we can use predictive analytics to find insights into why their employees are leaving.<br/></span></p>
<p style="padding-left: 60px">Let's run through the fist steps for creating a predictive analytics plan:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Look at the available data</strong>: We've already done this by looking at the columns, datatypes, and the number of samples</li>
<li><strong>Determine the business needs</strong>: The client has clearly expressed their needs: reduce the number of employees who leave</li>
<li><strong>Assess the data for suitability</strong>: Let's try to determine a plan that can help satisfy the client's needs, given the provided data</li>
</ul>
</li>
</ul>
<p>Recall, as mentioned earlier, that effective analytics techniques lead to impactful business decisions. With that in mind, if we were able to predict how likely an employee is to quit, the business could selectively target those employees for special treatment. For example, their salary could be raised or their number of projects reduced. Furthermore, the impact of these changes could be estimated using the model!</p>
<p>To assess the validity of this plan, let's think about our data. Each row represents an employee who either works for the company or has <strong>left</strong>, as labeled by the column named left. We can therefore train a model to predict this target, given a set of features.</p>
<p>Assess the target variable. Check the distribution and number of missing entries by running the following code:</p>
<pre><span>    df.left.value_counts().plot('barh')<br/>    print(df.left.isnull().sum()) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9efa19b0-e218-4946-961a-2806e606a906.png" style="width:25.17em;height:19.50em;"/></p>
<p><span>Here's the output of the second code line:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3d6017b4-3f4f-49e8-8e9d-c2baa58efef3.png" style="width:24.33em;height:9.25em;"/></p>
<p>About three-quarters of the samples are employees who have not left. The group who has left makes up the other quarter of the samples. This tells us we are dealing with an imbalanced classification problem, which means we'll have to take special measures to account for each class when calculating accuracies. We also see that none of the target variables are missing (no <kbd>NaN</kbd> values).</p>
<p><span>Now, we'll assess the features:<br/></span></p>
<ol>
<li>Print the datatype of each by executing <kbd>df.dtypes</kbd>. Observe how we have a mix of continuous and discrete features:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1bc604a8-aa56-4d85-8823-477ee6a0c060.png" style="width:20.25em;height:19.00em;"/></p>
<ol start="2">
<li><span>Display the feature distributions by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    for f in df.columns:<br/>    try:<br/>    fig = plt.figure()<br/>    …<br/>    print('-'*30) </span></pre>
<p style="padding-left: 60px">This code snippet is a little complicated, but it's very useful for showing an overview of both the continuous and discrete features in our dataset. Essentially, it assumes each feature is continuous and attempts to plot its distribution, and reverts to simply plotting the value counts if the feature turns out to be discrete.</p>
<p style="padding-left: 60px"><span>The result is as follows:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7aa02a0b-5938-487c-8d83-534b3a3881bc.png" style="width:21.58em;height:47.67em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fda3c924-026b-4987-8e31-55e64f06fc49.png" style="width:19.83em;height:53.83em;"/></p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/114b846f-53c7-47d7-a659-2b59c0c918a2.png"/></p>
<p style="padding-left: 60px">For many features, we see a wide distribution over the possible values, indicating a good variety in the feature spaces. This is encouraging; features that are strongly grouped around a small range of values may not be very informative for the model. This is the case for <kbd>promotion_last_5years</kbd>, where we see that the vast majority of samples are 0.</p>
<p style="padding-left: 60px"><span>The next thing we need to do is remove any <kbd>NaN</kbd> values from the dataset.</span></p>
<ol>
<li><span>Check how many <kbd>NaN</kbd> values are in each column by running the following code:<br/>
<kbd>df.isnull().sum() / len(df) * 100</kbd><br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/362bc015-f9d4-41c1-94d7-2914c79cbc09.png" style="width:23.00em;height:19.83em;"/></p>
<p style="padding-left: 60px"><span>We can see there are about 2.5% missing for a<kbd>verage_montly_hours</kbd>, 1% missing for t<kbd>ime_spend_company</kbd>, and 98% missing for <kbd>is_smoker</kbd>! Let's use a couple of different strategies that we've learned about to handle these.<br/></span></p>
<ol start="2">
<li>Since there is barely any information in the <kbd>is_smoker</kbd> metric, let's drop this column. Do this by running: <kbd>del df['is_smoker']</kbd> .</li>
<li>Since time_spend_company is an integer fild, we'll use the median value to fil the NaN values in this column. This can be done with the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>    fill_value = df.time_spend_company.median()<br/>    df.time_spend_company = df.time_spend_company.fillna(fill_value) </span></pre>
<p style="padding-left: 60px"><span>The final column to deal with is <kbd>average_montly_hours</kbd>. We could do something similar and use the median or rounded mean as the integer fill value. Instead though, let's try to take advantage of its relationship with another variable. This may allow us to fill the missing data more accurately.</span></p>
<ol start="4">
<li><span>Make a boxplot of <kbd>average_montly_hours</kbd> segmented by <kbd>number_project</kbd>. This can be done by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  sns.boxplot(x='number_project', y='average_montly_hours', data=df) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ad737c9-9977-47cd-aacb-389f83329698.png" style="width:61.17em;height:41.17em;"/></p>
<p style="padding-left: 60px">We can see how the number of projects is correlated with a<kbd>verage_monthly_hours</kbd>, a result that is hardly surprising. We'll exploit this relationship by filling in the <kbd>NaN</kbd> values of <kbd>average_montly_hours</kbd> differently, depending on the number of projects for that sample. Specifically, we'll use the mean of each group.</p>
<ol start="5">
<li><span>Calculate the mean of each group by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    mean_per_project = df.groupby('number_project')\.<br/>    average_montly_hours.mean()<br/>    mean_per_project = dict(mean_per_project)<br/>    print(mean_per_project) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3a759d97-a02d-493a-995e-bbdd922d3a5f.png" style="width:29.50em;height:15.92em;"/></p>
<p style="padding-left: 60px"><span>We can then map this onto the <kbd>number_project</kbd> column and pass the resulting series object as the argument to <kbd>fillna</kbd>.<br/></span></p>
<ol start="6">
<li><span>Fill the <kbd>NaN</kbd> values in <kbd>average_montly_hours</kbd> by executing the following code:</span></li>
</ol>
<pre><span>      fill_values = df.number_project.map(mean_per_project)<br/>      df.average_montly_hours = df.average_montly_hours.fillna(fill_values) </span></pre>
<ol start="7">
<li><span>Confirm that <kbd>df</kbd> has no more <kbd>NaN</kbd> values by running the following assertion test. If it does not raise an error, then you have successfully removed the <kbd>NaNs</kbd> from the table:<br/>
<kbd>assert df.isnull().sum().sum() == 0</kbd><br/></span></li>
<li><span>Finally, we will transform the string and Boolean filds into integer representations.In particular, we'll manually convert the target variable <kbd>left</kbd> from <kbd>yes</kbd> and <kbd>no</kbd> to<kbd> 1</kbd> and <kbd>0</kbd> and build the one-hot encoded features. Do this by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  df.left = df.left.map({'no': 0, 'yes': 1})<br/>  df = pd.get_dummies(df) </span></pre>
<p class="mce-root"/>
<ol start="9">
<li><span>Print <kbd>df.columns</kbd> to show the fields:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1c2b5d84-ee4a-4f17-bda4-495ebf1c5f51.png" style="width:46.75em;height:12.58em;"/></p>
<p style="padding-left: 60px">We can see that <kbd>department</kbd> and <kbd>salary</kbd> have been split into various binary features.</p>
<p style="padding-left: 60px">The final step to prepare our data for machine learning is scaling the features, but for various reasons (for example, some models do not require scaling), we'll do it as part of the model-training workflow in the next activity.</p>
<ol start="10">
<li><span>We have completed the data preprocessing and are ready to move on to training models! Let's save our preprocessed data by running the following code:</span><span><br/></span></li>
</ol>
<pre style="padding-left: 30px">     <span>df.to_csv('../data/hr-analytics/hr_data_processed.csv', index=False) </span></pre>
<p>Again, we pause here to note how well the Jupyter Notebook suited our needs when performing this initial data analysis and clean-up. Imagine, for example, we left this project in its current state for a few months. Upon returning to it, we would probably not remember what exactly was going on when we left it. Referring back to this notebook though, we would be able to retrace our steps and quickly recall what we previously learned about the data. Furthermore, we could update the data source with any new data and re-run the notebook to prepare the new set of data for use in our machine learning algorithms. Recall that in this situation, it would be best to make a copy of the notebook fist, so as not to lose the initial analysis.</p>
<p>To summarize, we've learned and applied methods for preparing to train a machine learning model. We started by discussing steps for identifying a problem that can be solved with predictive analytics. This consisted of:</p>
<ul>
<li><span>Looking at the available data</span></li>
<li><span>Determining the business needs</span></li>
<li><span>Assessing the data for suitability<br/></span></li>
</ul>
<p>We also discussed how to identify supervised versus unsupervised and regression versus classification problems.</p>
<p>After identifying our problem, we learned techniques for using Jupyter Notebooks to build and test a data transformation pipeline. These techniques included methods and best practices for filing missing data, transforming categorical features, and building train/test data sets.</p>
<p>In the remainder of this chapter, we will use this preprocessed data to train a variety of classification models. To avoid blindly applying algorithms we don't understand, we start by introducing them and overviewing how they work. Then, we use Jupyter to train and compare their predictive capabilities. Here, we have the opportunity to discuss more advanced topics in machine learning like overfitting, k-fold cross-validation, and validation curves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training Classification Models</h1>
                </header>
            
            <article>
                
<p>As we've already seen in the previous chapter, using libraries such as scikit-learn and platforms such as Jupyter, predictive models can be trained in just a few lines of code. This is possible by abstracting away the difficult computations involved with optimizing model parameters. In other words, we deal with a black box where the internal operations are hidden instead. With this simplicity also comes the danger of misusing algorithms, for example, by over fitting during training or failing to properly test on unseen data. We'll show how to avoid these pitfalls while training classification models and produce trustworthy results with the use of k-fold cross validation and validation curves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Classification Algorithms</h1>
                </header>
            
            <article>
                
<p>Recall the two types of supervised machine learning: regression and classification. In regression, we predict a continuous target variable. For example, recall the linear and polynomial models from the fist chapter. In this chapter, we focus on the other type of supervised machine learning: classification. Here, the goal is to predict the class of a sample using the available metrics.</p>
<p>In the simplest case, there are only two possible classes, which means we are doing binary classification. This is the case for the example problem in this chapter, where we try to predict whether an employee has left or not. If we have more than two class labels instead, we are doing multi-class classification.</p>
<p>Although there is little difference between binary and multi-class classification when training models with scikit-learn, what's done inside the "black box" is notably different. In particular, multi-class classification models often use the one-versus-rest method. This works as follows for a case with three class labels. When the model is "fit" with the data, three models are trained, and each model predicts whether the sample is part of an individual class or part of some other class. This might bring to mind the one-hot encoding for features that we did earlier. When a prediction is made for a sample, the class label with the highest confidence level is returned.</p>
<p>In this chapter, we'll train three types of classification models: Support Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of these algorithms are quite different. As we will see, however, they are quite similar to train and use for predictions thanks to scikit-learn. Before swapping over to the Jupyter Notebook and implementing these, we'll briefly see how they work. SVM's attempt to find the best hyperplane to divide classes by. This is done by maximizing the distance between the hyperplane and the closest samples of each class, which are called support vectors.</p>
<p>This linear method can also be used to model nonlinear classes using the kernel trick. This method maps the features into a higher-dimensional space in which the hyper plane is determined. This hyperplane we've been talking about is also referred to as the decision surface, and we'll visualize it when training our models.</p>
<p>k-Nearest Neighbors classification algorithms memorize the training data and make predictions depending on the K nearest samples in the feature space. With three features, this can be visualized as a sphere surrounding the prediction sample. Often, however, we are dealing with more than three features and therefore hyperspheres are drawn to find the closest K samples.</p>
<p>Random Forests are an ensemble of decision trees, where each has been trained on different subsets of the training data.</p>
<p>A decision tree algorithm classifies a sample based on a series of decisions. For example, the fist decision might be "if feature x_1 is less than or greater than 0." The data would then be split on this condition and fed into descending branches of the tree. Each step in the decision tree is decided based on the feature split that maximizes the information gain.</p>
<p>Essentially, this term describes the mathematics that attempts to pick the best possible split of the target variable.</p>
<p>Training a Random Forest consists of creating bootstrapped (that is, randomly sampled data with replacement) datasets for a set of decision trees. Predictions are then made based on the majority vote. These have the benefit of less overfitting and better generalizability.</p>
<div class="packt_infobox">Decision trees can be used to model a mix of continuous and categorical data, which make them very useful. Furthermore, as we will see later in this chapter, the tree depth can be limited to reduce overfitting. For a detailed (but brief) look into the decision tree algorithm, check out this popular Stack Overflw answer: <a href="https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain/1859910#1859910">https://stackoverflow. com/a/1859910/3511819</a>. There, the author shows a simple example and discusses concepts such as node purity, information gain, and entropy.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training two-feature classification models with scikitlearn</h1>
                </header>
            
            <article>
                
<p>We'll continue working on the employee retention problem that we introduced in the fist topic. We previously prepared a dataset for training a classification model, in which we predicted whether an employee has left or not. Now, we'll take that data and use it to train classification models:</p>
<ol>
<li>If you have not already done so, start the <kbd>NotebookApp</kbd> and open the <kbd>chapter-2-workbook.ipynb file</kbd>. Scroll down to <kbd>Topic Training classification models</kbd>. Run the fist couple of cells to set the default fiure size and load the processed data that we previously saved to a <kbd>CSV file</kbd>.</li>
</ol>
<p style="padding-left: 60px">For this example, we'll be training classification models on two continuous features:</p>
<p style="padding-left: 60px"> <kbd>satisfaction_level</kbd> and <kbd>last_evaluation.</kbd></p>
<ol start="2">
<li><span>Draw the bivariate and univariate graphs of the continuous target variables by running the cell with the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  sns.jointplot('satisfaction_level', <br/> 'last_evaluation', data=df, kind='hex') </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1e1e552c-adc3-4adf-8df2-b8416adbf185.png" style="width:34.83em;height:34.83em;"/></p>
<p style="padding-left: 60px">As you can see in the preceding image, there are some very distinct patterns in the data.</p>
<ol start="3">
<li><span>Re-plot the bivariate distribution, segmenting on the target variable, by running the cell containing the following code:<br/></span></li>
</ol>
<pre><span>      plot_args = dict(shade=True, shade_lowest=False)<br/>      for i, c in zip((0, 1), ('Reds', 'Blues')):<br/>      sns.kdeplot(df.loc[df.left==i, 'satisfaction_level'],<br/>          df.loc[df.left==i, 'last_evaluation'],<br/>          cmap=c, **plot_args) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/64b9bb12-90af-4e78-bf54-a418f9543255.png" style="width:35.42em;height:34.25em;"/></p>
<p style="padding-left: 60px"><span>Now, we can see how the patterns are related to the target variable. For the remainder of this section, we'll try to exploit these patterns to train effective classification models.<br/></span></p>
<ol start="4">
<li><span>Split the data into training and test sets by running the cell containing the following code:<br/></span></li>
</ol>
<pre><span>     from sklearn.model_selection import train_test_split<br/>     features = ['satisfaction_level', 'last_evaluation']<br/></span><span>        X_train, X_test, y_train, y_test = <br/>        train_test_split(</span><span>df[features].values, df['left'].values,<br/>        test_size=0.3, random_state=1) </span></pre>
<p style="padding-left: 60px"><span>Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm, are most effective when the input data is scaled so that all of the features are on the same order. We'll accomplish this with scikit-learn's <kbd>StandardScaler</kbd>.</span></p>
<ol start="5">
<li><span>Load <kbd>StandardScaler</kbd> and create a new instance, as referenced by the scaler variable. Fit the <kbd>scaler</kbd> on the training set and transform it. Then, transform the test set. Run the cell containing the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.preprocessing import StandardScaler<br/>    scaler = StandardScaler()<br/>    X_train_std = scaler.fit_transform(X_train)<br/>    X_test_std = scaler.transform(X_test) </span></pre>
<div class="packt_infobox"><span>An easy mistake to make when doing machine learning is to "fit" the scaler on the whole dataset, when in fact it should only be "fit" to the training data. For example, scaling the data before splitting into training and testing sets is a mistake. We don't want this because the model training should not be influenced in any way by the test data.<br/></span></div>
<ol start="6">
<li>Import the scikit-learn support vector machine class and fit the model on the training data by running the cell containing the following code:</li>
</ol>
<pre style="padding-left: 60px">    from sklearn.svm import<br/>    SVC svm = SVC(kernel='linear', C=1, random_state=1)<br/>    svm.fit(X_train_std, y_train)</pre>
<p style="padding-left: 60px">Then, we train a linear SVM classification model. The C parameter controls the penalty for misclassification, allowing the variance and bias of the model to be controlled.</p>
<ol start="7">
<li><span>Compute the accuracy of this model on unseen data by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.metrics import accuracy_score<br/>    y_pred = svm.predict(X_test_std)<br/>    acc = accuracy_score(y_test, y_pred)<br/>    print('accuracy = {:.1f}%'.format(acc*100))<br/>    &gt;&gt; accuracy = 75.9% </span></pre>
<p style="padding-left: 60px">We predict the targets for our test samples and then use scikit-learn's <kbd>accuracy_ score</kbd> function to determine the accuracy. The result looks promising at ~75%! Not bad for our first model. Recall, though, the target is imbalanced. Let's see how accurate the predictions are for each class.</p>
<ol start="8">
<li><span>Calculate the confusion matrix and then determine the accuracy within each class by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>      from sklearn.metrics import confusion_matrix<br/>      cmat = confusion_matrix(y_test, y_pred)<br/>      scores = cmat.diagonal() / cmat.sum(axis=1) * 100<br/>      print('left = 0 : {:.2f}%'.format(scores[0]))<br/>      print('left = 1 : {:.2f}%'.format(scores[1]))<br/>      &gt;&gt; left = 0 : 100.00%<br/>      &gt;&gt; left = 1 : 0.00% </span></pre>
<p style="padding-left: 60px">It looks like the model is simply classifying every sample as 0, which is clearly not helpful at all. Let's use a contour plot to show the predicted class at each point in the feature space. This is commonly known as the decision-regions plot.</p>
<ol start="9">
<li><span>Plot the decision regions using a helpful function from the <kbd>mlxtend</kbd> library. Run the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>      from mlxtend.plotting import plot_decision_regions<br/>      N_samples = 200<br/>      X, y = X_train_std[:N_samples], y_train[:N_samples]<br/>      plot_decision_regions(X, y, clf=svm) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c998fa17-26e7-4a5a-8e40-4d599eebc52f.png" style="width:32.42em;height:31.33em;"/></p>
<p style="padding-left: 60px">The function plots decision regions along with a set of samples passed as arguments. In order to see the decision regions properly without too many samples obstructing our view, we pass only a 200-sample subset of the test data to the<kbd>plot_ decision_regions</kbd> function. In this case, of course, it does not matter. We see the result is entirely red, indicating every point in the feature space would be classified as 0.</p>
<p style="padding-left: 60px">It shouldn't be surprising that a linear model can't do a good job of describing these nonlinear patterns. Recall earlier we mentioned the kernel trick for using SVM's to classify nonlinear problems. Let's see if doing this can improve the result.</p>
<ol start="10">
<li><span>Print the docstring for scikit-learn's SVM by running the cell containing SVC. Scroll down and check out the parameter descriptions. Notice the <kbd>kernel</kbd> option, which is actually enabled by default as <kbd>rbf</kbd>. Use this kernel option to train a new SVM by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>     svm = SVC(kernel='rbf', C=1, random_state=1)<br/>     svm.fit(X_train_std, y_train)</span></pre>
<ol start="11">
<li>In order to assess this and future model performance more easily, let's define a function called <kbd>check_model_fit</kbd>, which computes various metrics that we can use to compare the models. Run the cell where this function is defied.</li>
</ol>
<p style="padding-left: 60px">Each computation done in this function has already been seen in this example; it simply calculates accuracies and plots the decision regions.</p>
<ol start="12">
<li><span>Show the newly trained kernel-SVM results on the training data by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px">  <span>check_model_fit(svm, X_test_std, y_test) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/82fd597b-ca0e-452a-981b-688c63eadbc0.png" style="width:32.17em;height:44.75em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e509aa55-12cd-47ce-953c-da0043bc4b5a.png" style="width:32.75em;height:31.83em;"/></p>
<p style="padding-left: 60px"><span>The result is much better. Now, we are able to capture some of the non-linear patterns in the data and correctly classify the majority of the employees who have left.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The plot_decision_regions Function</h1>
                </header>
            
            <article>
                
<p>The <kbd>plot_decision_regions</kbd> function is provided by <kbd>mlxtend</kbd>, a Python library developed by Sebastian <em>Raschka</em>. It's worth taking a peek at the source code (which is of course written in Python) to understand how these plots are drawn. It's really not too complicated.</p>
<p>In a Jupyter Notebook, import the function with from <kbd>mlxtend.plotting</kbd> import <kbd>plot_ decision_regions</kbd>, and then pull up the help with <kbd>plot_decision_regions?</kbd> and scroll to the bottom to see the local file path:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e86d69a7-2040-42ae-9d66-8be5df420c35.png" style="width:49.33em;height:11.17em;"/></p>
<p><span>Then, open up the file and check it out! For example, you could run cat in the notebook:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/00b708c7-fe1c-4dd7-9efa-89912901b660.png" style="width:71.25em;height:29.67em;"/></p>
<p>This is okay, but not ideal as there's no color markup for the code. It's better to copy it (so you don't accidentally alter the original) and open it with your favorite text editor.</p>
<p>When drawing attention to the code responsible for mapping the decision regions, we see a contour plot of predictions <kbd>Z</kbd> over an array <kbd>X_predict</kbd> that spans the feature space.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1fbc10b9-e569-4e2c-ab8f-6c81dcc7f82c.png" style="width:38.25em;height:25.83em;"/></p>
<p><span>Let's move on to the next model: k-Nearest Neighbors.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training k-nearest neighbors for our model</h1>
                </header>
            
            <article>
                
<ol>
<li><span>Load the scikit-learn KNN classification model and print the docstring by running the cell containing the following code:<br/></span></li>
</ol>
<pre><span>      from sklearn.neighbors import KNeighborsClassifier<br/>      KNeighborsClassifier?</span></pre>
<p style="padding-left: 60px">The <kbd>n_neighbors</kbd> parameter decides how many samples to use when making a classification. If the weights parameter is set to uniform, then class labels are decided by majority vote. Another useful choice for the weights is distance, where closer samples have a higher weight in the voting. Like most model parameters, the best choice for this depends on the particular dataset.</p>
<ol start="2">
<li><span>Train the KNN classifier with <kbd>n_neighbors=3</kbd>, and then compute the accuracy and decision regions. Run the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    knn = KNeighborsClassifier(n_neighbors=3)<br/>    knn.fit(X_train_std, y_train) <br/> <br/>    check_model_fit(knn, X_test_std, y_test) <br/></span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3a2dc2b3-be30-45cf-938f-143f97d0bdae.png" style="width:30.17em;height:42.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/581d3d92-766f-410c-91b7-9c4d89c603c3.png" style="width:30.75em;height:30.00em;"/></p>
<p style="padding-left: 60px">We see an increase in overall accuracy and a significant improvement for class 1 in particular. However, the decision region plot would indicate we are overfitting the data. This is evident by the hard, "choppy" decision boundary, and small pockets of blue everywhere. We can soften the decision boundary and decrease overfitting by increasing the number of nearest neighbors.</p>
<ol start="3">
<li><span>Train a KNN model with <kbd>n_neighbors=25</kbd> by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    knn = KNeighborsClassifier(n_neighbors=25)<br/>    knn.fit(X_train_std, y_train)<br/>    check_model_fit(knn, X_test_std, y_test) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60bcd33b-7a68-47dd-b3f6-89b6e32e3043.png" style="width:30.17em;height:42.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c5e825a1-6d29-4ddb-977b-78cd61dcce20.png" style="width:30.25em;height:29.42em;"/></p>
<p style="padding-left: 60px"><span>As we can see, the decision boundaries are significantly less choppy, and there are far less pockets of blue. The accuracy for class 1 is slightly less, but we would need to use a more comprehensive method such as k-fold cross validation to decide if<br/>
there's a significant difference between the two models.</span></p>
<p style="padding-left: 60px"><span>Note that increasing <kbd>n_neighbors</kbd> has no effect on training time, as the model is simply memorizing the data. The prediction time, however, will be greatly affected.</span></p>
<div class="packt_infobox">When doing machine learning with real-world data, it's important for the algorithms to run quick enough to serve their purposes. For example, a script to predict tomorrow's weather that takes longer than a day to run is completely useless! Memory is also a consideration that should be taken into account when dealing with substantial amounts of data.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a Random Forest</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Observe how similar it is to train and make predictions on each model, despite them each being so different internally.</span></p>
<ol>
<li><span>Train a Random Forest classification model composed of 50 decision trees, each with a max depth of 5. Run the cell containing the following code:<br/></span></li>
</ol>
<pre><span>       from sklearn.ensemble import RandomForestClassifier<br/>       forest = RandomForestClassifier(n_estimators=50,<br/>       max_depth=5,<br/>       random_state=1)<br/>       forest.fit(X_train, y_train)<br/>       check_model_fit(forest, X_test, y_test) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/14e57a88-7257-4dca-ade2-8f71dbf835dc.png" style="width:25.67em;height:36.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3383eccc-dfd2-4adc-adb3-9a3d11a74b87.png" style="width:26.00em;height:25.00em;"/></p>
<p style="padding-left: 60px">Note the distinctive axes-parallel decision boundaries produced by decision tree machine learning algorithms.</p>
<p style="padding-left: 60px">We can access any of the individual decision trees used to build the Random Forest. These trees are stored in the <kbd>estimators_attribute</kbd> of the model. Let's draw one of these decision trees to get a feel for what's going on. Doing this requires the <strong>graph</strong> viz dependency, which can sometimes be difficult to install.</p>
<ol start="2">
<li><span>Draw one of the decision trees in the Jupyter Notebook by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.tree import export_graphviz<br/>    import graphviz<br/>    dot_data = export_graphviz(<br/>        forest.estimators_[0],<br/>        out_file=None,<br/>        feature_names=features,<br/>        class_names=['no', 'yes'],<br/>        filled=True, rounded=True,<br/>        special_characters=True)<br/>    graph = graphviz.Source(dot_data)<br/>    graph </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6993f559-1315-4153-bf82-10085abc0ac1.png" style="width:41.17em;height:35.08em;"/></p>
<p style="padding-left: 60px">We can see that each path is limited to five nodes as a result of setting <kbd>max_depth=5</kbd>. The orange boxes represent predictions of <kbd>no</kbd> (has not left the company), and the blue boxes represent <kbd>yes</kbd> (has left the company). The shade of each box (light, dark, and so on) indicates the confidence level, which is related to the <kbd>gini</kbd> value.</p>
<p>To summarize, we have accomplished two of the learning objectives in this section:</p>
<ul>
<li>We gained a qualitative understanding of support vector machines (SVMs), k-Nearest Neighbor classifiers (kNNs), and Random Forest</li>
<li>We are now able to train a variety of models using scikit-learn and Jupyter Notebooks so that we can confidently build and compare predictive models</li>
</ul>
<p>In particular, we used the preprocessed data from our employee retention problem to train classification models to predict whether an employee has left the company or not. For the purposes of keeping things simple and focusing on the algorithms, we built models to predict this given only two features: the satisfaction level and last evaluation value. This two-dimensional feature space also allowed us to visualize the decision boundaries and identify what overfitting looks like.</p>
<p>In the following section, we will introduce two important topics in machine learning: k-fold cross-validation and validation curves</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assessing Models with k-Fold Cross-Validation and Validation Curves</h1>
                </header>
            
            <article>
                
<p>Thus far, we have trained models on a subset of the data and then assessed performance on the unseen portion, called the test set. This is good practice because the model performance on training data is not a good indicator of its effectiveness as a predictor. It's very easy to increase accuracy on a training dataset by overfitting a model, which can result in poorer performance on unseen data.</p>
<p>That said, simply training models on data split in this way is not good enough. There is a natural variance in data that causes accuracies to be different (if even slightly) depending on the training and test splits. Furthermore, using only one training/test split to compare models can introduce bias towards certain models and lead to overfitting.</p>
<p><strong>k-fold</strong> <strong>cross validation</strong> offers a solution to this problem and allows the variance to be accounted for by way of an error estimate on each accuracy calculation. This, in turn, naturally leads to the use of validation curves for tuning model parameters. These plot the accuracy as a function of a hyper parameter such as the number of decision trees used in a Random Forest or the max depth.</p>
<div class="packt_infobox">This is our fist time using the term hyperparameter. It references a parameter that is defined when initializing a model, for example, the C parameter of the SVM. This is in contradistinction to a parameter of the trained model, such as the equation of the decision boundary hyperplane for a trained SVM.</div>
<div><span>The method is illustrated in the following diagram, where we see how the k-folds can be selected from the dataset:<br/></span></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f874d66c-9bf3-4666-8a1d-862230c92a86.png" style="width:35.58em;height:17.42em;"/></div>
<p>The k-fold cross validation algorithm goes as follows:</p>
<ol>
<li>Split data into k "folds" of near-equal size.</li>
<li>Test and train k models on different fold combinations. Each model will include <em>k - 1</em> folds of training data and the left-out fold is used for testing. In this method, each fold ends up being used as the validation data exactly once.</li>
<li><span>Calculate the model accuracy by taking the mean of the k values. The standard deviation is also calculated to provide error bars on the value.<br/></span></li>
</ol>
<p>It's standard to set <em>k = 10</em>, but smaller values for k should be considered if using a big data set.</p>
<p>This validation method can be used to reliably compare model performance with different hyperparameters (for example, the C parameter for an SVM or the number of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely different models.</p>
<p>Once the best model has been identified, it should be re-trained on the entirety of the dataset before being used to predict actual classifications.</p>
<p>When implementing this with scikit-learn, it's common to use a slightly improved variation of the normal k-fold algorithm instead. This is called stratified k-fold. The improvement is that stratified k-fold cross validation maintains roughly even class label populations in the folds. As you can imagine, this reduces the overall variance in the models and decreases the likelihood of highly unbalanced models causing bias.</p>
<p>Validation curves are plots of a training and validation metric as a function of some model parameter. They allow to us to make good model parameter selections. In this book, we will use the accuracy score as our metric for these plots.</p>
<div class="packt_infobox">The documentation for plot validation curves is available here:<a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html"> http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html</a>.</div>
<div><span>Consider this validation curve, where the accuracy score is plotted as a function of the gamma SVM parameter:<br/></span></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c5981de6-b9f2-4114-8744-ea06e9a7a8ed.png" style="width:35.75em;height:26.92em;"/></div>
<p>Starting on the left side of the plot, we can see that both sets of data are agreeing on the score, which is good. However, the score is also quite low compared to other gamma values, so therefore we say the model is underfitting the data. Increasing the gamma, we can see a point where the error bars of these two lines no longer overlap. From this point on, we see the classifier overfitting the data as the models behave increasingly well on the training set compared to the validation set. The optimal value for the gamma parameter can be found by looking for a high validation score with overlapping error bars on the two lines.</p>
<p>Keep in mind that a learning curve for some parameter is only valid while the other parameters remain constant. For example, if training the SVM in this plot, we could decide to pick gamma on the order of 10-4. However, we may want to optimize the C parameter as well. With a different value for C, the preceding plot would be different and our selection for gamma may no longer be optimal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using k-fold cross validation and validation curves in Python with scikit-learn</h1>
                </header>
            
            <article>
                
<ol>
<li>If you've not already done so, start the <kbd>NotebookApp</kbd> and open the <kbd>chapter-2- workbook.ipynb file</kbd>. Scroll down to Subtopic <kbd>K-fold cross-validation</kbd> and <kbd>validation curves</kbd>.</li>
</ol>
<p style="padding-left: 60px">The training data should already be in the notebook's memory, but let's reload it as a reminder of what exactly we're working with.</p>
<ol start="2">
<li><span>Load the data and select the <kbd>satisfaction_level</kbd> and <kbd>last_evaluation</kbd> features for the training/validation set. We will not use the train-test split this time because we are going to use k-fold validation instead. Run the cell containing the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>      df = pd.read_csv('../data/hr-analytics/hr_data_processed.csv')<br/>      features = ['satisfaction_level', 'last_evaluation']<br/>      X = df[features].values<br/>      y = df.left.values </span></pre>
<ol start="3">
<li><span>Instantiate a Random Forest model by running the cell containing the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px">   <span>clf = RandomForestClassifier(n_estimators=100, max_depth=5) </span></pre>
<ol start="4">
<li>To train the model with stratified k-fold cross validation, we'll use the <kbd>model_ selection.cross_val_score function</kbd>.</li>
</ol>
<p style="padding-left: 60px">Train 10 variations of our model <kbd>clf</kbd> using stratified k-fold validation. Note that scikit-learn's c<kbd>ross_val_score</kbd> does this type of validation by default. Run the cell containing the following code:</p>
<pre style="padding-left: 60px"><span>    from sklearn.model_selection import cross_val_score<br/>    np.random.seed(1)<br/>    scores = cross_val_score(<br/>        estimator=clf,<br/>        X=X,<br/>        y=y,<br/>        cv=10)<br/>    print('accuracy = {:.3f} +/- {:.3f}'.format(scores.mean(), scores.<br/>    std()))<br/>    &gt;&gt; accuracy = 0.923 +/- 0.005 </span></pre>
<p style="padding-left: 60px">Note how we use <kbd>np.random.seed</kbd> to set the seed for the random number generator, therefore ensuring reproducibility with respect to the randomly selected samples for each fold and decision tree in the Random Forest.</p>
<ol start="5">
<li>With this method, we calculate the accuracy as the average of each fold. We can also see the individual accuracies for each fold by printing scores. To see these, run <kbd>print(scores)</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>    &gt;&gt; array([ 0.93404397, 0.91533333, 0.92266667, 0.91866667,<br/>        0.92133333,<br/>        0.92866667, 0.91933333, 0.92 , 0.92795197, 0.92128085]) </span></pre>
<p style="padding-left: 60px">Using <kbd>cross_val_score</kbd> is very convenient, but it doesn't tell us about the accuracies within each class. We can do this manually with the <kbd>model_selection</kbd>. <kbd>StratifiedKFold</kbd> class. This class takes the number of folds as an initialization parameter, then the split method is used to build randomly sampled "masks" for the data. A mask is simply an array containing indexes of items in another array, where the items can then be returned by doing this: <kbd>data[mask]</kbd> .</p>
<ol start="6">
<li>Define a custom class for calculating k-fold cross validation class accuracies. Run the cell containing the following code:</li>
</ol>
<pre><span>      from sklearn.model_selection import StratifiedKFold<br/>      …<br/>          print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))<br/>      return class_accuracy </span></pre>
<ol start="7">
<li><span>We can then calculate the class accuracies with code that's very similar to step 4. Do this by running the cell containing the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.model_selection import cross_val_score<br/>    np.random.seed(1)<br/>    …<br/>    &gt;&gt; fold: 10 accuracy: [ 0.98861646 0.70588235]<br/>    &gt;&gt; accuracy = [ 0.98722476 0.71715647] +/- [ 0.00330026 0.02326823] </span></pre>
<div style="padding-left: 60px"><span>Now we can see the class accuracies for each fold! Pretty neat, right?<br/></span></div>
<ol start="8">
<li>Let's move on to show how a validation curve can be calculated using <kbd>model_ selection.validation_curve</kbd>. This function uses stratified k-fold cross validation to train models for various values of a given parameter.</li>
</ol>
<p style="padding-left: 60px">Do the calculations required to plot a validation curve by training Random Forests over a range of max_depth values. Run the cell containing the following code:</p>
<pre style="padding-left: 60px">    from sklearn.model_selection import validation_curve<br/><span><br/>    clf = RandomForestClassifier(n_estimators=10)<br/>    max_depths = np.arange(3, 16, 3)<br/>    train_scores, test_scores = validation_curve(<br/>        estimator=clf,<br/>        X=X,<br/>        y=y,<br/>        param_name='max_depth',<br/>        param_range=max_depths,<br/>        cv=10);</span></pre>
<p style="padding-left: 60px"><span>This will return arrays with the cross validation scores for each model, where the models have different max depths. In order to visualize the results, we'll leverage a function provided in the scikit-learn documentation.<br/></span></p>
<ol start="9">
<li>Run the cell in which <kbd>plot_validation_curve</kbd> is defined. Then, run the cell containing the following code to draw the plot:</li>
</ol>
<pre>    plot_validation_curve(train_scores, test_scores, max_depths,<br/>    xlabel='max_depth')</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/47959f19-1125-4c31-bae3-2c3c929d2941.png" style="width:29.50em;height:28.42em;"/></p>
<p style="padding-left: 60px">Recall how setting the max depth for decision trees limits the amount of overfitting? This is reflected in the validation curve, where we see overfitting taking place for large max depth values to the right. A good value for <kbd>max_depth</kbd> appears to be <kbd>6</kbd>, where we see the training and validation accuracies in agreement. When <kbd>max_depth</kbd> is equal to <kbd>3</kbd>, we see the model underfitting the data as training and validation accuracies are lower.</p>
<p>To summarize, we have learned and implemented two important techniques for building reliable predictive models. The fist such technique was k-fold cross-validation, which is used to split the data into various train/test batches and generate a set accuracy. From this set, we then calculated the average accuracy and the standard deviation as a measure of the error. This is important so that we have a gauge of the variability of our model and we can produce trustworthy accuracy.</p>
<p>We also learned about another such technique to ensure we have trustworthy results: validation curves. These allow us to visualize when our model is overfitting based on comparing training and validation accuracies. By plotting the curve over a range of our selected hyperparameter, we are able to identify its optimal value.</p>
<p>In the final section of this chapter, we take everything we have learned so far and put it together in order to build our final predictive model for the employee retention problem. We seek to improve the accuracy, compared to the models trained thus far, by including all of the features from the dataset in our model. We'll see now-familiar topics such as k-fold cross-validation and validation curves, but we'll also introduce something new: dimensionality reduction techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality Reduction Techniques</h1>
                </header>
            
            <article>
                
<p>Dimensionality reduction can simply involve removing unimportant features from the training data, but more exotic methods exist, such as <strong>Principal Component Analysis (PCA)</strong> and <strong>Linear Discriminant Analysis (LDA)</strong>. These techniques allow for data compression, where the most important information from a large group of features can be encoded in just a few features.</p>
<p>In this subtopic, we'll focus on PCA. This technique transforms the data by projecting it into a new subspace of orthogonal "principal components," where the components with the highest eigenvalues encode the most information for training the model. Then, we can simply select a few of these principal components in place of the original high-dimensional dataset. For example, PCA could be used to encode the information from every pixel in an image. In this case, the original feature space would have dimensions equal to the number of pixels in the image. This high-dimensional space could then be reduced with PCA, where the majority of useful information for training predictive models might be reduced to just a few dimensions. Not only does this save time when training and using models, it allows them to perform better by removing noise in the dataset.</p>
<p>Like the models we've seen, it's not necessary to have a detailed understanding of PCA in order to leverage the benefits. However, we'll dig into the technical details of PCA just a bit further so that we can conceptualize it better. The key insight of PCA is to identify patterns between features based on correlations, so the PCA algorithm calculates the co variance matrix and then decomposes this into eigen vectors and eigenvalues. The vectors are then used to transform the data into a new subspace, from which a filed number of principal components can be selected.</p>
<p>In the following section, we'll see an example of how PCA can be used to improve our Random Forest model for the employee retention problem we have been working on. This will be done after training a classification model on the full feature space, to see how our accuracy is affected by dimensionality reduction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a predictive model for the employee retention problem</h1>
                </header>
            
            <article>
                
<p>We have already spent considerable effort planning a machine learning strategy, preprocessing the data, and building predictive models for the employee retention problem. Recall that our business objective was to help the client prevent employees from leaving. The strategy we decided upon was to build a classification model that would predict the probability of employees leaving. This way, the company can assess the likelihood of current employees leaving and take action to prevent it.</p>
<p>Given our strategy, we can summarize the type of predictive modeling we are doing as follows:</p>
<ul>
<li><span>Supervised learning on labeled training data</span></li>
<li><span>Classification problems with two class labels (binary)</span></li>
</ul>
<p>In particular, we are training models to determine whether an employee has left the company, given a set of continuous and categorical features. After preparing the data for machine learning in Activity, <em>Preparing to Train a Predictive Model for the Employee-Retention Problem</em>, we went on to implement SVM, k-Nearest Neighbors, and Random Forest algorithms using just two features. These models were able to make predictions with over 90% overall accuracy. When looking at the specific class accuracies, however, we found that employees who had left (<kbd>class-label 1</kbd>) could only be predicted with 70-80% accuracy. Let's see how much this can be improved by utilizing the full feature space.</p>
<ol>
<li>In the <kbd>chapter-2-workbook.ipynb notebook</kbd>, scroll down to the code for this section. We should already have the preprocessed data loaded from the previous sections, but this can be done again, if desired, by executing <kbd>df = pd.read_ csv('../data/hr-analytics/hr_data_processed.csv')</kbd> . Then, print the DataFrame columns with <kbd>print(df.columns)</kbd> .</li>
<li><span>Define list of all the features by copy and pasting the output from <kbd>df.columns</kbd> into a new list (making sure to remove the target variable left). Then, define <kbd>X</kbd> and <kbd>Y</kbd> as we have done before. This goes as follows:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    features = ['satisfaction_level', 'last_evaluation', 'number_project',<br/>    'average_montly_hours', 'time_spend_company', 'work_accident',<br/>        …<br/>        X = df[features].values<br/>        y = df.left.values </span></pre>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Looking at the feature names, recall what the values look like for each one. Scroll up to the set of histograms we made in the first activity to help jog your memory. The first two features are continuous; these are what we used for training models in the previous two exercises. After that, we have a few discrete features, such as <kbd>number_ project</kbd> and <kbd>time_spend_company</kbd>, followed by some binary fields such as <kbd>work_ accident</kbd> and <kbd>promotion_last_5years</kbd>. We also have a bunch of binary features, such as <kbd>department_IT</kbd> and <kbd>department_accounting</kbd>, which were created by one hot encoding.</p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Given a mix of features like this, Random Forests are a very attractive type of model. For one thing, they're compatible with feature sets composed of both continuous and categorical data, but this is not particularly special; for instance, an SVM can be trained on mixed feature types as well (given proper preprocessing).</p>
<div class="packt_infobox">If you're interested in training an SVM or k-Nearest Neighbors classifier on mixed-type input features, you can use the data-scaling prescription from this StackExchange answer: <a href="https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm">https://stats.stackexchange. com/questions/82923/mixing-continuous-and-binary-datawith-linear-svm/83086#83086</a>.</div>
<p class="mce-root"/>
<p style="padding-left: 60px">A simple approach would be to preprocess data as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Standardize continuous variables</li>
<li>One-hot-encode categorical features</li>
<li>Shift binary values to <kbd>-1</kbd> and 1 instead of <kbd>0</kbd> and <kbd>1</kbd></li>
<li>Then, the mixed-feature data could be used to train a variety of classification models</li>
</ul>
</li>
</ul>
<ol start="3">
<li>We need to figure out the best parameters for our Random Forest model. Let's start by tuning the<kbd>max_depth</kbd> hyperparameter using a validation curve. Calculate the training and validation accuracies by running the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>    %%time<br/>    np.random.seed(1)<br/>    clf = RandomForestClassifier(n_estimators=20)<br/>    max_depths = [3, 4, 5, 6, 7,<br/>    9, 12, 15, 18, 21]<br/>    train_scores, test_scores = validation_curve(<br/>    estimator=clf,<br/>        X=X,<br/>        y=y,<br/>    param_name='max_depth',<br/>    param_range=max_depths,<br/>    cv=5); </span></pre>
<p style="padding-left: 60px">We are testing 10 models with k-fold cross validation. By setting <kbd>k = 5</kbd>, we produce five estimates of the accuracy for each model, from which we extract the mean and standard deviation to plot in the validation curve. In total, we train 50 models, and since <kbd>n_estimators</kbd> is set to 20, we are training a total of 1,000 decision trees! All in roughly 10 seconds!</p>
<ol start="4">
<li>Plot the validation curve using our custom plot_validation_curve function from the last exercise. Run the following code:</li>
</ol>
<pre style="padding-left: 60px">    plot_validation_curve(train_scores, <span>test_scores,<br/>    max_depths, xlabel='max_depth'); </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d53046ce-fa55-44c4-a852-eab90b5f9500.png" style="width:32.00em;height:30.75em;"/></p>
<p style="padding-left: 60px">For small max depths, we see the model underfitting the data. Total accuracies dramatically increase by allowing the decision trees to be deeper and encode more complicated patterns in the data. As the max depth is increased further and the accuracy approaches 100%, we find the model overfits the data, causing the training and validation accuracies to grow apart. Based on this figure, let's select a <kbd>max_ depth</kbd> of 6 for our model.</p>
<p style="padding-left: 60px">We should really do the same for <kbd>n_estimators</kbd>, but in the spirit of saving time, we'll skip it. You are welcome to plot it on your own; you should find agreement between training and validation sets for a large range of values. Usually, it's better to use more decision tree estimators in the Random Forest, but this comes at the cost of increased training times. We'll use 200 estimators to train our model.</p>
<ol start="5">
<li>Use <kbd>cross_val_class_score</kbd>, the k-fold cross validation by class function we created earlier, to test the selected model, a Random Forest with <kbd>max_depth = 6</kbd> and <kbd>n_estimators = 200</kbd>:</li>
</ol>
<pre style="padding-left: 60px">     np.random.seed(1) <br/>     clf = RandomForestClassifier(n_estimators=200, max_depth=6) <br/>     scores = cross_val_class_score(clf, X, y)<br/>     print('accuracy = {} +/- {}'\ .format(scores.mean(axis=0),<br/>     scores.std(axis=0)))<br/>     &gt;&gt; accuracy = [ 0.99553722 0.85577359] +/- [ 0.00172575 0.02614334] </pre>
<p style="padding-left: 60px">The accuracies are way higher now that we're using the full feature set, compared to before when we only had the two continuous features!</p>
<ol start="6">
<li>Visualize the accuracies with a boxplot by running the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>    fig = plt.figure(figsize=(5, 7))<br/>    sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),<br/>    palette=sns.color_palette('Set1'))<br/>    plt.xlabel('Left')<br/>    plt.ylabel('Accuracy') </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png" style="width:25.33em;height:31.58em;"/></p>
<p style="padding-left: 60px"><span>Random Forests can provide an estimate of the feature performances.<br/></span></p>
<div class="packt_infobox"><span>The feature importance in scikit-learn is calculated based on how the node impurity changes with respect to each feature. For a more detailed explanation, take a look at the following Stack Overflow thread about how feature importance is determined in Random Forest Classifier:</span> <a href="https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined">https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined</a> .</div>
<ol start="7">
<li><span>Plot the feature importance, as stored in the attribute <kbd>feature_importances_</kbd>, by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    pd.Series(clf.feature_importances_, name='Feature importance',<br/>        index=df[features].columns)\<br/>        .sort_values()\<br/>        .plot.barh()<br/>    plt.xlabel('Feature importance')</span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a84fa4bd-e710-4a80-bc11-a6081cee8370.png" style="width:48.58em;height:40.50em;"/></p>
<p style="padding-left: 60px">It doesn't look like we're getting much in the way of useful contribution from the one-hot encoded variables: department and salary. Also, the p<kbd>romotion_ last_5years</kbd> and <kbd>work_accident</kbd> features don't appear to be very useful.</p>
<p style="padding-left: 60px"><span>Let's use Principal Component Analysis (PCA) to condense all of these weak features into just a few principal components.<br/></span></p>
<ol start="8">
<li><span>Import the PCA class from scikit-learn and transform the features. Run the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.decomposition import PCA<br/>    pca_features = \<br/>    …<br/>    pca = PCA(n_components=3)<br/>    X_pca = pca.fit_transform(X_reduce) </span></pre>
<ol start="9">
<li><span>Look at the string representation of <kbd>X_pca</kbd> by typing it alone and executing the cell:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    &gt;&gt; array([[-0.67733089, 0.75837169, -0.10493685],<br/>    &gt;&gt; [ 0.73616575, 0.77155888, -0.11046422],<br/>    &gt;&gt; [ 0.73616575, 0.77155888, -0.11046422],<br/>        &gt;&gt; ...,<br/>    &gt;&gt; [-0.67157059, -0.3337546 , 0.70975452],<br/>    &gt;&gt; [-0.67157059, -0.3337546 , 0.70975452],<br/>    &gt;&gt; [-0.67157059, -0.3337546 , 0.70975452]]) </span></pre>
<p style="padding-left: 60px"><span>Since we asked for the top three components, we get three vectors returned.<br/></span></p>
<ol start="10">
<li><span>Add the new features to our DataFrame with the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    df['first_principle_component'] = X_pca.T[0]<br/>    df['second_principle_component'] = X_pca.T[1]<br/>    df['third_principle_component'] = X_pca.T[2] </span></pre>
<p style="padding-left: 60px"><span>Select our reduced-dimension feature set to train a new Random Forest with. Run the following code:</span></p>
<pre style="padding-left: 60px"><span>    features = ['satisfaction_level', 'number_project', 'time_spend_<br/>        company',<br/>    'average_montly_hours', 'last_evaluation',<br/>    'first_principle_component',<br/>    'second_principle_component',<br/>    'third_principle_component']<br/>    X = df[features].values<br/>    y = df.left.values </span></pre>
<ol start="11">
<li><span>Assess the new model's accuracy with k-fold cross validation. This can be done by running the same code as before, where X now points to different features. The code is as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    np.random.seed(1)<br/>    clf = RandomForestClassifier(n_estimators=200, max_depth=6)<br/>    scores = cross_val_class_score(clf, X, y)<br/>    print('accuracy = {} +/- {}'\.format(scores.mean(axis=0), <br/>    scores.std(axis=0)))<br/>    &gt;&gt; accuracy = [ 0.99562463 0.90618594] +/- [ 0.00166047 0.01363927] </span></pre>
<ol start="12">
<li><span>Visualize the result in the same way as before, using a box plot. The code is as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    fig = plt.figure(figsize=(5, 7))<br/>    sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),<br/>    palette=sns.color_palette('Set1'))<br/>    plt.xlabel('Left')<br/>    plt.ylabel('Accuracy') </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png" style="width:24.58em;height:30.58em;"/></p>
<p style="padding-left: 60px">Comparing this to the previous result, we find an improvement in the class 1 accuracy! Now, the majority of the validation sets return an accuracy greater than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6% prior to dimensionality reduction!</p>
<p style="padding-left: 60px">Let's select this as our final model. We'll need to re-train it on the full sample space before using it in production.</p>
<ol start="13">
<li><span>Train the final predictive model by running the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    np.random.seed(1)<br/>    clf = RandomForestClassifier(n_estimators=200, max_depth=6)<br/>    clf.fit(X, y)</span></pre>
<ol start="14">
<li>Save the trained model to a binary file using <kbd>externals.joblib.dump</kbd>. Run the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>    from sklearn.externals import joblib<br/>    joblib.dump(clf, 'random-forest-trained.pkl') </span></pre>
<ol start="15">
<li>Check that it's saved into the working directory, for example, by running: <kbd>!ls *.pkl</kbd>. Then, test that we can load the model from the file by running the following code:</li>
</ol>
<pre style="padding-left: 60px">     <span>clf = joblib.load('random-forest-trained.pkl') </span></pre>
<p style="padding-left: 60px"><span>Congratulations! We've trained the final predictive model! Now, let's see an example of how it can be used to provide business insights for the client.<br/></span></p>
<p style="padding-left: 60px">Say we have a particular employee, who we'll call Sandra. Management has noticed she is working very hard and reported low job satisfaction in a recent survey. They would therefore like to know how likely it is that she will quit.</p>
<p style="padding-left: 60px">For the sake of simplicity, let's take her feature values as a sample from the training set (but pretend that this is unseen data instead).</p>
<ol start="16">
<li><span>List the feature values for Sandra by running the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    sandra = df.iloc[573]<br/>    X = sandra[features]<br/>        X<br/>    &gt;&gt; satisfaction_level 0.360000<br/>    &gt;&gt; number_project 2.000000<br/>    &gt;&gt; time_spend_company 3.000000<br/>    &gt;&gt; average_montly_hours 148.000000<br/>    &gt;&gt; last_evaluation 0.470000<br/>    &gt;&gt; first_principle_component 0.742801<br/>    &gt;&gt; second_principle_component -0.514568<br/>    &gt;&gt; third_principle_component -0.677421</span></pre>
<p style="padding-left: 60px"><span>The next step is to ask the model which group it thinks she should be in.</span></p>
<ol start="17">
<li><span>Predict the class label for Sandra by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>    clf.predict([X])<br/>    &gt;&gt; array([1]) </span></pre>
<p style="padding-left: 60px"><span>The model classifies her as having already left the company; not a good sign! We can take this a step further and calculate the probabilities of each class label.<br/></span></p>
<ol start="18">
<li><span>Use <kbd>clf.predict_proba</kbd> to predict the probability of our model predicting that Sandra has quit. Run the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>    clf.predict_proba([X])</span><br/><span>    &gt;&gt; array([[ 0.06576239, 0.93423761]])</span></pre>
<p style="padding-left: 60px"><span>We see the model predicting that she has quit with 93% accuracy.Since this is clearly a red flag for management, they decide on a plan to reduce her number of monthly hours to 100 and the time spent at the company to 1.<br/></span></p>
<ol start="19">
<li><span>Calculate the new probabilities with Sandra's newly planned metrics. Run the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>   X.average_montly_hours = 100<br/>   X.time_spend_company = 1<br/>     clf.predict_proba([X])<br/>     &gt;&gt; array([[ 0.61070329, 0.38929671]])</span></pre>
<p style="padding-left: 60px"><span>Excellent! We can now see that the model returns a mere 38% likelihood that she has quit! Instead, it now predicts she will not have left the company.<br/></span></p>
<p><span>Our model has allowed management to make a data-driven decision. By reducing her amount of time with the company by this particular amount, the model tells us that she will most likely remain an employee at the company!<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how predictive models can be trained in Jupyter Notebooks.</p>
<p>To begin with, we talked about how to plan a machine learning strategy. We thought about how to design a plan that can lead to actionable business insights and stressed the importance of using the data to help set realistic business goals. We also explained machine learning terminology such as supervised learning, unsupervised learning, classification, and regression.</p>
<p>Next, we discussed methods for preprocessing data using scikit-learn and pandas. This included lengthy discussions and examples of a surprisingly time-consuming part of machine learning: dealing with missing data.</p>
<p>In the latter half of the chapter, we trained predictive classification models for our binary problem, comparing how decision boundaries are drawn for various models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed how validation curves can be used to make good parameter choices and how dimensionality reduction can improve model performance. Finally, at the end of our activity, we explored how the final model can be used in practice to make data-driven decisions.</p>


            </article>

            
        </section>
    </body></html>