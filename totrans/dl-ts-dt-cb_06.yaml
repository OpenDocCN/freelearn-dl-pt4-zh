- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Deep Learning Architectures for Time Series Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we’ve learned how to create forecasting models using different
    types of neural networks but, so far, we’ve worked with basic architectures such
    as feedforward neural networks or LSTMs. This chapter describes how to build forecasting
    models with state-of-the-art approaches such as DeepAR or Temporal Fusion Transformers.
    These have been developed by tech giants such as Google and Amazon and are available
    in different Python libraries. These advanced deep learning architectures are
    designed to tackle different types of forecasting problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable forecasting with N-BEATS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the learning rate with PyTorch Forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with GluonTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a DeepAR model with GluonTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Transformer with NeuralForecast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Temporal Fusion Transformer with GluonTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an Informer model with NeuralForecast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing different Transformers with NeuralForecast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to train state-of-the-art deep learning
    forecasting models.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy` (1.23.5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` (1.5.3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn` (1.2.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sktime` (0.24.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` (2.0.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch-forecasting` (1.0.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch-lightning` (2.1.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gluonts` (0.13.5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neuralforecast` (1.6.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install these libraries in one go using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable forecasting with N-BEATS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe introduces **Neural Basis Expansion Analysis for Interpretable Time
    Series Forecasting** (**N-BEATS**), a deep learning method for forecasting problems.
    We’ll show you how to train N-BEATS using PyTorch Forecasting and interpret its
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'N-BEATS is particularly designed for problems involving several univariate
    time series. So, we’ll use the dataset introduced in the previous chapter (check,
    for example, the *Preparing multiple time series for a global* *model* recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our goal is to forecast the next seven values (`HORIZON`) of a time series based
    on the past seven lags (`N_LAGS`).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create the training, validation, and testing datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by calling the `setup()` method in the `GlobalDataModule` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'N-BEATS is available off-the-shelf in PyTorch Forecasting. You can define a
    model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create an `NBeats` instance using the `from_dataset()` method in the preceding
    code. The following parameters need to be defined:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`dataset`: The `TimeSeriesDataSet` instance that contains the training set.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stack_types`: The mode you want to run N-BEATS on. A `trend` and `seasonality`
    type of stack enables the model to be interpretable, while a `[''generic'']` setup
    is usually more accurate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_blocks`: A block is the cornerstone of the N-BEATS model. It contains
    a set of fully connected layers that model the time series.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_block_layers`: The number of fully connected layers in each block.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`widths`: The width of the fully connected layers in each block.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sharing`: A Boolean parameter that denotes whether the weights are shared
    blocks per stack. In the interpretable mode, this parameter should be set to `True`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backcast_loss_ratio`: The relevance of the backcast loss in the model. Backcasting
    (predicting the input sample) is an important mechanism in the training of N-BEATS.
    This parameter balances the loss of the backcast with the loss of the forecast.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After creating the model, you can pass it on to a PyTorch Lightning `Trainer`
    to train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also include an early stopping callback to guide the training process. The
    model is trained using the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We pass the training data loader to train the model, and the validation data
    loader for early stopping.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After fitting a model, we can evaluate its testing performance and use it to
    make predictions. Before that, we need to load the model from the saved checkpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can get the forecasts and respective true values from the test set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We estimate the forecasting performance as the average absolute difference
    between these two quantities (that is, the mean absolute error):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Depending on your device, you may need to convert the `predictions` object to
    a PyTorch `tensor` object using `predictions.cpu()` before computing the difference
    specified in the preceding code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The workflow for forecasting new instances is also made quite simple by the
    data module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Essentially, the data module gets the latest observations and passes them to
    the model, which makes the forecasts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'One of the most interesting aspects of N-BEATS is its interpretability components.
    These can be valuable for inspecting the forecasts, and the driver behind them:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can break down the forecasts into different components and plot them using
    the `plot_interpretation()` method. To do that, we need to get the raw forecasts
    beforehand as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code, we call the plot for the first instance of the test
    set (`idx=0`). Here’s what the plot looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Breaking down the N-BEATS forecasts into different parts](img/B21145_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Breaking down the N-BEATS forecasts into different parts'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows the `trend` and `seasonality` components of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'N-BEATS is based on two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: A double stack of residual connections that involve forecasting and backcasting.
    Backcasting, in the context of N-BEATS, refers to reconstructing a time series’s
    past values. It helps the model learn better data representations by forcing it
    to understand the time series structure in both directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep stack of densely connected layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This combination leads to a model with both high forecasting accuracy and interpretability
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for training, evaluating, and using the model follows the framework
    provided by PyTorch Lightning. The data preparation logic is developed in the
    data module component, specifically within the `setup()` function. The modeling
    stage is created in two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you define the N-BEATS model architecture. In this example, we use the
    `from_dataset()` method to create an `NBeats` instance based on the input data
    directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the training process logic is defined in the `Trainer` instance, including
    any callback you might need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some callbacks, such as early stopping, save the best version of the model in
    a local file, which you can load after training.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the interpretation step, carried out with the `plot_interpretation`
    part, is a special feature of N-BEATS that helps practitioners understand the
    predictions made by the forecasting model. This can also aid in understanding
    the conditions in which the model is not applicable in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'N-BEATS is an important model to have in your forecasting arsenal. For example,
    in the M5 forecasting competition, which featured a set of demand time series,
    this model was used in many of the best solutions. You can see more details here:
    [https://www.sciencedirect.com/science/article/pii/S0169207021001874](https://www.sciencedirect.com/science/article/pii/S0169207021001874).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few things you can do to maximize the potential of N-BEATS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the PyTorch Forecasting library documentation to get a better
    sense of how to select the values of each parameter: [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nbeats.NBeats.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another interesting method is NHiTS, which you can read about at the following
    link: [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.nhits.NHiTS.html#pytorch_forecasting.models.nhits.NHiTS).
    Its implementation from the PyTorch Forecasting library follows a similar logic
    to N-BEATS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As mentioned before, N-BEATS was developed to handle datasets involving several
    univariate time series. Yet, it was extended to handle exogenous variables by
    the N-BEATSx method, which is available in the `neuralforecast` library: [https://nixtla.github.io/neuralforecast/models.nbeatsx.html](https://nixtla.github.io/neuralforecast/models.nbeatsx.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regarding interpretability, there are two other approaches you can take besides
    N-BEATS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use model-agnostic explainers such as TimeShap: [https://github.com/feedzai/timeshap](https://github.com/feedzai/timeshap).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use a **Temporal Fusion Transformer** (**TFT**) deep learning model, which
    also contains special interpretability operations. You can check an example at
    the following link: [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Interpret-model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the learning rate with PyTorch Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we show how to optimize the learning rate of a model based on
    PyTorch Forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The learning rate is a cornerstone parameter of all deep learning methods.
    As the name implies, it controls how quickly the learning process of the network
    is. In this recipe, we’ll use the same setup as the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We’ll also use N-BEATS as an example. However, the process is identical for
    all models based on PyTorch Forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimization of the learning rate can be carried out using the `Tuner`
    class from PyTorch Lightning. Here is an example with N-BEATS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we define a `Tuner` instance as a wrapper of a `Trainer`
    object. We also define an `NBeats` model as in the previous section. Then, we
    use the `lr_optim()` method to optimize the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After this process, we can check which learning rate value is recommended and
    also inspect the results across the different tested values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the results in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Learning rate optimization with PyTorch Forecasting](img/B21145_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Learning rate optimization with PyTorch Forecasting'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the suggested learning rate is about **0.05**.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lr_find()` method from PyTorch Lightning works by testing different learning
    rate values and selecting one that minimizes the loss of the model. This method
    uses the training and validation data loaders to this effect.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to select a sensible value for the learning rate because different
    values can lead to models with different performances. A large learning rate converges
    faster but to a sub-optimal solution. However, a small learning rate can take
    a prohibitively long time to converge.
  prefs: []
  type: TYPE_NORMAL
- en: After the optimization is done, you can create a model using the selected learning
    rate as we did in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about how to get the most out of models such as N-BEATS
    in the *Tutorials* section of PyTorch Forecasting, which is available at the following
    link: [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with GluonTS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GluonTS is a flexible and extensible toolkit for probabilistic time series modeling
    using PyTorch. The toolkit provides state-of-the-art deep learning architectures
    specifically designed for time series tasks and an array of utilities for time
    series data processing, model evaluation, and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: The main objective of this section is to introduce the essential components
    of the `gluonts` library, emphasizing its core functionalities, adaptability,
    and user-friendliness.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin our journey, ensure that `gluonts` is installed as well as its backend
    dependency, `pytorch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With the installations complete, we can now dive into the capabilities of `gluonts`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by accessing a sample dataset provided by the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will load the `nn5_daily_without_missing` dataset, one of the datasets
    that `gluonts` offers for experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its characteristics can be inspected after loading a dataset using the `get_dataset()`
    function. Each `dataset` object contains metadata that offers insights into the
    time series frequency, associated features, and other relevant attributes. You
    can learn a bit more about the dataset by checking the metadata as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To enhance time series data, `gluonts` provides a list of transformers. For
    instance, the `AddAgeFeature` data transformer adds an `age` feature to the dataset,
    representing the lifespan of each time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Data intended for training in `gluonts` is commonly denoted as a collection
    of dictionaries, each representing a time series accompanied by potential features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the fundamental models in `gluonts` is the `SimpleFeedForwardEstimator`
    model. Here’s its setup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the estimator is initialized by determining the prediction length, context
    length (indicating the number of preceding time steps to consider), and the data
    frequency, among other parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the model, simply invoke the `train()` method on the estimator supplying
    the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This process trains the model using the provided data, resulting in a predictor
    prepared for forecasting. Here’s how we can get the prediction from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, predictions can be generated with the `make_evaluation_predictions()`
    method, which can then be plotted against the actual values. Here’s the plot with
    the forecasts and actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Comparative analysis of forecasts with and without AddAgeFeature](img/B21145_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Comparative analysis of forecasts with and without AddAgeFeature'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we show a comparative analysis of the forecasts with
    and without `AddAgeFeature`. The use of this feature improves forecasting accuracy,
    which indicates that it’s an important variable in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GluonTS provides a series of built-in features that are useful for time series
    analysis and forecasting. For example, data transformers allow you to quickly
    build new features based on the raw dataset. As utilized in our experiment, the
    `AddAgeFeature` transformer appends an `age` attribute to each time series. The
    age of a time series can often provide relevant contextual information to the
    model. A good example where we can find it useful is when working with stocks,
    where older stocks might exhibit different volatility patterns than newer ones.
  prefs: []
  type: TYPE_NORMAL
- en: Training in GluonTS adopts a dictionary-based structure, where each dictionary
    corresponds to a time series and includes additional associated features. This
    structure makes it easier to append, modify, or remove features.
  prefs: []
  type: TYPE_NORMAL
- en: We tested a simple model in our experiment by using the `SimpleFeedForwardEstimator`
    model. We defined two instances of the model, one that was trained with the `AddAgeFeature`
    and one without. The model trained with the `age` feature showed better forecasting
    accuracy, as we can see in *Figure 6**.3*. This improvement highlights the importance
    of feature engineering in time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Training a DeepAR model with GluonTS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepAR is a state-of-the-art forecasting method that utilizes autoregressive
    recurrent networks to predict future values of time series data. Amazon introduced
    it; it was designed for forecasting tasks that can benefit from longer horizons,
    such as demand forecasting. The method is particularly powerful when there’s a
    need to generate forecasts for multiple related time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the same dataset as in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how to build a DeepAR model with this data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by formatting the data for training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this by using the `ListDataset` data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the DeepAR estimator with the `DeepAREstimator` class, specifying
    parameters such as `prediction_length` (forecasting horizon), `context_length`
    (number of lags), and `freq` (sampling frequency):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the estimator, train the DeepAR model using the `train()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the trained model, make predictions on your test data and visualize the
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the plot of the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Comparison of predictions from DeepAR and the true values from
    our dataset](img/B21145_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Comparison of predictions from DeepAR and the true values from
    our dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The model is able to match the true values closely.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DeepAR uses an RNN architecture, often leveraging LSTM units or GRUs to model
    time series data.
  prefs: []
  type: TYPE_NORMAL
- en: The `context_length` parameter is crucial as it determines how many past observations
    the model will consider as its context when making a prediction. For instance,
    if you set `context_length` to `7`, the model will use the last week’s data to
    forecast future values.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the `prediction_length` parameter defines the horizon, (i.e., how
    many steps the model should predict into the future). In the given code, we’ve
    used a horizon of one week.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR also stands out because of its ability to generate probabilistic forecasts.
    Instead of giving a single-point estimate, it provides a distribution over possible
    future values, allowing us to understand the uncertainty associated with our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when working with multiple related time series, DeepAR exploits the
    commonalities between the series to make more accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DeepAR shines when the following conditions are met:'
  prefs: []
  type: TYPE_NORMAL
- en: You have multiple related time series; DeepAR can use information from all series
    to improve forecasts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your data has seasonality or recurring patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to generate probabilistic forecasts, which predict a point estimate
    and provide uncertainty intervals. We will discuss uncertainty estimation in the
    next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can train a single DeepAR model for a global dataset and generate forecasts
    for all time series in the dataset. On the other hand, for individual time series,
    DeepAR can be trained on each series separately, although this might be less efficient.
  prefs: []
  type: TYPE_NORMAL
- en: This model could be particularly useful for demand forecasting in retail, stock
    price prediction, and predicting web traffic, among other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Transformer model with NeuralForecast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we turn our attention to Transformer architectures that have been driving
    recent advances in various fields of artificial intelligence. In this recipe,
    we will show you how to train a vanilla Transformer using the NeuralForecast Python
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers have become a dominant architecture in the deep learning community,
    especially for **natural language processing** (**NLP**) tasks. Transformers have
    been adopted for various tasks beyond NLP, including time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional models that analyze time series data point by point in sequence,
    Transformers evaluate all time steps simultaneously. This approach is similar
    to observing an entire timeline at once, determining the significance of each
    moment in relation to others for a specific point in time.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of the Transformer architecture is the **attention mechanism**.
    This mechanism calculates a weighted sum of input values, or values from previous
    layers, according to their relevance to a specific input. Unlike RNNs, which process
    inputs step by step, this allows Transformers to consider all parts of an input
    sequence simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key components of the Transformer include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-attention mechanism**: Computes the attention scores for all pairs of
    input values and then creates a weighted combination of these values based on
    these scores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-head attention**: The model can focus on different input parts for
    different tasks or reasons by running multiple attention mechanisms in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Position-wise feedforward networks**: These apply linear transformations
    to the output of the attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional encoding**: Since the Transformer doesn’t have any inherent sense
    of order, positional encodings are added to the input embeddings to provide the
    model with information about the position of each element in the sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how to train a Transformer model. In this recipe, we’ll resort again
    to the dataset provided in the `gluonts` library. We’ll use the Transformer implementation
    available in the NeuralForecast library. NeuralForecast is a Python library that
    contains the implementation of several neural networks that are focused on forecasting
    problems, including several Transformer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s prepare the dataset for the Transformer model. Unlike sequence-to-sequence
    models such as RNNs, LSTMs, or GRUs, which process input sequences step by step,
    Transformers process entire sequences at once. Therefore, how we format and feed
    data into them can be slightly different:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading the dataset and the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, convert the dataset into a pandas DataFrame and standardize it. Recall
    that standardization is key for any deep learning model fitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the data ready, we’ll train a Transformer model. Unlike the DeepAR model,
    which uses recurrent architectures, the Transformer will rely on its attention
    mechanisms to consider various parts of the time series when making predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, visualize the forecast results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure includes the Transformer forecasts and actual values of
    the time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Comparison of Transformer predictions and our dataset’s true
    values](img/B21145_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Comparison of Transformer predictions and our dataset’s true values'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `neuralforecast` library requires the data in a specific format. Each observation
    consists of three pieces of information: the timestamp, the time series identifier,
    and the corresponding value. We started this recipe by preparing the data in this
    format. The Transformer is implemented in the `VanillaTransformer` class. We set
    a few parameters, such as the forecasting horizon, number of training steps, or
    early stopping related inputs. You can check the complete list of the parameters
    at the following link: [https://nixtla.github.io/neuralforecast/models.vanillatransformer.html](https://nixtla.github.io/neuralforecast/models.vanillatransformer.html).
    The training process is carried out by the `fit()` method in the `NeuralForecast`
    class instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers process time series data by encoding the entire sequence using
    self-attention mechanisms, capturing dependencies without regard for their distance
    in the input sequence. This global perspective is particularly valuable when patterns
    or dependencies exist over long horizons or when the relevance of past data changes
    dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings are used to ensure that the Transformer recognizes the
    order of data points. Without them, the model would treat the time series as a
    bag of values without any inherent order.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-head attention mechanism allows the Transformer to focus on different
    time steps and features concurrently, making it especially powerful for complex
    time series with multiple interacting patterns and seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformers can be highly effective for time series forecasting due to the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Their ability to capture long-term dependencies in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability with large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility in modeling both univariate and multivariate time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like other models, Transformers benefit from hyperparameter tuning, such as
    adjusting the number of attention heads, the size of the model (i.e., the number
    of layers and the dimension of the embeddings), and the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Temporal Fusion Transformer with GluonTS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TFT is an attention-based architecture developed at Google. It has recurrent
    layers to learn temporal relationships at different scales combined with self-attention
    layers for interpretability. TFTs also use variable selection networks for feature
    selection, gating layers to suppress unnecessary components, and quantile loss
    as their loss function to produce forecasting intervals.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we delve into training and performing inference with a TFT
    model using the GluonTS framework.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure you have the GluonTS library and PyTorch backend installed in your environment.
    We’ll use the `nn5_daily_without_missing` dataset from the GluonTS repository
    as a working example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we’ll train a TFT model with this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the dataset in place, let’s define the TFT estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin with specifying hyperparameters such as the prediction length,
    context length, and training frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the estimator, proceed to train the TFT model using the training
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once trained, we can make predictions using the model. Utilize the `make_evaluation_predictions``()`
    function to accomplish this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we can visualize our forecasts to understand the model’s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following is a comparison of the predictions of the model with the actual
    values of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: Comparison of predictions from a TFT and the true values from
    our dataset](img/B21145_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Comparison of predictions from a TFT and the true values from our
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the implementation of TFT that is available in `gluonts`. The main parameters
    are the number of lags (context length) and forecasting horizon. You can also
    test different values for some of the parameters of the model, such as the number
    of attention heads (`num_heads`) or the size of the Transformer hidden states
    (`hidden_dim`). The full list of parameters can be found at the following link:
    [https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html](https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.estimator.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TFT fits several use cases due to its complete feature set:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal processing**: TFT addresses the challenge of integrating past observations
    and known future inputs through a sequence-to-sequence model, leveraging LSTM
    Encoder-Decoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention mechanism**: The model employs attention mechanisms, enabling it
    to dynamically assign importance to different time steps. This ensures that the
    model remains focused only on relevant historical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gating mechanisms**: TFT architectures leverage gated residual networks that
    provide flexibility in the modeling process, adapting to the complexity of the
    data. This adaptability is important for handling diverse datasets, especially
    smaller or noisier ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable selection networks**: This component is used to determine the relevance
    of each covariate to the forecast. By weighting the input features’ importance,
    it filters out noise and relies only on significant predictors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static covariate encoders**: TFT encodes static information into multiple
    context vectors, enriching the model’s input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantile prediction**: By forecasting various percentiles at each time step,
    TFT provides a range of possible outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretable outputs**: Even though TFT is a deep learning model, it provides
    insights into feature importance, ensuring transparency in predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond its architectural innovations, TFT’s interpretability makes it a good
    choice when there is a need to explain how the predictions were produced. Components
    such as variable network selection and the temporal multi-head attention layer
    shed light on the importance of different inputs and temporal dynamics, making
    TFT not just a forecasting tool but also an analytical one.
  prefs: []
  type: TYPE_NORMAL
- en: Training an Informer model with NeuralForecast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll explore the `neuralforecast` Python library to train an
    Informer model, another Transformer-based deep learning approach for forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Informer is a Transformer method tailored for long-term forecasting – that is,
    predicting with a large forecasting horizon. The main difference relative to a
    vanilla Transformer is that Informer provides an improved self-attention mechanism,
    which significantly reduces the computational requirements to run the model and
    generate long-sequence predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll show you how to train Informer using `neuralforecast`.
    We’ll use the same dataset as in the previous recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This time, instead of creating `DataModule` to handle the data preprocessing,
    we’ll use the typical workflow of the `neuralforecast`-based models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by preparing the time series dataset in the specific format expected
    by the `neuralforecast` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We transformed the dataset into a pandas DataFrame with three columns: `ds`,
    `unique_id`, and `y`. These represent the timestamp, the ID of the time series,
    and the value of the corresponding time series, respectively. In the preceding
    code, we transformed all the time series into a common value range using a standard
    scaler from `scikit-learn`. We also set the validation set size to 20% of the
    size of the time series. Now, we can set up the Informer model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set Informer with a context length (number of lags) of `7` to forecast the
    next 7 values at each time step. The number of training steps was set to `1000`,
    and we also set up an early stopping mechanism to help the fitting process. These
    are only a subset of the parameters you can use to set up Informer. You can check
    out the following link for the complete list of parameters: [https://nixtla.github.io/neuralforecast/models.informer.html](https://nixtla.github.io/neuralforecast/models.informer.html).
    The model is passed on to a `NeuralForecast` class instance, where we also set
    the frequency of the time series to daily (the `D` keyword). Then, the training
    process is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `nf` object is used to fit the model and can then be used to make predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The forecasts are structured as a pandas DataFrame, so you can check a sample
    of the forecasts by using the `head()` method.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `neuralforecast` library provides yet another simple framework to train
    powerful models for time series problems. In this case, we handle the data logic
    outside of the framework because it handles the passing of the data to models
    internally.
  prefs: []
  type: TYPE_NORMAL
- en: The `NeuralForecast` class instance takes a list of models as input (in this
    case, with a single `Informer` instance) and takes care of the training process.
    This library can be a good solution if you want to use state-of-the-art models
    off the shelf. The limitation is that it is not as flexible as the base PyTorch
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we described how to train a particular Transformer model using
    `neuralforecast`. But, this library contains other Transformers that you can try,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TFT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PatchTST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can check the complete list of models at the following link: [https://nixtla.github.io/neuralforecast/core.html](https://nixtla.github.io/neuralforecast/core.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different Transformers with NeuralForecast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NeuralForecast contains several deep learning methods that you can use to tackle
    time series problems. In this recipe, we’ll walk you through the process of comparing
    different Transformer-based models using `neuralforecast`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use the same dataset as in the previous recipe (the `df` object). We
    set the validation and test size to 10% of the data size each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how to compare different models using `neuralforecast`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by defining the models we want to compare. In this case, we’ll compare
    an Informer model with a vanilla Transformer, which we set up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The training parameters are set equally for each model. We can use the `NeuralForecast`
    class to compare different models using the `cross_validation()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cv` object is the result of the comparison. Here’s a sample of the forecasts
    of each model in a particular time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: Forecasts of two Transformer models in an example time series](img/B21145_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Forecasts of two Transformer models in an example time series'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Informer model seems to produce better forecasts, which we can check by
    computing the mean absolute error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The error of Informer is 0.42, which is better than the 0.53 score obtained
    by `VanillaTransformer`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the hood, the `cross_validation()` method works as follows. Each model
    is trained using the training and validation sets. Then, they are evaluated on
    testing instances. The forecasting performance on the test set provides a reliable
    estimate for the performance we expect the models to have when applied in practice.
    So, you should select the model that maximizes forecasting performance, and retrain
    it using the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The `neuralforecast` library contains other models that you can compare. You
    can also compare different configurations of the same method and see which one
    works best for your data.
  prefs: []
  type: TYPE_NORMAL
