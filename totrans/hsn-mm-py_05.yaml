- en: Parameter Inference Using the Bayesian Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed inferring the parameters using the maximum-likelihood
    approach. In this chapter, we will explore the same issue through a Bayesian approach.
    The main topics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Bayesian learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian learning in HMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximate algorithms for estimating distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the maximum-likelihood approach to learning, we try to find the most optimal
    parameters for our model that maximizes our likelihood function. But data in real
    life is usually really noisy, and in most cases, it doesn''t represent the true
    underlying distribution. In such cases, the maximum-likelihood approach fails.
    For example, consider tossing a fair coin a few times. It is possible that all
    of our tosses result in either heads or tails. If we use a maximum-likelihood
    approach on this data, it will assign a probability of 1 to either heads or tails,
    which would suggest that we would never get the other side of the coin. Or, let''s
    take a less extreme case: let''s say we toss a coin 10 times and get three heads
    and seven tails. In this case, a maximum-likelihood approach will assign a probability
    of 0.3 to heads and 0.7 to tails, which is not the true distribution of a fair
    coin. This problem is also commonly known as **overfitting**.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian learning takes a slightly different approach to learn these parameters.
    We start by assigning a prior distribution over the parameters of our model. The
    prior makes our assumptions about the model explicit. In the case of tossing the
    coin, we can start by using a prior that assigns equal probabilities to both heads
    and tails. Then we apply the Bayes theorem to compute the posterior distribution
    over our parameters based on the data. This allows us to shift our belief (prior)
    toward where the data points to, and this makes us do a less extreme estimate
    of the parameters. And in this way, Bayesian learning can solve one of the major
    drawbacks of maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'In more general terms, in the case of Bayesian learning, we try to learn a
    distribution over the parameters of our model instead of learning a single parameter
    that maximizes the likelihood. For learning this distribution over the parameters,
    we use the Bayes theorem, given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f629c8c-5429-46a3-a693-2f2d0b609354.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *P(θ)* is our prior over the parameters of the model, *P(D|θ)* is the
    likelihood of the data given the parameters, and *P(D)* is the probability of
    the observed data. *P(D)* can also be written in terms of prior and likelihood
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbdfc5df-65e2-41d4-a980-28e5f75354ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's talk about each of these terms separately and see how can we compute
    them. The prior, *P(θ)*, is a probability distribution over the parameters representing
    our belief about the values of the parameters. For example, in the case of coin
    tossing, we can have our initial belief as *θ* is in between 0 and 1 and is uniformly
    distributed. The likelihood term, *P(D|θ)*, is the same term that we tried to
    maximize in [Chapter 4](8d06a68a-e427-4f7d-9472-9be25b5351c0.xhtml), *Parameter
    Inference using Maximum Likelihood*. It represents how likely our observed data
    is, given the parameters of the model. The next term, *P(D)*, is the probability
    of observing our data and it acts as the normalizing term. It is computationally
    difficult to compute because it requires us to sum over all the possible values
    of *θ* and, for any sufficiently large number of parameters, it quickly becomes
    intractable. In the next sections of this chapter, we will see the different algorithms
    that we can use to approximate these values. The term that we are trying to compute,
    *P(D|θ),* is known as the **posterior**. It represents our final probability distribution
    over the parameters of the model given our observed data. Basically, our prior
    is updated using the likelihood term to give the final distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem that Bayesian learning solves is the model selection. Since
    Bayesian learning gives a distribution over the different possible models rather
    than a single model, we have a couple of options of how we want to do predictions
    from these models. The first method is to just select a specific model that has
    the maximum probability, which is also commonly known as the **Maximum Aposteriori**
    (**MAP**) estimate. The other possible way is to compute the expectation of the
    prediction from all the models based on the posterior distribution. This allows
    us to regularize our predictions since we are computing expectation over all possible
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the priors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common question when doing Bayesian learning is how to select the appropriate
    prior. As David Mackay has said, *there is no inference without assumptions, we
    need to make a guess for the prior*. Our prior should be representative of what
    we think the most likely parameters are for our model. A huge benefit of using
    our own prior is that we make our assumption about the model explicit. Once we
    start applying Bayes, theorem using our prior and the observed data, our posterior
    would be a shift from our prior toward a distribution that represents our data
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretically, this sounds good as we can probably select very complex priors
    that capture our idea of the model, but for applying the Bayes theorem, we need
    to multiply our prior with the likelihood, and for complex distributions, it very
    quickly becomes computationally intractable. Therefore, in practice, we usually
    select a prior that is a conjugate distribution to our likelihood. A conjugate
    prior allows us to have a closed-form solution to the Bayes theorem. Because of
    this, Gaussian distributions are used for priors and likelihoods as multiplying
    a Gaussian distribution with another Gaussian distribution results in a Gaussian
    distribution. Also computations it's not expensive to compute the product of two
    Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: Intractability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from selecting difficult priors, another source of intractability in
    Bayesian learning is the denominator term of Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94eab2cf-86ad-4a4d-8333-2fe419d34fd4.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding equation for computing *P(D)*, we need to compute
    a summation over all the possible values of *θ*, which is the set of all the parameters
    of our model. If we have a lot of parameters in our model, it is computationally
    intractable to compute this term since the size of the term grows exponentially
    with the number of parameters. A lot of work has been done to approximate this
    value, as we will see in the next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian learning in HMM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous section, in the case of Bayesian learning we assume
    all the variables as a random variable, assign a prior to it, and then try to
    compute the posterior based on that. Therefore, in the case of HMM, we can assign
    a prior on our transition probabilities, emission probabilities, or the number
    of observation states.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the first problem that we need to solve is to select the prior. Theoretically,
    a prior can be any distribution over the parameters of the model, but in practice,
    we usually try to use a conjugate prior to the likelihood, so that we have a closed-form
    solution to the equation. For example, in the case when the output of the HMM
    is discrete, a common choice of prior is the Dirichlet distribution. It is mainly
    for two reasons, the first of which is that the Dirichlet distribution is a conjugate
    distribution to multinomial distribution which allows us to multiply them easily.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conjugate distribution**: A family of priors is said to be conjugate to a
    family of likelihoods if the posterior obtained by multiplying the prior by the
    likelihood is in the same family of distribution as the prior distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, since the likelihood of the initial state given the *π* parameter
    vector is multinomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5960650-e1ce-46e9-8598-7c16419a6e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if the prior probability of *π* is Dirichlet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b0a0141-4f7d-4935-a5f1-661689aafd0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *u = [u[1], u[2], ..., u[K]]* is the hyperparameter vector and *Z* is
    the normalizing constant. We can now compute the posterior from the likelihood
    and the prior, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df4cf2c1-e1bf-4876-91f9-e7338aa89ada.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can see that the posterior is also a Dirichlet distribution. Hence we
    can say that the Dirichlet distribution is a conjugate prior to the multinomial
    distribution. And in a similar way, we can set up Dirichlet priors for our transition
    matrix and emission matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The second reason for choosing a Dirichlet prior is that it has the desirable
    property that its hyperparameters can be interpreted as a hypothetical count of
    observations. In the preceding example, if *u[i] = 2* and *u[j] = 1* for *j ≠
    i*, the MAP estimate of *π* would be the same as a maximum-likelihood estimation
    with the assumption that the training data had an extra data point with the initial
    state being in state *i*. This conjugate property allows us to do MAP estimation
    in the case of Dirichlet priors by doing a minor variation in the Baum-Welch algorithm.
    It also gives theoretical justification for the seemingly ad hoc but very common
    regularization method for HMMs, which just adds a small positive number to all
    elements of the parameter vector.
  prefs: []
  type: TYPE_NORMAL
- en: In the last couple of paragraphs, we talked specifically about the case when
    the output is discrete. But the same concepts can be extended to the case of continuous
    output as well. Conjugate distributions exist in the case of continuous distributions
    as well. One of the most commonly used distributions is the Gaussian distribution
    as it stays in the same family after different operations.
  prefs: []
  type: TYPE_NORMAL
- en: Approximating required integrals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed before, the Bayesian approach treats all unknown quantities
    as random variables. We assign prior distributions to these variables and then
    estimate the posterior distribution over these after the data is observed. In
    the case of HMMs, the unknown quantities comprise the structure of the HMM, that
    is, the number of states, the parameters of the network, and the hidden states.
    Unlike maximum-likelihood or MAP estimations, in which we find point estimates
    for these parameters, we now have distributions over these parameters. This allows
    us to compare between model structures, but for doing that we need to integrate
    over both the parameters and the hidden states of the model. This is commonly
    known as Bayesian integration.
  prefs: []
  type: TYPE_NORMAL
- en: Since these integrations are computationally intractable, we resort to approximate
    methods to compute these values. In the next few subsections, we will give an
    overview of some of these methods. A detailed analysis of these methods is outside
    the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sampling methods are one of the most common ways to estimate intractable distributions.
    The general idea is to sample points from the distribution space in a way such
    that we get more samples from high-probability areas. And then based on these
    samples we estimate the distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace approximations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Laplace approximations use the central limit theorem, which from well-behaved
    priors and data asserts that the posterior parameter will converge in the limit
    of a large number of training samples to a Gaussian around the MAP estimate of
    the parameters. To estimate the evidence using the Laplace approximation, MAP
    parameters are found in the usual optimization routines and then the Hessian of
    the log-likelihood is computed at the MAP estimate. The evidence is approximated
    by evaluating the *P(θ,D)/P(θ|D)* ratio at the MAP estimate of *θ*, using the
    Gaussian approximation in the denominator. The Laplace approximation suffers from
    several disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the Hessian matrix from the parameters is usually very costly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gaussian approximation is not very good for models with parameters that
    are positive and sum to 1, especially when there are many parameters relative
    to the size of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, the Laplace approximation is usually not used for HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: Stolke and Omohundro's method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the famous paper *HMM induction by Bayesian model merging* Stolke and Omohundro
    present a new technique for approximating the Bayesian integrals of HMMs. Consider the
    case of having all the states of the HMM to be observed and the priors to be Dirichlet
    distributions. In this case, when learning the parameters using Bayesian learning,
    the posteriors are also going to be Dirichlet distributions, and then the evidence
    integral can be represented as a product of Dirichlet integrals, which can be
    easily computed. Therefore, in a sense, we can say that the reason for the intractability
    of evidence integrals is the fact that the states and parameters are hidden.
  prefs: []
  type: TYPE_NORMAL
- en: Stolke and Omohundro's method proposed to find the single most likely sequence
    of hidden states using a Viterbi-like algorithm and using this sequence as observed
    states. Using these observed values, we can easily do evidence integrals. The
    method proposes to iterate between these two steps, incrementally searching over
    model structures, merging or splitting states based on comparisons of this approximate
    evidence. In their paper, Stolke and Omohundro show that this method of trading
    off integration over hidden variables by integrating over parameters is able to
    get good results.
  prefs: []
  type: TYPE_NORMAL
- en: Variational methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variational methods are another very common method used for approximating distributions.
    The general idea is to start by choosing a simpler family of distributions and
    then try to find the hyperparameters of this distribution such that the distribution
    is as close as possible to our original distribution. There are different metrics
    that are used to determine the closeness of two distributions; the most commonly
    used metric is Kullback-Leiber divergence. This method basically converts an inference
    problem into an optimization problem where we try to minimize our divergence metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of HMMs, we usually make an assumption that the hidden states are
    independent of the parameters of the model. This allows us to approximate distributions
    over both the hidden states and parameters simultaneously. More specifically,
    the evidence can be lower bounded by applying Jenson''s inequality twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6441a3c-4b9b-4f5f-b46e-47a3ede9e751.png)'
  prefs: []
  type: TYPE_IMG
- en: The variational Bayesian approach iteratively maximizes ![](img/a53434d0-4c21-4db9-9f3f-611212895d68.png) as
    a functional of the two free distributions, *Q(S) *and *Q(θ)*. In the preceding
    equations, we can see that this maximization is equivalent to minimizing the KL
    divergence between *Q(S)Q(θ)* and the joint posterior over hidden states and the *P(S,θ|D,M)* parameters.
    David MacKay first presented a variational Bayesian approach to learning in HMMs.
    He assumed the prior to be a Dirichlet distribution, making the assumption that
    the parameters are independent of the hidden states, he showed that the optimal
    *Q(θ)* is a Dirichlet distribution. Furthermore, he showed that the optimal *Q(S)* could
    be obtained by applying the forward-backward algorithm to an HMM with pseudo-parameters
    given by ![](img/881223eb-6a9e-4209-a083-d9fa362eac42.png), which can be evaluated
    for Dirichlet distributions. Thus the whole variational Bayesian method can be
    implemented as a simple modification of the Baum-Welch algorithm. Essentially
    we can state that the variational Bayesian method is a combination of special
    cases of both the MAP approach and Stolke and Omohundro's approach. This is very
    promising, especially given that it has been used successfully for non-trivial
    model-structure learning in other models; its potential has not been fully explored
    for HMMs and their extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, there are no packages in Python that support learning using Bayesian
    learning and it would be really difficult to write the complete code to fit in
    this book. And even though there are a lot of advantages to using Bayesian learning,
    it is usually computationally infeasible in a lot of cases. For these reasons,
    we are skipping the code for Bayesian learning in HMMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about applying Bayesian learning in the case of learning
    parameters in HMMs. Bayesian learning has a few benefits over the maximum-likelihood
    estimator, but it turns out to be computationally quite expensive except when
    we have closed-form solutions. Closed-form solutions are only possible when we
    use conjugate priors. In the following chapters, we will discuss detailed applications
    of HMMs for a wide variety of problems.
  prefs: []
  type: TYPE_NORMAL
