- en: Data Cleaning and Advanced Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of data analytics in general is to uncover actionable insights that
    result in positive business outcomes. In the case of predictive analytics, the
    aim is to do this by determining the most likely future outcome of a target, based
    on previous trends and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of predictive analytics are not restricted to big technology companies.
    Any business can find ways to benefit from machine learning, given the right data.
  prefs: []
  type: TYPE_NORMAL
- en: Companies all around the world are collecting massive amounts of data and using
    predictive analytics to cut costs and increase profits. Some of the most prevalent
    examples of this are from the technology giants Google, Facebook, and Amazon,
    who utilize big data on a huge scale. For example, Google and Facebook serve you
    personalized ads based on predictive algorithms that guess what you are most likely
    to click on. Similarly, Amazon recommends personalized products that you are most
    likely to buy, given your previous purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Modern predictive analytics is done with machine learning, where computer models
    are trained to learn patterns from data. As we saw briefly in the previous chapter,
    software such as scikit-learn can be used with Jupyter Notebooks to efficiently
    build and test machine learning models. As we will continue to see, Jupyter Notebooks
    are an ideal environment for doing this type of work, as we can perform adhoc
    testing and analysis, and easily save the results for reference later.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will again take a hands-on approach by running through various examples
    and activities in a Jupyter Notebook. Where we saw a couple of examples of machine
    learning in the previous chapter, here we'll take a much slower and more thoughtful
    approach. Using an employee retention problem as our overarching example for the
    chapter, we will discuss how to approach predictive analytics, what things to
    consider when preparing the data for modeling, and how to implement and compare
    a variety of models using Jupyter Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Plan a machine learning classification strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess data to prepare it for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use validation curves to tune model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use dimensionality reduction to enhance model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing to Train a Predictive Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will cover the preparation required to train a predictive model. Although
    not as technically glamorous as training the models themselves, this step should
    not be taken lightly. It's very important to ensure you have a good plan before
    proceeding with the details of building and training a reliable model. Furthermore,
    once you've decided on the right plan, there are technical steps in preparing
    the data for modeling that should not be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: We must be careful not to go so deep into the weeds of technical tasks that
    we lose sight of the goal.Technical tasks include things that require programming
    skills, for example, constructing visualizations, querying databases, and validating predictive
    models. It's easy to spend hours trying to implement a specific feature or get
    the plots looking just right. Doing this sort of thing is certainly beneficial
    to our programming skills, but we should not forget to ask ourselves if it's really
    worth our time with respect to the current project.
  prefs: []
  type: TYPE_NORMAL
- en: Also, keep in mind that Jupyter Notebooks are particularly well-suited for this
    step, as we can use them to document our plan, for example, by writing rough notes
    about the data or a list of models we are interested in training. Before starting
    to train models, it's good practice to even take this a step further and write
    out a well-structured plan to follow. Not only will this help you stay on track
    as you build and test the models, but it will allow others to understand what
    you're doing when they see your work.
  prefs: []
  type: TYPE_NORMAL
- en: After discussing the preparation, we will also cover another step in preparing
    to train the predictive model, which is cleaning the dataset. This is another
    thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing
    ground for performing dataset transformations and keeping track of the exact changes.
    The data transformations required for cleaning raw data can quickly become intricate
    and convoluted; therefore, it's important to keep track of your work. As discussed
    in the fist chapter, tools other than Jupyter Notebooks just don't offer very
    good options for doing this efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Determining a Plan for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When formulating a plan for doing predictive modeling, one should start by considering
    stakeholder needs. A perfect model will be useless if it doesn't solve a relevant
    problem. Planning a strategy around business needs ensures that a successful model
    will lead to actionable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may be possible in principle to solve many business problems, the
    ability to deliver the solution will always depend on the availability of the
    necessary data. Therefore, it's important to consider the business needs in the
    context of the available data sources. When data is plentiful, this will have
    little effect, but as the amount of available data becomes smaller, so too does
    the scope of problems that can be solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'These ideas can be formed into a standard process for determining a predictive
    analytics plan, which goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the available data to understand the range of realistically solvable
    business problems. At this stage, it might be too early to think about the exact
    problems that can be solved. Make sure you understand the data fields available
    and the
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: time frames they apply to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Determine the business needs by speaking with key stakeholders. Seek out a problem
    where the solution will lead to actionable business decisions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assess the data for suitability by considering the availability of a sufficiently diverse
    and large feature space. Also, take into account the condition of the data: are
    there large chunks of missing values for certain variables or time ranges?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 and 3 should be repeated until a realistic plan has taken shape. At
    this point, you will already have a good idea of what the model input will be
    and what you might expect as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve identified a problem that can be solved with machine learning,
    along with the appropriate data sources, we should answer the following questions
    to lay a framework for the project. Doing this will help us determine which types
    of machine learning models we can use to solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the training data labeled with the target variable we want to predict?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the answer is yes, then we will be doing supervised machine learning. Supervised
    learning has many real-world use cases, whereas it's much rarer to find business
    cases for doing predictive analytics on unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: If the answer is no, then you are using unlabeled data and hence doing unsupervised
    machine learning. An example of an unsupervised learning method is cluster analysis,
    where labels are assigned to the nearest cluster for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: If the data is labeled, then are we solving a regression or classification problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a regression problem, the target variable is continuous, for example, predicting
    the amount of rain tomorrow in centimeters. In a classification problem, the target
    variable is discrete and we are predicting class labels. The simplest type of
    classification problem is binary, where each sample is grouped into one of two
    classes. For example, will it rain tomorrow or not?
  prefs: []
  type: TYPE_NORMAL
- en: What does the data look like? How many distinct sources?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the size of the data in terms of width and height, where width refers
    to the number of columns (features) and height refers to the number of rows. Certain
    algorithms are more effective at handling large numbers of features than others.
    Generally, the bigger the dataset, the better in terms of accuracy. However, training
    can be very slow and memory intensive for large datasets. This can always be reduced
    by performing aggregations on the data or using dimensionality reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there are different data sources, can they be merged into a single table?
    If not, then we may want to train models for each and take an ensemble average
    for the final prediction model. An example where we may want to do this is with
    various sets of times series data on different scales. Consider we have the following
    data sources: a table with the AAPL stock closing prices on a daily time scale
    and iPhone sales data on a monthly time scale.'
  prefs: []
  type: TYPE_NORMAL
- en: We could merge the data by adding the monthly sales data to each sample in the
    daily time scale table, or grouping the daily data by month, but it might be better
    to build two models, one for each dataset, and use a combination of the results
    from each in the final prediction model.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing Data for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preprocessing has a huge impact on machine learning. Like the saying "you
    are what you eat," the model's performance is a direct reflection of the data
    it's trained on. Many models depend on the data being transformed so that the
    continuous feature values have comparable limits. Similarly, categorical features
    should be encoded into numerical values. Although important, these steps are relatively
    simple and do not take very long.
  prefs: []
  type: TYPE_NORMAL
- en: The aspect of preprocessing that usually takes the longest is cleaning up messy
    data. Just take a look at this pie plot showing what data scientists from a particular
    survey spent most of their time doing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32567c8e-b535-47f7-926b-58915f4cb7cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Another thing to consider is the size of the datasets being used by many data
    scientists. As the dataset size increases, the prevalence of messy data increases
    as well, along with the difficulty in cleaning it.
  prefs: []
  type: TYPE_NORMAL
- en: Simply dropping the missing data is usually not the best option, because it's
    hard to justify throwing away samples where most of the fields have values. In
    doing so, we could lose valuable information that may hurt final model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in data preprocessing can be grouped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Merging data sets on common fields to bring all data into a single table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering to improve the quality of data, for example, the use of dimensionality
    reduction techniques to build new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning the data by dealing with duplicate rows, incorrect or missing values,
    and other issues that arise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the training data sets by standardizing or normalizing the required
    data and splitting it into training and testing sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore some of the tools and methods for doing the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data preprocessing tools and methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start the `NotebookApp` from the project directory by executing `jupyter notebook`.Navigate
    to the `chapter-2` directory and open up the `chapter-2-workbook.ipynb` file.
    Find the cell near the top where the packages are loaded, and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to start by showing off some basic tools from Pandas and scikit-learn.
    Then, we'll take a deeper dive into methods for rebuilding missing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to Subtopic `Preprocessing data for machine learning` and run the
    cell containing `pd.merge`? to display the docstring for the merge function in
    the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6f724102-2815-42ef-869b-f59d448858ec.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the function accepts a left and right DataFrame to merge. You
    can specify one or more columns to group on as well as how they are grouped, that
    is,to use the left, right, outer, or inner sets of values. Let's see an example
    of this in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exit the help popup and run the cell containing the following sample DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we will build two simple DataFrames from scratch. As can be seen, they
    contain a `product` column with some shared entries.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to perform an inner merge on the `product` shared column and
    print the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to perform the inner merge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9c21a6e7-9029-4b81-a4a2-b0ded6813e94.png)'
  prefs: []
  type: TYPE_IMG
- en: Note how only the shared items, **red shirt** and white dress, are included.
    To include all entries from both tables, we can do an outer merge instead. Let's
    do this now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to perform an outer merge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to perform an outer merge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cb3d8fcc-b912-44f1-a9ff-aa8df63a2f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is our fist time encountering an `NaN` value in this book, now is
    a good time to discuss how these work in Python.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, you can define an `NaN` variable by doing, for example, `a = float('nan')`.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you want to test for equality, you cannot simply use standard comparison methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s best to do this instead with a high-level function from a library such
    as `NumPy`. This is illustrated with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e351060-c60e-41e8-98fb-de8f72b9e064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some of these results may seem counter intuitive. There is logic behind this
    behavior, however, and for a deeper understanding of the fundamental reasons for
    standard comparisons returning False, check out this excellent Stack Overflow
    thread: [https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values.](https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values)'
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that our most recently merged table has duplicated data
    in the fist few rows. Let's see how to handle this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.drop_duplicates()` to return a version of the DataFrame
    with no duplicate rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6eb2f78-014e-415c-9f05-60aa2a799270.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the easiest and "standard" way to drop duplicate rows. To apply these
    changes to df, we can either `set inplace=True` or do something like `df = df.drop_duplicated()`
    . Let's see another method, which uses masking to select or drop duplicate rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.duplicated()` to print the True/False series, marking
    duplicate rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/76823424-ab27-4c20-b56a-053833baa96f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can take the sum of this result to determine how many rows have duplicates,
    or it can be used as a mask to select the duplicated rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do this by running the next two cells:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3c292a6c-ab36-4293-9e9b-4288b5cba555.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the opposite of the mask with a simple tilde (`~`) to extract
    the deduplicated DataFrame. Run the following code and convince yourself the output is
    the same as that from `df.drop_duplicates()` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df[~df.duplicated()]`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a3e75fd-9f5c-42ce-8b71-cb95595583d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can also be used to drop duplicates from a subset of the full DataFrame.
    For example, run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df[~df[''product''].duplicated()]`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cac8d491-5f80-4fb4-b09f-440e34f9453c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are doing the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a mask (a `True`/`False` series) for the product row, where duplicates
    are marked with `e`
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the tilde (`~`) to take the opposite of that mask, so that duplicates
    are instead marked with False and everything else is `True`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using that mask to filter out the `False` rows of `df`, which correspond to
    the duplicated products
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As expected, we now see that only the first red shirt row remains, as the duplicate
    product rows have been removed.
  prefs: []
  type: TYPE_NORMAL
- en: In order to proceed with the steps, let's replace `df` with a deduplicated version
    of itself. This can be done by running `drop_duplicates` and passing the parameter
    `inplace=True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deduplicate the DataFrame and save the result by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop_duplicates(inplace=True)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Continuing on to other preprocessing methods, let's ignore the duplicated rows
    and first deal with the missing data. This is necessary because models cannot
    be trained on incomplete samples. Using the missing price data for blue pants
    and white tuxedo as an example, let's show some different options for handling
    `NaN` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'One option is to drop the rows, which might be a good idea if your NaN samples are
    missing the majority of their values. Do this by running the cell containing `df.dropna()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aeae3f40-b4db-4577-a076-173d33862f59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If most of the values are missing for a feature, it may be best to drop that
    column entirely. Do this by running the cell containing the same method as before,
    but this time with the axes parameter passed to indicate columns instead of rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/98ef084c-27d4-496d-8fea-db589f8eb018.png)'
  prefs: []
  type: TYPE_IMG
- en: Simply dropping the `NaN` values is usually not the best option, because losing
    data is never good, especially if only a small fraction of the sample values is
    missing.Pandas offers a method for filling in `NaN` entries in a variety of different
    ways, some of which we'll illustrate now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.fillna?` to print the docstring for the Pandas
    `NaN-fill` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4fb8a796-95f2-4d38-a0db-f9624e3b14a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the options for the value parameter; this could be, for example, a single
    value or a dictionary/series type map based on index. Alternatively, we can leave
    the value as None and pass a fill method instead. We'll see examples of each in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill in the missing data with the average product price by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.fillna(value=df.price.mean())`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/d2508e74-e8e6-49b6-ab28-f45226336411.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, fill in the missing data using the pad method by running the cell containing the
    following code instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.fillna(method=''pad'')`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/06574946-eeaf-44c8-8d7c-ff33db9d53e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the **white dress** price was used to pad the missing values below
    it.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, we will prepare our simple table to be used for training
    a machine learning algorithm. Don't worry, we won't actually try to train any
    models on such a small dataset! We start this process by encoding the class labels
    for the categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before encoding the labels, run the fist cell in the `Building training data
    sets` section to add another column of data representing the average product ratings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/780a265b-e2e9-4999-94c9-bebd4b545d11.png)'
  prefs: []
  type: TYPE_IMG
- en: Imagining we want to use this table to train a predictive model, we should first
    think about changing all the variables to numeric types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest column to handle is the Boolean list:`in_stock`. This should be
    changed to numeric values, for example, 0 and 1, before using it to train a predictive
    model. This can be done in many ways, for example, by running the cell containing
    the following code: `df.in_stock = df.in_stock.map({False: 0, True: 1})`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/870248af-69ea-4dec-b052-fca89b2e7dbe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another option for encoding features is scikit-learn''s LabelEncoder, which
    can be used to map the class labels to integers at a higher level. Let''s test
    this by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6e2ce61-258a-4a0a-bfe5-743d9a90ce04.png)'
  prefs: []
  type: TYPE_IMG
- en: This might bring to mind the preprocessing we did in the previous chapter, when
    building the polynomial model. Here, we instantiate a label encoder and then "train"
    it and "transform" our data using the `fit_transform` method. We apply the result
    to a copy of our DataFrame, `_df`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features can then be converted back using the class we reference with the variable
    `rating_encoder, by running rating_encoder.inverse_transform(df.rating)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d0c3289c-bfb4-4b4f-9566-163d2acb87d5.png)'
  prefs: []
  type: TYPE_IMG
- en: You may notice a problem here. We are working with a so-called "ordinal" feature,
    where there's an inherent order to the labels. In this case, we should expect
    that a rating of "low" would be encoded with a 0 and a rating of "high" would
    be encoded with a 2\. However, this is not the result we see. In order to achieve
    proper ordinal label encoding, we should again use map, and build the dictionary
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encode the ordinal labels properly by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/001e575c-a915-4803-9378-acd1388b0269.png)'
  prefs: []
  type: TYPE_IMG
- en: We first create the mapping dictionary. This is done using a dictionary comprehension
    and enumeration, but looking at the result, we see that it could just as easily
    be defined *manually* instead. Then, as done earlier for the `in_stock` column,
    we apply the dictionary mapping to the feature. Looking at the result, we see
    that rating now makes more sense than before, where `low` is labeled with 0, `medium`
    with 1, and `high` with 2.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've discussed ordinal features, let's touch on another type called
    nominal features. These are fields with no inherent order, and in our case, we
    see that `product` is a perfect example.
  prefs: []
  type: TYPE_NORMAL
- en: Most scikit-learn models can be trained on data like this, where we have strings
    instead of integer-encoded labels. In this situation, the necessary conversions
    are done under the hood. However, this may not be the case for all models in scikit
    learn, or other machine learning and deep learning libraries. Therefore, it's
    good practice to encode these ourselves during preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: 'A commonly used technique to convert class labels from strings to numerical
    values is called one-hot encoding. This splits the distinct classes out into separate
    features. It can be accomplished elegantly with `pd.get_dummies()` . Do this by
    running the cell containing the following code: `df = pd.get_dummies(df)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final DataFrame then looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96dcf83e-e7c7-47ed-b2f4-3c92f4714a67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we see the result of one-hot encoding: the product column has been split into
    4, one for each unique value. Within each column, we find either a 1 or 0 representing
    whether that row contains the particular value or product.'
  prefs: []
  type: TYPE_NORMAL
- en: Moving on and ignoring any data scaling (which should usually be done), the
    final step is to split the data into training and test sets to use for machine
    learning. This can be done using scikit-learn's train_test_split. Let's assume
    we are going to try to predict whether an item is in stock, given the other feature
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/086e1b46-616e-436b-8560-0681c6cf3ce2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we are selecting subsets of the data and feeding them into the `train_test_
    split` function. This function has four outputs, which are unpacked into the training
    and testing splits for features (X) and the target (y).
  prefs: []
  type: TYPE_NORMAL
- en: Observe the shape of the output data, where the test set has roughly 30% of
    the samples and the training set has roughly 70%.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see similar code blocks later, when preparing real data to use for training
    predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the section on cleaning data for use in machine learning applications.
    Let's take a minute to note how effective our Jupyter Notebook was for testing
    various methods of transforming the data, and ultimately documenting the pipeline
    we decided upon. This could easily be applied to an updated version of the data
    by altering only specific cells of code, prior to processing. Also, should we
    desire any changes to the processing, these can easily be tested in the notebook,
    and specific cells may be changed to accommodate the alterations. The best way
    to achieve this would probably be to copy the notebook over to a new file, so
    that we can always keep a copy of the original analysis for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to an activity, we'll now apply the concepts from this section to
    a large dataset as we prepare it for use in training predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Preparing to Train a Predictive Model for the Employee-Retention Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you are hired to do freelance work for a company who wants to find insights
    into why their employees are leaving. They have compiled a set of data they think
    will be helpful in this respect. It includes details on employee satisfaction
    levels, evaluations, time spent at work, department, and salary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The company shares their data with you by sending you a file called `hr_data.csv`
    and asking what you think can be done to help stop employees from leaving. To
    apply the concepts we''ve learned thus far to a real-life problem. In particular,
    we seek to:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine a plan for using predictive analytics to provide impactful business insights,
    given the available data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the data for use in machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with this activity and continuing through the remainder of this chapter,
    we'll be using *Human Resources Analytics*, which is a Kaggle dataset. There is
    a small difference between the dataset we use in this book and the online version.
    Our human resource analytics data contains some `NaN` values. These were manually
    removed from the online version of the
  prefs: []
  type: TYPE_NORMAL
- en: dataset, for the purposes of illustrating data cleaning techniques. We have also
    added a column of data called `is_smoker`, for the same purposes.
  prefs: []
  type: TYPE_NORMAL
- en: With the `chapter-2-workbook.ipynb` notebook file open, scroll to the Activity
    section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check the head of the table by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Judging by the output, convince yourself that it looks to be in standard CSV
    format. For CSV files, we should be able to simply load the data with `pd.read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Load the data with Pandas by running `df = pd.read_csv('../data/hranalytics/hr_data.csv')`
    . Use tab completion to help type the file path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect the columns by printing df.columns and make sure the data has loaded as
    expected by printing the DataFrame head and tail with `df.head()` and `df.tail()`
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1a181777-dd12-4549-b10e-63fb4fcfe683.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that it appears to have loaded correctly. Based on the tail index
    values,there are nearly 15,000 rows; let's make sure we didn't miss any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the number of rows (including the header) in the `CSV file` with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/18be9678-2c67-40ca-a02d-0ae482179cfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compare this result to `len(df)` to make sure we''ve loaded all the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b9116ca7-4323-4383-baf0-3067742ecd08.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that our client's data has been properly loaded, let's think about how we
    can use predictive analytics to find insights into why their employees are leaving.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through the fist steps for creating a predictive analytics plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Look at the available data**: We''ve already done this by looking at the
    columns, datatypes, and the number of samples'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Determine the business needs**: The client has clearly expressed their needs:
    reduce the number of employees who leave'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assess the data for suitability**: Let''s try to determine a plan that can
    help satisfy the client''s needs, given the provided data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall, as mentioned earlier, that effective analytics techniques lead to impactful
    business decisions. With that in mind, if we were able to predict how likely an
    employee is to quit, the business could selectively target those employees for
    special treatment. For example, their salary could be raised or their number of
    projects reduced. Furthermore, the impact of these changes could be estimated
    using the model!
  prefs: []
  type: TYPE_NORMAL
- en: To assess the validity of this plan, let's think about our data. Each row represents
    an employee who either works for the company or has **left**, as labeled by the
    column named left. We can therefore train a model to predict this target, given
    a set of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assess the target variable. Check the distribution and number of missing entries
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9efa19b0-e218-4946-961a-2806e606a906.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s the output of the second code line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d6017b4-3f4f-49e8-8e9d-c2baa58efef3.png)'
  prefs: []
  type: TYPE_IMG
- en: About three-quarters of the samples are employees who have not left. The group
    who has left makes up the other quarter of the samples. This tells us we are dealing
    with an imbalanced classification problem, which means we'll have to take special
    measures to account for each class when calculating accuracies. We also see that
    none of the target variables are missing (no `NaN` values).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll assess the features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the datatype of each by executing `df.dtypes`. Observe how we have a
    mix of continuous and discrete features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1bc604a8-aa56-4d85-8823-477ee6a0c060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Display the feature distributions by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet is a little complicated, but it's very useful for showing
    an overview of both the continuous and discrete features in our dataset. Essentially,
    it assumes each feature is continuous and attempts to plot its distribution, and
    reverts to simply plotting the value counts if the feature turns out to be discrete.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7aa02a0b-5938-487c-8d83-534b3a3881bc.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/fda3c924-026b-4987-8e31-55e64f06fc49.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/114b846f-53c7-47d7-a659-2b59c0c918a2.png)'
  prefs: []
  type: TYPE_IMG
- en: For many features, we see a wide distribution over the possible values, indicating
    a good variety in the feature spaces. This is encouraging; features that are strongly
    grouped around a small range of values may not be very informative for the model.
    This is the case for `promotion_last_5years`, where we see that the vast majority
    of samples are 0.
  prefs: []
  type: TYPE_NORMAL
- en: The next thing we need to do is remove any `NaN` values from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check how many `NaN` values are in each column by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.isnull().sum() / len(df) * 100`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/362bc015-f9d4-41c1-94d7-2914c79cbc09.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see there are about 2.5% missing for a`verage_montly_hours`, 1% missing for
    t`ime_spend_company`, and 98% missing for `is_smoker`! Let's use a couple of different
    strategies that we've learned about to handle these.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there is barely any information in the `is_smoker` metric, let''s drop
    this column. Do this by running: `del df[''is_smoker'']` .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since time_spend_company is an integer fild, we''ll use the median value to
    fil the NaN values in this column. This can be done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The final column to deal with is `average_montly_hours`. We could do something similar
    and use the median or rounded mean as the integer fill value. Instead though,
    let's try to take advantage of its relationship with another variable. This may
    allow us to fill the missing data more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a boxplot of `average_montly_hours` segmented by `number_project`. This can
    be done by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3ad737c9-9977-47cd-aacb-389f83329698.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see how the number of projects is correlated with a`verage_monthly_hours`,
    a result that is hardly surprising. We'll exploit this relationship by filling
    in the `NaN` values of `average_montly_hours` differently, depending on the number
    of projects for that sample. Specifically, we'll use the mean of each group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the mean of each group by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3a759d97-a02d-493a-995e-bbdd922d3a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can then map this onto the `number_project` column and pass the resulting series
    object as the argument to `fillna`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fill the `NaN` values in `average_montly_hours` by executing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm that `df` has no more `NaN` values by running the following assertion
    test. If it does not raise an error, then you have successfully removed the `NaNs`
    from the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`assert df.isnull().sum().sum() == 0`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we will transform the string and Boolean filds into integer representations.In
    particular, we''ll manually convert the target variable `left` from `yes` and
    `no` to` 1` and `0` and build the one-hot encoded features. Do this by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Print `df.columns` to show the fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1c2b5d84-ee4a-4f17-bda4-495ebf1c5f51.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that `department` and `salary` have been split into various binary
    features.
  prefs: []
  type: TYPE_NORMAL
- en: The final step to prepare our data for machine learning is scaling the features,
    but for various reasons (for example, some models do not require scaling), we'll
    do it as part of the model-training workflow in the next activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have completed the data preprocessing and are ready to move on to training models!
    Let''s save our preprocessed data by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, we pause here to note how well the Jupyter Notebook suited our needs
    when performing this initial data analysis and clean-up. Imagine, for example,
    we left this project in its current state for a few months. Upon returning to
    it, we would probably not remember what exactly was going on when we left it.
    Referring back to this notebook though, we would be able to retrace our steps
    and quickly recall what we previously learned about the data. Furthermore, we
    could update the data source with any new data and re-run the notebook to prepare
    the new set of data for use in our machine learning algorithms. Recall that in
    this situation, it would be best to make a copy of the notebook fist, so as not
    to lose the initial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we''ve learned and applied methods for preparing to train a machine
    learning model. We started by discussing steps for identifying a problem that
    can be solved with predictive analytics. This consisted of:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the available data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the business needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the data for suitability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also discussed how to identify supervised versus unsupervised and regression
    versus classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying our problem, we learned techniques for using Jupyter Notebooks
    to build and test a data transformation pipeline. These techniques included methods
    and best practices for filing missing data, transforming categorical features,
    and building train/test data sets.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will use this preprocessed data to train
    a variety of classification models. To avoid blindly applying algorithms we don't
    understand, we start by introducing them and overviewing how they work. Then,
    we use Jupyter to train and compare their predictive capabilities. Here, we have
    the opportunity to discuss more advanced topics in machine learning like overfitting,
    k-fold cross-validation, and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Training Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've already seen in the previous chapter, using libraries such as scikit-learn
    and platforms such as Jupyter, predictive models can be trained in just a few
    lines of code. This is possible by abstracting away the difficult computations
    involved with optimizing model parameters. In other words, we deal with a black
    box where the internal operations are hidden instead. With this simplicity also
    comes the danger of misusing algorithms, for example, by over fitting during training
    or failing to properly test on unseen data. We'll show how to avoid these pitfalls
    while training classification models and produce trustworthy results with the
    use of k-fold cross validation and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Classification Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the two types of supervised machine learning: regression and classification.
    In regression, we predict a continuous target variable. For example, recall the
    linear and polynomial models from the fist chapter. In this chapter, we focus
    on the other type of supervised machine learning: classification. Here, the goal
    is to predict the class of a sample using the available metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, there are only two possible classes, which means we are
    doing binary classification. This is the case for the example problem in this
    chapter, where we try to predict whether an employee has left or not. If we have
    more than two class labels instead, we are doing multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is little difference between binary and multi-class classification
    when training models with scikit-learn, what's done inside the "black box" is
    notably different. In particular, multi-class classification models often use
    the one-versus-rest method. This works as follows for a case with three class
    labels. When the model is "fit" with the data, three models are trained, and each
    model predicts whether the sample is part of an individual class or part of some
    other class. This might bring to mind the one-hot encoding for features that we
    did earlier. When a prediction is made for a sample, the class label with the
    highest confidence level is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll train three types of classification models: Support
    Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of
    these algorithms are quite different. As we will see, however, they are quite
    similar to train and use for predictions thanks to scikit-learn. Before swapping
    over to the Jupyter Notebook and implementing these, we''ll briefly see how they
    work. SVM''s attempt to find the best hyperplane to divide classes by. This is
    done by maximizing the distance between the hyperplane and the closest samples
    of each class, which are called support vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: This linear method can also be used to model nonlinear classes using the kernel
    trick. This method maps the features into a higher-dimensional space in which
    the hyper plane is determined. This hyperplane we've been talking about is also
    referred to as the decision surface, and we'll visualize it when training our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: k-Nearest Neighbors classification algorithms memorize the training data and
    make predictions depending on the K nearest samples in the feature space. With
    three features, this can be visualized as a sphere surrounding the prediction
    sample. Often, however, we are dealing with more than three features and therefore
    hyperspheres are drawn to find the closest K samples.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests are an ensemble of decision trees, where each has been trained
    on different subsets of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree algorithm classifies a sample based on a series of decisions.
    For example, the fist decision might be "if feature x_1 is less than or greater
    than 0." The data would then be split on this condition and fed into descending
    branches of the tree. Each step in the decision tree is decided based on the feature
    split that maximizes the information gain.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this term describes the mathematics that attempts to pick the best
    possible split of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Random Forest consists of creating bootstrapped (that is, randomly
    sampled data with replacement) datasets for a set of decision trees. Predictions
    are then made based on the majority vote. These have the benefit of less overfitting
    and better generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees can be used to model a mix of continuous and categorical data,
    which make them very useful. Furthermore, as we will see later in this chapter,
    the tree depth can be limited to reduce overfitting. For a detailed (but brief)
    look into the decision tree algorithm, check out this popular Stack Overflw answer:
    [https://stackoverflow. com/a/1859910/3511819](https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain/1859910#1859910).
    There, the author shows a simple example and discusses concepts such as node purity,
    information gain, and entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: Training two-feature classification models with scikitlearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll continue working on the employee retention problem that we introduced
    in the fist topic. We previously prepared a dataset for training a classification
    model, in which we predicted whether an employee has left or not. Now, we''ll
    take that data and use it to train classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have not already done so, start the `NotebookApp` and open the `chapter-2-workbook.ipynb
    file`. Scroll down to `Topic Training classification models`. Run the fist couple
    of cells to set the default fiure size and load the processed data that we previously
    saved to a `CSV file`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this example, we''ll be training classification models on two continuous
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`satisfaction_level` and `last_evaluation.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw the bivariate and univariate graphs of the continuous target variables
    by running the cell with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1e1e552c-adc3-4adf-8df2-b8416adbf185.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding image, there are some very distinct patterns
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Re-plot the bivariate distribution, segmenting on the target variable, by running
    the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/64b9bb12-90af-4e78-bf54-a418f9543255.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see how the patterns are related to the target variable. For the remainder
    of this section, we'll try to exploit these patterns to train effective classification
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm,
    are most effective when the input data is scaled so that all of the features are
    on the same order. We'll accomplish this with scikit-learn's `StandardScaler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `StandardScaler` and create a new instance, as referenced by the scaler variable.
    Fit the `scaler` on the training set and transform it. Then, transform the test set.
    Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: An easy mistake to make when doing machine learning is to "fit" the scaler on
    the whole dataset, when in fact it should only be "fit" to the training data.
    For example, scaling the data before splitting into training and testing sets
    is a mistake. We don't want this because the model training should not be influenced
    in any way by the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the scikit-learn support vector machine class and fit the model on the
    training data by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Then, we train a linear SVM classification model. The C parameter controls the
    penalty for misclassification, allowing the variance and bias of the model to
    be controlled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the accuracy of this model on unseen data by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We predict the targets for our test samples and then use scikit-learn's `accuracy_
    score` function to determine the accuracy. The result looks promising at ~75%!
    Not bad for our first model. Recall, though, the target is imbalanced. Let's see
    how accurate the predictions are for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the confusion matrix and then determine the accuracy within each
    class by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It looks like the model is simply classifying every sample as 0, which is clearly
    not helpful at all. Let's use a contour plot to show the predicted class at each
    point in the feature space. This is commonly known as the decision-regions plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the decision regions using a helpful function from the `mlxtend` library.
    Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c998fa17-26e7-4a5a-8e40-4d599eebc52f.png)'
  prefs: []
  type: TYPE_IMG
- en: The function plots decision regions along with a set of samples passed as arguments.
    In order to see the decision regions properly without too many samples obstructing
    our view, we pass only a 200-sample subset of the test data to the`plot_ decision_regions`
    function. In this case, of course, it does not matter. We see the result is entirely
    red, indicating every point in the feature space would be classified as 0.
  prefs: []
  type: TYPE_NORMAL
- en: It shouldn't be surprising that a linear model can't do a good job of describing
    these nonlinear patterns. Recall earlier we mentioned the kernel trick for using
    SVM's to classify nonlinear problems. Let's see if doing this can improve the
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the docstring for scikit-learn''s SVM by running the cell containing
    SVC. Scroll down and check out the parameter descriptions. Notice the `kernel`
    option, which is actually enabled by default as `rbf`. Use this kernel option
    to train a new SVM by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In order to assess this and future model performance more easily, let's define
    a function called `check_model_fit`, which computes various metrics that we can
    use to compare the models. Run the cell where this function is defied.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each computation done in this function has already been seen in this example;
    it simply calculates accuracies and plots the decision regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Show the newly trained kernel-SVM results on the training data by running the
    cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/82fd597b-ca0e-452a-981b-688c63eadbc0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e509aa55-12cd-47ce-953c-da0043bc4b5a.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is much better. Now, we are able to capture some of the non-linear patterns
    in the data and correctly classify the majority of the employees who have left.
  prefs: []
  type: TYPE_NORMAL
- en: The plot_decision_regions Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `plot_decision_regions` function is provided by `mlxtend`, a Python library
    developed by Sebastian *Raschka*. It's worth taking a peek at the source code
    (which is of course written in Python) to understand how these plots are drawn.
    It's really not too complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter Notebook, import the function with from `mlxtend.plotting` import
    `plot_ decision_regions`, and then pull up the help with `plot_decision_regions?`
    and scroll to the bottom to see the local file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e86d69a7-2040-42ae-9d66-8be5df420c35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, open up the file and check it out! For example, you could run cat in
    the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00b708c7-fe1c-4dd7-9efa-89912901b660.png)'
  prefs: []
  type: TYPE_IMG
- en: This is okay, but not ideal as there's no color markup for the code. It's better
    to copy it (so you don't accidentally alter the original) and open it with your
    favorite text editor.
  prefs: []
  type: TYPE_NORMAL
- en: When drawing attention to the code responsible for mapping the decision regions,
    we see a contour plot of predictions `Z` over an array `X_predict` that spans
    the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fbc10b9-e569-4e2c-ab8f-6c81dcc7f82c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s move on to the next model: k-Nearest Neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: Training k-nearest neighbors for our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the scikit-learn KNN classification model and print the docstring by running the
    cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `n_neighbors` parameter decides how many samples to use when making a classification.
    If the weights parameter is set to uniform, then class labels are decided by majority
    vote. Another useful choice for the weights is distance, where closer samples
    have a higher weight in the voting. Like most model parameters, the best choice
    for this depends on the particular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the KNN classifier with `n_neighbors=3`, and then compute the accuracy
    and decision regions. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3a2dc2b3-be30-45cf-938f-143f97d0bdae.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/581d3d92-766f-410c-91b7-9c4d89c603c3.png)'
  prefs: []
  type: TYPE_IMG
- en: We see an increase in overall accuracy and a significant improvement for class
    1 in particular. However, the decision region plot would indicate we are overfitting
    the data. This is evident by the hard, "choppy" decision boundary, and small pockets
    of blue everywhere. We can soften the decision boundary and decrease overfitting
    by increasing the number of nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a KNN model with `n_neighbors=25` by running the cell containing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/60bcd33b-7a68-47dd-b3f6-89b6e32e3043.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c5e825a1-6d29-4ddb-977b-78cd61dcce20.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the decision boundaries are significantly less choppy, and there
    are far less pockets of blue. The accuracy for class 1 is slightly less, but we
    would need to use a more comprehensive method such as k-fold cross validation
    to decide if
  prefs: []
  type: TYPE_NORMAL
- en: there's a significant difference between the two models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that increasing `n_neighbors` has no effect on training time, as the model
    is simply memorizing the data. The prediction time, however, will be greatly affected.
  prefs: []
  type: TYPE_NORMAL
- en: When doing machine learning with real-world data, it's important for the algorithms
    to run quick enough to serve their purposes. For example, a script to predict
    tomorrow's weather that takes longer than a day to run is completely useless!
    Memory is also a consideration that should be taken into account when dealing
    with substantial amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observe how similar it is to train and make predictions on each model, despite
    them each being so different internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a Random Forest classification model composed of 50 decision trees, each with
    a max depth of 5\. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/14e57a88-7257-4dca-ade2-8f71dbf835dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3383eccc-dfd2-4adc-adb3-9a3d11a74b87.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the distinctive axes-parallel decision boundaries produced by decision
    tree machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We can access any of the individual decision trees used to build the Random
    Forest. These trees are stored in the `estimators_attribute` of the model. Let's
    draw one of these decision trees to get a feel for what's going on. Doing this
    requires the **graph** viz dependency, which can sometimes be difficult to install.
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw one of the decision trees in the Jupyter Notebook by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6993f559-1315-4153-bf82-10085abc0ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that each path is limited to five nodes as a result of setting `max_depth=5`.
    The orange boxes represent predictions of `no` (has not left the company), and
    the blue boxes represent `yes` (has left the company). The shade of each box (light,
    dark, and so on) indicates the confidence level, which is related to the `gini`
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we have accomplished two of the learning objectives in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: We gained a qualitative understanding of support vector machines (SVMs), k-Nearest
    Neighbor classifiers (kNNs), and Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now able to train a variety of models using scikit-learn and Jupyter
    Notebooks so that we can confidently build and compare predictive models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In particular, we used the preprocessed data from our employee retention problem
    to train classification models to predict whether an employee has left the company
    or not. For the purposes of keeping things simple and focusing on the algorithms,
    we built models to predict this given only two features: the satisfaction level
    and last evaluation value. This two-dimensional feature space also allowed us
    to visualize the decision boundaries and identify what overfitting looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we will introduce two important topics in machine
    learning: k-fold cross-validation and validation curves'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Models with k-Fold Cross-Validation and Validation Curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we have trained models on a subset of the data and then assessed performance
    on the unseen portion, called the test set. This is good practice because the
    model performance on training data is not a good indicator of its effectiveness
    as a predictor. It's very easy to increase accuracy on a training dataset by overfitting
    a model, which can result in poorer performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: That said, simply training models on data split in this way is not good enough.
    There is a natural variance in data that causes accuracies to be different (if
    even slightly) depending on the training and test splits. Furthermore, using only
    one training/test split to compare models can introduce bias towards certain models
    and lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**k-fold** **cross validation** offers a solution to this problem and allows
    the variance to be accounted for by way of an error estimate on each accuracy
    calculation. This, in turn, naturally leads to the use of validation curves for
    tuning model parameters. These plot the accuracy as a function of a hyper parameter
    such as the number of decision trees used in a Random Forest or the max depth.'
  prefs: []
  type: TYPE_NORMAL
- en: This is our fist time using the term hyperparameter. It references a parameter
    that is defined when initializing a model, for example, the C parameter of the
    SVM. This is in contradistinction to a parameter of the trained model, such as
    the equation of the decision boundary hyperplane for a trained SVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is illustrated in the following diagram, where we see how the k-folds
    can be selected from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f874d66c-9bf3-4666-8a1d-862230c92a86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The k-fold cross validation algorithm goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split data into k "folds" of near-equal size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test and train k models on different fold combinations. Each model will include
    *k - 1* folds of training data and the left-out fold is used for testing. In this
    method, each fold ends up being used as the validation data exactly once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the model accuracy by taking the mean of the k values. The standard deviation
    is also calculated to provide error bars on the value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's standard to set *k = 10*, but smaller values for k should be considered
    if using a big data set.
  prefs: []
  type: TYPE_NORMAL
- en: This validation method can be used to reliably compare model performance with
    different hyperparameters (for example, the C parameter for an SVM or the number
    of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely
    different models.
  prefs: []
  type: TYPE_NORMAL
- en: Once the best model has been identified, it should be re-trained on the entirety
    of the dataset before being used to predict actual classifications.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing this with scikit-learn, it's common to use a slightly improved
    variation of the normal k-fold algorithm instead. This is called stratified k-fold.
    The improvement is that stratified k-fold cross validation maintains roughly even
    class label populations in the folds. As you can imagine, this reduces the overall
    variance in the models and decreases the likelihood of highly unbalanced models
    causing bias.
  prefs: []
  type: TYPE_NORMAL
- en: Validation curves are plots of a training and validation metric as a function
    of some model parameter. They allow to us to make good model parameter selections.
    In this book, we will use the accuracy score as our metric for these plots.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for plot validation curves is available here:[ http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this validation curve, where the accuracy score is plotted as a function
    of the gamma SVM parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5981de6-b9f2-4114-8744-ea06e9a7a8ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting on the left side of the plot, we can see that both sets of data are
    agreeing on the score, which is good. However, the score is also quite low compared
    to other gamma values, so therefore we say the model is underfitting the data.
    Increasing the gamma, we can see a point where the error bars of these two lines
    no longer overlap. From this point on, we see the classifier overfitting the data
    as the models behave increasingly well on the training set compared to the validation
    set. The optimal value for the gamma parameter can be found by looking for a high
    validation score with overlapping error bars on the two lines.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that a learning curve for some parameter is only valid while the
    other parameters remain constant. For example, if training the SVM in this plot,
    we could decide to pick gamma on the order of 10-4\. However, we may want to optimize
    the C parameter as well. With a different value for C, the preceding plot would
    be different and our selection for gamma may no longer be optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-fold cross validation and validation curves in Python with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you've not already done so, start the `NotebookApp` and open the `chapter-2-
    workbook.ipynb file`. Scroll down to Subtopic `K-fold cross-validation` and `validation
    curves`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training data should already be in the notebook's memory, but let's reload
    it as a reminder of what exactly we're working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the data and select the `satisfaction_level` and `last_evaluation` features for
    the training/validation set. We will not use the train-test split this time because we
    are going to use k-fold validation instead. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a Random Forest model by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: To train the model with stratified k-fold cross validation, we'll use the `model_
    selection.cross_val_score function`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train 10 variations of our model `clf` using stratified k-fold validation.
    Note that scikit-learn''s c`ross_val_score` does this type of validation by default.
    Run the cell containing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note how we use `np.random.seed` to set the seed for the random number generator,
    therefore ensuring reproducibility with respect to the randomly selected samples
    for each fold and decision tree in the Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this method, we calculate the accuracy as the average of each fold. We
    can also see the individual accuracies for each fold by printing scores. To see
    these, run `print(scores)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `cross_val_score` is very convenient, but it doesn''t tell us about the
    accuracies within each class. We can do this manually with the `model_selection`.
    `StratifiedKFold` class. This class takes the number of folds as an initialization
    parameter, then the split method is used to build randomly sampled "masks" for
    the data. A mask is simply an array containing indexes of items in another array,
    where the items can then be returned by doing this: `data[mask]` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a custom class for calculating k-fold cross validation class accuracies.
    Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then calculate the class accuracies with code that''s very similar to
    step 4\. Do this by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see the class accuracies for each fold! Pretty neat, right?
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to show how a validation curve can be calculated using `model_
    selection.validation_curve`. This function uses stratified k-fold cross validation
    to train models for various values of a given parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do the calculations required to plot a validation curve by training Random
    Forests over a range of max_depth values. Run the cell containing the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This will return arrays with the cross validation scores for each model, where
    the models have different max depths. In order to visualize the results, we'll
    leverage a function provided in the scikit-learn documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell in which `plot_validation_curve` is defined. Then, run the cell
    containing the following code to draw the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/47959f19-1125-4c31-bae3-2c3c929d2941.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall how setting the max depth for decision trees limits the amount of overfitting?
    This is reflected in the validation curve, where we see overfitting taking place
    for large max depth values to the right. A good value for `max_depth` appears
    to be `6`, where we see the training and validation accuracies in agreement. When
    `max_depth` is equal to `3`, we see the model underfitting the data as training
    and validation accuracies are lower.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have learned and implemented two important techniques for building
    reliable predictive models. The fist such technique was k-fold cross-validation,
    which is used to split the data into various train/test batches and generate a
    set accuracy. From this set, we then calculated the average accuracy and the standard
    deviation as a measure of the error. This is important so that we have a gauge
    of the variability of our model and we can produce trustworthy accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also learned about another such technique to ensure we have trustworthy
    results: validation curves. These allow us to visualize when our model is overfitting
    based on comparing training and validation accuracies. By plotting the curve over
    a range of our selected hyperparameter, we are able to identify its optimal value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final section of this chapter, we take everything we have learned so
    far and put it together in order to build our final predictive model for the employee
    retention problem. We seek to improve the accuracy, compared to the models trained
    thus far, by including all of the features from the dataset in our model. We''ll
    see now-familiar topics such as k-fold cross-validation and validation curves,
    but we''ll also introduce something new: dimensionality reduction techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction can simply involve removing unimportant features from
    the training data, but more exotic methods exist, such as **Principal Component
    Analysis (PCA)** and **Linear Discriminant Analysis (LDA)**. These techniques
    allow for data compression, where the most important information from a large
    group of features can be encoded in just a few features.
  prefs: []
  type: TYPE_NORMAL
- en: In this subtopic, we'll focus on PCA. This technique transforms the data by
    projecting it into a new subspace of orthogonal "principal components," where
    the components with the highest eigenvalues encode the most information for training
    the model. Then, we can simply select a few of these principal components in place
    of the original high-dimensional dataset. For example, PCA could be used to encode
    the information from every pixel in an image. In this case, the original feature
    space would have dimensions equal to the number of pixels in the image. This high-dimensional
    space could then be reduced with PCA, where the majority of useful information
    for training predictive models might be reduced to just a few dimensions. Not
    only does this save time when training and using models, it allows them to perform
    better by removing noise in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Like the models we've seen, it's not necessary to have a detailed understanding
    of PCA in order to leverage the benefits. However, we'll dig into the technical
    details of PCA just a bit further so that we can conceptualize it better. The
    key insight of PCA is to identify patterns between features based on correlations,
    so the PCA algorithm calculates the co variance matrix and then decomposes this
    into eigen vectors and eigenvalues. The vectors are then used to transform the
    data into a new subspace, from which a filed number of principal components can
    be selected.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we'll see an example of how PCA can be used to improve
    our Random Forest model for the employee retention problem we have been working
    on. This will be done after training a classification model on the full feature
    space, to see how our accuracy is affected by dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Training a predictive model for the employee retention problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already spent considerable effort planning a machine learning strategy,
    preprocessing the data, and building predictive models for the employee retention
    problem. Recall that our business objective was to help the client prevent employees
    from leaving. The strategy we decided upon was to build a classification model
    that would predict the probability of employees leaving. This way, the company
    can assess the likelihood of current employees leaving and take action to prevent
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our strategy, we can summarize the type of predictive modeling we are
    doing as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning on labeled training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification problems with two class labels (binary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, we are training models to determine whether an employee has left
    the company, given a set of continuous and categorical features. After preparing
    the data for machine learning in Activity, *Preparing to Train a Predictive Model
    for the Employee-Retention Problem*, we went on to implement SVM, k-Nearest Neighbors,
    and Random Forest algorithms using just two features. These models were able to
    make predictions with over 90% overall accuracy. When looking at the specific
    class accuracies, however, we found that employees who had left (`class-label
    1`) could only be predicted with 70-80% accuracy. Let's see how much this can
    be improved by utilizing the full feature space.
  prefs: []
  type: TYPE_NORMAL
- en: In the `chapter-2-workbook.ipynb notebook`, scroll down to the code for this
    section. We should already have the preprocessed data loaded from the previous
    sections, but this can be done again, if desired, by executing `df = pd.read_
    csv('../data/hr-analytics/hr_data_processed.csv')` . Then, print the DataFrame
    columns with `print(df.columns)` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define list of all the features by copy and pasting the output from `df.columns` into
    a new list (making sure to remove the target variable left). Then, define `X`
    and `Y` as we have done before. This goes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the feature names, recall what the values look like for each one.
    Scroll up to the set of histograms we made in the first activity to help jog your
    memory. The first two features are continuous; these are what we used for training
    models in the previous two exercises. After that, we have a few discrete features,
    such as `number_ project` and `time_spend_company`, followed by some binary fields
    such as `work_ accident` and `promotion_last_5years`. We also have a bunch of
    binary features, such as `department_IT` and `department_accounting`, which were
    created by one hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Given a mix of features like this, Random Forests are a very attractive type
    of model. For one thing, they're compatible with feature sets composed of both
    continuous and categorical data, but this is not particularly special; for instance,
    an SVM can be trained on mixed feature types as well (given proper preprocessing).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re interested in training an SVM or k-Nearest Neighbors classifier
    on mixed-type input features, you can use the data-scaling prescription from this
    StackExchange answer: [https://stats.stackexchange. com/questions/82923/mixing-continuous-and-binary-datawith-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple approach would be to preprocess data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize continuous variables
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot-encode categorical features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shift binary values to `-1` and 1 instead of `0` and `1`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the mixed-feature data could be used to train a variety of classification
    models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to figure out the best parameters for our Random Forest model. Let''s
    start by tuning the`max_depth` hyperparameter using a validation curve. Calculate
    the training and validation accuracies by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We are testing 10 models with k-fold cross validation. By setting `k = 5`, we
    produce five estimates of the accuracy for each model, from which we extract the
    mean and standard deviation to plot in the validation curve. In total, we train
    50 models, and since `n_estimators` is set to 20, we are training a total of 1,000
    decision trees! All in roughly 10 seconds!
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the validation curve using our custom plot_validation_curve function from
    the last exercise. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d53046ce-fa55-44c4-a852-eab90b5f9500.png)'
  prefs: []
  type: TYPE_IMG
- en: For small max depths, we see the model underfitting the data. Total accuracies
    dramatically increase by allowing the decision trees to be deeper and encode more
    complicated patterns in the data. As the max depth is increased further and the
    accuracy approaches 100%, we find the model overfits the data, causing the training
    and validation accuracies to grow apart. Based on this figure, let's select a
    `max_ depth` of 6 for our model.
  prefs: []
  type: TYPE_NORMAL
- en: We should really do the same for `n_estimators`, but in the spirit of saving
    time, we'll skip it. You are welcome to plot it on your own; you should find agreement
    between training and validation sets for a large range of values. Usually, it's
    better to use more decision tree estimators in the Random Forest, but this comes
    at the cost of increased training times. We'll use 200 estimators to train our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `cross_val_class_score`, the k-fold cross validation by class function
    we created earlier, to test the selected model, a Random Forest with `max_depth
    = 6` and `n_estimators = 200`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The accuracies are way higher now that we're using the full feature set, compared
    to before when we only had the two continuous features!
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the accuracies with a boxplot by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forests can provide an estimate of the feature performances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature importance in scikit-learn is calculated based on how the node
    impurity changes with respect to each feature. For a more detailed explanation,
    take a look at the following Stack Overflow thread about how feature importance
    is determined in Random Forest Classifier: [https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the feature importance, as stored in the attribute `feature_importances_`,
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a84fa4bd-e710-4a80-bc11-a6081cee8370.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It doesn''t look like we''re getting much in the way of useful contribution
    from the one-hot encoded variables: department and salary. Also, the p`romotion_
    last_5years` and `work_accident` features don''t appear to be very useful.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's use Principal Component Analysis (PCA) to condense all of these weak features
    into just a few principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the PCA class from scikit-learn and transform the features. Run the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the string representation of `X_pca` by typing it alone and executing
    the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Since we asked for the top three components, we get three vectors returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the new features to our DataFrame with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Select our reduced-dimension feature set to train a new Random Forest with. Run
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Assess the new model''s accuracy with k-fold cross validation. This can be
    done by running the same code as before, where X now points to different features.
    The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the result in the same way as before, using a box plot. The code
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0bda50cd-77fc-4490-a3d6-d27fa4e65b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing this to the previous result, we find an improvement in the class 1
    accuracy! Now, the majority of the validation sets return an accuracy greater
    than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6%
    prior to dimensionality reduction!
  prefs: []
  type: TYPE_NORMAL
- en: Let's select this as our final model. We'll need to re-train it on the full
    sample space before using it in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the final predictive model by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the trained model to a binary file using `externals.joblib.dump`. Run
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that it''s saved into the working directory, for example, by running:
    `!ls *.pkl`. Then, test that we can load the model from the file by running the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! We've trained the final predictive model! Now, let's see an example
    of how it can be used to provide business insights for the client.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have a particular employee, who we'll call Sandra. Management has noticed
    she is working very hard and reported low job satisfaction in a recent survey.
    They would therefore like to know how likely it is that she will quit.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, let's take her feature values as a sample from the
    training set (but pretend that this is unseen data instead).
  prefs: []
  type: TYPE_NORMAL
- en: 'List the feature values for Sandra by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to ask the model which group it thinks she should be in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the class label for Sandra by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The model classifies her as having already left the company; not a good sign!
    We can take this a step further and calculate the probabilities of each class
    label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `clf.predict_proba` to predict the probability of our model predicting
    that Sandra has quit. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We see the model predicting that she has quit with 93% accuracy.Since this is
    clearly a red flag for management, they decide on a plan to reduce her number
    of monthly hours to 100 and the time spent at the company to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the new probabilities with Sandra''s newly planned metrics. Run the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Excellent! We can now see that the model returns a mere 38% likelihood that
    she has quit! Instead, it now predicts she will not have left the company.
  prefs: []
  type: TYPE_NORMAL
- en: Our model has allowed management to make a data-driven decision. By reducing
    her amount of time with the company by this particular amount, the model tells
    us that she will most likely remain an employee at the company!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how predictive models can be trained in Jupyter
    Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we talked about how to plan a machine learning strategy. We thought
    about how to design a plan that can lead to actionable business insights and stressed
    the importance of using the data to help set realistic business goals. We also
    explained machine learning terminology such as supervised learning, unsupervised
    learning, classification, and regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we discussed methods for preprocessing data using scikit-learn and pandas.
    This included lengthy discussions and examples of a surprisingly time-consuming
    part of machine learning: dealing with missing data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of the chapter, we trained predictive classification models
    for our binary problem, comparing how decision boundaries are drawn for various
    models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed
    how validation curves can be used to make good parameter choices and how dimensionality
    reduction can improve model performance. Finally, at the end of our activity,
    we explored how the final model can be used in practice to make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
