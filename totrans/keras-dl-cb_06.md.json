["```py\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nimport seaborn as sns\nfrom tensorflow.python.training.gradient_descent import GradientDescentOptimizer\n\nsns.set(color_codes=True)\n\nseed = 42\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# gaussian data distribution\nclass DataDist(object):\n    def __init__(self):\n        self.mue = 4\n        self.sigma = 0.5\n\n    def sample(self, N):\n        samples = np.random.normal(self.mue, self.sigma, N)\n        samples.sort()\n        return samples\n\n# data distribution with noise\nclass GeneratorDist(object):\n    def __init__(self, rnge):\n        self.rnge = rnge\n\n    def sample(self, N):\n        return np.linspace(-self.rnge, self.rnge, N) + \\\n               np.random.random(N) * 0.01\n\n# linear method\ndef linearUnit(input, output_dim, scope=None, stddev=1.0):\n    with tf.variable_scope(scope or 'linearUnit'):\n        weight = tf.get_variable(\n            'weight',\n            [input.get_shape()[1], output_dim],\n            initializer=tf.random_normal_initializer(stddev=stddev)\n        )\n        bias = tf.get_variable(\n            'bias',\n            [output_dim],\n            initializer=tf.constant_initializer(0.0)\n        )\n        return tf.matmul(input, weight) + bias\n\n# generator network\ndef generatorNetwork(input, hidden_size):\n    hidd0 = tf.nn.softplus(linearUnit(input, hidden_size, 'g0'))\n    hidd1 = linearUnit(hidd0, 1, 'g1')\n    return hidd1\n\n# discriminator network\ndef discriminatorNetwork(input, h_dim, minibatch_layer=True):\n    hidd0 = tf.nn.relu(linearUnit(input, h_dim * 2, 'd0'))\n    hidd1 = tf.nn.relu(linearUnit(hidd0, h_dim * 2, 'd1'))\n\n    if minibatch_layer:\n        hidd2 = miniBatch(hidd1)\n    else:\n        hidd2 = tf.nn.relu(linearUnit(hidd1, h_dim * 2, scope='d2'))\n\n    hidd3 = tf.sigmoid(linearUnit(hidd2, 1, scope='d3'))\n    return hidd3\n\n# minibatch\ndef miniBatch(input, numKernels=5, kernelDim=3):\n    x = linearUnit(input, numKernels * kernelDim, scope='minibatch', stddev=0.02)\n    act = tf.reshape(x, (-1, numKernels, kernelDim))\n    differences = tf.expand_dims(act, 3) - \\\n            tf.expand_dims(tf.transpose(act, [1, 2, 0]), 0)\n    absDiffs = tf.reduce_sum(tf.abs(differences), 2)\n    minibatchFeatures = tf.reduce_sum(tf.exp(-absDiffs), 2)\n    return tf.concat([input, minibatchFeatures], 1)\n\n# optimizer\ndef optimizer(loss, var_list):\n    learning_rate = 0.001\n    step = tf.Variable(0, trainable=False)\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n        loss,\n        global_step=step,\n        var_list=var_list\n    )\n    return optimizer\n\n# log\ndef log(x):\n    return tf.log(tf.maximum(x, 1e-5))\n\nclass GAN(object):\n    def __init__(self, params):\n        with tf.variable_scope('Generator'):\n            self.zee = tf.placeholder(tf.float32, shape=(params.batchSize, 1))\n            self.Gee = generatorNetwork(self.zee, params.hidden_size)\n\n        self.xVal = tf.placeholder(tf.float32, shape=(params.batchSize, 1))\n        with tf.variable_scope('Discriminator'):\n            self.Dis1 = discriminatorNetwork(\n                self.xVal,\n                params.hidden_size,\n                params.minibatch\n            )\n        with tf.variable_scope('D', reuse=True):\n            self.Dis2 = discriminatorNetwork(\n                self.Gee,\n                params.hidden_size,\n                params.minibatch\n            )\n\n        self.lossD = tf.reduce_mean(-log(self.Dis1) - log(1 - self.Dis2))\n        self.lossG = tf.reduce_mean(-log(self.Dis2))\n\n        vars = tf.trainable_variables()\n        self.dParams = [v for v in vars if v.name.startswith('D/')]\n        self.gParams = [v for v in vars if v.name.startswith('G/')]\n\n        self.optD = optimizer(self.lossD, self.dParams)\n        self.optG = optimizer(self.lossG, self.gParams)\n\n'''\nTrain GAN model\n'''\ndef trainGan(model, data, gen, params):\n    animFrames = []\n\n    with tf.Session() as session:\n        tf.local_variables_initializer().run()\n        tf.global_variables_initializer().run()\n\n        for step in range(params.numSteps + 1):\n            x = data.sample(params.batchSize)\n            z = gen.sample(params.batchSize)\n            lossD, _, = session.run([model.lossD, model.optD], {\n                model.x: np.reshape(x, (params.batchSize, 1)),\n                model.z: np.reshape(z, (params.batchSize, 1))\n            })\n\n            z = gen.sample(params.batchSize)\n            lossG, _ = session.run([model.lossG, model.optG], {\n                model.z: np.reshape(z, (params.batchSize, 1))\n            })\n\n            if step % params.log_every == 0:\n                print('{}: {:.4f}\\t{:.4f}'.format(step, lossD, lossG))\n\n            if params.animPath and (step % params.animEvery == 0):\n                animFrames.append(\n                    getSamples(model, session, data, gen.range, params.batchSize)\n                )\n\n        if params.animPath:\n            saveAnimation(animFrames, params.animPath, gen.range)\n        else:\n            samps = getSamples(model, session, data, gen.range, params.batchSize)\n            plotDistributions(samps, gen.range)\n\ndef getSamples(\n        model,\n        session,\n        data,\n        sampleRange,\n        batchSize,\n        numPoints=10000,\n        numBins=100\n):\n    xs = np.linspace(-sampleRange, sampleRange, numPoints)\n    binss = np.linspace(-sampleRange, sampleRange, numBins)\n\n    # decision boundary\n    db = np.zeros((numPoints, 1))\n    for i in range(numPoints // batchSize):\n        db[batchSize * i:batchSize * (i + 1)] = session.run(\n            model.D1,\n            {\n                model.x: np.reshape(\n                    xs[batchSize * i:batchSize * (i + 1)],\n                    (batchSize, 1)\n                )\n            }\n        )\n\n    # data distribution\n    d = data.sample(numPoints)\n    pds, _ = np.histogram(d, bins=binss, density=True)\n\n    zs = np.linspace(-sampleRange, sampleRange, numPoints)\n    g = np.zeros((numPoints, 1))\n    for i in range(numPoints // batchSize):\n        g[batchSize * i:batchSize * (i + 1)] = session.run(\n            model.G,\n            {\n                model.z: np.reshape(\n                    zs[batchSize * i:batchSize * (i + 1)],\n                    (batchSize, 1)\n                )\n            }\n        )\n    pgs, _ = np.histogram(g, bins=binss, density=True)\n\n    return db, pds, pgs\n\ndef plotDistributions(samps, sampleRange):\n    db, pd, pg = samps\n    dbX = np.linspace(-sampleRange, sampleRange, len(db))\n    pX = np.linspace(-sampleRange, sampleRange, len(pd))\n    f, ax = plt.subplots(1)\n    ax.plot(dbX, db, label='Decision Boundary')\n    ax.set_ylim(0, 1)\n    plt.plot(pX, pd, label='Real Data')\n    plt.plot(pX, pg, label='Generated Data')\n    plt.title('1D Generative Adversarial Network')\n    plt.xlabel('Data Values')\n    plt.ylabel('Probability Density')\n    plt.legend()\n    plt.show()\n\ndef saveAnimation(animFrames, animPath, sampleRange):\n    f, ax = plt.subplots(figsize=(6, 4))\n    f.suptitle('1D GAN', fontsize=15)\n    plt.xlabel('dataValues')\n    plt.ylabel('probabilityDensity')\n    ax.set_xlim(-6, 6)\n    ax.set_ylim(0, 1.4)\n    lineDb, = ax.plot([], [], label='decision boundary')\n    linePd, = ax.plot([], [], label='real data')\n    linePg, = ax.plot([], [], label='generated data')\n    frameNumber = ax.text(\n        0.02,\n        0.95,\n        '',\n        horizontalalignment='left',\n        verticalalignment='top',\n        transform=ax.transAxes\n    )\n    ax.legend()\n\n    db, pd, _ = animFrames[0]\n    dbX = np.linspace(-sampleRange, sampleRange, len(db))\n    pX = np.linspace(-sampleRange, sampleRange, len(pd))\n\n    def init():\n        lineDb.set_data([], [])\n        linePd.set_data([], [])\n        linePg.set_data([], [])\n        frameNumber.set_text('')\n        return (lineDb, linePd, linePg, frameNumber)\n\n    def animate(i):\n        frameNumber.set_text(\n            'Frame: {}/{}'.format(i, len(animFrames))\n        )\n        db, pd, pg = animFrames[i]\n        lineDb.set_data(dbX, db)\n        linePd.set_data(pX, pd)\n        linePg.set_data(pX, pg)\n        return (lineDb, linePd, linePg, frameNumber)\n\n    anim = animation.FuncAnimation(\n        f,\n        animate,\n        init_func=init,\n        frames=len(animFrames),\n        blit=True\n    )\n    anim.save(animPath, fps=30, extra_args=['-vcodec', 'libx264'])\n\n# start gan modeling\ndef gan(args):\n    model = GAN(args)\n    trainGan(model, DataDist(), GeneratorDist(range=8), args)\n\n# input arguments\ndef parseArguments():\n    argParser = argparse.ArgumentParser()\n    argParser.add_argument('--num-steps', type=int, default=5000,\n                           help='the number of training steps to take')\n    argParser.add_argument('--hidden-size', type=int, default=4,\n                           help='MLP hidden size')\n    argParser.add_argument('--batch-size', type=int, default=8,\n                           help='the batch size')\n    argParser.add_argument('--minibatch', action='store_true',\n                           help='use minibatch discrimination')\n    argParser.add_argument('--log-every', type=int, default=10,\n                           help='print loss after this many steps')\n    argParser.add_argument('--anim-path', type=str, default=None,\n                           help='path to the output animation file')\n    argParser.add_argument('--anim-every', type=int, default=1,\n                           help='save every Nth frame for animation')\n    return argParser.parse_args()\n\n# start the gan app\nif __name__ == '__main__':\n    gan(parseArguments())\n```", "```py\n0: 6.6300 0.1449\n 10: 6.5390 0.1655\n 20: 6.4552 0.1866\n 30: 6.3789 0.2106\n 40: 6.3190 0.2372\n 50: 6.2814 0.2645\n 60: 6.2614 0.2884\n 70: 6.2556 0.3036\n 80: 6.2814 0.3104\n 90: 6.2796 0.3113\n 100: 6.3008 0.3106\n 110: 6.2923 0.3112\n 120: 6.2792 0.3153\n 130: 6.3299 0.3196\n 140: 6.3512 0.3205\n 150: 6.2999 0.3197\n 160: 6.3513 0.3236\n 170: 6.3521 0.3291\n 180: 6.3377 0.3292\n```"]