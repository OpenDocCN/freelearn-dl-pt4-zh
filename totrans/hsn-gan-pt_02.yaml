- en: Generative Adversarial Networks Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) have brought about a revolutionary
    storm in the **machine learning** (**ML**) community. They, to some extent, have
    changed the way people solve practical problems in **Computer Vision** (**CV**)
    and **Natural Language Processing** (**NLP**). Before we dive right into the storm,
    let''s prepare you with the fundamental insights of GANs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will understand the idea behind adversarial learning and
    the basic components of a GAN model. You will also get a brief understanding on
    how GANs work and how it can be built with NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start exploiting the new features in PyTorch, we will first learn
    to build a simple GAN with NumPy to generate sine signals so that you may have
    a profound understanding of the mechanism beneath GANs. By the end of this chapter,
    you may relax a little as we walk you through many showcases about how GANs are
    used to address practical problems in CV and NLP fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator and discriminator networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What GAN we do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and a useful reading list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamentals of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To introduce how GANs work, let''s use an analogy:'
  prefs: []
  type: TYPE_NORMAL
- en: A long, long time ago, there were two neighboring kingdoms on an island. One
    was called Netland, and the other was called Ganland. Both kingdoms produced fine
    wine, armor, and weapons. In Netland, the king demanded that the blacksmiths who
    specialized in making armor worked at the east corner of the castle, while those
    who made swords worked at the west side so that the lords and knights could choose
    the best equipment the kingdom had to offer. The king of Ganland, on the other
    hand, put all of the blacksmiths in the same corner and demanded that the armor
    makers and sword makers should test their work against each other every day. If
    a sword broke through the armor, the sword would sell at a good price and the
    armor would be melted and reforged. If it didn't, the sword would be remade and
    men would strive to buy the armor. One day, the two kings were arguing over which
    kingdom made better wine until the quarrel escalated into war. Though outnumbered,
    the soldiers of Ganland wore the armor and swords that had been improved for years
    in the daily adversarial tests, and the Netland soldiers could not break their
    strong armor nor withstand their sharp swords. In the end, the defeated king of
    Netland, however reluctant he was, agreed that Ganland had better wine and blacksmiths.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning – classification and generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ML** is the study of recognizing patterns from data without hardcoded rules
    given by humans. The recognizing of patterns (**Pattern Recognition** or **PR**)
    is the automatic discovering of the similarities and differences among raw data,
    which is an essential way to realize **Artificial Intelligence** (**AI**) that
    only exists in novels and movies. Although it is hard to tell when exactly real
    AI will come to birth in the future, the development of ML has given us much confidence
    in recent years. ML has already been vastly used in many fields, such as CV, NLP,
    recommendation systems, **Intelligent Transportation Systems** (**ITS**), medical
    diagnoses, robotics, and advertising.'
  prefs: []
  type: TYPE_NORMAL
- en: A ML model is typically described as a system that takes in data and gives certain
    outputs based on the parameters it contains. The **learning** of the model is
    actually adjusting the parameters to get better outputs. As illustrated in the
    following diagram, we feed training data into the model and get a certain output.
    We then use one or several criteria to measure the output, to tell how well our
    model performs. In this step, a set of desired outputs (or ground truth) with
    respect to the training data would be very helpful. If ground truth data is used
    in training, this process is often called **supervised learning**. If not, it
    is often regarded as **unsupervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We constantly adjust the model''s parameters based on its performance (in other
    words, whether it gives us the results we want) so that it yields better results
    in the future. This process is called **model training**. The training of a model
    takes as long as it pleases us. Typically, we stop the training after a certain
    number of iterations or when the performance is good enough. When the training
    process has finished, we apply the trained model to predict on new data (testing
    data). This process is called **model testing**. Sometimes, people use different
    sets of data for training and testing to see how well the model performs on samples
    it never meets, which is called the **generalization** capability. Sometimes an
    additional step called **model** **evaluation **is involved, when the parameters
    of the model are so complicated that we need another set of data to see whether
    our model or training process has been designed well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d437726-177b-4779-8b14-90d062d9a810.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical machine learning system, with model training and testing
  prefs: []
  type: TYPE_NORMAL
- en: What types of problems this model can solve is essentially determined by the
    types of input and output data we want. For example, a classification model takes
    an input of any number of dimensions (audio, text, image, or video) and gives
    a 1-dimension output (single values indicating the predicted labels). A generative
    model typically takes a 1-dimension input (a latent vector) and generates high-dimension
    outputs (images, videos, or 3D models). It maps low-dimensional data to high-dimensional
    data, at the same time, trying to make the output samples look as convincing as
    possible. However, it is worth pointing out that we'll meet generative models
    that don't obey this rule in the future chapters. Until [Chapter 5](685b2621-6dbb-4157-a258-f3cf2825728c.xhtml),
    *Generating Images Based on Label Information*, it's a simple rule to bear in
    mind.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to AI, there are two groups of believers in the community. The
    symbolists acknowledge the necessity of human experience and knowledge. They believe
    the low-level patterns constitute high-level decisions based on explicit rules
    given by humans. The connectionists believe that AI can be realized by an analogous
    network similar to human neural systems and adjusting the connections between
    simple neurons is the key to this system. Apparently, the exploding development
    of deep learning adds a score to the connectionists' side. What is your opinion?
  prefs: []
  type: TYPE_NORMAL
- en: Introducing adversarial learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, generative problems are solved by statistics-based methods such
    as a **Boltzmann machine**, **Markov chain**, or **variational encoder**. As mathematically
    profound as they are, the generated samples are as of yet far from perfect. A
    classification model maps high-dimension data to low-dimension, while a generative
    model often maps low-dimension data to high-dimension ones. People in both fields
    have been working hard to improve their models. Let's look back to the little
    made-up opening story. Can we get the two different models to work against each
    other and improve themselves at the same time? If we take the output of a generative
    model as the input of the classification model, we can measure the performance
    of the generative model (the armor) with the classification model (the sword).
    At the same time, we can improve the classification model (the sword) by feeding
    generated samples (the armor) along with real samples, since we can agree that
    more data is often better for the training of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process where the two models try to weaken each other and, as
    a result, improve each other is called **adversarial learning**. As demonstrated
    in the following diagram, the models, A and B, have totally opposite agendas (for
    example, classification and generation). However, during each step of the training,
    the output of Model A improves Model B, and the output of Model B improves Model
    A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d722cae9-5f24-4757-a699-30cda072c9e3.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical adversarial learning system
  prefs: []
  type: TYPE_NORMAL
- en: '**GANs** are designed based on this very idea, which was proposed by Goodfellow,
    Pouget-Abadie, Mirza, et al in 2014\. Now, GANs have become the most thriving
    and popular method to synthesize audio, text, images, video, and 3D models in
    the ML community. In this book, we will walk you through the basic components
    and mechanisms of different types of GANs and learn how to use them to address
    various practical problems. In the next section, we will introduce the basic structure
    of GANs to show you how and why they work so well.'
  prefs: []
  type: TYPE_NORMAL
- en: Generator and discriminator networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will show you the basic components of GANs and explain how they work
    with/against each other to achieve our goal to generate realistic samples. A typical
    structure of a GAN is shown in the following diagram. It contains two different
    networks: a generator network and a discriminator network. The **generator** network
    typically takes random noises as input and generates fake samples. Our goal is
    to let the fake samples be as close to the real samples as possible. That''s where
    the discriminator comes in. The **discriminator** is, in fact, a classification
    network, whose job is to tell whether a given sample is fake or real. The generator
    tries its best to trick and confuse the discriminator to make the wrong decision,
    while the discriminator tries its best to distinguish the fake samples from the
    real ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this process, the differences between fake and real samples are used to
    improve the generator. Therefore, the generator gets better at generating realistic-looking
    samples while the discriminator gets better at picking them out. Since real samples
    are used to train the discriminator, the training process is therefore supervised.
    Even though the generator always gives fake samples without the knowledge of ground
    truth, the overall training of GAN is still **supervised**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e656f056-7176-4841-8ab2-84e6d1b0207e.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic process of a GAN
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical background of GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the math behind this process to get a better understanding
    of the mechanism. Let ![](img/e4fdcaef-f6c7-4251-a263-378e85581a51.png) and ![](img/a0f5ff27-298f-4194-b834-47e74a4de2b5.png) represent
    the generator and discriminator networks, respectively. Let ![](img/5358da90-7104-466b-be88-de95b9cb655c.png) represent
    the performance criterion of the system. The optimization objective is described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7629f5ad-ff01-49bf-9ad6-352d182d819a.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, ![](img/c7c4bd1a-b0be-4fb7-afb3-bde5f3740804.png) is the real
    sample, ![](img/84f608eb-69dc-45b5-826f-6ff15659158b.png) is the generated sample,
    and ![](img/a97b65f1-02ea-45fb-b1d4-5388d6d06533.png) is the random noise that ![](img/5e2f7c11-0834-46eb-9358-5696a9e2ab65.png) uses
    to generate fake samples. ![](img/9df998ed-dd13-4c2e-97bf-21c29d75270d.png) is
    the expectation over ![](img/2264cd61-a60b-4641-92ab-0ac367b897ee.png), which
    means the average value of any function, ![](img/a5f718a9-f8bc-4a3f-b73b-ac07079acc1e.png), over
    all samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, the goal of the discriminator, ![](img/c8f7d82c-acd3-44cc-b621-1349279a4cfb.png), is
    to maximize the prediction confidence of real samples. Therefore, ![](img/757e3f28-fb7d-427c-b8e1-fc4ee1c135cd.png) needs
    to be trained with **gradient ascent** (the ![](img/7be72e8f-bfb8-4247-8c3b-3d61c34bbbff.png) operator
    in the objective). The update rule for, ![](img/ef174eed-8739-4cb1-87ee-1d477315c13e.png), is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26319362-d37f-4fee-80bc-20ea6964f938.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, ![](img/2a2c4476-f6b1-4742-823c-987aea9ca695.png) is the parameter
    of ![](img/4126c5f6-8a46-49d0-9773-29c51ddb4bc4.png) (such as convolution kernels
    and weights in fully-connected layers), ![](img/52e8bf3e-64f4-4da7-85a4-24aa6a5f0a18.png) is
    the size of the mini batch (or batch size for short), and ![](img/e24446fd-41d9-41c5-b767-89690d10ec97.png) is
    the index of the sample in the mini-batch. Here, we assume that we are using mini-batches
    to feed the training data, which is fairly reasonable since it's the most commonly
    used and empirically effective strategy. Therefore, the gradients need to be averaged
    over ![](img/d9812252-2aa9-4ef4-b0b6-5ec52c08c78b.png) samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 different ways to feed training data into models: (1) one sample
    at a time, which is often referred to as **stochastic** (for example, **Stochastic
    Gradient Descent** or** SGD**); (2) a handful of samples at a time, which is called
    **mini**-**batch**; and (3) all samples at one time, which is, in fact, called
    **batch**. The stochastic way introduces too much randomness so that one bad sample
    could jeopardize the good work of several previous training steps. The full batch
    requires too much memory to calculate. Therefore, we feed data to all of the models
    by mini-batch in this book, even though we might slothfully refer to it as just
    batch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the generator network, ![](img/99821001-761d-489e-b507-b024acb31baf.png), is
    to fool the discriminator, ![](img/1d1ef16d-f059-4fa9-ad79-5f3bc703dd35.png), and
    let ![](img/0f2bdefe-7641-4334-8ad1-845340116f44.png) believe that the generated
    samples are real. Therefore, the training of ![](img/b23e0a77-9872-4650-b680-cc43b69a698f.png) is
    to maximize ![](img/3e009fdd-496e-490d-96ec-b4e9dd4f056e.png) or minimize ![](img/87430d00-7be9-43a2-9378-602ed797e643.png).
    Therefore, ![](img/71036e04-30c2-49b9-be85-0064aae009be.png) needs to be trained
    with **gradient descent** (the ![](img/32d44fa3-3ea0-44ce-9054-56489723c54d.png) operator
    in the objective). The update rule for ![](img/251e6d73-6aad-4e01-8d2b-fa4f4e755424.png) is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33b9dec3-6746-48c0-8fae-5996ceeedb2a.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, ![](img/4e9f2bc5-1dca-4d79-8165-8ea31c605113.png) is the parameters
    of ![](img/a3e44616-38e4-4983-9b89-275d94dc1edf.png)<q>,</q> ![](img/3417277d-b9e1-4f8a-bbfb-3caa4be5f150.png) is
    the size of the mini-batch, and ![](img/21482551-6613-41e5-aa29-1313c5155c0a.png) is
    the index of the sample in the mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: If you are unfamiliar with the concept of GD, think of it as a little boy kicking
    a sticky ball on bumpy terrain. The boy wants the ball to be at the bottom of
    the lowest pit so that he can call it a day and go home. The ball is sticky so
    it doesn't roll after it hits the ground, even on a slope. Therefore, where the
    ball will hit is determined by which direction and how hard the boy kicks it.
    The amount of force the boy kicks the ball with is described by the step size
    (or the **learning rate**). The direction of kicking is determined by the characteristics
    of the terrain under his feet. An efficient choice would be the downhill direction,
    which is the negative gradient of the loss function with respect to the parameters.
    Therefore, we often use gradient descent to minimize an objective function. However,
    the boy is so obsessed with the ball that he only stares at the ball and refuses
    to look up to find the lowest pit in a wider range. Therefore, the GD method is
    sometimes inefficient because it takes a very long time to reach the bottom. We
    will introduce several tips on how to improve the efficiency of GD in [Chapter
    3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml), *Best Practices for Model Design
    and Training*. The **gradient ascent** is the opposite of gradient descent, which
    is to find the highest peak.
  prefs: []
  type: TYPE_NORMAL
- en: Using NumPy to train a sine signal generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maybe math is even more confusing than a big chunk of code to some. Now, let's
    look at some code to digest the equations we've thrown at you. Here, we will use
    Python to implement a very simple adversarial learning example to generate sine
    (sin) signals.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will be only using NumPy, a powerful linear algebra
    Python library to implement a GAN model. We will need to calculate the gradients
    by ourselves so that you can have an in-depth understanding of what might be happening beneath
    the popular deep learning toolkits such as PyTorch. Rest assured that we won't
    do this in future chapters because we can use the powerful computational graph
    provided by PyTorch to calculate the gradients for us!
  prefs: []
  type: TYPE_NORMAL
- en: Designing the network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the generator network is described in the following diagram.
    It takes a 1-dimension random value as input and gives a 10-dimension vector as
    output. It has 2 hidden layers with each containing 10 neurons. The calculation
    in each layer is a matrix multiplication. Therefore, the network is, in fact,
    a **Multilayer Perceptron** (**MLP**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df51118f-249b-4bbd-be3a-df084d96438d.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of the generator network
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the discriminator network is described in the following
    diagram. It takes a 10-dimension vector as input and gives a 1-dimension value
    as output. The output is the prediction label (real or fake) of the input sample.
    The discriminator network is also an MLP with two hidden layers and each containing
    10 neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c299314f-b7d9-44f0-aa27-754e7f165ab5.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of the discriminator network
  prefs: []
  type: TYPE_NORMAL
- en: Defining activation functions and the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be only using NumPy ([http://www.numpy.org](http://www.numpy.org)) to
    calculate and train our GAN model (and optionally using Matplotlib ([https://matplotlib.org](https://matplotlib.org)) to
    visualize the signals). If you don't already have the Python environment on your
    machine, please refer to [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml), *Getting
    Started with PyTorch 1.3*, to learn how to set up a working Python environment.
    If your Python environment is properly set up, let's move on to the actual code.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the following code can be placed in a `simple*.*py` file (such as `simple_gan.py`).
    Let''s look at the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `NumPy` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a few constant variables that are needed in our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the real sine samples (with `numpy.sin`) that we want to estimate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous snippet, we use a `bool` variable, `random`, to introduce randomness
    into the real samples, as real-life data has. The real samples look like this
    (50 samples with `random=True`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb9a644e-7dcc-44bf-b223-b2d3a6945b06.png)'
  prefs: []
  type: TYPE_IMG
- en: The real sine samples
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the activation functions and their derivatives. If you are not familiar
    with the concept of activation functions, just remember that their jobs are to
    adjust the outputs of a layer so that its next layer can have a better understanding
    of these output values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `helper` function to initialize the layer parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `loss` function (both forward and backward):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is called **binary cross-entropy**, which is typically used in binary classification
    problems (in which a sample either belongs to class A or class B). Sometimes,
    one of the networks is trained too well so that the `sigmoid` output of the discriminator
    might be either too close to 0 or 1\. Both of the scenarios lead to numerical
    errors of the `log` function. Therefore, we need to restrain the maximum and minimum
    values of the output value.
  prefs: []
  type: TYPE_NORMAL
- en: Working on forward pass and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s create our generator and discriminator networks. We put the code
    in the same `simple_gan.py` file as well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the parameters of the generator network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We keep track of the inputs and outputs of all the layers because we need them
    to calculate the derivatives to update the parameters later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the forward calculation (to generate samples based on random noise):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s basically the same calculation process repeated 3 times. Each layer calculates
    its output according to this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b6b6174-39a7-4f27-be1c-2bda2ab5e28d.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, ![](img/5daabc50-fcc2-47c3-8f4f-fee2d1f48ec9.png) represents
    the output value of a layer, <q>f</q> represents the activation function, and
    subscript <q>l</q> represents the index of the layer. Here, we use `ReLU` in the
    hidden layers and `Tanh` in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to define the backward calculation for the generator network
    (to calculate the derivatives and update the parameters). This part of the code
    is a bit long. It''s really repeating the same process 3 times:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the derivatives of loss with respect to the output of this layer (for
    example, the derivative with respect to `output` or `x2`).
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the derivatives of loss with respect to the parameters (for example,
    the derivative with respect to `w3` or `b3`).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters with the derivatives.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Passing the gradients to the preceding layer. The derivatives are calculated
    as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1c4e523a-2f8a-4000-8d35-8e079df29fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/04542316-3e43-4141-944c-723a441a651e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/73bb6f13-fc16-4422-98ad-a4b39b10b11a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this process, the derivative of the loss with respect to the output, which
    is denoted by `delta` in the code, is the key to propagate the gradients from
    layer <q>l+1</q> to layer <q>l</q>. Therefore, this process is called **backpropagation**.
    The propagation from layer <q>l+1</q> to layer <q>l</q> is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75e53cd1-5243-4db0-82ed-aad2eba959fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate the derivatives with respect to the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the derivatives with respect to the parameters in the third layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the gradients to the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And update the parameters of the third layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the parameters in the second layer and pass the gradients to the first
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the parameters in the first layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that the following code looks similar to the preceding code.
    It is only mentioned here to point out that these lines help to keep the data
    from becoming unstable. You don''t have to add these three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This code is included because the training of GANs can be very unstable and
    we need to clip the gradients and the parameters to ensure a stable training process.
  prefs: []
  type: TYPE_NORMAL
- en: We will elaborate on the topics of activation functions, loss functions, weight
    initialization, gradient clipping, weight clipping, and more in [Chapter 3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml),
    *Best Practices for Model Design and Training.* These are extremely useful for
    stabilizing and improving the training of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define the discriminator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And now define its forward calculation (to predict the label based on the input
    sample):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we use LeakyReLU as the activation function for hidden layers and sigmoid
    for the output layer. Now, let''s define the backward calculation for the discriminator
    network (to calculate the derivatives and update the parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that the main difference in the backward calculation of the discriminator
    is that it''s trained with gradient ascent. Therefore, to update its parameters,
    we need to add the gradients. So, in the preceding code, you will see lines like
    this that take care of it for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Training our GAN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that all the necessary components are defined, we can begin the training
    of our GAN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the preceding code, the training of the GAN model mainly
    has 3 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the discriminator with real data (and recognize it as real).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the discriminator with fake data (and recognize it as fake).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the generator with fake data (and recognize it as real).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first two steps teach the discriminator how to tell the difference between
    real and fake data. The third step teaches the generator how to fool the discriminator
    by generating fake data that is similar to real data. This is the core idea of
    adversarial learning and the reason why GANs can generate relatively realistic
    audio, text, images, and videos.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use SGD to train the model for 50,000 iterations. If you are interested,
    feel free to implement a mini-batch GD to see whether it produces better results
    in a shorter time. You are also welcome to change the network architectures (for
    example, the number of layers, the number of neurons in each layer, and the data
    dimension, `X_DIM`) to see how results change with the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s use Matplotlib to visualize the generated samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It may take a few seconds to finish the training, depending on how powerful
    your CPU is. When the training is finished, the samples generated by the generator
    network may look like this (50 samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3c5a92f-6bb4-4362-adc7-aa78d6eb8184.png)'
  prefs: []
  type: TYPE_IMG
- en: The generated sine samples
  prefs: []
  type: TYPE_NORMAL
- en: Pretty convincing, right? It's amazing to see how it captures the peaks and
    valleys of the original sine waves. Imagine what GANs are capable of with much
    more complex structures!
  prefs: []
  type: TYPE_NORMAL
- en: What GAN we do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs can do a lot more than generating sine signals. We can apply GANs to address
    many different practical problems by altering the input and output dimensions
    of the generator and combining them with other methods. For example, we can generate
    text and audio (1-dimension), images (2-dimension), video, and 3D models (3-dimension)
    based on random input. If we keep the same dimension of input and output, we can
    perform denoising and translation on these types of data. We can feed real data
    into the generator and let it output data with larger dimensions, for example,
    image super-resolution. We can also feed one type of data and let it give another
    type of data, for example, generate audio based on text, generate images based
    on text, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Even though it has only been 4 years since GANs first came out (at the time
    of writing), people have kept working on improving GANs and new GAN models are
    coming out almost weekly. If you take a look at [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo),
    you can see that there have been at least 500 different GAN models. It's nearly
    impossible for us to learn and evaluate each one of them. You'll be amazed to
    see that it is actually common to find several models sharing the same name! Therefore,
    in this book, we won't even try to introduce you to most of the GAN models out
    there. We will, however, help you to get familiar with the most typical GAN models
    in different applications and learn how to use them to address practical problems.
  prefs: []
  type: TYPE_NORMAL
- en: We will also introduce you to some useful tricks and techniques to improve the
    performance of GANs. We hope that, by the time you finish this book, you have
    a wide yet in-depth understanding of the mechanisms of various GAN models so that
    you will feel confident to design your own GANs to creatively solve the problems
    you may encounter in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what GANs are capable of and what their advantages are
    compared to traditional approaches in these fields: image processing, NLP, and
    3D modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the field of image processing, GANs are applied to many applications, including
    image synthesis, image translation, and image restoration. These topics are the
    most common in the study and application of GANs and make up most of the content
    in this book. Images are one of the easiest to show and spread media form on the
    internet; therefore, any latest breakthrough in the image-wise application of
    GANs would receive overwhelming attention in the deep learning community.
  prefs: []
  type: TYPE_NORMAL
- en: Image synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image synthesis is, in short, the creation of new images. Early in 2015, **DCGANs** (**Deep
    Convolutional Generative Adversarial Networks**) came out. It was one of the first
    well-performing and stable approaches to address the hard-to-train issues presented
    in earlier GAN models. It generates 64 x 64 images based on a random vector with
    a length of 100\. Some images generated by DCGANs are shown in the following screenshot.
    You may notice that some of the images are far from being realistic because of
    the blocky appearance of the pixels. In the paper by Radford, Metz, and Chintala
    (2015), they present many interesting and inspiring visual experiments and reveal
    even more potential of GANs. We will talk about the architecture and training
    procedure of DCGANs later in [Chapter 4](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml),
    *Building Your First GAN with PyTorch*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69a09d37-57b3-4a02-aa33-b0a2560dee60.png)![](img/d68c77e3-9e2c-498f-942a-86d8a822b817.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Images generated by a DCGAN (left: human faces; right: bedroom)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, GANs perform extraordinarily in image synthesis. Take BigGAN, for example.
    It was proposed in a paper submitted to ICLR 2019 (*7^(th) International Conference
    on Learning Representations*) by Brock, Donahue, and Simonyan. It received a lot
    of attention on social media even during the open review process. It's capable
    of generating images as large as 512 x 512 with high quality.
  prefs: []
  type: TYPE_NORMAL
- en: In future chapters, we will also take a look at GAN models that look further
    into attributes of images, rather than just class conditions. We will talk about
    Conditional GANs, which allow you to generate images interactively, and Age-cGAN,
    which generates human faces of any age of your desire. We will also look into
    how to use GANs to generate adversarial examples that even the best classifiers
    cannot correctly recognize in [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml),
    *Training Your GANs to Break Different Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Image translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we describe image synthesis as a process where we expect the outputs to be
    2-dimension images when feeding 1-dimension vector into the models (again, note
    that there are exceptions since you can generate images based on other types of
    data if you want), image translation (more precisely, image-to-image translation)
    would be the process where we feed 2-dimension images into models that also give
    2-dimension data as outputs. A lot of interesting things can be done with image
    translation. For example, pix2pix (Isola, Zhu, Zhou, et al, 2016) transforms label
    maps into images, including turning edge sketches into colorized images, generating
    street view photos based on semantic segmentation information, transferring image
    styles, and so on. We will get to an upgraded version of pix2pix, pix2pixHD, in
    [Chapter 6](209b2357-05d7-48d4-9c91-e061eccf8344.xhtml), *Image-to-Image Translation
    and Its Applications*, along with other image-to-image translation methods such
    as CycleGAN and DiscoGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image translation can be used in other computer vision applications
    and address more traditional problems, such as image restoration, image in-painting,
    and super-resolution. Image restoration is one of the most important research
    areas in computer vision. Mathematicians and computer scientists have been trying
    to figure out how to remove the annoying noises off photos or reveal more information
    out of blur images for decades. Traditionally, these problems are solved by iterative
    numerical calculations, which often require profound mathematical backgrounds
    to master. Now, with GANs at hand, these problems can be solved by image-to-image
    translation. For example, SRGAN (Ledig, Theis, Huszar, et al, 2016) can upscale
    images to 4X of size with high quality, which we will talk about in detail in
    [Chapter 7](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml), *Image Restoration with
    GANs*. Yes, Chen, Lim, et al (2016) proposed using a DCGAN-like model to address
    human face inpainting problems. More recently, Yu, Lin, Yang, et al (2018) designed
    a GAN model that fills in an arbitrary shape of blank holes in images and the
    generated pixels are quite convincing as well.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image translation is also a good application of GANs, in which new images
    are generated based on the description text. Reed, Akata, Yan, et al (2016) came
    up with a procedure that extracts distinguish features from detailed description
    text and uses the information to generate flower or bird images that match perfectly
    to the description. Months later, Zhang, Xu, Li, et al (2016) proposed StackGAN
    to generate 256 x 256 images with high fidelity based on description text. We
    will talk about text-to-image translation in [Chapter 9](464e6361-6a52-4de2-960a-4fa0576f42c7.xhtml),
    *Image Generation from Description Text*.
  prefs: []
  type: TYPE_NORMAL
- en: Video synthesis and translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A video is a sequence of images. Therefore, most of the image translation methods
    can be directly applied to video. However, a crucial performance criterion of
    video synthesis or translation is the calculation speed. For example, if we want
    to develop a camera application with different image styles for mobile devices,
    our users would certainly hope that they can see the processed results in real-time.
    Take video surveillance systems as another example. It is completely feasible
    to use GANs to denoise and enhance the video signals (provided that your clients
    trust your models without reservation). A fast model that processes each frame
    in milliseconds to keep up with the frame rate would certainly be worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: We'd like to point out an interesting gesture transfer project called **Everybody
    Dance Now**. It extracts the movements of the dancer from a source video, then
    maps the same movements to the person in the target video by image-to-image translation.
    This way, anyone can use this model to make dancing videos of their own!
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is the study of how to use computers to process and analyze natural human
    languages. Other than generating images, GANs can also be used to generate sequential
    and time-dependent data, such as text and audio. SeqGAN, proposed by Yu, Zhang,
    Wang, et al (2016), is designed to generate sequential signals, like poems and
    music. Shortly after, Mogren (2016) proposed C-RNN-GAN, which is designed to generate
    classical music under acoustic restraints. In 2017, Dong, Hsiao, Yang, et al designed
    MuseGAN to generate polyphonic music of multiple instruments, including bass,
    drums, guitar, piano, and strings. Feel free to visit the following web^(10) site
    to enjoy the generated music!
  prefs: []
  type: TYPE_NORMAL
- en: Speech enhancement is one of the main research areas in audio signal processing.
    Traditionally, people use spectral subtraction, Wiener filtering, subspace approaches,
    and more to remove the noises in audio or speech signals. However, the performances
    of these methods have only been satisfactory under certain circumstances. Pascual,
    Bonafonte, and Serrà (2017) designed SEGAN to address this problem and achieved
    impressive results^(11). We will talk about the applications of GANs in the field
    of NLP in [Chapter 10](e05f7dcc-b1fe-4b9b-a893-124e67718cac.xhtml), *Sequence
    Synthesis with GANs*.
  prefs: []
  type: TYPE_NORMAL
- en: 3D modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know GANs can generate 2D data based on 1D inputs, it's only natural
    to consider leveling this up to generate 3D data based on 1D or 2D signals with
    GANs. 3D-GAN (Wu, Zhang, Xue, et al, 2016) is designed exactly for this purpose.
    It learns the mapping between the latent vector and 3D models to generate 3D objects
    based on a 1D vector. It is also completely feasible to use GANs to predict 3D
    models based on 2D silhouettes. Gadelha, Maji, and Wang (2016) designed PrGAN
    to generate 3D objects based on binary silhouette images from any viewpoint. We
    will discuss how to generate 3D objects with GANs in detail in [Chapter 11](09d087ce-5d7e-4bd4-af48-693ac63d891c.xhtml),
    *Reconstructing 3D Models with GANs*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered a tremendous amount of information just in this first chapter.
    You've seen how GANs came about and have a basic grasp of the roles of generators
    and discriminators. You've even seen a few examples of some of the things that
    GANs can do. We've even created a GAN program using just NumPy. Not to mention
    we now know why Ganland has better blacksmiths and wine.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll dive into the wondrous world of PyTorch, what it is, and how to
    install it.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a list of references and other helpful links.
  prefs: []
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Goodfellow I, Pouget-Abadie J, Mirza M, et al. (2014). Generative adversarial
    nets. NIPS, 2672-2680.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang, J. (2017, Dec 23). *Symbolism vs. Connectionism: A Closing Gap in Artificial
    Intelligence*, retrieved from [https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence](https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Radford A, Metz L, Chintala S. (2015). *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*. arXiv preprint arXiv:1511.06434.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Dev Nag". (2017, Feb 11). **Generative Adversarial Networks** (**GANs**) in
    50 lines of code (PyTorch), retrieved from [https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Brock A, Donahue J, Simonyan K. (2018). *Large Scale GAN Training for High Fidelity
    Natural Image Synthesis*. arXiv preprint arXiv:1809.11096.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isola P, Zhu J Y, Zhou T, Efros A. (2016). *Image-to-Image Translation with
    Conditional Adversarial Networks*. arXiv preprint arXiv:1611.07004.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ledig C, Theis L, Huszar F, et al (2016). *Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network*. arXiv preprint arXiv:1609.04802.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yeh R A, Chen C, Lim T Y, et al (2016). *Semantic Image Inpainting with Deep
    Generative Models*. arXiv preprint arXiv:1607.07539.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yu J, Lin Z, Yang J, et al (2018). *Free-Form Image Inpainting with Gated Convolution*. arXiv
    preprint arXiv:1806.03589.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reed S, Akata Z, Yan X, et al (2016). *Generative Adversarial Text to Image
    Synthesis*. arXiv preprint arXiv:1605.05396.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhang H, Xu T, Li H, et al (2016). *StackGAN: Text to Photo-realistic Image
    Synthesis with Stacked Generative Adversarial Networks*. arXiv preprint arXiv:1612.03242.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Yu L, Zhang W, Wang J, et al (2016). *SeqGAN: Sequence Generative Adversarial
    Nets with Policy Gradient*. arXiv preprint arXiv:1609.05473.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mogren O. (2016). *C-RNN-GAN: Continuous recurrent neural networks with adversarial
    training*. arXiv preprint arXiv:1611.09904.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dong H W, Hsiao W Y, Yang L C, et al (2017). *MuseGAN: Multi-track Sequential
    Generative Adversarial Networks for Symbolic Music Generation and Accompaniment*. arXiv
    preprint arXiv:1709.06298.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pascual S, Bonafonte A, Serrà J. (2017). *SEGAN: Speech Enhancement Generative
    Adversarial Network*. arXiv preprint arXiv:1703.09452.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wu J, Zhang C, Xue T, et al (2016). *Learning a Probabilistic Latent Space of
    Object Shapes via 3D Generative-Adversarial Modeling*. arXiv preprint arXiv:1610.07584.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gadelha M, Maji S, Wang R. (2016). *3D Shape Induction from 2D Views of Multiple
    Objects*. arXiv preprint arXiv:1612.05872.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
