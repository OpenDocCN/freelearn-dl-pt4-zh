["```py\ndataDirectory <- \"../data\"\nif (!file.exists(paste(dataDirectory,'/train.csv',sep=\"\")))\n{\n  link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'\n  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep=\"\")))\n    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=\"\"))\n  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=\"\"), exdir = dataDirectory)\n  if (file.exists(paste(dataDirectory,'/test.csv',sep=\"\")))\n    file.remove(paste(dataDirectory,'/test.csv',sep=\"\"))\n}\n```", "```py\ntrain <- read.csv(paste(dataDirectory,'/train.csv',sep=\"\"), header=TRUE, nrows=20)\n```", "```py\ntail(train[,1:6])\n   label pixel0 pixel1 pixel2 pixel3 pixel4\n15     3      0      0      0      0      0\n16     1      0      0      0      0      0\n17     2      0      0      0      0      0\n18     0      0      0      0      0      0\n19     7      0      0      0      0      0\n20     5      0      0      0      0      0\n\ntail(train[,(ncol(train)-5):ncol(train)])\n   pixel778 pixel779 pixel780 pixel781 pixel782 pixel783\n15        0        0        0        0        0        0\n16        0        0        0        0        0        0\n17        0        0        0        0        0        0\n18        0        0        0        0        0        0\n19        0        0        0        0        0        0\n20        0        0        0        0        0        0\n```", "```py\nplotInstance <-function (row,title=\"\")\n {\n  mat <- matrix(row,nrow=28,byrow=TRUE)\n  mat <- t(apply(mat, 2, rev))\n  image(mat, main = title,axes = FALSE, col = grey(seq(0, 1, length = 256)))\n }\n par(mfrow = c(3, 3))\n par(mar=c(2,2,2,2))\n for (i in 1:9)\n {\n  row <- as.numeric(train[i,2:ncol(train)])\n  plotInstance(row, paste(\"index:\",i,\", label =\",train[i,1]))\n }\n```", "```py\nrequire(mxnet)\noptions(scipen=999)\n\ndfMnist <- read.csv(\"../data/train.csv\", header=TRUE)\nyvars <- dfMnist$label\ndfMnist$label <- NULL\n\nset.seed(42)\ntrain <- sample(nrow(dfMnist),0.9*nrow(dfMnist))\ntest <- setdiff(seq_len(nrow(dfMnist)),train)\ntrain.y <- yvars[train]\ntest.y <- yvars[test]\ntrain <- data.matrix(dfMnist[train,])\ntest <- data.matrix(dfMnist[test,])\n\nrm(dfMnist,yvars)\n```", "```py\ntrain <- t(train / 255.0)\ntest <- t(test / 255.0)\n```", "```py\ntable(train.y)\n## train.y\n##    0    1    2    3    4    5    6    7    8    9\n## 3716 4229 3736 3914 3672 3413 3700 3998 3640 3782\n```", "```py\ndata <- mx.symbol.Variable(\"data\")\nfullconnect1 <- mx.symbol.FullyConnected(data, name=\"fullconnect1\", num_hidden=256)\nactivation1 <- mx.symbol.Activation(fullconnect1, name=\"activation1\", act_type=\"relu\")\nfullconnect2 <- mx.symbol.FullyConnected(activation1, name=\"fullconnect2\", num_hidden=128)\nactivation2 <- mx.symbol.Activation(fullconnect2, name=\"activation2\", act_type=\"relu\")\nfullconnect3 <- mx.symbol.FullyConnected(activation2, name=\"fullconnect3\", num_hidden=10)\nsoftmax <- mx.symbol.SoftmaxOutput(fullconnect3, name=\"softmax\")\n```", "```py\ndevices <- mx.gpu()\nmx.set.seed(0)\nmodel <- mx.model.FeedForward.create(softmax, X=train, y=train.y,\n                                     ctx=devices,array.batch.size=128,\n                                     num.round=10,\n                                     learning.rate=0.05, momentum=0.9,\n                                     eval.metric=mx.metric.accuracy,\n                                     epoch.end.callback=mx.callback.log.train.metric(1))\n```", "```py\npreds1 <- predict(model, test)\npred.label1 <- max.col(t(preds1)) - 1\nres1 <- data.frame(cbind(test.y,pred.label1))\ntable(res1)\n##      pred.label1\n## test.y   0   1   2   3   4   5   6   7   8   9\n##      0 405   0   0   1   1   2   1   1   0   5\n##      1   0 449   1   0   0   0   0   4   0   1\n##      2   0   0 436   0   0   0   0   3   1   1\n##      3   0   0   6 420   0   1   0   2   8   0\n##      4   0   1   1   0 388   0   2   0   1   7\n##      5   2   0   0   6   1 363   3   0   2   5\n##      6   3   1   3   0   2   1 427   0   0   0\n##      7   0   2   3   0   1   0   0 394   0   3\n##      8   0   4   2   4   0   2   1   1 403   6\n##      9   1   0   1   2   7   0   1   1   0 393\n\naccuracy1 <- sum(res1$test.y == res1$pred.label1) / nrow(res1)\naccuracy1\n## 0.971\n```", "```py\ndata <- mx.symbol.Variable('data')\n# first convolution layer\nconvolution1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=64)\nactivation1 <- mx.symbol.Activation(data=convolution1, act_type=\"tanh\")\npool1 <- mx.symbol.Pooling(data=activation1, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\n\n# second convolution layer\nconvolution2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=32)\nactivation2 <- mx.symbol.Activation(data=convolution2, act_type=\"relu\")\npool2 <- mx.symbol.Pooling(data=activation2, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\n\n# flatten layer and then fully connected layers\nflatten <- mx.symbol.Flatten(data=pool2)\nfullconnect1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=512)\nactivation3 <- mx.symbol.Activation(data=fullconnect1, act_type=\"relu\")\nfullconnect2 <- mx.symbol.FullyConnected(data=activation3, num_hidden=10)\n# final softmax layer\nsoftmax <- mx.symbol.SoftmaxOutput(data=fullconnect2)\n\n```", "```py\ntrain.array <- train\ndim(train.array) <- c(28,28,1,ncol(train))\ntest.array <- test\ndim(test.array) <- c(28,28,1,ncol(test))\n```", "```py\ndevices <- mx.gpu()\nmx.set.seed(0)\nmodel2 <- mx.model.FeedForward.create(softmax, X=train.array, y=train.y,\n                                     ctx=devices,array.batch.size=128,\n                                     num.round=10,\n                                     learning.rate=0.05, momentum=0.9, wd=0.00001,\n                                     eval.metric=mx.metric.accuracy,\n                                     epoch.end.callback=mx.callback.log.train.metric(1))\n```", "```py\npreds2 <- predict(model2, test.array)\npred.label2 <- max.col(t(preds2)) - 1\nres2 <- data.frame(cbind(test.y,pred.label2))\ntable(res2)\n## pred.label2\n## test.y   0   1   2   3   4   5   6   7   8   9\n##      0 412   0   0   0   0   1   1   1   0   1\n##      1   0 447   1   1   1   0   0   4   1   0\n##      2   0   0 438   0   0   0   0   3   0   0\n##      3   0   0   6 427   0   1   0   1   2   0\n##      4   0   0   0   0 395   0   0   1   0   4\n##      5   1   0   0   5   0 369   2   0   1   4\n##      6   2   0   0   0   1   1 432   0   1   0\n##      7   0   0   2   0   0   0   0 399   0   2\n##      8   1   0   1   0   1   1   1   1 414   3\n##      9   2   0   0   0   4   0   0   1   1 398\n\naccuracy2\n## 0.9835714\n```", "```py\ngraph.viz(model2$symbol)\n```", "```py\ndata <- mx.symbol.Variable('data')\n# first convolution layer\nconvolution1 <- mx.symbol.Convolution(data=data, kernel=c(5,5),\n                                      stride=c(1,1), pad=c(2,2), num_filter=64)\nactivation1 <- mx.symbol.Activation(data=convolution1, act_type=act_type1)\npool1 <- mx.symbol.Pooling(data=activation1, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\n\n# second convolution layer\nconvolution2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5),\n                                      stride=c(1,1), pad=c(2,2), num_filter=32)\nactivation2 <- mx.symbol.Activation(data=convolution2, act_type=act_type1)\npool2 <- mx.symbol.Pooling(data=activation2, pool_type=\"max\",\n                           kernel=c(2,2), stride=c(2,2))\n\n# flatten layer and then fully connected layers with activation and dropout\nflatten <- mx.symbol.Flatten(data=pool2)\nfullconnect1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=512)\nactivation3 <- mx.symbol.Activation(data=fullconnect1, act_type=act_type1)\ndrop1 <- mx.symbol.Dropout(data=activation3,p=0.4)\nfullconnect2 <- mx.symbol.FullyConnected(data=drop1, num_hidden=10)\n# final softmax layer\nsoftmax <- mx.symbol.SoftmaxOutput(data=fullconnect2)\n```", "```py\nlogger <- mx.metric.logger$new()\nmodel2 <- mx.model.FeedForward.create(softmax, X=train.array, y=train.y,\n                                     ctx=devices, num.round=20,\n                                     array.batch.size=64,\n                                     learning.rate=0.05, momentum=0.9,\n                                     wd=0.00001,\n                                     eval.metric=mx.metric.accuracy,\n                                     eval.data=list(data=test.array,labels=test.y),\n                                     epoch.end.callback=mx.callback.log.train.metric(100,logger))\n```", "```py\npreds2 <- predict(model2, test.array)\npred.label2 <- max.col(t(preds2)) - 1\nres2 <- data.frame(cbind(test.y,pred.label2))\ntable(res2)\n      pred.label2\ntest.y   0   1   2   3   4   5   6   7   8   9\n     0 489   0  12  10   0   0  53   0   3   0\n     1   0 586   1   6   1   0   1   0   0   0\n     2   8   1 513   7  56   0  31   0   0   0\n     3  13   0   3 502  16   0  26   1   1   0\n     4   1   1  27  13 517   0  32   0   2   0\n     5   1   0   0   0   0 604   0   9   0   3\n     6  63   0  47   9  28   0 454   0   3   0\n     7   0   0   0   1   0  10   0 575   1  11\n     8   0   0   1   0   1   2   1   0 618   0\n     9   0   0   0   0   0   1   0  17   1 606\naccuracy2 <- sum(res2$test.y == res2$pred.label2) / nrow(res2)\naccuracy2\n# 0.9106667\n```", "```py\n# use the log data collected during model training\ndfLogger<-as.data.frame(round(logger$train,3))\ndfLogger2<-as.data.frame(round(logger$eval,3))\ndfLogger$eval<-dfLogger2[,1]\ncolnames(dfLogger)<-c(\"train\",\"eval\")\ndfLogger$epoch<-as.numeric(row.names(dfLogger))\n\ndata_long <- melt(dfLogger, id=\"epoch\")\n\nggplot(data=data_long,\n       aes(x=epoch, y=value, colour=variable,label=value)) +\n  ggtitle(\"Model Accuracy\") +\n  ylab(\"accuracy\") +\n  geom_line()+geom_point() +\n  geom_text(aes(label=value),size=3,hjust=0, vjust=1) +\n  theme(legend.title=element_blank()) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_x_discrete(limits= 1:nrow(dfLogger))\n```"]