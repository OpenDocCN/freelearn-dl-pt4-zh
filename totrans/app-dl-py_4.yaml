- en: Introduction to Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MNIST dataset does not contain numbers on the edges of images. Hence, neither
    network assigns relevant values to the pixels located in that region. Both networks
    are much better at classifying numbers correctly if we draw them closer to the
    center of the designated area. This shows that neural networks can only be as
    powerful as the data that is used to train them. If the data used for training
    is very different than what we are trying to predict, the network will most likely
    produce disappointing results.In this chapter, we will cover the basics of neural
    networks and how to set up a deep learning programming environment. We will also
    explore the common components of a neural network and its essential operations.
    We will conclude this chapter by exploring a trained neural network created using
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is about understanding what neural networks can do. We will not
    cover mathematical concepts underlying deep learning algorithms, but will instead
    describe the essential pieces that make a deep learning system. We will also look
    at examples where neural networks have been used to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give you a practical intuition on how to engineer systems
    that use neural networks to solve problems—including how to determine if a given
    problem can be solved at all with such algorithms. At its core, this chapter challenges
    you to think about your problem as a mathematical representation of ideas. By
    the end of this chapter, you will be able to think about a problem as a collection
    of these representations and then start to recognize how these representations
    may be learned by deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Cover the basics of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a deep learning programming environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the common components of a neural network and its essential operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclude this chapterby exploring a trained neural network created using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Neural Networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks—also known as **Artificial Neural Networks**—were fist proposed
    in the 40s by MIT professors Warren McCullough and Walter Pitts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Explained: Neural networks. MIT News Office,April
    14, 2017\. Available at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://news.mit.edu/2017/explained-neural-networksdeep-learning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414).'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by advancements in neuroscience, they proposed to create a computer
    system that reproduced how the brain works (human or otherwise). At its core was
    the idea of a computer system that worked as an interconnected network. That is,
    a system that has many simple components. These components both interpret data
    and influence each other on how to interpret data. This same core idea remains
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is largely considered the contemporary study of neural networks.
    Think of it as a current name given to neural networks. The main difference is
    that the neural networks used in deep learning are typically far greater in size—that
    is, they have many more nodes and layers—than earlier neural networks. Deep learning
    algorithms and applications typically require resources to achieve success, hence
    the use of the word *deep* to emphasize its size and the large number of interconnected
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Successful Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks have been under research since their inception in the 40s in
    one form or another. It is only recently, however, that deep learning systems
    have been successfully used in large-scale industry applications.
  prefs: []
  type: TYPE_NORMAL
- en: Contemporary proponents of neural networks have demonstrated great success in
    speech recognition, language translation, image classification, and other feilds.
    Its current prominence is backed by a significant increase in available computing
    power and the emergence of **Graphic Processing Units** (**GPUs**) and **Tensor
    Processing Units** (**TPUs**)— which are able to perform many more simultaneous
    mathematical operations than regular CPUs, as well as a much greater availability
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Power consumption of different AlphaGo algorithms. AlphaGo is an initiative
    by DeepMind to develop a series of algorithms to beat the game Go. It is considered
    a prime example of the power of deep learning. TPUs are a chipset developed by
    Google for the use in deep learning programs.
  prefs: []
  type: TYPE_NORMAL
- en: The graphic depicts the number of GPUs and TPUs used to train different versions
    of the AlphaGo algorithm. Source: [https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will not be using GPUs to fulfil our activities. GPUs are not
    required to work with neural networks. In a number of simple examples—like the
    ones provided in this book—all computations can be performed using a simple laptop's
    CPU. However, when dealing with very large datasets, GPUs can be of great help
    given that the long time to train a neural network would be unpractical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of fields in which neural networks have had great impact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translating text**: In 2017, Google announced that it was releasing a new
    algorithm for its translation service called **Transformer**. The algorithm consisted
    of a recurrent neural network (LSTM) that is trained used bilingual text. Google
    showed that its algorithm had gained notable accuracy when comparing to industry
    standards (BLEU) and was also computationally efficient. At the time of writing,
    Transformer is reportedly used by Google Translate as its main translation algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Research Blog. Transformer: A Novel Neural Network Architecture for
    Language Understanding. August 31, 2017\. Available at: [https://research.googleblog.com/2017/08/transformernovel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-driving vehicles**: Uber, NVIDIA, and Waymo are believed to be using
    deep learning models to control different vehicle functions that control driving.
    Each company is researching a number of possibilities, including training the
    network using humans, simulating vehicles driving in virtual environments, and
    even creating a small city-like environment in which vehicles can be trained based
    on expected and unexpected events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alexis C. Madrigal: Inside Waymo''s Secret World for Training SelfDriving Cars.
    The Atlantic. August 23, 2017\. Available at: [https://](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[www.theatlantic.com/technology/archive/2017/08/ inside-waymos-secret-testing-and-simulationfacilities/537648/](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'NVIDIA: *End-to-End Deep Learning for Self-Driving Cars*. August 17, 2016\.
    Available at: [https://devblogs.nvidia.com/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[parallelforall/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dave Gershgorn: Uber''s new AI team is looking for the shortest route to self-driving
    cars. Quartz. December 5, 2016\. Available at: [https://qz.com/853236/ubers-new-ai-team-is-looking-for-theshortest-route-to-self-driving-cars/](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)[.](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image recognition**: Facebook and Google use deep learning models to identify
    entities in images and automatically tag these entities as persons from a set
    of contacts. In both cases, the networks are trained with previously tagged images
    as well as with images from the target friend or contact. Both companies report
    that the models are able to suggest a friend or contact with a high level of accuracy
    in most cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there are many more examples in other industries, the application of deep
    learning models is still in its infancy. Many more successful applications are
    yet to come, including the ones that you create.
  prefs: []
  type: TYPE_NORMAL
- en: Why Do Neural Networks Work So Well?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why are neural networks so powerful? Neural networks are powerful because they
    can be used to predict any given function with reasonable approximation. If one
    is able to represent a problem as a mathematical function and also has data that
    represents that function correctly, then a deep learning model can, in principle—and
    given enough resources—be able to approximate that function. This is typically
    called the *universality principle of neural networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information refer, Michael Nielsen: Neural Networks and Deep Learning:
    A visual proof that neural nets can compute any function. Available at: [http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not be exploring mathematical proofs of the universality principle
    in this book. However, two characteristics of neural networks should give you
    the right intuition on how to understand that principle: representation learning
    and function approximation.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information refer, Kai Arulkumaran, Marc Peter Deisenroth,Miles Brundage,
    and Anil Anthony Bharath. A Brief Survey of Deep Reinforcement Learning. arXiv.
    September 28, 2017\. Available at: [https://www.arxiv-vanity.com/papers/1708.05866/
    .](https://www.arxiv-vanity.com/papers/1708.05866/)
  prefs: []
  type: TYPE_NORMAL
- en: Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data used to train a neural network contains representations (also known
    as *features*) that explain the problem you are trying to solve. For instance,
    if one is interested in recognizing faces from images, the color values of each
    pixel from a set of images that contain faces will be used as a starting point.
    The model will then continuously learn higher-level representations by combining
    pixels together as it goes through its training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5bc5ed5-3b48-435d-88ab-dbec13f2bed6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Series of higher-level representations that begin on input data.
    Derivate image based on original image from: Yann LeCun, Yoshua Bengio & Geoffrey
    Hinton. "Deep Learning". Nature 521, 436–444 (28 May 2015) doi:10.1038/ nature14539'
  prefs: []
  type: TYPE_NORMAL
- en: In formal words, neural networks are computation graphs in which each step computes
    higher abstraction representations from input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each one of these steps represents a progression into a different abstraction
    layer. Data progresses through these layers, building continuously higher-level
    representations. The process finishes with the highest representation possible:
    the one the model is trying to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: Function Approximation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When neural networks learn new representations of data, they do so by combining
    weights and biases with neurons from different layers. They adjust the weights
    of these connections every time a training cycle takes place using a mathematical
    technique called back propagation. The weights and biases improve at each round,
    up to the point that an optimum is achieved.This means that a neural network can
    measure how wrong it is on every training cycle, adjust the weights and biases
    of each neuron, and try again. If it determines that a certain modification produces
    better results than the previous round, it will invest in that modification until
    an optimal solution is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, that procedure is the reason why neural networks can approximate
    functions. However, there are many reasons why a neural network may not be able
    to predict a function with perfection, chief among them being that:'
  prefs: []
  type: TYPE_NORMAL
- en: Many functions contain stochastic properties (that is, random properties)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be overfitting to peculiarities from the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be a lack of training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many practical applications, simple neural networks are able to approximate
    a function with reasonable precision. These sorts of applications will be our
    focus.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning techniques are best suited to problems that can be defiled with
    formal mathematical rules (that is, as data representations). If a problem is
    hard to define this way, then it is likely that deep learning will not provide
    a useful solution. Moreover, if the data available for a given problem is either
    biased or only contains partial representations of the underlying functions that
    generate that problem, then deep learning techniques will only be able to reproduce
    the problem and not learn to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that deep learning algorithms are learning different representations
    of data to approximate a given function. If data does not represent a function
    appropriately, it is likely that a function will be incorrectly represented by
    a neural network. Consider the following analogy: you are trying to predict the
    national prices of gasoline (that is, fuel) and create a deep learning model.
    You use your credit card statement with your daily expenses on gasoline as an
    input data for that model. The model may eventually learn the patterns of your
    gasoline consumption, but it will likely misrepresent price fluctuations of gasoline
    caused by other factors only represented weekly in your data such as government
    policies, market competition, international politics, and so on. The model will
    ultimately yield incorrect results when used in production.'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this problem, make sure that the data used to train a model represents
    the problem the model is trying to address as accurately as possible.
  prefs: []
  type: TYPE_NORMAL
- en: For an in-depth discussion of this topic, refer to François Chollet's upcoming
    book Deep Learning with Python. François is the creator of Keras, a Python library
    used in this book. The chapter, The limitations of deep learning, is particularly
    important for understanding this topic. The working version of that book is available
    at: [https://blog.keras.io/the-limitations-of-deeplearning.html](https://blog.keras.io/the-limitations-of-deep-learning.html).
  prefs: []
  type: TYPE_NORMAL
- en: Inherent Bias and Ethical Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Researchers have suggested that the use of the deep learning model without considering
    the inherent bias in the training data can lead not only to poor performing solutions,
    but also to ethical complications.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in late 2016, researchers from the Shanghai Jiao Tong University
    in China created a neural network which correctly classified criminals using only
    pictures from their faces. The researchers used 1,856 images of Chinese men in
    which half had been convicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Their model identifiled inmates with 89.5 percent accuracy. ([https:// blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html)).
    MIT Technology Review. Neural Network Learns to Identify Criminals by Their Faces.
    November 22, 2016\. Available at: [https://www.technologyreview.com/s/602955/neuralnetwork-learns-to-identify-criminals-by-their-faces/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper resulted in great furor within the scientific community and popular
    media. One key issue with the proposed solution is that it fails to properly recognize
    the bias inherent in the input data. Namely, the data used in this study came
    from two different sources: one for criminals and one for non-criminals. Some
    researchers suggest that their algorithm identifies patterns associated with the
    different data sources used in the study instead of identifying relevant patterns
    from people''s faces. While there are technical considerations one can make about
    the reliability of the model, the key criticism is on ethical grounds: one ought
    to clearly recognize the inherent bias in input data used by deep learning algorithms
    and consider how its application will have an impact on people''s lives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Timothy Revell. Concerns as face recognition tech used to ''identify'' criminals.
    New Scientist. December 1, 2016\. Available at: [https://www.newscientist.com/article/2114900-concernsas-face-recognition-tech-used-to-identify-criminals/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/).
    For understanding more about the topic of ethics in learning algorithms (including
    deep learning), refer to the work done by the AI Now Institute ([https://ainowinstitute.org/](https://ainowinstitute.org/)),
    an organization created for the understanding of the social implications of intelligent
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Common Components and Operations of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks have two key components: layers and nodes. Nodes are responsible
    for specific operations, and layers are groups of nodes used to differentiate
    different stages of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, neural networks have the following three categories of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: Where the input data is received and first interpreted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden**: Where computations take place, modifying data as it goes through'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: Where an output is assembled and evaluated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/45a615a6-4d0b-4b53-aeec-f523b88145a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of the most common layers in a neural network. By Glosser.ca
    - Own work, Derivative of File: Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers are the most important layers in neural networks. They are referred
    to as *hidden* because the representations generated in them are not available
    in the data, but are learned from it. It is within these layers where the main
    computations take place in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes are where data is represented in the network. There are two values associated
    with nodes: biases and weights. Both of these values affect how data is represented
    by the nodes and passed on to other nodes. When a network learns, it effectively
    adjusts these values to satisfy an optimization function.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the work in neural networks happens in the hidden layers. Unfortunately,
    there isn't a clear rule for determining how many layers or nodes a network should
    have. When implementing a neural network, one will probably spend time experimenting
    with different combinations of layers and nodes. It is advised to always start
    with a single layer and also with a number of nodes that reflect the number of
    features the input data has (that is, how many *columns* are available in a dataset).
    One will then continue to add layers and nodes until satisfactory performance
    is achieved—or whenever the network starts over fitting to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Contemporary neural network practice is generally restricted to the experimentation
    with the number of nodes and layers (for example, how deep the network is), and
    the kinds of operations performed at each layer. There are many successful instances
    in which neural networks outperformed other algorithms simply by adjusting these
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: As an intuition, think about data entering a neural network system via the input
    layer, then moving through the network from node to node. The path that data takes
    will depend on how interconnected the nodes are, the weights and the biases of
    each node, the kind of operations that are performed in each layer, and the state
    of data at the end of such operations. Neural networks often require many "runs"
    (or epochs) in order to keep tuning the weights and biases of nodes, meaning that
    data flaws over the different layers of the graph multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section offered you an overview of neural networks and deep learning.
    Additionally, we discussed a starter''s intuition to understand the following
    key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can, in principle, approximate most functions, given that it
    has enough resources and data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers and nodes are the most important structural components of neural networks.
    One typically spends a lot of time altering those components to find a working
    architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights and biases are the key properties that a network "learns" during its
    training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those concepts will prove useful in our next section as we explore a real-world
    trained neural network and make modifications to train our own.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Deep Learning Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we finish this chapter, we want you to interact with a real neural network.
    We will start by covering the main software components used throughout this book
    and make sure that they are properly installed. We will then explore a pretrained
    neural network and explore a few of the components and operations discussed earlier
    in the What are Neural Networks? section.
  prefs: []
  type: TYPE_NORMAL
- en: Software Components for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use the following software components for deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using Python 3 . Python is a general-purpose programming language
    which is very popular with the scientific community—hence its adoption in deep
    learning. Python 2 is not supported in this book but can be used to train neural
    networks instead of Python 3\. Even if you chose to implement your solutions in
    Python 2, consider moving to Python 3 as its modern feature set is far more robust
    than that of its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a library used for performing mathematical operations in the form
    of graphs. TensorFlow was originally developed by Google and today it is an open-source
    project with many contributors. It has been designed with neural networks in mind
    and is among the most popular choices when creating deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is also well-known for its production components. It comes with TensorFlow
    Serving [(https://github.com/tensorflow/serving),](https://github.com/tensorflow/serving)
    a high-performance system for serving deep learning models. Also, trained TensorFlow
    models can be consumed in other high-performance programming languages such as
    Java, Go, and C. This means that one can deploy these models in anything from
    a micro-computer (that is, a RaspberryPi) to an Android device.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to interact efficiently with TensorFlow, we will be using Keras ([https://keras.io/](https://keras.io/)),
    a Python package with a high-level API for developing neural networks. While TensorFlow
    focuses on components that interact with each other in a computational graph,
    Keras focuses specifically on neural networks. Keras uses TensorFlow as its backend
    engine and makes developing such applications much easier.
  prefs: []
  type: TYPE_NORMAL
- en: As of November 2017 (TensorFlow version 1.4), Keras is distributed as part of
    TensorFlow. It is available under the `tf.keras` namespace. If you have TensorFlow
    1.4 or higher installed, you already have Keras available in your system.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is a data visualization suite for exploring TensorFlow models and
    is natively integrated with TensorFlow. TensorBoard works by consuming checkpoint
    and summary files created by TensorFlow as it trains a neural network. Those can
    be explored either in near real-time (with a 30 second delay) or after the network
    has finished training.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard makes the process of experimenting and exploring a neural network
    much easier—plus it's quite exciting to follow the training of your network!
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebooks, Pandas, and NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working to create deep learning models with Python, it is common to start
    working interactively, slowly developing a model that eventually turns into more
    structured software. Three Python packages are used frequently during that process:
    Jupyter Notebooks, Pandas, and `NumPy`:'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebooks create interactive Python sessions that use a web browser
    as its interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas is a package for data manipulation and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy is frequently used for shaping data and performing numerical computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These packages are used occasionally. They typically do not form part of a production
    system, but are often used when exploring data and starting to build a model.
    We focus on the other tools in much more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The book *Learning Pandas* by Michael Heydt (June 2017, Packt Publishing) and
    *Learning Jupyter* by Dan Toomey (November 2016, Packt Publishing) offer a comprehensive
    guide on how to use these technologies. These books are good references for continuing
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component**  | **Description**  | **Minimum Version**  |'
  prefs: []
  type: TYPE_TB
- en: '| Python  | General-purpose programming language. Popular language   used in 
    the development of deep learning applications.  |  3.6  |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow  | Open-source graph computation Python package typically used
    for  developing deep learning systems.  |  1.4  |'
  prefs: []
  type: TYPE_TB
- en: '| Keras  | Python package that provides a high-level interface to TensorFlow.  |
     2.0.8-tf (distributed with TensorFlow)'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorBoard  | Browser-based software for visualizing neural network statistics.  |
     0.4.0  |'
  prefs: []
  type: TYPE_TB
- en: '| Jupyter Notebook  | Browser-based software for working interactively  with
    Python sessions.  |  5.2.1  |'
  prefs: []
  type: TYPE_TB
- en: '| Pandas  | Python package for analyzing and manipulating data.  |  0.21.0  |'
  prefs: []
  type: TYPE_TB
- en: '| NumPy  | Python package for high-performance numerical computations.  |  1.13.3  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Software components necessary for creating a deep learning environment'
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Verifying Software Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we explore a trained neural network, let's verify that all the software
    components that we need are available. We have included a script that verifies
    these components work. Let's take a moment to run the script and deal with any
    eventual problems we may find.
  prefs: []
  type: TYPE_NORMAL
- en: We will now be testing if the software components required for this book are
    available in your working environment. First, we suggest the creation of a Python
    virtual environment using Python's native module `venv`. Virtual environments
    are used for managing project dependencies. We advise each project you create
    to have its own virtual environments. Let's create one now.
  prefs: []
  type: TYPE_NORMAL
- en: If you are more comfortable with `conda` environments, feel free to use those
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Python virtual environment can be created by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The latter command will append the string (`venv`) to the beginning of your command
    line. Use the following command to deactivate your virtual environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to always activate your Python virtual environment when working on
    a project.
  prefs: []
  type: TYPE_NORMAL
- en: 'After activating your virtual environment, make sure that the right components
    are installed by executing pip over the file `requirements.txt`. This will attempt
    to install the models used in this book in that virtual environment. It will do
    nothing if they are already available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0d4be57f-1d5b-4db6-8117-43b5b8dde66b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Image of a terminal running pip to install dependencies from requirements.txt'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install dependencies by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will install all required dependencies for your system. If they are already
    installed, this command should simply inform you.
  prefs: []
  type: TYPE_NORMAL
- en: These dependencies are essential for working with all code activities.
  prefs: []
  type: TYPE_NORMAL
- en: As a final step on this activity, let's execute the script `test_stack.py`.
    That script formally verifies that all the required packages for this book are
    installed and available in your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Students, run the script `Chapter_4/activity_1/test_stack.py` to check if the
    dependencies Python 3, TensorFlow, and Keras are available. Use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The script returns helpful messages stating what is installed and what needs
    to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following script command in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a help message that explains what each command does. If you
    do not see that message – or see an error message instead – please ask for assistance from
    your instructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3898b1a8-36ce-43d2-bb14-5b5b79874129.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Image of a terminal running `python3 test_stack.py`. The script returns
    messages informing that all dependencies are installed correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a similar message to the following appears, there is no need to worry:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Runtime Warning: compile time version 3.5 of module ''`tensorflow.python.framework.fast_tensor_util`''does
    not match runtime version 3.6 return f(*args, **kwds)'
  prefs: []
  type: TYPE_NORMAL
- en: That message appears if you are running Python 3.6 and the distributed
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow wheel was compiled under a different version (in this case, 3.5).
    You can safely ignore that message.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have verified that Python 3, TensorFlow, Keras, TensorBoard, and the
    packages outlined in `requirements.txt` have been installed, we can continue to
    a demo on how to train a neural network and then go on to explore a trained network
    using these same tools.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a Trained Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore a trained neural network. We do that to understand
    how a neural network solves a real-world problem (predict handwritten digits)
    and also to get familiar with the TensorFlow API. When exploring that neural network,
    we will recognize many components introduced in previous sections such as nodes
    and layers, but we will also see many that we don't recognize (such as activation
    functions)—we will explore those in further sections. We will then walk through
    an exercise on how that neural network was trained and then train that same network
    on our own.
  prefs: []
  type: TYPE_NORMAL
- en: The network that we will be exploring has been trained to recognize numerical
    digits (integers) using images of handwritten digits. It uses the MNIST dataset
    ([http:// yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)), a classic
    dataset frequently used for exploring pattern  recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Modifiled National Institute of Standards and Technology** (**MNIST**)
    dataset contains a training set of 60,000 images and a test set of 10,000 images.
    Each image contains a single handwritten number. This dataset—which is a derivate
    from one created by the US Government—was originally used to test different approaches
    to the problem of recognizing handwritten text by computer systems. Being able
    to do that was important for the purpose of increasing the performance of postal
    services, taxation systems, and government services. The MNIST dataset is considered
    too naïve for contemporary methods. Different and more recent datasets are used
    in contemporary research (for example, CIFAR). However, the MNIST dataset is still
    very useful for understanding how neural networks work because known models can
    achieve a high level of accuracy with great efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CIFAR dataset is a machine learning dataset that contains images organized
    in different classes. Different than the MNIST dataset, the CIFAR dataset contains
    classes in many different areas such as animals, activities, and objects. The
    CIFAR dataset is available at: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bd59dd7-5cc3-4825-8b1e-02a29a48e636.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Excerpt from the training set of the MNIST dataset. Each image is
    a separate 20x20 pixels image with a single handwritten digit. The original dataset
    is available at: http://yann.lecun.com/exdb/mnist/.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's train a neural network to recognize new digits using the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be implementing a special-purpose neural network called "Convolutional
    Neural Network" to solve this problem (we will discuss those in more detail in
    further sections). Our network contains three hidden layers: two fully connected
    layers and a convolutional layer. The convolutional layer is defiled by the following
    TensorFlow snippet of Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We execute that snippet of code only once during the training of our network.
  prefs: []
  type: TYPE_NORMAL
- en: The variables W and B stand for weights and biases. These are the values used
    by the nodes within the hidden layers to alter the network's interpretation of
    the data as it passes through the network. Do not worry about the other variables
    for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **fully connected layers** are defiled by the following snippet of Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we also have the two TensorFlow variables W and B. Notice how simple
    the initialization of these variables is: W is initialized as a random value from
    a pruned Gaussian distribution (pruned with `size_in and size_out`) with a standard
    deviation of 0.1, and B (the bias term) is initialized as `0.1,` a constant. Both
    these values will continuously change during each run. That snippet is executed
    twice, yielding two fully connected networks— one passing data to the other.'
  prefs: []
  type: TYPE_NORMAL
- en: Those 11 lines of Python code represent our complete neural network. We will
    go into a lot more detail in *Chapter 5*, *Model Architecture* about each one
    of those components using Keras. For now, focus on understanding that the network
    is altering the values of W and B in each layer on every run and how these snippets
    form different layers. These 11 lines of Python are the culmination of dozens
    of years of neural network research.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now train that network to evaluate how it performs in the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the following steps to set up this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open two terminal instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In both of them, navigate to the directory `chapter_4/exercise_a`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In both of them, make sure that your Python 3 virtual environment is active
    and that the requirements outlined in `requirements.txt` are installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In one of them, start a TensorBoard server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$ tensorboard --logdir=mnist_example/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the other, run the `train_mnist.py` script from within that directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your browser in the TensorBoard URL provided when you start the server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the terminal that you ran the script `train_mnist.py`, you will see a progress
    bar with the epochs of the model. When you open the browser page, you will see
    a couple of graphs. Click on the one that reads `Accuracy`, enlarge it and let
    the page refresh (or click on the `refresh` button). You will see the model gaining
    accuracy as epochs go by.
  prefs: []
  type: TYPE_NORMAL
- en: Use that moment to explain the power of neural networks in reaching a high level
    of accuracy very early in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that in about 200 epochs (or steps), the network surpassed 90 percent
    accuracy. That is, the network is getting 90 percent of the digits in the test
    set correctly. The network continues to gain accuracy as it trains up to the 2,000th
    step, reaching a 97 percent accuracy at the end of that period.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's also test how well those networks perform with unseen data. We will
    use an open-source web application created by Shafeen Tejani to explore if a trained
    network correctly predicts handwritten digits that we create.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Network Performance with Unseen Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visit the website [http://mnist-demo.herokuapp.com/](http://mnist-demo.herokuapp.com/)
    in your browser and draw a number between 0 and 9 in the designated white box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a788e857-da7e-40ac-9b39-b83d78219375.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Web application in which we can manually draw digits and test the
    accuracy of two trained networks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://github.com/ShafeenTejani/mnist-demo](https://github.com/ShafeenTejani/mnist-demo)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the application, you can see the results of two neural networks. The one
    that we have trained is on the left (called CNN). Does it classify all your handwritten
    digits correctly? Try drawing numbers at the edge of the designated area. For
    instance, try drawing the number **1** close to the right edge of that area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bf7e60d-4981-42f6-97df-64bbc1d1fe72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Both networks have a difficult time estimating values drawn on the
    edges of the area'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we see the number **1** drawn to the right side of the drawing
    area. The probability of this number being a **1** is **0** in both networks.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset does not contain numbers on the edges of images. Hence, neither
    network assigns relevant values to the pixels located in that region. Both networks
    are much better at classifying numbers correctly if we draw them closer to the
    center of the designated area. This shows that neural networks can only be as
    powerful as the data that is used to train them. If the data used for training
    is very different than what we are trying to predict, the network will most likely
    produce disappointing results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: Exploring a Trained Neural Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the neural network that we have trained during
    our exercise. We will also train a few other networks by altering hyper parameters
    from our original one. Let's start by exploring the network trained in our exercise.
  prefs: []
  type: TYPE_NORMAL
- en: We have provided that same trained network as binary files in the directory.
    Let's open that trained network using TensorBoard and explore its components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your terminal, navigate to the directory `chapter_4/activity_2` and execute
    the following command to start TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open the URL provided by TensorBoard in your browser. You should be able
    to see the TensorBoard scalars page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82521ed5-42e0-4c0b-8c13-3d47a26ca2b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Image of a terminal after starting a TensorBoard instance'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you open the URL provided by the `tensorboard` command, you should be
    able to see the following TensorBoard page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52461105-3b8e-47e5-9bef-def7c8431481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Image of the TensorBoard landing page'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now explore our trained neural network and see how it performed.
  prefs: []
  type: TYPE_NORMAL
- en: On the TensorBoard page, click on the **Scalars** page and enlarge the **Accuracy**
    graph. Now, move the **Smoothing** slider to **0.9**.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy graph measures how accurate the network was able to guess the labels
    of a test set. At fist, the network guesses those labels completely wrong. This
    happens because we have initialized the weights and biases of our network with
    random values, so its fist attempts are a guess. The network will then change
    the weights and biases of its layers on a second run; the network will continue
    to invest in the nodes that give positive results by altering their weights and
    biases, and penalize those that don't by gradually reducing their impact on the
    network (eventually reaching 0). As you can see, this is a really efficient technique
    that quickly yields great results.
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus our attention on the **Accuracy** graph. See how the algorithm manages
    to reach great accuracy (> 95 percent) after around 1,000 epochs? What happens
    between 1,000 and 2,000 epochs?
  prefs: []
  type: TYPE_NORMAL
- en: Would it get more accurate if we continued to train with more epochs? Between
    1,000 and 2,000 is when the accuracy of the network continues to improve, but
    at a decreasing rate. The network may improve slightly if trained with more epochs,
    but it will not reach 100 percent accuracy with the current architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script is a modified version of an official Google script that was created
    to show how TensorFlow works. We have divided the script into functions that are
    easier to understand and added many comments to guide your learning. Try running
    that script by modifying the variables at the top of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, try running that script by modifying the values of those variables. For
    instance, try modifying the learning rate to **0.1** and the epochs to **100**.
    Do you think the network can achieve comparable results?
  prefs: []
  type: TYPE_NORMAL
- en: There are many other parameters that you can modify in your neural network.
    For now, experiment with the epochs and the learning rate of your network. You
    will notice that those two on their own can greatly change the output of your
    network—but only by so much. Experiment to see if you can train this network faster
    with the current architecture just by altering those two parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Verify how your network is training using TensorBoard. Alter those parameters
    a few more times by multiplying the starting values by 10 until you notice that
    the network is improving. This process of tuning the network and finding improved
    accuracy is similar to what is used in industry applications today to improve
    existing neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a TensorFlow-trained neural network using TensorBoard
    and trained our own modified version of that network with different epochs and
    learning rates. This gave you hands-on experiences on how to train a highly performant
    neural network and also allowed you to explore some of its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Do you think we can achieve similar accuracy with real Bitcoin data? We will
    attempt to predict future Bitcoin prices using a common neural network algorithm
    during C*hapter 5*, *Model Architecture*. In C*hapter 6*, *Model Evaluation and
    Optimization*, we will evaluate and improve that model and, finally, in C*hapter
    7*, *Productization*, we will create a program that serves the prediction of that
    system via a HTTP API.
  prefs: []
  type: TYPE_NORMAL
