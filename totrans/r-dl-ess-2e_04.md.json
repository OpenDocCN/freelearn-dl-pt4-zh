["```py\nlibrary(mxnet) # 1\nctx = mx.cpu() # 2\na <- mx.nd.ones(c(2,3),ctx=ctx) # 3\nb <- a * 2 + 1 # 4\ntypeof(b) # 5\n[1] \"externalptr\"\nclass(b) # 6\n[1] \"MXNDArray\"\nb # 7\n     [,1] [,2] [,3]\n[1,]    3    3    3\n[2,]    3    3    3\n```", "```py\ndata <- mx.symbol.Variable(\"data\")\n```", "```py\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=64)\nact1 <- mx.symbol.Activation(fc1, name=\"activ1\", act_type=activ)\n\ndrop1 <- mx.symbol.Dropout(data=act1,p=0.2)\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=32)\nact2 <- mx.symbol.Activation(fc2, name=\"activ2\", act_type=activ)\n\n.....\nsoftmax <- mx.symbol.SoftmaxOutput(fc4, name=\"sm\")\n```", "```py\nsoftmax <- mx.symbol.SoftmaxOutput(fc4, name=\"sm\")\nmodel <- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,\n                                     ctx = devices,num.round = num_epochs,\n                                     ................\n```", "```py\ntrain.x <- read.table(\"../data/UCI HAR Dataset/train/X_train.txt\")\ntrain.y <- read.table(\"../data/UCI HAR Dataset/train/y_train.txt\")[[1]]\ntest.x <- read.table(\"../data/UCI HAR Dataset/test/X_test.txt\")\ntest.y <- read.table(\"../data/UCI HAR Dataset/test/y_test.txt\")[[1]]\nfeatures <- read.table(\"../data/UCI HAR Dataset/features.txt\")\nmeanSD <- grep(\"mean\\\\(\\\\)|std\\\\(\\\\)\", features[, 2])\ntrain.y <- train.y-1\ntest.y <- test.y-1\n```", "```py\ntrain.x <- t(train.x[,meanSD])\ntest.x <- t(test.x[,meanSD])\ntrain.x <- data.matrix(train.x)\ntest.x <- data.matrix(test.x)\n```", "```py\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=64)\nact1 <- mx.symbol.Activation(fc1, name=\"relu1\", act_type=\"relu\")\nfc2 <- mx.symbol.FullyConnected(act1, name=\"fc2\", num_hidden=32)\nact2 <- mx.symbol.Activation(fc2, name=\"relu2\", act_type=\"relu\")\nfc3 <- mx.symbol.FullyConnected(act2, name=\"fc3\", num_hidden=6)\nsoftmax <- mx.symbol.SoftmaxOutput(fc3, name=\"sm\")\n```", "```py\ndevices <- mx.cpu()\nmx.set.seed(0)\ntic <- proc.time()\nmodel <- mx.model.FeedForward.create(softmax, X = train.x, y = train.y,\n                                      ctx = devices,num.round = 20,\n                                      learning.rate = 0.08, momentum = 0.9,\n                                      eval.metric = mx.metric.accuracy,\n                                      initializer = mx.init.uniform(0.01),\n                                      epoch.end.callback =\n                                        mx.callback.log.train.metric(1))\nStart training with 1 devices\n[1] Train-accuracy=0.185581140350877\n[2] Train-accuracy=0.26104525862069\n[3] Train-accuracy=0.555091594827586\n[4] Train-accuracy=0.519127155172414\n[5] Train-accuracy=0.646551724137931\n[6] Train-accuracy=0.733836206896552\n[7] Train-accuracy=0.819100215517241\n[8] Train-accuracy=0.881869612068966\n[9] Train-accuracy=0.892780172413793\n[10] Train-accuracy=0.908674568965517\n[11] Train-accuracy=0.898572198275862\n[12] Train-accuracy=0.896821120689655\n[13] Train-accuracy=0.915544181034483\n[14] Train-accuracy=0.928879310344828\n[15] Train-accuracy=0.926993534482759\n[16] Train-accuracy=0.934401939655172\n[17] Train-accuracy=0.933728448275862\n[18] Train-accuracy=0.934132543103448\n[19] Train-accuracy=0.933324353448276\n[20] Train-accuracy=0.934132543103448\nprint(proc.time() - tic)\n   user system elapsed \n   7.31 3.03 4.31 \n```", "```py\n\npreds1 <- predict(model, test.x)\npred.label <- max.col(t(preds1)) - 1\nt <- table(data.frame(cbind(test.y,pred.label)),\n            dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test.y),2)\nprint(t)\n      Predicted\nActual   0   1   2   3   4   5\n     0 477  15   4   0   0   0\n     1 108 359   4   0   0   0\n     2  13  42 365   0   0   0\n     3   0   0   0 454  37   0\n     4   0   0   0 141 391   0\n     5   0   0   0  16   0 521\nprint(sprintf(\" Deep Learning Model accuracy = %1.2f%%\",acc))\n[1] \" Deep Learning Model accuracy = 87.11%\"\n```", "```py\nNumber of weeks we have data: 117.\nNumber of transaction lines: 2541019.\nNumber of transactions (baskets): 390320.\nNumber of unique Customers: 5000.\nNumber of unique Products: 4997.\nNumber of unique Stores: 761.\n```", "```py\nPROD_CODE: Number of unique codes: 4997\\. Number of repeated codes: 0.\nPROD_CODE_10: Number of unique codes:250\\. Number of repeated codes: 0.\nPROD_CODE_20: Number of unique codes:90\\. Number of repeated codes: 0.\nPROD_CODE_30: Number of unique codes:31\\. Number of repeated codes: 0.\nPROD_CODE_40: Number of unique codes:9.\n```", "```py\nPROD_CODE_40 : Chilled goods\n  PROD_CODE_30 : Dairy\n    PROD_CODE_20 : Fresh Milk\n      PROD_CODE_10 : Full-fat Milk\n        PROD_CODE : Brand x Full-fat Milk\n```", "```py\nlibrary(readr)\nlibrary(reshape2)\nlibrary(dplyr)\n\nsource(\"import.R\")\n\n# step 1, merge files\nimport_data(data_directory,bExploreData=0)\n\n# step 2, group and pivot data\nfileName <- paste(data_directory,\"all.csv\",sep=\"\")\nfileOut <- paste(data_directory,\"predict.csv\",sep=\"\")\ndf <- read_csv(fileName,col_types = cols(.default = col_character()))\n\n# convert spend to numeric field\ndf$SPEND<-as.numeric(df$SPEND)\n\n# group sales by date. we have not converted the SHOP_DATE to date\n# but since it is in yyyymmdd format,\n# then ordering alphabetically will preserve date order\nsumSalesByDate<-df %>%\n   group_by(SHOP_WEEK,SHOP_DATE) %>%\n   summarise(sales = sum(SPEND)\n   )\n\n# we want to get the cut-off date to create our data model\n# this is the last date and go back 13 days beforehand\n# therefore our X data only looks at everything from start to max date - 13 days\n# and our Y data only looks at everything from max date - 13 days to end (i.e. 14 days)\nmax(sumSalesByDate$SHOP_DATE)\n[1] \"20080706\"\nsumSalesByDate2 <- sumSalesByDate[order(sumSalesByDate$SHOP_DATE),]\ndatCutOff <- as.character(sumSalesByDate2[(nrow(sumSalesByDate2)-13),]$SHOP_DATE)\ndatCutOff\n[1] \"20080623\"\nrm(sumSalesByDate,sumSalesByDate2)\n```", "```py\n# we are going to limit our data here from year 2008 only\n# group data and then pivot it\nsumTemp <- df %>%\n   filter((SHOP_DATE < datCutOff) & (SHOP_WEEK>=\"200801\")) %>%\n   group_by(CUST_CODE,SHOP_WEEK,PROD_CODE_40) %>%\n   summarise(sales = sum(SPEND)\n   )\nsumTemp$fieldName <- paste(sumTemp$PROD_CODE_40,sumTemp$SHOP_WEEK,sep=\"_\")\ndf_X <- dcast(sumTemp,CUST_CODE ~ fieldName, value.var=\"sales\")\ndf_X[is.na(df_X)] <- 0\n```", "```py\n# y data just needs a group to get sales after cut-off date\ndf_Y <- df %>%\n   filter(SHOP_DATE >= datCutOff) %>%\n   group_by(CUST_CODE) %>%\n   summarise(sales = sum(SPEND)\n   )\ncolnames(df_Y)[2] <- \"Y_numeric\"\n\n# use left join on X and Y data, need to include all values from X\n# even if there is no Y value\ndfModelData <- merge(df_X,df_Y,by=\"CUST_CODE\", all.x=TRUE)\n# set binary flag\ndfModelData$Y_categ <- 0\ndfModelData[!is.na(dfModelData$Y_numeric),]$Y_categ <- 1\ndfModelData[is.na(dfModelData$Y_numeric),]$Y_numeric <- 0\nrm(df,df_X,df_Y,sumTemp)\n\nnrow(dfModelData)\n[1] 3933\ntable(dfModelData$Y_categ)\n   0    1 \n1560 2373 \n\n# shuffle data\ndfModelData <- dfModelData[sample(nrow(dfModelData)),]\n\nwrite_csv(dfModelData,fileOut)\n```", "```py\nset.seed(42)\nfileName <- \"../dunnhumby/predict.csv\"\ndfData <- read_csv(fileName,\n                    col_types = cols(\n                      .default = col_double(),\n                      CUST_CODE = col_character(),\n                      Y_categ = col_integer())\n                    )\nnobs <- nrow(dfData)\ntrain <- sample(nobs, 0.9*nobs)\ntest <- setdiff(seq_len(nobs), train)\npredictorCols <- colnames(dfData)[!(colnames(dfData) %in% c(\"CUST_CODE\",\"Y_numeric\",\"Y_categ\"))]\n\n# data is right-skewed, apply log transformation\nqplot(dfData$Y_numeric, geom=\"histogram\",binwidth=10,\n       main=\"Y value distribution\",xlab=\"Spend\")+theme(plot.title = element_text(hjust = 0.5))\ndfData[, c(\"Y_numeric\",predictorCols)] <- log(0.01+dfData[, c(\"Y_numeric\",predictorCols)])\nqplot(dfData$Y_numeric, geom=\"histogram\",binwidth=0.5,\n       main=\"log(Y) value distribution\",xlab=\"Spend\")+theme(plot.title = element_text(hjust = 0.5))\ntrainData <- dfData[train, c(predictorCols)]\ntestData <- dfData[test, c(predictorCols)]\ntrainData$Y_categ <- dfData[train, \"Y_categ\"]$Y_categ\ntestData$Y_categ <- dfData[test, \"Y_categ\"]$Y_categ\n```", "```py\n#Logistic Regression Model\nlogReg=glm(Y_categ ~ .,data=trainData,family=binomial(link=\"logit\"))\npr <- as.vector(ifelse(predict(logReg, type=\"response\",\n                                testData) > 0.5, \"1\", \"0\"))\n# Generate the confusion matrix showing counts.\nt<-table(dfData[test, c(predictorCols, \"Y_categ\")]$\"Y_categ\", pr,\n          dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test),2)\nprint(t)\n      Predicted\nActual   0   1\n     0 130  42\n     1  48 174\nprint(sprintf(\" Logistic regression accuracy = %1.2f%%\",acc))\n[1] \" Logistic regression accuracy = 77.16%\"\nrm(t,pr,acc)\n\nrf <- randomForest::randomForest(as.factor(Y_categ) ~ .,\n                                  data=trainData,\n                                  na.action=randomForest::na.roughfix)\npr <- predict(rf, newdata=testData, type=\"class\")\n# Generate the confusion matrix showing counts.\nt<-table(dfData[test, c(predictorCols, \"Y_categ\")]$Y_categ, pr,\n          dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test),2)\nprint(t)\n      Predicted\nActual   0   1\n     0 124  48\n     1  30 192\nprint(sprintf(\" Random Forest accuracy = %1.2f%%\",acc))\n[1] \" Random Forest accuracy = 80.20%\"\nrm(t,pr,acc)\n\nxgb <- xgboost(data=data.matrix(trainData[,predictorCols]), label=trainData[,\"Y_categ\"]$Y_categ,\n                nrounds=75, objective=\"binary:logistic\")\npr <- as.vector(ifelse(\n   predict(xgb, data.matrix(testData[, predictorCols])) > 0.5, \"1\", \"0\"))\nt<-table(dfData[test, c(predictorCols, \"Y_categ\")]$\"Y_categ\", pr,\n          dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test),2)\nprint(t)\n      Predicted\nActual   0   1\n     0 125  47\n     1  44 178\nprint(sprintf(\" XGBoost accuracy = %1.2f%%\",acc))\n[1] \" XGBoost accuracy = 76.90%\"\nrm(t,pr,acc)\n```", "```py\nrequire(mxnet)\n\n# MXNet expects matrices\ntrain_X <- data.matrix(trainData[, predictorCols])\ntest_X <- data.matrix(testData[, predictorCols])\ntrain_Y <- trainData$Y_categ\n\n# hyper-parameters\nnum_hidden <- c(128,64,32)\ndrop_out <- c(0.2,0.2,0.2)\nwd=0.00001\nlr <- 0.03\nnum_epochs <- 40\nactiv <- \"relu\"\n\n# create our model architecture\n# using the hyper-parameters defined above\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=num_hidden[1])\nact1 <- mx.symbol.Activation(fc1, name=\"activ1\", act_type=activ)\n\ndrop1 <- mx.symbol.Dropout(data=act1,p=drop_out[1])\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=num_hidden[2])\nact2 <- mx.symbol.Activation(fc2, name=\"activ2\", act_type=activ)\n\ndrop2 <- mx.symbol.Dropout(data=act2,p=drop_out[2])\nfc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\", num_hidden=num_hidden[3])\nact3 <- mx.symbol.Activation(fc3, name=\"activ3\", act_type=activ)\n\ndrop3 <- mx.symbol.Dropout(data=act3,p=drop_out[3])\nfc4 <- mx.symbol.FullyConnected(drop3, name=\"fc4\", num_hidden=2)\nsoftmax <- mx.symbol.SoftmaxOutput(fc4, name=\"sm\")\n\n# run on cpu, change to 'devices <- mx.gpu()'\n# if you have a suitable GPU card\ndevices <- mx.cpu()\nmx.set.seed(0)\ntic <- proc.time()\n# This actually trains the model\nmodel <- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,\n                                      ctx = devices,num.round = num_epochs,\n                                      learning.rate = lr, momentum = 0.9,\n                                      eval.metric = mx.metric.accuracy,\n                                      initializer = mx.init.uniform(0.1),\n                                      wd=wd,\n                                      epoch.end.callback = mx.callback.log.train.metric(1))\nprint(proc.time() - tic)\n   user system elapsed \n   9.23 4.65 4.37 \n\npr <- predict(model, test_X)\npred.label <- max.col(t(pr)) - 1\nt <- table(data.frame(cbind(testData[,\"Y_categ\"]$Y_categ,pred.label)),\n            dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test),2)\nprint(t)\n      Predicted\nActual   0   1\n     0 136  36\n     1  54 168\nprint(sprintf(\" Deep Learning Model accuracy = %1.2f%%\",acc))\n[1] \" Deep Learning Model accuracy = 77.16%\"\nrm(t,pr,acc)\nrm(data,fc1,act1,fc2,act2,fc3,act3,fc4,softmax,model)\n```", "```py\nset.seed(42)\nfileName <- \"../dunnhumby/predict.csv\"\ndfData <- read_csv(fileName,\n                    col_types = cols(\n                      .default = col_double(),\n                      CUST_CODE = col_character(),\n                      Y_categ = col_integer())\n )\nnobs <- nrow(dfData)\ntrain <- sample(nobs, 0.9*nobs)\ntest <- setdiff(seq_len(nobs), train)\npredictorCols <- colnames(dfData)[!(colnames(dfData) %in% c(\"CUST_CODE\",\"Y_numeric\",\"Y_numeric\"))]\n\ndfData[, c(\"Y_numeric\",predictorCols)] <- log(0.01+dfData[, c(\"Y_numeric\",predictorCols)])\ntrainData <- dfData[train, c(predictorCols,\"Y_numeric\")]\ntestData <- dfData[test, c(predictorCols,\"Y_numeric\")]\n\nxtrain <- model.matrix(Y_numeric~.,trainData)\nxtest <- model.matrix(Y_numeric~.,testData)\n```", "```py\n# lm Regression Model\nregModel1=lm(Y_numeric ~ .,data=trainData)\npr1 <- predict(regModel1,testData)\nrmse <- sqrt(mean((exp(pr1)-exp(testData[,\"Y_numeric\"]$Y_numeric))^2))\nprint(sprintf(\" Regression RMSE = %1.2f\",rmse))\n[1] \" Regression RMSE = 29.30\"\nmae <- mean(abs(exp(pr1)-exp(testData[,\"Y_numeric\"]$Y_numeric)))\nprint(sprintf(\" Regression MAE = %1.2f\",mae))\n[1] \" Regression MAE = 13.89\"\n```", "```py\nrequire(mxnet)\nLoading required package: mxnet\n\n# MXNet expects matrices\ntrain_X <- data.matrix(trainData[, predictorCols])\ntest_X <- data.matrix(testData[, predictorCols])\ntrain_Y <- trainData$Y_numeric\n\nset.seed(42)\n# hyper-parameters\nnum_hidden <- c(256,128,128,64)\ndrop_out <- c(0.4,0.4,0.4,0.4)\nwd=0.00001\nlr <- 0.0002\nnum_epochs <- 100\nactiv <- \"tanh\"\n\n# create our model architecture\n# using the hyper-parameters defined above\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=num_hidden[1])\nact1 <- mx.symbol.Activation(fc1, name=\"activ1\", act_type=activ)\ndrop1 <- mx.symbol.Dropout(data=act1,p=drop_out[1])\n\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=num_hidden[2])\nact2 <- mx.symbol.Activation(fc2, name=\"activ2\", act_type=activ)\ndrop2 <- mx.symbol.Dropout(data=act2,p=drop_out[2])\n\nfc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\", num_hidden=num_hidden[3])\nact3 <- mx.symbol.Activation(fc3, name=\"activ3\", act_type=activ)\ndrop3 <- mx.symbol.Dropout(data=act3,p=drop_out[3])\n\nfc4 <- mx.symbol.FullyConnected(drop3, name=\"fc4\", num_hidden=num_hidden[4])\nact4 <- mx.symbol.Activation(fc4, name=\"activ4\", act_type=activ)\ndrop4 <- mx.symbol.Dropout(data=act4,p=drop_out[4])\n\nfc5 <- mx.symbol.FullyConnected(drop4, name=\"fc5\", num_hidden=1)\nlro <- mx.symbol.LinearRegressionOutput(fc5)\n\n```", "```py\n# run on cpu, change to 'devices <- mx.gpu()'\n# if you have a suitable GPU card\ndevices <- mx.cpu()\nmx.set.seed(0)\ntic <- proc.time()\n# This actually trains the model\nmodel <- mx.model.FeedForward.create(lro, X = train_X, y = train_Y,\n ctx = devices,num.round = num_epochs,\n learning.rate = lr, momentum = 0.9,\n eval.metric = mx.metric.rmse,\n initializer = mx.init.uniform(0.1),\n wd=wd,\n epoch.end.callback = mx.callback.log.train.metric(1))\nprint(proc.time() - tic)\n user system elapsed \n 13.90 1.82 10.50 \n\npr4 <- predict(model, test_X)[1,]\nrmse <- sqrt(mean((exp(pr4)-exp(testData[,\"Y_numeric\"]$Y_numeric))^2))\nprint(sprintf(\" Deep Learning Regression RMSE = %1.2f\",rmse))\n[1] \" Deep Learning Regression RMSE = 28.92\"\nmae <- mean(abs(exp(pr4)-exp(testData[,\"Y_numeric\"]$Y_numeric)))\nprint(sprintf(\" Deep Learning Regression MAE = %1.2f\",mae))\n[1] \" Deep Learning Regression MAE = 14.33\"\nrm(data,fc1,act1,fc2,act2,fc3,act3,fc4,lro,model)\n```", "```py\n# hyper-parameters\nnum_hidden <- c(256,128,64,32)\ndrop_out <- c(0.2,0.2,0.1,0.1)\nwd=0.0\nlr <- 0.03\nnum_epochs <- 50\nactiv <- \"relu\"\n\n# create our model architecture\n# using the hyper-parameters defined above\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=num_hidden[1])\nact1 <- mx.symbol.Activation(fc1, name=\"activ1\", act_type=activ)\n\ndrop1 <- mx.symbol.Dropout(data=act1,p=drop_out[1])\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=num_hidden[2])\nact2 <- mx.symbol.Activation(fc2, name=\"activ2\", act_type=activ)\n\ndrop2 <- mx.symbol.Dropout(data=act2,p=drop_out[2])\nfc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\", num_hidden=num_hidden[3])\nact3 <- mx.symbol.Activation(fc3, name=\"activ3\", act_type=activ)\n\ndrop3 <- mx.symbol.Dropout(data=act3,p=drop_out[3])\nfc4 <- mx.symbol.FullyConnected(drop3, name=\"fc4\", num_hidden=num_hidden[4])\nact4 <- mx.symbol.Activation(fc4, name=\"activ4\", act_type=activ)\n\ndrop4 <- mx.symbol.Dropout(data=act4,p=drop_out[4])\nfc5 <- mx.symbol.FullyConnected(drop4, name=\"fc5\", num_hidden=2)\nsoftmax <- mx.symbol.SoftmaxOutput(fc5, name=\"sm\")\n\n# run on cpu, change to 'devices <- mx.gpu()'\n# if you have a suitable GPU card\ndevices <- mx.cpu()\nmx.set.seed(0)\ntic <- proc.time()\n# This actually trains the model\nmodel <- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,\n ctx = devices,num.round = num_epochs,\n learning.rate = lr, momentum = 0.9,\n eval.metric = mx.metric.accuracy,\n initializer = mx.init.uniform(0.1),\n wd=wd,\n epoch.end.callback = mx.callback.log.train.metric(1))\nprint(proc.time() - tic)\n user system elapsed \n1919.75 1124.94 871.31\n\npr <- predict(model, test_X)\npred.label <- max.col(t(pr)) - 1\nt <- table(data.frame(cbind(testData[,\"Y_categ\"]$Y_categ,pred.label)),\n dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/length(test),2)\nprint(t)\n      Predicted\nActual     0     1\n 0     10714  4756\n 1      3870 19649\nprint(sprintf(\" Deep Learning Model accuracy = %1.2f%%\",acc))\n[1] \" Deep Learning Model accuracy = 77.88%\"\n```"]