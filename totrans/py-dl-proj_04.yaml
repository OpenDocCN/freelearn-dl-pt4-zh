- en: Building an NLP Pipeline for Building Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our project has expanded once again, thanks to the good work that we've been
    doing. We started off working for a restaurant chain, helping them to classify
    handwritten digits for use in a text notification system, used to alert their
    waiting guests that their table was ready. Based on this success, and when the
    owners realized that their customers were actually responding to the texts, we
    were asked to contribute a deep learning solution using **Natural Language Processing**
    (**NLP**) to accurately classify text into a meaningful sentiment category that
    would give the owners an indication as to their satisfaction with the dining experience.
  prefs: []
  type: TYPE_NORMAL
- en: Do you know what happens to deep learning engineers that do good work? They
    get asked to do more!
  prefs: []
  type: TYPE_NORMAL
- en: This project for the next business use case is pretty cool. What we're being
    asked to do is to create a natural language pipeline that would power a chatbot
    for open domain question answering. The (hypothetical) restaurant chain has a
    website with their menu, history, location, hours, and other information, and
    they would like to add the ability for a website visitor to ask a question in
    a query box, and have our deep learning NLP chatbot find the relevant information
    and present that back. They think that getting the right information back to the
    website visitor quickly would help drive in-store visits and improve the general
    customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: '**Named Entity Recognition** (**NER**) is the approach we will be using, which
    will give us the power we need to quickly classify the input text, which we can
    then match to the relevant content for a response. It''s a great way to take advantage
    of a large corpus of unstructured data that changes without using hardcoded heuristics.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the building blocks of the NLP model, including
    pre-processing, tokenizing, and tagging parts of speech. We will use this understanding
    to build a system able to read an unstructured piece of text, in order to formulate
    an answer for a specific question. We will also describe how to include this deep
    learning component in a classic NLP pipeline to retrieve information, in order
    to provide an open-domain question answering system that doesn't require a structured
    knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a basic FAQ-based chatbot using statistical modeling in a framework, capable
    of detecting intents and entities for answering open-domain questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to generate dense representations of sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a document reader for extracting answers from unstructured text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to integrate deep learning models into a classic NLP pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the goal**: To build a chatbot that understands the context (intent)
    and can also extract the entities. To do this, we need an NLP pipeline that can
    perform intent classification, along with NER extraction to then provide an accurate
    response.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Skills learned**: You will learn how to build an open-domain question answering
    system using a classic NLP pipeline, with a document reader component that uses
    deep learning techniques to generate sentence representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Basics of NLP pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Textual data is a very large source of information, and properly handling it
    is crucial to success. So, to handle this textual data, we need to follow some
    basic text processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the processing steps covered in this section are commonly used in NLP
    and involve combining a number of steps into one executable flow. This is what
    we refer to as the NLP pipeline. This flow can be a combination of tokenization,
    stemming, word frequency, parts of speech tagging, and many more elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look into the details on how to implement the steps in the NLP pipeline
    and, specifically, what each stage of processing does. We will use the **Natural
    Language Toolkit** (**NLTK**) package—an NLP toolkit written in Python, which
    you can install with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code for this project is available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter04/Basic%20NLP%20Pipeline.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization separates a corpus into sentences, words, or tokens. Tokenization
    is needed to make our texts ready for further processing and is the first step
    in creating an NLP pipeline. A token can vary according to the task we are performing
    or the domain in which we are working, so keep an open mind as to what you consider
    as a token!
  prefs: []
  type: TYPE_NORMAL
- en: '**Know the code**: NLTK is powerful, as much of the hard coding work is already
    done in the library. You can read more about NLTK tokenization at [http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI.tokenize_sents).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to load a corpus and use NLTK tokenizer to first tokenize the raw
    corpus into sentences, and then tokenize each sentence further into words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Part-of-Speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some words have multiple meanings, for example, *charge* is a noun, but can
    also be a verb, (*to) charge*. Knowing a **Part-of-Speech** (**POS**) can help
    to disambiguate the meaning. Each token in a sentence has several attributes that
    we can use for our analysis. The POS of a word is one example: nouns are a person,
    place, or thing; verbs are actions or occurrences and adjectives are words that
    describe nouns. Using these attributes, it becomes straightforward to create a
    summary of a piece of text by counting the most common nouns, verbs, and adjectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Extracting nouns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s extract all of the nouns present in the corpus. This is very useful
    practice when you want to extract something specific. We are using `NN`, `NNS`,
    `NNP`, and `NNPS` tags to extract the nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Extracting verbs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s extract all of the verbs present in the corpus. In this case, we are
    using `VB`, `VBD`, `VBG`, `VBN`, `VBP`, and `VBZ` as verb tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use `spacy` to tokenize a piece of text and access the POS attribute
    for each token. As an example application, we''ll tokenize the previous paragraph
    and count the most common nouns with the following code. We''ll also lemmatize
    the tokens, which gives the root form a word, to help us standardize across forms
    of a word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Dependency parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dependency parsing is a way to understand the relationships between words in
    a sentence. Dependency relations are a more fine-grained attribute, available
    to help build the model''s understanding of the words through their relationships
    in a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These relationships between words can get complicated, depending on how sentences
    are structured. The result of dependency-parsing a sentence is a tree data structure,
    with the verb as the root, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6669edc6-66ac-4c9d-955d-3a31c46118e2.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree structure of the dependency parsing of a sentence, with the verb as
    the root.
  prefs: []
  type: TYPE_NORMAL
- en: NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, there''s NER. Named entities are the proper nouns of sentences. Computers
    have gotten pretty good at figuring out if they''re in a sentence and at classifying
    what type of entity they are. `spacy` handles NER at the document level, since
    the name of an entity can span several tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, we just saw some of the basic building blocks of the NLP pipeline. These
    pipelines are consistently used in various NLP projects, be it in machine learning
    or in the deep learning space.
  prefs: []
  type: TYPE_NORMAL
- en: Does something look familiar?
  prefs: []
  type: TYPE_NORMAL
- en: We used a few of these NLP pipeline building blocks in the previous chapter,
    [Chapter 3](4dcd4b65-934b-4a8a-a252-9af7513a4787.xhtml), *Word Representation
    Using word2vec*, to build our word2vec models. This more in-depth explanation
    of the building blocks of the NLP pipeline helps us take the next step in our
    projects, as we look to deploy more and more complex models!
  prefs: []
  type: TYPE_NORMAL
- en: As with everything in this book on *Python Deep Learning Projects*, we encourage
    you to also try your own combinations of the previous processes for the use cases
    you work on in your data science career. Now, let's implement a chatbot using
    these pipelines!
  prefs: []
  type: TYPE_NORMAL
- en: Building conversational bots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about some basic statistical modeling approaches
    to build an information retrieval system using **term frequency**-**inverse document
    frequency** (**TF-IDF**), which we can use with the NLP pipelines to build fully
    functional chatbots. Also, later on, we will learn to build a much more advanced
    conversational bot that can extract a specific piece of information, such as location,
    capture time, and so on, using NER.
  prefs: []
  type: TYPE_NORMAL
- en: What is TF-IDF?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TF-IDFs are a way to represent documents as feature vectors. But what are they?
    TF-IDFs can be understood as a modification of the raw **term frequency** (**TF**)
    and **inverse document frequency** (**IDF**). The TF is the count of how often
    a particular word occurs in a given document. The concept behind the TF-IDF is
    to downweight terms proportionally to the number of documents in which they occur.
    Here, the idea is that terms that occur in many different documents are likely
    to be unimportant, or don't contain any useful information for NLP tasks, such
    as document classification.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we think about building a chatbot with the TF-IDF approach, we first need
    to form a data structure that supports training data with labels. Now, let's take
    an example of a chatbot built to answer questions from users.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, using historical data, we can form a dataset where we have two
    columns, one of which is the question, and the second of which is the answer to
    that question, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Question** | **Answer** |'
  prefs: []
  type: TYPE_TB
- en: '| When does your shop open? | Our shop timings are 9:00 A.M-9:00 P.M on weekdays
    and 11:00 A.M-12:00 midnight on weekends. |'
  prefs: []
  type: TYPE_TB
- en: '| What is today''s special? | Today, we have a variety of Italian pasta, with
    special sauce and a lot more other options in the bakery. |'
  prefs: []
  type: TYPE_TB
- en: '| What is the cost of an Americano? | Americano with a single shot will cost
    $1.4 and the double shot will cost $2.3. |'
  prefs: []
  type: TYPE_TB
- en: '| Do you sell ice-creams? | We do have desserts such as ice-cream, brownies,
    and pastries. |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s take the previous example, and consider it a sample dataset. It is a
    very small example and, in the original hypothetical scenario, we will have a
    much larger dataset to work with. The typical process will be as follows: the
    user will interact with the bot and write a random query about the store. The
    bot will simply send that query to the NLP engine, using an API, and then it is
    up to the NLP model to decide what to return for a new query (test data). In reference
    to our dataset, all of the questions are the training data and the answers are
    labels. In the event of a new query, the TF-IDF algorithm will match it to one
    of the questions with a confidence score, which tells us that the new question
    asked by the user is close to some specific question from the dataset, and the
    answer against that question is the answer that our bots return.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take the preceding example even further. When the user queries, "Can I
    get an Americano, btw how much it will cost?", we can see that words like *I*,
    *an*, and *it* are the ones that will have a higher occurrence frequency in other
    questions as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we match our remaining important words, we will see that this question
    is most close to: "What is the cost of an Americano?"So, our bot will respond
    back with the historical answer to this type of question: "Americano with a single
    shot will cost $1.4 and the double shot will cost $2.3."'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After creating the data structure in tabular format, as mentioned previously,
    we will be calculating the predicted answer to a question every time a user queries
    our bot. We load all of the question-answer pairs from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load our CSV file using `pandas`, and perform some pre-processing on
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/tfidf_version).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the vectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s initialize the TF-IDF vectorizer and define a few parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_df`: When building the vocabulary, ignore terms that have a document frequency
    strictly lower than the given threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ngram_range`: Configures our vectorizer to capture *n*-words at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm`: This is used to normalize term vectors using L1 or L2 norms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: Handles Unicode characters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many more parameters that you can look into, configure, and play
    around with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we train the model on the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Processing the query
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To process the query, we find out its similarity with other questions. We do
    this by taking a dot product of the training data matrix with a transpose of the
    query data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we take out the similarity between the query and train data as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Rank results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We create a sorted dictionary of similarities for a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the sorted dictionary, we check for the index of the most similar
    question, and the response with the value at that index in the answers column.
    If nothing is found, then we can return our default answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Advanced chatbots using NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just created a very basic chatbot that can understand the user's query and
    then respond to the customer accordingly. But it is not yet capable of understanding
    the context, because it can not extract information such as the product name,
    places, or any other entities.
  prefs: []
  type: TYPE_NORMAL
- en: To build a bot that understands the context (intent) and can also extract the
    entities, we need an NLP pipeline that can perform intent classification, along
    with NER extraction, and then provide an accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: Keep your eyes on the goal! This is the goal of our open-domain question answering
    bot.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we will use an open source project called Rasa NLU ([https://github.com/RasaHQ/rasa_nlu](https://github.com/RasaHQ/rasa_nlu)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Rasa NLU is a **Natural Language Understanding** tool for understanding a text;
    in particular, what is being said in short pieces of text. For example, imagine
    that the system is given a short message like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In such a case, the system returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, by harnessing the power of RASA, we can build a chatbot that can do intent
    classification and NER extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Great, let's do it!
  prefs: []
  type: TYPE_NORMAL
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04/rasa_version)
  prefs: []
  type: TYPE_NORMAL
- en: Installing Rasa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s install Rasa in our local environment or server using these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If it fails to install, then you can look into a detailed approach at [https://nlu.rasa.com/installation.html](https://nlu.rasa.com/installation.html).
  prefs: []
  type: TYPE_NORMAL
- en: Rasa uses a variety of NLP pipelines such as `spacy`, `sklearn`, or MITIE. You
    can use any one of them or build your own custom pipelines, which can include
    any deep model, such as CNN with word2vec, which we created in the previous chapter.
    In our case, we will be using `spacy` with `sklearn`.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous project, we created a dataset in a CSV file with two columns
    for question and answer pairs. We need to do this again, but in a different format.
    In this case, we need questions associated with its intent, as shown in the following
    diagram, so we have a query as **hello** with its intent labeled as **greet**.
    Similarly, we will label all of the questions with their respective intents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have all of the forms of questions and intents ready, we need to label
    the entities. In this case, as shown in the following diagram, we have a **location** entity
    with a **centre** value, and a **cuisine** entity with the value as **mexican**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/859b52cc-ab9e-4a95-be06-7ae12bd2b065.png)'
  prefs: []
  type: TYPE_IMG
- en: The figure illustrated the content of the data what we are preparing for the
    chatbot. Lest most is the list of all intents which we need out bot to understand.
    Then we have respective sample utterneces for each intent. And the right most
    part represents the annotation of the specific entity with its label 'location'
    and 'cuisine' in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To feed data into Rasa, we need to store this information in a specific JSON
    format, which looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The final version of the JSON should have this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To make it simple, there is an online tool into which you can feed and annotate
    all of the data, and download the JSON version of it. You can run the editor locally
    by following the instructions from [https://github.com/RasaHQ/rasa-nlu-trainer](https://github.com/RasaHQ/rasa-nlu-trainer) or
    simply use the online version of it from [https://rasahq.github.io/rasa-nlu-trainer/](https://rasahq.github.io/rasa-nlu-trainer/).
  prefs: []
  type: TYPE_NORMAL
- en: Save this JSON file as `restaurant.json` in the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we're going to create a configuration file. This configuration file will
    define the pipeline that is to be used in the process of training and building
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file called `config_spacy.yml` in your working directory, and insert
    the following code into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Know the code**: spaCy configuration customization is there for a reason.
    Other data scientists have found some utility in the ability to change values
    here, and it''s good practice to explore this as you get more familiar with this
    technology. There is a huge list of configurations, which you can look into at [https://nlu.rasa.com/config.html](https://nlu.rasa.com/config.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration states that we will be using English language models, and
    the pipeline running on the backend will be spaCy with scikit-learn. Now, to begin
    the training process, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This takes the configuration file and the training data file as input. The `--path`
    parameter is the location where the trained model will be stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training process is completed, you''ll see a new folder named
    in the `projects/default/model_YYYYMMDD-HHMMSS` format, with the timestamp when
    the training finished. The complete project structure will look as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d457b2a-fa48-40f8-a2b0-c9bbbf791ba1.png)'
  prefs: []
  type: TYPE_IMG
- en: The folder structure after the training process is completed. The model folder
    will contain all the binary files and metadata which was learned during the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s the moment to make your bot go live! While using Rasa, you don''t
    need to write any API services—everything is available in the package itself.
    So, to expose the trained model as a service, you need to execute the following
    command, which takes the path of the stored trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything goes fine, then a RESTful API will be exposed at port `5000`,
    and you should see this log on the console screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the API, you can use the following command. We are querying the model,
    making a statement such as "`I am looking for Mexican food`":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So here, we can see that model has performed quite accurately with the intent
    classification and the entity extraction process. It is able to classify the intent
    as `restaurant_search` with 75.8% of accuracy, and is also able to detect the `cuisine`
    entity with the value as `mexican`.
  prefs: []
  type: TYPE_NORMAL
- en: Serving chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to now, we have seen how to build chatbots using the two methods of TF-IDF
    and Rasa NLU. Let''s expose both of them as APIs. The architecture of this simple
    chatbot framework will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c640950b-e21d-4dfc-9bd8-c4c61eb06454.png)'
  prefs: []
  type: TYPE_IMG
- en: This chatbot pipeline illustrates that we can have any User Interface (Slack,
    Skype, and so on) integrated with the chatbot_api which we exposed . And under
    the hood we can setup any number of algorithms 'TFIDF' and 'RASA'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the Packt repository for this chapter (available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter04))
    for the API code and look into the `chatbot_api.py`file. Here, we have implemented
    a common API that can load both versions of bot, and you can now build a whole
    framework on top of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute the serving of the APIs, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the chapter directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will expose the Rasa module at `localhost:5000`. If you have not trained
    the Rasa engine, then please apply the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In a separate console, execute the following command. This will expose an API
    at `localhost:8080`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now your chatbot is ready to be accessed via API. Try the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Call the following API to execute the TFIDF version:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the following API to execute the Rasa version:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we were asked to create a natural language pipeline that would
    power a chatbot for open domain question answering. A (hypothetical) restaurant
    chain has much text-based data on their website, including their menu, history,
    location, hours, and other information, and they would like to add the ability
    for a website visitor to ask a question in a query box. Our deep learning NLP
    chatbot would then find the relevant information and present that back to the
    visitor.
  prefs: []
  type: TYPE_NORMAL
- en: We got started by showing how we could build a simple FAQ chatbot that took
    in random queries, matched that up to predefined questions, and returned a response
    with a confidence score that indicated the similarity between the input question
    and the question in our database. But this was only a stepping stone to our real
    goal, which was to create a chatbot that could capture the intent of the question
    and prepare an appropriate response.
  prefs: []
  type: TYPE_NORMAL
- en: We explored an NER approach to give us the added power that we needed to quickly
    classify input text, which we could then match to the relevant content for a response.
    This was determined to fit our goal of allowing for open-domain question answering
    and to take advantage of a large corpus of unstructured data that changes without
    using hardcoded heuristics (as in our hypothetical restaurant example).
  prefs: []
  type: TYPE_NORMAL
- en: We learned to use the building blocks of the NLP model, including pre-processing, tokenizing,
    and tagging POS. We used this understanding to build a system able to read an
    unstructured text in order to comprehend an answer to a specific question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we gained these skills in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a basic FAQ-based chatbot using statistical modeling in a framework
    capable of detecting intents and entities for answering open-domain questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a dense representation of sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a document reader for extracting answers from unstructured text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learned how to integrate deep learning models into a classic NLP pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These skills will come very much in handy in your career, as you see similar
    business use cases, and also as conversational user interfaces continue to gain
    in popularity. Well done—let's see what's in store for our next deep learning
    project in Python!
  prefs: []
  type: TYPE_NORMAL
