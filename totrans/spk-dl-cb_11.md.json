["```py\nfrom __future__ import absolute_import, division, print_function\nimport codecs\nimport glob\nimport logging\nimport multiprocessing\nimport os\nimport pprint\nimport re\nimport nltk\nimport gensim.models.word2vec as w2v\nimport sklearn.manifold\nimport numpy\nas np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%pylab inline\n```", "```py\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n```", "```py\nbook_names = sorted(glob.glob(\"./*.txt\"))\nprint(\"Found books:\")\nbook_names\n```", "```py\ncorpus = u''\nfor book_name in book_names:\nprint(\"Reading '{0}'...\".format(book_name))\nwith codecs.open(book_name,\"r\",\"Latin1\") as book_file:\ncorpus += book_file.read()\nprint(\"Corpus is now {0} characters long\".format(len(corpus)))\nprint()\n```", "```py\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n```", "```py\nraw_sentences = tokenizer.tokenize(corpus)\n```", "```py\ndef sentence_to_wordlist(raw):\n     clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n     words = clean.split()\n     return words\n```", "```py\nsentences = []\nfor raw_sentence in raw_sentences:\n  if len(raw_sentence) > 0:\n  sentences.append(sentence_to_wordlist(raw_sentence))\n```", "```py\nprint(raw_sentences[50])\nprint(sentence_to_wordlist(raw_sentences[50]))\n```", "```py\ntoken_count = sum([len(sentence) for sentence in sentences])\nprint(\"The book corpus contains {0:,} tokens\".format(token_count))\n```", "```py\nnum_features = 300\nmin_word_count = 3\nnum_workers = multiprocessing.cpu_count()\ncontext_size = 7\ndownsampling = 1e-3\nseed = 1\n```", "```py\ngot2vec = w2v.Word2Vec(\n    sg=1,\n    seed=seed,\n    workers=num_workers,\n    size=num_features,\n    min_count=min_word_count,\n    window=context_size,\n    sample=downsampling\n)\n```", "```py\ngot2vec.build_vocab(sentences,progress_per=10000, keep_raw_vocab=False, trim_rule=None)\n```", "```py\ngot2vec.train(sentences, total_examples=got2vec.corpus_count, total_words=None, epochs=got2vec.iter, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False)\n```", "```py\nif not os.path.exists(\"trained\"):\n     os.makedirs(\"trained\")\ngot2vec.wv.save(os.path.join(\"trained\", \"got2vec.w2v\"), ignore=[])\n```", "```py\ngot2vec = w2v.KeyedVectors.load(os.path.join(\"trained\", \"got2vec.w2v\"))\n```", "```py\n tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n```", "```py\n all_word_vectors_matrix = got2vec.wv.syn0\n print (all_word_vectors_matrix)\n```", "```py\n all_word_vectors_matrix_2d =  tsne.fit_transform(all_word_vectors_matrix)\n```", "```py\n points = pd.DataFrame(\n     [\n            (word, coords[0], coords[1])\n             for word, coords in [\n              (word, all_word_vectors_matrix_2d[got2vec.vocab[word].index])\n                for word in got2vec.vocab\n         ]\n    ],\n    columns=[\"word\", \"x\", \"y\"]\n)\n```", "```py\npoints.head(10)\n```", "```py\nsns.set_context(\"poster\")\npoints.plot.scatter(\"x\", \"y\", s=10, figsize=(15, 15))\n```", "```py\ndef plot_region(x_bounds, y_bounds):\n    slice = points[\n        (x_bounds[0] <= points.x) &\n        (points.x <= x_bounds[1]) &\n        (y_bounds[0] <= points.y) &\n        (points.y <= y_bounds[1])\n        ]\n    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n        for i, point in slice.iterrows():\n            ax.text(point.x + 0.005, point.y + 0.005, point.word,                                                  fontsize=11)\n```", "```py\nplot_region(x_bounds=(20.0, 25.0), y_bounds=(15.5, 20.0))\n```", "```py\n def nearest_similarity_cosmul(start1, end1, end2):\n    similarities = got2vec.most_similar_cosmul(\n        positive=[end2, start1],\n        negative=[end1]\n)\nstart2 = similarities[0][0]\nprint(\"{start1} is related to {end1}, as {start2} is related to         {end2}\".format(**locals()))\nreturn start2\n```", "```py\nnearest_similarity_cosmul(\"Stark\", \"Winterfell\", \"Riverrun\")\nnearest_similarity_cosmul(\"Jaime\", \"sword\", \"wine\")\nnearest_similarity_cosmul(\"Arya\", \"Nymeria\", \"dragons\")\n```"]