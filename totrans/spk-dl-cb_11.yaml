- en: Creating and Visualizing Word Vectors Using Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing the necessary libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing further
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing further
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before training a neural network on text data and generating text using LSTM
    cells, it is important to understand how text data (such as words, sentences,
    customer reviews, or stories) is converted to word vectors first before it is
    fed into a neural network. This chapter will describe how to convert a text into
    a corpus and generate word vectors from the corpus, which makes it easy to group
    similar words using techniques such as Euclidean distance calculation or cosine
    distance calculation between different word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to acquire some data to work with. For this chapter, we will
    require a lot of text data to convert it into tokens and visualize it to understand
    how neural networks rank word vectors based on Euclidean and Cosine distances.
    It is an important step in understanding how different words get associated with
    each other. This, in turn, can be used to design better, more efficient language
    and text-processing models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The text data for the model needs to be in files of `.txt` format, and you must
    ensure that the files are placed in the current working directory. The text data
    can be anything from Twitter feeds, news feeds, customer reviews, computer code,
    or whole books saved in the `.txt` format in the working directory. In our case,
    we have used the *Game of Thrones* books as the input text to our model. However,
    any text can be substituted in place of the books, and the same model will work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many classical texts are no longer protected under copyright. This means that
    you can download all of the text for these books for free and use them in experiments,
    such as creating generative models. The best place to get access to free books
    that are no longer protected by copyright is Project Gutenberg ([https://www.gutenberg.org/](https://www.gutenberg.org/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by visiting the Project Gutenberg website and browsing for a book that
    interests you. Click on the book, and then click on UTF-8, which allows you to
    download the book in plain-text format. The link is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/de2e19ef-378b-4782-b3b1-0393bdb35026.png)'
  prefs: []
  type: TYPE_IMG
- en: Project Gutenberg Dataset download page
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on Plain Text UTF-8, you should see a page that looks like the
    following screenshot. Right click on the page and click on Save As... Next, rename
    the file to whatever you choose and save it in your working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/44d32c31-04e8-4ee0-b4e6-2106b0afafc4.png)'
  prefs: []
  type: TYPE_IMG
- en: You should now see a `.txt` file with the specified filename in your current
    working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project Gutenberg adds a standard header and footer to each book; this is not
    part of the original text. Open the file in a text editor, and delete the header
    and the footer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Check for the current working directory using the following command: `pwd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The working directory can be changed using the `cd` command as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4399df8e-ceb1-4d15-aa8f-abd4ab5b8e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that, in our case, the text files are contained in a folder named `USF`,
    and, therefore, this is set as the working directory. You may similarly store
    one or more `.txt` files in the working directory for use as input to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: UTF-8 specifies the type of encoding of the characters in the text file. **UTF-8**
    stands for **Unicode Transformation Format**. The **8** means it uses **8-bit**
    blocks to represent a character.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: UTF-8 is a compromise character encoding that can be as compact as ASCII (if
    the file is just plain-English text) but can also contain any Unicode characters
    (with some increase in file size).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is not necessary for the text file to be in a UTF-8 format, as we will use
    the codecs library at a later stage to encode all the text into the Latin1 encoding
    format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about UTF-8 and Latin1 encoding formats, visit the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/UTF-8](https://en.wikipedia.org/wiki/UTF-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html](http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visit the following link to understand the need for word vectors in neural
    networks better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listed below are some other useful articles related to the topic of converting
    words to vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817](https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817)'
  prefs: []
  type: TYPE_NORMAL
- en: Importing the necessary libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin, we require the following libraries and dependencies, which
    need to be imported into our Python environment. These libraries will make our
    tasks a lot easier, as they have readily available functions and models that can
    be used instead of doing that ourselves. This also makes the code more compact
    and readable.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following libraries and dependencies will be required to create word vectors
    and plots and visualize the n-dimensional word vectors in a 2D space:'
  prefs: []
  type: TYPE_NORMAL
- en: '`future`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codecs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glob`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `multiprocessing` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`os`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `pprint` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nltk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word2Vec`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the following commands into your Jupyter notebook to import all the required
    libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output that looks like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cf709ffc-0cbc-419d-8b2c-b3b55f7c4daa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, import the `stopwords` and `punkt` libraries using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output you see must look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0e52652d-b201-4877-8399-99a018016314.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will describe the purpose of each library being used for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The `future` library is the missing link between Python 2 and Python 3\. It
    acts as a bridge between the two versions and allows us to use syntax from both
    versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `codecs` library will be used to perform the encoding of all words present
    in the text file. This constitutes our dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regex is the library used to look up or search for a file really quickly. The
    `glob` function allows quick and efficient searching through a large database
    for a required file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `multiprocessing` library allows us to perform concurrency, which is a way
    of running multiple threads and having each thread run a different process. It
    is a way of making programs run faster by parallelization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `os` library allows easy interaction with the operating system, such as
    a Mac, Windows, and so on, and performs functions such as reading a file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `pprint` library provides a capability for pretty-printing arbitrary Python
    data structures in a form that can be used as input to the interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `re` module provides regular expression matching operations similar to those
    found in Perl.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NLTK is a natural language toolkit capable of tokenizing words in very short
    code. When fed in a whole sentence, the `nltk` function breaks up sentences and
    outputs tokens for each word. Based on these tokens, the words may be organized
    into different categories. NLTK does this by comparing each word with a huge database
    of pre-trained words called a **lexicon**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Word2Vec` is Google''s model, trained on a huge dataset of word vectors. It
    groups semantically similar words close to one another. This will be the most
    important library for this section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sklearn.manifold` allows the dimensionality reduction of the dataset by employing
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**) techniques. Since
    each word vector is multi-dimensional, we require some form of dimensionality
    reduction techniques to bring the dimensionality of these words down to a lower
    dimensional space so it can be visualized in a 2D space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Numpy` is a commonly used `math` library. `Matplotlib` is the `plotting` library
    we will utilize, and `pandas` provide a lot of flexibility in data handling by
    allowing easy reshaping, slicing, indexing, subsetting, and manipulation of data.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Seaborn` library is another statistical data visualization library that
    we require along with `matplotlib`. `Punkt` and `Stopwords` are two data-processing
    libraries that simplify tasks such as splitting a piece of text from a corpus
    into tokens (that is, via tokenization) and removing `stopwords`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information regarding some of the libraries utilized, visit the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.python.org/3/library/codecs.html](https://docs.python.org/3/library/codecs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/2/library/pprint.html](https://docs.python.org/2/library/pprint.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nltk.org/](https://www.nltk.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A number of data-preprocessing steps are to be performed before the data is
    fed into the model. This section will describe how to clean the data and prepare
    it so it can be fed into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the text from the `.txt` files is first converted into one big corpus. This
    is done by reading each sentence from each file and adding it to an empty corpus.
    A number of preprocessing steps are then executed to remove irregularities such
    as white spaces, spelling errors, `stopwords`, and so on. The cleaned text data
    has to then be tokenized, and the tokenized sentences are added to an empty array
    by running them through a loop.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type in the following commands to search for the `.txt` files within the working
    directory and print the names of the files found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In our case, there are five books named `got1`, `got2`, `got3`, `got4`, and
    `got5` saved in the working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `corpus`, read each sentence starting with the first file, encode
    it, and add the encoded characters to a `corpus` using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the code in the preceding steps, which should result in an output that
    looks like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9dc0f8ab-814f-46ce-9efa-8946d4378b2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Load the English pickle `tokenizer` from `punkt` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Tokenize` the entire `corpus` into sentences using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the function to split sentences into their constituent words as well
    as remove unnecessary characters in the following manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all the raw sentences where each word of the sentence is tokenized to a
    new array of sentences. This is done by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Print a random sentence from the corpus to visually see how the `tokenizer`
    splits sentences and creates a word list from the result. This is done using the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the total tokens from the dataset using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Executing the tokenizer and tokenizing all the sentences in the corpus should
    result in an output that looks like the one in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1257f121-c354-454a-99af-54282e394815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, removing unnecessary characters, such as hyphens and special characters,
    are done in the following manner. Splitting up all the sentences using the user-defined
    `sentence_to_wordlist()` function produces an output as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18472244-abe7-4029-a5aa-3ab539a44964.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Adding the raw sentences to a new array named `sentences[]` produces an output
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7d44661-9a51-4107-8661-8c09e6610aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On printing the total number of tokens in the corpus, we notice that there
    are 1,110,288 tokens in the entire corpus. This is illustrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a535b630-52be-4d91-8b31-bf9626571f4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The pre-trained `tokenizer` from NLTK is used to tokenize the entire corpus
    by counting each sentence as a token. Every tokenized sentence is added to the
    variable `raw_sentences`, which stores the tokenized sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, common stopwords are removed, and the text is cleaned by splitting
    each sentence into its words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A random sentence along with its wordlist is printed to understand how this
    works. In our case, we have chosen to print the 50th sentence in the `raw_sentences` array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The total number of tokens (in our case, sentences) in the sentences array are
    counted and printed. In our case, we see that 1,110,288 tokens are created by
    the `tokenizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More information about tokenizing paragraphs and sentences can be found by
    visiting the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk](https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/](https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about how regular expressions work, visit the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python](https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python)'
  prefs: []
  type: TYPE_NORMAL
- en: Building and training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have the text data in the form of tokens in an array, we are able to
    input it in the array format to the model. First, we have to define a number of
    hyperparameters for the model. This section will describe how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare model hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a model using `Word2Vec`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model on the prepared dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save and checkpoint the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the model hyperparameters that are to be declared include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality of resulting word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum word count threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of parallel threads to run while training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context window length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling (for frequently occurring words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting a seed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the previously mentioned hyperparameters are declared, the model can be
    built using the `Word2Vec` function from the `Gensim` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare the hyperparameters for the model using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model, using the declared hyperparameters, with the following lines
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model''s vocabulary using the tokenized sentences and iterating through
    all the tokens. This is done using the `build_vocab` function in the following
    manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a directory named trained, if it doesn''t already exist. Save and checkpoint
    the `trained` model using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the saved model at any point, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The declaration of model parameters does not produce any output. It just makes
    space in the memory to store variables as model parameters. The following screenshot
    describes this process:![](img/b01b9b42-aaa9-41a6-91dd-550faff73ec6.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is built using the preceding hyperparameters. In our case, we have
    named the model `got2vec` ,but the model may be named as per your liking. The
    model definition is illustrated in the following screenshot:![](img/abe1bb95-6a89-45e2-8060-229f2759473c.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the `build_vocab` command on the model should produce an output as seen
    in the following screenshot:![](img/54a633e8-d76c-42a8-b40c-aea8e440a79d.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model is done by defining the parameters as seen in the following
    screenshot:![](img/a6116087-5efc-474e-9e00-69289485ade7.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The above command produces an output as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d7dd945-1ed2-497f-9cd1-2e7daa05b4c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The commands to save, checkpoint, and load the model produce the following
    output, as shown in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/12a49260-afaf-484f-925c-cd73e1682646.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we notice the `build_vocab` function identifies 23,960 different
    word types from a list of 1,110,288 words. However, this number will vary for
    different text corpora.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each word is represented by a 300-dimensional vector since we have declared
    the dimensionality to be 300\. Increasing this number increases the training time
    of the model but also makes sure the model generalizes easily to new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The downsampling rate of 1e![](img/4b993d4b-623a-44ac-8980-51f3d1ea5c11.png)3
    is found to be a good rate. This is specified to let the model know when to downsample
    frequently occurring words, as they are not of much importance when it comes to
    analysis. Examples of such words are this, that, those, them, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed is set to make results reproducible. Setting a seed also makes debugging
    a lot easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model takes about 30 seconds using regular CPU computing since
    the model is not very complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model, when check-pointed, is saved under the `trained` folder inside the
    working directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on `Word2Vec` models and the Gensim library, visit the
    following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing further
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will describe how to squash the dimensionality of all the trained
    words and put it all into one giant matrix for visualization purposes. Since each
    word is a 300-dimensional vector, it needs to be brought down to a lower dimension
    for us to visualize it in a 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is saved and checkpointed after training, begin by loading it
    into memory, as you did in the previous section. The libraries and modules that
    will be utilized in this section are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tSNE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Seaborn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Squash the dimensionality of the 300-dimensional word vectors by using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Put all the word vectors into one giant matrix (named `all_word_vectors_matrix`),
    and view it using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `tsne` technique to fit all the learned representations into a two-
    dimensional space using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Gather all the word vectors, as well as their associated words, using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `X` and `Y` coordinates and associated words of the first ten points can
    be obtained using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot all the points using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A selected region of the plotted graph can be zoomed into for a closer inspection.
    Do this by slicing the original data using the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the sliced data using the following command. The sliced data can be visualized
    as a zoomed-in region of the original plot of all data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The t-SNE algorithm is a non-linear dimensionality reduction technique. Computers
    are easily able to interpret and process many dimensions during their computations.
    However, humans are only capable of visualizing two or three dimensions at a time.
    Therefore, these dimensionality reduction techniques come in very handy when trying
    to draw insights from data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On applying t-SNE to the 300-dimensional vectors, we are able to squash it into
    just two dimensions to plot it and view it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By specifying `n_components` as 2, we let the algorithm know that it has to
    squash the data into a two-dimensional space. Once this is done, we add all the
    squashed vectors into one giant matrix named `all_word_vectors_matrix`, which
    is illustrated in the following screenshot:![](img/325c8759-34c8-4a40-ad78-7e050945cfe2.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The t-SNE algorithm needs to be trained on all these word vectors. The training
    takes about five minutes on a regular CPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the t-SNE is finished training on all the word vectors, it outputs 2D vectors
    for each word. These vectors may be plotted as points by converting all of them
    into a data frame. This is done as shown in the following screenshot:![](img/e3554859-c59a-4361-929c-5aac54dcf654.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We see that the preceding code produces a number of points where each point
    represents a word along with its X and Y coordinates. On inspection of the first
    twenty points of the data frame, we see an output as illustrated in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a3db5c42-0c48-4990-a657-2a8117e6df71.png)'
  prefs: []
  type: TYPE_IMG
- en: On plotting all the points using the `all_word_vectors_2D` variable, you should
    see an output that looks similar to the one in the following screenshot:![](img/82c9e244-106f-4c01-825f-dbb00d5bd297.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The above command will produce a plot of all tokens or words generated from
    the entire text as shown in the following screenshot:![](img/7f14da02-37c8-42f0-a62c-019961507c8f.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the `plot_region` function to zoom into a certain area of the plot
    so that we are able to actually see the words, along with their coordinates. This
    step is illustrated in the following screenshot:![](img/3c916ae2-d6a1-4d0b-b069-5a2a880562af.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An enlarged or zoomed in area of the plot can be visualized by setting the `x_bounds` and
    `y_bounds`, values as shown in the following screenshot:![](img/a0214205-14a3-4165-ae29-22535d292dbb.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A different region of the same plot can be visualized by varying the `x_bounds`
    and `y_bounds` values as shown in the following two screenshots:![](img/ab6375bb-8bea-4697-8109-3e05c12cee31.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1041a93a-3c8e-42f2-a001-c0dea26cbc11.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following additional points are of note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on how the t-SNE algorithm works, visit the following
    link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More information about cosine distance similarity and ranking can be found
    by visiting the following link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the following link to explore the different functions of the `Seaborn`
    library:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Analyzing further
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will describe further analysis that can be performed on the data
    after visualization. For example, exploring cosine distance similarity between
    different word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following link is a great blog on how cosine distance similarity works
    and also discusses some of the math involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various natural-language processing tasks can be performed using the different
    functions of `Word2Vec`. One of them is finding the most semantically similar
    words given a certain word (that is, word vectors that have a high cosine similarity
    or a short Euclidean distance between them). This can be done by using the `most_similar` function
    form `Word2Vec`, as shown in the following screenshot:![](img/9ac6fa2d-4380-4100-80be-ec37006e6efd.png)This
    screenshots  all the closest words related to the word `Lannister`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/30dd7d1c-bbed-4cef-9a87-377ff69d022a.png)This screenshot shows a list
    of all the words related to word `Jon`:![](img/ec9b90eb-4a33-49a9-ac48-75b56b4f65e6.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various methods to measure the semantic similarity between words.
    The one we are using in this section is based on cosine similarity. We can also
    explore linear relationships between words by using the following lines of code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To find the cosine similarity of nearest words to a given set of words, use
    the following commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding process is illustrated in the following screenshot:![](img/09338881-485f-4402-945c-c4ff5abc61b5.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results are as follows:![](img/0af641a8-cf30-4c33-8994-300ef137b545.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As seen in this section, word vectors form the basis of all NLP tasks. It is
    important to understand them and the math that goes into building these models
    before diving into more complicated NLP models such as **recurrent neural networks** and **Long
    Short-Term Memory** (**LSTM**) cells.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Further reading can be undertaken for a better understanding of the use of
    cosine distance similarity, clustering and other machine learning techniques used
    in ranking word vectors. Provided below are a few links to useful published papers
    on this topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf](https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1530163881&Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf](http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
