<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Pain Points of Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p><span>In this chapter,</span><span> </span><span>the following recipes will be covered</span><span>:</span></p>
<ul>
<li>Pain Point #1: Importing MNIST images</li>
<li>Pain Point #2: Visualizing MNIST images</li>
<li>Pain Point #3: Exporting MNIST images as files</li>
<li>Pain Point #4: Augmenting MNIST images</li>
<li>Pain Point #5: Utilizing alternate sources for trained images</li>
<li>Pain Point #6: Prioritizing high-level libraries for CNNs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) have been enjoying a bit of resurgence in the last couple of years. They have shown great success when it comes to image recognition. This is quite relevant these days with the advent of modern smartphones as anyone now has the ability to take large volumes of pictures of objects and post them on social media sites. Just due to this phenomenon, convolutional neural networks are in high demand these days.</p>
<p class="mce-root"><span>There are several features that make a CNN optimally perform. They require the following features:</span></p>
<ul>
<li>A high volume of training data</li>
<li>Visual and spatial data</li>
<li>An emphasis on filtering (pooling), activation, and convoluting as opposed to a fully connected layer that is more apparent in a traditional neural network</li>
</ul>
<p><span>While CNNs have gained great popularity, there are some limitations in working with them primarily due to their computational needs as well as the volume of training data required to get a well-performing model. We will focus on techniques that can be applied to the data that will ultimately assist with the development of a convolutional neural network while addressing these limitations. In later chapters, we will apply some of these techniques when we develop models for image classification. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #1: Importing MNIST images</h1>
                </header>
            
            <article>
                
<p>One of the most common datasets used for image classification is the<span> </span><kbd>MNIST</kbd><span> </span>dataset, which is composed of thousands of samples of handwritten digits. The <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) is, according to Yann LeCun, Corinna Cortes, and Christopher J.C. Burges, useful for the following reasons:</p>
<div class="packt_infobox">It is a good database for people who<span> </span>want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.</div>
<p>There are several methods to import the MNIST images into our Jupyter notebook. We will cover the following two methods in this chapter:</p>
<ol>
<li>Directly through the TensorFlow library</li>
<li>Manually through the MNIST website</li>
</ol>
<div class="packt_infobox">One thing to note is that we will be primarily using MNIST images as our example of how to improve performance within a convolutional neural network. All of these techniques that will be applied on MNIST images can be applied to any image that will be used to train a CNN.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The only requirement needed is to install<span> </span><kbd>TensorFlow</kbd>. It will likely not come pre-installed with the <span class="packt_screen">anaconda3</span> packages; therefore, a simple<span> </span><kbd>pip</kbd><span> </span>install will either confirm the availability of<span> </span><kbd>TensorFlow</kbd><span> </span>or install it if not currently available.<span> </span><kbd>TensorFlow</kbd><span> </span>can be easily installed in the Terminal, as seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1296 image-border" src="assets/32eb9853-862f-45e7-a53a-c61008231b04.png" style="width:122.00em;height:72.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>The</span><span> </span><kbd>TensorFlow</kbd><span> </span><span>library has a conveniently built-in set of examples that can be used directly. One of those example datasets is</span><span> </span><kbd>MNIST</kbd><span>. This section will walk through the steps of accessing those images.</span></p>
<ol>
<li>Import <kbd>TensorFlow</kbd><span> </span>into the library with an alias of<span> </span><kbd>tf</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf</pre>
<ol start="2">
<li>Download and extract images from the library and save to a local folder using the following script:</li>
</ol>
<pre style="padding-left: 60px">from tensorflow.examples.tutorials.mnist import input_data<br/>data = input_data.read_data_sets('MNIST/', one_hot=True)</pre>
<ol start="3">
<li>Retrieve a final count of the training and testing datasets that will be used to evaluate the accuracy of the image classification using the following script:</li>
</ol>
<pre style="padding-left: 60px">print('Image Inventory')<br/>print('----------')<br/>print('Training: ' + str(len(data.train.labels)))<br/>print('Testing: '+ str(len(data.test.labels)))<br/>print('----------')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains the process used to access the MNIST datasets:</p>
<ol start="1">
<li>Once we receive a confirmation that the <kbd>TensorFlow</kbd> library has been properly installed, it is imported into the notebook.</li>
<li>We can confirm the version of <kbd>TensorFlow</kbd> as well as extract the images to our local folder of <kbd>MNIST/</kbd>. <span>The extraction process is visible in the output of the notebook, as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1538 image-border" src="assets/fe736aad-311b-4ecf-ba55-204923aff582.png" style="width:41.08em;height:20.08em;"/></div>
<ol start="3">
<li>The four extracted files are named the following<span>:</span>
<ol>
<li><kbd>t10k-images-idx3-ubyte.gz</kbd></li>
<li><kbd>t10k-labels-idx1-ubyte.gz</kbd></li>
<li><kbd>train-images-idx3-ubyte.gz</kbd></li>
<li><kbd>train-labels-idx1-ubyte.gz</kbd></li>
</ol>
</li>
<li>They have<span> been downloaded to the</span><span> </span><kbd>MNIST/</kbd><span> sub</span><span>folder as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1618 image-border" src="assets/eae6887f-d725-4376-ae25-a3876d69a939.png" style="width:56.42em;height:12.67em;"/></div>
<ol start="5">
<li>In addition, the four files can be viewed in our notebook, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d23979c-afee-413f-8834-7d6c3f5f3f45.png" style="width:22.58em;height:7.25em;"/></div>
<ol start="6">
<li><span>The four files are the testing and training images along with the accompanying testing and training labels identifying each image in the testing and training datasets. Additionally, the</span><span> </span><kbd>one_hot<span> </span>= True</kbd><span> feature is explicitly defined. This </span><span>indicates that one-hot encoding is active with the labels, which assists with feature selection within modeling as each column value will be either 0 or 1.</span></li>
<li>A subclass of the library is also imported that stores the handwritten images of MNIST to the specified local folder. The folder containing all of the images should be approximately 12 MB in size for <span class="packt_screen">55,000</span> <span>training images and</span> <span class="packt_screen">10,000</span> <span>testing images, as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/102e5c7e-5458-4ada-9747-9346ffea364f.png" style="width:28.17em;height:9.50em;"/></div>
<ol start="8">
<li><span>The 10,000 images will be used to test the accuracy of our model that will be trained on the 55,000 images.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Occasionally, there may be errors or warnings when trying to access the MNIST datasets directly through <kbd>TensorFlow</kbd>. As was seen earlier on in the section, we received the following warning when importing MNIST:</p>
<div class="packt_figref packt_infobox">WARNING:tensorflow:From &lt;ipython-input-3-ceaef6f48460&gt;:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.<br/>
Instructions for updating:<br/>
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.</div>
<p>The dataset may become deprecated in a future release of <kbd>TensorFlow</kbd> and therefore, no longer be directly accessible. Sometimes we may just encounter a typical <em>HTTP 403 error</em> when extracting the MNIST images through <kbd>TensorFlow</kbd>. This may be due to the website being temporarily unavailable. Have no fear in either case, there is a manual approach to downloading the four<span> </span><kbd>.gz</kbd><span> </span>files using the following link:</p>
<p><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></p>
<p>The files are located on the website, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1621 image-border" src="assets/a65f6b81-fc22-476b-9ebb-bf7ab594063c.png" style="width:162.50em;height:53.92em;"/></div>
<p>Download the files and save them to an accessible local folder similar to what was done with the files that came directly from <kbd>TensorFlow</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about the<span> </span><kbd>MNIST</kbd><span> </span>database of handwritten digits, visit the following website: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p>
<p>To learn more about one-hot encoding, visit the following website: <a href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f">https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #2: Visualizing MNIST images</h1>
                </header>
            
            <article>
                
<p><span>Plotting images is often a major pain point when dealing with graphics within a Jupyter notebook.</span><span> </span>Displaying the handwritten images from the training dataset is critical, especially when comparing the actual value of the label that is associated with the handwritten image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The only Python libraries that will be imported to visualize the handwritten images are<span> </span><kbd>numpy</kbd><span> </span>and<span> </span><kbd>matplotlib</kbd>. Both should already be available through the packages in Anaconda. If for some reason they are not available, they can both be<span> </span><kbd>pip</kbd><span> </span>installed at the Terminal using the following commands:</p>
<ul>
<li><kbd>pip install matplotlib</kbd></li>
<li><kbd>pip install numpy</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>This section will walk through the steps to visualize the MNIST handwritten images in a Jupyter notebook</span>:</p>
<ol>
<li>Import the following libraries, <kbd>numpy</kbd> and <kbd>matplotlib</kbd>, and configure <kbd>matplotlib</kbd> to plot <kbd>inline</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</pre>
<ol start="2">
<li>Plot the first two sample images using the following script:</li>
</ol>
<pre style="padding-left: 60px">for i in range(2):<br/>    image = data.train.images[i]<br/>    image = np.array(image, dtype='float')<br/>    label = data.train.labels[i]<br/>    pixels = image.reshape((28, 28))<br/>    plt.imshow(pixels, cmap='gray')<br/>    print('-----------------')<br/>    print(label)<br/>    plt.show()</pre>
<ol start="3"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>This section will walk through the process of how the MNIST handwritten images are viewed in a Jupyter notebook</span><span>:</span></p>
<ol>
<li>A loop is generated in Python that will sample two images from the training dataset.</li>
</ol>
<ol start="2">
<li>Initially, the images are just a series of values in float format between 0 and 1 that are stored in a<span> </span><kbd>numpy</kbd><span> </span>array. The value of the array is a labeled image called <kbd>image</kbd>. The <kbd>image</kbd> array is then reshaped into a 28 x 28 matrix called<span> </span><kbd>pixels</kbd><span> </span>that has a black color for any value at 0 and a gray shade color for any color that is not 0. The higher the value, the lighter the gray shade of color. An example can be seen in the following screenshot for the digit <span class="packt_screen">8</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe88bf0f-e733-4d79-b8d4-6e4619614ec6.png" style="width:46.00em;height:31.58em;"/></div>
<ol start="3">
<li>The output of the loop produces two handwritten images for the numbers <span class="packt_screen">7</span> and <span class="packt_screen">3</span> along with their labels, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1301 image-border" src="assets/fe1e9692-b9e0-4732-b6b2-ebe49448854a.png" style="width:23.25em;height:38.83em;"/></div>
<ol start="4">
<li>In addition to the images being plotted, the label from the training dataset is also printed above the image. The label is an array of length 10, with values of 0 or 1 only for all 10 digits. For digit 7, the 8th element in the array is of value 1 and for digit 3, the 4th element in the array is of value 1. All other values are 0. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>It may not be immediately obvious what the numeric value of the image is. While most will be able to identify that the first image is a 7 and the second image is a 3, it would be helpful to have confirmation from the label array.</p>
<p>There are 10 elements in the array, each referencing a value for labels 0 through 9 in numeric order. Since the first array has a positive or 1 value in the 8th slot, that is an indication that the value of the image is a 7, as 7 in the 8th index in the array. All other values should be 0. Additionally, the second image has a value of 1 in the 4th spot, indicating a positive value for 3.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Leun, Cortes, and Burges discuss why the image pixelations were set at 28 x 28 in the following statement:</p>
<div class="packt_quote"><span>he original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. The images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.</span></div>
<div class="packt_quote CDPAlignRight CDPAlign">--Leun, Cortes, and Burges from<span> </span><a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/.</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #3: Exporting MNIST images as files</h1>
                </header>
            
            <article>
                
<p>We often need to work within the image directly and not as an array vector. This section will guide us through converting our arrays to <kbd>.png</kbd> images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Exporting the vectors to images requires importing the following library:</p>
<ul>
<li><kbd>import image from matplotlib</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to convert a sample of MNIST arrays to files in a local folder.</p>
<ol>
<li>Create a subfolder to save our images to our main folder of<span> </span><kbd>MNIST/</kbd><span> </span>using the following script:</li>
</ol>
<pre style="padding-left: 60px">if not os.path.exists('MNIST/images'):<br/>   os.makedirs('MNIST/images/')<br/>os.chdir('MNIST/images/')</pre>
<ol start="2">
<li>Loop through the first 10 samples of MNIST arrays and convert them to <kbd>.png</kbd> files using the following script:</li>
</ol>
<pre style="padding-left: 60px">from matplotlib import image<br/>for i in range(1,10):<br/>     png = data.train.images[i]<br/>     png = np.array(png, dtype='float')<br/>     pixels = png.reshape((28, 28))<br/>     image.imsave('image_no_{}.png'.format(i), pixels, cmap = 'gray')</pre>
<ol start="3">
<li>Execute the following script to see the list of images from<span> </span><kbd>image_no_1.png</kbd><span> </span>to<span> </span><kbd>image_no_9.png</kbd>:</li>
</ol>
<pre style="padding-left: 60px">print(os.listdir())</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how the MNIST arrays are converted to images and saved to a local folder.</p>
<ol>
<li>We create a subfolder called <kbd>MNIST/images</kbd> to help us store our temporary <kbd>.png</kbd> images and separate them from the MNIST arrays and labels.</li>
</ol>
<ol start="2">
<li>Once again we loop through <kbd>data.train</kbd> images and obtain nine arrays that can be used for sampling. The images are then saved as <kbd>.png</kbd> files to our local directory with the following format: <kbd>'image_no_{}.png'.format(i), pixels, cmap = 'gray'</kbd></li>
<li>The output of the nine images can be seen in our local directory, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/755def3b-f925-46cd-9f1a-eaed32ab2651.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In addition to seeing the list of images in our directory, we can also view the image in our directory within Linux, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/36415522-ca99-46de-b043-a9d84796217d.png" style="width:44.67em;height:16.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about <kbd>image.imsave</kbd> from <kbd>matplotlib</kbd> visit the following website: </p>
<p><a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imsave.html">https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imsave.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #4: Augmenting MNIST images</h1>
                </header>
            
            <article>
                
<p>One of the main drawbacks of working with image recognition is the lack of variety in some of the images available. This may cause the convolutional neural network to not operate as optimally as we would like, and return less than ideal results due to the lack of variety in the training data. There are techniques available to bypass that shortcoming and we discuss one of them in this section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Once again much of the heavy lifting is already done for us. We will use a popular Python package, <kbd>augmentor</kbd>, that is frequently used with machine learning and deep learning modeling to generate additional versions of existing images distorted and augmented for variety. </p>
<p>The package will first have to be <kbd>pip</kbd> installed using the following script: <kbd>pip install augmentor</kbd></p>
<p>We should then have confirmation that the package is installed, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/48cf4d3f-6f9f-43d8-b2dc-d607f1cf2794.png" style="width:54.00em;height:29.00em;"/></div>
<p>We will then need to import the pipeline class from augmentor:</p>
<ul>
<li><kbd>from Augmentor import Pipeline</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>This section walks through the steps to increase the frequency and augmentation of our nine sample images.</span></p>
<ol>
<li>Initialize the <kbd>augmentor</kbd> function using the following script:</li>
</ol>
<pre style="padding-left: 60px">from Augmentor import Pipeline<br/>augmentor = Pipeline('/home/asherif844/sparkNotebooks/Ch03/MNIST/images')</pre>
<ol start="2">
<li>Execute the following script so that the <kbd>augmentor</kbd> function can <kbd>rotate</kbd> our images with the following specifications:</li>
</ol>
<pre style="padding-left: 60px">augmentor.rotate(probability=0.9, max_left_rotation=25, max_right_rotation=25)</pre>
<ol start="3">
<li>Execute the following script so that each image is augmented through two iterations 10 times each:</li>
</ol>
<pre style="padding-left: 60px">for i in range(1,3):<br/>     augmentor.sample(10)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>This section explains how our nine images are used to create additional images that are distorted.</span></p>
<ol>
<li>We need to create a <kbd>Pipeline</kbd> for our image transformation and specify the location of the images that will be used.  This ensures the following:
<ol>
<li>The source location of the images</li>
<li>The number of images that will be transformed </li>
<li>The destination location of the images</li>
</ol>
</li>
<li>We can see that our destination location is created with a subfolder called <kbd>/output/</kbd> as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b894e720-39ad-4b5d-8811-934591624989.png"/></div>
<ol start="3">
<li>The <kbd>augmentor</kbd> function is configured to rotate each image up to 25 degrees to the right or 25 degrees to the left with a 90 percent probability. Basically, the probability configuration determines how often an augmentation takes place. </li>
<li>A loop is created to go through each image twice and apply two transformations to each image; however, since we did add a probability to each transformation some images may not get transformed and others may get transformed more than twice. Once the transformations are complete, we should get a message indicating so, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3d8549c-a738-41f1-949e-96d894837ede.png"/></div>
<ol start="5">
<li>Once we have the augmentations complete, we can visit the <kbd>/output/</kbd> subdirectory and see how each digit is slightly altered, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1619 image-border" src="assets/ba5e0a68-9854-4537-bfd1-7147786af584.png" style="width:162.50em;height:108.75em;"/></div>
<ol start="6">
<li>We can see that we have several variations of the digits <span class="packt_screen">3</span>, <span class="packt_screen">1</span>, <span class="packt_screen">8</span>, <span class="packt_screen">0</span>, and <span class="packt_screen">9</span> all with varying degrees of rotation. We now have tripled our sample data set and added more variety without having to go out and extract more images for training and testing purposes.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We only applied the <kbd>rotate</kbd> transformation; however, there are several transformation and augmentation features available to apply to images:</p>
<ul>
<li>Perspective skewing</li>
<li>Elastic distortions</li>
<li>Shearing</li>
<li>Cropping</li>
<li>Mirroring</li>
</ul>
<p>While not all of these transformations will be necessary when looking to increase frequency and variety of a training dataset, it may be beneficial to use some combination of features and evaluate model performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about <kbd>augmentor</kbd> visit the following website:</p>
<p><a href="https://augmentor.readthedocs.io/en/master/">https://augmentor.readthedocs.io/en/master/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #5: Utilizing alternate sources for trained images</h1>
                </header>
            
            <article>
                
<p>Sometimes there are just not enough resources available to perform a convolutional neural network. The resources could be limited from a computational perspective or a data collection perspective. In situations like these, we rely on other sources to help us with classifying our images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The technique for utilizing pre-trained models as the source for testing outcomes on other datasets is referred to as transfer learning. The advantage here is that much of the CPU resources allotted for training images is outsourced to a pre-trained model. Transfer learning has become a common extension of deep learning more recently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section explains how the process of transfer learning works.</p>
<ol>
<li><span>Collect a series of datasets or images that you are interested in classifying, </span>just as you would with traditional machine learning or deep learning.</li>
<li>Split the dataset into a training and testing split such as 75/25 or 80/20. </li>
<li>Identify a pre-trained model that will be used to identify the patterns and recognition of the images you are looking to classify.</li>
<li>Build a deep learning pipeline that connects the training data to the pre-trained model and develops the weights and parameters needed to identify the test data.</li>
<li>Finally, evaluate the model performance on the test data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains the process of transfer learning when applied to the MNIST dataset.</p>
<ol>
<li>We are definitely taking a shortcut approach with transfer learning as we are either limited in resources, time, or both as we are taking prior work that has already been done and hoping that it will help us solve something new. </li>
<li>Since we are dealing with an image classification problem, we should use a pre-trained model that has worked with classifying common images in the past. There are many common ones out there but two that stand out are:
<ol>
<li>The ResNet model developed at Microsoft.</li>
<li>The Inception model developed at Google.</li>
</ol>
</li>
<li>Both models are useful for image classification because both Microsoft and Google have a wide spectrum of images that are available to them to train a robust model that can extract features at a more detailed level.</li>
<li>Directly within Spark, there is the ability to build a deep learning pipeline and to call about a class called <kbd>DeepImageFeaturizer</kbd> and apply the <kbd>InceptionV3</kbd> model to a set of features collected from training data. The trained dataset is then evaluated on the testing data using some type of binary or multiclassification evaluator.</li>
<li>A pipeline within deep learning or machine learning is simply the workflow process used to get from an initial environment of data collection to a final evaluation or classification environment on the collected data by applying a model.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>As with everything, there are pros and cons to using transfer learning. As we discussed earlier on in the section, transfer learning is ideal when you are limited in resources to perform your own modeling on a large dataset. There is always the chance that the source data at hand does not exhibit many of the features unique to it in the pre-trained models leading to poor model performance. There is always the option to switch from one pre-trained model to another and evaluate model performance. Again, transfer learning is a fail fast approach that can be taken when other options are not available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about ResNet at Microsoft, visit the following website:</p>
<p><a href="https://resnet.microsoft.com/">https://resnet.microsoft.com/</a></p>
<p>To learn more about Inception at Google, visit the following website:</p>
<p><a href="https://www.tensorflow.org/tutorials/image_recognition">https://www.tensorflow.org/tutorials/image_recognition</a></p>
<p>To learn more specifically about InceptionV3, you can read the following paper titled <q>Rethinking the Inception Architecture for Computer Vision</q> at Cornell University:</p>
<p><a href="https://arxiv.org/abs/1512.00567">https://arxiv.org/abs/1512.00567</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain Point #6: Prioritizing high-level libraries for CNNs</h1>
                </header>
            
            <article>
                
<p>There are many libraries available to perform convolutional neural networks. Some of them are considered low-level such as TensorFlow, where much of the configuration and setup requires extensive coding. This can be considered a major pain point for an inexperienced developer. There are other libraries, such as Keras, that are high-level frameworks built on top of libraries such as TensorFlow. These libraries require much less code to get up and running with building a convolutional neural network. Often times developers getting started with building a neural network will try and implement a model with TensorFlow and run into several issues along the way. This section will propose initially building a convolutional neural network with Keras instead to predict the hand-written images from the MNIST dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this section, we will be working with Keras to train a model for recognizing handwritten images from MNIST. You can install Keras by executing the following command at the terminal:</p>
<pre><strong>pip install keras</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to build a model to recognize handwritten images from MNIST.</p>
<ol>
<li>Create testing and training images and labels based on the MNIST dataset from the following variables using the following script:</li>
</ol>
<pre style="padding-left: 60px">xtrain = data.train.images<br/>ytrain = np.asarray(data.train.labels)<br/>xtest = data.test.images <br/>ytest = np.asarray(data.test.labels)</pre>
<ol start="2">
<li>Reshape the testing and training arrays using the following script:</li>
</ol>
<pre style="padding-left: 60px">xtrain = xtrain.reshape( xtrain.shape[0],28,28,1)<br/>xtest = xtest.reshape(xtest.shape[0],28,28,1)<br/>ytest= ytest.reshape(ytest.shape[0],10)<br/>ytrain = ytrain.reshape(ytrain.shape[0],10)</pre>
<ol start="3">
<li>Import the following from <kbd>keras</kbd> to build the convolutional neural network model:</li>
</ol>
<pre style="padding-left: 60px">import keras<br/>import keras.backend as K<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Flatten, Conv2D</pre>
<ol start="4">
<li>Set the image ordering using the following script:</li>
</ol>
<pre style="padding-left: 60px">K.set_image_dim_ordering('th')</pre>
<ol start="5">
<li>Initialize the <kbd>Sequential</kbd> <kbd>model</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()</pre>
<ol start="6">
<li>Add layers to the <kbd>model</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', <br/>            input_shape=(1,28,28)))<br/>model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dense(10, activation='sigmoid'))</pre>
<ol start="7">
<li>Compile the <kbd>model</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">model.compile(optimizer='adam',loss='binary_crossentropy', <br/>              metrics=['accuracy'])</pre>
<ol start="8">
<li>Train the <kbd>model</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">model.fit(xtrain,ytrain,batch_size=512,epochs=5,<br/>            validation_data=(xtest, ytest))</pre>
<ol start="9">
<li>Test the <kbd>model</kbd> performance using the following script:</li>
</ol>
<pre style="padding-left: 60px">stats = model.evaluate(xtest, ytest)<br/>print('The accuracy rate is {}%'.format(round(stats[1],3)*100))<br/>print('The loss rate is {}%'.format(round(stats[0],3)*100))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how the convolutional neural network is built on Keras to identify handwritten images from MNIST.</p>
<ol>
<li>For any model development, we need to identify our testing and training datasets as well as the features and the labels. In our case, it is pretty straightforward as the MNIST data from TensorFlow is already broken up into <kbd>data.train.images</kbd> for the features and <kbd>data.train.labels</kbd> for the labels. Additionally, we want to convert the labels into arrays, so we utilize <kbd>np.asarray()</kbd> for <kbd>ytest</kbd> and <kbd>ytrain</kbd>.</li>
</ol>
<ol start="2">
<li>The arrays for <kbd>xtrain</kbd>, <kbd>xtest</kbd>, <kbd>ytrain</kbd>, and <kbd>ytest</kbd> are currently not in the proper shape to be used for a convolutional neural network within Keras. As we identified early on in the chapter, the features for the MNIST images represent 28 x 28-pixel images and the labels indicate one of ten values from 0 through 9. The x-arrays will be reshaped to <span class="packt_screen">(,28,28,1)</span> and the y-arrays will be reshaped to <span class="packt_screen">(,10)</span>. The <kbd>shape</kbd> of the new arrays can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9d8995f0-b3cd-4f67-ac86-77e4906fd698.png" style="width:34.42em;height:20.75em;"/></div>
<ol start="3">
<li>As mentioned previously, Keras is a high-level library; therefore, it does not perform tensor or convolutional operations without the assistance of a lower level library such as TensorFlow. In order to configure these operations, we set the <kbd>backend</kbd> to be <kbd>K</kbd> for <kbd>Keras</kbd> with the image dimensional ordering, <kbd>image_dim_ordering</kbd>, set to <kbd>tf</kbd> for TensorFlow.</li>
</ol>
<div class="packt_infobox">Please note that the backend could also be set to other low-level libraries, such as <kbd>Theano</kbd>. Instead of <kbd>tf</kbd>, we would set the dimensional ordering to <kbd>th</kbd>. Additionally, we would need to reconstruct the shaping of the features. However, in the past few years, <kbd>Theano</kbd> has not garnered the same adoption rate <kbd>TensorFlow</kbd> has.</div>
<ol start="4">
<li>Once we import the necessary libraries to build the CNN model, we can begin constructing the sequences or layers, <kbd>Sequential()</kbd>, of the model. For demonstration purposes, we will keep this model as simple as possible with only 4 layers to prove that we can still gain a high accuracy with minimal complexity. Each layer is added using the <kbd>.add()</kbd> method.
<ol>
<li>The first layer is set to build a 2-Dimensional (<kbd>Conv2D</kbd>) convolution layer, which is common for spatial images such as the MNIST data. Since it is the first layer, we must explicitly define the <kbd>input_shape</kbd> of the incoming data. Additionally, we specify a <kbd>kernel_size</kbd> that is used to set the height and width of the window filter used for convolution. Usually, this is either a 3x3 window or 5x5 window for the 32 filters. Additionally, we have to set an activation function for this layer and rectified linear units, <kbd>relu</kbd>, are a good option here for efficiency purposes, especially early on in the neural network.</li>
<li>Next, the second layer flattens the first layer inputs to retrieve a classification that we can use to determine whether the image is one of a possible 10 digits.</li>
<li>Third, we pass the outputs from the second layer into a <kbd>dense</kbd> layer that has 128 hidden layers with another <kbd>relu</kbd> activation function. The function within a densely connected layer incorporates the <kbd>input_shape</kbd> and <kbd>kernel_size</kbd> as well as the bias to create the output for each of the 128 hidden layers. </li>
<li>The final layer is the output that will determine what the predicted value will be for the MNIST image. We add another <kbd>dense</kbd> layer with a <kbd>sigmoid</kbd> function to output probabilities for each of the 10 possible scenarios our MNIST image could be. Sigmoid functions are useful for binary or multiclass classification outcomes.</li>
</ol>
</li>
<li>The next step is to compile the model using <kbd>adam</kbd> for the <kbd>optimizer</kbd> and evaluating <kbd>accuracy</kbd> for the <kbd>metrics</kbd>. The <kbd>adam</kbd> optimizer is common for CNN models as is using <kbd>categorical_crossentropy</kbd> as a loss function when dealing with multiclassification scenarios for 10 possible outcomes as is our case. </li>
</ol>
<ol start="6">
<li>We train the model using a <kbd>batch_size</kbd> of <kbd>512</kbd> images at a time over <kbd>5</kbd> runs or <kbd>epochs</kbd>. The loss and accuracy of each epoch are captured and can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/523622b0-32c1-4a34-a0b7-37358cd01f84.png" style="width:59.00em;height:40.75em;"/></div>
<ol start="7">
<li>We calculate the <span class="packt_screen">accuracy</span> and the <span class="packt_screen">loss rate</span> by evaluating the trained model on the test dataset as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2bcbcfc2-85d3-453b-8acd-9cb6a5f7b1bf.png" style="width:59.00em;height:11.00em;"/></div>
<ol start="8">
<li>Our model seems to be performing well with a <span class="packt_screen">98.6%</span> <span class="packt_screen">accuracy rate</span> and a <span class="packt_screen">5% loss rate</span>. </li>
<li>We built a simple convolutional neural network in Keras using five lines of code for the actual model design. Keras is a great way to get a model up and running in little time and code. Once you are ready to move onto more sophisticated model development and control, it may make more sense to build a convolutional neural network in TensorFlow.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In addition to retrieving the accuracy of the model we can also produce the shapes within each layer of the CNN modeling process by executing the following script:</p>
<pre>model.summary()</pre>
<p>The output of the <kbd>model.summary()</kbd> can be seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aac3bf89-2b4a-407a-a159-a8ae8df4bf62.png" style="width:42.25em;height:17.67em;"/></div>
<p>We see that the output shape of the first layer <span class="packt_screen">(None, 24, 24, 32)</span> was flattened out into a shape of <span class="packt_screen">(None, 18432)</span> by multiplying 24 x 24 x 32 within the second layer. Additionally, we see our third and fourth layers have the shape that we assigned them using the <span class="packt_screen">Dense</span> layer function</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about 2D convolutional layer development in Keras, visit the following website:</p>
<p><a href="https://keras.io/layers/convolutional/#conv2d">https://keras.io/layers/convolutional/#conv2d</a></p>
<p>To learn how to build a convolutional neural network in TensorFlow with MNIST images, visit the following website:</p>
<p><a href="https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros">https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros</a></p>


            </article>

            
        </section>
    </body></html>