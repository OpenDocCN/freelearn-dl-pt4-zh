<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer013">
			<h1 id="_idParaDest-76" class="chapter-number"><a id="_idTextAnchor140"/>2</h1>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor141"/>Getting Started with PyTorch</h1>
			<p>In this chapter, we’ll explore <strong class="bold">PyTorch</strong>, a leading deep learning library <span class="No-Break">in Python.</span></p>
			<p>We go over several operations that are useful for understanding how neural networks are built using PyTorch. Besides tensor operations, we will also explore how to train different types of neural networks. Specifically, we will focus on feedforward, recurrent, <strong class="bold">long short-term memory</strong><strong class="bold"> </strong>(<strong class="bold">LSTM</strong>), and 1D <span class="No-Break">convolutional networks.</span></p>
			<p>In later chapters, we will also cover other types of neural networks, such as transformers. Here, we will use synthetic data for demonstrative purposes, which will help us showcase both the implementation and theory behind <span class="No-Break">each model.</span></p>
			<p>Upon completing this chapter, you will have gained a robust understanding of PyTorch, equipping you with the tools for more advanced deep <span class="No-Break">learning projects.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li><span class="No-Break">Installing PyTorch</span></li>
				<li>Basic operations <span class="No-Break">in PyTorch</span></li>
				<li>Advanced operations <span class="No-Break">in PyTorch</span></li>
				<li>Building a simple neural network <span class="No-Break">with PyTorch</span></li>
				<li>Training a feedforward <span class="No-Break">neural network</span></li>
				<li>Training a recurrent <span class="No-Break">neural network</span></li>
				<li>Training an LSTM <span class="No-Break">neural network</span></li>
				<li>Training a convolutional <span class="No-Break">neural network</span></li>
			</ul>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor142"/>Technical requirements</h1>
			<p>Before starting, you will need to ensure that your system meets the following <span class="No-Break">technical requirements:</span></p>
			<ul>
				<li><strong class="bold">Python 3.9</strong>: You can download Python <span class="No-Break">from </span><a href="https://www.python.org/downloads/"><span class="No-Break">https://www.python.org/downloads/</span></a><span class="No-Break">.</span></li>
				<li>pip (23.3.1) or Anaconda: These are popular package managers for Python. pip comes with Python by default. Anaconda can be downloaded <span class="No-Break">from </span><a href="https://www.anaconda.com/products/distribution"><span class="No-Break">https://www.anaconda.com/products/distribution</span></a><span class="No-Break">.</span></li>
				<li>torch (2.2.0): The main library we will be using for deep learning in <span class="No-Break">this chapter.</span></li>
				<li><strong class="bold">CUDA (optional)</strong>: If you have a CUDA-capable GPU on your machine, you can install a version of PyTorch that supports CUDA. This will enable computations on your GPU and can significantly speed up your deep <span class="No-Break">learning experiments.</span></li>
			</ul>
			<p>It’s worth noting that the code presented in this chapter is platform-independent and should run on any system with the preceding <span class="No-Break">requirements satisfied.</span></p>
			<p>The code for this chapter can be found at the following GitHub <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor143"/>Installing PyTorch</h1>
			<p>To start <a id="_idIndexMarker047"/>with PyTorch, we need to install it first. As of the time of writing, PyTorch supports Linux, macOS, and Windows platforms. Here, we will guide you through the installation process on these <span class="No-Break">operating systems.</span></p>
			<h2 id="_idParaDest-80">Getting ready<a id="_idTextAnchor144"/></h2>
			<p><strong class="source-inline">PyTorch</strong> is usually installed via <strong class="source-inline">pip</strong> or Anaconda. We recommend creating a new Python environment before installing the library, especially if you will be working on multiple Python projects on your system. This is to prevent any conflicts between different versions of Python libraries that different projects <span class="No-Break">may require.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor145"/>How to do it…</h2>
			<p>Let’s see how to install <strong class="source-inline">PyTorch</strong>. We’ll describe how to do this using either <strong class="source-inline">pip</strong> or <strong class="source-inline">Anaconda</strong>. We’ll also provide some information about how to use a <span class="No-Break">CUDA environment.</span></p>
			<p>If you’re using <strong class="source-inline">pip</strong>, Python’s package manager, you can install PyTorch by running the following command in <span class="No-Break">your terminal:</span></p>
			<pre class="console">
pip install torch<a id="_idTextAnchor146"/></pre>			<p>With the Anaconda Python distribution, you can install PyTorch using the <span class="No-Break">following command:</span></p>
			<pre class="console">
conda install pytorch torchvision -c pytorc<a id="_idTextAnchor147"/>h</pre>			<p>If you have a <a id="_idIndexMarker048"/>CUDA-capable GPU on your machine, you can install a version of <strong class="source-inline">PyTorch</strong> that supports CUDA to enable computations on your GPU. This can significantly speed up your deep learning experiments. The PyTorch website provides a tool that generates the appropriate installation command based on your needs. Visit the PyTorch website, select your preferences (such as OS, package manager, Python version, and CUDA version) in the <strong class="bold">Quick Start Locally</strong> section, and then copy the generated command into <span class="No-Break">your terminal<a id="_idTextAnchor148"/>.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor149"/>How it works…</h2>
			<p>After you’ve installed <strong class="source-inline">PyTorch</strong>, you can verify that everything is working correctly by opening a Python interpreter and running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import torch
print(torch.__version__)</pre>			<p>This should output the version of <strong class="source-inline">PyTorch</strong> that you installed. Now, you’re ready to start using <strong class="source-inline">PyTorch</strong> for <span class="No-Break">deep learning!</span></p>
			<p>In the next sections, we will familiarize ourselves with the basics of <strong class="source-inline">PyTorch</strong> and build our first <span class="No-Break">neural network.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor150"/>Basic operations in PyTorch</h1>
			<p>Before we<a id="_idIndexMarker049"/> start building neural networks with <strong class="source-inline">PyTorch</strong>, it is essential to understand the basics of how to manipulate data using this library. In <strong class="source-inline">PyTorch</strong>, the fundamental unit of data is the tensor, a generalization of matrices to an arbitrary number of dimensions (also known as a <span class="No-Break">multidimensional array).</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor151"/>Getting ready</h2>
			<p>A tensor <a id="_idIndexMarker050"/>can be a number (a 0D tensor), a vector (a 1D tensor), a matrix (a 2D tensor), or any multi-dimensional data (a 3D tensor, a 4D tensor, and so on). <strong class="source-inline">PyTorch</strong> provides various functions to create and <span class="No-Break">manipulate tensors.</span></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor152"/>How to do it…</h2>
			<p>Let’s start by <span class="No-Break">importing </span><span class="No-Break"><strong class="source-inline">PyTorch</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import torch</pre>			<p>We can create a tensor in <strong class="source-inline">PyTorch</strong> using various techniques. Let’s start by creating tensors <span class="No-Break">from lists:</span></p>
			<pre class="source-code">
t1 = torch.tensor([1, 2, 3])
print(t1)
t2 = torch.tensor([[1, 2], [3, 4]])
print(t2)</pre>			<p><strong class="source-inline">PyTorch</strong> can seamlessly integrate with NumPy, allowing for easy tensor creation from <span class="No-Break"><strong class="source-inline">NumPy</strong></span><span class="No-Break"> arrays:</span></p>
			<pre class="source-code">
import numpy as np
np_array = np.array([5, 6, 7])
t3 = torch.from_numpy(np_array)
print(t3)</pre>			<p><strong class="source-inline">PyTorch</strong> also provides functions to generate tensors with specific values, such as zeros <span class="No-Break">or ones:</span></p>
			<pre class="source-code">
t4 = torch.zeros((3, 3))
print(t4)
t5 = torch.ones((3, 3))
print(t5)
t6 = torch.eye(3)
print(t6)</pre>			<p>These are <a id="_idIndexMarker051"/>commonly used methods in <strong class="source-inline">NumPy</strong> that are also available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">PyTorch</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor153"/>How it works…</h2>
			<p>Now that we know how to create tensors, let’s look at some basic operations. We can perform all standard arithmetic operations <span class="No-Break">on tensors:</span></p>
			<pre class="source-code">
result = t1 + t3
print(result)
result = t3 - t1
print(result)
result = t1 * t3
print(result)
result = t3 / t1
print(result)</pre>			<p>You can reshape tensors using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">reshape()</strong></span><span class="No-Break"> method:</span></p>
			<pre class="source-code">
t7 = torch.arange(9) # Creates a 1D tensor [0, 1, 2, ..., 8]
t8 = t7.reshape((3, 3)) # Reshapes the tensor to a 3x3 matrix
print(t8)</pre>			<p>This is a brief introduction to tensor operations in <strong class="source-inline">PyTorch</strong>. As you dive deeper, you’ll find that <strong class="source-inline">PyTorch</strong> offers various operations to manipulate tensors, giving you the flexibility and control needed to implement complex deep learning models <span class="No-Break">and algorithms.</span></p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor154"/>Advanced operations in PyTorch</h1>
			<p>After <a id="_idIndexMarker052"/>exploring basic tensor operations, let’s now dive into more advanced operations in <strong class="source-inline">PyTorch</strong>, specifically the linear algebra operations that form the backbone of most numerical computations in <span class="No-Break">deep learning.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor155"/>Getting ready</h2>
			<p>Linear algebra is a subset of mathematics. It deals with vectors, vector spaces, and linear transformations between these spaces, such as rotations, scaling, and shearing. In the context of deep learning, we deal with high-dimensional vectors (tensors), and operations on these vectors play a crucial role in the internal workings <span class="No-Break">of models.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor156"/>How to do it…</h2>
			<p>Let’s start by revisiting the tensors we created in the <span class="No-Break">previous section:</span></p>
			<pre class="source-code">
print(t1)
print(t2)</pre>			<p>The dot product of two vectors is a scalar that measures the vectors’ direction and magnitude. In <strong class="source-inline">PyTorch</strong>, we can calculate the dot product of two <strong class="source-inline">1D</strong> tensors using the <span class="No-Break"><strong class="source-inline">torch.dot()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
dot_product = torch.dot(t1, t3)
print(dot_product)</pre>			<p>Unlike element-wise multiplication, matrix multiplication, also known as the dot product, is the operation of multiplying two matrices to produce a new matrix. <strong class="source-inline">PyTorch</strong> provides the <strong class="source-inline">torch.mm()</strong> function to perform <span class="No-Break">matrix multiplication:</span></p>
			<pre class="source-code">
matrix_product = torch.mm(t2, t5)
print(matrix_product)</pre>			<p>The transpose of a matrix is a new matrix whose rows are the columns of the original matrix and whose columns are the rows. You can compute the transpose of a tensor using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">T</strong></span><span class="No-Break"> attribute:</span></p>
			<pre class="source-code">
t_transposed = t2.T
print(t_transposed)</pre>			<p>There are other operations you can perform, such as calculating the determinant of a matrix and finding the inverse of a matrix. Let’s look at a couple of <span class="No-Break">these operations:</span></p>
			<pre class="source-code">
det = torch.det(t2)
print(det)
inverse = torch.inverse(t2)
print(inverse)</pre>			<p>Note that these two operations are only defined for <strong class="source-inline">2D</strong> <span class="No-Break">tensors (matrices).</span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor157"/>How it works…</h2>
			<p><strong class="source-inline">PyTorch</strong> is a <a id="_idIndexMarker053"/>highly optimized library for performing basic and advanced operations, particularly linear algebra operations that are crucial in <span class="No-Break">deep learning.</span></p>
			<p>These operations make <strong class="source-inline">PyTorch</strong> a powerful tool for building and training neural networks and performing high-level computations in a more general context. In the next section, we will use these building blocks to start constructing deep <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-91">Building a simple neural network with P<a id="_idTextAnchor158"/>yTorch</h1>
			<p>This section <a id="_idIndexMarker054"/>will build a simple two-layer neural network from scratch using only basic tensor operations to solve a time<a id="_idIndexMarker055"/> series prediction problem. We aim to demonstrate how one might manually implement a feedforward pass, backpropagation, and optimization steps without leveraging <strong class="source-inline">PyTorch</strong>’s predefined layers and <span class="No-Break">optimization routines.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor159"/>Getting ready</h2>
			<p>We use synthetic data for this demonstration. Suppose we have a simple time series data of <strong class="source-inline">100</strong> samples, each with <strong class="source-inline">10</strong> time steps. Our task is to predict the next time step based on the <span class="No-Break">previous ones:</span></p>
			<pre class="source-code">
X = torch.randn(100, 10)
y = torch.randn(100, 1)</pre>			<p>Now, let’s create a <span class="No-Break">neural network.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor160"/>How to do it…</h2>
			<p>Let’s start <a id="_idIndexMarker056"/>by defining our model parameters and their initial values. Here, we are creating a simple two-layer network, so we have two sets of weights <span class="No-Break">and biases:</span></p>
			<p>We use the <strong class="source-inline">requires_grad_()</strong> function to tell <strong class="source-inline">PyTorch</strong> that we want to compute gradients with respect to these tensors during the <span class="No-Break">backward pass.</span></p>
			<p>Next, we<a id="_idIndexMarker057"/> define our model. For this simple network, we’ll use a sigmoid activation function for the <span class="No-Break">hidden layer:</span></p>
			<pre class="source-code">
input_size = 10
hidden_size = 5
output_size = 1
W1 = torch.randn(hidden_size, input_size).requires_grad_()
b1 = torch.zeros(hidden_size, requires_grad=True)
W2 = torch.randn(output_size, hidden_size).requires_grad_()
b2 = torch.zeros(output_size, requires_grad=True)
def simple_neural_net(x, W1, b1, W2, b2):
    z1 = torch.mm(x, W1.t()) + b1
    a1 = torch.sigmoid(z1)
    z2 = torch.mm(a1, W2.t()) + b2
    return z2</pre>			<p>Now, we’re ready to train our model. Let’s define the learning rate and the number <span class="No-Break">of epochs:</span></p>
			<pre class="source-code">
lr = 0.01
epochs = 100
loss_fn = torch.nn.MSELoss()
for epoch in range(epochs):
    y_pred = simple_neural_net(X, W1, b1, W2, b2)
    loss = loss_fn(y_pred.squeeze(), y)
    loss.backward()
    with torch.no_grad():
        W1 -= lr * W1.grad
        b1 -= lr * b1.grad
        W2 -= lr * W2.grad
        b2 -= lr * b2.grad
    W1.grad.zero_()
    b1.grad.zero_()
    W2.grad.zero_()
    b2.grad.zero_()
    if epoch % 10 == 0:
        print(f'Epoch: {epoch} \t Loss: {loss.item()}')</pre>			<p>This<a id="_idIndexMarker058"/> basic code demonstrates the essential <a id="_idIndexMarker059"/>parts of a neural network: the forward pass, where we compute predictions; the backward pass, where gradients are computed; and the update step, where we adjust our weights to minimize <span class="No-Break">the loss.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor161"/>There’s more…</h2>
			<p>This chapter is focused on exploring the intricacies of the training process of a neural network. In future chapters, we’ll show how to train deep neural networks without worrying about most of <span class="No-Break">these details.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor162"/>Training a feedforward neural network</h1>
			<p>This recipe <a id="_idIndexMarker060"/>walks you through<a id="_idIndexMarker061"/> the process of building a feedforward neural network <span class="No-Break">using PyTorch.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor163"/>Getting ready</h2>
			<p>Feedforward neural networks, also <a id="_idIndexMarker062"/>known as <strong class="bold">multilayer perceptrons</strong> (<strong class="bold">MLPs</strong>), are one of the simplest types of artificial neural networks. The data flows from the input layer to the output layer, passing through hidden layers without any loop. In this type of neural network, all hidden units in one layer are connected to the units of the <span class="No-Break">following layer.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor164"/>How to do it…</h2>
			<p>Let’s <a id="_idIndexMarker063"/>create a simple feedforward neural network using <strong class="source-inline">PyTorch</strong>. First, we need to import the necessary <span class="No-Break"><strong class="source-inline">PyTorch</strong></span><span class="No-Break"> modules:</span></p>
			<pre class="source-code">
import torch
import torch.nn as nn</pre>			<p>Now, we can<a id="_idIndexMarker064"/> define a simple feedforward neural network with one <span class="No-Break">hidden layer:</span></p>
			<pre class="source-code">
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
net = Net()
print(net)</pre>			<p>In the preceding code, <strong class="source-inline">nn.Module</strong> is the base class for all neural network modules in <strong class="source-inline">PyTorch</strong>, and our network is a subclass <span class="No-Break">of it.</span></p>
			<p>The <strong class="source-inline">forward()</strong> method in this class represents the forward pass of the network. This is the computation that the network performs when transforming inputs into outputs. Here’s a <span class="No-Break">step-by-step explanation:</span></p>
			<ul>
				<li>The <strong class="source-inline">forward()</strong> method takes an input tensor <strong class="source-inline">x</strong>. This tensor represents the input data. Its shape should be compatible with the network’s layers. In this case, as the first linear layer (<strong class="source-inline">self.fc1</strong>) expects <strong class="source-inline">10</strong> input features, the last dimension of <strong class="source-inline">x</strong> should <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
				<li>The input tensor is first passed through a linear transformation, represented by <strong class="source-inline">self.fc1</strong>. This object is an instance of <strong class="source-inline">PyTorch</strong>’s <strong class="source-inline">nn.Linear</strong> class, and it performs a linear transformation that involves multiplying the input data with a weight matrix and adding a bias vector. As defined in the <strong class="source-inline">__init__</strong><strong class="source-inline">()</strong> method, this layer transforms the 10D space to a 5D space using a linear transformation. This reduction is often seen as the neural network “learning” or “extracting” features from the <span class="No-Break">input data.</span></li>
				<li>The <a id="_idIndexMarker065"/>output of the first layer is then<a id="_idIndexMarker066"/> passed through a <strong class="bold">rectified linear unit</strong><strong class="bold"> </strong>(<strong class="bold">ReLU</strong>) activation function using <strong class="source-inline">torch.relu()</strong>. This is a simple non-linearity that replaces <a id="_idIndexMarker067"/>negative values in the tensor with zeros. This allows the neural network to model more complex relationships between the inputs and <span class="No-Break">the outputs.</span></li>
				<li>The output from the <strong class="source-inline">ReLU</strong><strong class="source-inline">()</strong> function is then passed through another linear transformation, <strong class="source-inline">self.fc2</strong>. As before, this object is an instance of <strong class="source-inline">PyTorch</strong>’s <strong class="source-inline">nn.Linear</strong> class. This layer reduces the dimensionality of the tensor from <strong class="source-inline">5</strong> (the output size of the previous layer) to <strong class="source-inline">1</strong> (the desired <span class="No-Break">output size).</span></li>
			</ul>
			<p>Finally, the output of the second linear layer is returned by the <strong class="source-inline">forward()</strong> method. This output can then be used for various purposes, such as computing a loss for training the network, or as the final output in an inference task (that is when the network is used <span class="No-Break">for prediction).</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor165"/>How it works…</h2>
			<p>To train the network, we need a dataset to train on, a loss function, and <span class="No-Break">an optimizer.</span></p>
			<p>Let’s use the same synthetic dataset that we defined for our <span class="No-Break">previous example:</span></p>
			<pre class="source-code">
X = torch.randn(100, 10)
Y = torch.randn(100, 1)</pre>			<p>We can use the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) loss<a id="_idIndexMarker068"/> for our task, which is a common loss function for regression problems. PyTorch provides a built-in implementation of this <span class="No-Break">loss function:</span></p>
			<pre class="source-code">
loss_fn = nn.MSELoss()</pre>			<p>We will<a id="_idIndexMarker069"/> use <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) as our optimizer. SGD is a type of iterative <a id="_idIndexMarker070"/>method for optimizing <a id="_idIndexMarker071"/>the <span class="No-Break">objective function:</span></p>
			<pre class="source-code">
optimizer = torch.optim.SGD(net.parameters(), lr=0.01)</pre>			<p>Now we can train our network. We’ll do this for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span></p>
			<pre class="source-code">
for epoch in range(100):
    output = net(X)
    loss = loss_fn(output, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')</pre>			<p>In each epoch, we perform a forward pass, compute the loss, perform a backward pass to calculate gradients, and then update <span class="No-Break">our weights.</span></p>
			<p>You have now trained a simple feedforward neural network using <strong class="source-inline">PyTorch</strong>. In the upcoming sections, we will dive deeper into more complex network architectures and their applications in time <span class="No-Break">series analysis.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor166"/>Training a recurrent neural network</h1>
			<p><strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) are <a id="_idIndexMarker072"/>a class of neural networks that are especially effective for tasks involving sequential data, such as time series forecasting and natural <span class="No-Break">language processing.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor167"/>Getting ready</h2>
			<p>RNNs use<a id="_idIndexMarker073"/> sequential information by having hidden layers capable of passing information from one step in the sequence to <span class="No-Break">the next.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor168"/>How to do it…</h2>
			<p>Similar to the feedforward network, we begin by defining our <strong class="source-inline">RNN</strong> class. For simplicity, let’s define a <span class="No-Break">single-layer </span><span class="No-Break"><strong class="source-inline">RNN</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)  # get RNN output
        out = self.fc(out[:, -1, :])
        return out
rnn = RNN(10, 20, 1)
print(rnn)</pre>			<p>Here, <strong class="source-inline">input_size</strong> is the number of input features per time step, <strong class="source-inline">hidden_size</strong> is the number of neurons in the hidden layer, and <strong class="source-inline">output_size</strong> is the number of output features. In the <strong class="source-inline">forward()</strong><strong class="source-inline"> </strong>method, we pass the input <strong class="source-inline">x</strong> and the initial hidden state <strong class="source-inline">h0</strong> to the recurrent layer. The RNN returns the output and the final hidden state, which we ignore for now. We then take the last output of the sequence (<strong class="source-inline">out[:, -1, :]</strong>) and pass it through a fully connected layer to get our final output. The hidden states act as the memory of the network, encoding the temporal context of the inputs up to the current time step, which is why this type of neural network is useful for <span class="No-Break">sequential data.</span></p>
			<p>Let’s note some<a id="_idIndexMarker074"/> details we used in our code in <span class="No-Break">this example:</span></p>
			<ul>
				<li> <strong class="source-inline">x.device</strong>: This refers to the device where the <strong class="source-inline">x</strong> tensor is located. In <strong class="source-inline">PyTorch</strong>, tensors can be on the CPU or a GPU, and <strong class="source-inline">.device</strong> is a property that tells you where the tensor currently resides. This is particularly important when you are running computations on a GPU, as all inputs to a computation must be on the same device. In the line of code <strong class="source-inline">h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)</strong>, we’re ensuring that the initial hidden state tensor <strong class="source-inline">h0</strong> is on the same device as the <strong class="source-inline">x</strong> <span class="No-Break">input tensor.</span></li>
				<li><strong class="source-inline">x.size(0)</strong>: This refers to the size of the <strong class="source-inline">0th</strong> dimension of the tensor <strong class="source-inline">x</strong>. In <strong class="source-inline">PyTorch</strong>, <strong class="source-inline">size()</strong> returns the shape of the tensor, and <strong class="source-inline">size(0)</strong> gives the size of the first dimension. In the context of this RNN, <strong class="source-inline">x</strong> is expected to be a 3D tensor with shape (<strong class="source-inline">batch_size</strong>,  <strong class="source-inline">sequence_length</strong>, <strong class="source-inline">num_features</strong>), so <strong class="source-inline">x.size(0)</strong> would return the <span class="No-Break">batch size.</span></li>
			</ul>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor169"/>How it works…</h2>
			<p>The training process for RNNs is similar to that of feedforward networks. We’ll use the same synthetic dataset, loss function (MSE), and optimizer (SGD) from the previous example. However, let’s modify the input data to be 3D, as required by the RNN (<strong class="source-inline">batch_size</strong>, <strong class="source-inline">sequence_length</strong>, <strong class="source-inline">num_features</strong>). The three dimensions of the input tensor to an RNN represent the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="source-inline">batch_size</strong>: This represents the number of sequences in one batch of data. In time series terms, you can think of one sample as one sub-sequence (for example, the sales of the past five days). So, a batch contains multiple such samples or sub-sequences, allowing the model to process and learn from multiple <span class="No-Break">sequences simultaneously.</span></li>
				<li><strong class="source-inline">sequence_length</strong>: This is essentially the size of the window you use to look at your data. It specifies the number of time steps included in each input sub-sequence. For instance, if you’re predicting today’s temperature based on past data, <strong class="source-inline">sequence_length</strong> determines how many days back in the past your model looks at <span class="No-Break">each step.</span></li>
				<li><strong class="source-inline">num_features</strong>: This dimension indicates the number of features (variables) in each time step of the data sequence. In the context of time series, a univariate series (such as daily temperature at a single location) has one feature per time step. In contrast, a multivariate series (such as daily temperature, humidity, and wind speed at the same location) has multiple features per <span class="No-Break">time step.</span></li>
			</ul>
			<p>Let’s create a<a id="_idIndexMarker075"/> synthetic dataset as <span class="No-Break">an example:</span></p>
			<pre class="source-code">
X = torch.randn(100, 5, 10)
Y = torch.randn(100, 1)</pre>			<p>Now, we can train our network. We’ll do this for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span></p>
			<pre class="source-code">
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)
for epoch in range(100):
    output = rnn(X)
    loss = loss_fn(output, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")</pre>			<p>Now, we have trained an RNN. This is a big step towards applying these models to real-world time series data, which we will discuss in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor170"/>Training an LSTM neural network</h1>
			<p>RNNs suffer <a id="_idIndexMarker076"/>from a fundamental problem of “vanishing gradients” where, due to the nature of backpropagation in neural networks, the influence of earlier inputs on the overall error diminishes drastically as the sequence gets longer. This is especially problematic in sequence processing tasks where long-term dependencies exist (i.e., future outputs depend on much <span class="No-Break">earlier inputs).</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor171"/>Getting ready</h2>
			<p>LSTM networks were introduced to overcome this problem. They use a more complex internal structure for each of their cells compared to RNNs. Specifically, an LSTM has the ability to decide which information to discard or to store based on an internal structure called a cell. This cell uses gates (input, forget, and output gates) to control the flow of information into and out of the cell. This helps maintain and manipulate the “long-term” information, thereby mitigating the vanishing <span class="No-Break">gradient problem.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor172"/>How to do it…</h2>
			<p>We begin by defining our <strong class="source-inline">LSTM</strong> class. For simplicity, we’ll define a single-layer <strong class="source-inline">LSTM</strong> network. Note that <strong class="source-inline">PyTorch</strong>’s LSTM expects inputs to be 3D in the format <strong class="source-inline">batch_size</strong>, <strong class="source-inline">seq_length</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">num_features</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
class LSTM(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
     super(LSTM, self).__init__()
     self.hidden_size = hidden_size
     self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
     self.fc = nn.Linear(hidden_size, output_size)
  def forward(self, x):
     h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
     c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
     out, _ = self.lstm(x, (h0, c0))  # get LSTM output
     out = self.fc(out[:, -1, :])
     return out
lstm = LSTM(10, 20, 1) # 10 features, 20 hidden units, 1 output
print(lstm)</pre>			<p>The <strong class="source-inline">forward()</strong> <a id="_idIndexMarker077"/>method is very similar to the one that we introduced earlier for RNNs. The main difference resides in the fact that in the RNNs’ case, we initialized a single hidden state <strong class="source-inline">h0</strong>, and passed it to the <strong class="source-inline">RNN</strong> layer along with the input <strong class="source-inline">x</strong>. In the LSTM, however, you need to initialize both a hidden state <strong class="source-inline">h0</strong> and a cell state <strong class="source-inline">c0</strong> because of the internal structure of LSTM cells. These states are then passed as a tuple to the <strong class="source-inline">LSTM</strong> layer along with the <span class="No-Break">input </span><span class="No-Break"><strong class="source-inline">x</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor173"/>How it works…</h2>
			<p>The training process for LSTM networks is similar to that of feedforward networks and RNNs. We’ll use the same synthetic dataset, loss function (MSE), and optimizer (SGD) from the <span class="No-Break">previous examples:</span></p>
			<pre class="source-code">
X = torch.randn(100, 5, 10)
Y = torch.randn(100, 1)
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(lstm.parameters(), lr=0.01)
for epoch in range(100):
      output = lstm(X)
      loss = loss_fn(output, Y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      print(f'Epoch {epoch+1}, Loss: {loss.item()}')</pre>			<h1 id="_idParaDest-107"><a id="_idTextAnchor174"/>Training a convolutional neural network</h1>
			<p><strong class="bold">Convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) are a class of neural networks particularly <a id="_idIndexMarker078"/>effective for tasks involving grid-like input data such as images, audio spectrograms, and even certain types of time <span class="No-Break">series data.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor175"/>Getting ready</h2>
			<p>The central idea of CNNs is to apply a convolution operation on the input data with convolutional filters (also known as kernels), which slide over the input data to produce output <span class="No-Break">feature maps.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor176"/>How to do it…</h2>
			<p>For simplicity, let’s define a single-layer <strong class="source-inline">1D</strong> convolutional neural network, which is particularly suited for time series and sequence data. In <strong class="source-inline">PyTorch</strong>, we can use the <strong class="source-inline">nn.Conv1d</strong> layer <span class="No-Break">for this:</span></p>
			<pre class="source-code">
class ConvNet(nn.Module):
    def __init__(self,
        input_size,
        hidden_size,
        output_size,
        kernel_size,
        seq_length):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size)
        self.fc = nn.Linear(hidden_size*(seq_length-kernel_size+1),
            output_size)
    def forward(self, x):
        x = x.transpose(1, 2)
        out = torch.relu(self.conv1(x))
        out = out.view(out.size(0), -1)  # flatten the tensor
        out = self.fc(out)
        return out
convnet = ConvNet(5, 20, 1, 3, 10)
print(convnet)</pre>			<p>In the <strong class="source-inline">forward</strong><a id="_idIndexMarker079"/> method, we pass the input through a convolutional layer followed by a <strong class="source-inline">ReLU</strong><strong class="source-inline">()</strong> activation function and finally pass it through a fully connected layer. The <strong class="source-inline">Conv1d</strong> layer expects an input of shape (<strong class="source-inline">batch_size</strong>, <strong class="source-inline">num_channels</strong>, and <strong class="source-inline">sequence_length</strong>). Here, <strong class="source-inline">num_channels</strong> refers to the number of input channels (equivalent to the number of features in the time series data), and <strong class="source-inline">sequence_length</strong> refers to the number of time steps in <span class="No-Break">each sample.</span></p>
			<p>The <strong class="source-inline">Linear</strong> layer will take the output from the <strong class="source-inline">Conv1d</strong> layer and reduce it to the desired output size. The input to the <strong class="source-inline">Linear</strong> layer is calculated as <strong class="source-inline">hidden_size*(seq_length-kernel_size+1)</strong>, where <strong class="source-inline">hidden_size</strong> is the number of output channels from the <strong class="source-inline">Conv1d</strong> layer, and <strong class="source-inline">seq_length-kernel_size+1</strong> is the output sequence length after the <span class="No-Break">convolution operation.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor177"/>How it works…</h2>
			<p>The training process for <strong class="source-inline">1D</strong> CNNs is similar to the previous network types. We’ll use the same loss function (MSE), and optimizer (SGD), but let’s modify the input data to be of size (<strong class="source-inline">batch_size</strong>, <strong class="source-inline">sequence_length</strong>, <strong class="source-inline">num_channels</strong>). Recall that the number of channels is equivalent to the number <span class="No-Break">of features:</span></p>
			<pre class="source-code">
X = torch.randn(100, 10, 5)
Y = torch.randn(100, 1)</pre>			<p>Now, we can train our network. We’ll do this for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span></p>
			<pre class="source-code">
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(convnet.parameters(), lr=0.01)
for epoch in range(100):
    output = convnet(X)
    loss = loss_fn(output, Y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')</pre>			<p>In the <a id="_idIndexMarker080"/>preceding code, we iterate over each epoch. After each training cycle, we print the error of the model into the console to monitor the <span class="No-Break">training process.</span></p>
		</div>
	</div>
</div>
</body></html>