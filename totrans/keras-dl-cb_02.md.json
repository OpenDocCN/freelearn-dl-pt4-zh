["```py\nX = tf.placeholder(\"float\", shape=[None, x_size])\ny = tf.placeholder(\"float\", shape=[None, y_size])\nweights_1 = initialize_weights((x_size, hidden_size), stddev)\nweights_2 = initialize_weights((hidden_size, y_size), stddev)\nsigmoid = tf.nn.sigmoid(tf.matmul(X, weights_1))\ny = tf.matmul(sigmoid, weights_2)\n```", "```py\ncost = tf.reduce_mean(tf.nn.OPERATION_NAME(labels=<actual value>, logits=<predicted value>))\nupdates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)\n```", "```py\nsigmoid_cross_entropy_with_logits(\n  _sentinel=None,\n  labels=None,\n  logits=None,\n  name=None\n)Formula implemented is max(x, 0) - x * z + log(1 + exp(-abs(x)))\n```", "```py\nsoftmax = exp(logits) / reduce_sum(exp(logits), dim)\n```", "```py\nlog_softmax(\n logits,\n dim=-1,\n name=None\n)\n```", "```py\nsoftmax_cross_entropy_with_logits(\n  _sentinel=None,\n  labels=None,\n  logits=None,\n  dim=-1,\n  name=None\n)\n```", "```py\nsoftmax cross entropy between logits and labels. While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. For exclusive labels, use (where one and only one class is true at a time) sparse_softmax_cross_entropy_with_logits.\n```", "```py\nsparse_softmax_cross_entropy_with_logits(\n  _sentinel=None,\n  labels=None,\n  logits=None,\n  name=None\n)\n```", "```py\nweighted_cross_entropy_with_logits(\n  targets,\n  logits,\n  pos_weight,\n  name=None\n)\n```", "```py\ndef run(h_size, stddev, sgd_step)\n```", "```py\ndef load_iris_data():\n    from numpy import genfromtxt\n    data = genfromtxt('iris.csv', delimiter=',')\n    target = genfromtxt('target.csv', delimiter=',').astype(int)\n    # Prepend the column of 1s for bias\n    L, W  = data.shape\n    all_X = np.ones((L, W + 1))\n    all_X[:, 1:] = data\n    num_labels = len(np.unique(target))\n    all_y = np.eye(num_labels)[target]\n    return train_test_split(all_X, all_y, test_size=0.33, random_state=RANDOMSEED)\n```", "```py\n# Size of Layers\nx_size = train_x.shape[1] # Input nodes: 4 features and 1 bias\ny_size = train_y.shape[1] # Outcomes (3 iris flowers)\n# variables\nX = tf.placeholder(\"float\", shape=[None, x_size])\ny = tf.placeholder(\"float\", shape=[None, y_size])\nweights_1 = initialize_weights((x_size, h_size), stddev)\nweights_2 = initialize_weights((h_size, y_size), stddev)\n```", "```py\ndef forward_propagation(X, weights_1, weights_2):\n    sigmoid = tf.nn.sigmoid(tf.matmul(X, weights_1))\n    y = tf.matmul(sigmoid, weights_2)\n    return y\n```", "```py\ny_pred = forward_propagation(X, weights_1, weights_2)\npredict = tf.argmax(y_pred, dimension=1)\n```", "```py\n# constructor for GradientDescentOptimizer\n__init__(\n  learning_rate,\n  use_locking=False,\n  name='GradientDescent'\n)\n```", "```py\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))\nupdates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)\n```", "```py\nsess = tf.Session()\n```", "```py\n    init = tf.initialize_all_variables()\n    steps = 50\n    sess.run(init)\n    x  = np.arange(steps)\n    test_acc = []\n    train_acc = []\n    print(\"Step, train accuracy, test accuracy\")\n    for step in range(steps):\n        # Train with each example\n        for i in range(len(train_x)):\n            sess.run(updates_sgd, feed_dict={X: train_x[i: i + 1], y: train_y[i: i + 1]})\n\n        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n                                 sess.run(predict, feed_dict={X: train_x, y: train_y}))\n        test_accuracy = np.mean(np.argmax(test_y, axis=1) ==\n                                sess.run(predict, feed_dict={X: test_x, y: test_y}))\n\n        print(\"%d, %.2f%%, %.2f%%\"\n              % (step + 1, 100\\. * train_accuracy, 100\\. * test_accuracy))\n\n        test_acc.append(100\\. * test_accuracy)\n        train_acc.append(100\\. * train_accuracy)\n```", "```py\ndef run(h_size, stddev, sgd_step):\n ...\n\ndef main():\nrun(128,0.1,0.01)\n\nif __name__ == '__main__':\nmain()\n```", "```py\ndef run(h_size, stddev, sgd_steps):\n    ....\n    test_accs = []\n    train_accs = []\n    time_taken_summary = []\n    for sgd_step in sgd_steps:\n        start_time = time.time()\n        updates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)\n        sess = tf.Session()\n        init = tf.initialize_all_variables()\n        steps = 50\n        sess.run(init)\n        x  = np.arange(steps)\n        test_acc = []\n        train_acc = []\n\n        print(\"Step, train accuracy, test accuracy\")\n\n        for step in range(steps):\n                # Train with each example\n                for i in range(len(train_x)):\n                    sess.run(updates_sgd, feed_dict={X: train_x[i: i + 1], \n                              y: train_y[i: i + 1]})\n\n                train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n                                         sess.run(predict, \n                                         feed_dict={X: train_x, y: train_y}))\n                test_accuracy = np.mean(np.argmax(test_y, axis=1) ==\n                                        sess.run(predict, \n                                        feed_dict={X: test_x, y: test_y}))\n\n                print(\"%d, %.2f%%, %.2f%%\"\n                      % (step + 1, 100\\. * train_accuracy, 100\\. * test_accuracy))\n                #x.append(step)\n                test_acc.append(100\\. * test_accuracy)\n                train_acc.append(100\\. * train_accuracy)\n        end_time = time.time()\n        diff = end_time -start_time\n        time_taken_summary.append((sgd_step,diff))\n        t = [np.array(test_acc)]\n        t.append(train_acc)\n        train_accs.append(train_acc)\n```", "```py\ndef main():\n    sgd_steps = [0.01,0.02,0.03]\n    run(128,0.1,sgd_steps)\n\nif __name__ == '__main__':\n    main()\n```", "```py\ntest_folders = ['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D',\n'./notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', \n'./notMNIST_small/I', './notMNIST_small/J']\ntrain_folders = ['./notMNIST_large_v2/A', './notMNIST_large_v2/B', './notMNIST_large_v2/C', './notMNIST_large_v2/D',\n'./notMNIST_large_v2/E', './notMNIST_large_v2/F', './notMNIST_large_v2/G', './notMNIST_large_v2/H',\n'./notMNIST_large_v2/I', './notMNIST_large_v2/J']\nmaybe_pickle(data_folders, min_num_images_per_class, force=False):\n```", "```py\ndef load_letter(folder, min_num_images):\n  image_files = os.listdir(folder)\n  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n                         dtype=np.float32)\n  num_images = 0\n  for image in image_files:\n    image_file = os.path.join(folder, image)\n    try:\n      image_data = (ndimage.imread(image_file).astype(float) - \n                    pixel_depth / 2) / pixel_depth\n      if image_data.shape != (image_size, image_size):\n        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n      dataset[num_images, :, :] = image_data\n      num_images = num_images + 1\n    except IOError as e:\n      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n\n  dataset = dataset[0:num_images, :, :]\n  if num_images < min_num_images:\n    raise Exception('Fewer images than expected: %d < %d' %\n                    (num_images, min_num_images))\n\n  print('Dataset tensor:', dataset.shape)\n  print('Mean:', np.mean(dataset))\n  print('Standard deviation:', np.std(dataset))\n  return dataset\n```", "```py\ntrain_datasets = maybe_pickle(train_folders, 100)\ntest_datasets = maybe_pickle(test_folders, 50)\n```", "```py\ntrain_size = 1000\nvalid_size = 500\ntest_size = 500\n\nvalid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n    train_datasets, train_size, valid_size)\n  _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n```", "```py\nTraining dataset and labels shape: (1000, 28, 28) (1000,)\nValidation dataset and labels shape: (500, 28, 28) (500,)\nTesting dataset and labels shape: (500, 28, 28) (500,)\n```", "```py\n\n  try:\n    f = open(pickle_file, 'wb')\n    save = {\n      'train_dataset': train_dataset,\n      'train_labels': train_labels,\n      'valid_dataset': valid_dataset,\n      'valid_labels': valid_labels,\n      'test_dataset': test_dataset,\n      'test_labels': test_labels,\n    }\n    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n    f.close()\n  except Exception as e:\n    print('Unable to save data to', pickle_file, ':', e)\n    raise\n```", "```py\nfrom __future__ import print_function\nimport numpy as np\nimport os\nfrom scipy import ndimage\nfrom six.moves import cPickle as pickle\n\ndata_root = '.' # Change me to store data elsewhere\n\nnum_classes = 10\nnp.random.seed(133)\n\ntest_folders = ['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D',\n                './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', \n                './notMNIST_small/I', './notMNIST_small/J']\ntrain_folders = ['./notMNIST_large_v2/A', './notMNIST_large_v2/B', './notMNIST_large_v2/C', './notMNIST_large_v2/D',\n                 './notMNIST_large_v2/E', './notMNIST_large_v2/F', './notMNIST_large_v2/G', './notMNIST_large_v2/H',\n                 './notMNIST_large_v2/I', './notMNIST_large_v2/J']\n\nimage_size = 28  # Pixel width and height.\npixel_depth = 255.0\n\ndef load_letter(folder, min_num_images):\n  image_files = os.listdir(folder)\n  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n                         dtype=np.float32)\n  num_images = 0\n  for image in image_files:\n    image_file = os.path.join(folder, image)\n    try:\n      image_data = (ndimage.imread(image_file).astype(float) - \n                    pixel_depth / 2) / pixel_depth\n      if image_data.shape != (image_size, image_size):\n        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n      dataset[num_images, :, :] = image_data\n      num_images = num_images + 1\n    except IOError as e:\n      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n\n  dataset = dataset[0:num_images, :, :]\n  if num_images < min_num_images:\n    raise Exception('Fewer images than expected: %d < %d' %\n                    (num_images, min_num_images))\n\n  print('Dataset tensor:', dataset.shape)\n  print('Mean:', np.mean(dataset))\n  print('Standard deviation:', np.std(dataset))\n  return dataset\n\ndef maybe_pickle(data_folders, min_num_images_per_class, force=False):\n  dataset_names = []\n  for folder in data_folders:\n    set_filename = folder + '.pickle'\n    dataset_names.append(set_filename)\n    if os.path.exists(set_filename) and not force:\n      print('%s already present - Skipping pickling.' % set_filename)\n    else:\n      print('Pickling %s.' % set_filename)\n      dataset = load_letter(folder, min_num_images_per_class)\n      try:\n        with open(set_filename, 'wb') as f:\n          #pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n          print(pickle.HIGHEST_PROTOCOL)\n          pickle.dump(dataset, f, 2)\n      except Exception as e:\n        print('Unable to save data to', set_filename, ':', e)\n\n  return dataset_names\n\ndef make_arrays(nb_rows, img_size):\n  if nb_rows:\n    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n    labels = np.ndarray(nb_rows, dtype=np.int32)\n  else:\n    dataset, labels = None, None\n```", "```py\nwith open(pickle_file, 'rb') as f:\n save = pickle.load(f)\n training_dataset = save['train_dataset']\n training_labels = save['train_labels']\n validation_dataset = save['valid_dataset']\n validation_labels = save['valid_labels']\n test_dataset = save['test_dataset']\n test_labels = save['test_labels']\n\nprint 'Training set', training_dataset.shape, training_labels.shape\nprint 'Validation set', validation_dataset.shape, validation_labels.shape\nprint 'Test set', test_dataset.shape, test_labels.shape\n```", "```py\nTraining set (1000, 28, 28) (1000,)\nValidation set (500, 28, 28) (500,)\nTest set (500, 28, 28) (500,)\n```", "```py\ndef reformat(dataset, labels):\n dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n labels = (np.arange(num_of_labels) == labels[:, None]).astype(np.float32)\n return dataset, labels\ntrain_dataset, train_labels = reformat(training_dataset, training_labels)\n valid_dataset, valid_labels = reformat(validation_dataset, validation_labels)\n test_dataset, test_labels = reformat(test_dataset, test_labels)\n\n print 'Training dataset shape', train_dataset.shape, train_labels.shape\n print 'Validation dataset shape', valid_dataset.shape, valid_labels.shape\n print 'Test dataset shape', test_dataset.shape, test_labels.shape\n```", "```py\nTraining dataset shape (1000, 784) (1000, 10)\nValidation dataset shape (500, 784) (500, 10)\nTest dataset shape (500, 784) (500, 10)\n```", "```py\ngraph = tf.Graph()\n no_of_neurons = 1024\n with graph.as_default():\n   # Placeholder that will be fed\n   # at run time with a training minibatch in the session\n   tf_train_dataset = tf.placeholder(tf.float32,\n     shape=(batch_size, image_size * image_size))\n   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_of_labels))\n   tf_valid_dataset = tf.constant(valid_dataset)\n   tf_test_dataset = tf.constant(test_dataset)\n\n   # Variables.\n   w1 = tf.Variable(tf.truncated_normal([image_size * image_size, no_of_neurons]))\n   b1 = tf.Variable(tf.zeros([no_of_neurons]))\n\n   w2 = tf.Variable(\n   tf.truncated_normal([no_of_neurons, num_of_labels]))\n   b2 = tf.Variable(tf.zeros([num_of_labels]))\n```", "```py\nhidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\nlogits = tf.matmul(hidden1, w2) + b2\n```", "```py\nloss = tf.reduce_mean(\n     tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n# Training computation.\n\nloss = tf.reduce_mean(\n     tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n   # Optimizer.\n   optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n```", "```py\ntrain_prediction = tf.nn.softmax(logits)\n```", "```py\ntf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)\n valid_prediction = tf.nn.softmax(\n     tf.matmul( tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), \n                w2\n              ) + b2)\n test_prediction = tf.nn.softmax(\n     tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)\n```", "```py\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print(\"Initialized\")\n  for step in xrange(num_steps):\n   offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n   # Generate a minibatch.\n   batch_data = train_dataset[offset:(offset + batch_size), :]\n   batch_labels = train_labels[offset:(offset + batch_size), :]\n   feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n   _, l, predictions = session.run(\n     [optimizer, loss, train_prediction], feed_dict=feed_dict)\n   minibatch_accuracy = accuracy(predictions, batch_labels)\n   validation_accuracy = accuracy(\n    valid_prediction.eval(), valid_labels)\n   if (step % 10 == 0):\n     print(\"Minibatch loss at step\", step, \":\", l)\n     print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n     print(\"Validation accuracy: %.1f%%\" % validation_accuracy)\nminibatch_acc.append( minibatch_accuracy)\nvalidation_acc.append( validation_accuracy)\nt = [np.array(minibatch_acc)]\nt.append(validation_acc)\n```", "```py\n print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n title = \"NotMNIST DataSet - Single Hidden Layer - 1024 neurons Activation function: RELU\"\n label = ['Minibatch Accuracy', 'Validation Accuracy']\n draw_plot(x, t, title, label)\n\n```", "```py\nx_val = np.linspace(start=-10., stop=10., num=1000)\n```", "```py\n # ReLU activation\n  y_relu = session.run(tf.nn.relu(x_val))\n  # ReLU-6 activation\n  y_relu6 = session.run(tf.nn.relu6(x_val))\n  # Sigmoid activation\n  y_sigmoid = session.run(tf.nn.sigmoid(x_val))\n  # Hyper Tangent activation\n  y_tanh = session.run(tf.nn.tanh(x_val))\n  # Softsign activation\n  y_softsign = session.run(tf.nn.softsign(x_val))\n\n  # Softplus activation\n  y_softplus = session.run(tf.nn.softplus(x_val))\n  # Exponential linear activation\n  y_elu = session.run(tf.nn.elu(x_val))\n```", "```py\nplt.plot(x_val, y_softplus, 'r--', label='Softplus', linewidth=2)\nplt.plot(x_val, y_relu, 'b:', label='RELU', linewidth=2)\nplt.plot(x_val, y_relu6, 'g-.', label='RELU6', linewidth=2)\nplt.plot(x_val, y_elu, 'k-', label='ELU', linewidth=1)\nplt.ylim([-1.5,7])\nplt.legend(loc='top left')\nplt.title('Activation functions', y=1.05)\nplt.show()\nplt.plot(x_val, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)\nplt.plot(x_val, y_tanh, 'b:', label='tanh', linewidth=2)\nplt.plot(x_val, y_softsign, 'g-.', label='Softsign', linewidth=2)\nplt.ylim([-1.5,1.5])\nplt.legend(loc='top left')\nplt.title('Activation functions with Vanishing Gradient', y=1.05)\nplt.show()\n```", "```py\nRELU = 'RELU'\nRELU6 = 'RELU6'\nCRELU = 'CRELU'\nSIGMOID = 'SIGMOID'\nELU = 'ELU'\nSOFTPLUS = 'SOFTPLUS'\ndef activation(name, features):\n  if name == RELU:\n    return tf.nn.relu(features)\n  if name == RELU6:\n    return tf.nn.relu6(features)\n  if name == SIGMOID:\n    return tf.nn.sigmoid(features)\n  if name == CRELU:\n    return tf.nn.crelu(features)\n  if name == ELU:\n    return tf.nn.elu(features)\n  if name == SOFTPLUS:\n    return tf.nn.softplus(features)\n```", "```py\nbatch_size = 128\n#activations = [RELU, RELU6, SIGMOID, CRELU, ELU, SOFTPLUS]\nactivations = [RELU, RELU6, SIGMOID, ELU, SOFTPLUS]\nplot_loss = False\ndef run(name):\n print(name)\n with open(pickle_file, 'rb') as f:\n   save = pickle.load(f)\n   training_dataset = save['train_dataset']\n   training_labels = save['train_labels']\n   validation_dataset = save['valid_dataset']\n   validation_labels = save['valid_labels']\n   test_dataset = save['test_dataset']\n   test_labels = save['test_labels'] \n train_dataset, train_labels = reformat(training_dataset, training_labels)\n valid_dataset, valid_labels = reformat(validation_dataset, \n    validation_labels)\n test_dataset, test_labels = reformat(test_dataset, test_labels)\n\n graph = tf.Graph()\n no_of_neurons = 1024\n with graph.as_default():\n\n tf_train_dataset = tf.placeholder(tf.float32,\n shape=(batch_size, image_size * image_size))\n tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, \n   num_of_labels))\n tf_valid_dataset = tf.constant(valid_dataset)\n tf_test_dataset = tf.constant(test_dataset)\n # Define Variables.\n # Training computation...\n # Optimizer ..\n # Predictions for the training, validation, and test data.\n train_prediction = tf.nn.softmax(logits)\n valid_prediction = tf.nn.softmax(\n tf.matmul(activation(name,tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2)\n test_prediction = tf.nn.softmax(\n tf.matmul(activation(name,tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)\n\n num_steps = 101\n minibatch_acc = []\n validation_acc = []\n loss_array = []\n with tf.Session(graph=graph) as session:\n   tf.initialize_all_variables().run()\n   print(\"Initialized\")\n   for step in xrange(num_steps):\n     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n     # Generate a minibatch.\n     batch_data = train_dataset[offset:(offset + batch_size), :]\n     batch_labels = train_labels[offset:(offset + batch_size), :]\n     feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n\n     _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n     minibatch_accuracy = accuracy(predictions, batch_labels)\n     validation_accuracy = accuracy(\n      valid_prediction.eval(), valid_labels)\n     if (step % 10 == 0):\n       print(\"Minibatch loss at step\", step, \":\", l)\n       print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions,\n         batch_labels))\n       print(\"Validation accuracy: %.1f%%\" % accuracy(\n     valid_prediction.eval(), valid_labels))\n     minibatch_acc.append(minibatch_accuracy)\n     validation_acc.append(validation_accuracy)\n     loss_array.append(l)\n     print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(),\n       test_labels))\n     return validation_acc, loss_array\n```"]