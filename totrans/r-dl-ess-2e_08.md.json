["```py\nlibrary(tensorflow)\n\n> # tensor of rank-0\n> var1 <- tf$constant(0.1)\n> print(var1)\nTensor(\"Const:0\", shape=(), dtype=float32)\n\n> sess <- tf$InteractiveSession()\nT:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3019 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\n> sess$run(tf$global_variables_initializer())\n> var2 <- tf$constant(2.3)\n> var3 = var1 + var2\n> print(var1)\nTensor(\"Const:0\", shape=(), dtype=float32)\n num 0.1\n\n> print(var2)\nTensor(\"Const_1:0\", shape=(), dtype=float32)\n num 2.3\n\n> print(var3)\nTensor(\"Add:0\", shape=(), dtype=float32)\n num 2.4\n\n> # tensor of rank-1\n> var4 <- tf$constant(4.5,shape=shape(5L))\n> print(var4)\nTensor(\"Const_2:0\", shape=(5,), dtype=float32)\n num [1:5(1d)] 4.5 4.5 4.5 4.5 4.5\n\n> # tensor of rank-2\n> var5 <- tf$constant(6.7,shape=shape(3L,3L))\n> print(var5)\nTensor(\"Const_3:0\", shape=(3, 3), dtype=float32)\n num [1:3, 1:3] 6.7 6.7 6.7 6.7 6.7 ...\n```", "```py\nlibrary(keras)\n\nmnist_data <- dataset_mnist()\nxtrain <- array_reshape(mnist_data$train$x,c(nrow(mnist_data$train$x),28,28,1))\nytrain <- to_categorical(mnist_data$train$y,10)\nxtrain <- xtrain / 255.0\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filters=32,kernel_size=c(5,5),activation='relu',\n                input_shape=c(28,28,1)) %>% \n  layer_max_pooling_2d(pool_size=c(2,2)) %>% \n  layer_dropout(rate=0.25) %>% \n  layer_conv_2d(filters=32,kernel_size=c(5,5),activation='relu') %>% \n  layer_max_pooling_2d(pool_size=c(2,2)) %>% \n  layer_dropout(rate=0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units=256,activation='relu') %>% \n  layer_dropout(rate=0.4) %>% \n  layer_dense(units=10,activation='softmax')\n\nmodel %>% compile(\n  loss=loss_categorical_crossentropy,\n  optimizer=\"rmsprop\",metrics=\"accuracy\"\n)\n```", "```py\nmodel %>% fit(\n  xtrain,ytrain,\n  batch_size=128,epochs=10,\n callbacks=callback_tensorboard(\"/tensorflow_logs\",\n histogram_freq=1,write_images=0),\n  validation_split=0.2\n)\n# from cmd line,run 'tensorboard --logdir /tensorflow_logs'\n```", "```py\n$ tensorboard --logdir /tensorflow_logs\n```", "```py\nTensorBoard 0.4.0rc2 at http://xxxxxx:6006 (Press CTRL+C to quit)\n```", "```py\n> library(tensorflow)\n\n# confirm that TensorFlow library has loaded\n> sess=tf$Session()\n> hello_world <- tf$constant('Hello world from TensorFlow')\n> sess$run(hello_world)\nb'Hello world from TensorFlow'\n```", "```py\nlibrary(tensorflow)\n\nset.seed(42)\n# create 50000 x variable between 0 and 100\nx_var <- runif(50000,min=0,max=1)\n#y = approx(1.3x + 0.8)\ny_var <- rnorm(50000,0.8,0.04) + x_var * rnorm(50000,1.3,0.05)\n\n# y_pred = beta0 + beta1 * x\nbeta0 <- tf$Variable(tf$zeros(shape(1L)))\nbeta1 <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))\ny_pred <- beta0 + beta1*x_var\n```", "```py\n# create our loss value which we want to minimize\nloss <- tf$reduce_mean((y_pred-y_var)^2)\n# create optimizer\noptimizer <- tf$train$GradientDescentOptimizer(0.6)\ntrain <- optimizer$minimize(loss)\n```", "```py\n# create TensorFlow session and initialize variables\nsess = tf$Session()\nsess$run(tf$global_variables_initializer())\n\n# solve the regression\nfor (step in 0:80) {\n  if (step %% 10 == 0)\n    print(sprintf(\"Step %1.0f:beta0=%1.4f, beta1=%1.4f\",step,sess$run(beta0), sess$run(beta1)))\n  sess$run(train)\n}\n[1] \"Step 0:beta0=0.0000, beta1=-0.3244\"\n[1] \"Step 10:beta0=1.0146, beta1=0.8944\"\n[1] \"Step 20:beta0=0.8942, beta1=1.1236\"\n[1] \"Step 30:beta0=0.8410, beta1=1.2229\"\n[1] \"Step 40:beta0=0.8178, beta1=1.2662\"\n[1] \"Step 50:beta0=0.8077, beta1=1.2850\"\n[1] \"Step 60:beta0=0.8033, beta1=1.2932\"\n[1] \"Step 70:beta0=0.8014, beta1=1.2967\"\n[1] \"Step 80:beta0=0.8006, beta1=1.2983\"\n```", "```py\nlibrary(RSNNS) # for decodeClassLabels\nlibrary(tensorflow)\nlibrary(keras)\n\nmnist <- dataset_mnist()\nset.seed(42)\n\nxtrain <- array_reshape(mnist$train$x,c(nrow(mnist$train$x),28*28))\nytrain <- decodeClassLabels(mnist$train$y)\nxtest <- array_reshape(mnist$test$x,c(nrow(mnist$test$x),28*28))\nytest <- decodeClassLabels(mnist$test$y)\nxtrain <- xtrain / 255.0\nxtest <- xtest / 255.0\nhead(ytrain)\n     0 1 2 3 4 5 6 7 8 9\n[1,] 0 0 0 0 0 1 0 0 0 0\n[2,] 1 0 0 0 0 0 0 0 0 0\n[3,] 0 0 0 0 1 0 0 0 0 0\n[4,] 0 1 0 0 0 0 0 0 0 0\n[5,] 0 0 0 0 0 0 0 0 0 1\n[6,] 0 0 1 0 0 0 0 0 0 0\n```", "```py\n# placeholders\nx <- tf$placeholder(tf$float32, shape(NULL,28L*28L))\ny <- tf$placeholder(tf$float32, shape(NULL,10L))\nx_image <- tf$reshape(x, shape(-1L,28L,28L,1L))\n```", "```py\n# first convolution layer\nconv_weights1 <- tf$Variable(tf$random_uniform(shape(5L,5L,1L,16L), -0.4, 0.4))\nconv_bias1 <- tf$constant(0.0, shape=shape(16L))\nconv_activ1 <- tf$nn$tanh(tf$nn$conv2d(x_image, conv_weights1, strides=c(1L,1L,1L,1L), padding='SAME') + conv_bias1)\npool1 <- tf$nn$max_pool(conv_activ1, ksize=c(1L,2L,2L,1L),strides=c(1L,2L,2L,1L), padding='SAME')\n\n# second convolution layer\nconv_weights2 <- tf$Variable(tf$random_uniform(shape(5L,5L,16L,32L), -0.4, 0.4))\nconv_bias2 <- tf$constant(0.0, shape=shape(32L))\nconv_activ2 <- tf$nn$relu(tf$nn$conv2d(pool1, conv_weights2, strides=c(1L,1L,1L,1L), padding='SAME') + conv_bias2)\npool2 <- tf$nn$max_pool(conv_activ2, ksize=c(1L,2L,2L,1L),strides=c(1L,2L,2L,1L), padding='SAME')\n\n# densely connected layer\ndense_weights1 <- tf$Variable(tf$truncated_normal(shape(7L*7L*32L,512L), stddev=0.1))\ndense_bias1 <- tf$constant(0.0, shape=shape(512L))\npool2_flat <- tf$reshape(pool2, shape(-1L,7L*7L*32L))\ndense1 <- tf$nn$relu(tf$matmul(pool2_flat, dense_weights1) + dense_bias1)\n\n# dropout\nkeep_prob <- tf$placeholder(tf$float32)\ndense1_drop <- tf$nn$dropout(dense1, keep_prob)\n\n# softmax layer\ndense_weights2 <- tf$Variable(tf$truncated_normal(shape(512L,10L), stddev=0.1))\ndense_bias2 <- tf$constant(0.0, shape=shape(10L))\n\nyconv <- tf$nn$softmax(tf$matmul(dense1_drop, dense_weights2) + dense_bias2)\n```", "```py\ncross_entropy <- tf$reduce_mean(-tf$reduce_sum(y * tf$log(yconv), reduction_indices=1L))\ntrain_step <- tf$train$AdamOptimizer(0.0001)$minimize(cross_entropy)\ncorrect_prediction <- tf$equal(tf$argmax(yconv, 1L), tf$argmax(y, 1L))\naccuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\n```", "```py\nsess <- tf$InteractiveSession()\nsess$run(tf$global_variables_initializer())\n\n# if you get out of memory errors when running on gpu\n# then lower the batch_size\nbatch_size <- 128\nbatches_per_epoch <- 1+nrow(xtrain) %/% batch_size\nfor (epoch in 1:10)\n{\n  for (batch_no in 0:(-1+batches_per_epoch))\n  {\n    nStartIndex <- 1 + batch_no*batch_size\n    nEndIndex <- nStartIndex + batch_size-1\n    if (nEndIndex > nrow(xtrain))\n      nEndIndex <- nrow(xtrain)\n    xvalues <- xtrain[nStartIndex:nEndIndex,]\n    yvalues <- ytrain[nStartIndex:nEndIndex,]\n    if (batch_no %% 100 == 0) {\n      batch_acc <- accuracy$eval(feed_dict=dict(x=xvalues,y=yvalues,keep_prob=1.0))\n      print(sprintf(\"Epoch %1.0f, step %1.0f: training accuracy=%1.4f\",epoch, batch_no, batch_acc))\n    }\n    sess$run(train_step,feed_dict=dict(x=xvalues,y=yvalues,keep_prob=0.5))\n  }\n  cat(\"\\n\")\n}\n```", "```py\n[1] \"Epoch 1, step 0: training accuracy=0.0625\"\n[1] \"Epoch 1, step 100: training accuracy=0.8438\"\n[1] \"Epoch 1, step 200: training accuracy=0.8984\"\n[1] \"Epoch 1, step 300: training accuracy=0.9531\"\n[1] \"Epoch 1, step 400: training accuracy=0.8750\"\n```", "```py\n# calculate test accuracy\n# have to run in batches to prevent out of memory errors\nbatches_per_epoch <- 1+nrow(xtest) %/% batch_size\ntest_acc <- vector(mode=\"numeric\", length=batches_per_epoch)\nfor (batch_no in 0:(-1+batches_per_epoch))\n{\n  nStartIndex <- 1 + batch_no*batch_size\n  nEndIndex <- nStartIndex + batch_size-1\n  if (nEndIndex > nrow(xtest))\n    nEndIndex <- nrow(xtest)\n  xvalues <- xtest[nStartIndex:nEndIndex,]\n  yvalues <- ytest[nStartIndex:nEndIndex,]\n  batch_acc <- accuracy$eval(feed_dict=dict(x=xvalues,y=yvalues,keep_prob=1.0))\n  test_acc[batch_no+1] <- batch_acc\n}\n# using the mean is not totally accurate as last batch is not a complete batch\nprint(sprintf(\"Test accuracy=%1.4f\",mean(test_acc)))\n[1] \"Test accuracy=0.9802\"\n```", "```py\nresponse <- function() \"Y_categ\"\nfeatures <- function() predictorCols\n\nFLAGS <- flags(\n  flag_numeric(\"layer1\", 256),\n  flag_numeric(\"layer2\", 128),\n  flag_numeric(\"layer3\", 64),\n  flag_numeric(\"layer4\", 32),\n  flag_numeric(\"dropout\", 0.2)\n)\nnum_hidden <- c(FLAGS$layer1,FLAGS$layer2,FLAGS$layer3,FLAGS$layer4)\n\nclassifier <- dnn_classifier(\n  feature_columns = feature_columns(column_numeric(predictorCols)),\n  hidden_units = num_hidden,\n  activation_fn = \"relu\",\n  dropout = FLAGS$dropout,\n  n_classes = 2\n)\n\nbin_input_fn <- function(data)\n{\n input_fn(data, features = features(), response = response())\n}\ntr <- train(classifier, input_fn = bin_input_fn(trainData))\n[\\] Training -- loss: 22.96, step: 2742 \n\ntr\nTrained for 2,740 steps. \nFinal step (plot to see history):\n mean_losses: 61.91\ntotal_losses: 61.91\n```", "```py\nplot(tr)\n```", "```py\n# predictions <- predict(classifier, input_fn = bin_input_fn(testData))\nevaluation <- evaluate(classifier, input_fn = bin_input_fn(testData))\n[-] Evaluating -- loss: 37.77, step: 305\n\nfor (c in 1:ncol(evaluation))\n print(paste(colnames(evaluation)[c],\" = \",evaluation[c],sep=\"\"))\n[1] \"accuracy = 0.77573162317276\"\n[1] \"accuracy_baseline = 0.603221416473389\"\n[1] \"auc = 0.842994153499603\"\n[1] \"auc_precision_recall = 0.887594640254974\"\n[1] \"average_loss = 0.501933991909027\"\n[1] \"label/mean = 0.603221416473389\"\n[1] \"loss = 64.1636199951172\"\n[1] \"precision = 0.803375601768494\"\n[1] \"prediction/mean = 0.562777876853943\"\n[1] \"recall = 0.831795573234558\"\n[1] \"global_step = 2742\"\n```", "```py\nmodel_dir(classifier)\n\"C:\\\\Users\\\\xxxxxx\\\\AppData\\\\Local\\\\Temp\\\\tmpv1e_ri23\"\n# dnn_classifier has a model_dir parameter to load an existing model\n?dnn_classifier\n```", "```py\nlibrary(tfruns)\n# FLAGS <- flags(\n# flag_numeric(\"layer1\", 256),\n# flag_numeric(\"layer2\", 128),\n# flag_numeric(\"layer3\", 64),\n# flag_numeric(\"layer4\", 32),\n# flag_numeric(\"dropout\", 0.2),\n# flag_string(\"activ\",\"relu\")\n# )\n\ntraining_run('tf_estimators.R')\ntraining_run('tf_estimators.R', flags = list(layer1=128,layer2=64,layer3=32,layer4=16))\ntraining_run('tf_estimators.R', flags = list(dropout=0.1,activ=\"tanh\"))\n```", "```py\nls_runs(order=eval_accuracy)\nls_runs(order=eval_accuracy)[,1:5]\nData frame: 3 x 5 \n                    run_dir eval_accuracy eval_accuracy_baseline eval_auc eval_auc_precision_recall\n3 runs/2018-08-02T19-50-17Z        0.7746                 0.6032   0.8431                    0.8874\n2 runs/2018-08-02T19-52-04Z        0.7724                 0.6032   0.8425                    0.8873\n1 runs/2018-08-02T19-53-39Z        0.7711                 0.6032   0.8360                    0.8878\n```", "```py\ndir1 <- ls_runs(order=eval_accuracy)[1,1]\nview_run(dir1)\n```", "```py\ndir1 <- ls_runs(order=eval_accuracy)[1,1]\ndir2 <- ls_runs(order=eval_accuracy)[2,1]\ncompare_runs(runs=c(dir1,dir2))\n```"]