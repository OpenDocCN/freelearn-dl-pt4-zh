["```py\npip install gym==0.25.1\n```", "```py\nimport gymnasium as gym \n\nENV_ID = \"MinitaurBulletEnv-v0\" \nENTRY = \"pybullet_envs.bullet.minitaur_gym_env:MinitaurBulletEnv\" \nRENDER = True \n\nif __name__ == \"__main__\": \n    gym.register(ENV_ID, entry_point=ENTRY, max_episode_steps=1000, \n                 reward_threshold=15.0, disable_env_checker=True) \n    env = gym.make(ENV_ID, render=RENDER) \n\n    print(\"Observation space:\", env.observation_space) \n    print(\"Action space:\", env.action_space) \n    print(env) \n    print(env.reset()) \n    input(\"Press any key to exit\\n\") \n    env.close()\n```", "```py\nHID_SIZE = 128 \n\nclass ModelA2C(nn.Module): \n    def __init__(self, obs_size: int, act_size: int): \n        super(ModelA2C, self).__init__() \n\n        self.base = nn.Sequential( \n            nn.Linear(obs_size, HID_SIZE), \n            nn.ReLU(), \n        ) \n        self.mu = nn.Sequential( \n            nn.Linear(HID_SIZE, act_size), \n            nn.Tanh(), \n        ) \n        self.var = nn.Sequential( \n            nn.Linear(HID_SIZE, act_size), \n            nn.Softplus(), \n        ) \n        self.value = nn.Linear(HID_SIZE, 1)\n```", "```py\n def forward(self, x: torch.Tensor): \n        base_out = self.base(x) \n        return self.mu(base_out), self.var(base_out), self.value(base_out)\n```", "```py\nclass AgentA2C(ptan.agent.BaseAgent): \n    def __init__(self, net: ModelA2C, device: torch.device): \n        self.net = net \n        self.device = device \n\n    def __call__(self, states: ptan.agent.States, agent_states: ptan.agent.AgentStates): \n        states_v = ptan.agent.float32_preprocessor(states) \n        states_v = states_v.to(self.device) \n\n        mu_v, var_v, _ = self.net(states_v) \n        mu = mu_v.data.cpu().numpy() \n        sigma = torch.sqrt(var_v).data.cpu().numpy() \n        actions = np.random.normal(mu, sigma) \n        actions = np.clip(actions, -1, 1) \n        return actions, agent_states\n```", "```py\ndef test_net(net: model.ModelA2C, env: gym.Env, count: int = 10, \n             device: torch.device = torch.device(\"cpu\")): \n    rewards = 0.0 \n    steps = 0 \n    for _ in range(count): \n        obs, _ = env.reset() \n        while True: \n            obs_v = ptan.agent.float32_preprocessor([obs]) \n            obs_v = obs_v.to(device) \n            mu_v = net(obs_v)[0] \n            action = mu_v.squeeze(dim=0).data.cpu().numpy() \n            action = np.clip(action, -1, 1) \n            obs, reward, done, is_tr, _ = env.step(action) \n            rewards += reward \n            steps += 1 \n            if done or is_tr: \n                break \n    return rewards / count, steps / count\n```", "```py\ndef calc_logprob(mu_v: torch.Tensor, var_v: torch.Tensor, actions_v: torch.Tensor): \n    p1 = - ((mu_v - actions_v) ** 2) / (2*var_v.clamp(min=1e-3)) \n    p2 = - torch.log(torch.sqrt(2 * math.pi * var_v)) \n    return p1 + p2\n```", "```py\nGAMMA = 0.99 \nREWARD_STEPS = 2 \nBATCH_SIZE = 32 \nLEARNING_RATE = 5e-5 \nENTROPY_BETA = 1e-4 \n\nTEST_ITERS = 1000\n```", "```py\n states_v, actions_v, vals_ref_v = common.unpack_batch_a2c( \n                    batch, net, device=device, last_val_gamma=GAMMA ** REWARD_STEPS) \n                batch.clear() \n\n                optimizer.zero_grad() \n                mu_v, var_v, value_v = net(states_v) \n\n                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v) \n                adv_v = vals_ref_v.unsqueeze(dim=-1) - value_v.detach() \n                log_prob_v = adv_v * calc_logprob(mu_v, var_v, actions_v) \n                loss_policy_v = -log_prob_v.mean() \n                ent_v = -(torch.log(2*math.pi*var_v) + 1)/2 \n                entropy_loss_v = ENTROPY_BETA * ent_v.mean() \n\n                loss_v = loss_policy_v + entropy_loss_v + loss_value_v \n                loss_v.backward() \n                optimizer.step()\n```", "```py\nclass DDPGActor(nn.Module): \n    def __init__(self, obs_size: int, act_size: int): \n        super(DDPGActor, self).__init__() \n\n        self.net = nn.Sequential( \n            nn.Linear(obs_size, 400), \n            nn.ReLU(), \n            nn.Linear(400, 300), \n            nn.ReLU(), \n            nn.Linear(300, act_size), \n            nn.Tanh() \n        ) \n\n    def forward(self, x: torch.Tensor): \n        return self.net(x)\n```", "```py\nclass DDPGCritic(nn.Module): \n    def __init__(self, obs_size: int, act_size: int): \n        super(DDPGCritic, self).__init__() \n\n        self.obs_net = nn.Sequential( \n            nn.Linear(obs_size, 400), \n            nn.ReLU(), \n        ) \n\n        self.out_net = nn.Sequential( \n            nn.Linear(400 + act_size, 300), \n            nn.ReLU(), \n            nn.Linear(300, 1) \n        ) \n\n    def forward(self, x: torch.Tensor, a: torch.Tensor): \n        obs = self.obs_net(x) \n        return self.out_net(torch.cat([obs, a], dim=1))\n```", "```py\nclass AgentDDPG(ptan.agent.BaseAgent): \n    def __init__(self, net: DDPGActor, device: torch.device = torch.device(’cpu’), \n                 ou_enabled: bool = True, ou_mu: float = 0.0, ou_teta: float = 0.15, \n                 ou_sigma: float = 0.2, ou_epsilon: float = 1.0): \n        self.net = net \n        self.device = device \n        self.ou_enabled = ou_enabled \n        self.ou_mu = ou_mu \n        self.ou_teta = ou_teta \n        self.ou_sigma = ou_sigma \n        self.ou_epsilon = ou_epsilon \n\n    def initial_state(self): \n        return None\n```", "```py\n def __call__(self, states: ptan.agent.States, agent_states: ptan.agent.AgentStates): \n        states_v = ptan.agent.float32_preprocessor(states) \n        states_v = states_v.to(self.device) \n        mu_v = self.net(states_v) \n        actions = mu_v.data.cpu().numpy()\n```", "```py\n if self.ou_enabled and self.ou_epsilon > 0: \n            new_a_states = [] \n            for a_state, action in zip(agent_states, actions): \n                if a_state is None: \n                    a_state = np.zeros(shape=action.shape, dtype=np.float32) \n                a_state += self.ou_teta * (self.ou_mu - a_state) \n                a_state += self.ou_sigma * np.random.normal(size=action.shape) \n\n                action += self.ou_epsilon * a_state \n                new_a_states.append(a_state)\n```", "```py\n else: \n            new_a_states = agent_states \n        actions = np.clip(actions, -1, 1) \n        return actions, new_a_states\n```", "```py\n act_net = model.DDPGActor(env.observation_space.shape[0], \n                              env.action_space.shape[0]).to(device) \n    crt_net = model.DDPGCritic(env.observation_space.shape[0], \n                               env.action_space.shape[0]).to(device) \n    print(act_net) \n    print(crt_net) \n    tgt_act_net = ptan.agent.TargetNet(act_net) \n    tgt_crt_net = ptan.agent.TargetNet(crt_net) \n\n    writer = SummaryWriter(comment=\"-ddpg_\" + args.name) \n    agent = model.AgentDDPG(act_net, device=device) \n    exp_source = ptan.experience.ExperienceSourceFirstLast( \n        env, agent, gamma=GAMMA, steps_count=1) \n    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE) \n    act_opt = optim.Adam(act_net.parameters(), lr=LEARNING_RATE) \n    crt_opt = optim.Adam(crt_net.parameters(), lr=LEARNING_RATE)\n```", "```py\n batch = buffer.sample(BATCH_SIZE) \n                states_v, actions_v, rewards_v, dones_mask, last_states_v = \\ \n                    common.unpack_batch_ddqn(batch, device)\n```", "```py\n crt_opt.zero_grad() \n                q_v = crt_net(states_v, actions_v) \n                last_act_v = tgt_act_net.target_model(last_states_v) \n                q_last_v = tgt_crt_net.target_model(last_states_v, last_act_v) \n                q_last_v[dones_mask] = 0.0 \n                q_ref_v = rewards_v.unsqueeze(dim=-1) + q_last_v * GAMMA\n```", "```py\n critic_loss_v = F.mse_loss(q_v, q_ref_v.detach()) \n                critic_loss_v.backward() \n                crt_opt.step() \n                tb_tracker.track(\"loss_critic\", critic_loss_v, frame_idx) \n                tb_tracker.track(\"critic_ref\", q_ref_v.mean(), frame_idx)\n```", "```py\n act_opt.zero_grad() \n                cur_actions_v = act_net(states_v) \n                actor_loss_v = -crt_net(states_v, cur_actions_v) \n                actor_loss_v = actor_loss_v.mean()\n```", "```py\n actor_loss_v.backward() \n                act_opt.step() \n                tb_tracker.track(\"loss_actor\", actor_loss_v, frame_idx)\n```", "```py\n tgt_act_net.alpha_sync(alpha=1 - 1e-3) \n                tgt_crt_net.alpha_sync(alpha=1 - 1e-3)\n```", "```py\nclass D4PGCritic(nn.Module): \n    def __init__(self, obs_size: int, act_size: int, \n                 n_atoms: int, v_min: float, v_max: float): \n        super(D4PGCritic, self).__init__() \n\n        self.obs_net = nn.Sequential( \n            nn.Linear(obs_size, 400), \n            nn.ReLU(), \n        ) \n\n        self.out_net = nn.Sequential( \n            nn.Linear(400 + act_size, 300), \n            nn.ReLU(), \n            nn.Linear(300, n_atoms) \n        ) \n\n        delta = (v_max - v_min) / (n_atoms - 1) \n        self.register_buffer(\"supports\", torch.arange(v_min, v_max + delta, delta))\n```", "```py\n def forward(self, x: torch.Tensor, a: torch.Tensor): \n        obs = self.obs_net(x) \n        return self.out_net(torch.cat([obs, a], dim=1)) \n\n    def distr_to_q(self, distr: torch.Tensor): \n        weights = F.softmax(distr, dim=1) * self.supports \n        res = weights.sum(dim=1) \n        return res.unsqueeze(dim=-1)\n```", "```py\nclass AgentD4PG(ptan.agent.BaseAgent): \n    def __init__(self, net: DDPGActor, device: torch.device = torch.device(\"cpu\"), \n                 epsilon: float = 0.3): \n        self.net = net \n        self.device = device \n        self.epsilon = epsilon \n\n    def __call__(self, states: ptan.agent.States, agent_states: ptan.agent.AgentStates): \n        states_v = ptan.agent.float32_preprocessor(states) \n        states_v = states_v.to(self.device) \n        mu_v = self.net(states_v) \n        actions = mu_v.data.cpu().numpy() \n        actions += self.epsilon * np.random.normal(size=actions.shape) \n        actions = np.clip(actions, -1, 1) \n        return actions, agent_states\n```", "```py\nGAMMA = 0.99 \nBATCH_SIZE = 64 \nLEARNING_RATE = 1e-4 \nREPLAY_SIZE = 100000 \nREPLAY_INITIAL = 10000 \nREWARD_STEPS = 5 \n\nTEST_ITERS = 1000 \n\nVmax = 10 \nVmin = -10 \nN_ATOMS = 51 \nDELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)\n```", "```py\n batch = buffer.sample(BATCH_SIZE) \n                states_v, actions_v, rewards_v, dones_mask, last_states_v = \\ \n                    common.unpack_batch_ddqn(batch, device) \n\n                crt_opt.zero_grad() \n                crt_distr_v = crt_net(states_v, actions_v) \n                last_act_v = tgt_act_net.target_model(last_states_v) \n                last_distr_v = F.softmax( \n                    tgt_crt_net.target_model(last_states_v, last_act_v), dim=1)\n```", "```py\n proj_distr = distr_projection( \n                    last_distr_v.detach().cpu().numpy(), rewards_v.detach().cpu().numpy(), \n                    dones_mask.detach().cpu().numpy(), gamma=GAMMA**REWARD_STEPS) \n                proj_distr_v = torch.tensor(proj_distr).to(device)\n```", "```py\n prob_dist_v = -F.log_softmax(crt_distr_v, dim=1) * proj_distr_v \n                critic_loss_v = prob_dist_v.sum(dim=1).mean() \n                critic_loss_v.backward() \n                crt_opt.step()\n```", "```py\n act_opt.zero_grad() \n                cur_actions_v = act_net(states_v) \n                crt_distr_v = crt_net(states_v, cur_actions_v) \n                actor_loss_v = -crt_net.distr_to_q(crt_distr_v) \n                actor_loss_v = actor_loss_v.mean() \n                actor_loss_v.backward() \n                act_opt.step()\n```"]