- en: Machine Learning and Deep Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In almost all of the applications that we have been discussing up to now, the
    implicit assumption has been that you are creating a new machine learning NLP
    pipeline. Now, that may not always be the case. If you are already working on
    an established platform, fastText may also be a good addition to make the pipeline
    better.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give you some of the methods and recipes for implementing
    fastText using popular frameworks such as scikit-learn, Keras, TensorFlow, and
    PyTorch. We will look at how we can augment the power of word embeddings in fastText,
    using other deep neural architectures such as **convolutional neural networks**
    (**CNN**) or attention networks to solve various NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn and fastText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings layer in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torchtext
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn and fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be talking about how to integrate fastText into your
    statistical models. The most common and popular library for statistical machine
    learning is scikit-learn, so we will focus on that.
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn is one of the most popular machine learning tools and the reason
    is that the API is very simple and uniform. The flow is like this:'
  prefs: []
  type: TYPE_NORMAL
- en: You basically convert your data into matrix format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you create an instance of the predictor class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the instance, you run the `fit` method on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model is created, you can run `predict` on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that you can create a custom classifier by defining the fit and predict
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Custom classifiers for fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we are interested in combining fastText word vectors with the linear
    classifiers, you cannot pass the whole vectors and would need a way to define
    a single vector. In this case, let''s go with the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you will need to pass the token dictionary to the model, which can be
    built from the fastText library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Bringing the whole thing together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use scikit-learn''s `Pipeline` to combine the whole pipeline, demonstrated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The whole code is shown in the statistical machine learning notebook. For further
    ways of building a better model is if you can find better ways of reducing the
    word vectors, TF-IDF is shown in the shared notebook. Another way of reducing the
    word vectors is looking at the hashing transformer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will take a look at how to embed fastText vectors in
    deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen, when you need to work with text in machine learning, you need
    to convert the text into numerical values. The logic is the same in neural architectures
    as well. In neural networks, you implement this using the embeddings layer. All
    modern deep learning libraries provide an embeddings API for use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The embeddings layer is a useful and versatile layer used for various purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: It can be used to learn word embeddings to be used in an application later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used with a larger model where the embeddings are also tuned as part
    of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to load a pretrained word embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is in the third point that will be the focus of this section. The idea is
    to utilize fastText to create superior embeddings, which can then be injected
    into your model using this embedding layer. Normally the embeddings layer is initialized
    with random weights, but in this case we will be injecting it with the word embeddings
    from our fastText model.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a widely popular high-level neural network API. It supports TensorFlow,
    CNTK, and Theano as the backend. Due to the user-friendly API of Keras, many people
    use it in lieu of the base libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layer in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The embedding layer will be the first hidden layer of the Keras network and
    you will need to specify three arguments: input dimension, output dimension, and
    input length. Since we will be using fastText to make our model better, we will
    also need to pass the weights parameter with the embedding matrix and make the
    trainable matrix to be false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Another thing that we need to take care of is that we need to map words to integers
    and integers to words. In Keras, you do this using the `Tokenizer` class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at this in action as part of a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk in terms of mixing word embeddings and neural networks, convolutional
    networks are something that have yielded good results. CNNs are created by applying
    several layers of convolutions with nonlinear activation functions such as ReLu
    or *tanh* applied to the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets talk a little bit about what a convolution means. A convolution of any
    function with respect to another function is the integral that expresses the amount
    of overlap of one function as it is passed over the other function. You can think
    of this as blending one function into another. In signal theory, this is how experts
    understand convolutions: the output signal is a convolution of the input signal
    with the impulse response of the environment. The impulse response of any environment
    essentially identifies and distinguishes the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional feedforward network, we connect each input neuron to each output
    neuron in the next layer. In CNNs, we instead use convolutions on the input to
    compute the output. During the training phase, a CNN will automatically learn
    the values of the filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are generally used with word embeddings, and it is here that fastText
    comes into the picture and has the potential to bring huge gains in terms of classification
    accuracy by providing better word representations. The architecture is thus composed
    of three key sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you have these three pieces of the architecture already in place in your
    classification pipeline, you can identify the word embeddings part and see whether
    changing that to fasttext gives you an improvement in terms of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will be taking a look at the previous example of Yelp reviews
    and trying to classify them using a convolution neural network. For the sake of
    brevity, we will fit a pretrained dataset that has been released, but you will
    probably be working on a domain-specific use case and hence you should be integrating
    the creation of the model, as shown in the previous chapter. As you have seen,
    you can use fastText library or the Gensim library.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a high level, the steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Text samples in the dataset are converted into sequences of word indices. A
    *word index* would simply be an integer ID for the word. We will only consider
    the top 20,000 most common words in the dataset, and we will truncate the sequences
    to a maximum of 1,000 words. This is done for ease of computation. You can play
    around with this approach and find the approach that brings the greatest generalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare an embedding matrix, which will contain at index `i` the embedding vector
    for the word `i` in the word index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The embedding matrix is then loaded to the Keras embedding layer, and will set
    the layer to be frozen so that it is not updated while training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The layers that come after it will be the convolution networks. At the end,
    there will be a softmax layer to converge the output to our five categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this case, we can use the pandas library to create the list of input text
    and the list of the output labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will need to format our text samples and labels into tensors that can
    be fed into the neural network. This is the place where we will be using the `Tokenizer`
    class. We will also need to pad the sequences as we need matrices of equal lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will be using our fastText embeddings. In this case, we are using pretrained
    embeddings, but you can train your own embeddings on the fly during the training
    process. You have the choice of loading from the `.vec` file, but since this is
    fastText, we will load from the BIN file. The advantage of using the BIN file
    is that the out of vocabulary case will be avoided to a large extent. We will
    use the fastText model and generate an embedding matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We load this to the embedding layer. It is important to note that the trainable
    parameter should be set to `False` to prevent the weights from being updated,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build a 1D ConvNet to apply to our Yelp classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary of this model is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can try out some other hyperparameters and try to improve the accuracy
    from here.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you saw how to use the fastText word embeddings as part of
    a larger CNN Keras classifier. Using a similar approach, you can use fastText
    embeddings with whichever neural architectures benefit from word embeddings in
    Keras.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a computation library developed by Google. It is quite popular
    now and is used by many companies to create their neural network models. After
    what you have seen in Keras, the logic behind augmenting TensorFlow models using
    fastText is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create word embeddings in TensorFlow, you will need to create an embeddings
    matrix where all the tokens in your list of documents have unique IDs, and so
    each document is a vector of these IDs. Now, let''s say you have an embedding
    in a NumPy array called `word_embedding`, with `vocab_size` rows and `embedding_dim`
    columns, and you want to create a tensor `W`. Taking a specific example, the sentence
    "I have a cat." can be split up into ["I", "have", "a", "cat", "."], and the tensor
    of the corresponding `word_ids` will be of shape 5\. To map these word IDs into
    vectors, create the word embedding variable and use the `tf.nn.embedding_lookup`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After this, the `embedded_word_ids` tensor will have `shape [5, embedding_size]`
    in our example and contain the embeddings (dense vectors) for each of the five
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to use a word embedding with pretrained vectors, create `W` as a
    `tf.Variable` and initialize it from the NumPy array via a `tf.placeholder()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then pass the actual embeddings in the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will avoid storing a copy of the embeddings in the graph, but it does require
    enough memory to keep two copies of the matrix at once (one for the NumPy array
    and one for the `tf.Variable`). You would want to have word embeddings constant
    during the training process, so as discussed earlier, the trainable parameter
    for the embeddings needs to be `False`.
  prefs: []
  type: TYPE_NORMAL
- en: RNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP has always been considered to be an excellent use case for LSTMs and RNN-type
    neural architectures. LSTMs and RNNs use sequential processing. NLP has always
    been considered one of the biggest use cases because the meaning of any sentence
    is context-based. The meaning of a word can be considered to have meaning based
    on all the words that came before it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, when you are running an LSTM network, you need to convert the words into
    an embedding layer. Generally in such cases, a random initializer is used. But,
    you probably should be able to increase the performance of the model using a fastText
    model. Let's take a look at how a fastText model is used in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the crawl vectors that are released by Facebook are loaded
    into memory. In your use case, you should probably train a fastText model on your
    text corpus and load that model. We are creating the embedding using the VEC file
    here, but you can chose to load from a `.bin` file as well, as shown in the Keras
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the block of text that is your target of interest, and then run the normal
    cleaning step. Similar to the example in Keras, you will next need a mechanism
    that maps the tokens to a unique integer and a way to get the token back from
    the integer. So, we will need to create a dictionary and a reverse dictionary
    with the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the embedding array from the dictionary that we created using the
    fastText model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next, we set up the RNN model. We will be reading three words at a time and
    hence our `x` is a matrix with an undetermined number of rows and three columns
    wide. Another input of note is the `embedding_placeholder` has one row per word
    and has a width of 300 dimensions, corresponding to the number of dimensions for
    the input word vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the TensorFlow `tf.nn.embedding_lookup()` function can be used to look
    up each of our inputs from `x` in matrix `W`, resulting in the 3D tensor `embedded_chars`.
    This can then be fed into the RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the RNN, we need to figure out how to train it and what kind
    of cost function can be used. FastText internally uses the softmax function. The
    softmax function may not be suitable as a cost function in this case because,
    by definition, softmax normalizes the vectors before comparing. Thus, the actual
    vector may grow or shrink in an arbitrary manner. There may be a good case for
    having the final vectors with the same magnitude as the vectors in the training
    set, and thus with the same magnitudes as the pretrained vectors. In this example,
    the focus is on L2 loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the same logic as the previous two libraries, you can use the `torch.nn.EmbeddingBag`
    class to inject the pretrained embeddings. There is a small drawback though. Keras
    and TensorFlow make the assumption that your tensors are actually implemented
    as NumPy arrays, while in the case of PyTorch, that's not the case. PyTorch implements
    the torch tensor. Generally, this is not an issue, but this means that you will
    need to write your own text conversion and tokenizing pipelines. To circumvent
    all this rewriting and reinvention of the wheel, you can use the torchtext library.
  prefs: []
  type: TYPE_NORMAL
- en: The torchtext library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The torchtext is an excellent library that takes care of most of the preprocessing
    steps that you need to build your NLP model. Basically, think of torchtext as
    something that acts like *configuration as code* in a loose sense of the term.
    So, it makes sense to understand the torchtext data paradigm, which takes around
    three hours, instead of writing custom code, which will probably seem easier but
    would involve countless hours of confusion and debugging. And to top it off, it
    can build prebuilt models, including fastText.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how that is done.
  prefs: []
  type: TYPE_NORMAL
- en: Data classes in torchtext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first call all the required libraries. Take note that you are calling
    data that contains the required data classes for our use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use `spacy` for the tokenization step, for which torchtext has excellent
    support. torchtext provides excellent support for calling and loading fastText
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will download the `wiki.simple.bin` model. If you provide the name `en`,
    it will download and load `wiki.en.bin`. If you load `fr`, then it will load `wiki.fr.bin`,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will probably be loading the data from a CSV or from a text file. In that
    case, you will need to open the file, possibly in pandas, extract the relevant
    fields, and then save them in a separate file. torchtext is not able to distinguish
    between training and validation, and hence you will probably need to separate
    out those files as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You will now need to define the data and build the vocabulary. You can do this
    using the data module. This module has the data classes to define the pipelining
    steps and run the batching, padding, and numericalization. First, you will need
    to define the type of fields using `data.Fields`. This class defines the common
    datatypes that can be used to create the required tensor. You can also define
    some common instructions to define how the tensor should be created. Once the
    fields are created, you can call `TabularDataset` to create the dataset using
    the instructions defined in the field. The common instructions are passed as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`sequential=True` means that the column has sequences. We probably want that
    to be the case for labels as the example is pretty much a comparison, but in cases
    where that is not the case, set this to false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are specifying the tokenizer to be spacy in this case, but you can specify
    custom functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fix_length` pads or trims all sequences to fixed lengths of 150 in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lower` specifies we are setting all English letters to lowercase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the datasets are created, you will need to create the vocabulary so that
    we can convert the tokens into integer numbers later. It is here that we are going
    to build the vocabulary from the fastText vectors that we loaded earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Using the iterators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can now use an iterator to iterate over the dataset. In this case, we are
    using `BucketIterator`, which has the additional advantage that it batches examples
    of similar lengths together. This reduces the amount of padding needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So, you will be able to run simple `for` loops on these iterators and they will
    provide inputs based on batches.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, once all these steps are done, you can initialize your PyTorch model,
    and you will need to set the pretrained vectors as the weights of the model. In
    the example, an RNN model was created and the word vectors were initialized from
    the earlier field vectors. This will take care of handling the `lookup_table`
    in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The code shown here includes only the things that should be new to you. For
    the full code, take a look at the `pytorch torchtext rnn.ipynb` notebook in the
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a look at how to integrate fastText word vectors into
    either linear machine learning models or deep learning models created in Keras,
    TensorFlow, and PyTorch. You also saw how word vectors can be easily assimilated
    into existing neural architectures that you might be using in your business application.
    If you are initializing the embeddings from random values, I would highly recommend
    that you try to initialize them using fastText values, and then see whether there
    are performance improvements in your model.
  prefs: []
  type: TYPE_NORMAL
