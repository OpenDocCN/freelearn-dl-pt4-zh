- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: 'Actor-Critic Method: A2C and A3C'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼šA2Cå’ŒA3C
- en: In ChapterÂ [11](ch015.xhtml#x1-18200011), we started to investigate a policy-based
    alternative to the familiar value-based methods family. In particular, we focused
    on the method called REINFORCE and its modification, which uses discounted reward
    to obtain the gradient of the policy (which gives us the direction in which to
    improve the policy). Both methods worked well for a small CartPole problem, but
    for a more complicated Pong environment, we got no convergence.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[11](ch015.xhtml#x1-18200011)ç« ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹ç ”ç©¶ä¸€ç§åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œä½œä¸ºä¼ ç»Ÿå€¼åŸºæ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨äº†åä¸ºREINFORCEçš„æ–¹æ³•åŠå…¶ä¿®æ”¹ç‰ˆï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æŠ˜æ‰£å¥–åŠ±æ¥è·å¾—ç­–ç•¥çš„æ¢¯åº¦ï¼ˆè¯¥æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬æ”¹å–„ç­–ç•¥çš„æ–¹å‘ï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨å°å‹çš„CartPoleé—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„Pongç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å¾—åˆ°æ”¶æ•›ã€‚
- en: Here, we will discuss another extension to the vanilla policy gradient method,
    which magically improves the stability and convergence speed of that method. Despite
    the modification being only minor, the new method has its own name, actor-critic,
    and itâ€™s one of the most powerful methods in deep reinforcement learning (RL).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦ä¸€ç§å¯¹æ™®é€šç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ‰©å±•ï¼Œå®ƒç¥å¥‡åœ°æ”¹å–„äº†è¯¥æ–¹æ³•çš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚å°½ç®¡è¿™ç§ä¿®æ”¹åªæ˜¯å¾®å°çš„ï¼Œä½†æ–°æ–¹æ³•æœ‰äº†è‡ªå·±çš„åå­—â€”â€”æ¼”å‘˜-è¯„è®ºå‘˜ï¼Œå®ƒæ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­æœ€å¼ºå¤§çš„æ–¹æ³•ä¹‹ä¸€ã€‚
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Explore how the baseline impacts statistics and the convergence of gradients
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢åŸºå‡†æ–¹æ³•å¦‚ä½•å½±å“ç»Ÿè®¡æ•°æ®å’Œæ¢¯åº¦çš„æ”¶æ•›æ€§
- en: Cover an extension of the baseline idea
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©å±•åŸºå‡†æ–¹æ³•çš„æ¦‚å¿µ
- en: Implement the advantage actor-critic (A2C) method and check it on the Pong environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå‘˜ï¼ˆA2Cï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨Pongç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•
- en: 'Add asynchronous execution to the A2C method using two different ways: data
    parallelism and gradient parallelism'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•ï¼šæ•°æ®å¹¶è¡Œå’Œæ¢¯åº¦å¹¶è¡Œï¼Œä¸ºA2Cæ–¹æ³•å¢åŠ å¼‚æ­¥æ‰§è¡Œ
- en: Variance reduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹å·®å‡å°‘
- en: 'In the previous chapter, I briefly mentioned that one of the ways to improve
    the stability of policy gradient methods is to reduce the variance of the gradient.
    Now letâ€™s try to understand why this is important and what it means to reduce
    the variance. In statistics, variance is the expected square deviation of a random
    variable from the expected value of that variable:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸€ç« ä¸­ï¼Œæˆ‘ç®€è¦æåˆ°è¿‡ï¼Œæ”¹å–„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç¨³å®šæ€§çš„ä¸€ç§æ–¹å¼æ˜¯å‡å°‘æ¢¯åº¦çš„æ–¹å·®ã€‚ç°åœ¨è®©æˆ‘ä»¬å°è¯•ç†è§£ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Œä»¥åŠå‡å°‘æ–¹å·®æ„å‘³ç€ä»€ä¹ˆã€‚åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œæ–¹å·®æ˜¯éšæœºå˜é‡ä¸è¯¥å˜é‡çš„æœŸæœ›å€¼ä¹‹é—´çš„å¹³æ–¹åå·®çš„æœŸæœ›å€¼ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq44.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq44.png)'
- en: Variance shows us how far values are dispersed from the mean. When variance
    is high, the random variable can take values that deviate widely from the mean.
    In the following plot, there is a normal (Gaussian) distribution with the same
    value for the mean, Î¼ = 10, but with different values for the variance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹å·®å±•ç¤ºäº†æ•°å€¼ä¸å‡å€¼ä¹‹é—´çš„åˆ†æ•£ç¨‹åº¦ã€‚å½“æ–¹å·®è¾ƒé«˜æ—¶ï¼Œéšæœºå˜é‡å¯èƒ½ä¼šå–åˆ°ä¸å‡å€¼ç›¸å·®è¾ƒå¤§çš„å€¼ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªå‡å€¼ä¸ºÎ¼ = 10çš„æ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒï¼Œä½†å…¶æ–¹å·®å€¼ä¸åŒã€‚
- en: '![PIC](img/B22150_12_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_01.png)'
- en: 'FigureÂ 12.1: The effect of variance on Gaussian distribution'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12.1ï¼šæ–¹å·®å¯¹é«˜æ–¯åˆ†å¸ƒçš„å½±å“
- en: Now letâ€™s return to policy gradients. It was stated in the previous chapter
    that the idea is to increase the probability of good actions and decrease the
    chance of bad ones. In math notation, our policy gradient was written as âˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log
    Ï€(a|s)]. The scaling factor Q(s,a) specifies how much we want to increase or decrease
    the probability of the action taken in the particular state. In the REINFORCE
    method, we used the discounted total reward as the scaling of the gradient. In
    an attempt to increase REINFORCE stability, we subtracted the mean reward from
    the gradient scale.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å›åˆ°ç­–ç•¥æ¢¯åº¦ã€‚å‰ä¸€ç« ä¸­æåˆ°è¿‡ï¼Œç­–ç•¥æ¢¯åº¦çš„æ ¸å¿ƒæ€æƒ³æ˜¯æé«˜è‰¯å¥½åŠ¨ä½œçš„æ¦‚ç‡å¹¶é™ä½ä¸è‰¯åŠ¨ä½œçš„æ¦‚ç‡ã€‚åœ¨æ•°å­¦è¡¨ç¤ºä¸­ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦è¢«å†™ä¸ºâˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log
    Ï€(a|s)]ã€‚ç¼©æ”¾å› å­Q(s,a)æŒ‡å®šäº†æˆ‘ä»¬å¸Œæœ›åœ¨ç‰¹å®šçŠ¶æ€ä¸‹å¢åŠ æˆ–å‡å°‘åŠ¨ä½œæ¦‚ç‡çš„å¤šå°‘ã€‚åœ¨REINFORCEæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ˜æ‰£æ€»å¥–åŠ±ä½œä¸ºæ¢¯åº¦çš„ç¼©æ”¾å› å­ã€‚ä¸ºäº†æé«˜REINFORCEçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä»æ¢¯åº¦çš„ç¼©æ”¾å› å­ä¸­å‡å»äº†å¹³å‡å¥–åŠ±ã€‚
- en: 'To understand why this helped, letâ€™s consider the very simple scenario of an
    optimization step on which we have three actions with different total discounted
    rewards: Q[1], Q[2], and Q[3]. Now letâ€™s check what will happen with policy gradients
    with regard to the relative values of those Q[s].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆè¿™æœ‰å¸®åŠ©ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªéå¸¸ç®€å•çš„ä¼˜åŒ–æ­¥éª¤åœºæ™¯ï¼Œåœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸‰ç§åŠ¨ä½œï¼Œå®ƒä»¬çš„æ€»æŠ˜æ‰£å¥–åŠ±ä¸åŒï¼šQ[1]ã€Q[2]å’ŒQ[3]ã€‚ç°åœ¨è®©æˆ‘ä»¬æ£€æŸ¥åœ¨å…³äºè¿™äº›Q[s]çš„ç›¸å¯¹å€¼çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥æ¢¯åº¦ä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: As the first example, let both Q[1] and Q[2] be equal to some small positive
    number and Q[3] be a large negative number. So, actions at the first and second
    steps led to some small reward, but the third step was not very successful. The
    resulting combined gradient for all three steps will try to push our policy far
    from the action at step three and slightly toward the actions taken at steps one
    and two, which is a totally reasonable thing to do.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€ä¸ªä¾‹å­ï¼Œå‡è®¾Q[1]å’ŒQ[2]éƒ½ç­‰äºæŸä¸ªå°çš„æ­£æ•°ï¼Œè€ŒQ[3]æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„è´Ÿæ•°ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥çš„è¡ŒåŠ¨è·å¾—äº†ä¸€äº›å°çš„å¥–åŠ±ï¼Œä½†ç¬¬ä¸‰æ­¥çš„ç»“æœå¹¶ä¸ç†æƒ³ã€‚æ‰€æœ‰ä¸‰æ­¥çš„ç»¼åˆæ¢¯åº¦å°†å°è¯•å°†æˆ‘ä»¬çš„ç­–ç•¥è¿œç¦»ç¬¬ä¸‰æ­¥çš„è¡ŒåŠ¨ï¼Œå¹¶ç¨å¾®æ¨åŠ¨å®ƒæœç€ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥çš„è¡ŒåŠ¨æ–¹å‘å‘å±•ï¼Œè¿™å®Œå…¨æ˜¯åˆç†çš„åšæ³•ã€‚
- en: 'Now letâ€™s imagine that our reward is always positive and only the value is
    different. This corresponds to adding some constant to each of the rewards from
    the previous example: Q[1], Q[2], and Q[3]. In this case, Q[1] and Q[2] will become
    large positive numbers and Q[3] will have a small positive value. However, our
    policy update will become different! We will try hard to push our policy toward
    actions at the first and second steps, and slightly push it toward an action at
    step three. So, strictly speaking, we are no longer trying to avoid the action
    taken for step three, despite the fact that the relative rewards are the same.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„å¥–åŠ±å§‹ç»ˆä¸ºæ­£ä¸”åªæœ‰æ•°å€¼ä¸åŒã€‚è¿™ç›¸å½“äºåœ¨å‰ä¸€ä¸ªä¾‹å­ä¸­çš„æ¯ä¸ªå¥–åŠ±å€¼ä¸Šæ·»åŠ ä¸€ä¸ªå¸¸é‡ï¼šQ[1]ã€Q[2]å’ŒQ[3]ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒQ[1]å’ŒQ[2]å°†å˜ä¸ºå¤§çš„æ­£æ•°ï¼Œè€ŒQ[3]å°†å…·æœ‰ä¸€ä¸ªå°çš„æ­£å€¼ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ›´æ–°å°†å˜å¾—ä¸åŒï¼æˆ‘ä»¬å°†åŠªåŠ›å°†ç­–ç•¥æ¨å‘ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥çš„è¡ŒåŠ¨ï¼Œå¹¶ç¨å¾®æ¨å‘ç¬¬ä¸‰æ­¥çš„è¡ŒåŠ¨ã€‚å› æ­¤ï¼Œä¸¥æ ¼æ¥è¯´ï¼Œå°½ç®¡ç›¸å¯¹å¥–åŠ±ç›¸åŒï¼Œæˆ‘ä»¬ä¸å†è¯•å›¾é¿å…ç¬¬ä¸‰æ­¥çš„è¡ŒåŠ¨ã€‚
- en: This dependency of our policy update on the constant added to the reward can
    slow down our training significantly, as we may require many more samples to average
    out the effect of such a shift in the policy gradient. Even worse, as our total
    discounted reward changes over time, with the agent learning how to act better
    and better, our policy gradient variance can also change. For example, in the
    Atari Pong environment, the average reward in the beginning is âˆ’21â€¦ âˆ’ 20, so all
    the actions look almost equally bad.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç­–ç•¥æ›´æ–°ä¾èµ–äºæ·»åŠ åˆ°å¥–åŠ±ä¸­çš„å¸¸é‡ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å‡ç¼“è®­ç»ƒè¿›åº¦ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½éœ€è¦æ›´å¤šçš„æ ·æœ¬æ¥å¹³æ»‘è¿™ç§ç­–ç•¥æ¢¯åº¦çš„å˜åŒ–ã€‚æ›´ç³Ÿçš„æ˜¯ï¼Œéšç€æˆ‘ä»¬æ€»çš„æŠ˜æ‰£å¥–åŠ±éšæ—¶é—´å˜åŒ–ï¼Œä»£ç†ä¸æ–­å­¦ä¹ å¦‚ä½•åšå¾—æ›´å¥½ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦æ–¹å·®ä¹Ÿå¯èƒ½å‘ç”Ÿå˜åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨Atari
    Pongç¯å¢ƒä¸­ï¼Œå¼€å§‹æ—¶çš„å¹³å‡å¥–åŠ±æ˜¯âˆ’21...âˆ’20ï¼Œå› æ­¤æ‰€æœ‰çš„è¡ŒåŠ¨çœ‹èµ·æ¥å‡ ä¹åŒæ ·ç³Ÿç³•ã€‚
- en: 'To overcome this in the previous chapter, we subtracted the mean total reward
    from the Q-value and called this mean the baseline. This trick normalized our
    policy gradient: in the case of the average reward being âˆ’21, getting a reward
    of âˆ’20 looks like a win for the agent and it pushes its policy toward the taken
    actions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬ä»Qå€¼ä¸­å‡å»äº†æ€»å¥–åŠ±çš„å‡å€¼ï¼Œå¹¶ç§°è¿™ä¸ªå‡å€¼ä¸ºåŸºå‡†ã€‚è¿™ä¸€æŠ€å·§å°†æˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦å½’ä¸€åŒ–ï¼šä¾‹å¦‚ï¼Œå½“å¹³å‡å¥–åŠ±ä¸ºâˆ’21æ—¶ï¼Œè·å¾—âˆ’20çš„å¥–åŠ±çœ‹èµ·æ¥åƒæ˜¯ä»£ç†çš„èƒœåˆ©ï¼Œè¿™å°†æ¨åŠ¨å…¶ç­–ç•¥æœç€é‡‡å–çš„è¡ŒåŠ¨æ–¹å‘å‘å±•ã€‚
- en: CartPole variance
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CartPoleæ–¹å·®
- en: 'To check this theoretical conclusion in practice, letâ€™s plot our policy gradient
    variance during the training for both the baseline version and the version without
    the baseline. The complete example is in Chapter12/01_cartpole_pg.py, and most
    of the code is the same as in ChapterÂ [11](ch015.xhtml#x1-18200011). The differences
    in this version are the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨å®è·µä¸­éªŒè¯è¿™ä¸ªç†è®ºç»“è®ºï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶åŸºå‡†ç‰ˆæœ¬å’Œä¸ä½¿ç”¨åŸºå‡†ç‰ˆæœ¬çš„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç­–ç•¥æ¢¯åº¦æ–¹å·®ã€‚å®Œæ•´ç¤ºä¾‹ä½äºChapter12/01_cartpole_pg.pyï¼Œä¸”å¤§éƒ¨åˆ†ä»£ç ä¸ç¬¬[11ç« ](ch015.xhtml#x1-18200011)ç›¸åŒã€‚è¯¥ç‰ˆæœ¬çš„ä¸åŒä¹‹å¤„å¦‚ä¸‹ï¼š
- en: It now accepts the command-line option --baseline, which enables the mean subtraction
    from the reward. By default, no baseline is used.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒç°åœ¨æ¥å—å‘½ä»¤è¡Œé€‰é¡¹`--baseline`ï¼Œå¯ç”¨ä»å¥–åŠ±ä¸­å‡å»å‡å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸ä½¿ç”¨åŸºå‡†ã€‚
- en: On every training loop, we gather the gradients from the policy loss and use
    this data to calculate the variance.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªè®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä»ç­–ç•¥æŸå¤±ä¸­è·å–æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥è®¡ç®—æ–¹å·®ã€‚
- en: 'To gather only the gradients from the policy loss and exclude the gradients
    from the entropy bonus added for exploration, we need to calculate the gradients
    in two stages. Luckily, PyTorch allows this to be done easily. In the following
    code, only the relevant part of the training loop is included to illustrate the
    idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä»…æ”¶é›†æ¥è‡ªç­–ç•¥æŸå¤±çš„æ¢¯åº¦ï¼Œå¹¶æ’é™¤ä¸ºäº†æ¢ç´¢è€Œæ·»åŠ çš„ç†µå¥–åŠ±çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦åˆ†ä¸¤é˜¶æ®µè®¡ç®—æ¢¯åº¦ã€‚å¹¸è¿çš„æ˜¯ï¼ŒPyTorchä½¿å¾—è¿™ä¸€æ“ä½œå˜å¾—ç®€å•ã€‚ä»¥ä¸‹ä»£ç ä¸­ä»…åŒ…å«äº†è®­ç»ƒå¾ªç¯çš„ç›¸å…³éƒ¨åˆ†ï¼Œç”¨äºè¯´æ˜è¿™ä¸€æ€è·¯ï¼š
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We calculate the policy loss as before, by calculating the log from the probabilities
    of taken actions and multiplying it by policy scales (which are the total discounted
    reward if we are not using the baseline or the total reward minus the baseline).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åƒä»¥å‰ä¸€æ ·è®¡ç®—ç­–ç•¥æŸå¤±ï¼Œé€šè¿‡è®¡ç®—å·²é‡‡å–åŠ¨ä½œçš„æ¦‚ç‡çš„å¯¹æ•°å¹¶å°†å…¶ä¹˜ä»¥ç­–ç•¥å°ºåº¦ï¼ˆå¦‚æœæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨åŸºå‡†çº¿ï¼Œå®ƒæ˜¯æ€»æŠ˜æ‰£å¥–åŠ±ï¼Œæˆ–è€…æ˜¯æ€»å¥–åŠ±å‡å»åŸºå‡†çº¿ï¼‰ã€‚
- en: 'In the next step, we ask PyTorch to backpropagate the policy loss, calculating
    the gradients and keeping them in our modelâ€™s buffers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬è¯·æ±‚ PyTorch åå‘ä¼ æ’­ç­–ç•¥æŸå¤±ï¼Œè®¡ç®—æ¢¯åº¦å¹¶å°†å®ƒä»¬ä¿å­˜åœ¨æ¨¡å‹çš„ç¼“å†²åŒºä¸­ï¼š
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we have previously performed optimizer.zero_grad(), those buffers will contain
    only the gradients from the policy loss. One tricky thing here is the retain_graph=True
    option when we call backward(). It instructs PyTorch to keep the graph structure
    of the variables. Normally, this is destroyed by the backward() call, but in our
    case, this is not what we want. In general, retaining the graph could be useful
    when we need to backpropagate the loss multiple times before the call to the optimizer,
    although this is not a very common situation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬ä¹‹å‰æ‰§è¡Œäº† optimizer.zero_grad()ï¼Œè¿™äº›ç¼“å†²åŒºå°†åªåŒ…å«æ¥è‡ªç­–ç•¥æŸå¤±çš„æ¢¯åº¦ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ£˜æ‰‹çš„åœ°æ–¹æ˜¯æˆ‘ä»¬åœ¨è°ƒç”¨ backward()
    æ—¶ä½¿ç”¨äº† retain_graph=True é€‰é¡¹ã€‚å®ƒæŒ‡ç¤º PyTorch ä¿ç•™å˜é‡çš„å›¾ç»“æ„ã€‚é€šå¸¸ï¼Œè°ƒç”¨ backward() æ—¶ä¼šé”€æ¯å›¾ç»“æ„ï¼Œä½†åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå½“æˆ‘ä»¬éœ€è¦åœ¨è°ƒç”¨ä¼˜åŒ–å™¨ä¹‹å‰å¤šæ¬¡åå‘ä¼ æ’­æŸå¤±æ—¶ï¼Œä¿ç•™å›¾ç»“æ„å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ï¼Œå°½ç®¡è¿™ä¸æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„æƒ…å†µã€‚
- en: 'Then, we iterate all parameters from our model (every parameter of our model
    is a tensor with gradients) and extract their grad field in a flattened NumPy
    array:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å‚æ•°ï¼ˆæ¨¡å‹çš„æ¯ä¸ªå‚æ•°éƒ½æ˜¯ä¸€ä¸ªåŒ…å«æ¢¯åº¦çš„å¼ é‡ï¼‰ï¼Œå¹¶å°†å®ƒä»¬çš„ grad å­—æ®µæå–åˆ°ä¸€ä¸ªå±•å¹³çš„ NumPy æ•°ç»„ä¸­ï¼š
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This gives us one long array with all gradients from our modelâ€™s variables.
    However, our parameter update should take into account not only the policy gradient
    but also the gradient provided by our entropy bonus. To achieve this, we calculate
    the entropy loss and call backward() again. To be able to do this the second time,
    we need to pass retain_graph=True.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªåŒ…å«æ¨¡å‹å˜é‡ä¸­æ‰€æœ‰æ¢¯åº¦çš„é•¿æ•°ç»„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„å‚æ•°æ›´æ–°ä¸ä»…åº”è¯¥è€ƒè™‘ç­–ç•¥æ¢¯åº¦ï¼Œè¿˜åº”è€ƒè™‘ç”±ç†µå¥–åŠ±æä¾›çš„æ¢¯åº¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®¡ç®—ç†µæŸå¤±å¹¶å†æ¬¡è°ƒç”¨
    backward()ã€‚ä¸ºäº†èƒ½å¤Ÿç¬¬äºŒæ¬¡æ‰§è¡Œè¿™ä¸€æ“ä½œï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ retain_graph=Trueã€‚
- en: 'On the second backward() call, PyTorch will backpropagate our entropy loss
    and add the gradients to the internal gradientsâ€™ buffers. So, what we now need
    to do is just ask our optimizer to perform the optimization step using those combined
    gradients:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒæ¬¡è°ƒç”¨ backward() æ—¶ï¼ŒPyTorch å°†åå‘ä¼ æ’­æˆ‘ä»¬çš„ç†µæŸå¤±ï¼Œå¹¶å°†æ¢¯åº¦æ·»åŠ åˆ°å†…éƒ¨æ¢¯åº¦ç¼“å†²åŒºä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨éœ€è¦åšçš„å°±æ˜¯è¯·æ±‚ä¼˜åŒ–å™¨ä½¿ç”¨è¿™äº›åˆå¹¶çš„æ¢¯åº¦æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼š
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Later, the only thing we need to do is write statistics that we are interested
    in into TensorBoard:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬éœ€è¦åšçš„å”¯ä¸€äº‹æƒ…å°±æ˜¯å°†æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç»Ÿè®¡æ•°æ®å†™å…¥ TensorBoardï¼š
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By running this example twice, once with the --baseline command-line option
    and once without it, we get a plot of variance of our policy gradient. The following
    charts show the smoothed reward (average for last 100 episodes) and variance (smoothed
    with window 20):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿è¡Œè¿™ä¸ªç¤ºä¾‹ä¸¤æ¬¡ï¼Œä¸€æ¬¡ä½¿ç”¨ --baseline å‘½ä»¤è¡Œé€‰é¡¹ï¼Œä¸€æ¬¡ä¸ä½¿ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®å›¾ã€‚ä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºäº†å¹³æ»‘çš„å¥–åŠ±ï¼ˆè¿‡å» 100 é›†çš„å¹³å‡å€¼ï¼‰å’Œæ–¹å·®ï¼ˆä½¿ç”¨çª—å£
    20 å¹³æ»‘ï¼‰ï¼š
- en: '![PIC](img/B22150_12_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_02.png)'
- en: 'FigureÂ 12.2: Smoothed reward (left) and variance (right)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12.2ï¼šå¹³æ»‘å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ–¹å·®ï¼ˆå³ï¼‰
- en: 'These next two charts show the gradientsâ€™ magnitude (L2) and maximum value.
    All values are smoothed with window 20:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„ä¸¤ä¸ªå›¾è¡¨æ˜¾ç¤ºäº†æ¢¯åº¦çš„å¤§å°ï¼ˆL2 èŒƒæ•°ï¼‰å’Œæœ€å¤§å€¼ã€‚æ‰€æœ‰å€¼éƒ½ç»è¿‡çª—å£ 20 å¹³æ»‘å¤„ç†ï¼š
- en: '![PIC](img/B22150_12_03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_03.png)'
- en: 'FigureÂ 12.3: Gradientsâ€™ L2 norm (left) and maximum value (right)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12.3ï¼šæ¢¯åº¦çš„ L2 èŒƒæ•°ï¼ˆå·¦ï¼‰å’Œæœ€å¤§å€¼ï¼ˆå³ï¼‰
- en: As you can see, variance for the version with the baseline is two to three orders
    of magnitude lower than the version without one, which helps the system to converge
    faster.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œå¸¦æœ‰åŸºå‡†çº¿çš„ç‰ˆæœ¬çš„æ–¹å·®æ¯”æ²¡æœ‰åŸºå‡†çº¿çš„ç‰ˆæœ¬ä½ä¸¤ä¸ªåˆ°ä¸‰ä¸ªæ•°é‡çº§ï¼Œè¿™æœ‰åŠ©äºç³»ç»Ÿæ›´å¿«åœ°æ”¶æ•›ã€‚
- en: Advantage actor-critic (A2C)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿è¡Œä¸ºè€…-è¯„è®ºå‘˜ï¼ˆA2Cï¼‰
- en: 'The next step in reducing the variance is making our baseline state-dependent
    (which is a good idea, as different states could have very different baselines).
    Indeed, to decide on the suitability of a particular action in some state, we
    use the discounted total reward of the action. However, the total reward itself
    could be represented as a value of the state plus the advantage of the action:
    Q(s,a) = V (s) + A(s,a). You saw this in ChapterÂ [8](ch012.xhtml#x1-1240008),
    when we discussed DQN modifications, particularly dueling DQN.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘æ–¹å·®çš„ä¸‹ä¸€æ­¥æ˜¯ä½¿æˆ‘ä»¬çš„åŸºå‡†çŠ¶æ€ä¾èµ–æ€§ï¼ˆè¿™æ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºä¸åŒçš„çŠ¶æ€å¯èƒ½å…·æœ‰éå¸¸ä¸åŒçš„åŸºå‡†ï¼‰ã€‚å®é™…ä¸Šï¼Œä¸ºäº†å†³å®šæŸä¸ªçŠ¶æ€ä¸‹æŸä¸ªåŠ¨ä½œçš„é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥åŠ¨ä½œçš„æŠ˜æ‰£æ€»å¥–åŠ±ã€‚ç„¶è€Œï¼Œæ€»å¥–åŠ±æœ¬èº«å¯ä»¥è¡¨ç¤ºä¸ºçŠ¶æ€çš„å€¼åŠ ä¸ŠåŠ¨ä½œçš„ä¼˜åŠ¿ï¼šQ(s,a)
    = V (s) + A(s,a)ã€‚ä½ åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­è§è¿‡è¿™ç§æ–¹æ³•ï¼Œå½“æ—¶æˆ‘ä»¬è®¨è®ºäº† DQN çš„ä¿®æ”¹ï¼Œç‰¹åˆ«æ˜¯å¯¹æŠ—
    DQNã€‚
- en: 'So, why canâ€™t we use V (s) as a baseline? In that case, the scale of our gradient
    will be just advantage, A(s,a), showing how this taken action is better in respect
    to the average stateâ€™s value. In fact, we can do this, and it is a very good idea
    for improving the policy gradient method. The only problem here is that we donâ€™t
    know the value, V (s), of the state that we need to subtract from the discounted
    total reward, Q(s,a). To solve this, letâ€™s use another neural network, which will
    approximate V (s) for every observation. To train it, we can exploit the same
    training procedure we used in DQN methods: we will carry out the Bellman step
    and then minimize the mean square error to improve V (s) approximation.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸èƒ½ç”¨ V(s) ä½œä¸ºåŸºå‡†å‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ¢¯åº¦è§„æ¨¡å°†åªæ˜¯ä¼˜åŠ¿ A(s,a)ï¼Œè¡¨ç¤ºæ­¤åŠ¨ä½œç›¸å¯¹äºå¹³å‡çŠ¶æ€å€¼çš„æ”¹å–„ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œè¿™å¯¹äºæ”¹è¿›ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ä¸»æ„ã€‚å”¯ä¸€çš„é—®é¢˜æ˜¯æˆ‘ä»¬ä¸çŸ¥é“éœ€è¦ä»æŠ˜æ‰£æ€»å¥–åŠ±
    Q(s,a) ä¸­å‡å»çš„çŠ¶æ€å€¼ V(s)ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒå°†ä¸ºæ¯ä¸ªè§‚æµ‹å€¼è¿‘ä¼¼ V(s)ã€‚ä¸ºäº†è®­ç»ƒå®ƒï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨åœ¨ DQN æ–¹æ³•ä¸­ä½¿ç”¨çš„ç›¸åŒè®­ç»ƒè¿‡ç¨‹ï¼šæˆ‘ä»¬å°†æ‰§è¡Œè´å°”æ›¼æ­¥éª¤ï¼Œç„¶åæœ€å°åŒ–å‡æ–¹è¯¯å·®æ¥æ”¹è¿›
    V(s) çš„è¿‘ä¼¼ã€‚
- en: 'When we know the value for any state (or at least have some approximation of
    it), we can use it to calculate the policy gradient and update our policy network
    to increase probabilities for actions with good advantage values and decrease
    the chance of actions with bad advantage values. The policy network (which returns
    a probability distribution of actions) is called the actor, as it tells us what
    to do. Another network is called critic, as it allows us to understand how good
    our actions were by returning V (s). This improvement is known under a separate
    name, the advantage actor-critic method, which is often abbreviated to A2C. FigureÂ [12.4](#x1-206002r4)
    is an illustration of its architecture:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬çŸ¥é“ä»»ä½•çŠ¶æ€çš„å€¼ï¼ˆæˆ–è‡³å°‘æœ‰ä¸€äº›è¿‘ä¼¼å€¼ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒæ¥è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œå¹¶æ›´æ–°æˆ‘ä»¬çš„ç­–ç•¥ç½‘ç»œï¼Œä»¥å¢åŠ å…·æœ‰è‰¯å¥½ä¼˜åŠ¿å€¼çš„åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶å‡å°‘å…·æœ‰ä¸è‰¯ä¼˜åŠ¿å€¼çš„åŠ¨ä½œçš„æœºä¼šã€‚ç­–ç•¥ç½‘ç»œï¼ˆè¿”å›åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼‰è¢«ç§°ä¸ºæ¼”å‘˜ï¼ˆactorï¼‰ï¼Œå› ä¸ºå®ƒå‘Šè¯‰æˆ‘ä»¬è¯¥åšä»€ä¹ˆã€‚å¦ä¸€ä¸ªç½‘ç»œç§°ä¸ºè¯„è®ºå‘˜ï¼ˆcriticï¼‰ï¼Œå› ä¸ºå®ƒé€šè¿‡è¿”å›
    V(s) è®©æˆ‘ä»¬äº†è§£æˆ‘ä»¬çš„åŠ¨ä½œæœ‰å¤šå¥½ã€‚è¿™ç§æ”¹è¿›æœ‰ä¸€ä¸ªç‹¬ç«‹çš„åç§°ï¼Œç§°ä¸ºä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼Œé€šå¸¸ç¼©å†™ä¸º A2Cã€‚å›¾[12.4](#x1-206002r4)æ˜¯å…¶æ¶æ„çš„ç¤ºæ„å›¾ï¼š
- en: '![PVoalliuceynneett OÏ€Vbs((((eacrasrci|s)vtt)aoictr))ions ](img/B22150_12_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![PVoalliuceynneett OÏ€Vbs((((eacrasrci|s)vtt)aoictr))ions ](img/B22150_12_04.png)'
- en: 'FigureÂ 12.4: The A2C architecture'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12.4ï¼šA2C æ¶æ„
- en: In practice, the policy and value networks partially overlap, mostly due to
    efficiency and convergence considerations. In this case, the policy and value
    are implemented as different heads of the network, taking the output from the
    common body and transforming it into the probability distribution and a single
    number representing the value of the state.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œç­–ç•¥ç½‘ç»œå’Œå€¼ç½‘ç»œéƒ¨åˆ†é‡å ï¼Œä¸»è¦æ˜¯å‡ºäºæ•ˆç‡å’Œæ”¶æ•›æ€§çš„è€ƒè™‘ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç­–ç•¥å’Œå€¼è¢«å®ç°ä¸ºç½‘ç»œçš„ä¸åŒâ€œå¤´éƒ¨â€ï¼Œå®ƒä»¬ä»å…±äº«çš„ä¸»ä½“è·å–è¾“å‡ºï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒå’Œä¸€ä¸ªè¡¨ç¤ºçŠ¶æ€å€¼çš„å•ä¸€æ•°å­—ã€‚
- en: 'This helps both networks to share low-level features (such as convolution filters
    in the Atari agent), but combine them in a different way. The following figure
    shows this architecture:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰åŠ©äºä¸¤ä¸ªç½‘ç»œå…±äº«ä½å±‚æ¬¡ç‰¹å¾ï¼ˆä¾‹å¦‚ Atari ä»£ç†ä¸­çš„å·ç§¯æ»¤æ³¢å™¨ï¼‰ï¼Œä½†ä»¥ä¸åŒçš„æ–¹å¼å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™ç§æ¶æ„ï¼š
- en: '![CPVooamllmiuocenynneetntet OÏ€Vbs((((e(acrasrbci|s)vott)adoictyr))io)ns ](img/B22150_12_05.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CPVooamllmiuocenynneetntet OÏ€Vbs((((e(acrasrbci|s)vott)adoictyr))io)ns ](img/B22150_12_05.png)'
- en: 'FigureÂ 12.5: The A2C architecture with a shared network body'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 12.5ï¼šå¸¦æœ‰å…±äº«ç½‘ç»œä¸»ä½“çš„ A2C æ¶æ„
- en: 'From a training point of view, we complete these steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š
- en: Initialize network parameters, ğœƒ, with random values.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”¨éšæœºå€¼åˆå§‹åŒ–ç½‘ç»œå‚æ•°ï¼Œğœƒã€‚
- en: Play N steps in the environment, using the current policy, Ï€[ğœƒ], and saving
    the state, s[t], action, a[t], and reward, r[t].
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ç¯å¢ƒä¸­æ‰§è¡Œ N æ­¥ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥ Ï€[ğœƒ]ï¼Œå¹¶ä¿å­˜çŠ¶æ€ s[t]ã€åŠ¨ä½œ a[t] å’Œå¥–åŠ± r[t]ã€‚
- en: Set R â† 0 if the end of the episode is reached or V [ğœƒ](s[t]).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœåˆ°è¾¾å›åˆç»“æŸæˆ– V [ğœƒ](s[t])ï¼Œåˆ™è®¾ç½® R â† 0ã€‚
- en: 'For i = t âˆ’ 1â€¦t[start] (note that steps are processed backward):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äº i = t âˆ’ 1â€¦t[start]ï¼ˆæ³¨æ„æ­¥éª¤æ˜¯é€†å‘å¤„ç†çš„ï¼‰ï¼š
- en: R â†r[i] + Î³R
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: R â†r[i] + Î³R
- en: 'Accumulate the policy gradients:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç´¯ç§¯ç­–ç•¥æ¢¯åº¦ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq45.png)'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq45.png)'
- en: 'Accumulate the value gradients:'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç´¯ç§¯å€¼æ¢¯åº¦ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq46.png)'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq46.png)'
- en: Update the network parameters using the accumulated gradients, moving in the
    direction of the policy gradients, âˆ‚ğœƒ[Ï€], and in the opposite direction of the
    value gradients, âˆ‚ğœƒ[v].
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç´¯ç§¯çš„æ¢¯åº¦æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œæ²¿ç€ç­–ç•¥æ¢¯åº¦ âˆ‚ğœƒ[Ï€] çš„æ–¹å‘ç§»åŠ¨ï¼Œåæ–¹å‘åˆ™æ˜¯å€¼æ¢¯åº¦ âˆ‚ğœƒ[v]ã€‚
- en: Repeat from step 2 until convergence is reached.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚
- en: 'This algorithm is just an outline and similar to those that are usually printed
    in research papers. In practice, several extensions to improve the stability of
    the method may be used:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®—æ³•åªæ˜¯ä¸€ä¸ªå¤§è‡´çš„æ¡†æ¶ï¼Œç±»ä¼¼äºé€šå¸¸åœ¨ç ”ç©¶è®ºæ–‡ä¸­æ‰“å°çš„å†…å®¹ã€‚å®é™…ä¸Šï¼Œå¯èƒ½ä¼šä½¿ç”¨ä¸€äº›æ‰©å±•æ–¹æ³•æ¥æé«˜è¯¥æ–¹æ³•çš„ç¨³å®šæ€§ï¼š
- en: 'An entropy bonus is usually added to improve exploration. Itâ€™s typically written
    as an entropy value added to the loss function:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šå¸¸ä¼šæ·»åŠ ä¸€ä¸ªç†µå¥–åŠ±æ¥æ”¹å–„æ¢ç´¢ã€‚è¿™é€šå¸¸è¡¨ç°ä¸ºä¸€ä¸ªç†µå€¼ï¼Œæ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq47.png)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq47.png)'
- en: This function has a minimum when the probability distribution is uniform, so
    by adding it to the loss function, we push our agent away from being too certain
    about its actions. The value of Î² is a hyperparameter scaling the entropy bonus
    and prioritizing the exploration during the training. Normally, it is constant
    or linearly decreased during the training.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“æ¦‚ç‡åˆ†å¸ƒæ˜¯å‡åŒ€æ—¶ï¼Œè¿™ä¸ªå‡½æ•°æœ‰ä¸€ä¸ªæœ€å°å€¼ï¼Œå› æ­¤é€šè¿‡å°†å…¶æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®©æ™ºèƒ½ä½“é¿å…å¯¹è‡ªå·±çš„åŠ¨ä½œè¿‡äºç¡®å®šã€‚Î² çš„å€¼æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨æ¥ç¼©æ”¾ç†µå¥–åŠ±å¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆè¿›è¡Œæ¢ç´¢ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯å¸¸æ•°æˆ–åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§é€’å‡çš„ã€‚
- en: 'Gradient accumulation is usually implemented as a loss function combining all
    three components: policy loss, value loss, and entropy loss. You should be careful
    with signs of these losses, as policy gradients show you the direction of policy
    improvement, but both the value and entropy losses should be minimized.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯é€šå¸¸ä½œä¸ºä¸€ä¸ªæŸå¤±å‡½æ•°å®ç°ï¼Œç»“åˆäº†ä¸‰ä¸ªéƒ¨åˆ†ï¼šç­–ç•¥æŸå¤±ã€å€¼æŸå¤±å’Œç†µæŸå¤±ã€‚ä½ åº”è¯¥æ³¨æ„è¿™äº›æŸå¤±çš„ç¬¦å·ï¼Œå› ä¸ºç­–ç•¥æ¢¯åº¦æ˜¾ç¤ºäº†ç­–ç•¥æ”¹è¿›çš„æ–¹å‘ï¼Œä½†å€¼æŸå¤±å’Œç†µæŸå¤±åº”è¯¥æœ€å°åŒ–ã€‚
- en: To improve stability, itâ€™s worth using several environments, providing you with
    observations concurrently (when you have multiple environments, your training
    batch will be created from their observations). We will look at several ways of
    doing this later in this chapter when we discuss the A3C method.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜ç¨³å®šæ€§ï¼Œå€¼å¾—ä½¿ç”¨å¤šä¸ªç¯å¢ƒï¼Œæä¾›å¹¶è¡Œçš„è§‚å¯Ÿæ•°æ®ï¼ˆå½“ä½ æœ‰å¤šä¸ªç¯å¢ƒæ—¶ï¼Œè®­ç»ƒæ‰¹æ¬¡å°†ä»è¿™äº›è§‚å¯Ÿæ•°æ®ä¸­åˆ›å»ºï¼‰ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åç»­è®¨è®º A3C æ–¹æ³•æ—¶æ¢è®¨å‡ ç§å®ç°æ–¹å¼ã€‚
- en: The version of the preceding method that uses several environments running in
    parallel is called advantage asynchronous actor-critic, which is also known as
    A3C. The A3C method will be discussed later, but for now, letâ€™s implement A2C.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢æ–¹æ³•çš„ç‰ˆæœ¬ï¼Œé€šè¿‡å¹¶è¡Œè¿è¡Œå¤šä¸ªç¯å¢ƒæ¥å®ç°ï¼Œç§°ä¸ºä¼˜åŠ¿å¼‚æ­¥æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼Œä¹Ÿè¢«ç§°ä¸º A3Cã€‚A3C æ–¹æ³•å°†åœ¨åç»­è®¨è®ºï¼Œä½†ç°åœ¨ï¼Œæˆ‘ä»¬å…ˆå®ç° A2Cã€‚
- en: A2C on Pong
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A2C åœ¨ Pong ä¸­çš„åº”ç”¨
- en: In the previous chapter, you saw a (not very successful) attempt to solve our
    favorite Pong environment with policy gradient methods. Letâ€™s try it again with
    the actor-critic method at hand. The full source code is available in Chapter12/02_pong_a2c.py.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç« ä¸­ï¼Œä½ çœ‹åˆ°äº†ä¸€æ¬¡ï¼ˆä¸å¤ªæˆåŠŸçš„ï¼‰å°è¯•ï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•è§£å†³æˆ‘ä»¬æœ€å–œæ¬¢çš„ Pong ç¯å¢ƒã€‚è®©æˆ‘ä»¬å†å°è¯•ä¸€ä¸‹ï¼Œæ‰‹å¤´æœ‰æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ã€‚å®Œæ•´çš„æºä»£ç å¯ä»¥åœ¨
    Chapter12/02_pong_a2c.py ä¸­æ‰¾åˆ°ã€‚
- en: 'We start, as usual, by defining hyperparameters (imports are omitted):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·ï¼Œä»å®šä¹‰è¶…å‚æ•°å¼€å§‹ï¼ˆçœç•¥äº†å¯¼å…¥éƒ¨åˆ†ï¼‰ï¼š
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These values are not tuned, which is left as an exercise for the reader. We
    have one new value here: CLIP_GRAD. This hyperparameter specifies the threshold
    for gradient clipping, which basically prevents our gradients from becoming too
    large at the optimization stage and pushing our policy too far. Clipping is implemented
    using the PyTorch functionality, but the idea is very simple: if the L2 norm of
    the gradient is larger than this hyperparameter, then the gradient vector is clipped
    to this value.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å€¼å¹¶æœªè°ƒæ•´ï¼Œè¿™éƒ¨åˆ†ç•™ç»™è¯»è€…è‡ªå·±å®Œæˆã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ–°çš„å€¼ï¼šCLIP_GRADã€‚è¿™ä¸ªè¶…å‚æ•°æŒ‡å®šäº†æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼ï¼ŒåŸºæœ¬ä¸Šå®ƒé˜²æ­¢äº†åœ¨ä¼˜åŒ–é˜¶æ®µæ¢¯åº¦å˜å¾—è¿‡å¤§ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„ç­–ç•¥è¿‡äºåç¦»ã€‚è£å‰ªæ˜¯ä½¿ç”¨PyTorchçš„åŠŸèƒ½å®ç°çš„ï¼Œä½†è¿™ä¸ªæ¦‚å¿µéå¸¸ç®€å•ï¼šå¦‚æœæ¢¯åº¦çš„L2èŒƒæ•°å¤§äºè¿™ä¸ªè¶…å‚æ•°ï¼Œåˆ™æ¢¯åº¦å‘é‡ä¼šè¢«è£å‰ªåˆ°è¿™ä¸ªå€¼ã€‚
- en: The REWARD_STEPS hyperparameter determines how many steps ahead we will take
    to approximate the total discounted reward for every action.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: REWARD_STEPSè¶…å‚æ•°ç¡®å®šæˆ‘ä»¬å°†å‘å‰èµ°å¤šå°‘æ­¥ï¼Œä»¥è¿‘ä¼¼æ¯ä¸ªè¡ŒåŠ¨çš„æ€»æŠ˜æ‰£å¥–åŠ±ã€‚
- en: 'In the policy gradient methods, we used about 10 steps, but in A2C, we will
    use our value approximation to get a state value for further steps, so it will
    be fine to decrease the number of steps. The following is our network architecture:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤§çº¦10æ­¥ï¼Œä½†åœ¨A2Cä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„å€¼è¿‘ä¼¼æ¥è·å¾—è¿›ä¸€æ­¥æ­¥éª¤çš„çŠ¶æ€å€¼ï¼Œå› æ­¤å‡å°‘æ­¥æ•°æ˜¯å¯ä»¥çš„ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„ç½‘ç»œæ¶æ„ï¼š
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It has a shared convolution body and two heads: the first returns the policy
    with the probability distribution over our actions and the second head returns
    one single number, which will approximate the stateâ€™s value. It might look similar
    to our dueling DQN architecture from ChapterÂ [8](ch012.xhtml#x1-1240008), but
    our training procedure is different.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå…·æœ‰å…±äº«çš„å·ç§¯ä½“å’Œä¸¤ä¸ªå¤´éƒ¨ï¼šç¬¬ä¸€ä¸ªè¿”å›åŒ…å«æˆ‘ä»¬è¡ŒåŠ¨æ¦‚ç‡åˆ†å¸ƒçš„ç­–ç•¥ï¼Œç¬¬äºŒä¸ªå¤´éƒ¨è¿”å›ä¸€ä¸ªå•ä¸€æ•°å­—ï¼Œè¯¥æ•°å­—å°†è¿‘ä¼¼äºçŠ¶æ€çš„å€¼ã€‚å®ƒå¯èƒ½çœ‹èµ·æ¥ä¸æˆ‘ä»¬åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­æåˆ°çš„å¯¹æŠ—æ€§DQNæ¶æ„ç›¸ä¼¼ï¼Œä½†æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹ä¸åŒã€‚
- en: 'The forward pass through the network returns a tuple of two tensors â€“ policy
    and value:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œçš„å‰å‘ä¼ é€’è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„â€”â€”ç­–ç•¥å’Œå€¼ï¼š
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we have to discuss a large and important function, which takes the batch
    of environment transitions and returns three tensors: the batch of states, batch
    of actions taken, and batch of Q-values calculated using the formula Q(s,a) =
    âˆ‘ [i=0]^(Nâˆ’1)Î³^ir[i] + Î³^NV (s[N]). This Q-value will be used in two places: to
    calculate mean squared error (MSE) loss to improve the value approximation in
    the same way as DQN, and to calculate the advantage of the action.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦è®¨è®ºä¸€ä¸ªé‡è¦çš„å¤§å‡½æ•°ï¼Œå®ƒæ¥å—ç¯å¢ƒè½¬ç§»çš„æ‰¹æ¬¡å¹¶è¿”å›ä¸‰ä¸ªå¼ é‡ï¼šçŠ¶æ€æ‰¹æ¬¡ã€é‡‡å–çš„è¡ŒåŠ¨æ‰¹æ¬¡å’Œä½¿ç”¨å…¬å¼Q(s,a) = âˆ‘ [i=0]^(Nâˆ’1)Î³^ir[i]
    + Î³^NV(s[N])è®¡ç®—çš„Qå€¼æ‰¹æ¬¡ã€‚è¿™ä¸ªQå€¼å°†åœ¨ä¸¤ä¸ªåœ°æ–¹ä½¿ç”¨ï¼šè®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ä»¥æ”¹å–„å€¼çš„è¿‘ä¼¼ï¼Œå°±åƒDQNä¸€æ ·ï¼›ä»¥åŠè®¡ç®—è¡ŒåŠ¨çš„ä¼˜åŠ¿ã€‚
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the beginning, we just walk through our batch of transitions and copy their
    fields into the lists. Note that the reward value already contains the discounted
    reward for REWARD_STEPS, as we use the ptan.ExperienceSourceFirstLast class. We
    also need to handle episode-ending situations and remember indices of batch entries
    for non-terminal episodes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬åªéœ€è¦éå†æˆ‘ä»¬çš„è½¬ç§»æ‰¹æ¬¡å¹¶å°†å®ƒä»¬çš„å­—æ®µå¤åˆ¶åˆ°åˆ—è¡¨ä¸­ã€‚æ³¨æ„ï¼Œå¥–åŠ±å€¼å·²ç»åŒ…å«äº†REWARD_STEPSçš„æŠ˜æ‰£å¥–åŠ±ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†ptan.ExperienceSourceFirstLastç±»ã€‚æˆ‘ä»¬è¿˜éœ€è¦å¤„ç†å›åˆç»“æŸçš„æƒ…å†µï¼Œå¹¶è®°ä½éç»ˆæ­¢å›åˆçš„æ‰¹æ¬¡æ¡ç›®ç´¢å¼•ã€‚
- en: 'In the following code, we convert the gathered state and actions into a PyTorch
    tensor and copy them into the graphics processing unit (GPU) if needed:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†æ”¶é›†åˆ°çš„çŠ¶æ€å’ŒåŠ¨ä½œè½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶æ ¹æ®éœ€è¦å°†å…¶å¤åˆ¶åˆ°å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ä¸­ï¼š
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the extra call to np.asarray() might look redundant, but without it,
    the performance of tensor creation degrades 5-10x. This is known as [issue #13918](https://github.com/pytorch/pytorch/issues/13918)
    in PyTorch, and at the time of writing, it hasnâ€™t been solved, so one solution
    is to pass a single NumPy array instead of a list of arrays.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™é‡Œï¼Œå¯¹np.asarray()çš„é¢å¤–è°ƒç”¨å¯èƒ½çœ‹èµ·æ¥æ˜¯å¤šä½™çš„ï¼Œä½†æ²¡æœ‰å®ƒï¼Œå¼ é‡åˆ›å»ºçš„æ€§èƒ½ä¼šé™ä½5åˆ°10å€ã€‚è¿™åœ¨PyTorchä¸­è¢«ç§°ä¸º[é—®é¢˜ #13918](https://github.com/pytorch/pytorch/issues/13918)ï¼Œå¹¶ä¸”åœ¨å†™ä½œæ—¶å°šæœªè§£å†³ï¼Œå› æ­¤ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä¼ é€’ä¸€ä¸ªå•ä¸€çš„NumPyæ•°ç»„ï¼Œè€Œä¸æ˜¯æ•°ç»„åˆ—è¡¨ã€‚'
- en: 'The rest of the function calculates Q-values, taking into account the terminal
    episodes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†è®¡ç®—Qå€¼ï¼Œè€ƒè™‘äº†ç»ˆæ­¢å›åˆçš„æƒ…å†µï¼š
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code prepares the variable with the last state in our transition
    chain and queries our network for V (s) approximation. Then, this value is multiplied
    by the discount factor and added to the immediate rewards.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„ä»£ç å‡†å¤‡äº†å˜é‡ï¼Œå­˜å‚¨æˆ‘ä»¬è½¬ç§»é“¾ä¸­çš„æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œå¹¶æŸ¥è¯¢æˆ‘ä»¬çš„ç½‘ç»œä»¥è·å–V(s)çš„è¿‘ä¼¼å€¼ã€‚ç„¶åï¼Œå°†è¯¥å€¼ä¹˜ä»¥æŠ˜æ‰£å› å­å¹¶åŠ ä¸Šå³æ—¶å¥–åŠ±ã€‚
- en: 'At the end of the function, we pack our Q-values into the tensor and return
    it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡½æ•°çš„æœ«å°¾ï¼Œæˆ‘ä»¬å°†Qå€¼æ‰“åŒ…åˆ°å¼ é‡ä¸­å¹¶è¿”å›ï¼š
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following code, you can notice a new way to create environments, the
    class gym.vector.SyncVectorEnv, which is being passed a list of lambda functions
    creating the underlying environments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç ä¸­ï¼Œä½ å¯ä»¥æ³¨æ„åˆ°ä¸€ç§æ–°çš„åˆ›å»ºç¯å¢ƒçš„æ–¹å¼ï¼Œä½¿ç”¨ç±»gym.vector.SyncVectorEnvï¼Œå®ƒä¼ å…¥ä¸€ä¸ªåŒ…å«åˆ›å»ºåº•å±‚ç¯å¢ƒçš„lambdaå‡½æ•°çš„åˆ—è¡¨ï¼š
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The class gym.vector.SyncVectorEnv is provided by Gymnasium and allows wrapping
    several environments into one single â€œvectorizedâ€ environment. Underlying environments
    have to have identical action and observation spaces, which allows the vectorized
    environment to accept a vector of actions and return batches of observations and
    rewards. You can find more details in the Gymnasium documentation: [https://gymnasium.farama.org/api/vector/](https://gymnasium.farama.org/api/vector/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»gym.vector.SyncVectorEnvæ˜¯Gymnasiumæä¾›çš„ï¼Œå…è®¸å°†å¤šä¸ªç¯å¢ƒå°è£…æˆä¸€ä¸ªå•ä¸€çš„â€œå‘é‡åŒ–â€ç¯å¢ƒã€‚åº•å±‚ç¯å¢ƒå¿…é¡»å…·æœ‰ç›¸åŒçš„åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´ï¼Œè¿™ä½¿å¾—å‘é‡åŒ–ç¯å¢ƒèƒ½å¤Ÿæ¥å—ä¸€ç»„åŠ¨ä½œå¹¶è¿”å›ä¸€æ‰¹è§‚å¯Ÿå’Œå¥–åŠ±ã€‚ä½ å¯ä»¥åœ¨Gymnasiumæ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šç»†èŠ‚ï¼š[https://gymnasium.farama.org/api/vector/](https://gymnasium.farama.org/api/vector/)ã€‚
- en: Synchronized vectorized environments (the SyncVectorEnv class) are almost identical
    to the optimization we used in ChapterÂ [9](ch013.xhtml#x1-1600009), in the section
    Several environments, where we passed multiple gym environments into the experience
    source to increase the performance of the DQN training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ­¥å‘é‡åŒ–ç¯å¢ƒï¼ˆSyncVectorEnvç±»ï¼‰å‡ ä¹ä¸æˆ‘ä»¬åœ¨ç¬¬[9](ch013.xhtml#x1-1600009)ç« â€œå¤šä¸ªç¯å¢ƒâ€éƒ¨åˆ†ä¸­ä½¿ç”¨çš„ä¼˜åŒ–å®Œå…¨ç›¸åŒï¼Œå½“æ—¶æˆ‘ä»¬å°†å¤šä¸ªgymç¯å¢ƒä¼ å…¥ç»éªŒæºä»¥æé«˜DQNè®­ç»ƒçš„æ€§èƒ½ã€‚
- en: 'But in the case of vectorized environments, a different experience source class
    has to be used: VectorExperienceSourceFirstLast, which takes into account vectorization
    and optimizes the agent application to the observation. From the outside, the
    interface of this experience source is exactly as before.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨å‘é‡åŒ–ç¯å¢ƒçš„æƒ…å†µä¸‹ï¼Œå¿…é¡»ä½¿ç”¨ä¸åŒçš„ç»éªŒæºç±»ï¼šVectorExperienceSourceFirstLastï¼Œå®ƒè€ƒè™‘äº†å‘é‡åŒ–ï¼Œå¹¶ä¼˜åŒ–äº†ä»£ç†å¯¹è§‚å¯Ÿçš„åº”ç”¨ã€‚ä»å¤–éƒ¨çœ‹ï¼Œè¿™ä¸ªç»éªŒæºçš„æ¥å£ä¸ä¹‹å‰å®Œå…¨ç›¸åŒã€‚
- en: The command-line argument --use-async (which switches our wrapper class from
    SyncVectorEnv to AsyncVectorEnv) is not relevant at the moment â€“ we will use it
    later, when discussing the A3C method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å‘½ä»¤è¡Œå‚æ•°--use-asyncï¼ˆå®ƒå°†æˆ‘ä»¬çš„åŒ…è£…ç±»ä»SyncVectorEnvåˆ‡æ¢ä¸ºAsyncVectorEnvï¼‰ç›®å‰ä¸ç›¸å…³â€”â€”æˆ‘ä»¬ç¨åä¼šä½¿ç”¨å®ƒï¼Œåœ¨è®¨è®ºA3Cæ–¹æ³•æ—¶ã€‚
- en: 'Then, we create the network, agent, and experience source:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºç½‘ç»œã€ä»£ç†å’Œç»éªŒæºï¼š
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One very important detail here is passing the eps parameter to the optimizer.
    If youâ€™re familiar with the Adam algorithm, you may know that epsilon is a small
    number added to the denominator to prevent zero-division situations. Normally,
    this value is set to some small number, such as 10^(âˆ’8) or 10^(âˆ’10), but in our
    case, these values turned out to be too small. I have no mathematically strict
    explanation for this, but with the default value of epsilon, the method does not
    converge at all. Very likely, the division to a small value of 10^(âˆ’8) makes the
    gradients too large, which turns out to be fatal for training stability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„ç»†èŠ‚æ˜¯å°†epså‚æ•°ä¼ é€’ç»™ä¼˜åŒ–å™¨ã€‚å¦‚æœä½ ç†Ÿæ‚‰Adamç®—æ³•ï¼Œä½ å¯èƒ½çŸ¥é“epsilonæ˜¯ä¸€ä¸ªåŠ åˆ°åˆ†æ¯ä¸Šçš„å°æ•°ï¼Œç”¨æ¥é˜²æ­¢é›¶é™¤é”™è¯¯ã€‚é€šå¸¸ï¼Œè¿™ä¸ªå€¼è®¾ç½®ä¸ºä¸€äº›å°æ•°å­—ï¼Œå¦‚10^(-8)æˆ–10^(-10)ï¼Œä½†åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œè¿™äº›å€¼å¤ªå°äº†ã€‚æˆ‘æ²¡æœ‰ä¸¥æ ¼çš„æ•°å­¦è§£é‡Šï¼Œä½†ä½¿ç”¨é»˜è®¤çš„epsilonå€¼æ—¶ï¼Œæ–¹æ³•æ ¹æœ¬æ— æ³•æ”¶æ•›ã€‚å¾ˆå¯èƒ½ï¼Œé™¤ä»¥ä¸€ä¸ªå°å€¼10^(-8)ä¼šå¯¼è‡´æ¢¯åº¦è¿‡å¤§ï¼Œè¿™å¯¹è®­ç»ƒç¨³å®šæ€§æ¥è¯´æ˜¯è‡´å‘½çš„ã€‚
- en: Another detail is to use VectorExperienceSourceFirstLast instead of ExperienceSourceFirstLast.
    This is required because of the vectorized environment wrapping several normal
    Atari environments. The vectorized environment also exposes the attributes single_observation_space
    and single_action_space, which are the observation and action spaces of an individual
    environment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªç»†èŠ‚æ˜¯ä½¿ç”¨VectorExperienceSourceFirstLastè€Œä¸æ˜¯ExperienceSourceFirstLastã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå‘é‡åŒ–ç¯å¢ƒå°†å¤šä¸ªæ™®é€šçš„Atariç¯å¢ƒå°è£…åœ¨ä¸€èµ·ã€‚å‘é‡åŒ–ç¯å¢ƒè¿˜æš´éœ²äº†single_observation_spaceå’Œsingle_action_spaceè¿™ä¸¤ä¸ªå±æ€§ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯å•ä¸ªç¯å¢ƒçš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ã€‚
- en: 'In the training loop, we use two wrappers:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåŒ…è£…å™¨ï¼š
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The first wrapper in this code is already familiar to you: common.RewardTracker,
    which computes the mean reward for the last 100 episodes and tells us when this
    mean reward exceeds the desired threshold. Another wrapper, TBMeanTracker, is
    from the PTAN library and is responsible for writing into TensorBoard the mean
    of the measured parameters for the last 10 steps. This is helpful, as training
    can take millions of steps and we donâ€™t want to write millions of points into
    TensorBoard, but rather write smoothed values every 10 steps.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ä¸­çš„ç¬¬ä¸€ä¸ªåŒ…è£…å™¨ä½ å·²ç»å¾ˆç†Ÿæ‚‰ï¼šcommon.RewardTrackerï¼Œå®ƒè®¡ç®—æœ€å100ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬å½“è¿™ä¸ªå¹³å‡å¥–åŠ±è¶…è¿‡æ‰€éœ€é˜ˆå€¼æ—¶ã€‚å¦ä¸€ä¸ªåŒ…è£…å™¨TBMeanTrackeræ¥è‡ªPTANåº“ï¼Œè´Ÿè´£å°†æœ€å10æ­¥ä¸­æµ‹é‡çš„å‚æ•°çš„å¹³å‡å€¼å†™å…¥TensorBoardã€‚è¿™æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ï¼Œå› ä¸ºè®­ç»ƒå¯èƒ½éœ€è¦ä¸Šç™¾ä¸‡æ­¥ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ¯ä¸€æ­¥éƒ½å†™å…¥TensorBoardï¼Œè€Œæ˜¯æ¯10æ­¥å†™å…¥å¹³æ»‘åçš„å€¼ã€‚
- en: 'The next code chunk is responsible for our calculation of losses, which is
    the core of the A2C method. First, we unpack our batch using the function we described
    earlier and ask our network to return the policy and values for this batch:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ®µä»£ç è´Ÿè´£æˆ‘ä»¬è®¡ç®—æŸå¤±çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯A2Cæ–¹æ³•çš„æ ¸å¿ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰æè¿°çš„å‡½æ•°è§£åŒ…æ‰¹æ¬¡ï¼Œå¹¶è¦æ±‚ç½‘ç»œè¿”å›è¯¥æ‰¹æ¬¡çš„ç­–ç•¥å’Œå€¼ï¼š
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The policy is returned in an unnormalized form, so to convert it into the probability
    distribution, we need to apply softmax to it. As the policy loss requires the
    logarithm of the probability distribution, we will use the function log_softmax,
    which is more numerically stable than calling softmax and then log.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥ä»¥æœªå½’ä¸€åŒ–çš„å½¢å¼è¿”å›ï¼Œå› æ­¤ä¸ºäº†å°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶åº”ç”¨softmaxã€‚ç”±äºç­–ç•¥æŸå¤±éœ€è¦æ¦‚ç‡åˆ†å¸ƒçš„å¯¹æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨log_softmaxå‡½æ•°ï¼Œè¿™æ¯”å…ˆè°ƒç”¨softmaxå†å–å¯¹æ•°æ›´åŠ ç¨³å®šã€‚
- en: 'In the value loss part, we calculate the MSE between the value returned by
    our network and the approximation we performed using the Bellman equation unrolled
    four steps forward:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»·å€¼æŸå¤±éƒ¨åˆ†ï¼Œæˆ‘ä»¬è®¡ç®—ç½‘ç»œè¿”å›çš„å€¼ä¸æˆ‘ä»¬é€šè¿‡å±•å¼€å››æ­¥çš„è´å°”æ›¼æ–¹ç¨‹æ‰€è¿›è¡Œçš„è¿‘ä¼¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we calculate the policy loss to obtain the policy gradient:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—ç­–ç•¥æŸå¤±ä»¥è·å¾—ç­–ç•¥æ¢¯åº¦ï¼š
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first two steps obtain a log of our policy and calculate the advantage of
    actions, which is A(s,a) = Q(s,a) âˆ’V (s). The call to value_t.detach() is important,
    as we donâ€™t want to propagate the policy gradient into our value approximation
    head. Then, we take the log of probability for the actions taken and scale them
    with the advantage. Our policy gradient loss value will be equal to the negated
    mean of this scaled log of policy, as the policy gradient directs us toward policy
    improvement, but loss value is supposed to be minimized.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤æ­¥è·å¾—æˆ‘ä»¬ç­–ç•¥çš„æ—¥å¿—å¹¶è®¡ç®—è¡ŒåŠ¨çš„ä¼˜åŠ¿ï¼Œä¼˜åŠ¿A(s,a) = Q(s,a) âˆ’V (s)ã€‚è°ƒç”¨ value_t.detach() å¾ˆé‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›å°†ç­–ç•¥æ¢¯åº¦ä¼ æ’­åˆ°æˆ‘ä»¬çš„ä»·å€¼è¿‘ä¼¼å¤´éƒ¨ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹é‡‡å–çš„è¡ŒåŠ¨çš„æ¦‚ç‡å–å¯¹æ•°ï¼Œå¹¶ç”¨ä¼˜åŠ¿å¯¹å…¶è¿›è¡Œç¼©æ”¾ã€‚æˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦æŸå¤±å€¼å°†ç­‰äºè¯¥ç¼©æ”¾åçš„ç­–ç•¥å¯¹æ•°çš„è´Ÿå‡å€¼ï¼Œå› ä¸ºç­–ç•¥æ¢¯åº¦å¼•å¯¼æˆ‘ä»¬æœç€ç­–ç•¥æ”¹è¿›çš„æ–¹å‘ï¼Œä½†æŸå¤±å€¼åº”è¯¥æœ€å°åŒ–ã€‚
- en: 'The last piece of our loss function is entropy loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŸå¤±å‡½æ•°çš„æœ€åä¸€éƒ¨åˆ†æ˜¯ç†µæŸå¤±ï¼š
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Entropy loss is equal to the scaled entropy of our policy, taken with the opposite
    sign (entropy is calculated as H(Ï€) = âˆ’âˆ‘ Ï€ log Ï€).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µæŸå¤±ç­‰äºæˆ‘ä»¬ç­–ç•¥çš„ç¼©æ”¾ç†µï¼Œå¹¶å–å…¶ç›¸åç¬¦å·ï¼ˆç†µçš„è®¡ç®—å…¬å¼æ˜¯H(Ï€) = âˆ’âˆ‘ Ï€ log Ï€ï¼‰ã€‚
- en: 'In the following code, we calculate and extract gradients of our policy, which
    will be used to track the maximum gradient, its variance, and the L2 norm:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—å¹¶æå–æˆ‘ä»¬ç­–ç•¥çš„æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦å°†ç”¨äºè¿½è¸ªæœ€å¤§æ¢¯åº¦ã€å…¶æ–¹å·®å’ŒL2èŒƒæ•°ï¼š
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As the final step of our training, we backpropagate the entropy loss and the
    value loss, clip gradients, and ask our optimizer to update the network:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè®­ç»ƒçš„æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬åå‘ä¼ æ’­ç†µæŸå¤±å’Œä»·å€¼æŸå¤±ï¼Œè£å‰ªæ¢¯åº¦ï¼Œå¹¶è¦æ±‚ä¼˜åŒ–å™¨æ›´æ–°ç½‘ç»œï¼š
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At the end of the training loop, we track all of the values that we are going
    to monitor in TensorBoard:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯çš„æœ€åï¼Œæˆ‘ä»¬è¿½è¸ªæ‰€æœ‰éœ€è¦åœ¨TensorBoardä¸­ç›‘æ§çš„å€¼ï¼š
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are plenty of values that we need to monitor and we will discuss them
    in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šå€¼éœ€è¦ç›‘æ§ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­è®¨è®ºå®ƒä»¬ã€‚
- en: Results
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'To start the training, run 02_pong_a2c.py with the --dev (for GPU) and -n options
    (which provides a name for the run for TensorBoard):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹è®­ç»ƒï¼Œè¯·è¿è¡Œ 02_pong_a2c.py å¹¶ä½¿ç”¨ --devï¼ˆè¡¨ç¤ºä½¿ç”¨GPUï¼‰å’Œ -n é€‰é¡¹ï¼ˆä¸ºTensorBoardæä¾›ä¸€ä¸ªè¿è¡Œåç§°ï¼‰ï¼š
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As a word of warning, the training process is lengthy. With the original hyperparameters,
    it requires about 10 million frames to solve, which is about three hours on a
    GPU.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè­¦å‘Šï¼Œè®­ç»ƒè¿‡ç¨‹æ¯”è¾ƒæ¼«é•¿ã€‚ä½¿ç”¨åŸå§‹è¶…å‚æ•°ï¼Œå®ƒå¤§çº¦éœ€è¦1000ä¸‡å¸§æ¥è§£å†³é—®é¢˜ï¼Œå¤§çº¦åœ¨GPUä¸Šéœ€è¦ä¸‰ä¸ªå°æ—¶ã€‚
- en: Later in the chapter, weâ€™ll check the asynchronous version of the A2C method,
    which executes the environment in a separate process (which increases both training
    stability and performance). But first, letâ€™s focus on our plots in TensorBoard.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« åé¢ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹A2Cæ–¹æ³•çš„å¼‚æ­¥ç‰ˆæœ¬ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ç‹¬çš„è¿›ç¨‹ä¸­æ‰§è¡Œç¯å¢ƒï¼ˆè¿™æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼‰ã€‚ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬é›†ä¸­å…³æ³¨TensorBoardä¸­çš„å›¾è¡¨ã€‚
- en: 'The reward dynamics look much better than in the example from the previous
    chapter:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±åŠ¨æ€æ¯”ä¸Šä¸€ç« çš„ç¤ºä¾‹è¦å¥½å¾—å¤šï¼š
- en: '![PIC](img/B22150_12_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_06.png)'
- en: 'FigureÂ 12.6: Smoothed reward (left) and mean batch values (right)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 12.6ï¼šå¹³æ»‘å¥–åŠ±ï¼ˆå·¦ä¾§ï¼‰å’Œå¹³å‡æ‰¹æ¬¡å€¼ï¼ˆå³ä¾§ï¼‰
- en: The left plot is the mean training episodes reward averaged over the 100 last
    episodes. The right plot, â€œbatch value,â€ shows Q-values approximated using the
    Bellman equation and an overall positive dynamic in Q approximation. This shows
    that our training process is improving more or less consistently over time.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¾§çš„å›¾æ˜¯è¿‡å»100ä¸ªè®­ç»ƒå›åˆçš„å¹³å‡å¥–åŠ±ã€‚å³ä¾§çš„å›¾ï¼Œâ€œæ‰¹æ¬¡å€¼â€ï¼Œå±•ç¤ºäº†ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è¿‘ä¼¼çš„Qå€¼ä»¥åŠQè¿‘ä¼¼çš„æ•´ä½“æ­£å‘åŠ¨æ€ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹åœ¨æ—¶é—´ä¸ŠåŸºæœ¬ä¸Šæ˜¯æŒç»­æ”¹è¿›çš„ã€‚
- en: 'The next four charts are related to our loss and include the individual loss
    components and the total loss:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„å››ä¸ªå›¾ä¸æˆ‘ä»¬çš„æŸå¤±ç›¸å…³ï¼ŒåŒ…å«äº†å„ä¸ªæŸå¤±ç»„ä»¶å’Œæ€»æŸå¤±ï¼š
- en: '![PIC](img/B22150_12_07.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_07.png)'
- en: 'FigureÂ 12.7: Entropy loss (left) and policy loss (right)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 12.7ï¼šç†µæŸå¤±ï¼ˆå·¦ä¾§ï¼‰å’Œç­–ç•¥æŸå¤±ï¼ˆå³ä¾§ï¼‰
- en: '![PIC](img/B22150_12_08.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_08.png)'
- en: 'FigureÂ 12.8: Value loss (left) and total loss (right)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 12.8ï¼šä»·å€¼æŸå¤±ï¼ˆå·¦ä¾§ï¼‰å’Œæ€»æŸå¤±ï¼ˆå³ä¾§ï¼‰
- en: 'Here, we must note the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¿…é¡»æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: First, our value loss (FigureÂ [12.8](#x1-208037r8), on the left) is decreasing
    consistently, which shows that our V (s) approximation is improving during the
    training.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬çš„ä»·å€¼æŸå¤±ï¼ˆå›¾Â [12.8](#x1-208037r8)ï¼Œåœ¨å·¦ä¾§ï¼‰æŒç»­å‡å°‘ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„V(s)è¿‘ä¼¼å€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾—åˆ°äº†æ”¹å–„ã€‚
- en: The second observation is that our entropy loss (FigureÂ [12.7](#x1-208036r7),
    on the left) is growing in the middle of the training, but it doesnâ€™t dominate
    in the total loss. This basically means that our agent becomes more confident
    in its actions as the policy becomes less uniform.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªè§‚å¯Ÿç»“æœæ˜¯æˆ‘ä»¬çš„ç†µæŸå¤±ï¼ˆå›¾Â [12.7](#x1-208036r7)ï¼Œå·¦ä¾§ï¼‰åœ¨è®­ç»ƒçš„ä¸­æœŸå¢é•¿ï¼Œä½†å®ƒåœ¨æ€»æŸå¤±ä¸­å¹¶ä¸å ä¸»å¯¼åœ°ä½ã€‚è¿™åŸºæœ¬ä¸Šæ„å‘³ç€éšç€ç­–ç•¥å˜å¾—ä¸å†å‡åŒ€ï¼Œæˆ‘ä»¬çš„ä»£ç†åœ¨å…¶åŠ¨ä½œä¸Šå˜å¾—æ›´åŠ è‡ªä¿¡ã€‚
- en: The last thing to note here is that policy loss (FigureÂ [12.7](#x1-208036r7),
    on the right) is decreasing most of the time and is correlated to the total loss,
    which is good, as we are interested in the gradients for our policy first of all.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ€åéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç­–ç•¥æŸå¤±ï¼ˆå›¾Â [12.7](#x1-208036r7)ï¼Œå³ä¾§ï¼‰å¤§å¤šæ•°æ—¶å€™åœ¨å‡å°‘ï¼Œå¹¶ä¸”ä¸æ€»æŸå¤±ç›¸å…³è”ï¼Œè¿™æ˜¯å¥½çš„ï¼Œå› ä¸ºæˆ‘ä»¬é¦–å…ˆå…³æ³¨çš„æ˜¯æˆ‘ä»¬ç­–ç•¥çš„æ¢¯åº¦ã€‚
- en: 'The last set of plots displays the advantage value and policy gradient metrics:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ç»„å›¾æ˜¾ç¤ºäº†ä¼˜åŠ¿å€¼å’Œç­–ç•¥æ¢¯åº¦åº¦é‡ï¼š
- en: '![PIC](img/B22150_12_09.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_09.png)'
- en: 'FigureÂ 12.9: Advantage (left) and L2 of gradients (right)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 12.9ï¼šä¼˜åŠ¿ï¼ˆå·¦ä¾§ï¼‰å’Œæ¢¯åº¦çš„L2èŒƒæ•°ï¼ˆå³ä¾§ï¼‰
- en: '![PIC](img/B22150_12_10.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_12_10.png)'
- en: 'FigureÂ 12.10: Max of gradients (left) and gradients variance (right)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 12.10ï¼šæ¢¯åº¦çš„æœ€å¤§å€¼ï¼ˆå·¦ä¾§ï¼‰å’Œæ¢¯åº¦æ–¹å·®ï¼ˆå³ä¾§ï¼‰
- en: The advantage is a scale of our policy gradients, and it equals Q(s,a) âˆ’V (s).
    We expect it to oscillate around 0 (because, on average, the effect of the single
    action on the stateâ€™s value shouldnâ€™t be large), and the chart meets our expectations.
    The gradient charts demonstrate that our gradients are not too small and not too
    large. Variance is very small at the beginning of the training (for 2 million
    frames), but starts to grow later, which means that our policy is changing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿æ˜¯æˆ‘ä»¬ç­–ç•¥æ¢¯åº¦çš„å°ºåº¦ï¼Œå®ƒç­‰äºQ(s,a) âˆ’ V(s)ã€‚æˆ‘ä»¬æœŸæœ›å®ƒåœ¨0å‘¨å›´æ³¢åŠ¨ï¼ˆå› ä¸ºä»å¹³å‡è€Œè¨€ï¼Œå•ä¸€åŠ¨ä½œå¯¹çŠ¶æ€å€¼çš„å½±å“ä¸åº”è¯¥å¾ˆå¤§ï¼‰ï¼Œè€Œå›¾è¡¨ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚æ¢¯åº¦å›¾è¡¨è¡¨æ˜æˆ‘ä»¬çš„æ¢¯åº¦æ—¢ä¸å¤ªå°ä¹Ÿä¸å¤ªå¤§ã€‚æ–¹å·®åœ¨è®­ç»ƒçš„æœ€åˆé˜¶æ®µï¼ˆå‰200ä¸‡å¸§ï¼‰éå¸¸å°ï¼Œä½†åæ¥å¼€å§‹å¢é•¿ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„ç­–ç•¥åœ¨å‘ç”Ÿå˜åŒ–ã€‚
- en: Asynchronous Advantage Actor-Critic (A3C)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå‘˜ï¼ˆA3Cï¼‰
- en: In this section, we will extend the A2C method. This extension adds true asynchronous
    environment interaction, and is called asynchronous advantage actor-critic (A3C).
    This method is one of the most widely used by RL practitioners.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ‰©å±•A2Cæ–¹æ³•ã€‚è¿™ä¸ªæ‰©å±•åŠ å…¥äº†çœŸæ­£çš„å¼‚æ­¥ç¯å¢ƒäº¤äº’ï¼Œè¢«ç§°ä¸ºå¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå‘˜ï¼ˆA3Cï¼‰ã€‚è¯¥æ–¹æ³•æ˜¯RLå®è·µè€…æœ€å¹¿æ³›ä½¿ç”¨çš„ç®—æ³•ä¹‹ä¸€ã€‚
- en: 'We will take a look at two approaches for adding asynchronous behavior to the
    basic A2C method: data-level and gradient-level parallelism. They have different
    resource requirements and characteristics, which makes them applicable to different
    situations.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»‹ç»ä¸¤ç§ä¸ºåŸºç¡€A2Cæ–¹æ³•æ·»åŠ å¼‚æ­¥è¡Œä¸ºçš„æ–¹å¼ï¼šæ•°æ®çº§å¹¶è¡Œå’Œæ¢¯åº¦çº§å¹¶è¡Œã€‚å®ƒä»¬æœ‰ä¸åŒçš„èµ„æºéœ€æ±‚å’Œç‰¹ç‚¹ï¼Œè¿™ä½¿å¾—å®ƒä»¬é€‚ç”¨äºä¸åŒçš„æƒ…å†µã€‚
- en: Correlation and sample efficiency
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸å…³æ€§ä¸æ ·æœ¬æ•ˆç‡
- en: One of the approaches to improving the stability of the policy gradient family
    of methods is using multiple environments in parallel. The reason behind this
    is the fundamental problem we discussed in ChapterÂ [6](#), when we talked about
    the correlation between samples, which breaks the independent and identically
    distributed (iid) assumption, which is critical for stochastic gradient descent
    (SGD) optimization. The negative consequence of such correlation is very high
    variance in gradients, which means that our training batch contains very similar
    examples, all of them pushing our network in the same direction. However, this
    may be totally the wrong direction in the global sense, as all those examples
    may be from one single lucky or unlucky episode.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¹è¿›ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç¨³å®šæ€§çš„ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨å¤šä¸ªç¯å¢ƒå¹¶è¡Œè®­ç»ƒã€‚å…¶èƒŒåçš„åŸå› æ˜¯æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« ä¸­è®¨è®ºçš„åŸºæœ¬é—®é¢˜ï¼Œå³æ ·æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè¿™ç ´åäº†ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰å‡è®¾ï¼Œè€Œè¿™ä¸ªå‡è®¾å¯¹äºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¼˜åŒ–è‡³å…³é‡è¦ã€‚ç›¸å…³æ€§çš„è´Ÿé¢å½±å“æ˜¯æ¢¯åº¦æ–¹å·®éå¸¸å¤§ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„è®­ç»ƒæ‰¹æ¬¡åŒ…å«äº†éå¸¸ç›¸ä¼¼çš„æ ·æœ¬ï¼Œå®ƒä»¬éƒ½ä¼šå°†æˆ‘ä»¬çš„ç½‘ç»œæ¨å‘ç›¸åŒçš„æ–¹å‘ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ–¹å‘åœ¨å…¨å±€ä¸Šå¯èƒ½å®Œå…¨æ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºæ‰€æœ‰è¿™äº›æ ·æœ¬å¯èƒ½æ¥è‡ªåŒä¸€ä¸ªå¹¸è¿æˆ–ä¸å¹¸è¿çš„å›åˆã€‚
- en: With our deep Q-network (DQN), we solved the issue by storing a large number
    of previous states in the replay buffer and sampling our training batch from this
    buffer. If the buffer is large enough, the random sample from it will be a much
    better representation of the statesâ€™ distribution at large. Unfortunately, this
    solution wonâ€™t work for policy gradient methods. This is because most of them
    are on-policy, which means that we have to train on samples generated by our current
    policy, so remembering old transitions will not be possible anymore. You can try
    to do this, but the resulting policy gradient will be for the old policy used
    to generate the samples and not for your current policy that you want to update.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬çš„æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨é‡æ”¾ç¼“å†²åŒºä¸­å­˜å‚¨å¤§é‡çš„å†å²çŠ¶æ€ï¼Œå¹¶ä»è¿™ä¸ªç¼“å†²åŒºä¸­æŠ½å–è®­ç»ƒæ‰¹æ¬¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœç¼“å†²åŒºè¶³å¤Ÿå¤§ï¼Œä»ä¸­éšæœºæŠ½å–çš„æ ·æœ¬å°†æ›´å¥½åœ°ä»£è¡¨çŠ¶æ€çš„æ•´ä½“åˆ†å¸ƒã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä¸ªæ–¹æ³•ä¸èƒ½åº”ç”¨äºç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚è¿™æ˜¯å› ä¸ºå¤§å¤šæ•°ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ˜¯åŸºäºå½“å‰ç­–ç•¥è¿›è¡Œè®­ç»ƒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆçš„æ ·æœ¬æ¥è®­ç»ƒï¼Œå› æ­¤ä¸èƒ½å†è®°ä½æ—§çš„è½¬æ¢ã€‚ä½ å¯ä»¥å°è¯•è¿™æ ·åšï¼Œä½†æœ€ç»ˆå¾—åˆ°çš„ç­–ç•¥æ¢¯åº¦ä¼šæ˜¯åŸºäºæ—§ç­–ç•¥ç”Ÿæˆæ ·æœ¬çš„æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä½ æƒ³æ›´æ–°çš„å½“å‰ç­–ç•¥ã€‚
- en: Researchers have focused on this issue for many years. Several ways to address
    it have been proposed, but the problem is still far from being solved. The most
    commonly used solution is gathering transitions using several parallel environments,
    all of them exploiting the current policy. This breaks the correlation within
    one single episode, as we now train on several episodes obtained from different
    environments. At the same time, we are still using our current policy. The one
    very large disadvantage of this is sample inefficiency, as we basically throw
    away all the experience that we have obtained after one single training round.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶äººå‘˜å·²ç»ç ”ç©¶è¿™ä¸ªé—®é¢˜å¤šå¹´ï¼Œæå‡ºäº†å‡ ç§è§£å†³æ–¹æ¡ˆï¼Œä½†è¿™ä¸ªé—®é¢˜ä»è¿œæœªè§£å†³ã€‚æœ€å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å¤šä¸ªå¹¶è¡Œç¯å¢ƒæ”¶é›†è½¬æ¢ï¼Œè¿™äº›ç¯å¢ƒéƒ½åˆ©ç”¨å½“å‰çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æ‰“ç ´äº†å•ä¸€å›åˆä¸­çš„ç›¸å…³æ€§ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯åœ¨å¤šä¸ªä¸åŒç¯å¢ƒä¸­æ”¶é›†çš„å¤šä¸ªå›åˆä¸Šè¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨å½“å‰çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªé‡å¤§ç¼ºç‚¹æ˜¯æ ·æœ¬æ•ˆç‡ä½ï¼Œå› ä¸ºæˆ‘ä»¬åŸºæœ¬ä¸Šä¼šä¸¢å¼ƒåœ¨å•ä¸€è®­ç»ƒè½®æ¬¡ä¸­è·å¾—çš„æ‰€æœ‰ç»éªŒã€‚
- en: Itâ€™s very simple to compare DQN with policy gradient approaches. For example,
    for DQN, if we use 1 million samples of a replay buffer and a training batch size
    of 32 samples for every new frame, every single transition will be used approximately
    32 times before it is pushed from the experience replay. For the priority replay
    buffer, which was discussed in ChapterÂ [8](ch012.xhtml#x1-1240008), this number
    could be much higher, as the sample probability is not uniform. In the case of
    policy gradient methods, each experience obtained from the environment can be
    used only once, as our method requires fresh data, so the data efficiency of policy
    gradient methods could be an order of magnitude lower than the value-based, off-policy
    methods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, our A2C agent converged on Pong in 8 million frames, which
    is just eight times more than 1 million frames for basic DQN in ChapterÂ [6](#)
    and ChapterÂ [8](ch012.xhtml#x1-1240008). So, this shows us that policy gradient
    methods are not completely useless; theyâ€™re just different and have their own
    specificities that you need to take into account on method selection. If your
    environment is â€œcheapâ€ in terms of the agent interaction (the environment is fast,
    has a low memory footprint, allows parallelization, and so on), policy gradient
    methods could be a better choice. On the other hand, if the environment is â€œexpensiveâ€
    and obtaining a large amount of experience could slow down the training process,
    the value-based methods could be a smarter way to go.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Adding an extra â€œAâ€ to A2C
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From a practical point of view, communicating with several parallel environments
    is simple. We already did this in ChapterÂ [9](ch013.xhtml#x1-1600009) and earlier
    in the current chapter, but it wasnâ€™t explicitly stated. In the A2C agent, we
    passed an array of Gym environments into the ExperienceSource class, which switched
    it into round-robin data gathering mode. This means that every time we ask for
    a transition from the experience source, the class uses the next environment from
    our array (of course, keeping the state for every environment). This simple approach
    is equivalent to parallel communication with environments, but with one single
    difference: communication is not parallel in the strict sense but performed in
    a serial way. However, samples from our experience source are shuffled. This idea
    is shown in the following figure:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_12_11.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 12.11: An agent training from multiple environments in parallel'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: This method works fine and helped us to get convergence in the A2C method, but
    it is still not perfect in terms of computing resource utilization, as all the
    processing is done sequentially. Even a modest workstation nowadays has several
    CPU cores, which can be used for computation, such as training and environment
    interaction. On the other hand, parallel programming is harder than the traditional
    paradigm, when you have a clear stream of execution. Luckily, Python is a very
    expressive and flexible language with lots of third-party libraries, which allows
    you to do parallel programming without much trouble. We have already seen the
    example of the torch.multiprocessing library in ChapterÂ [9](ch013.xhtml#x1-1600009),
    where we parallelized agentsâ€™ execution during the DQN training. But there are
    other higher-level libraries, like ray, which allow us to parallelize execution
    of the code, hiding the low-level communication details.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to actor-critic parallelization, two approaches exist:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Data parallelism: We can have several processes, each of them communicating
    with one or more environments and providing us with transitions (s,r,a,sâ€²). All
    those samples are gathered together in one single training process, which calculates
    losses and performs an SGD update. Then, the updated neural network (NN) parameters
    need to be broadcast to all other processes to use in future environment communications.
    This model is illustrated in FigureÂ [12.12](#x1-211008r12).'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gradients parallelism: As the goal of the training process is the calculation
    of gradients to update our NN, we can have several processes calculating gradients
    on their own training samples. Then, these gradients can be summed together to
    perform the SGD update in one process. Of course, updated NN weights also have
    to be propagated back to all workers to keep data on-policy. This is illustrated
    in FigureÂ [12.13](#x1-211009r13).'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_12_12.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 12.12: The first approach to actor-critic parallelism, based on distributed
    training samples being gathered'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_12_13.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 12.13: The second approach to parallelism, gathering gradients for the
    model'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the two methods might not look very significant from
    the diagrams, but you need to be aware of the computation cost. The heaviest operation
    in A2C optimization is the training process, which consists of loss calculation
    from data samples (forward pass) and the calculation of gradients with respect
    to this loss. The SGD optimization step is quite lightweight â€“ basically, just
    adding the scaled gradients to the NNâ€™s weights. By moving the computation of
    loss and gradients in the second approach (gradient parallelism) from the central
    process, we eliminated the major potential bottleneck and made the whole process
    significantly more scalable.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the choice of the method mainly depends on your resources and your
    goals. If you have one single optimization problem and lots of distributed computation
    resources, such as a couple of dozen GPUs spread over several machines in the
    networks, then gradients parallelism will be the best approach to speed up your
    training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: However, in the case of one single GPU, both methods will provide a similar
    performance, but the first approach is generally simpler to implement, as you
    donâ€™t need to deal with low-level gradient values. In this chapter, we will compare
    both methods on our favorite Pong game to see the difference between the approaches
    and look at PyTorchâ€™s multiprocessing capabilities.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: A3C with data parallelism
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first version of A3C parallelization that we will check (which was outlined
    in FigureÂ [12.12](#x1-211008r12)) has both one main process that carries out training
    and several child processes communicating with environments and gathering experience
    to train on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we already implemented this version in ChapterÂ [9](ch013.xhtml#x1-1600009)
    when we ran several agents in subprocesses when we trained the DQN model (then
    we got a speed-up of 27% in terms of FPS). In this section, Iâ€™m not going to reimplement
    the same approach with the A3C method, but rather want to illustrate the â€œpower
    of libraries.â€
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'We already briefly mentioned the class gym.vector.SyncVectorEnv from Gymnasium
    (it exists only in the Farama fork, not in the original OpenAI Gym) and the PTAN
    experience source, which supports â€œvectorizedâ€ environments: VectorExperienceSourceFirstLast.
    The class SyncVectorEnv handles wrapped environments sequentially, but there is
    a drop-in replacement class, AsyncVectorEnv, which uses mp.multiprocessing for
    subenvironments. So, to get the data-parallel version of the A2C method, we just
    need to replace SyncVectorEnv with AsyncVectorEnv and weâ€™re done.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The code in Chapter12/02_pong_a2c.py already supports this replacement, which
    is done by passing the --use-async command-line option.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The asynchronous version with 50 environments shows a performance of 2000 FPS,
    which is a 2x improvement over the sequential version. The following charts compare
    the performance and reward dynamics of these two versions:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_12_14.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 12.14: Comparison of A2C and A3C in terms of reward (left) and speed
    (right)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: A3C with gradient parallelism
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next approach that we will consider to parallelize A2C implementation will
    have several child processes, but instead of feeding training data to the central
    training loop, they will calculate the gradients using their local training data,
    and send those gradients to the central master process. This process is responsible
    for combining those gradients (which is basically just summing them) and performing
    an SGD update on the shared network.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The difference might look minor, but this approach is much more scalable, especially
    if you have several powerful nodes with multiple GPUs connected to the network.
    In this case, the central process in the data-parallel model quickly becomes a
    bottleneck, as the loss calculation and backpropagation are computationally demanding.
    Gradient parallelization allows for the spreading of the load on several GPUs,
    performing only a relatively simple operation of gradient combination in a central
    place.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complete example is in the Chapter12/03_a3c_grad.py file, and it uses the
    same Chapter12/lib/common.py module that weâ€™ve already seen.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we first define the hyperparameters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'These are mostly the same as in the previous example, except BATCH_SIZE is
    replaced by two parameters: GRAD_BATCH and TRAIN_BATCH. The value of GRAD_BATCH
    defines the size of the batch used by every child process to compute the loss
    and get the value of the gradients. The second parameter, TRAIN_BATCH, specifies
    how many gradient batches from the child processes will be combined on every SGD
    iteration. Every entry produced by the child process has the same shape as our
    network parameters, and we sum up TRAIN_BATCH values of them together. So, for
    every optimization step, we use the TRAIN_BATCH * GRAD_BATCH training samples.
    As the loss calculation and backpropagation are quite heavy operations, we use
    a large GRAD_BATCH to make them more efficient.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Due to this large batch, we should keep TRAIN_BATCH relatively low to keep our
    network update on policy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have two functions â€“ make_env(), which is used to create a wrapped Pong
    environment, and grads_func(), which is much more complicated and implements most
    of the training logic we normally do in the training loop. As a compensation,
    the training loop in the main process becomes almost trivial:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'On the creation of the child process, we pass several arguments to the grads_func()
    function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The name of the process, which is used to create the TensorBoard writer. In
    this example, every child process writes its own TensorBoard dataset.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shared NN.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A torch.device instance, specifying the computation device.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The queue used to deliver the calculated gradients to the central process.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our child process function looks very similar to the main training loop in
    the data-parallel version, which is not surprising, as the responsibilities of
    our child process increased. However, instead of asking the optimizer to update
    the network, we gather gradients and send them to the queue. The rest of the code
    is almost the same:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Up to this point, weâ€™ve gathered the batch with transitions and handled the
    end-of-episode rewards.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next part of the function, we calculate the combined loss from the training
    data and perform backpropagation of the loss:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the following code, we send our intermediate values that weâ€™re going to
    monitor during the training to TensorBoard:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At the end of the loop, we need to clip the gradients and extract them from
    the networkâ€™s parameters into a separate buffer (to prevent them from being corrupted
    by the next iteration of the loop). Here, we effectively store gradients in the
    tensor.grad field for every network parameter. This could be done without bothering
    with synchronization with other workers, as our networkâ€™s parameters are shared,
    but the gradients are locally allocated by every process:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The last line in grads_func puts None into the queue, signaling that this child
    process has reached the game solved state and training should be stopped.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The main process starts with the creation of the network and sharing of its
    weights:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, as in the previous section, we need to set a start method for torch.multiprocessing
    and limit the number of threads started by OpenMP. This is done by setting the
    environment variable OMP_NUM_THREADS, which instructs the OpenMP library about
    the number of threads it can start. OpenMP ([https://www.openmp.org/](https://www.openmp.org/))
    is heavily used by the Gym and OpenCV libraries to provide a speed-up on multicore
    systems, which is a good thing most of the time. By default, the process that
    uses OpenMP starts a thread for every core in the system. But in our case, the
    effect from OpenMP is the opposite: as weâ€™re implementing our own parallelism,
    by launching several processes, extra threads overload the cores with frequent
    context switches, which negatively impacts performance. To avoid this, we explicitly
    limit the amount of threads to one thread. If you want, you can experiment yourself
    with this parameter. On my system, I experienced a 3-4x performance drop without
    this environment variable assignment.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create the communication queue and spawn the required count of child
    processes:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can get to the training loop:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The major difference from the data-parallel version of A3C lies in the training
    loop, which is much simpler here, as child processes have done all the heavy calculations
    for us. In the beginning of the loop, we handle the situation when one of the
    processes has reached the required mean reward (when this happens, we have None
    in the queue). In this case, we just exit the loop to stop the training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We sum gradients together for all the parameters in our network:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we have accumulated enough gradient pieces, we convert the sum of the
    gradients into the PyTorch FloatTensor and assign them to the grad field of the
    network parameters. To average the gradients from different children, we call
    the optimizerâ€™s step() function for every TRAIN_BATCH gradient obtained. For intermediate
    steps, we just sum the corresponding gradients together:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: After that, all we need to do is call the optimizerâ€™s step() method to update
    the network parameters using the accumulated gradients.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'On the exit from the training loop, we stop all child processes to make sure
    that we terminated them, even if Ctrl + C was pressed to stop the optimization:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This step is needed to prevent zombie processes from occupying GPU resources.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This example can be started the same way as the previous example, and after
    a while, it should start displaying the speed and mean reward. However, you need
    to be aware that displayed information is local for every child process, which
    means that speed, the count of games completed, and the number of frames need
    to be multiplied by the number of processes. My benchmarks have shown speed to
    be around 500-600 FPS for every child, which gives 2,000-2,400 FPS in total.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Convergence dynamics are also very similar to the previous version. The total
    number of observations is about 8â€¦10 million, which requires about 1.5 hours to
    complete. The reward chart on the left shows individual processes, but the speed
    chart on the right shows the sum of all processes. As you can see, gradient parallelism
    gives slightly higher performance than data parallelism:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_12_15.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 12.15: Comparison of A2C and A3C in terms of reward (left) and speed
    (right)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about one of the most widely used methods in deep
    RL: A2C, which wisely combines the policy gradient update with the value of the
    state approximation. We analyzed the effect of the baseline on the statistics
    and convergence of gradients. Then, we checked the extension of the baseline idea:
    A2C, where a separate network head provides us with the baseline for the current
    state. In addition, we discussed why it is important for policy gradient methods
    to gather training data from multiple environments, due to their on-policy nature.
    We also implemented two different approaches to A3C, in order to parallelize and
    stabilize the training process. Parallelization will come up once again in this
    book, when we discuss black-box methods (ChapterÂ [17](ch021.xhtml#x1-31100017)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: In the next two chapters, we will take a look at practical problems that can
    be solved using policy gradient methods, which will wrap up the policy gradient
    methods part of the book.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
