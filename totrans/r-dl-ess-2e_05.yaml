- en: Image Classification Using Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not an exaggeration to say that the huge growth of interest in deep learning
    can be mostly attributed to convolutional neural networks. **Convolutional neural
    networks** (**CNNs**) are the main building blocks of image classification models
    in deep learning, and have replaced most techniques that were previously used
    by specialists in the field. Deep learning models are now the de facto method
    to perform all large-scale image tasks, including image classification, object
    detection, detecting artificially generated images, and even attributing text
    descriptions to images. In this chapter, we will look at some of these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Why are CNNs so important? To explain why, we can look at the history of the
    ImageNet competition. The **ImageNet** competition is an open large-scale image
    classification challenge that has one thousand categories. It can be considered
    as the unofficial world championship for image classification. Teams, mostly fielded
    by academics and researchers, compete from around the world. In 2011, an error
    rate of around 25% was the benchmark. In 2012, a team led by Alex Krizhevsky and
    advised by Geoffrey Hinton achieved a huge leap by winning the competition with
    an error rate of 16%. Their solution consisted of 60 million parameters and 650,000
    neurons, five convolutional layers, some of which are followed by max-pooling
    layers, and three fully-connected layers with a final 1,000-way softmax layer
    to do the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: Other researchers built on their techniques in subsequent years, with the result
    that the original ImageNet competition is essentially considered *solved.* In
    2017, almost all teams achieved an error rate of less than 5%. Most people consider
    that the 2012 ImageNet victory heralded the dawn of the new deep learning revolution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at image classification using CNN. We are going
    to start with the `MNIST` dataset, which is considered as the *Hello World* of
    deep learning tasks. The `MNIST` dataset consists of grayscale images of size
    28 x 28 of 10 classes, the numbers 0-9\. This is a much easier task than the ImageNet
    competition; there are 10 categories rather than 1,000, the images are in grayscale
    rather than color and most importantly, there are no backgrounds in the MNIST
    images that can potentially confuse the model. Nevertheless, the MNIST task is
    an important one in its own right; for example, most countries use postal codes
    containing digits. Every country uses automatic address routing solutions that
    are more complex variations of this task.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the MXNet library from Amazon for this task. The MXNet library is
    an excellent library introduction to deep learning as it allows us to code at
    a higher level than other libraries such as TensorFlow, which we cover later on
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What are CNNs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MXNet for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are the cornerstone of image classification in deep learning. This section
    gives an introduction to them, explains the history of CNNs, and will explain
    why they are so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will look at a simple deep learning architecture. Deep
    learning models are difficult to train, so using an existing architecture is often
    the best place to start. An architecture is an existing deep learning model that
    was state-of-the-art when initially released. Some examples are AlexNet, VGGNet,
    GoogleNet, and so on. The architecture we will look at is the original LeNet architecture
    for digit classification from Yann LeCun and others from the mid 1990s. This architecture
    was used for the `MNIST` dataset. This dataset is comprised of grayscale images
    of 28 x 28 size that contain the digits 0 to 9\. The following diagram shows the
    LeNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f59bb2d3-51a3-4092-8010-a129b9248d7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: The LeNet architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The original images are 28 x 28 in size. We have a series of hidden layers which
    are convolution and pooling layers (here, they are labeled *subsampling*). Each
    convolutional layer changes structure; for example, when we apply the convolutions
    in the first hidden layer, our output size is three dimensional. Our final layer
    is of size 10 x 1, which is the same size as the number of categories. We can
    apply a `softmax` function here to convert the values in this layer to probabilities
    for each category. The category with the highest probability would be the category
    prediction for each image.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows how convolutional layers work in greater depth. At a basic
    level, convolutional layers are nothing more than a set of filters. When you look
    at images while wearing glasses with a red tint, everything appears to have a
    red hue. Now, imagine if these glasses consisted of different tints embedded within
    them, maybe a red tint with one or more horizontal green tints. If you had such
    a pair of glasses, the effect would be to highlight certain aspects of the scene
    in front of you. Any part of the scene that had a green horizontal line would
    become more focused.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers apply a selection of patches (or convolutions) over the
    previous layer’s output. For example, for a face recognition task, the first layer’s
    patches identify basic features in the image, for example, an edge or a diagonal
    line. The patches are moved across the image to match different parts of the image.
    Here is an example of a 3 x 3 convolutional block applied across a 6 x 6 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e8858f5-faf5-4c81-80d8-23b1caa232f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: An example of a single convolution applied across an image'
  prefs: []
  type: TYPE_NORMAL
- en: 'The values in the convolutional block are multiplied element by element (that
    is, not matrix multiplication), and the values are added to give a single value.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3701b84b-891e-4c43-bba3-2dc35c04c6f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: An example of a convolution block applied to two parts of an input
    layer'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, our convolutional block is a diagonal pattern. The first block
    in the image (*A1:C3*) is also a diagonal pattern, so when we multiply the elements
    and sum them, we get a relatively large value of **6.3**. In comparison, the second
    block in the image (*D4:F6*) is a horizontal line pattern, so we get a much smaller
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be difficult to visualize how convolutional layers work across the entire
    image, so the following R Shiny application will show it more clearly. This application
    is included in the code for this book in the `Chapter5/server.R` file. Open this
    file in **RStudio** and select **Run app**. Once the application is loaded, select
    **Convolutional Layers** from the left menu bar. The application loads the first
    100 images from the `MNIST` dataset, which we will use later for our first deep
    learning image classification task. The images are grayscale images of size 28
    x 28 of handwritten digits 0 to 9\. Here is a screenshot of the application with
    the fourth image selected, which is a four:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4719d54a-cdbd-4ef9-a302-fc9a7b1c9bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: The Shiny application showing a horizontal convolutional filter'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once loaded, you can use the slider to browse through the images. In the top-right
    corner, there are four choices of convolutional layers to apply to the image.
    In the previous screenshot, a horizontal line convolutional layer is selected
    and we can see what this looks like in the text box in the top right corner. When
    we apply the convolutional filter to the input image on the left, we can see that
    the resulting image on the right is almost entirely grey, except for where the
    horizontal line was in the original image. Our convolutional filter has matched
    the parts in the image that have a horizontal line. If we change the convolutional
    filter to a vertical line, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/684ef9f1-9e06-476a-91ce-a339e28b065b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The Shiny application showing a vertical convolutional filter'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can see that, after the convolution is applied, the vertical lines in
    the original image are highlighted in the resultant image on the right. In effect,
    applying these filters is a type of feature extraction. I encourage you to use
    the application and browse through images and see how different convolutions apply
    to images of the different categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the basis of convolutional filters, and while it is a simple concept,
    it becomes powerful when you start doing two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining many convolutional filters to create convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying another set of convolutional filters (that is, a convolutional layer)
    to the output of a previous convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may take some time to get your head around. If I apply a filter to an image
    and then apply a filter to that output, what do I get? And if I then apply that
    a third time, that is, apply a filter to an image and then apply a filter to that
    output, and then apply a filter to that output, what do I get? The answer is that
    each subsequent layer combines identified features from the previous layers to
    find even more complicated patterns, for example, corners, arcs, and so on. Later
    layers find even richer features such as a circle with an arc over it, indicating
    the eye of a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parameters that are used to control the movement of the convolution:
    padding and strides. In the following diagram, we can see that the original image
    is of size 6 x 6, while there are 4 x 4 subgraphs. We have therefore reduced the
    data representation from a 6 x 6 matrix to a 4 x 4 matrix. When we apply a convolution
    of size *c1*, *c2* to data of size n, m, the output will be *n-c1+1*, *m-c2+1*.
    If we want our output to be the same size as our input, we can pad the input by
    adding zeros to borders of the images. For the previous example, we add a 1-pixel
    border around the entire image. The following diagram shows how the first 3 x
    3 convolution would be applied to the image with padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/328c6a41-1e2f-4474-bd01-479977cc7e23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Padding applied before a convolution
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter we can apply to convolutions is strides, which control
    the movement of the convolution. The default is 1, which means the convolution
    moves by 1 each time, first to the right and then down. In practice, this value
    is rarely changed, so we will not consider it further.
  prefs: []
  type: TYPE_NORMAL
- en: We now know that convolutions act like small feature generators, that they are
    applied across an input layer (which is image data for the first layer), and that
    subsequent convolution layers find even more complicated features. But how are
    they calculated? Do we need to carefully craft a set of convolutions manually
    to apply them to our model? The answer is no; these convolutions are automatically
    calculated for us through the magic of the gradient descent algorithm. The best
    patterns are found after many iterations through the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do convolutions work once we get beyond 2-3 levels of layers? The answer
    is that it is difficult for anyone to understand the exact mathematics of how
    convolutional layers work. Even the original designers of these architects may
    not fully understand what is happening in the hidden layers in a series of CNNs.
    If this worries you, then recall that the solution that won the ImageNet competition
    in 2012 had 60 million parameters. With the advance in computing power, deep learning
    architectures may have hundreds of millions of parameters. It is simply not possible
    for any person to fully understand what is happening in such a complicated model.
    This is why they are often called **black-box** models.
  prefs: []
  type: TYPE_NORMAL
- en: It might surprise you at first. How can deep learning achieve human-level performance
    in image classification and how can we build deep learning models if we do not
    fully understand how they work? This question has divided the deep learning community,
    largely along the demarcation between industry and academia. Many (but not all)
    researchers believe that we should get a more fundamental understanding of how
    deep learning models work. Some researchers also believe that we can only develop
    the next generation of artificial intelligence applications by getting a better
    understanding of how current architectures work. At a recent NIPS conference (one
    of the oldest and most notable conferences for deep learning), deep learning was
    unfavorably compared to alchemy. Meanwhile, practitioners in the industry are
    not concerned with how deep learning works. They are more focused on building
    ever more complex deep learning architectures to maximize accuracy or performance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is a crude representation of the state of the industry; not
    all academics are inward looking and not all practitioners are just tweaking models
    to get small improvements. Deep learning is still relatively new (although the
    foundation blocks of neural networks have been known about for decades). But this
    tension does exist and has been around for awhile – for example, a popular deep
    learning architecture introduced *Inception* modules, which were named after the
    *Inception* movie. In the film, Leonardo DiCaprio leads a team that alter people’s
    thoughts and opinions by embedding themselves within people’s dreams. Initially,
    they go one layer deep, but then go deeper, in effect going to dreams within dreams.
    As they go deeper, the worlds get more complicated and the outcomes less certain.
    We will not go into detail here about what *Inception modules* are, but they combine
    convolutional and max pooling layers in parallel. The authors of the paper acknowledged
    the memory and computational cost of the model within the paper, but by naming
    the key component as an *Inception module,* they were subtly suggesting which
    side of the argument they were on.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the breakthrough performance of the winner of the 2012 ImageNet competition,
    two researchers were unsatisfied that there was no insight into how the model
    worked. They decided to reverse-engineer the algorithm, attempting to show the
    input pattern that caused a given activation in the feature maps. This was a non-trivial
    task, as some layers used in the original model (for example, pooling layers)
    discarded information. Their paper showed the top 9 activations for each layer.
    Here is the feature visualization for the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/798437be-2e43-4581-99e1-db373bd99ef0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Feature visualization for the first layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image is in two parts; on the left we can see the convolution (the paper
    only highlights 9 convolutions for each layer). On the right, we can see examples
    of patterns within images that match that convolution. For example, the convolution
    in the top-left corner is a diagonal edge detector. Here is the feature visualization
    for the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8995cb80-262e-4472-81dd-a0201ee82632.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Feature visualization for the second layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the image on the left is an interpretation of the convolution, while
    the image on the right shows examples of image patches that activate for that
    convolution. Here, we are starting to see some combinatorial patterns. For example,
    in the top-left, we can see patterns with stripes. Even more interesting is the
    example in the second row and second column. Here, we see circle shapes, which
    can indicate an eyeball in a person or an animal. Now, let''s move on to feature
    visualization for the third layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44322ff1-5a85-4c01-af92-00d8d8716b79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Feature visualization for the third layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: In the third layer, we are seeing some really interesting patterns. In the second
    row and second column, we have identified parts of a car (wheels). In the third
    row and third column, we have begun to identify peoples' faces. In the second
    row and fourth column, we are identifying text within the images.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors show examples for more layers. I encourage you to
    read the paper to get further insight into how convolutional layers work.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that, while deep learning models can achieve human-level
    performance on image classification, they do not interpret images as humans do.
    They have no concept of what a cat is or what a dog is. They can only match the
    patterns given. In the paper, the authors highlight an example where the matched
    patterns have little in common; the model is matching features in the background
    (grass) instead of foreground objects.
  prefs: []
  type: TYPE_NORMAL
- en: In another image classification task, the model failed to work in practice.
    The task was to classify wolves versus dogs. The model failed in practice because
    the model was trained with data which had wolves in their natural habitat, that
    is, snow. Therefore, the model assumed its task was to differentiate between *snow* and
    *dog*. Any image of a wolf in another setting was wrongly classified.
  prefs: []
  type: TYPE_NORMAL
- en: The lesson from this is that your training data should be varied and closely
    related to the data that the model will be expected to predict against. This may
    sound obvious in theory, but it is not always easy to do so in practice. We will
    discuss this further in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pooling layers are used in CNNs to reduce the number of parameters in the model
    and therefore they reduce overfitting. They can be thought of as a type of dimensionality
    reduction. Similar to convolutional layers, a pooling layer moves over the previous
    layer but the operation and return value are different. It returns a single value
    and the operation is usually the maximum value of the cells in that patch, hence
    the name max-pooling. You can also perform other operations, for example, average
    pooling, but this is less common. Here is an example of max-pooling using a 2
    x 2 block. The first block has the values 7, 0, 6, 6 and the maximum value of
    these is 7, so the output is 7\. Note that padding is not normally used with max-pooling
    and that it usually applies a stride parameter to move the block. Here, the stride
    is 2, so once we get the max of the first block, we move across, 2 cells to the
    right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fdf19a0-cf55-4d13-bccd-91b5b63c0283.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Max-Pooling applied to a matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that max-pooling reduces the output by a factor of 4; the input
    was 6 x 6 and the output is 3 x 3\. If you have not seen this before, your first
    reaction is probably disbelief. Why are we throwing away data? Why do we use max-pooling
    at all? There are three parts to this answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pooling**: It is normally applied after a convolutional layer, so instead
    of executing over pixels, we execute over matched patterns. Downsizing after convolutional
    layers does not discard 75% of the input data; there is still enough signal there
    to find the pattern if it exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: If you have studied machine learning, you will know that
    many models have problems with correlated features and that you are generally
    advised to remove correlated features. In image data, features are highly correlated
    with the spatial pattern around them. Applying max-pooling reduces the data while
    maintaining the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution speed**: When we consider the two earlier reasons, we can see that
    max-pooling greatly reduces the size of the network without removing too much
    of the signal. This makes training the model much quicker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note the difference in the parameters used in the convolutional
    layer compared to the pooling layer. In general, a convolutional block is bigger
    (3 x 3) than the pooling block (2 x 2) and they should not overlap. For example,
    do not use a 4 x 4 convolutional block and a 2 x 2 pooling block. If they did
    overlap, the pooling block would just operate over the same convolutional blocks
    and the model would not train correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dropout** is a form of regularization which aims to prevent a model from
    overfitting. Overfitting is when the model is memorizing parts of the training
    dataset, but is not as accurate on unseen test data. When you build a model, you
    can check if overfitting is a problem by looking at the gap between the accuracy
    on the training set against the accuracy on the test set. If performance is much
    better on the training dataset, then the model is overfitting. Dropout refers
    to removing nodes randomly from a network temporarily during training. It is usually
    only applied to hidden layers, and not input layers. Here is an example of dropout
    applied to a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25613efe-f5b3-4596-8cca-80e3f0bc38e2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: An example of dropout in a deep learning model'
  prefs: []
  type: TYPE_NORMAL
- en: For each forward pass, a different set of nodes is removed, and therefore the
    network is different each time. In the original paper, dropout is compared to
    ensemble techniques, and in a way it is. There are some similarities to how dropout
    works and how random forest selects a random selection of features for each tree.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to look at dropout is that each node in a layer must learn to work
    with all the nodes in that layer and the inputs it gets from the previous layer.
    It prevents one or a small number of nodes in a layer from getting large weights
    and dominating the outputs from that layer. This means that each node in a layer
    will work as a group and prevent some nodes from being too lazy and other nodes
    from being too dominant.
  prefs: []
  type: TYPE_NORMAL
- en: Flatten layers, dense layers, and softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After applying multiple convolutional layers, the resulting data structure is
    a multi-dimensional matrix (or tensor). We must transform this into a matrix that
    is in the shape of the required output. For example, if our classification task
    has 10 classes (for example, 10 for the `MNIST` example), we need the output of
    the model to be a 1 x 10 matrix. We do this by taking the results of our convolutional
    and max-pooling layers and using a Flatten layer to reshape the data. The last
    layer should have the same number of nodes as the number of classes we wish to
    predict for. If our task is binary classification, the `activation` function in
    our last layer will be sigmoid. If our task is binary classification, the `activation`
    function in our last layer will be softmax.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying the softmax/sigmoid activation, we may optionally apply a number
    of dense layers. A dense layer is just a normal hidden layer, as we saw in [Chapter
    1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),* Getting Started with Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a softmax layer because the values in the last layer are numeric but
    range from -infinity to + infinity. We must convert these series of input values
    into a series of probabilities that says how likely the instance is for each category.
    The function to transform these numeric values to a series of probabilities must
    have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Each output value must be between 0.0 to 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the output values should be exactly 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to do this is to just rescale the values by dividing each input value
    by the sum of the absolute input values. That approach has two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not handle negative values correctly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rescaling the input values may give us probabilities that are too close to each
    other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two issues can be solved by first applying **e^x** (where e is 2.71828)
    to each input value and then rescaling those values. This transforms any negative
    number to a small positive number, and it also causes the probabilities to be
    more polarized. This can be demonstrated with an example; here, we can see the
    result from our dense layers. The values for categories 5 and 6 are quite close
    at 17.2 and 15.8, respectively. However, when we apply the `softmax` function,
    the probability value for category 5 is 4 times the probability value for category
    6\. The `softmax` function tends to result in probabilities that emphasize one
    category over all others, which is exactly what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cbe0564-d081-4f69-9b77-5dc0e94402be.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Example of the softmax function
  prefs: []
  type: TYPE_NORMAL
- en: Image classification using the MXNet library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MXNet package was introduced in [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*, so go back to that chapter for instructions
    on how to install the package if you have not already done so. We will demonstrate
    how to get almost 100% accuracy on a classification task for image data. We will
    use the `MNIST` dataset that we introduced in [Chapter 2](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks*. This dataset contains
    images of handwritten digits (0-9), and all images are of size 28 x 28\. It is
    the *Hello World!* equivalent in deep learning. There’s a long-term competition
    on Kaggle that uses this dataset. The script `Chapter5/explore.Rmd` is an R markdown
    file that explores this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will check if the data has already been downloaded, and if it has
    not, we will download it. If the data is not available at this link, see the code
    in `Chapter2/chapter2.R` for an alternative way to get the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we read the data into R and check it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have `20` rows and `785` columns. Here, we will look at the rows at the
    tail of the dataset and look at the first 6 columns and the last 6 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have `785` columns. The first column is the data label, and then we have
    784 columns named `pixel0`, …, `pixel783` with the pixel values. Our images are
    *28 x 28 = 784*, so everything looks OK.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start building models, it is always a good idea to ensure that your
    data is in the correct format and that your features and labels are aligned correctly.
    Let's plot the first 9 instances with their data labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will create a `helper` function called `plotInstance` that takes
    in the pixel values and outputs the image with an optional header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code shows the first 9 images and their classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca8e8a1a-ba42-4536-8103-7e5ac5b98ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: The first 9 images in the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This completes our data exploration. Now, we can move on to creating some deep
    learning models using the MXNet library. We will create two models—the first is
    a standard neural network which we will use as a baseline. The second deep learning
    model is based on an architecture called **LeNet**. This is an old architecture,
    but is suitable in this case because our images are low resolution and do not
    contain backgrounds. Another advantage of LeNet is that it is possible to train
    quickly, even on CPUs, because it does not have many layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is in  `Chapter5/mnist.Rmd`. We must read data into
    R and convert it into matrices. We will split the training data into a train set
    and test set to get an unbiased estimate of accuracy. Because we have a large
    number of rows, we can use a split ratio of 90/10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each image is represented as row of 784 (28 x 28) pixel values. The value of
    each pixel is in the range 0-255, we linearly transform it into 0-1 by dividing
    by 255\. We also transpose the input matrix to because column major format in
    order to use it in  `mxnet`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Before creating a model, we should check that our dataset is balanced, i.e.
    the number of instances for each digit is reasonably even:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This looks ok, we can now move on to creating some deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Base model (no convolutional layers)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have explored the data and we are satisfied that it looks OK, the
    next step is to create our first deep learning model. This is similar to the example
    we saw in the previous chapter. The code for this is in `Chapter5/mnist.Rmd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at this code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: In `mxnet`, we use its own data type symbol to configure the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create the first hidden layer (`fullconnect1 <- ....`). This parameters are
    the data as input, the layer's name and the number of neurons in the layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We apply an activation function to the `fullconnect`  layer (`activation1 <-
    ....`). The `mx.symbol.Activation` function takes the output from the first hidden
    layer, `fullconnect1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second hidden layer (`fullconnect1 <- ....`) takes `activation1` as the
    input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second activation is similar to  `activation1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The  `fullconnect3`  is the output layer. This layer has 10 neurons because
    this is a multi-classification problem and there are 10 classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use a softmax activation to get a probabilistic prediction for each
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s train the base model. I have a GPU installed, so I can use that.
    You may need to change the line to `devices <- mx.cpu()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To make a prediction, we will call the `predict` function. We can then create
    a confusion matrix and calculate our accuracy level on test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of our base model is `0.971`. Not bad, but let's see if we can
    improve on it.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can create a model based on the LeNet architecture. This is a very
    simple model; we have two sets of convolutional and pooling layers and then a
    Flatten layer, and finally two dense layers. The code for this is in `Chapter5/mnist.Rmd`.
    First let''s define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s reshape the data so that it can be used in MXNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of our CNN model is `0.9835714`, which is quite an improvement
    over the accuracy of our base model, which was `0.971`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can visualize our model in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot, which shows the architecture of the deep
    learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44ca3de2-5605-49eb-9042-80099ff1b36f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Convolutional deep learning model (LeNet)'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have built a deep learning model that is over 98% accurate!
  prefs: []
  type: TYPE_NORMAL
- en: We saw the architecture of LeNet in *Figure 5.1* and we have programmed it using
    the MXNet library. Let's analyze the LeNet architecture in more detail. Essentially,
    we have two convolutional groups and two fully connected layers. Our convolutional
    groups have a convolutional layer, followed by an `activation` function and then
    a pooling layer. This combination of layers is very common across many deep learning
    image classification tasks. The first convolution layer has 64 blocks of 5 x 5
    size with no padding. This will possibly miss some features at the edges of the
    images, but if we look back at our sample images in *Figure 5.15*, we can see
    that most images do not have any data around the borders. We use pooling layers
    with `pool_type=max`. Other types are possible; average pooling was commonly used
    but has fallen out of favor recently. It is another hyper-parameter to try. We
    calculate our pools in 2 x 2 and then stride `(“jump”)` by 2\. Therefore, each
    input value is only used once in the max pool layer.
  prefs: []
  type: TYPE_NORMAL
- en: We use `tanh` as an `activation` function for our first convolutional block,
    and then use  `relu`  for subsequent layers. If you wish, you can try to change
    these and see what effect they have. Once we have executed our convolutional layers,
    we can use Flatten to restructure the data into a format that can be used by a
    fully connected layer. A fully connected layer is just a collection of nodes in
    a layer, that is, similar to the layers in the base model in the previous code.
    We have two layers, one with 512 nodes and the other with 10 nodes. We select
    10 nodes in our last layer as this is the number of categories in our problem.
    Finally, we use a softmax to convert the numeric quantities in this layer into
    a set of probabilities for each category. We have achieved 98.35% accuracy, which
    is quite an improvement on a *normal* deep learning model, but there is still
    room for improvement. Some models can get 99.5% accuracy on this dataset, that
    is, 5 wrongly classified records in 1,000\. Next, we will look at a different
    dataset that, while similar to MNIST, is harder than MNIST. This is the Fashion
    `MNIST` dataset, which has grayscale images of the same size as MNIST and also
    has 10 categories.
  prefs: []
  type: TYPE_NORMAL
- en: Classification using the fashion MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This dataset is in the same structure as `MNIST`, so we can just change our
    dataset and use the existing boilerplate code we have for loading the data. The
    script `Chapter5/explore_Fashion.Rmd` is an R markdown file that explores this
    dataset; it is almost identical to the `explore.Rmd` that we used for the `MNIST`
    dataset, so we will not repeat it. The only change to the `explore.Rmd` is to
    output the labels. We will look at 16 examples because this is a new dataset.
    Here are some sample images from this dataset that are created using the same
    boilerplate code we used to create the example for the `MNIST` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/367f26fc-344b-499f-a1de-5e190590764f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Some images from the Fashion MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting fact about this dataset is that the company that released it
    also created a GitHub repository where they tested machine learning libraries
    against this dataset. The benchmarks are available at [http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/).
    If we look through these results, none of the machine libraries they tried achieved
    over 90% accuracy (they did not try deep learning). This is the target we want
    to beat with a deep learning classifier. The deep learning model code is in `Chapter5/fmnist.R`
    and achieves over 91% accuracy on this dataset. There are some small, but significant
    differences to the model architecture above. Try to spot them without peeking
    at the explanation.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's define the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first change is that we switch to use `relu` as an `Activation` function
    for all layers. Another change was that we use padding for the convolutional layers,
    which we did to capture the features at the borders of the image. We increased
    the number of nodes in each layer to add depth to the model. We also added a dropout
    layer to prevent the model from overfitting. We also added logging to our model,
    which outputs the train and validation metrics for each epoch. We used these to
    check how our model performs and to decide if it is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the accuracy results and the diagnostic plot for this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note is that we are using the same validation/test set for showing
    metrics during training and to evaluate the final model. This is not a good practice,
    but it is acceptable here because we are not using validation metrics to tune
    the hyperparameters of the model. The accuracy of our CNN model is `0.9106667`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the accuracy of the train and validation sets as the model is trained. The
    deep learning model code has a `callback` function which saves the metrics as
    the model is trained. We can use this to plot the training and validation metrics
    for each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us how our model is performing after each epoch (or training run). This
    produces the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b92b126f-4ded-40c8-b316-1fbae5700357.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: Training and validation accuracy by epoch'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main points to be taken from this graph are that:'
  prefs: []
  type: TYPE_NORMAL
- en: The model is overfitting. We can see a clear gap between performance on the
    training set at **0.95xxx** and the validation set at **0.91xxx**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could have probably stopped the model training after 8 epochs, as performance
    did not improve after this point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have discussed in previous chapters, deep learning models will almost
    always overfit by default, but there are methods to negate this. The second issue
    is related to *early stopping*, and it is vital that you know how to do this so
    that you do not waste hours in continuing to train a model which is no longer
    improving. This is especially relevant if you are building the model using cloud
    resources. We will look at these and more issues related to building deep learning
    models in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References/further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These papers are classical deep learning papers in this domain. Some of them
    document winning approaches to ImageNet competitions. I encourage you to download
    and read all of them. You may not understand them at first, but their importance
    will become more evident as you continue on your journey in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. *ImageNet Classification
    with Deep Convolutional Neural Networks*. Advances in neural information processing
    systems. 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy, Christian, et al. *Going Deeper with Convolutions*. Cvpr, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun, Yann, et al. *Learning Algorithms for Classification: A Comparison on
    Handwritten Digit Recognition*. Neural networks: the statistical mechanics perspective
    261 (1995): 276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler, Matthew D., and Rob Fergus. *Visualizing and Understanding Convolutional
    Networks*. European conference on computer vision. Springer, Cham, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava, Nitish, et al. *Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting*. The Journal of Machine Learning Research 15.1 (2014): 1929-1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we used deep learning for image classification. We discussed
    the different layer types that are used in image classification: convolutional
    layers, pooling layers, dropout, dense layers, and the softmax activation function.
    We saw an R-Shiny application that shows how convolutional layers perform feature
    engineering on image data.'
  prefs: []
  type: TYPE_NORMAL
- en: We used the MXNet deep learning library in R to create a base deep learning
    model which got 97.1% accuracy. We then developed a CNN deep learning model based
    on the LeNet architecture, which achieved over 98.3% accuracy on test data. We
    also used a slightly harder dataset (`Fashion MNIST`) and created a new model
    that achieved over 91% accuracy. This accuracy score was better than all of the
    other scores that used non-deep learning algorithms. In the next chapter, we will
    build on what we have covered and show you how we can take advantage of pre-trained
    models for classification and as building blocks for new deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss important topics in deep learning
    concerning tuning and optimizing your models. This includes how to use the limited
    data you may have, data pre-processing, data augmentation, and hyperparameter
    selection.
  prefs: []
  type: TYPE_NORMAL
