<html><head></head><body>
		<div id="_idContainer010">
			<h1 id="_idParaDest-42" class="chapter-number"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.2.1">Mastering Prompt Generation and Understanding How Auto-GPT Generates Prompts</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">You are really into the book! </span><span class="koboSpan" id="kobo.3.2">Congratulations on reaching </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">this chapter!</span></span></p>
			<p><span class="koboSpan" id="kobo.5.1">In the previous chapters, we explored the basics of Auto-GPT and its installation. </span><span class="koboSpan" id="kobo.5.2">Now, we are going to delve deeper into one of the most crucial aspects of working with this powerful language model – prompt generation. </span><span class="koboSpan" id="kobo.5.3">In this chapter, we will demystify the process of how Auto-GPT generates prompts, understand why they are so important, and learn how to craft effective prompts to get the most out of Auto-GPT. </span><span class="koboSpan" id="kobo.5.4">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">get started!</span></span></p>
			<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will explore </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">these topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.9.1">What are prompts, and why are </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">they important?</span></span></li>
				<li><span class="koboSpan" id="kobo.11.1">Tips to craft </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">effective prompts</span></span></li>
				<li><span class="koboSpan" id="kobo.13.1">An overview of how Auto-GPT </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">generates prompts</span></span></li>
				<li><span class="koboSpan" id="kobo.15.1">Examples of what works and what </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">confuses GPT</span></span></li>
			</ul>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.17.1">What are prompts, and why are they important?</span></h1>
			<p><span class="koboSpan" id="kobo.18.1">We usually know how to talk to ChatGPT, which is chatting with it directly. </span><span class="koboSpan" id="kobo.18.2">ChatGPT responds </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.19.1">directly with its answer to whatever we ask it. </span><span class="koboSpan" id="kobo.19.2">But how does this relate </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">to prompts?</span></span></p>
			<p><span class="koboSpan" id="kobo.21.1">The text we send is called a prompt; it can be a question, a statement, a task, or just whatever we </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.22.1">want to tell the </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">large language model</span></strong><span class="koboSpan" id="kobo.24.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.25.1">LLM</span></strong><span class="koboSpan" id="kobo.26.1">) . </span><span class="koboSpan" id="kobo.26.2">However, this text is not fed into the </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">LLM directly.</span></span></p>
			<p><span class="koboSpan" id="kobo.28.1">The application generally provides the context of the conversation, such as constraints (for example, </span><em class="italic"><span class="koboSpan" id="kobo.29.1">You are a helpful assistant. </span><span class="koboSpan" id="kobo.29.2">Never argue with the user, and answer requests only if it is ethical and helpful to </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.30.1">the user</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">).</span></span></p>
			<p><span class="koboSpan" id="kobo.32.1">Prompts are the </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.33.1">initial inputs that you provide to a language model to generate a response. </span><span class="koboSpan" id="kobo.33.2">They can take several forms, such as a question, a statement, or </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">a task.</span></span></p>
			<p><span class="koboSpan" id="kobo.35.1">For example, if you were to ask the model, </span><em class="italic"><span class="koboSpan" id="kobo.36.1">What is the weather like today?</span></em><span class="koboSpan" id="kobo.37.1">, this would be a </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">question prompt.</span></span></p>
			<p><span class="koboSpan" id="kobo.39.1">A statement prompt could be something like </span><em class="italic"><span class="koboSpan" id="kobo.40.1">Tell me about the history of Rome</span></em><span class="koboSpan" id="kobo.41.1">,” while a task prompt might be </span><em class="italic"><span class="koboSpan" id="kobo.42.1">Write a short story about a spaceship</span></em><span class="koboSpan" id="kobo.43.1">. </span><span class="koboSpan" id="kobo.43.2">Each type of prompt serves a different purpose and can elicit different types of responses from the model. </span><span class="koboSpan" id="kobo.43.3">Understanding how to use these different types of prompts effectively is key to getting the most out of your interaction </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">with Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.45.1">Here was a prompt that was sent to Auto-GPT, which </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">someone removed:</span></span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.47.1">please add the text that was here before which was containing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.48.1">the prompt—</span></em></span></p>
			<p><span class="koboSpan" id="kobo.49.1">With that </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.50.1">prompt, we provided the </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">following constraints:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.52.1">“Never argue with </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">the user”:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Interpretation</span></strong><span class="koboSpan" id="kobo.55.1">: The user is always the source </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">of truth.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Effect</span></strong><span class="koboSpan" id="kobo.58.1">: This constraint emphasizes a user-centric approach where the AI refrains from challenging the user’s statements or perspectives. </span><span class="koboSpan" id="kobo.58.2">It ensures that the AI’s responses are in agreement or neutral to the user’s input, but this could potentially limit the depth of interactive dialogue. </span><span class="koboSpan" id="kobo.58.3">It may also cause the AI to “role play” and act as if the situation was only a story, trying to respond in a manner that fits the context or even the aforementioned sentence. </span><span class="koboSpan" id="kobo.58.4">This may cause Auto-GPT to make wrong assumptions or become blind to untrue information, such as steps to setting up something that are made up and only sound kind </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">of right.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.60.1">“You are a </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">helpful assistant”:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Role definition</span></strong><span class="koboSpan" id="kobo.63.1">: Clearly establishes the AI’s role as </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">an assistant.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Tone and interaction</span></strong><span class="koboSpan" id="kobo.66.1">: Sets a predefined tone and direction for conversations. </span><span class="koboSpan" id="kobo.66.2">The AI is programmed to strictly adhere to an assistant’s role. </span><span class="koboSpan" id="kobo.66.3">However, this might not always align with user expectations. </span><span class="koboSpan" id="kobo.66.4">Users may anticipate a </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.67.1">more informal, friendly tone and interactive engagement, such as receiving proactive questions such as “</span><em class="italic"><span class="koboSpan" id="kobo.68.1">How can I assist you today?</span></em><span class="koboSpan" id="kobo.69.1">” Instead, the AI’s involvement might be limited to reactive responses without </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">creative input.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.71.1">“Answer questions only if it is ethical and helpful to </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">the user”:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.73.1">The scope of capabilities</span></strong><span class="koboSpan" id="kobo.74.1">: Imposes significant limitations on the AI’s functionality. </span><span class="koboSpan" id="kobo.74.2">The AI is programmed to prioritize ethical considerations and user utility in </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">every interaction.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.76.1">An impact on decision-making assistance</span></strong><span class="koboSpan" id="kobo.77.1">: This could lead to an overly cautious approach where the AI refrains from performing tasks that might relieve users of decision-making responsibilities. </span><span class="koboSpan" id="kobo.77.2">For example, if faced with complex or morally ambiguous tasks, the AI might opt for minimal engagement rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">comprehensive assistance.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.79.1">An example in practice</span></strong><span class="koboSpan" id="kobo.80.1">: When asked to assist in creative tasks, such as writing a chapter about the varying colors of roses, the AI might limit its response to outlining potential approaches (e.g., suggesting bullet points for the chapter), rather than actively engaging in the creative </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">process itself.</span></span></li></ul></li>
			</ul>
			<p><span class="koboSpan" id="kobo.82.1">Prompts define the task as well as the context of what the LLM is supposed </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">to answer.</span></span></p>
			<p><span class="koboSpan" id="kobo.84.1">As we discussed in </span><a href="B21128_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.85.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.86.1">, Auto-GPT sends a fairly huge prompt, where it defines the context, commands, and constraints, as well as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.87.1">"user"</span></strong><span class="koboSpan" id="kobo.88.1"> message that says </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">the following:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.90.1">
role": "user",
"content": "Determine which next command to use, and respond using the format specified above:" :" :"</span></pre>			<p><span class="koboSpan" id="kobo.91.1">This way, GPT actually plays a role in a hypothetical story that contains the current context of information and the possible commands that only Auto-GPT can execute, responding to </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.92.1">the query prompt of the user with what command it thinks it should </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">use next.</span></span></p>
			<p><span class="koboSpan" id="kobo.94.1">This is even more interesting if you consider that when instructing, for example, ChatGPT to do something, it would generally respond with the so-annoying phrase, “</span><em class="italic"><span class="koboSpan" id="kobo.95.1">As an AI </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.96.1">language model...</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">”</span></span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.98.1">Phrasing</span></h2>
			<p><span class="koboSpan" id="kobo.99.1">When we look at the prompts that Auto-GPT uses, we could easily think about rephrasing them, making them shorter to save tokens, or adding </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">more context.</span></span></p>
			<p><span class="koboSpan" id="kobo.101.1">However, each of </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.102.1">those operations has </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">a downside.</span></span></p>
			<p><span class="koboSpan" id="kobo.104.1">LLMs are </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.105.1">not human; they aren’t really an intelligence that understands the input that it is given and what it outputs. </span><span class="koboSpan" id="kobo.105.2">They are only trained to generate text that is the most probable, given a </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">certain input.</span></span></p>
			<p><span class="koboSpan" id="kobo.107.1">This means that even though a sentence could have the exact same meaning as another, both sentences may be interpreted </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">completely differently.</span></span></p>
			<p><span class="koboSpan" id="kobo.109.1">Due to the nature of machine learning, the texts that were used to train an LLM pretty much also define how it stores </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">its knowledge.</span></span></p>
			<p><span class="koboSpan" id="kobo.111.1">If the sentence “</span><em class="italic"><span class="koboSpan" id="kobo.112.1">I have just sold my car because I just wanted to buy an ice cream</span></em><span class="koboSpan" id="kobo.113.1">” never appeared in the training data, an LLM would have a harder time understanding the sentence, given that the tokens are not related. </span><span class="koboSpan" id="kobo.113.2">“Buy” would be related to “car” and “ice cream,” but “car” and “ice cream” would be </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">considerably unrelated.</span></span></p>
			<p><span class="koboSpan" id="kobo.115.1">As GPT-4 is not open source, we have to rely on Llama to understand how </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">models work.</span></span></p>
			<p><span class="koboSpan" id="kobo.117.1">Llama has </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.118.1">a few parameters, such as length penalties and uniqueness (i.e., how often the same words appear). </span><span class="koboSpan" id="kobo.118.2">These limit the vector length of each </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.119.1">embedding, </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">for example.</span></span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.121.1">Embeddings</span></h2>
			<p><span class="koboSpan" id="kobo.122.1">Embeddings are a crucial part of how Auto-GPT generates prompts. </span><span class="koboSpan" id="kobo.122.2">They are essentially a way of </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.123.1">representing words and phrases in a numerical form that a model can understand. </span><span class="koboSpan" id="kobo.123.2">Each word or phrase is represented as a point in a multidimensional space, where the distance and direction between points can represent the relationship between words </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">or phrases.</span></span></p>
			<p><span class="koboSpan" id="kobo.125.1">For instance, in this </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.126.1">multidimensional space, the words “king” and “queen” might be close together, indicating a strong relationship, while “king” and “ice cream” would be further apart, indicating a weaker relationship. </span><span class="koboSpan" id="kobo.126.2">This is how a model understands context and can generate </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">relevant responses.</span></span></p>
			<p><span class="koboSpan" id="kobo.128.1">The process of creating these embeddings involves breaking down the input text into tokens, which can be words, parts of words, or even single characters, depending on the language. </span><span class="koboSpan" id="kobo.128.2">These tokens are then mapped to vectors in the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">multidimensional space.</span></span></p>
			<p><span class="koboSpan" id="kobo.130.1">The model then uses these vectors to generate a response. </span><span class="koboSpan" id="kobo.130.2">It does this by calculating the probability of each possible next token, based on the current context. </span><span class="koboSpan" id="kobo.130.3">The token with the highest probability is selected, and the process is repeated until a complete response </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">is generated.</span></span></p>
			<p><span class="koboSpan" id="kobo.132.1">This is a simplified explanation of the process, and the actual implementation involves a lot more complexity, including the use of attention mechanisms to determine which parts of the input are most relevant, and the use of transformer models to handle long sequences </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">of tokens.</span></span></p>
			<p><span class="koboSpan" id="kobo.134.1">However, the key takeaway is that a model generates prompts by understanding the context of the input and calculating the most probable next token. </span><span class="koboSpan" id="kobo.134.2">This is why the phrasing of prompts is so important, as it can greatly influence the model’s understanding of the context and, therefore, the </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">generated response.</span></span></p>
			<p><span class="koboSpan" id="kobo.136.1">In the next section, we will look at some tips to craft effective prompts and provide some examples of what works and what </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">confuses Auto-GPT.</span></span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.138.1">Tips to craft effective prompts</span></h1>
			<p><span class="koboSpan" id="kobo.139.1">The art of crafting effective prompts is a skill that requires a nuanced understanding of a language </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.140.1">model’s capabilities and limitations. </span><span class="koboSpan" id="kobo.140.2">The following guidelines will help you to create prompts that are more likely to yield the desired responses </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">from Auto-GPT:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Precision is key</span></strong><span class="koboSpan" id="kobo.143.1">: The specificity of your prompt can significantly influence the relevance of the response. </span><span class="koboSpan" id="kobo.143.2">For instance, a vague prompt such as “</span><em class="italic"><span class="koboSpan" id="kobo.144.1">Tell me about dogs</span></em><span class="koboSpan" id="kobo.145.1">” might yield a generic response about dogs. </span><span class="koboSpan" id="kobo.145.2">However, a more specific prompt such as “</span><em class="italic"><span class="koboSpan" id="kobo.146.1">What are the different breeds of dogs and their characteristics?</span></em><span class="koboSpan" id="kobo.147.1">” is likely to generate a more detailed and </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">informative response.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.149.1">Clarity and simplicity</span></strong><span class="koboSpan" id="kobo.150.1">: It’s crucial to remember that while Auto-GPT is a sophisticated language model, it’s not a human. </span><span class="koboSpan" id="kobo.150.2">Therefore, it’s best to avoid jargon or complex language that could potentially confuse the model. </span><span class="koboSpan" id="kobo.150.3">Instead, opt for clear, simple language that the model can </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">easily interpret.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.152.1">Contextual Clues</span></strong><span class="koboSpan" id="kobo.153.1">: One of the key tips to craft effective prompts is to provide sufficient contextual clues. </span><span class="koboSpan" id="kobo.153.2">This means giving a model enough information to understand the broader context of the conversation or task. </span><span class="koboSpan" id="kobo.153.3">For example, if you were asking the model to write a story set in medieval times, you might provide some context about the setting, characters, and plot before giving the actual prompt. </span><span class="koboSpan" id="kobo.153.4">This could be done through a more detailed prompt or by utilizing the conversation history feature of Auto-GPT. </span><span class="koboSpan" id="kobo.153.5">By providing sufficient context, you can help the model generate a more relevant and </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">coherent response.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.155.1">Experimentation</span></strong><span class="koboSpan" id="kobo.156.1">: Don’t hesitate to experiment with different phrasing and approaches. </span><span class="koboSpan" id="kobo.156.2">A slight change in the phrasing of your prompt can sometimes lead to a significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">different response.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.158.1">Next, we will look at a few examples of effective and </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">ineffective prompts.</span></span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.160.1">Examples of effective and ineffective prompts</span></h2>
			<p><span class="koboSpan" id="kobo.161.1">To better </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.162.1">understand these principles, let’s examine some </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.163.1">examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">such prompts:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Example 1</span></strong><span class="koboSpan" id="kobo.166.1">: “Tell me a joke.” </span><span class="koboSpan" id="kobo.166.2">This prompt is simple and clear, and Auto-GPT is likely to respond with a joke, demonstrating its ability to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">creative content.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.168.1">Example 2</span></strong><span class="koboSpan" id="kobo.169.1">: “What is the meaning of life?” </span><span class="koboSpan" id="kobo.169.2">This prompt is philosophical and broad, which might lead to a vague or generic response, as the model might struggle to provide a concise and </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">meaningful answer.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.171.1">Example 3</span></strong><span class="koboSpan" id="kobo.172.1">: “As a language model, explain the concept of machine learning.” </span><span class="koboSpan" id="kobo.172.2">This prompt is clear, and specific, and provides context, which will likely result in a detailed explanation of </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">the concept.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.174.1">Example 4</span></strong><span class="koboSpan" id="kobo.175.1">: “Translate the following text into French: ‘Hello, how are you?’” </span><span class="koboSpan" id="kobo.175.2">This prompt is clear, specific, and task-oriented, which should lead to a </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">correct translation.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.177.1">In conclusion, understanding the intricacies of how Auto-GPT generates prompts and mastering the art of crafting effective prompts can significantly enhance your interaction with the model. </span><span class="koboSpan" id="kobo.177.2">Remember, the key is to be specific, use clear and simple language, provide ample context, and embrace experimentation. </span><span class="koboSpan" id="kobo.177.3">With these guidelines in mind, you’re well on your way to becoming a proficient user </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">of Auto-GPT.</span></span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.179.1">An overview of how Auto-GPT generates prompts</span></h1>
			<p><span class="koboSpan" id="kobo.180.1">Here, we will understand the prompt generation process </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">in Auto-GPT.</span></span></p>
			<p><span class="koboSpan" id="kobo.182.1">Auto-GPT’s </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.183.1">prompt generation process is a sophisticated mechanism that involves a deep understanding of the input context and the calculation of the most probable next token. </span><span class="koboSpan" id="kobo.183.2">This process is not just about generating responses but also about setting the stage for the conversation, defining the roles, and establishing the rules </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">of engagement.</span></span></p>
			<p><span class="koboSpan" id="kobo.185.1">Let’s delve deeper into </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">this process:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.187.1">Tokenization</span></strong><span class="koboSpan" id="kobo.188.1">: The initial step involves breaking down the input text into tokens, which </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.189.1">could be words, parts of words, or even individual characters, depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">the language.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.191.1">Embedding</span></strong><span class="koboSpan" id="kobo.192.1">: Each </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.193.1">token is then mapped to a vector in a multidimensional space, creating an “embedding.” </span><span class="koboSpan" id="kobo.193.2">The position of each vector in this space signifies the meaning of the corresponding token in relation to all </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">other tokens.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Contextual understanding</span></strong><span class="koboSpan" id="kobo.196.1">: Auto-GPT uses these embeddings to comprehend </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.197.1">the context of the input. </span><span class="koboSpan" id="kobo.197.2">It calculates the distance and direction between the vectors, representing the relationships between </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">the tokens.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.199.1">Response generation</span></strong><span class="koboSpan" id="kobo.200.1">: The model then generates a response by calculating the probability </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.201.1">of each possible next token, based on the current context. </span><span class="koboSpan" id="kobo.201.2">The token with the highest probability is selected, and the process is repeated until a complete response </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">is generated.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.203.1">An attention mechanism</span></strong><span class="koboSpan" id="kobo.204.1">: An attention mechanism is employed to determine </span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.205.1">which parts of the input are most relevant to the current context. </span><span class="koboSpan" id="kobo.205.2">This allows the model to focus on the most important parts of the input when generating </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">a response.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.207.1">Transformer models</span></strong><span class="koboSpan" id="kobo.208.1">: To handle long sequences of tokens, the model uses transformer </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.209.1">models. </span><span class="koboSpan" id="kobo.209.2">These models can process the tokens in parallel, making them much more efficient than traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">sequential models.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.211.1">In </span><a href="B21128_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.212.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.213.1">, we discussed </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.214.1">the default prompt that Auto-GPT uses, which includes constraints, context, goals, and commands. </span><span class="koboSpan" id="kobo.214.2">This default prompt sets the stage for a conversation, defines the roles, and establishes the rules of engagement. </span><span class="koboSpan" id="kobo.214.3">For instance, constraints such as “</span><em class="italic"><span class="koboSpan" id="kobo.215.1">Never argue with the user</span></em><span class="koboSpan" id="kobo.216.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.217.1">You are a helpful assistant</span></em><span class="koboSpan" id="kobo.218.1">” set the tone and direction of the conversation. </span><span class="koboSpan" id="kobo.218.2">The context and goals provide a clear understanding of the task at hand, and the commands guide the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">model’s responses.</span></span></p>
			<p><span class="koboSpan" id="kobo.220.1">This is a high-level overview of the process, and the actual implementation involves a lot more complexity. </span><span class="koboSpan" id="kobo.220.2">However, the key takeaway is that Auto-GPT generates prompts by understanding </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.221.1">the context of the input and calculating the most probable next token. </span><span class="koboSpan" id="kobo.221.2">This is why the phrasing of the prompts is so important, as it can greatly influence the model’s understanding of the context and, therefore, the </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">generated response.</span></span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.223.1">Examples of what works, and what confuses GPT</span></h1>
			<p><span class="koboSpan" id="kobo.224.1">Here are </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.225.1">some examples of what GPT understands and what it might miss </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">out on:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.227.1">Example 1 – an effective prompt</span></strong><span class="koboSpan" id="kobo.228.1">: Here are the AI settings for </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">this one:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.230.1">Role</span></strong><span class="koboSpan" id="kobo.231.1">: An AI-powered author and researcher specializing in creating comprehensive, well-structured, and engaging content on Auto-GPT and its plugins, while maintaining an open line of communication with the user for feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">and guidance</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.233.1">Goals</span></strong><span class="koboSpan" id="kobo.234.1">: Conduct a thorough analysis of the current state of the book and identify areas </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">for improvement</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.236.1">Prompt</span></strong><span class="koboSpan" id="kobo.237.1">: AuthorGPT, I have placed a text file in your working directory; can you analyze the current state of the book and suggest areas </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">for improvement?</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.239.1">This prompt aligns perfectly with the role and goal defined in the AI settings. </span><span class="koboSpan" id="kobo.239.2">The model is likely to respond with a detailed analysis of the book and suggestions </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">for improvement.</span></span></p></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">Example 2 – ineffective prompts</span></strong><span class="koboSpan" id="kobo.242.1">: Understanding hallucinations in GPT models – a </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">compact exploration.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.244.1">Hallucinations in GPT models refer to occasions where a model generates text that seems logical but is not based on reality. </span><span class="koboSpan" id="kobo.244.2">This usually occurs when the model encounters vague or incomplete prompts. </span><span class="koboSpan" id="kobo.244.3">Interestingly, generative AI such as GPT can begin to “hallucinate.” </span><span class="koboSpan" id="kobo.244.4">We’ll delve into the mechanics of this phenomenon shortly, but first, let’s define it more clearly. </span><span class="koboSpan" id="kobo.244.5">Here are the </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">AI settings:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.246.1">Role</span></strong><span class="koboSpan" id="kobo.247.1">: An AI-powered author and researcher specializing in creating comprehensive, well-structured, and engaging content on Auto-GPT and its plugins, while </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.248.1">maintaining an open line of communication with the user for feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">and guidance.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.250.1">Goals</span></strong><span class="koboSpan" id="kobo.251.1">: Conduct a thorough analysis of the current state of the book and identify areas </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">for improvement</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.253.1">Prompt</span></strong><span class="koboSpan" id="kobo.254.1">: AuthorGPT, can you analyze the current state of the book and suggest areas </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">for improvement?</span></span></li></ul></li>
			</ul>
			<p><span class="koboSpan" id="kobo.256.1">With this prompt, you would expect Auto-GPT to ask you for the context, but I found that GPT tries to improvise instead and starts </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">to hallucinate.</span></span></p>
			<p><span class="koboSpan" id="kobo.258.1">Hallucination with GPT means it starts to act as if it is doing something factual whereas it isn’t. </span><span class="koboSpan" id="kobo.258.2">Let’s understand this more in </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">depth ahead.</span></span></p>
			<h3><span class="koboSpan" id="kobo.260.1">What does hallucination mean in GPT?</span></h3>
			<p><span class="koboSpan" id="kobo.261.1">Hallucination in GPT models manifests when a model acts as though it’s performing a task. </span><span class="koboSpan" id="kobo.261.2">This </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.262.1">ranges from creating code files for projects that only contain placeholders to fabricating facts that sound contextually appropriate. </span><span class="koboSpan" id="kobo.262.2">For example, in certain situations, it might discuss a topic closely related to the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">context’s keywords.</span></span></p>
			<p><span class="koboSpan" id="kobo.264.1">Hallucination in </span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.265.1">language models such as GPT happens when the model produces text that appears plausible but is unanchored in reality. </span><span class="koboSpan" id="kobo.265.2">This is often the result of the model working with ambiguous or </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">insufficient information.</span></span></p>
			<p><span class="koboSpan" id="kobo.267.1">Take, for instance, if you request the model to write a story about a non-existent character. </span><span class="koboSpan" id="kobo.267.2">It might “hallucinate” details about the character’s life, appearance, or traits. </span><span class="koboSpan" id="kobo.267.3">While this can yield creative and unforeseen results, it can also lead to text that is illogical or unrelated to the </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">original prompt.</span></span></p>
			<p><span class="koboSpan" id="kobo.269.1">A case in point is when I asked Auto-GPT to develop a </span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">three.js</span></strong><span class="koboSpan" id="kobo.271.1">-based RPG browser game. </span><span class="koboSpan" id="kobo.271.2">It began researching how to gather weather data from a non-existent API. </span><span class="koboSpan" id="kobo.271.3">This was a result of processing excessive context with GPT-3.5-turbo, which I had used before transitioning </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">to GPT-4.</span></span></p>
			<p><span class="koboSpan" id="kobo.273.1">Auto-GPT’s memory sometimes harbors incorrect facts or memories. </span><span class="koboSpan" id="kobo.273.2">To economize tokens, its memory is condensed by </span><em class="italic"><span class="koboSpan" id="kobo.274.1">Chat Completion</span></em><span class="koboSpan" id="kobo.275.1"> prompts with GPT. </span><span class="koboSpan" id="kobo.275.2">This can lead to misunderstandings, especially when summarizing texts that have already been summarized and merging </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">these summaries.</span></span></p>
			<p><span class="koboSpan" id="kobo.277.1">Confusion </span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.278.1">can also stem from tokens closely related to the current context. </span><span class="koboSpan" id="kobo.278.2">For example, if writing </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.279.1">a weather API tool and a web-based game are both related to “JavaScript,” the model might perceive the weather API as a relevant topic. </span><span class="koboSpan" id="kobo.279.2">Such confusion is less frequent with GPT-4, thanks to its advanced parameters and </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">enhanced precision.</span></span></p>
			<h3><span class="koboSpan" id="kobo.281.1">Confusing a prompt and its impact on AI performance</span></h3>
			<p><span class="koboSpan" id="kobo.282.1">Here are </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.283.1">some AI settings where the prompt can be confusing, impacting the performance of the AI it </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">works with:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Role</span></strong><span class="koboSpan" id="kobo.286.1">: An AI-powered author and researcher specializing in creating comprehensive, well-structured, and engaging content on Auto-GPT and its plugins, while maintaining an open line of communication with the user for feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">and guidance</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.288.1">Goals</span></strong><span class="koboSpan" id="kobo.289.1">: Conduct a thorough analysis of the current state of the book and identify areas </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">for improvement</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Prompt</span></strong><span class="koboSpan" id="kobo.292.1">: AuthorGPT, can you tell me </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">a joke?</span></span><p class="list-inset"><span class="koboSpan" id="kobo.294.1">This seemingly simple prompt can lead to confusion for the model, primarily because it diverges from its established role and goal. </span><span class="koboSpan" id="kobo.294.2">The request for a joke seems out of place in the context of analyzing a book and </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">suggesting enhancements.</span></span></p></li>
			</ul>
			<p><span class="koboSpan" id="kobo.296.1">Initially, the model might comply and tell a joke. </span><span class="koboSpan" id="kobo.296.2">However, this deviation can have longer-term repercussions. </span><span class="koboSpan" id="kobo.296.3">As the model integrates this interaction into its memory summary, it may become increasingly perplexed. </span><span class="koboSpan" id="kobo.296.4">This is because Auto-GPT instructs GPT to retain as much information as possible to prevent topic shifts, hallucinations, or previous steps </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">being forgotten.</span></span></p>
			<p><span class="koboSpan" id="kobo.298.1">A specific issue arises when Auto-GPT focuses on a particular task but encounters an unrelated prompt. </span><span class="koboSpan" id="kobo.298.2">For example, if it receives a command irrelevant to the ongoing context, the model might lose track of its previous actions. </span><span class="koboSpan" id="kobo.298.3">It could end up in a loop of searching for </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.299.1">information related to the new input, attempting to reconcile it with the earlier task. </span><span class="koboSpan" id="kobo.299.2">As a result, Auto-GPT might start intertwining the unrelated joke with its Google Search results, leading to a mix-up </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">of topics.</span></span></p>
			<p><span class="koboSpan" id="kobo.301.1">This scenario highlights the importance of aligning prompts with the AI’s defined role and objectives to maintain effectiveness and </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">avoid confusion.</span></span></p>
			<h3><span class="koboSpan" id="kobo.303.1">An effective prompt and its impact on AI performance</span></h3>
			<p><span class="koboSpan" id="kobo.304.1">Here are </span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.305.1">the AI settings for an effective prompt and its advantages </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.306.1">on </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">AI performance:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Role</span></strong><span class="koboSpan" id="kobo.309.1">: An AI-powered author and researcher specializing in creating comprehensive, well-structured, and engaging content on Auto-GPT and its plugins, while maintaining an open line of communication with the user for feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">and guidance</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.311.1">Goal</span></strong><span class="koboSpan" id="kobo.312.1">: Develop a comprehensive plan to create task lists that will help you structure research, a detailed outline per chapter, and </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">individual parts</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.314.1">Prompt</span></strong><span class="koboSpan" id="kobo.315.1">: AuthorGPT, can you help me develop a comprehensive plan to create task lists to structure my research and outline for </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">each chapter?</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.317.1">This prompt is clear, specific, and aligns perfectly with the role and goal defined in the AI settings. </span><span class="koboSpan" id="kobo.317.2">The model is likely to provide a detailed plan for creating task lists and </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">structuring research.</span></span></p>
			<h3><span class="koboSpan" id="kobo.319.1">Confusing a prompt and its impact on AI performance</span></h3>
			<p><span class="koboSpan" id="kobo.320.1">Here is an </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.321.1">example of a confusing prompt and its impact on </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">AI performance:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.323.1">Role</span></strong><span class="koboSpan" id="kobo.324.1">: An AI-powered author and researcher specializing in creating comprehensive, well-structured, and engaging content on Auto-GPT and its plugins, while maintaining an open line of communication with the user for feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">and guidance</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.326.1">Goal</span></strong><span class="koboSpan" id="kobo.327.1">: Develop a </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.328.1">comprehensive plan to create task lists that will help you structure research, a detailed outline per chapter, and </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">individual parts.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.330.1">Prompt</span></strong><span class="koboSpan" id="kobo.331.1">: AuthorGPT, what is the weather </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">like today?</span></span></li>
			</ul>
			<h3><span class="koboSpan" id="kobo.333.1">Understanding the disparity</span></h3>
			<p><span class="koboSpan" id="kobo.334.1">This prompt stands in stark contrast to AI’s designated role and goal. </span><span class="koboSpan" id="kobo.334.2">The model, configured to focus on task list creation and research structuring, faces a dilemma with a prompt that is unrelated to </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">these tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.336.1">This is likely to confuse the model because it doesn’t align with the defined role and goal. </span><span class="koboSpan" id="kobo.336.2">The model might struggle to provide a relevant response because the prompt doesn’t involve creating a plan or </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">structuring research.</span></span></p>
			<p><span class="koboSpan" id="kobo.338.1">Here are </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.339.1">some potential AI </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">behavior scenarios:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.341.1">While Auto-GPT may respond correctly by researching the weather or just shrugging off that question by explaining that it is not the main focus, it could get confused and drop the previous task, either partially or completely. </span><span class="koboSpan" id="kobo.341.2">This could result in a very inaccurate future behavior, or even the issue escalating to GPT not responding correctly to the Auto-GPT module that communicates with it, causing a </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">fatal error.</span></span></li>
				<li><span class="koboSpan" id="kobo.343.1">The model might also try to relate every decision to checking the weather now, or it will keep checking the weather in later steps if Auto-GPT does not manage to compress its memory correctly (for example, when it tries to summarize the memory to reduce data amounts, it may put more emphasis on weather </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">data now).</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.345.1">In conclusion, crafting effective prompts for Auto-GPT involves aligning your prompts with the defined role and goal in the AI settings. </span><span class="koboSpan" id="kobo.345.2">Clear, specific prompts that align with these parameters are more likely to yield relevant responses, while prompts that don’t align can confuse </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">the model.</span></span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.347.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.348.1">In this chapter, we delved into the intricacies of prompt generation and how Auto-GPT generates prompts. </span><span class="koboSpan" id="kobo.348.2">We started by defining prompts and their importance in shaping the responses of the language model. </span><span class="koboSpan" id="kobo.348.3">We learned that prompts can be questions, statements, tasks, or any text that we want to communicate to a </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">language model.</span></span></p>
			<p><span class="koboSpan" id="kobo.350.1">We also discussed the role of constraints in providing context to a conversation and guiding a model’s responses. </span><span class="koboSpan" id="kobo.350.2">We examined how specific constraints can influence the tone, direction, and ethical boundaries of </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">the conversation.</span></span></p>
			<p><span class="koboSpan" id="kobo.352.1">We then explored the technical aspects of prompt generation, including tokenization, embedding, context understanding, response generation, attention mechanisms, and transformer models. </span><span class="koboSpan" id="kobo.352.2">We learned that a model generates prompts by understanding the context of the input and calculating the most probable </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">next token.</span></span></p>
			<p><span class="koboSpan" id="kobo.354.1">Then, we provided tips to craft effective prompts, emphasizing the importance of specificity, clarity, context, and experimentation. </span><span class="koboSpan" id="kobo.354.2">We also looked at examples of effective and confusing prompts, demonstrating how alignment with the defined role and goal in the AI settings can influence a </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">model’s responses.</span></span></p>
			<p><span class="koboSpan" id="kobo.356.1">Finally, we examined examples of prompts based on specific AI settings, demonstrating how effective prompts align with the defined role and goal, while confusing prompts </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">do not.</span></span></p>
			<p><span class="koboSpan" id="kobo.358.1">In conclusion, mastering prompt generation and understanding how Auto-GPT generates prompts can significantly enhance your interaction with a model. </span><span class="koboSpan" id="kobo.358.2">The key is to craft clear, specific prompts that align with the model’s role and goal, provide ample context, and not be afraid </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">to experiment.</span></span></p>
			<p><span class="koboSpan" id="kobo.360.1">In the next chapter, we will use the skills we acquired with plugins and learn how to </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">customize prompts.</span></span></p>
		</div>
	</body></html>