- en: Handwritten Digits Classification Using ConvNets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to this chapter on using **convolution neural networks** (**ConvNets**)
    for the classification of handwritten digits. In [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* we built a simple neural network for classifying
    handwritten digits. This was 87% accurate, but we were not happy with its performance.
    In this chapter, we will understand what convolution is and build a ConvNet for
    classifying the handwritten digits to help the restaurant chain become more accurate
    in sending text messages to the right person. If you have not been through [Chapter
    2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression*, please go through it once so that you can
    get an understanding of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building deeper models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It would be better if you implement the code snippets as you go through this
    chapter, either in a Jupyter Notebook or any source code editor. This will make
    it easier for you to follow along as well as understand how the different sections
    of the code work.
  prefs: []
  type: TYPE_NORMAL
- en: All of the Python files and the Jupyter Notebook files for this chapter can
    be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Code implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will be using the Keras deep learning library, which is
    a high-level neural network API capable of running on top of TensorFlow, Theano,
    and CNTK.
  prefs: []
  type: TYPE_NORMAL
- en: Know the code! We will not spend time understanding how Keras works, but if
    you are interested, refer to this easy-to-understand official documentation from
    Keras at [https://keras.io/](https://keras.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Importing all of the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the `numpy`, `matplotlib`, `keras`, `scipy`, and `tensorflow`
    packages in this exercise. Here, TensorFlow is used as the backend for Keras.
    You can install these packages with `pip`. For the MNIST data, we will be using
    the dataset available in the `keras` module with a simple `import`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important that you set `seed` for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s import the `mnist` module that''s available in `keras` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, unpack the `mnist` train and test images with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data has been imported, let''s explore these digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f46b58b-f424-4863-a117-e6ee356075d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Printout information of the data'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, we can see that we have `60000` train images,
    `10000` test images with each image being `28`*`28` in size, and a total of `10`
    predictable classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s plot `9` handwritten digits. Before that, we will need to import
    `matplotlib` for plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86089bae-b734-4ffe-b382-c4e84ea7b84e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Visualizing MNIST digits'
  prefs: []
  type: TYPE_NORMAL
- en: 'Print out the maximum and minimum pixel value of the pixels in the `training_set`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93e4e3f7-85d4-48ed-9cba-d22009aecff7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Printout of the maximum and minimum pixel value in the data'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the maximum and minimum pixel values in the training set are
    `255` and `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the hyperparameters that we will be using throughout
    our code. These are totally configurable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you look back at [Chapter 2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression*, you'll see that the `optimizer` used there
    was `Adam`. Therefore, we will import the `Adam` optimizer from the `keras` module
    and set its learning rate, as shown in the preceding code. For most cases that
    will follow, we will be training for `20` `epochs` for ease of comparison.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `optimizers` and their APIs in Keras, visit [https://keras.io/optimizers/](https://keras.io/optimizers/).
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with different learning rates, optimizers, and batch sizes to see
    how these factors affect the quality of your model. If you get better results,
    show this to the deep learning community.
  prefs: []
  type: TYPE_NORMAL
- en: Building and training a simple deep neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have loaded the data into memory, we need to build a simple neural
    network model to predict the MNIST digits. We will use the same architecture we
    used in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training NN
    for Prediction Using Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be building a `Sequential` model. So, let''s import it from Keras and
    initialize it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the Keras Model API, visit [https://keras.io/models/model/](https://keras.io/models/model/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing that we need to do is define the `Dense`/Perceptron layer. In
    Keras, this can be done by importing the `Dense` layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to add the `Dense` layer to the `Sequential` model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the Keras `Dense` API call, visit [https://keras.io/layers/core/](https://keras.io/layers/core/).
  prefs: []
  type: TYPE_NORMAL
- en: The `add` command performs the job of appending a layer to the `Sequential`
    model, in this case, `Dense`.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Dense` layer in the preceding code, we have defined the number of neurons
    in the first hidden layer, which is `300`. We have also defined the `input_shape` parameter
    as being equal to `(784,)` to indicate to the model that it will be accepting
    input arrays of the shape `(784,)`. This means that the input layer will have
    `784` neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The type of activation function that needs to be applied to the result can be
    defined with the `activation` parameter. In this case, this is `relu`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add another `Dense` layer of `300` neurons by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And the final `Dense` layer with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, the final layer has `10` neurons as we need it to predict scores for `10`
    classes. The `activation` function that has been chosen here is `softmax` so that
    we can limit the scores between 0 and 1, and the sum of scores to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling the model in Keras is super-easy and can be done with following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All you need to do to compile the model is call the `compile` method of the
    model and specify the `loss`, `optimizer`, and `metrics` parameters, which in
    this case are `sparse_categorical_crossentropy`, `Adam`, and `['accuracy']`.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the Keras Model's `compile` method, visit [https://keras.io/models/model/](https://keras.io/models/model/).
  prefs: []
  type: TYPE_NORMAL
- en: The metrics that need to be monitored during this learning process must be specified
    as a list to the `metrics` parameter of the `compile` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print out the summary of the model with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0f371cb-8471-40ad-8f5e-77dc01bf564d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Summary of the multilayer Perceptron model'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this model has `328,810` trainable parameters, which is reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, split the train data into train and validation data by using the `train_test_split`
    function that we imported from `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We have split the data so that we end up with 55,000 training examples and 5,000
    validation examples.
  prefs: []
  type: TYPE_NORMAL
- en: You will also see that we have reshaped the arrays so that each image is of
    shape `(784,)`. This is because we have defined the model to accept images/arrays
    of shape `(784,)`.
  prefs: []
  type: TYPE_NORMAL
- en: Like we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression*, we will now train our model on 55,000 training
    examples, validate on 5,000 examples, and test on 10,000 examples.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning the fit to a variable stores relevant information inside it, such
    as train and validation loss and accuracy at each `epoch`, which can then be used
    for plotting the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To fit a model in Keras, along with train digits and train labels, call the
    `fit` method of the model with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`epochs`: The number of epochs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The number of images in each batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_data`: The tuple of validation images and validation labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Look at the *Defining the hyperparameters* section of the chapter for the defined
    values of `epochs` and `batch_size`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4bbaf1a-b09d-4eb6-9936-4013b7c625de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60b81072-408b-47a5-a4fc-2f2b7bd14c23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Metrics printed out during the training of MLP'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the model on test data, you can call the `evaluate` method of the
    `model` by feeding the test images and test labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb0eebef-2954-4f4d-a944-9046e9d05718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Printout of the evaluation of MLP'
  prefs: []
  type: TYPE_NORMAL
- en: From the validation and test accuracy, we can see that after 20 epochs of training,
    we have reached the same level of accuracy as we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* but with very few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define a function to plot the train and validation loss and accuracy
    that we have stored in the `history` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f2661fe-de56-4d6f-af06-92ef81b6086d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: MLP loss/accuracy plot during training'
  prefs: []
  type: TYPE_NORMAL
- en: MLP – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements training and evaluation of a simple MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution can be defined as the process of striding a small kernel/filter/array
    over a target array and obtaining the sum of element-wise multiplication between
    the kernel and a subset of equal size of the target array at that location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, you have a target `array` of length 10 and a `kernel` of length 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you start the convolution, implement the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The `kernel` will be multiplied with the subset of the target `array` within
    indices 0 through 2\. This will be between [-1,1,0] (kernel) and [0,1,0] (from
    index 0 through to 2 of the target array). The result of this element-wise multiplication
    will then be summed up to obtain what is called the result of convolution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `kernel` will then be stridden by 1 unit and then multiplied with the subset
    of the target `array` within the indices 1 through 3, just like in *Step 1*, and
    the result is obtained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 2* is repeated until a subset equal to the length of the `kernel` is
    not possible at a new stride location.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result of convolution at each stride is stored in an `array`. This `array` that's
    holding the result of the convolution is called the feature map. The length of
    the 1-D feature map (with step/stride of 1) is equal to the difference in length
    of the `kernel` and the target `array` plus 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Only in this case, we need to take the following equation into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '*length of the feature map = length of the target array - length of the kernel
    + 1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a code snippet implementing 1-D convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19786906-1ec7-4677-8f4d-83fabcddbfd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Printout of example feature map'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have an understanding of how convolution works, let's put it into
    use and build a CNN classifier on MNIST digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, import the `Conv2D` API from the `layers` module of Keras. You can
    do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the convolution will be defined to accept images of shape `28`*`28`*`1`,
    we need to reshape all the images to be of `28`*`28`*`1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b66504ca-6c9c-4ed8-8308-8e13e21102db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Shape of data after reshaping'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the `model`, just like we did previously, we need to initialize the
    `model` as `Sequential`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, add the `Conv2D` layer to the `model` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Conv2D` API, we have defined the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`units`: `32` (number of kernels/filters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size`: `(3,3)` (size of each kernel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape`: `28*28*1` (shape of the input array it will receive)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: `relu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For additional information on the `Conv2D` API, visit [https://keras.io/layers/convolutional/](https://keras.io/layers/convolutional/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the preceding convolution is `32` feature maps of size 26*26\.
    These 2-D feature maps now have to be converted into a 1-D feature map. This can
    be done in Keras with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding snippet is just like a layer of neurons in a simple
    neural network. The `Flatten` function converts all of the 2-D feature maps into
    a single `Dense` layer. In this layer, will we add a `Dense` layer with `128`
    neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we need to get scores for each of the `10` possible classes, we must
    add another `Dense` layer with `10` neurons, with `softmax` as the `activation`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, just like in the case of the simple dense neural network we built in the
    preceding code, we will compile and fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/057e49ab-6aaa-44dc-a757-f0963c239c92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Summary of the convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: From the model's summary, we can see that this convolution classifier has `2,770,634`
    parameters. This is a lot of parameters compared to the Perceptron model. Let's
    fit this model and evaluate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fit the convolution neural network model on the data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ddd7b69-c41e-4470-af04-6baa6893e70d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output from the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7b06e8e-39a6-4e6b-bb51-28bb823a12d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Metrics printed out during the training of the convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the convolution classifier's accuracy is 97.72% on the validation
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can evaluate the convolution model on the test data with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f297d6d-c560-43fc-a4aa-6695398f9c48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Printout of the evaluation of the convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model is 97.92% accurate on test data, 97.72% on validation
    data, and 99.71% on train data. It is clear from the loss as well that the model
    is slightly overfitting on the train data. We will talk about how to handle overfitting
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s plot the train and validation metrics to see how the training has
    progressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b4f6081-a092-4032-a404-6fd8ee593819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Loss/accuracy plot of the convolution classifier during training'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements the training and evaluation of a convolution classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Max pooling can be defined as the process of summarizing a group of values with
    the maximum value within that group. Similarly, if you computed the average, it
    would be average pooling. Pooling operations are usually performed on the generated
    feature maps after convolution to reduce the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the example array we considered for convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you were to perform max pooling on this `array` with the pool size set
    to size 1*2 and a stride of 2, the result would be an array of [1,1,1,1,1]. The
    `array` of size 1*10 has been reduced to a size of 1*5 due to max pooling.
  prefs: []
  type: TYPE_NORMAL
- en: Here, since the pool size is of shape 1*2, you would take the subset of the
    target `array` from index 0 to index 2, which will be [0,1], and compute the maximum
    of this subset as 1\. You would do the same for the subset from index 2 to index
    4, from index 4 to index 6, index 6 to index 8, and finally index 8 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, average pooling can be implemented by computing the average value
    of the pooled section. In this case, it would result in the array [0.5, 0.5, 0.5,
    0.5, 0.5].
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a couple of code snippets that are implementing max and average
    pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/733c7c85-bf2a-4c0e-b81d-3b731c007ceb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: Max pooling operation''s result on an array'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet for average pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3631da1b-3ee5-4b59-ab05-926159042499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: Average pooling operation''s result on an array'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a diagram explaining the max pooling operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae6d1b2c-13c4-478e-85c1-a416714eaa3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: 2*2 max pooling with stride 2 (Source: https://en.wikipedia.org/wiki/Convolutional_neural_network)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code for a digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/523de311-01e5-4497-9088-248fc21f24f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Random MNIST digit'
  prefs: []
  type: TYPE_NORMAL
- en: This image is of shape `28`*`28`. Now, if you were to perform a 2*2 max pooling
    operation of this, the resulting image would have a shape of `14`*`14`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s write a function to implement a 2*2 max pooling operation on a
    MNIST digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ab0b5fc-261c-4e9e-a7e5-b1a429a169b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Random MNIST digit after max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that the convolution classifier that we built in the previous
    section has around 2.7 million parameters. It has been proven that having a lot
    of parameters can lead to overfitting in a lot of cases. This is where pooling
    comes in. It helps us to retain the important features in the data as well as reduce
    the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement a convolution classifier with max pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the max pool operation from Keras with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, define and compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0eef9af-8c2a-479a-95f7-4aa3e2f29ef7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: Summary of the convolution classifier with max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: From the summary, we can see that with a pooling filter of 2*2 with stride 2,
    the number of parameters has come down to `693,962`, which is 1/4^(th) of the
    number of parameters in the convolution classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s fit the model on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37aaddc3-716d-47fb-92ea-405aa4daa2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e318c23-82ba-4a2f-8a78-827d1400e94d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.20: Metrics printed out during the training of the convolution classifier
    with max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the convolution classifier with max pooling has an accuracy
    of 97.72% on the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, evaluate the convolution model with max pooling on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af78f6d0-bf8f-4991-b522-a8bc839f5652.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.21: Printout of the evaluation of the convolution classifier with
    max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model is 97.88% accurate on the test data, 97.72% on the
    validation data, and 99.74% on the train data. The convolution model with pooling
    gives the same level of performance as the convolution model without pooling,
    but with four times less parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can clearly see from the loss that the model is slightly overfitting
    on the train data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did previously, plot the train and validation metrics to see how
    the training has progressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecc6dcb1-e810-4ef6-9d75-7bb64805b8f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.22: Loss/accuracy plot of the convolution classifier with max pooling
    during training'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution with pooling – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements the training and evaluation of a convolution classifier
    with the pooling operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dropout is a regularization technique used to prevent overfitting. During training,
    it is implemented by randomly sampling a neural network from the original neural
    network during each forward and backward propagation, and then training this subset
    network on the batch of input data. During testing, no dropout is implemented.
    The test results are obtained as an ensemble of all of the sampled networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73b7aca8-0bd1-4833-89c6-70da2ebdacf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.23: Dropout, as shown in the Dropout: A Simple Way to Prevent Neural
    Networks from'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting paper (Source: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, implementing `Dropout` is easy. First, import it from the `layers`
    module of `keras`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, place the layer where needed. In the case of our CNN, we will place one
    after the max pool operation and one after the `Dense` layer, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4647d988-3e13-49f7-a703-ec54f465ee6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.24: Summary of the convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: Since `Dropout` is a regularization technique, adding it to a model will not
    result in a change in the number of trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, train the model on the standard 20 `epochs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d19d9627-f78b-4236-aab5-ca2f67b12a81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b9bfe45-5557-4d4d-bd77-0d80bad973e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.25: Metrics printed out during the training of the convolution classifier
    with max pooling and dropout'
  prefs: []
  type: TYPE_NORMAL
- en: We see that the convolution classifier with max pooling and dropout is 98.52%
    accurate on the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let’s evaluate the model and capture the loss and the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3216a23-ca5a-470c-b77c-2a48a300d96e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.26: Printout of the evaluation of the convolution classifier with
    max pooling and dropout'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model is 98.42% accurate on the test data, 98.52% on the
    validation data, and 99.26% on the train data. The convolution model with pooling
    and dropout gives the same level of performance as the convolution model without
    pooling, but with four times fewer parameters. If you look at the `loss` as well,
    this model was able to reach a better minima than the other models we have trained
    before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the metrics to understand how the training has progressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c0d735f-5b28-4408-97f2-8e48e40a59da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.27: Loss/accuracy plot of the convolution classifier with max pooling
    and dropout during training'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution with pooling – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements the training and evaluation of a convolution classifier
    with the max pool and `Dropout` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Going deeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The convolution classifier with max pooling and dropout seems to be the best
    classifier so far. However, we also noticed that there was a slight amount of
    overfitting on the train data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's build a deeper model to see if we can create a classifier that is more
    accurate than the other models we have trained so far, and see if we can get it
    to reach an even better minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a deeper model by adding two more convolution layers to our best
    model so far:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is a convolution 2-D layer with 32 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is a convolution 2-D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third layer is a convolution 2-D layer with 128 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the code for the deeper model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c6da246-d210-45da-8a69-311a1f58142d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.28: Summary of the deep convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: From the summary, we can see that the deeper model has only `110,474` parameters.
    Now, let's see if a deeper model with fewer parameters can do a better job than
    we have done so far.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like we did previously, fit the model, but with `epochs` set as `40` instead
    of 20, since the deeper model takes longer to learn. Try training the model for
    20 epochs first to see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b362dee-0822-44f6-9e85-93421de78866.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01f12117-7e01-4ade-8fcc-45cb8d3c09dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.29: Metrics printed out during the training of the deep convolution
    classifier'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, evaluate the model with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c817d9c-5f72-48fe-b3c4-bc89d13f8b3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.30: Printout of the evaluation of the deep convolution classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the model is 99.01% accurate on the test data, 98.84% on the
    validation data, and 98.38% on the train data. The deeper convolution model with pooling
    and dropout gives a much better performance with just 110,000 parameters. If you
    look at the `loss` as well, this model was able to reach a better minima than
    the other models that we trained previously:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the metrics to understand how the training has progressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e92baa61-56d7-43d5-bfbd-2aae16e39bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.31: Loss/accuracy plot of the deep convolution classifier during training'
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the best training plots you can get. We can see no overfitting
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution with pooling and Dropout – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements the training and evaluation of a deep convolution classifier
    with the max pool and `Dropout` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a situation where you might want to build a convolution classifier on
    a small set of images. The problem here is that the classifier will easily overfit
    on this small set of data. The reason why the classifier will overfit is that
    there are very few images that are similar. That is, there are not a lot of variations
    for the model to capture within a specific class so that it can be robust and
    perform well on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides a preprocessing utility called `ImageDataGenerator` that can
    be used to augment image data with simple configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its capabilities include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`zoom_range`: Randomly zoom in on images to a given zoom level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horizontal_flip`: Randomly flip images horizontally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vertical_flip`: Randomly flip images vertically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale`: Multiply the data with the factor provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also includes capabilities for random rotations, random shear, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the official Keras documentation ([https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/))
    to learn more about some of the additional functionalities of the `image_data_generator` API.
  prefs: []
  type: TYPE_NORMAL
- en: Using ImageDataGenerator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `image_data_generator` API transforms and augments the data in batches on
    the go, and is also super easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `ImageDataGenerator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement a random horizontal flip augmenter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the augmenter on the train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: After the fit, we usually use the `transform` command. Here, instead of `transform`,
    we have the `flow` command. It accepts the images and its corresponding labels,
    and then generates batches of transformed data of the specified batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s transform a bunch of images and look at the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a115712-ae5d-4032-9f25-b05d9ac6cae3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.32: Digits after horizontal flip augmentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can implement a random zoom augmenter, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce701893-0f28-45d4-96c9-83e7cd303fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.33: Digits after zoom augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Fitting ImageDataGenerator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's build a classifier using the same architecture as the deep convolution
    model with pooling and Dropout, but on augmented data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the features of the `ImageDataGenerator`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We have defined that the `ImageDataGenerator` can perform the following operations
  prefs: []
  type: TYPE_NORMAL
- en: Rescaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random zoom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random horizontal flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rescaling operation scales the pixel values to a range between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to fit this generator on the train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to define and compile the deep convolution model like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we need to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60dafac5-58d2-45dd-9a27-3f75978b2878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5fae92c-b6f1-4ae0-addc-46b9e2630e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.34: Metrics printed out during the training of the deep convolution
    classifier on augmented data
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we need to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d368191-8ef0-4800-912b-8cf6cfc6245a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.35: Printout of the evaluation of the deep convolution classifier
    on augmented data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to plot the deep convolution classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c796851-f809-4951-99ca-b22b888d51c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.36: Loss/accuracy plot of the deep convolution classifier during training
    on augmented data'
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements the training and evaluation of a deep convolution classifier
    on augmented data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Additional topic – convolution autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An autoencoder is a combination of two parts: an encoder and a decoder. The
    encoder and decoder of a simple autoencoder are usually made up of dense layers,
    whereas in a convolution autoencoder, they are made of convolution layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca01c5ff-38df-424d-b324-035f7f964ac9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.37: The structure of an autoencoder (image source: Wikipedia)'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder part of the autoencoder accepts an image and compresses it into
    a smaller size with the help of a pooling operation. In our case, this is max
    pooling. The decoder accepts the input of the encoder and learns to expand the
    image to our desired size by using convolution and upsampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a situation where you want to build high-resolution images out of blurred
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3792613d-9ac2-4ab4-964e-678ad0ad96c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.38: Low-resolution digits on the left and high-resolution digits on
    the right'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution autoencoders are capable of doing this job very well. The preceding
    high-resolution digits that you can see were actually generated using convolution
    autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, you will have built a convolution autoencoder that
    accepts low-resolution 14*14*1 MNIST digits and generates high-resolution 28*28*1
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider restarting your session before starting this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Generating low-resolution images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate low-resolution images, define a function called `reshape()` that
    will resize the input image/digit to size `14`*`14`. After defining this, we will
    use the `reshape()` function to generate low-resolution train and test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '`XX_train` and `XX_test` will be the images that we will feed into the encoder,
    and `X_train` and `X_test` will be the targets.'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scale the train input, test input, and target images to range between 0 and
    1 so that the learning process is faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Defining the autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolution autoencoder we are going to build will accept 14*14*1 images
    as input with 28*28*1 images as the targets, and will have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is a convolution 2-D layer with 64 filters of size 3*3, followed
    by batch normalization, with `activation` as `relu`, followed by downsampling
    with `MaxPooling2D` of size 2*2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer, or the final layer in this encoder part, is again a convolution
    2-D layer with 128 filters of size 3*3, batch normalization, with `activation`
    as `relu`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is a convolution 2-D layer with 128 filters of size 3*3 with
    `activation` as `relu`, followed by upsampling that's performed with `UpSampling2D `
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is a convolution 2-D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by upsampling with `UpSampling2D`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third layer, or the final layer in this decoder part, is again a convolution
    2-D layer with 1 filter of size 3*3 with `activation` as `sigmoid`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the code for our autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2158296-4726-440e-8dbb-18237991eae7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.39: Autoencoder summary'
  prefs: []
  type: TYPE_NORMAL
- en: We are using `mean_squared_error` as the `loss`, as we want the model to predict
    the pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: If you take a look at the summary, the input image of size 14*14*1 is compressed
    along the width and the height dimensions to a size of 7*7, but is expanded along
    the channel dimension from 1 to 128\. These small/compressed feature maps are
    then fed to the decoder to learn the mappings that are required to generate high-resolution
    images of the defined dimension, which in this case is 28*28*1.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions about the usage of he Keras API, please visit the
    Keras official documentation at [https://keras.io/](https://keras.io/)[.](https://keras.io/)
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like any regular model fit, fit the autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aedb2da6-ebc4-4b7d-ba04-c87d3b280378.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19eeb2fe-1fa9-46d1-8115-f9821562ed81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.40: Printout during the training of the autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that inside the fit, we have specified a parameter called `validation_split`
    and that we have set it to `0.2`. This will split the train data into train and
    validation data, with validation data having 20% of the original train data.
  prefs: []
  type: TYPE_NORMAL
- en: Loss plot and test results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s get to plotting the train and validation loss progression during
    training. We will also plot the high-resolution image result from the model by
    feeding the test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0c7474d-8cf7-4c5e-915b-25802632cb27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.41: Train/val loss plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of high-resolution images that have been generated
    from low-resolution images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3eda9b-96b0-4237-9a71-29cbe5ac921c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.42: High-resolution test (28*28) images generated from low-resolution
    test (14*14) images'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder – Python file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module implements training an autoencoder on MNIST data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project was all about building a CNN classifier to classify handwritten
    digits better than we did in [Chapter 2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression,* with a multilayer Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Our deep convolution neural network classifier with max pooling and dropout
    hit 99.01% accuracy on a test set of 10,000 images/digits. This is good. This
    is almost 12% better than our multilayer Perceptron model.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some implications. What are the implications of this accuracy?
    It is important that we understand this. Just like we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* let's calculate the incidence of an error
    occurring that would result in a customer service issue.
  prefs: []
  type: TYPE_NORMAL
- en: Just to refresh our memory, in this hypothetical use case, we assumed that the restaurant
    has an average of 30 tables at each location, and that those tables turn over
    two times per night during the rush hour when the system is likely to be used,
    and finally that the restaurant chain has 35 locations. This means that each day
    of operation, there are approximately 21,000 handwritten numbers being captured
    (30 tables x 2 turns/day x 35 locations x 10-digit phone number).
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal is to classify all of the digits properly, since even a single-digit
    misclassification will result in a failure. With the classifier that we have built,
    it would improperly classify 208 digits per day. If we consider the worst case
    scenario, out of the 2,100 patrons, 208 phone numbers would be misclassified.
    That is, even in the worst case, 90.09% ((2,100-208)/2,100) of the time, we would
    be sending the text to the right patron.
  prefs: []
  type: TYPE_NORMAL
- en: The best case scenario would be that if all ten digits were misclassified in
    each phone number, we would only be improperly classifying 21 phone numbers. This
    means that we would have a failure rate of ((2,100-21)/2,100) 1%. This is as good
    as it gets.
  prefs: []
  type: TYPE_NORMAL
- en: Unless you aim at reducing that 1% error...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we understood how to implement a convolution neural network
    classifier in Keras. You now have a brief understanding of what convolution, average,
    max pooling, and dropout are, and you also built a deep model. You understood
    how to reduce overfitting as well as how to generate more/validation in data to
    build a generalizable model when you have less data than you need. Finally, we
    assessed the model's performance on test data and determined that we succeeded
    in achieving our goal. We ended this chapter by introducing you to autoencoders.
  prefs: []
  type: TYPE_NORMAL
