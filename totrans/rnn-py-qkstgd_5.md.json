["```py\nimport time\nimport tensorflow as tflow\nimport tensorlayer as tlayer\nfrom sklearn.utils import shuffle\nfrom tensorlayer.layers import EmbeddingInputlayer, Seq2Seq, DenseLayer, retrieve_seq_length_op2\n```", "```py\nfrom data.twitter import data\nmetadata, idx_q, idx_a = data.load_data(PATH='data/twitter/')\n(trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(idx_q, idx_a)\n```", "```py\nbatch_size = 34\nembedding_dimension = 1024\nlearning_rate = 0.0001\nnumber_epochs = 1000\n```", "```py\nxseq_len = len(trainX)\nyseq_len = len(trainY)\nassert xseq_len == yseq_len\n\nn_step = int(xseq_len/batch_size)\n\nw2idx = metadata['w2idx']\nidx2w = metadata['idx2w']\n\nxvocab_size = len(metadata['idx2w'])\nstart_id = xvocab_size\nend_id = xvocab_size+1\n\nw2idx.update({'start_id': start_id})\nw2idx.update({'end_id': end_id})\nidx2w = idx2w + ['start_id', 'end_id']\n\nxvocab_size = yvocab_size = xvocab_size + 2\n```", "```py\nxseq_len = len(trainX)\nyseq_len = len(trainY)\nassert xseq_len == yseq_len\n```", "```py\nw2idx = metadata['w2idx']\nidx2w = metadata['idx2w']\n```", "```py\nxvocab_size = len(metadata['idx2w'])\nstart_id = xvocab_size\nend_id = xvocab_size+1\n```", "```py\nencode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\ndecode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\ntarget_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\ntarget_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\")\n```", "```py\nnet_out, _ = model(encode_seqs, decode_seqs, is_train=True, reuse=False)\n```", "```py\nloss = tl.cost.cross_entropy_seq_with_mask(logits=net_out.outputs, target_seqs=target_seqs, input_mask=target_mask, name='cost')\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n```", "```py\ndef model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n   with tf.variable_scope(\"model\", reuse=reuse):\n        with tf.variable_scope(\"embedding\") as vs:\n            net_encode = EmbeddingInputlayer(\n                inputs = encode_seqs,\n                vocabulary_size = xvocab_size,\n                embedding_size = embedding_dimension,\n                name = 'seq_embedding')\n                vs.reuse_variables()\n            net_decode = EmbeddingInputlayer(\n                inputs = decode_seqs,\n                vocabulary_size = xvocab_size,\n                embedding_size = embedding_dimension,\n                name = 'seq_embedding')\n           net_rnn = Seq2Seq(net_encode, net_decode,\n                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n                n_hidden = embedding_dimension,\n                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n                encode_sequence_length = \n                retrieve_seq_length_op2(encode_seqs),\n                decode_sequence_length = \n                retrieve_seq_length_op2(decode_seqs),\n                initial_state_encode = None,\n                n_layer = 3,\n                return_seq_2d = True,\n                name = 'seq2seq')\n                net_out = DenseLayer(net_rnn, n_units=xvocab_size, \n                act=tf.identity, name='output')\n      return net_out, net_rnn\n```", "```py\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\nsess.run(tf.global_variables_initializer())\n```", "```py\ndef train():\n    print(\"Start training\")\n    for epoch in range(number_epochs):\n        epoch_time = time.time()\n        trainX_shuffled, trainY_shuffled = shuffle(trainX, trainY, \n        random_state=0)\n        total_err, n_iter = 0, 0\n\n        for X, Y in tl.iterate.minibatches(inputs=trainX_shuffled, \n        targets=trainY_shuffled, batch_size=batch_size, shuffle=False):\n\n            X = tl.prepro.pad_sequences(X)\n\n            _decode_seqs = tl.prepro.sequences_add_start_id(Y, \n             start_id=start_id, remove_last=False)\n            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)   \n\n            _target_seqs = tl.prepro.sequences_add_end_id(Y, \n             end_id=end_id)\n            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n\n            _, err = sess.run([optimizer, loss],\n                                {encode_seqs: X,\n                                decode_seqs: _decode_seqs,\n                                target_seqs: _target_seqs,\n                                target_mask: _target_mask})\n\n            if n_iter % 200 == 0:\n                print(\"Epoch[%d/%d] step:[%d/%d] loss:%f took:%.5fs\" % \n                (epoch, number_epochs, n_iter, n_step, err, time.time() \n                 - epoch_time))\n\n                total_err += err; n_iter += 1\n```", "```py\nencode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\ndecode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\nnet, net_rnn = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\ny = tf.nn.softmax(net.outputs)\n```", "```py\ndef predict():\n    seeds = [\"happy birthday have a nice day\",\n            \"the presidential debate held last night was spectacular\"]\n    for seed in seeds:\n        seed_id = [w2idx[w] for w in seed.split(\" \")]\n        for _ in range(5):  # 5 Replies\n            # 1\\. encode, get state\n            state = sess.run(net_rnn.final_state_encode,\n                            {encode_seqs2: [seed_id]})\n\n            # 2\\. decode, feed start_id, get first word\n            o, state = sess.run([y, net_rnn.final_state_decode],\n                            {net_rnn.initial_state_decode: state,\n                            decode_seqs2: [[start_id]]})\n\n            w_id = tl.nlp.sample_top(o[0], top_k=3)\n            w = idx2w[w_id]\n\n            # 3\\. decode, feed state iteratively\n            sentence = [w]\n            for _ in range(30): # max sentence length\n                o, state = sess.run([y, net_rnn.final_state_decode],\n                                {net_rnn.initial_state_decode: state,\n                                decode_seqs2: [[w_id]]})\n                w_id = tl.nlp.sample_top(o[0], top_k=2)\n                w = idx2w[w_id]\n                if w_id == end_id:\n                    break\n                sentence = sentence + [w]\n\n            print(\" >\", ' '.join(sentence))\n```"]