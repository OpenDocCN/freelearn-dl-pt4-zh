<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Going Further - 21 Problems</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to introduce 21 real life problems that you can use deep learning and TensorFlow to tackle. We will start by talking about some public large-scale datasets and competitions. Then, we will show some awesome TensorFlow projects on Github. We will also introduce some interesting projects that have been done in other deep learning frameworks so that you can get inspired and implement your own TensorFlow solution. Finally, we will work through a simple technique to convert a Caffe model to a TensorFlow model and introduce using a high-level TensorFlow library, TensorFlow-Slim.</p>
<p>In this chapter, we will look into the following topics:</p>
<ul>
<li>Large-scale, public datasets and competitions</li>
<li>Awesome <span>TensorFlow </span>projects</li>
<li>Some inspired deep learning projects from other frameworks</li>
<li>Converting a Caffe model to <span>TensorFlow</span></li>
<li>Introducing <span>TensorFlow</span>-Slim</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset and challenges</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you some popular datasets and competitions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 1 - ImageNet dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="http://image-net.org/"><span class="URLPACKT">http://image-net.org/</span></a></p>
<p><strong>ImageNet</strong> is a large scale visual recognition challenge that has runs annually since 2010. The dataset is organized according to the WorkNet hierarchy. There are over ten million URLs of images with hand-annotated labels to indicate what objects are in the picture. There are at least one million images that have bounding boxes included.</p>
<p>The ImageNet challenge is held every year to evaluate algorithms for the following three problems:</p>
<ul>
<li>Object localization for 1,000 categories.</li>
<li>Object detection for 200 fully-labeled categories.</li>
<li>Object detection from video for 30 fully labeled categories. In July 17, 2017, the results of the 2017 challenge were announced with many advanced and interesting algorithms.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 2 - COCO dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="http://mscoco.org/"><span class="URLPACKT">http://mscoco.org/</span></a></p>
<p>COCO is a dataset for image recognition, segmentation, and captioning sponsored by Microsoft. There are 80 object categories in this dataset with more than 300,000 images and two million instances. There are also challenges for detections, captions, and key-points every year.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 3 - Open Images dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/openimages/dataset"><span class="URLPACKT">https://github.com/openimages/dataset</span></a></p>
<p>Open Images is a new dataset from Google, with over nine million URLs spanning over 6000 categories. Each image is processed by Google's vision model and verified by a human. As of July 20, 2017, there are also over two million bounding box annotations spanning over 600 objects.</p>
<p>The difference is that Open Images covers more real-life objects than others, which can be very useful when developing real-life applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 4 - YouTube-8M dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://research.google.com/youtube8m/"><span class="URLPACKT">https://research.google.com/youtube8m/</span></a></p>
<p>YouTube-8M is a large-scale video dataset from Google with 7 million video URLs over 4,716 classes and 450,000 hours of video. Google also provides pre-computed, state-of-the-art audio-visual features, so that one can build their model based on these features with ease. Training from raw videos may take weeks, which is not reasonable in normal situations. This dataset's goal is to achieve video understanding, representation learning, noisy data modeling, transfer learning, and domain adaptation for videos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 5 - AudioSet dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://research.google.com/audioset/"><span class="URLPACKT">https://research.google.com/audioset/</span></a></p>
<p>AudioSet is a large-scale audio events dataset from Google with 632 audio event classes and a collection of over 2.1 million manually annotated sound clips. Audio classes span from human and animal sounds to musical instruments and common, everyday environmental sounds. Using this dataset, you can create a system to recognize audio events for audio understanding, security applications, and much more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 6 - LSUN challenge</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="http://lsun.cs.princeton.edu/2017/">http://lsun.cs.princeton.edu/2017/</a></p>
<p>LSUN challenge provides a large scale scene understanding dataset covering three major problems:</p>
<ul>
<li>Scene classification</li>
<li>Segmentation task on street images</li>
<li>Saliency prediction</li>
</ul>
<p>In scene classification problems, the expected output of the algorithm is the top most likely scene category in the image. At the time of writing, there are 10 different classes such as bedroom, classroom, and restaurant. In the segmentation problem, you can try to solve the pixel-level segmentation and instance-specific segmentation. In saliency prediction problems, the goal is to predict where a human looks in a scene image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 7 - MegaFace dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="http://megaface.cs.washington.edu/"><span class="URLPACKT">http://megaface.cs.washington.edu/</span></a></p>
<p>MegaFace provides a large scale dataset for face recognition. The MegaFace dataset is divided into three parts:</p>
<ul>
<li>Training set</li>
<li>Test set</li>
<li>Distractors</li>
</ul>
<p>The <strong>Training set</strong> contains 4.7 million photos of over 672,057 unique identities. The <strong>test set</strong> contains the images from the FaceScrub and FGNet dataset. The <strong>distractors</strong> contain one million photos of 690,572 unique users. Currently, there are two challenges in the MegaFace website. In challenge 1, you can train using any dataset and test your method with the one million distractors. Your method needs to discriminate between a set of known people while classifying the distractors as unknown people. In challenge 2, you will train using the training set with 672K unique identities and test with 1 million distractors. MegaFace is currently the largest dataset for face recognition at the time of writing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 8 - Data Science Bowl 2017 challenge</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://www.kaggle.com/c/data-science-bowl-2017">https://www.kaggle.com/c/data-science-bowl-2017</a></p>
<p>Data Science Bowl 2017 is a one million dollar challenge focused on lung cancer detection. In the dataset, you will be given over a thousand CT images of high-risk patients. The goal of this challenge is to create an automatic system that can determine whether a patient will be diagnosed with lung cancer within one year. This is a very interesting and important project to work on that will save thousands of people in the near future.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 9 - StarCraft Game dataset</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/TorchCraft/StarData"><span class="URLPACKT">https://github.com/TorchCraft/StarData</span></a></p>
<p>This is the largest StarCraft--Brood War replay dataset at the time of writing this book. This dataset contains more than 60,000 games in 365GB, 1535 million frames, and 496 million player actions. This dataset is best suit for those who want to research about AI game playing. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow-based Projects</h1>
                </header>
            
            <article>
                
<p>In this section, we will introduce you to several problems that are implemented in <span>TensorFlow </span>and open-source on Github. We suggest that you take a look at these projects and learn how to improve your <span>TensorFlow </span>skills.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 10 - Human Pose Estimation</h1>
                </header>
            
            <article>
                
<p>Project Link: <a href="https://github.com/eldar/pose-tensorflow"><span class="URLPACKT">https://github.com/eldar/pose-tensorflow</span></a></p>
<p>This project is the open-source implementation of Deep Cut and ArtTrack in human body pose estimation. The goal of this project is to jointly solve the tasks of detection and pose estimation. We can use this method for various applications such as person detection in security or human action understanding. This project also provides great starting points for a lot of further research on human shape estimation with applications for virtual-try-on or garment recommendation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 11 - Object Detection - YOLO</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/thtrieu/darkflow">https://github.com/thtrieu/darkflow</a></p>
<p>Object detection is an interesting problem in Computer Vision. There are lots of methods to solve this problem. YOLO, by Joseph Redmon and others, is one of the state-of-the-art techniques. YOLO provides real-time object detection using deep neural networks. Version 2 of YOLO can recognize up to 9,000 different objects with high accuracy in real time. The original YOLO project is programmed in the darknet framework.</p>
<p>In <span>TensorFlow</span>, there is a great implementation of YOLO, called <strong>darkflow</strong>. The darkflow repository even has the utility that can allow you to export the model and serve on mobile devices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 12 - Object Detection - Faster RCNN</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/smallcorgi/Faster-RCNN_TF"><span class="URLPACKT">https://github.com/smallcorgi/Faster-RCNN_TF</span></a></p>
<p>Faster RCNN is another state-of-the-art method for Object Detection. This method offers high precision on the result and also inspires lots of methods for many other problems. The inference speed of Faster RCNN is not as fast as YOLO. However, if you need high precision on the detection results, you may want to consider Faster RCNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 13 - Person Detection - tensorbox</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/Russell91/TensorBox">https://github.com/Russell91/TensorBox</a></p>
<p>Tensorbox is a <span>TensorFlow </span>implementation of the method by Russell Stewart and Mykhaylo Andriluka. The goal of this method is a bit different from the preceding methods. Tensorbox focuses on solving the problem of crowd person detection. They use a recurrent LSTM layer for sequence generation of the bounding boxes and define a new loss function that operates of the set of detection results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 14 - Magenta</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/tensorflow/magenta"><span class="URLPACKT">https://github.com/tensorflow/magenta</span></a></p>
<p>Magenta is a project from the Google Brain team that focuses on Music and Art Generation using Deep Learning. This is a very active repository with many implementations of interesting problems such as image stylization, melody generation, or generating sketches. You can visit the following link to have access to Magenta's models:</p>
<p><a href="https://github.com/tensorflow/magenta/tree/master/magenta/models"><span class="URLPACKT">https://github.com/tensorflow/magenta/tree/master/magenta/models</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 15 - Wavenet</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/ibab/tensorflow-wavenet"><span class="URLPACKT">https://github.com/ibab/tensorflow-wavenet</span></a></p>
<p>WaveNet is a neural network architecture for audio generation from Google Deep Mind. WaveNet is trained to generate raw audio waveform and has shown good results for text-to-speech and audio generation. According to Deep Mind, WaveNet reduced the gap between the previous methods and human-level performance by over 50% in text-to-speech problems for both US English and Mandarin Chinese.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 16 - Deep Speech</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/mozilla/DeepSpeech"><span class="URLPACKT">https://github.com/mozilla/DeepSpeech</span></a></p>
<p>Deep Speech is an open source speech-to-text engine, based on a research paper from Baidu. Speech-to-text is a very interesting problem and Deep Speech is one of the state-of-the-art methods for solving it. With the <span>TensorFlow </span>implementation of Mozilla, you can even learn how to use <span>TensorFlow </span>across more than one machine. However, there is still a problem that personal researchers can't access the same large scale speech-to-text datasets as a large company. So, even though we can use Deep Speech or implement it ourselves, it is still hard to have a good model for production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interesting Projects</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you some interesting projects that are implemented in other deep learning frameworks. These projects give significant results over very difficult problems. You may want to challenge yourself to implement these methods in <span>TensorFlow</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 17 - Interactive Deep Colorization - iDeepColor</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://richzhang.github.io/ideepcolor/"><span class="URLPACKT">https://richzhang.github.io/ideepcolor/</span></a></p>
<p>Interactive Deep Colorization is research being carried out by Richard Zhang and Jun-Yan Zun, and others, for user-guided image colorization. In this system, users can give the network a few hints of colors for some points in the image and the network will propagate user inputs along with semantic information learned from large scale data. The colorization can be performed in real time with one single forward pass.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 18 - Tiny face detector</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/peiyunh/tiny"><span class="URLPACKT">https://github.com/peiyunh/tiny</span></a></p>
<p>This project is a face detector that focuses on finding the small faces in the image by Peiyun Hu and Deva Ramanan. While most face detectors only focus on large objects in the image, this tiny face detector method can work with very small faces, but still, reduce the error by a factor of two compared with prior methods on the WIDER FACE dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 19 - People search</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/ShuangLI59/person_search"><span class="URLPACKT">https://github.com/ShuangLI59/person_search</span></a></p>
<p>This project is the implementation of the paper by Tong Xiao, and others that focuses on the problem of person detection and re-identification. This project can be used in video surveillance. The existing person re-identification methods mainly assume that the person is cropped and aligned. However, in real-world scenarios, the person detection algorithm may fail to extract the perfect crop region of the person and lower the identification accuracy. In this project, the authors solve detection and identification jointly in a novel architecture inspired by Faster RCNN. The current project is implemented in the Caffe Deep Learning Framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 20 - Face Recognition - MobileID</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/liuziwei7/mobile-id"><span class="URLPACKT">https://github.com/liuziwei7/mobile-id</span></a></p>
<p>This project provides an extremely fast face recognition system that can run in 250 FPS with high accuracy. The model is learned by using the output of the state-of-the-art face recognition DeepID. However, the mobile ID model can perform so fast that it can be used in situations where processing and memory are limited.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem 21 - Question answering - DrQA</h1>
                </header>
            
            <article>
                
<p>Project link: <a href="https://github.com/facebookresearch/DrQA"><span class="URLPACKT">https://github.com/facebookresearch/DrQA</span></a></p>
<p>DrQA is a system for open-domain question answering from Facebook. DrQA focuses on solving the task of <em>machine reading</em> where the model will try to understand the Wikipedia documents and give the answer for any question from users. The current project is implemented in PyTorch. You may find it interesting to implement our own solution in <span>TensorFlow</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caffe to TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you how to take advantage of many pre-trained models from Caffe Model Zoo (<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">https://github.com/BVLC/caffe/wiki/Model-Zoo</a>). There are lots of Caffe models for different tasks with all kinds of architectures. After converting these models to <span>TensorFlow</span>, you can use it as a part of your architectures or you can fine-tune our model for different tasks. Using these pre-trained models as initial weights is an effective approach for training instead of training from scratch. We will show you how to use a <kbd>caffe-to-tensorflow</kbd> approach from Saumitro Dasgupta at <a href="https://github.com/ethereon/caffe-tensorflow">https://github.com/ethereon/caffe-tensorflow</a>.</p>
<p>However, there are lots of differences between Caffe and <span>TensorFlow</span>. This technique only supports a subset of layer types from Caffe. Even though there are some Caffe architectures that are verified by the author of this project such as ResNet, VGG, and GoogLeNet.</p>
<p>First, we need to clone the <kbd>caffe-tensorflow</kbd> repository using the <kbd>git clone</kbd> command:</p>
<pre><strong>ubuntu@ubuntu-PC:~/github$ git clone https://github.com/ethereon/caffe-tensorflow</strong>
<strong>Cloning into 'caffe-tensorflow'...</strong>
<strong>remote: Counting objects: 479, done.</strong>
<strong>remote: Total 479 (delta 0), reused 0 (delta 0), pack-reused 479</strong>
<strong>Receiving objects: 100% (510/510), 1.71 MiB | 380.00 KiB/s, done.</strong>
<strong>Resolving deltas: 100% (275/275), done.</strong>
<strong>Checking connectivity... done.</strong></pre>
<p>Then, we need to change the directory to the <kbd>caffe-to-tensorflow</kbd> directory and run the convert python script to see some help messages:</p>
<pre><strong>cd caffe-tensorflow</strong>
<strong>python convert.py -h</strong>
<strong>The resulting console will look like this:</strong>
<strong>usage: convert.py [-h] [--caffemodel CAFFEMODEL]</strong>
    <strong>              [--data-output-path DATA_OUTPUT_PATH]</strong>
    <strong>              [--code-output-path CODE_OUTPUT_PATH] [-p PHASE]</strong>
    <strong>              def_path</strong>
    
<strong>positional arguments:</strong>
<strong>def_path              Model definition (.prototxt) path</strong>
    
<strong>optional arguments:</strong>
  <strong>-h, --help            show this help message and exit</strong>
  <strong>--caffemodel CAFFEMODEL</strong>
    <strong>                    Model data (.caffemodel) path</strong>
 <strong> --data-output-path DATA_OUTPUT_PATH</strong>
    <strong>                    Converted data output path</strong>
  <strong>--code-output-path CODE_OUTPUT_PATH</strong>
    <strong>                    Save generated source to this path</strong>
  <strong>-p PHASE, --phase PHASE</strong>
    <strong>                    The phase to convert: test (default) or train</strong></pre>
<p>According to this help message, we can know the parameters of the <kbd>convert.py</kbd> script. In summary, we will use this <kbd>convert.py</kbd> to create the network architecture in <span>TensorFlow </span>with the flag code-output-path and convert the pre-trained weights with the flag data-output-path.</p>
<p>Before we start converting the models, we need to get some pull requests from contributors of this project. There are some issues with the current master branch that we can't use the latest <span>TensorFlow </span>(version 1.3 at the time of writing) and python-protobuf (version 3.4.0 at the time of writing). Therefore, we will get the code using the following pull requests:</p>
<p><a href="https://github.com/ethereon/caffe-tensorflow/pull/105"><span class="URLPACKT">https://github.com/ethereon/caffe-tensorflow/pull/105</span></a></p>
<p><a href="https://github.com/ethereon/caffe-tensorflow/pull/133"><span class="URLPACKT">https://github.com/ethereon/caffe-tensorflow/pull/133</span></a></p>
<p>You need to open the preceding links to see if the pull requests are merged or not. If it is still in <kbd>open</kbd> status, you will need to follow the next part. Otherwise, you can skip the merged <kbd>pull</kbd> requests.</p>
<p>First, we will get the code from pull request <kbd>105</kbd>:</p>
<pre><strong>ubuntu@ubuntu-PC:~/github$ git pull origin pull/105/head</strong>
<strong>remote: Counting objects: 33, done.</strong>
<strong>remote: Total 33 (delta 8), reused 8 (delta 8), pack-reused 25</strong>
<strong>Unpacking objects: 100% (33/33), done.</strong>
<strong>From https://github.com/ethereon/caffe-tensorflow</strong>
<strong>* branch            refs/pull/105/head -&gt; FETCH_HEAD</strong>
<strong>Updating d870c51..ccd1a52</strong>
<strong>Fast-forward</strong>
<strong>.gitignore                               |  5 +++++</strong>
<strong>convert.py                               |  8 ++++++++</strong>
<strong>examples/save_model/.gitignore           | 11 ++++++++++</strong>
<strong>examples/save_model/READMD.md            | 17 ++++++++++++++++</strong>
<strong>examples/save_model/__init__.py          |  0</strong>
<strong>examples/save_model/save_model.py        | 51 ++++++++++++++++++++++++++++++++++++++++++++++</strong>
<strong>kaffe/caffe/{caffepb.py =&gt; caffe_pb2.py} |  0</strong>
<strong>kaffe/caffe/resolver.py                  |  4 ++--</strong>
<strong>kaffe/tensorflow/network.py              |  8 ++++----</strong>
<strong>9 files changed, 98 insertions(+), 6 deletions(-)</strong>
<strong>create mode 100644 examples/save_model/.gitignore</strong>
<strong>create mode 100644 examples/save_model/READMD.md</strong>
<strong>create mode 100644 examples/save_model/__init__.py</strong>
<strong>create mode 100755 examples/save_model/save_model.py</strong>
<strong>rename kaffe/caffe/{caffepb.py =&gt; caffe_pb2.py} (100%)</strong></pre>
<p>Then, from pull request <kbd>133</kbd>:</p>
<pre><strong>- git pull origin pull/133/head</strong>
<strong>remote: Counting objects: 31, done.</strong>
<strong>remote: Total 31 (delta 20), reused 20 (delta 20), pack-reused 11</strong>
<strong>Unpacking objects: 100% (31/31), done.</strong>
<strong>From https://github.com/ethereon/caffe-tensorflow</strong>
<strong>* branch            refs/pull/133/head -&gt; FETCH_HEAD</strong>
<strong>Auto-merging kaffe/tensorflow/network.py</strong>
<strong>CONFLICT (content): Merge conflict in kaffe/tensorflow/network.py</strong>
<strong>Auto-merging .gitignore</strong>
<strong>CONFLICT (content): Merge conflict in .gitignore</strong>
<strong>Automatic merge failed; fix conflicts and then commit the result.</strong></pre>
<p>As you can see, there are some conflicts in the <kbd>kaffe/tensorflow/network.py</kbd> file. We will show you how to resolve these <kbd>conflicts</kbd>, as follows.</p>
<p>First, we will solve the conflict at line <span class="packt_screen">137</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/8cbf68bc-d3f7-4fb7-997b-3f6daa91a467.png"/></div>
<p>We remove the HEAD part from line <span class="packt_screen">137</span> to line <span class="packt_screen">140</span>. The final result will look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/24ec3b97-1597-4c60-87e7-ea065ad10bcc.png"/></div>
<p>Next, we will solve the conflict at line <span class="packt_screen">185</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="128" width="647" class=" image-border" src="assets/e98bcebb-7236-47f4-8628-55673a01f83a.png"/></div>
<p>We also remove the HEAD part from line <span class="packt_screen">185</span> to line <span class="packt_screen">187</span>. The final result will look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="77" width="644" class=" image-border" src="assets/699ebeba-5219-4f3e-851c-c7970784f55e.png"/></div>
<p>In the <kbd>caffe-to-tensorflow</kbd> directory, there is a directory named examples that contains the code and data for the MNIST and ImageNet challenge. We will show you how to work with the MNIST model. The ImageNet challenge is not much different.</p>
<p>First, we will convert the MNIST architecture from Caffe to <span>TensorFlow </span>using the following command:</p>
<pre>    <strong>ubuntu@ubuntu-PC:~/github$ python ./convert.py examples/mnist/lenet.prototxt --code-output-path=./mynet.py</strong>
    <strong>The result will look like this:</strong>
    
    <strong>------------------------------------------------------------</strong>
    <strong>    WARNING: PyCaffe not found!</strong>
    <strong>    Falling back to a pure protocol buffer implementation.</strong>
    <strong>    * Conversions will be drastically slower.</strong>
    <strong>    * This backend is UNTESTED!</strong>
    <strong>------------------------------------------------------------</strong>
    
    <strong>Type                 Name                                          Param               Output</strong>
    <strong>----------------------------------------------------------------------------------------------</strong>
    <strong>Input                data                                             --      (64, 1, 28, 28)</strong>
    <strong>Convolution          conv1                                            --     (64, 20, 24, 24)</strong>
    <strong>Pooling              pool1                                            --     (64, 20, 12, 12)</strong>
    <strong>Convolution          conv2                                            --       (64, 50, 8, 8)</strong>
    <strong>Pooling              pool2                                            --       (64, 50, 4, 4)</strong>
    <strong>InnerProduct         ip1                                              --      (64, 500, 1, 1)</strong>
    <strong>InnerProduct         ip2                                              --       (64, 10, 1, 1)</strong>
    <strong>Softmax              prob                                             --       (64, 10, 1, 1)</strong>
    <strong>Converting data...</strong>
    <strong>Saving source...</strong>
    <strong>Done.</strong></pre>
<p>Then, we will convert the MNIST pre-trained Caffe model at <kbd>examples/mnist/lenet_iter_10000.caffemodel</kbd> using the following command:</p>
<pre style="padding-left: 60px"> ubuntu@ubuntu-PC:~/github$ python ./convert.py  <br/> examples/mnist/lenet.prototxt --caffemodel  <br/> examples/mnist/lenet_iter_10000.caffemodel --data-output- <br/> path=./mynet.npy</pre>
<p>The result will look like this:</p>
<pre>    <strong>------------------------------------------------------------</strong>
    <strong>    WARNING: PyCaffe not found!</strong>
    <strong>    Falling back to a pure protocol buffer implementation.</strong>
    <strong>    * Conversions will be drastically slower.</strong>
    <strong>    * This backend is UNTESTED!</strong>
    <strong>------------------------------------------------------------</strong>
    
    <strong>Type                 Name                                          Param               Output</strong>
    <strong>----------------------------------------------------------------------------------------------</strong>
    <strong>Input                data                                             --      (64, 1, 28, 28)</strong>
    <strong>Convolution          conv1                                 <br/>(20, 1, 5, 5)     (64, 20, 24, 24)</strong>
    <strong>Pooling              pool1                                            --     (64, 20, 12, 12)</strong>
    <strong>Convolution          conv2                               <br/> (50, 20, 5, 5)       (64, 50, 8, 8)</strong>
    <strong>Pooling              pool2                                            --       (64, 50, 4, 4)</strong>
    <strong>InnerProduct         ip1                                   <br/>   (500, 800)      (64, 500, 1, 1)</strong>
    <strong>InnerProduct         ip2                                      <br/> (10, 500)       (64, 10, 1, 1)</strong>
    <strong>Softmax              prob                                             --       (64, 10, 1, 1)</strong>
    <strong>Converting data...</strong>
    <strong>Saving data...</strong>
    <strong>Done.</strong></pre>
<p>As you can see, these commands will create a python file named <kbd>mynet.py</kbd> and a <kbd>numpy</kbd> file named <kbd>mynet.npy</kbd> in the current directory. We also need to add the current directory to the <kbd>PYTHONPATH</kbd> to allow the further code to import <kbd>mynet.py</kbd>:</p>
<pre><strong>ubuntu@ubuntu-PC:~/github$ export PYTHONPATH=$PYTHONPATH:.</strong>
<strong>ubuntu@ubuntu-PC:~/github$ python examples/mnist/finetune_mnist.py</strong>
<strong>....</strong>
<strong>('Iteration: ', 900, 0.0087626642, 1.0)</strong>
<strong>('Iteration: ', 910, 0.018495116, 1.0)</strong>
<strong>('Iteration: ', 920, 0.0029206357, 1.0)</strong>
<strong>('Iteration: ', 930, 0.0010091728, 1.0)</strong>
<strong>('Iteration: ', 940, 0.071255416, 1.0)</strong>
<strong>('Iteration: ', 950, 0.045163739, 1.0)</strong>
<strong>('Iteration: ', 960, 0.005758767, 1.0)</strong>
<strong>('Iteration: ', 970, 0.012100354, 1.0)</strong>
<strong>('Iteration: ', 980, 0.12018739, 1.0)</strong>
<strong>('Iteration: ', 990, 0.079262167, 1.0)</strong></pre>
<p>The last two numbers in each line is the loss and accuracy of the fine-tune process. You can see that the fine-tune process can easily achieve 100% accuracy with the pre-trained weights from the Caffe model.</p>
<p>Now, we will take a look at the <kbd>finetune_mnist.py</kbd> file to see how the pre-trained weights are used.</p>
<p>First, they import the <kbd>mynet</kbd> python with the following code:</p>
<pre>    from mynet import LeNet as MyNet  </pre>
<p>Then, they create some placeholders for <kbd>images</kbd> and <kbd>labels</kbd> and compute the <kbd>loss</kbd> using the layers <kbd>ip2</kbd> as follows:</p>
<pre style="padding-left: 60px"> images = tf.placeholder(tf.float32, [None, 28, 28, 1]) 
 labels = tf.placeholder(tf.float32, [None, 10]) 
 net = MyNet({'data': images}) 
 
 ip2 = net.layers['ip2'] 
 pred = net.layers['prob'] 
 
 loss =  <br/> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=ip2,  <br/> labels=labels), 0) 
 Finally, they load the numpy file into the graph, using the load  <br/> method in the network class. 
 with tf.Session() as sess: 
    # Load the data 
    sess.run(tf.global_variables_initializer()) 
    net.load('mynet.npy', sess) </pre>
<p>After that, the fine-tune process is independent from the Caffe framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow-Slim</h1>
                </header>
            
            <article>
                
<p>TensorFlow-Slim is a light-weight library for defining, training, and evaluating complex models in <span>TensorFlow</span>. With the <span>TensorFlow</span>-Slim library, we can build, train, and evaluate the model easier by providing lots of high-level layers, variables, and regularizers. We recommend that you take a look at the <span>TensorFlow</span>-Slim library at the following link: <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim"><span class="URLPACKT">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim</span></a></p>
<p>There are also lots of pre-trained models that are provided using <span>TensorFlow</span>-Slim. You can take advantage of high-level <span>TensorFlow </span>layers and models at the following link:</p>
<p><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have provided lots of interesting challenges and problems that you can try to solve and learn from to improve your TensorFlow skills. At the end of this chapter, we also guided you to convert the Caffe model to <span>TensorFlow </span>and introduced you to the high-level <span>TensorFlow </span>library, <span>TensorFlow</span>-Slim.</p>


            </article>

            
        </section>
    </body></html>