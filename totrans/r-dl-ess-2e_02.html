<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training a Prediction Model</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">This chapter shows you how to build and train basic neural networks in R through hands-on examples and shows how to evaluate different hyper-parameters for models to find the best set. Another important issue in deep learning is dealing with overfitting, which is when a model performs well on the data it was trained on but poorly on unseen data. We will briefly look at this topic in this chapter, and cover it in more depth in <a href="6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml">Chapter 3</a>, <em>Deep Learning Fundamentals</em>. The chapter closes with an example use case classifying activity data from a smartphone as walking, going up or down stairs, sitting, standing, or lying down.</p>
<p class="mce-root">This chapter covers the following topics:</p>
<ul>
<li class="mce-root">Neural networks in R</li>
<li>Binary classification</li>
<li>Visualizing a neural network</li>
<li>Multi-classification using the nnet and RSNNS packages</li>
<li class="mce-root">The problem of overfitting data—the consequences explained</li>
<li class="mce-root">Use case—building and applying a neural network</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks in R</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will build several neural networks in this section. First, we will use the neuralnet package to create a neural network model that we can visualize. We will also use the <kbd>nnet</kbd> and <kbd>RSNNS</kbd> (Bergmeir, C., and Benítez, J. M. (2012)) packages. These are standard R packages and can be installed by the <kbd>install.packages</kbd> command or from the packages pane in RStudio. Although it is possible to use the <kbd>nnet</kbd> package directly, we are going to use it through the <kbd>caret</kbd> package, which is short for <strong>Classification and Regression Training</strong>. The <kbd>caret</kbd> package provides a standardized interface to work with many <strong>machine learning</strong> (<strong>ML</strong>) models in R, and also has some useful features for validation and performance assessment that we will use in this chapter and the next.</p>
<p class="mce-root">For our first examples of building neural networks, we will use the <kbd>MNIST</kbd> dataset, which is a classic classification problem: recognizing handwritten digits based on pictures. The data can be downloaded from the Apache MXNet site (<a href="https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip">https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip</a>). It is in the CSV format, where each column of the dataset, or feature, represents a pixel from the image. Each image has 784 pixels (28 x 28) and the pixels are in grayscale and range from 0 to 255. The first column contains the digit label, and the rest are pixel values, to be used for classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building neural network models</h1>
                </header>
            
            <article>
                
<p class="mce-root">The code is in the <kbd>Chapter2</kbd> folder of the code for this book. If you have not already downloaded and unzipped the code, go back to <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, for the link to download the code. Unzip the code into a folder in your machine, and you will see folders for different chapters. The code we will be following is <kbd>Chapter2\chapter2.R</kbd>.</p>
<p class="mce-root">We will use the <kbd>MNIST</kbd> dataset to build some neural network models. The first few lines in the script look to see whether the data file (<kbd>train.csv</kbd>) is in the data directory. If the file already exists in the data directory then it proceeds; if it isn't, it downloads a ZIP file from <a href="https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip">https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip</a>, and unzips it into the data folder. This check means that you don't have to download the data manually and the program only downloads the file once. <span>Here is the code to download the data:<br/></span></p>
<pre>dataDirectory &lt;- "../data"<br/>if (!file.exists(paste(dataDirectory,'/train.csv',sep="")))<br/>{<br/>  link &lt;- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'<br/>  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))<br/>    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))<br/>  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)<br/>  if (file.exists(paste(dataDirectory,'/test.csv',sep="")))<br/>    file.remove(paste(dataDirectory,'/test.csv',sep=""))<br/>}</pre>
<p>As an alternative, the <kbd>MNIST</kbd> data is also available in Keras, so we can download it from that library and save it as a CSV file:</p>
<pre>if (!file.exists(paste(dataDirectory,'/train.csv',sep="")))<br/>{<br/> library(keras)<br/> mnist &lt;- dataset_mnist()<br/> c(c(x_train,y_train),c(x_test,y_test)) %&lt;-% dataset_mnist()<br/> x_train &lt;- array_reshape(x_train,c(dim(x_train)[1],dim(x_train)[2]*dim(x_train)[3]))<br/> y_train &lt;- array_reshape(y_train,c(length(y_train),1))<br/> data_mnist &lt;- as.data.frame(cbind(y_train,x_train))<br/> colnames(data_mnist)[1] &lt;- "label"<br/> colnames(data_mnist)[2:ncol(data_mnist)] &lt;- paste("pixel",seq(1,784),sep="")<br/> write.csv(data_mnist,paste(dataDirectory,'/train.csv',sep=""),row.names = FALSE)<br/>}</pre>
<p>When you load any new dataset for the first time, the first thing you should do is a quick check on the data to ensure that the number of rows and columns are as expected, as shown in the following code:</p>
<pre>digits &lt;- read.csv("../data/train.csv")<br/>dim(digits)<br/>[1] 42000 785<br/><br/>head(colnames(digits), 4)<br/>[1] "label" "pixel0" "pixel1" "pixel2"<br/><br/>tail(colnames(digits), 4)<br/>[1] "pixel780" "pixel781" "pixel782" "pixel783"<br/><br/>head(digits[, 1:4])<br/>  label pixel0 pixel1 pixel2<br/>1     1      0      0      0<br/>2     0      0      0      0<br/>3     1      0      0      0<br/>4     4      0      0      0<br/>5     0      0      0      0<br/>6     0      0      0      0</pre>
<p>The data looks OK, we have <kbd>42000</kbd> rows and <kbd>785</kbd> columns. The header was imported correctly and the values are numeric. Now that we have loaded the data and performed some validation checks on it, we can move on to modeling. Our first model will use the <kbd>neuralnet</kbd> library as this allows us to visualize the neural net. <span>We will select only the rows where the label is either 5 or 6, and </span>create a binary classification task to differentiate between them. <span>Of course, you can pick any digits you choose, but using 5 and 6 is a good choice because </span>they are similar graphically, and therefore our model will have to work harder than if we picked two digits that were not so similar, for example, 1 and 8. We rename the labels as 0 and 1 for modeling and then separate that data into a train and a test split.</p>
<p>We then perform dimensionality-reduction using <strong>p<span>rincipal components analysis</span></strong> (<strong>PCA</strong>) on the training data—we use PCA because we want to reduce the number of predictor variables in our data to a reasonable number for plotting. PCA requires that we remove columns that have zero variance; these are the columns that have the same value for each instance. In our image data, there is a border around all images, that is, the values are all zero. Note how we find the columns that have zero variance using only the data used to train the model; it would be incorrect to apply this check first and then split the data for modelling.</p>
<div class="packt_infobox"><strong>Dimensionality-reduction</strong>: Our image data is grayscale data with values from 0 (black) to 255 (white). These values are highly correlated, that is, if a pixel is black (<span>that is,</span> 0), it is likely that the pixels around it are either black or dark gray. Similarly <span>if a pixel is white (255), it is likely that the pixels around it are either white or light gray. Dimensionality-reduction is an unsupervised machine learning technique that takes an input dataset and produces an output dataset with the same<br/>
number of rows but fewer columns. Crucially though, these fewer columns can explain most of the signal in the input dataset. PCA is one dimensionality-reduction algorithm. We use it here because we want to create a dataset with a small number of columns to plot the network, but we still want our algorithm to produce good results.</span></div>
<div>
<p>The following code selects the rows where the label is either 5 or 6 and creates a train/test split. It also removes columns where the variance is zero; these are columns that have the same value for every row:</p>
</div>
<pre>digits_nn &lt;- digits[(digits$label==5) | (digits$label==6),]<br/>digits_nn$y &lt;- digits_nn$label<br/>digits_nn$label &lt;- NULL<br/>table(digits_nn$y)<br/>   5    6 <br/>3795 4137 <br/><br/>digits_nn$y &lt;- ifelse(digits_nn$y==5, 0, 1)<br/>table(digits_nn$y)<br/>   0    1 <br/>3795 4137 <br/> <br/>set.seed(42)<br/>sample &lt;- sample(nrow(digits_nn), nrow(digits_nn)*0.8)<br/>test &lt;- setdiff(seq_len(nrow(digits_nn)), sample)<br/> <br/>digits.X2 &lt;- digits_nn[,apply(digits_nn[sample,1:(ncol(digits_nn)-1)], 2, var, na.rm=TRUE) != 0]<br/>length(digits.X2)<br/>[1] 624</pre>
<p>We have reduced the number of column data from <kbd>784</kbd> to <kbd>624</kbd>, that is, <kbd>160</kbd> columns had the same value for all rows. Now, we perform PCA on the data and plot the cumulative sum of the variances:</p>
<pre>df.pca &lt;- prcomp(digits.X2[sample,],center = TRUE,scale. = TRUE) <br/>s&lt;-summary(df.pca)<br/>cumprop&lt;-s$importance[3, ]<br/>plot(cumprop, type = "l",main="Cumulative sum",xlab="PCA component")</pre>
<p>The cumulative sum of PCA explained variance shows how many principal components are needed to explain the proportion of variance in the input data. <span>In layman's terms, this plot shows that we can use the first 100 variables (the </span><em>principal components</em><span>) and this will account for over 80% of the variance in the original data:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-568 image-border" src="assets/de8c020f-25b5-4642-a63a-171364740a40.png" style="width:32.58em;height:18.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.1: Cumulative sum of the Principal Components <span>explained variance</span>.</div>
<p>The next code block selects out the principal components that account for 50% of our variance and use those variables to create a neural network:</p>
<pre>num_cols &lt;- min(which(cumprop&gt;0.5))<br/>cumprop[num_cols]<br/> PC23 <br/>0.50275 <br/><br/>newdat&lt;-data.frame(df.pca$x[,1:num_cols])<br/>newdat$y&lt;-digits_nn[sample,"y"]<br/>col_names &lt;- names(newdat)<br/>f &lt;- as.formula(paste("y ~", paste(col_names[!col_names %in% "y"],collapse="+")))<br/>nn &lt;- neuralnet(f,data=newdat,hidden=c(4,2),linear.output = FALSE)</pre>
<p><span>We can see that 50% of the variance in the original data can be accounted by only 23 principal components. Next, we plot the neural network by calling the <kbd>plot</kbd> function:</span></p>
<pre>plot(nn)</pre>
<p>This produces a plot similar to the following screenshot. We can see the input variables (<strong>PC1</strong> to <strong>PC23</strong>), the hidden layers and biases, and even the network weights:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-569 image-border" src="assets/bc6d5871-c471-4093-9ff1-75faca8dcfed.png" style="width:40.42em;height:38.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.2: An example of a neural network with weights and biases</div>
<p>We selected 23 principal components to use as predictors for our neural network library. We chose to use two hidden layers, the first with four nodes and the second with two nodes. The plot outputs the coefficients, which are not all decipherable from the plot, but there are functions to access them if required.</p>
<p>Next, we will create predictions on a holdout or test dataset that was not used to build either the dimensionality-reduction or the neural network model. We have to first pass the test data into the <kbd>predict</kbd> function, passing in the <kbd>df.pca</kbd> object created earlier, to get the principal components for the test data. We can then pass this data into the neural network prediction (filtering the columns to the first 23 principal components) and then show the confusion matrix and overall accuracy:</p>
<pre>test.data &lt;- predict(df.pca, newdata=digits_nn[test,colnames(digits.X2)])<br/>test.data &lt;- as.data.frame(test.data)<br/>preds &lt;- compute(nn,test.data[,1:num_cols])<br/>preds &lt;- ifelse(preds$net.result &gt; 0.5, "1", "0")<br/>t&lt;-table(digits_nn[test,"y"], preds,dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/sum(t),2)<br/>print(t)<br/> Predicted<br/>Actual 0   1<br/> 0   740  17<br/> 1    17 813<br/>print(sprintf(" accuracy = %1.2f%%",acc))<br/>[1] " accuracy = 97.86%"</pre>
<p>We achieved <kbd>97.86%</kbd> accuracy, which is not bad considering we only used 23 principal components in our neural network. It is important to note that these 23 variables are not directly comparable to any columns in the input dataset or each other. In fact, the whole point of PCA, or any dimensionality-reduction algorithm, is to produce columns that are not correlated with each other.</p>
<p>Next, we will move on to create models that perform multi-classification, that is, they can classify digits 0-9. We will convert the labels (the digits 0 to 9) to a factor so R knows that this is a classification not a regression problem. For real-world problems, you should use all the data available, but if we used all 42,000 rows, it would take a very long time to train using the neural network packages in R. We will select 5,000 rows for training and 1,000 rows for test purposes. We should select the rows at random and ensure that there is no overlap between the rows in our training and test datasets. We also separate the data into the features or predictors (<kbd>digits.x</kbd>) and the outcome (<kbd>digits.Y</kbd>). We are using all the columns except the labels as the predictors here:</p>
<pre>sample &lt;- sample(nrow(digits), 6000)<br/>train &lt;- sample[1:5000]<br/>test &lt;- sample[5001:6000]<br/><br/>digits.X &lt;- digits[train, -1]<br/>digits.y_n &lt;- digits[train, 1]<br/>digits$label &lt;- factor(digits$label, levels = 0:9)<br/>digits.y &lt;- digits[train, 1]<br/><br/>digits.test.X &lt;- digits[test, -1]<br/>digits.test.y &lt;- digits[test, 1]<br/>rm(sample,train,test)</pre>
<p class="mce-root">Finally, before we get started building our neural network, let's quickly check the distribution of the digits. This can be important as, for example, if one digit occurs very rarely, we may need to adjust our modeling approach to ensure that, it's given enough weight in the performance evaluation if we care about accurately predicting that specific digit. The following code snippet creates a bar plot showing the frequency of each digit label:</p>
<pre>barplot(table(digits.y),main="Distribution of y values (train)")</pre>
<p><span>We can see from the plot that the categories are fairly evenly distributed so there is no need to increase the weight or importance given to any particular one:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-570 image-border" src="assets/8a41bd71-dbfe-46fd-ac22-d442ffbfc1e9.png" style="width:29.00em;height:16.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.3: Distribution of <em>y</em> values for train dataset</div>
<p class="mce-root">Now let's build and train our first neural network using the <kbd>nnet</kbd> package through the <kbd>caret</kbd> package wrapper. First, we use the <kbd>set.seed()</kbd> function and specify a specific seed so that the results are reproducible. The exact seed is not important, what matters is that the same seed is used each time you run the script. The <kbd>train()</kbd> function first takes the feature or predictor data (<kbd>x</kbd>), and then the outcome variable (<kbd>y</kbd>), as arguments. The <kbd>train()</kbd> function can work with a variety of models, determined via the method argument. Although many aspects of machine learning models are learned automatically, some parameters have to be set. These vary by the method used; for example, in neural networks, one parameter is the number of hidden units. The <kbd>train()</kbd> function provides an easy way to try a variety of these tuning parameters as a named data frame to the <kbd>tuneGrid</kbd> argument. It returns the performance measures for each set of tuning parameters and returns the best trained model. We will start with just five hidden neurons in our model, and a modest decay rate. The learning rate controls how much each iteration or step can influence the current weights. The decay rate is the regularization hyper-parameter, which is used to prevent the model from overfitting. Another argument, <kbd>trcontrol</kbd>, controls additional aspects of <kbd>train()</kbd>, and is used, when a variety of tuning parameters are being evaluated, to tell the caret package how to validate and pick the best tuning parameter.</p>
<p class="mce-root">For this example, we will set the method for training control to <em>none</em> as we only have one set of tuning parameters being used here. Finally, at the end, we can specify additional, named arguments that are passed on to the actual <kbd>nnet()</kbd> function (or whatever algorithm is specified). Because of the number of predictors (<kbd>784</kbd>), we increase the maximum number of weights to 10,000 and specify a maximum of 100 iterations. Due to the relatively small amount of data, and the paucity of hidden neurons, this first model does not take too long to run:</p>
<pre>set.seed(42) <br/>tic &lt;- proc.time()<br/>digits.m1 &lt;- caret::train(digits.X, digits.y,<br/>           method = "nnet",<br/>           tuneGrid = expand.grid(<br/>             .size = c(5),<br/>             .decay = 0.1),<br/>           trControl = trainControl(method = "none"),<br/>           MaxNWts = 10000,<br/>           maxit = 100)<br/>print(proc.time() - tic)<br/>   user system elapsed <br/>  54.47 0.06 54.85</pre>
<p class="mce-root">The <kbd>predict()</kbd> function generates a set of predictions for the data. We will use the test dataset to evaluate the model; this contains records that were not used to train the model. We examine the distribution of the predicted digits in the following diagram.</p>
<pre>digits.yhat1 &lt;- predict(digits.m1,newdata=digits.test.X)<br/>accuracy &lt;- 100.0*sum(digits.yhat1==digits.test.y)/length(digits.test.y)<br/>print(sprintf(" accuracy = %1.2f%%",accuracy))<br/>[1] " accuracy = 54.80%"<br/>barplot(table(digits.yhat1),main="Distribution of y values (model 1)")</pre>
<p><span>It is clear that this is not a good model because the distribution of the predicted values is very different from the distribution of the actual values:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-571 image-border" src="assets/fad60498-d289-4519-af5e-4e4398bccfbc.png" style="width:30.92em;height:17.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.4: Distribution of <em>y</em> values from prediction model</div>
<p class="mce-root">The <kbd>barplot</kbd> is a simple check of the predictions, and shows us that our model is not very accurate. We can also calculate the accuracy by finding the percentage of rows from the predictions that match the actual value. The accuracy for this model is <kbd>54.8%</kbd>, which is not good. A more formal evaluation of the model's performance is possible using the <kbd>confusionMatrix()</kbd> function in the <kbd>caret</kbd> package. Because there is a function by the same name in the <kbd>RSNNS</kbd> package, they are masked so we call the function using <kbd>caret::<span>confusionMatrix</span></kbd> to ensure the function from the <kbd>caret</kbd> package is used. The following code shows the confusion matrix and <span>performance metrics on the test set:</span></p>
<pre>caret::confusionMatrix(xtabs(~digits.yhat1 + digits.test.y))<br/>Confusion Matrix and Statistics<br/><br/>            digits.test.y<br/>digits.yhat1    0   1   2   3   4   5   6   7   8   9<br/>           0   61   1   0   1   0   2   0   0   0   1<br/>           1    1 104   0   2   0   4   3   9  12   8<br/>           2    6   2  91  56   4  20  68   1  41   1<br/>           3    0   0   0   0   0   0   0   0   0   0<br/>           4    2   0   4   1  67   1  22   4   2  21<br/>           5   39   0   6  45   4  46   0   5  30  16<br/>           6    0   0   0   0   0   0   0   0   0   0<br/>           7    0   0   0   6   9   0   0  91   2  75<br/>           8    0   0   0   0   0   0   0   0   0   0<br/>           9    0   0   0   0   0   0   0   3   0   0<br/><br/>Overall Statistics<br/>                                          <br/>               Accuracy : 0.46 <br/>                 95% CI : (0.4288, 0.4915)<br/>    No Information Rate : 0.122 <br/>    P-Value [Acc &gt; NIR] : &lt; 2.2e-16 <br/>                                          <br/>                  Kappa : 0.4019 <br/> Mcnemar's Test P-Value : NA <br/><br/>Statistics by Class:<br/><br/>                     Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6<br/>Sensitivity            0.5596   0.9720   0.9010    0.000   0.7976   0.6301    0.000<br/>Specificity            0.9944   0.9563   0.7786    1.000   0.9378   0.8436    1.000<br/>Pos Pred Value         0.9242   0.7273   0.3138      NaN   0.5403   0.2408      NaN<br/>Neg Pred Value         0.9486   0.9965   0.9859    0.889   0.9806   0.9666    0.907<br/>Prevalence             0.1090   0.1070   0.1010    0.111   0.0840   0.0730    0.093<br/>Detection Rate         0.0610   0.1040   0.0910    0.000   0.0670   0.0460    0.000<br/>Detection Prevalence   0.0660   0.1430   0.2900    0.000   0.1240   0.1910    0.000<br/>Balanced Accuracy      0.7770   0.9641   0.8398    0.500   0.8677   0.7369    0.500<br/>                     Class: 7 Class: 8 Class: 9<br/>Sensitivity            0.8053    0.000   0.0000<br/>Specificity            0.8963    1.000   0.9966<br/>Pos Pred Value         0.4973      NaN   0.0000<br/>Neg Pred Value         0.9731    0.913   0.8776<br/>Prevalence             0.1130    0.087   0.1220<br/>Detection Rate         0.0910    0.000   0.0000<br/>Detection Prevalence   0.1830    0.000   0.0030<br/>Balanced Accuracy      0.8508    0.500   0.4983</pre>
<p class="mce-root">Because we had multiple digits, there are three main sections to the performance output. First, the actual frequency cross tab is shown. Correct predictions are on the diagonal, with various frequencies of misclassification on the off diagonals. Next are the overall statistics, which refer to the model's performance across all classes. Accuracy is simply the proportion of cases correctly classified, along with a 95% confidence interval, which can be useful, especially for smaller datasets where there may be considerable uncertainty in the estimate.</p>
<p class="mce-root"><kbd>No Information Rate</kbd> refers to what accuracy could be expected without any information by merely guessing the most frequent class, in this case, 1, which occurred 11.16% of the time. The p-value tests whether the observed accuracy (44.3%) is significantly different from <kbd>No Information Rate</kbd> (11.2% ). Although statistically significant, this is not very meaningful for digit-classification, where we would expect to do far better than simply guessing the most frequent digit! Finally, individual performance metrics for each digit are shown. These are based on calculating that digit versus every other digit, so that each is a binary comparison.</p>
<p class="mce-root">Now that we have some basic understanding of how to set up, train, and evaluate model performance, we will try increasing the number of hidden neurons, which is one key way to improve model performance, at the cost of greatly increasing the model complexity. Recall from <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, that every predictor or feature connects to each hidden neuron, and each hidden neuron connects to each outcome or output. With <kbd>784</kbd> features, each additional hidden neuron adds a substantial number of parameters, which also results in longer run times. Depending on your computer, be prepared to wait a number of minutes for these next model to finish:</p>
<pre>set.seed(42) <br/>tic &lt;- proc.time()<br/>digits.m2 &lt;- caret::train(digits.X, digits.y,<br/>           method = "nnet",<br/>           tuneGrid = expand.grid(<br/>             .size = c(10),<br/>             .decay = 0.1),<br/>           trControl = trainControl(method = "none"),<br/>            MaxNWts = 50000,<br/>            maxit = 100)<br/>print(proc.time() - tic)<br/>   user system elapsed <br/> 154.49 0.09 155.33 <br/><br/>digits.yhat2 &lt;- predict(digits.m2,newdata=digits.test.X)<br/>accuracy &lt;- 100.0*sum(digits.yhat2==digits.test.y)/length(digits.test.y)<br/>print(sprintf(" accuracy = %1.2f%%",accuracy))<br/>[1] " accuracy = 66.30%"<br/>barplot(table(digits.yhat2),main="Distribution of y values (model 2)")</pre>
<p><span>Thi</span><span>s model is better than the previous model but the distribution of the predicted values is still uneven:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-572 image-border" src="assets/5a9739c7-1c4e-4251-bea0-3786fd878cf6.png" style="width:28.42em;height:16.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.5: <span>Distribution of <em>y</em> values from prediction model</span></div>
<p class="mce-root"><span>Increasing the number of hidden neurons from 5 to 10 improved our in-sample performance from an overall accuracy of <kbd>54.8%</kbd> to</span> <kbd>66.3%</kbd><span>, but this is still quite some way from ideal (imagine character-recognition software that mixed up over 30% of all the characters!). We increase again, this time to 40 hidden neurons, and wait even longer for the model to finish training:</span></p>
<pre>set.seed(42) <br/>tic &lt;- proc.time()<br/>digits.m3 &lt;- caret::train(digits.X, digits.y,<br/>           method = "nnet",<br/>           tuneGrid = expand.grid(<br/>             .size = c(40),<br/>             .decay = 0.1),<br/>           trControl = trainControl(method = "none"),<br/>           MaxNWts = 50000,<br/>           maxit = 100)<br/>print(proc.time() - tic)<br/>   user system elapsed <br/>2450.16 0.96 2457.55<br/><br/>digits.yhat3 &lt;- predict(digits.m3,newdata=digits.test.X)<br/>accuracy &lt;- 100.0*sum(digits.yhat3==digits.test.y)/length(digits.test.y)<br/>print(sprintf(" accuracy = %1.2f%%",accuracy))<br/>[1] " accuracy = 82.20%"<br/>barplot(table(digits.yhat3),main="Distribution of y values (model 3)")</pre>
<p><span>The distribution of the predicted values is even in this model, which is what we are looking for. However the accuracy is still only at 82.2%, which is quite low:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-573 image-border" src="assets/b4d0b983-1da7-448d-831c-9e7ec71fe882.png" style="width:30.33em;height:17.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.6: <span>Distribution of <em>y</em> values from prediction model</span></div>
<p class="mce-root">Using 40 hidden neurons has improved accuracy to <kbd>82.2%</kbd> overall and it took over 40 minutes to run on an i5 computer. Model performance for some digits is still not great. If this were a real research or business problem, we might continue trying additional neurons, tuning the decay rate, or modifying features in order to try to boost model performance further, but for now we will move on.</p>
<p class="mce-root">Next, we will take a look at how to train neural networks using the RSNNS package. This package provides an interface to a variety of possible models using the <strong>Stuttgart Neural Network Simulator</strong> (<strong>SNNS</strong>) code; however, for a basic, single­ hidden-layer, feed-forward neural network, we can use the <kbd>mlp()</kbd> convenience wrapper function, which stands for multi-layer perceptron. The RSNNS package is a bit trickier to use than the convenience of nnet via the <kbd>caret</kbd> package, but one benefit is that it can be far more flexible and allows for many other types of neural network architecture to be trained, including recurrent neural networks, and also has a greater variety of training strategies.</p>
<p class="mce-root">One difference between the nnet and RSNNS packages is that, for multi-class outcomes (such as digits), RSNNS requires a dummy encoding (that is, one-hot encoding), where each possible class is represented as a column coded as 0/1. This is facilitated using the <kbd>decodeClassLabels()</kbd> function, as shown in the following code snippet:</p>
<pre>head(decodeClassLabels(digits.y))<br/>     0 1 2 3 4 5 6 7 8 9<br/>[1,] 0 0 0 0 0 0 0 0 0 1<br/>[2,] 0 0 0 0 1 0 0 0 0 0<br/>[3,] 1 0 0 0 0 0 0 0 0 0<br/>[4,] 0 0 0 0 0 1 0 0 0 0<br/>[5,] 0 0 0 0 1 0 0 0 0 0<br/>[6,] 0 0 0 1 0 0 0 0 0 0</pre>
<p class="mce-root">Since we had reasonably good success with 40 hidden neurons, we will use the same size here. Rather than standard propagation as the learning function, we will use resilient propagation, based on the work of Riedmiller, M., and Braun, H. (1993). Resilient back-propagation is an <strong>optimization</strong> to standard back-propagation that applies faster weight-update mechanisms. One of the problems that occurs as the neural network increases in complexity is that they take a long time to train. We will discuss this in depth in subsequent chapters, but for now, you just need to know that this neural network is faster because it keeps track of past derivatives and takes bigger steps if they were in the same direction during back-propagation. Note also that, because a matrix of outcomes is passed, although the predicted probability will not exceed 1 for any single digit, the sum of predicted probabilities across all digits may exceed 1 and also may be less than 1 (that is, for some cases, the model may not predict they are very likely to represent any of the digits). The <kbd>predict</kbd> function returns a matrix where each column represents a single digit, so we use the <kbd>encodeClassLabels()</kbd> function to convert back into a single vector of digit labels to plot and evaluate the model's performance:</p>
<pre>set.seed(42) <br/>tic &lt;- proc.time()<br/>digits.m4 &lt;- mlp(as.matrix(digits.X),<br/>             decodeClassLabels(digits.y),<br/>             size = 40,<br/>             learnFunc = "Rprop",<br/>             shufflePatterns = FALSE,<br/>             maxit = 80)<br/>print(proc.time() - tic)<br/>   user system elapsed <br/> 179.71 0.08 180.99 <br/><br/>digits.yhat4 &lt;- predict(digits.m4,newdata=digits.test.X)<br/>digits.yhat4 &lt;- encodeClassLabels(digits.yhat4)<br/>accuracy &lt;- 100.0*sum(I(digits.yhat4 - 1)==digits.test.y)/length(digits.test.y)<br/>print(sprintf(" accuracy = %1.2f%%",accuracy))<br/>[1] " accuracy = 81.70%"<br/>barplot(table(digits.yhat4),main="Distribution of y values (model 4)")</pre>
<p>The following bar plot shows that the predicted values are relatively evenly distributed among the categories. This matches the <span>distribution of the actual category values:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-574 image-border" src="assets/9ce6c7c5-8599-4a8c-a1ba-1890a556db51.png" style="width:28.25em;height:16.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.7: <span>Distribution of <em>y</em> values from prediction model</span></div>
<p class="mce-root">The accuracy is 81.70% and it ran in 3 minutes on my computer. This is only slightly lower than when we used nnet with 40 hidden nodes, which took 40 minutes on the same machine! This demonstrates the importance of using an optimizer, which we will see in subsequent chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating predictions from a neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">For any given observation, there can be a probability of membership in any of a number of classes (for example, an observation may have a 40% chance of being a <em>5</em>, a 20% chance of being a <em>6</em>, and so on). To evaluate the performance of the model, some choices have to be made about how to go from the probability of class membership to a discrete classification. In this section, we will explore a few of these options in more detail.</p>
<p class="mce-root">As long as there are no perfect ties, the simplest method is to classify observations based on the highest predicted probability. Another approach, which the RSNNS package calls the <strong>winner takes all</strong> (<strong>WTA</strong>) method, chooses the class with the highest probability, provided the following conditions are met:</p>
<ul>
<li class="mce-root">There are no ties for highest probabilities</li>
<li class="mce-root">The highest probability is above a user-defined threshold (the threshold could be zero)</li>
<li class="mce-root">The remaining classes all have a predicted probability under the maximum minus another user-defined threshold</li>
</ul>
<p>Otherwise, observations are classified as unknown. If both thresholds are zero (the default), this equates to saying that there must be one unique maximum. The advantage of such an approach is that it provides some quality control. In the digit-classification example we have been exploring, there are 10 possible classes.</p>
<p class="mce-root">Suppose 9 of the digits had a predicted probability of 0.099, and the remaining class had a predicted probability of 0.109. Although one class is technically more likely than the others, the difference is fairly trivial and we may conclude that the model cannot with any certainty classify that observation. A final method, called 402040, classifies if only one value is above a user-defined threshold, and all other values are below another user-defined threshold; if multiple values are above the first threshold, or any value is not below the second threshold, it treats the observation as unknown. Again, the goal here is to provide some quality control.</p>
<p class="mce-root">It may seem like this is unnecessary because uncertainty in predictions should come out in the model performance. However, it can be helpful to know if your model was highly certain in its prediction and right or wrong, or uncertain and right or wrong.</p>
<p class="mce-root">Finally, in some cases, not all classes are equally important. For example, in a medical context where a variety of biomarkers and genes are collected on patients and used to classify whether they are, at risk of cancer, or at risk of heart disease, even a 40% chance of having cancer may be enough to warrant further investigation, even if they have a 60% chance of being healthy. This has to do with the performance measures we saw earlier where, beyond overall accuracy, we can assess aspects such as sensitivity, specificity, and positive and negative predictive values. There are cases where overall accuracy is less important than making sure no one is missed.</p>
<p class="mce-root">The following code shows the raw probabilities for the in-sample data, and the impact these different choices have on the predicted values:</p>
<pre><span>digits.yhat4_b &lt;- predict(digits.m4,newdata=digits.test.X)</span><br/><span>head(round(digits.yhat4_b, 2))</span><br/><span>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span><br/><span>18986 0.00 0.00 0.00 0.98 0.00 0.02 0.00 0.00 0.00 0.00</span><br/><span>41494 0.00 0.00 0.03 0.00 0.13 0.01 0.95 0.00 0.00 0.00</span><br/><span>21738 0.00 0.00 0.02 0.03 0.00 0.46 0.01 0.00 0.74 0.00</span><br/><span>37086 0.00 0.01 0.00 0.63 0.02 0.01 0.00 0.00 0.03 0.00</span><br/><span>35532 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.99 0.00 0.00</span><br/><span>17889 0.03 0.00 0.00 0.00 0.00 0.34 0.01 0.00 0.00 0.00</span><br/><br/><span>table(encodeClassLabels(digits.yhat4_b,method = "WTA", l = 0, h = 0))</span><br/>1 2 3 4 5 6 7 8 9 10<br/>102 116 104 117 93 66 93 127 89 93<br/><br/><span>table(encodeClassLabels(digits.yhat4_b,method = "WTA", l = 0, h = .5))</span><br/>0 1 2 3 4 5 6 7 8 9 10<br/>141 95 113 86 93 67 53 89 116 73 74<br/><br/><span>table(encodeClassLabels(digits.yhat4_b,method = "WTA", l = .2, h = .5))</span><br/>0 1 2 3 4 5 6 7 8 9 10<br/>177 91 113 77 91 59 50 88 116 70 68<br/><br/><span>table(encodeClassLabels(digits.yhat4_b,method = "402040", l = .4, h = .6))</span><br/><span>  0 1 2 3 4 5 6 7 8 9 10 </span><br/><span>254 89 110 71 82 46 41 79 109 65 54 </span></pre>
<p>We now proceed to examine problems related to overfitting the data and the impact on the evaluation of the model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The problem of overfitting data – the consequences explained</h1>
                </header>
            
            <article>
                
<p class="mce-root">A common issue in machine learning is overfitting data. Generally, overfitting is used to refer to the phenomenon where the model performs better on <span>the data used to train the model </span>than it does on data not used to train the model (holdout data, future real use, and so on). Overfitting occurs when a model memorizes part of the training data and fits what is essentially noise in the training data. The accuracy in the training data is high, but because the noise changes from one dataset to the next, this accuracy does not apply to unseen data, that is, we can say that the model does not generalize very well.</p>
<p class="mce-root">Overfitting can occur at any time, but tends to become more severe as the ratio of parameters to information increases. Usually, this can be thought of as the ratio of parameters to observations, but not always. For example, suppose we have a very imbalanced dataset where the outcome we want to predict is a rare event that occurs in 1 in 5 million cases. In that case, a sample size of 15 million may only have 3 positive cases. E<span>ven though the sample size is large</span>, the information is low. To consider a simple-but-extreme case, imagine fitting a straight line to two data points. The fit will be perfect, and in those two training data, your linear-regression model will appear to have fully accounted for all variations in the data. However, if we then applied that line to another 1,000 cases, it might not fit very well at all.</p>
<p class="mce-root">In the previous sections, we generated out-of-sample predictions for the our models, that is, we evaluated accuracy on test (or holdout) data. But we never checked whether our models were overfitting, that is, the accuracy levels on the test data. We can examine how well the model generalizes by checking the accuracy on the in-sample predictions. We can see that the accuracy on the in-sample data is 84.7%, compared to 81.7% on the holdout data. There is a 3.0% loss; or, put differently, using training data to evaluate model performance resulted in an overly optimistic estimate of the accuracy, and that overestimate was 3.0%:</p>
<pre>digits.yhat4.train &lt;- predict(digits.m4)<br/>digits.yhat4.train &lt;- encodeClassLabels(digits.yhat4.train)<br/>accuracy &lt;- 100.0*sum(I(digits.yhat4.train - 1)==digits.y)/length(digits.y)<br/>print(sprintf(" accuracy = %1.2f%%",accuracy))<br/>[1] " accuracy = 84.70%"</pre>
<p class="mce-root">Since we fitted several models earlier of varying complexity, we could examine the degree of overfitting or overly optimistic accuracy from in-sample versus out­ of-sample performance measures across them. The code here should be easy enough to follow. We call the predict function for our models and do not pass in any new data; this returns the predictions for the data the model was trained with. The rest of the code is boilerplate code to create the graphic plot.</p>
<pre>digits.yhat1.train &lt;- predict(digits.m1)<br/>digits.yhat2.train &lt;- predict(digits.m2)<br/>digits.yhat3.train &lt;- predict(digits.m3)<br/>digits.yhat4.train &lt;- predict(digits.m4)<br/>digits.yhat4.train &lt;- encodeClassLabels(digits.yhat4.train)<br/><br/>measures &lt;- c("AccuracyNull", "Accuracy", "AccuracyLower", "AccuracyUpper")<br/>n5.insample &lt;- caret::confusionMatrix(xtabs(~digits.y + digits.yhat1.train))<br/>n5.outsample &lt;- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat1))<br/>n10.insample &lt;- caret::confusionMatrix(xtabs(~digits.y + digits.yhat2.train))<br/>n10.outsample &lt;- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat2))<br/>n40.insample &lt;- caret::confusionMatrix(xtabs(~digits.y + digits.yhat3.train))<br/>n40.outsample &lt;- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat3))<br/>n40b.insample &lt;- caret::confusionMatrix(xtabs(~digits.y + I(digits.yhat4.train - 1)))<br/>n40b.outsample &lt;- caret::confusionMatrix(xtabs(~ digits.test.y + I(digits.yhat4 - 1)))<br/><br/>shrinkage &lt;- rbind(<br/>  cbind(Size = 5, Sample = "In", as.data.frame(t(n5.insample$overall[measures]))),<br/>  cbind(Size = 5, Sample = "Out", as.data.frame(t(n5.outsample$overall[measures]))),<br/>  cbind(Size = 10, Sample = "In", as.data.frame(t(n10.insample$overall[measures]))),<br/>  cbind(Size = 10, Sample = "Out", as.data.frame(t(n10.outsample$overall[measures]))),<br/>  cbind(Size = 40, Sample = "In", as.data.frame(t(n40.insample$overall[measures]))),<br/>  cbind(Size = 40, Sample = "Out", as.data.frame(t(n40.outsample$overall[measures]))),<br/>  cbind(Size = 40, Sample = "In", as.data.frame(t(n40b.insample$overall[measures]))),<br/>  cbind(Size = 40, Sample = "Out", as.data.frame(t(n40b.outsample$overall[measures])))<br/>  )<br/>shrinkage$Pkg &lt;- rep(c("nnet", "RSNNS"), c(6, 2))<br/>dodge &lt;- position_dodge(width=0.4)<br/><br/>ggplot(shrinkage, aes(interaction(Size, Pkg, sep = " : "), Accuracy,<br/>                      ymin = AccuracyLower, ymax = AccuracyUpper,<br/>                      shape = Sample, linetype = Sample)) +<br/>  geom_point(size = 2.5, position = dodge) +<br/>  geom_errorbar(width = .25, position = dodge) +<br/>  xlab("") + ylab("Accuracy + 95% CI") +<br/>  theme_classic() +<br/>  theme(legend.key.size = unit(1, "cm"), legend.position = c(.8, .2))</pre>
<p><span>The code produces the following plot, which shows the accuracy metrics and the confidence intervals for those metrics. One thing we notice from this plot is that, as the models get more complex, the gap between performance on the in-sample performance measures and the out-sample performance measures increases. This highlights that more complex models tend to overfit, that is, they perform better on the in-sample data than the unseen out-sample data:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-575 image-border" src="assets/88f817b7-5f97-42d6-8606-09e4ab39f888.png" style="width:62.50em;height:36.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.8: I<span>n-sample </span><span>and out-sample performance measures for on neural network models</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case – building and applying a neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">To close the chapter, we will discuss a more realistic use case for neural networks. We will use a public dataset by Anguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-Ortiz, J. L. (2013) that uses smartphones to track physical activity. The data can be downloaded at <a href="https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones">https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones</a>. The smartphones had an accelerometer and gyroscope from which 561 features from both time and frequency were used.</p>
<p class="mce-root">The smartphones were worn during walking, walking upstairs, walking downstairs, standing, sitting, and lying down. Although this data came from phones, similar measures could be derived from other devices designed to track activity, such as various fitness-tracking watches or bands. So this data can be useful if we want to sell devices and have them automatically track how many of these different activities the wearer engages in.</p>
<p class="mce-root">This data has already been normalized to range from -1 to + 1; usually we might want to perform some normalization if it has not already been applied. Download the data from the link and unzip it into the data <span>folder </span>that is on the same level as the chapter folder; we will use it in later chapters as well. We can import the training and testing data, as well as the labels. We will then take a quick look at the distribution of the outcome variable in the following code:</p>
<pre>use.train.x &lt;- read.table("../data/UCI HAR Dataset/train/X_train.txt")<br/>use.train.y &lt;- read.table("../data/UCI HAR Dataset/train/y_train.txt")[[1]]<br/><br/>use.test.x &lt;- read.table("../data/UCI HAR Dataset/test/X_test.txt")<br/>use.test.y &lt;- read.table("../data/UCI HAR Dataset/test/y_test.txt")[[1]]<br/><br/>use.labels &lt;- read.table("../data/UCI HAR Dataset/activity_labels.txt")<br/><br/>barplot(table(use.train.y),main="Distribution of y values (UCI HAR Dataset)")</pre>
<p>This produces the following bar plot, which shows that the categories are relatively evenly balanced:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-576 image-border" src="assets/b105e064-6747-4bd6-97b7-2aaf6d21844d.png" style="width:27.58em;height:17.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.9: <span>Distribution of <em>y</em> values for UCI HAR dataset</span></div>
<p class="mce-root">We are going to evaluate a variety of tuning parameters to show how we might experiment with different approaches to try to get the best possible model. <span>We will use</span><span> different hyper-parameters and evaluate which model performs the best. </span></p>
<div class="mce-root packt_infobox">Because the models can take some time to train and R normally only uses a single core, we will use some special packages to enable us to run multiple models in <span>parallel.</span> These packages are <kbd>parallel</kbd>, <kbd>foreach</kbd>, and <kbd>doSNOW</kbd>, which should have been loaded if you ran the script from the first line.</div>
<p class="mce-root">Now we can pick our tuning parameters and set up a local cluster as the backend for the <kbd>foreach</kbd> R package for parallel for loops. Note that if you do this on a machine with fewer than five cores, you should change <kbd>makeCluster(5)</kbd> to a lower number:</p>
<pre>## choose tuning parameters<br/>tuning &lt;- list(<br/>  size = c(40, 20, 20, 50, 50),<br/>  maxit = c(60, 100, 100, 100, 100),<br/>  shuffle = c(FALSE, FALSE, TRUE, FALSE, FALSE),<br/>  params = list(FALSE, FALSE, FALSE, FALSE, c(0.1, 20, 3)))<br/><br/>## setup cluster using 5 cores<br/>## load packages, export required data and variables<br/>## and register as a backend for use with the foreach package<br/>cl &lt;- makeCluster(5)<br/>clusterEvalQ(cl, {source("cluster_inc.R")})<br/>clusterExport(cl,<br/>  c("tuning", "use.train.x", "use.train.y",<br/>    "use.test.x", "use.test.y")<br/>  )<br/>registerDoSNOW(cl)</pre>
<p class="mce-root">Now we are ready to train all the models. The following code shows a parallel for loop, using code that is similar to what we have already seen, but this time setting some of the arguments based on the tuning parameters we previously stored in the list:</p>
<pre>## train models in parallel<br/>use.models &lt;- foreach(i = 1:5, .combine = 'c') %dopar% {<br/>  if (tuning$params[[i]][1]) {<br/>    set.seed(42) <br/>    list(Model = mlp(<br/>      as.matrix(use.train.x),<br/>      decodeClassLabels(use.train.y),<br/>      size = tuning$size[[i]],<br/>      learnFunc = "Rprop",<br/>      shufflePatterns = tuning$shuffle[[i]],<br/>      learnFuncParams = tuning$params[[i]],<br/>      maxit = tuning$maxit[[i]]<br/>      ))<br/>  } else {<br/>    set.seed(42) <br/>    list(Model = mlp(<br/>      as.matrix(use.train.x),<br/>      decodeClassLabels(use.train.y),<br/>      size = tuning$size[[i]],<br/>      learnFunc = "Rprop",<br/>      shufflePatterns = tuning$shuffle[[i]],<br/>      maxit = tuning$maxit[[i]]<br/>    ))<br/>  }<br/>}</pre>
<p class="mce-root">Because generating out-of-sample predictions can also take some time, we will do that in parallel as well. However, first we need to export the model results to each of the workers on our cluster, and then we can calculate the predictions:</p>
<pre>## export models and calculate both in sample,<br/>## 'fitted' and out of sample 'predicted' values<br/>clusterExport(cl, "use.models")<br/>use.yhat &lt;- foreach(i = 1:5, .combine = 'c') %dopar% {<br/>  list(list(<br/>    Insample = encodeClassLabels(fitted.values(use.models[[i]])),<br/>    Outsample = encodeClassLabels(predict(use.models[[i]],<br/>                                          newdata = as.matrix(use.test.x)))<br/>    ))<br/>}</pre>
<p class="mce-root">Finally, we can merge the actual and fitted or predicted values together into a dataset, calculate performance measures on each one, and store the overall results together for examination and comparison. We can use almost identical code to the code that follows to generate out-of-sample performance measures. That code is not shown in the book, but is available in the code bundle provided with the book. Some additional data-management is required here as sometimes a model may not predict each possible response level, but this can make for non-symmetrical frequency cross tabs, unless we convert the variable to a factor and specify the levels. We also drop <kbd>o</kbd> values, which indicate the model was uncertain about how to classify an observation:</p>
<pre>use.insample &lt;- cbind(Y = use.train.y,<br/>  do.call(cbind.data.frame, lapply(use.yhat, `[[`, "Insample")))<br/>colnames(use.insample) &lt;- c("Y", paste0("Yhat", 1:5))<br/><br/>performance.insample &lt;- do.call(rbind, lapply(1:5, function(i) {<br/>  f &lt;- substitute(~ Y + x, list(x = as.name(paste0("Yhat", i))))<br/>  use.dat &lt;- use.insample[use.insample[,paste0("Yhat", i)] != 0, ]<br/>  use.dat$Y &lt;- factor(use.dat$Y, levels = 1:6)<br/>  use.dat[, paste0("Yhat", i)] &lt;- factor(use.dat[, paste0("Yhat", i)], levels = 1:6)<br/>  res &lt;- caret::confusionMatrix(xtabs(f, data = use.dat))<br/><br/>  cbind(Size = tuning$size[[i]],<br/>        Maxit = tuning$maxit[[i]],<br/>        Shuffle = tuning$shuffle[[i]],<br/>        as.data.frame(t(res$overall[c("AccuracyNull", "Accuracy", "AccuracyLower", "AccuracyUpper")])))<br/>}))<br/><br/>use.outsample &lt;- cbind(Y = use.test.y,<br/>  do.call(cbind.data.frame, lapply(use.yhat, `[[`, "Outsample")))<br/>colnames(use.outsample) &lt;- c("Y", paste0("Yhat", 1:5))<br/>performance.outsample &lt;- do.call(rbind, lapply(1:5, function(i) {<br/>  f &lt;- substitute(~ Y + x, list(x = as.name(paste0("Yhat", i))))<br/>  use.dat &lt;- use.outsample[use.outsample[,paste0("Yhat", i)] != 0, ]<br/>  use.dat$Y &lt;- factor(use.dat$Y, levels = 1:6)<br/>  use.dat[, paste0("Yhat", i)] &lt;- factor(use.dat[, paste0("Yhat", i)], levels = 1:6)<br/>  res &lt;- caret::confusionMatrix(xtabs(f, data = use.dat))<br/><br/>  cbind(Size = tuning$size[[i]],<br/>        Maxit = tuning$maxit[[i]],<br/>        Shuffle = tuning$shuffle[[i]],<br/>        as.data.frame(t(res$overall[c("AccuracyNull", "Accuracy", "AccuracyLower", "AccuracyUpper")])))<br/>}))</pre>
<p class="mce-root">If we print the in-sample and out-of-sample performance, we can see how each of our models did and the effect of varying some of the tuning parameters. The output is shown in the following code. The fourth column (null accuracy) is dropped as it is not as important for this comparison:</p>
<pre>options(width = 80, digits = 3)<br/>performance.insample[,-4]<br/>  Size Maxit Shuffle Accuracy AccuracyLower AccuracyUpper<br/>1   40    60   FALSE    0.984         0.981         0.987<br/>2   20   100   FALSE    0.982         0.978         0.985<br/>3   20   100    TRUE    0.982         0.978         0.985<br/>4   50   100   FALSE    0.981         0.978         0.984<br/>5   50   100   FALSE    1.000         0.999         1.000<br/><br/>performance.outsample[,-4]<br/>  Size Maxit Shuffle Accuracy AccuracyLower AccuracyUpper<br/>1   40    60   FALSE    0.916         0.906         0.926<br/>2   20   100   FALSE    0.913         0.902         0.923<br/>3   20   100   TRUE     0.913         0.902         0.923<br/>4   50   100   FALSE    0.910         0.900         0.920<br/>5   50   100   FALSE    0.938         0.928         0.946</pre>
<p class="mce-root">As a reminder, the in-sample results evaluate the predictions on the training data and the out-<span>sample results evaluate the predictions on the holdout (or test) data. The best set of hyper-parameters is the last set, where we get an accuracy of 93.8% on unseen data. </span>This shows that we are able to classify the types of activity people are engaged in quite accurately based on the data from their smartphones. We can also see that <span>the more complex models perform better on </span>the in-sample data, which is not always the case with out-of-sample performance measures.</p>
<div class="mce-root packt_infobox">For each model, we have large differences between the accuracy for the <span>in-sample data against the out-of-sample data; the models clearly overfit</span>. We will get into ways to combat this overfitting in <a href="6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml">Chapter 3</a>,<em> Deep Learning Fundamentals</em>, as we train deep neural networks with multiple hidden layers.</div>
<p class="mce-root">Despite the slightly worse out-of-sample performance, the models still do well <span>– </span>far better than chance alone <span>–</span> and, for our example use case, we could pick the best model and be quite confident that using this will provide a good classification of a user's activities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">T<span>his chapter showed how to get started building and training neural networks to classify data, including image recognition and physical activity data. We looked at packages that can visualize a neural network and we created a number of models to perform classification on data with 10 different categories. Although we only used some neural network packages rather than deep learning packages, our models took a long time to train and we had issues with overfitting.</span></p>
<p>Some of the basic neural network models in this chapter took a long time to train, even though we did not use all the data available. For the MNIST data, we used approx. 8,000 rows for our binary classification task and only 6,000 rows for our multi-classification task. Even so, one model took almost an hour to train. Our deep learning models will be much more complicated and should be able to process millions of records. You can now see why specialist hardware is required for training deep learning models.</p>
<p class="mce-root"><span>Secondly, we see that a potential pitfall in machine learning is that more complex models will be more likely to overfit the training data, so that evaluating performance in the same data used to train the model results in biased, overly optimistic estimates of the model performance. Indeed, this can even make a difference as to which model is chosen as the best. Overfitting is also an issue for deep neural networks. In the next chapter, we will discuss various techniques used to prevent overfitting and obtain more accurate estimates of model performance. </span></p>
<p class="mce-root"><span>In the next chapter we will look at building a neural network from scratch and see how it applies to deep learning. We will also discuss some methods to deal with overfitting.</span></p>


            </article>

            
        </section>
    </body></html>