["```py\nGAMMA = 0.99 \nLEARNING_RATE = 0.01 \nEPISODES_TO_TRAIN = 4\n```", "```py\nclass PGN(nn.Module): \n    def __init__(self, input_size: int, n_actions: int): \n        super(PGN, self).__init__() \n\n        self.net = nn.Sequential( \n            nn.Linear(input_size, 128), \n            nn.ReLU(), \n            nn.Linear(128, n_actions) \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        return self.net(x)\n```", "```py\ndef calc_qvals(rewards: tt.List[float]) -> tt.List[float]: \n    res = [] \n    sum_r = 0.0 \n    for r in reversed(rewards): \n        sum_r *= GAMMA \n        sum_r += r \n        res.append(sum_r) \n    return list(reversed(res))\n```", "```py\nif __name__ == \"__main__\": \n    env = gym.make(\"CartPole-v1\") \n    writer = SummaryWriter(comment=\"-cartpole-reinforce\") \n\n    net = PGN(env.observation_space.shape[0], env.action_space.n) \n    print(net) \n\n    agent = ptan.agent.PolicyAgent( \n        net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True) \n    exp_source = ExperienceSourceFirstLast(env, agent, gamma=GAMMA) \n\n    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n```", "```py\n total_rewards = [] \n    done_episodes = 0 \n\n    batch_episodes = 0 \n    batch_states, batch_actions, batch_qvals = [], [], [] \n    cur_rewards = []\n```", "```py\n for step_idx, exp in enumerate(exp_source): \n        batch_states.append(exp.state) \n        batch_actions.append(int(exp.action)) \n        cur_rewards.append(exp.reward) \n\n        if exp.last_state is None: \n            batch_qvals.extend(calc_qvals(cur_rewards)) \n            cur_rewards.clear() \n            batch_episodes += 1\n```", "```py\n new_rewards = exp_source.pop_total_rewards() \n        if new_rewards: \n            done_episodes += 1 \n            reward = new_rewards[0] \n            total_rewards.append(reward) \n            mean_rewards = float(np.mean(total_rewards[-100:])) \n            print(f\"{step_idx}: reward: {reward:6.2f}, mean_100: {mean_rewards:6.2f}, \" \n                  f\"episodes: {done_episodes}\") \n            writer.add_scalar(\"reward\", reward, step_idx) \n            writer.add_scalar(\"reward_100\", mean_rewards, step_idx) \n            writer.add_scalar(\"episodes\", done_episodes, step_idx) \n            if mean_rewards > 450: \n                print(f\"Solved in {step_idx} steps and {done_episodes} episodes!\") \n                break\n```", "```py\n if batch_episodes < EPISODES_TO_TRAIN: \n            continue \n\n        optimizer.zero_grad() \n        states_t = torch.as_tensor(np.asarray(batch_states)) \n        batch_actions_t = torch.as_tensor(np.asarray(batch_actions)) \n        batch_qvals_t = torch.as_tensor(np.asarray(batch_qvals))\n```", "```py\n logits_t = net(states_t) \n        log_prob_t = F.log_softmax(logits_t, dim=1) \n        batch_idx = range(len(batch_states)) \n        act_probs_t = log_prob_t[batch_idx, batch_actions_t] \n        log_prob_actions_v = batch_qvals_t * act_probs_t \n        loss_t = -log_prob_actions_v.mean()\n```", "```py\n loss_t.backward() \n        optimizer.step() \n\n        batch_episodes = 0 \n        batch_states.clear() \n        batch_actions.clear() \n        batch_qvals.clear() \n\n    writer.close()\n```", "```py\nChapter11$ ./02_cartpole_reinforce.py \nPGN( \n  (net): Sequential( \n   (0): Linear(in_features=4, out_features=128, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=128, out_features=2, bias=True) \n  ) \n) \n31: reward:  31.00, mean_100:  31.00, episodes: 1 \n42: reward:  11.00, mean_100:  21.00, episodes: 2 \n54: reward:  12.00, mean_100:  18.00, episodes: 3 \n94: reward:  40.00, mean_100:  23.50, episodes: 4 \n159: reward:  65.00, mean_100:  31.80, episodes: 5 \n... \n65857: reward: 500.00, mean_100: 440.60, episodes: 380 \n66357: reward: 500.00, mean_100: 442.42, episodes: 381 \n66857: reward: 500.00, mean_100: 445.59, episodes: 382 \n67357: reward: 500.00, mean_100: 448.24, episodes: 383 \n67857: reward: 500.00, mean_100: 451.31, episodes: 384 \nSolved in 67857 steps and 384 episodes!\n```", "```py\nGAMMA = 0.99 \nLEARNING_RATE = 0.001 \nENTROPY_BETA = 0.01 \nBATCH_SIZE = 8 \n\nREWARD_STEPS = 10\n```", "```py\nclass PGN(nn.Module): \n    def __init__(self, input_size: int, n_actions: int): \n        super(PGN, self).__init__() \n\n        self.net = nn.Sequential( \n            nn.Linear(input_size, 128), \n            nn.ReLU(), \n            nn.Linear(128, n_actions) \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        return self.net(x)\n```", "```py\n exp_source = ptan.experience.ExperienceSourceFirstLast( \n        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n```", "```py\n for step_idx, exp in enumerate(exp_source): \n        reward_sum += exp.reward \n        baseline = reward_sum / (step_idx + 1) \n        writer.add_scalar(\"baseline\", baseline, step_idx) \n        batch_states.append(exp.state) \n        batch_actions.append(int(exp.action)) \n        batch_scales.append(exp.reward - baseline)\n```", "```py\n optimizer.zero_grad() \n        logits_t = net(states_t) \n        log_prob_t = F.log_softmax(logits_t, dim=1) \n        act_probs_t = log_prob_t[range(BATCH_SIZE), batch_actions_t] \n        log_prob_actions_t = batch_scale_t * act_probs_t \n        loss_policy_t = -log_prob_actions_t.mean()\n```", "```py\n prob_t = F.softmax(logits_t, dim=1) \n        entropy_t = -(prob_t * log_prob_t).sum(dim=1).mean() \n        entropy_loss_t = -ENTROPY_BETA * entropy_t \n        loss_t = loss_policy_t + entropy_loss_t \n\n        loss_t.backward() \n        optimizer.step()\n```", "```py\n new_logits_t = net(states_t) \n        new_prob_t = F.softmax(new_logits_t, dim=1) \n        kl_div_t = -((new_prob_t / prob_t).log() * prob_t).\\ \n            sum(dim=1).mean() \n        writer.add_scalar(\"kl\", kl_div_t.item(), step_idx)\n```", "```py\n grad_max = 0.0 \n        grad_means = 0.0 \n        grad_count = 0 \n        for p in net.parameters(): \n            grad_max = max(grad_max, p.grad.abs().max().item()) \n            grad_means += (p.grad ** 2).mean().sqrt().item() \n            grad_count += 1\n```", "```py\n writer.add_scalar(\"baseline\", baseline, step_idx) \n        writer.add_scalar(\"entropy\", entropy, step_idx) \n        writer.add_scalar(\"loss_entropy\", l_entropy, step_idx) \n        writer.add_scalar(\"loss_policy\", l_policy, step_idx) \n        writer.add_scalar(\"loss_total\", l_total, step_idx) \n        writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx) \n        writer.add_scalar(\"grad_max\", grad_max, step_idx) \n        writer.add_scalar(\"batch_scales\", bs_smoothed, step_idx) \n\n        batch_states.clear() \n        batch_actions.clear() \n        batch_scales.clear()\n```", "```py\n    class MeanBuffer: \n        def __init__(self, capacity: int): \n            self.capacity = capacity \n            self.deque = collections.deque(maxlen=capacity) \n            self.sum = 0.0 \n\n        def add(self, val: float): \n            if len(self.deque) == self.capacity: \n                self.sum -= self.deque[0] \n            self.deque.append(val) \n            self.sum += val \n\n        def mean(self) -> float: \n            if not self.deque: \n                return 0.0 \n            return self.sum / len(self.deque)\n    ```", "```py\nGAMMA = 0.99 \nLEARNING_RATE = 0.0001 \nENTROPY_BETA = 0.01 \nBATCH_SIZE = 128 \n\nREWARD_STEPS = 10 \nBASELINE_STEPS = 1000000 \nGRAD_L2_CLIP = 0.1 \n\nENV_COUNT = 32\n```"]