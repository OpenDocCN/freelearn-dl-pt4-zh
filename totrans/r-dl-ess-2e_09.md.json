["```py\nlibrary(keras)\nlibrary(corrplot)\nlibrary(neuralnet)\noptions(width = 70, digits = 2)\noptions(scipen=999)\ndataDirectory <- \"../data\"\nif (!file.exists(paste(dataDirectory,'/train.csv',sep=\"\")))\n{\n link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'\n if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep=\"\")))\n download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=\"\"))\n unzip(paste(dataDirectory,'/mnist_csv.zip',sep=\"\"), exdir = dataDirectory)\n if (file.exists(paste(dataDirectory,'/test.csv',sep=\"\")))\n file.remove(paste(dataDirectory,'/test.csv',sep=\"\"))\n}\n\ndata <- read.csv(\"../data/train.csv\", header=TRUE)\nset.seed(42)\nsample<-sample(nrow(data),0.5*nrow(data))\ntest <- setdiff(seq_len(nrow(data)),sample)\ntrain.x <- data[sample,-1]\ntest.x <- data[test,-1]\ntrain.y <- data[sample,1]\ntest.y <- data[test,1]\nrm(data)\ntrain.x <- train.x/255\ntest.x <- test.x/255\ntrain.x <- data.matrix(train.x)\ntest.x <- data.matrix(test.x)\ninput_dim <- 28*28 #784\n```", "```py\n# model 1\ninner_layer_dim <- 16\ninput_layer <- layer_input(shape=c(input_dim))\nencoder <- layer_dense(units=inner_layer_dim, activation='tanh')(input_layer)\ndecoder <- layer_dense(units=784)(encoder)\nautoencoder <- keras_model(inputs=input_layer, outputs = decoder)\nautoencoder %>% compile(optimizer='adam', loss='mean_squared_error',metrics='accuracy')\nhistory <- autoencoder %>% fit(train.x,train.x,\n epochs=40, batch_size=128,validation_split=0.2)\n\nTrain on 16800 samples, validate on 4200 samples\nEpoch 1/40\n16800/16800 [==============================] - 1s 36us/step - loss: 0.0683 - acc: 0.0065 - val_loss: 0.0536 - val_acc: 0.0052\nEpoch 2/40\n16800/16800 [==============================] - 1s 30us/step - loss: 0.0457 - acc: 0.0082 - val_loss: 0.0400 - val_acc: 0.0081\nEpoch 3/40\n16800/16800 [==============================] - 0s 29us/step - loss: 0.0367 - acc: 0.0101 - val_loss: 0.0344 - val_acc: 0.0121\n...\n...\nEpoch 38/40\n16800/16800 [==============================] - 0s 29us/step - loss: 0.0274 - acc: 0.0107 - val_loss: 0.0275 - val_acc: 0.0098\nEpoch 39/40\n```", "```py\n16800/16800 [==============================] - 1s 31us/step - loss: 0.0274 - acc: 0.0111 - val_loss: 0.0275 - val_acc: 0.0093\nEpoch 40/40\n16800/16800 [==============================] - 1s 32us/step - loss: 0.0274 - acc: 0.0120 - val_loss: 0.0275 - val_acc: 0.0095\n```", "```py\nsummary(autoencoder)\n______________________________________________________________________\nLayer (type)               Output Shape                 Param # \n======================================================================\ninput_1 (InputLayer)       (None, 784)                  0 \n______________________________________________________________________\ndense_1 (Dense)            (None, 16)                   12560 \n______________________________________________________________________\ndense_2 (Dense)            (None, 784)                  13328 \n======================================================================\nTotal params: 25,888\nTrainable params: 25,888\nNon-trainable params: 0\n______________________________________________________________________\n\nplot(history)\n```", "```py\n# model 2\ninner_layer_dim <- 32\ninput_layer <- layer_input(shape=c(input_dim))\nencoder <- layer_dense(units=inner_layer_dim, activation='tanh')(input_layer)\ndecoder <- layer_dense(units=784)(encoder)\nautoencoder <- keras_model(inputs=input_layer, outputs = decoder)\nautoencoder %>% compile(optimizer='adam',\n loss='mean_squared_error',metrics='accuracy')\nhistory <- autoencoder %>% fit(train.x,train.x,\n epochs=40, batch_size=128,validation_split=0.2)\n\nTrain on 16800 samples, validate on 4200 samples\nEpoch 1/40\n16800/16800 [==============================] - 1s 41us/step - loss: 0.0591 - acc: 0.0104 - val_loss: 0.0406 - val_acc: 0.0131\nEpoch 2/40\n16800/16800 [==============================] - 1s 34us/step - loss: 0.0339 - acc: 0.0111 - val_loss: 0.0291 - val_acc: 0.0093\nEpoch 3/40\n16800/16800 [==============================] - 1s 33us/step - loss: 0.0262 - acc: 0.0108 - val_loss: 0.0239 - val_acc: 0.0100\n...\n...\nEpoch 38/40\n16800/16800 [==============================] - 1s 33us/step - loss: 0.0174 - acc: 0.0130 - val_loss: 0.0175 - val_acc: 0.0095\nEpoch 39/40\n16800/16800 [==============================] - 1s 31us/step - loss: 0.0174 - acc: 0.0132 - val_loss: 0.0175 - val_acc: 0.0098\nEpoch 40/40\n16800/16800 [==============================] - 1s 34us/step - loss: 0.0174 - acc: 0.0126 - val_loss: 0.0175 - val_acc: 0.0100\n```", "```py\n# model 3\ninner_layer_dim <- 64\ninput_layer <- layer_input(shape=c(input_dim))\nencoder <- layer_dense(units=inner_layer_dim, activation='tanh')(input_layer)\ndecoder <- layer_dense(units=784)(encoder)\nautoencoder <- keras_model(inputs=input_layer, outputs = decoder)\nautoencoder %>% compile(optimizer='adam',\n loss='mean_squared_error',metrics='accuracy')\nhistory <- autoencoder %>% fit(train.x,train.x,\n epochs=40, batch_size=128,validation_split=0.2)\n\nTrain on 16800 samples, validate on 4200 samples\nEpoch 1/40\n16800/16800 [==============================] - 1s 50us/step - loss: 0.0505 - acc: 0.0085 - val_loss: 0.0300 - val_acc: 0.0138\nEpoch 2/40\n16800/16800 [==============================] - 1s 39us/step - loss: 0.0239 - acc: 0.0110 - val_loss: 0.0197 - val_acc: 0.0090\nEpoch 3/40\n16800/16800 [==============================] - 1s 41us/step - loss: 0.0173 - acc: 0.0115 - val_loss: 0.0156 - val_acc: 0.0117\n...\n...\nEpoch 38/40\n16800/16800 [==============================] - 1s 41us/step - loss: 0.0094 - acc: 0.0124 - val_loss: 0.0096 - val_acc: 0.0131\nEpoch 39/40\n16800/16800 [==============================] - 1s 39us/step - loss: 0.0095 - acc: 0.0128 - val_loss: 0.0095 - val_acc: 0.0121\nEpoch 40/40\n16800/16800 [==============================] - 1s 37us/step - loss: 0.0094 - acc: 0.0126 - val_loss: 0.0098 - val_acc: 0.0133\n```", "```py\nencoder <- keras_model(inputs=input_layer, outputs=encoder)\nencodings <- encoder %>% predict(test.x)\nencodings<-as.data.frame(encodings)\nM <- cor(encodings)\ncorrplot(M, method = \"circle\", sig.level = 0.1)\n```", "```py\nencodings$y <- test.y\nencodings <- encodings[encodings$y==5 | encodings$y==6,]\nencodings[encodings$y==5,]$y <- 0\nencodings[encodings$y==6,]$y <- 1\ntable(encodings$y)\n   0    1 \n1852 2075 \nnobs <- nrow(encodings)\ntrain <- sample(nobs, 0.9*nobs)\ntest <- setdiff(seq_len(nobs), train)\ntrainData <- encodings[train,]\ntestData <- encodings[test,]\ncol_names <- names(trainData)\nf <- as.formula(paste(\"y ~\", paste(col_names[!col_names %in%\"y\"],collapse=\"+\")))\nnn <- neuralnet(f,data=trainData,hidden=c(4,2),linear.output = FALSE)\npreds_nn <- compute(nn,testData[,1:(-1+ncol(testData))])\npreds_nn <- ifelse(preds_nn$net.result > 0.5, \"1\", \"0\")\nt<-table(testData$y, preds_nn,dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/sum(t),2)\nprint(t)\n      Predicted\nActual 0 1\n     0 182 5\n     1 3 203\nprint(sprintf(\" accuracy = %1.2f%%\",acc))\n[1] \" accuracy = 97.96%\"\n```", "```py\nlibrary(keras)\nlibrary(ggplot2)\ntrain.x <- read.table(\"UCI HAR Dataset/train/X_train.txt\")\ntrain.y <- read.table(\"UCI HAR Dataset/train/y_train.txt\")[[1]]\ntest.x <- read.table(\"UCI HAR Dataset/test/X_test.txt\")\ntest.y <- read.table(\"UCI HAR Dataset/test/y_test.txt\")[[1]]\n\nuse.labels <- read.table(\"UCI HAR Dataset/activity_labels.txt\")\ncolnames(use.labels) <-c(\"y\",\"label\")\n\nfeatures <- read.table(\"UCI HAR Dataset/features.txt\")\nmeanSD <- grep(\"mean\\\\(\\\\)|std\\\\(\\\\)|max\\\\(\\\\)|min\\\\(\\\\)|skewness\\\\(\\\\)\", features[, 2])\n\ntrain.x <- data.matrix(train.x[,meanSD])\ntest.x <- data.matrix(test.x[,meanSD])\ninput_dim <- ncol(train.x)\n```", "```py\n# model\ninner_layer_dim <- 40\ninput_layer <- layer_input(shape=c(input_dim))\nencoder <- layer_dense(units=inner_layer_dim, activation='tanh')(input_layer)\nencoder <- layer_dense(units=inner_layer_dim, activation='tanh')(encoder)\ndecoder <- layer_dense(units=inner_layer_dim)(encoder)\ndecoder <- layer_dense(units=inner_layer_dim)(decoder)\ndecoder <- layer_dense(units=input_dim)(decoder)\n\nautoencoder <- keras_model(inputs=input_layer, outputs = decoder)\nautoencoder %>% compile(optimizer='adam',\n loss='mean_squared_error',metrics='accuracy')\nhistory <- autoencoder %>% fit(train.x,train.x,\n epochs=30, batch_size=128,validation_split=0.2)\nTrain on 5881 samples, validate on 1471 samples\nEpoch 1/30\n5881/5881 [==============================] - 1s 95us/step - loss: 0.2342 - acc: 0.1047 - val_loss: 0.0500 - val_acc: 0.1013\nEpoch 2/30\n5881/5881 [==============================] - 0s 53us/step - loss: 0.0447 - acc: 0.2151 - val_loss: 0.0324 - val_acc: 0.2536\nEpoch 3/30\n5881/5881 [==============================] - 0s 44us/step - loss: 0.0324 - acc: 0.2772 - val_loss: 0.0261 - val_acc: 0.3413\n...\n...\n\nEpoch 27/30\n5881/5881 [==============================] - 0s 45us/step - loss: 0.0098 - acc: 0.2935 - val_loss: 0.0094 - val_acc: 0.3379\nEpoch 28/30\n5881/5881 [==============================] - 0s 44us/step - loss: 0.0096 - acc: 0.2908 - val_loss: 0.0092 - val_acc: 0.3215\nEpoch 29/30\n5881/5881 [==============================] - 0s 44us/step - loss: 0.0094 - acc: 0.2984 - val_loss: 0.0090 - val_acc: 0.3209\nEpoch 30/30\n5881/5881 [==============================] - 0s 44us/step - loss: 0.0092 - acc: 0.2955 - val_loss: 0.0088 - val_acc: 0.3209\n\n```", "```py\nsummary(autoencoder)\n_______________________________________________________________________\nLayer (type)                 Output Shape                           Param # \n=======================================================================\ninput_4 (InputLayer)         (None, 145)                            0 \n_______________________________________________________________________\ndense_16 (Dense)             (None, 40)                             5840 \n_______________________________________________________________________\ndense_17 (Dense)             (None, 40)                             1640 \n_______________________________________________________________________\ndense_18 (Dense)             (None, 40)                             1640 \n_______________________________________________________________________\ndense_19 (Dense)             (None, 40)                             1640 \n_______________________________________________________________________\ndense_20 (Dense)             (None, 145)                            5945 \n=======================================================================\nTotal params: 16,705\nTrainable params: 16,705\nNon-trainable params: 0\n_______________________________________________________________________\n```", "```py\n# anomaly detection\npreds <- autoencoder %>% predict(test.x)\npreds <- as.data.frame(preds)\nlimit <- 4\npreds$se_test <- apply((test.x - preds)^2, 1, sum)\npreds$y_preds <- ifelse(preds$se_test>limit,1,0)\npreds$y <- test.y\npreds <- merge(preds,use.labels)\ntable(preds$label)\nLAYING SITTING STANDING WALKING WALKING_DOWNSTAIRS WALKING_UPSTAIRS \n   537     491      532     496                420              471 \n\ntable(preds[preds$y_preds==1,]$label)\nLAYING SITTING STANDING WALKING WALKING_DOWNSTAIRS WALKING_UPSTAIRS \n    18       7        1      17                 45               11 \n```", "```py\nggplot(as.data.frame(table(preds[preds$y_preds==1,]$label)),aes(Var1, Freq)) +\n ggtitle(\"Potential anomalies by activity\") +\n geom_bar(stat = \"identity\") +\n xlab(\"\") + ylab(\"Frequency\") +\n theme_classic() +\n theme(plot.title = element_text(hjust = 0.5)) +\n theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n```", "```py\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(broom)\n\nset.seed(42)\nfile_list <- list.files(\"../dunnhumby/in/\", \"trans*\")\ntemp_file <- \"../dunnhumby/temp.csv\"\nout_file <- \"../dunnhumby/recommend.csv\"\nif (file.exists(temp_file)) file.remove(temp_file)\nif (file.exists(out_file)) file.remove(out_file)\noptions(readr.show_progress=FALSE)\n\ni <- 1\nfor (file_name in file_list)\n{\n  file_name<-paste(\"../dunnhumby/in/\",file_name,sep=\"\")\n  df<-suppressMessages(read_csv(file_name))\n\n  df2 <- df %>%\n    filter(CUST_CODE!=\"\") %>%\n    group_by(CUST_CODE,PROD_CODE_40) %>%\n    summarise(sales=sum(SPEND))\n\n  colnames(df2)<-c(\"cust_id\",\"prod_id\",\"sales\")\n  if (i ==1)\n    write_csv(df2,temp_file)\n  else\n    write_csv(df2,temp_file,append=TRUE)\n  print (paste(\"File\",i,\"/\",length(file_list),\"processed\"))\n  i <- i+1\n}\n[1] \"File 1 / 117 processed\"\n[1] \"File 2 / 117 processed\"\n[1] \"File 3 / 117 processed\"\n...\n...\n...\n[1] \"File 115 / 117 processed\"\n[1] \"File 116 / 117 processed\"\n[1] \"File 117 / 117 processed\"\nrm(df,df2)\n```", "```py\ndf_processed<-read_csv(temp_file)\nif (file.exists(temp_file)) file.remove(temp_file)\n\ndf2 <- df_processed %>%\n group_by(cust_id,prod_id) %>%\n summarise(sales=sum(sales))\n```", "```py\n# create quantiles\ndfProds <- df2 %>%\n group_by(prod_id) %>%\n do( tidy(t(quantile(.$sales, probs = seq(0, 1, 0.2)))) )\ncolnames(dfProds)<-c(\"prod_id\",\"X0\",\"X20\",\"X40\",\"X60\",\"X80\",\"X100\")\ndf2<-merge(df2,dfProds)\ndf2$rating<-0\ndf2[df2$sales<=df2$X20,\"rating\"] <- 1\ndf2[(df2$sales>df2$X20) & (df2$sales<=df2$X40),\"rating\"] <- 2\ndf2[(df2$sales>df2$X40) & (df2$sales<=df2$X60),\"rating\"] <- 3\ndf2[(df2$sales>df2$X60) & (df2$sales<=df2$X80),\"rating\"] <- 4\ndf2[(df2$sales>df2$X80) & (df2$sales<=df2$X100),\"rating\"] <- 5\n```", "```py\n# sanity check, are our ratings spread out relatively evenly\ndf2 %>%\n  group_by(rating) %>%\n  summarise(recs=n())\n  rating  recs\n1      1 68246\n2      2 62592\n3      3 62162\n4      4 63488\n5      5 63682\ndf2 %>%\n  filter(prod_id==df2[sample(1:nrow(df2), 1),]$prod_id) %>%\n  group_by(prod_id,rating) %>%\n  summarise(recs=n())\n  prod_id rating recs\n1 D00008       1  597\n2 D00008       2  596\n3 D00008       3  596\n4 D00008       4  596\n5 D00008       5  596\n\ndf2 <- df2[,c(\"cust_id\",\"prod_id\",\"rating\")]\nwrite_csv(df2,out_file)\n```", "```py\nlibrary(readr)\nlibrary(recommenderlab)\nlibrary(reshape2)\n\nset.seed(42)\nin_file <- \"../dunnhumby/recommend.csv\"\ndf <- read_csv(in_file)\ndfPivot <-dcast(df, cust_id ~ prod_id)\nm <- as.matrix(dfPivot[,2:ncol(dfPivot)])\n\nrecommend <- as(m,\"realRatingMatrix\")\ne <- evaluationScheme(recommend,method=\"split\",\n train=0.9,given=-1, goodRating=5)\ne\nEvaluation scheme using all-but-1 items\nMethod: ‘split’ with 1 run(s).\nTraining set proportion: 0.900\nGood ratings: >=5.000000\nData set: 5000 x 9 rating matrix of class ‘realRatingMatrix’ with 25688 ratings.\n\nr1 <- Recommender(getData(e,\"train\"),\"UBCF\")\nr1\nRecommender of type ‘UBCF’ for ‘realRatingMatrix’ \nlearned using 4500 users.\n\np1 <- predict(r1,getData(e,\"known\"),type=\"ratings\")\nerr1<-calcPredictionAccuracy(p1,getData(e,\"unknown\"))\nprint(sprintf(\" User based collaborative filtering model MSE = %1.4f\",err1[2]))\n[1] \" User based collaborative filtering model MSE = 0.9748\"\n```", "```py\nlibrary(readr)\nlibrary(keras)\n\nset.seed(42)\nuse_session_with_seed(42, disable_gpu = FALSE, disable_parallel_cpu = FALSE)\n\ndf<-read_csv(\"recommend.csv\")\ncusts <- as.data.frame(unique(df$cust_id))\ncusts$cust_id2 <- as.numeric(row.names(custs))\ncolnames(custs) <- c(\"cust_id\",\"cust_id2\")\ncusts$cust_id2 <- custs$cust_id2 - 1\nprods <- as.data.frame(unique(df$prod_id))\nprods$prod_id2 <- as.numeric(row.names(prods))\ncolnames(prods) <- c(\"prod_id\",\"prod_id2\")\nprods$prod_id2 <- prods$prod_id2 - 1\ndf<-merge(df,custs)\ndf<-merge(df,prods)\nn_custs = length(unique(df$cust_id2))\nn_prods = length(unique(df$prod_id2))\n\n# shuffle the data\ntrainData <- df[sample(nrow(df)),]\n```", "```py\nn_factors<-10\n# define the model\ncust_in <- layer_input(shape = 1)\ncust_embed <- layer_embedding(\n input_dim = n_custs \n ,output_dim = n_factors \n ,input_length = 1 \n ,embeddings_regularizer=regularizer_l2(0.0001)\n ,name = \"cust_embed\"\n )(cust_in)\nprod_in <- layer_input(shape = 1)\nprod_embed <- layer_embedding(\n input_dim = n_prods \n ,output_dim = n_factors \n ,input_length = 1\n ,embeddings_regularizer=regularizer_l2(0.0001)\n ,name = \"prod_embed\"\n )(prod_in)\n\nub = layer_embedding(\n input_dim = n_custs, \n output_dim = 1, \n input_length = 1, \n name = \"custb_embed\"\n )(cust_in)\nub_flat <- layer_flatten()(ub)\n\nmb = layer_embedding(\n input_dim = n_prods, \n output_dim = 1, \n input_length = 1, \n name = \"prodb_embed\"\n )(prod_in)\nmb_flat <- layer_flatten()(mb)\n\ncust_flat <- layer_flatten()(cust_embed)\nprod_flat <- layer_flatten()(prod_embed)\n\nx <- layer_dot(list(cust_flat, prod_flat), axes = 1)\nx <- layer_add(list(x, ub_flat))\nx <- layer_add(list(x, mb_flat))\n```", "```py\nmodel <- keras_model(list(cust_in, prod_in), x)\ncompile(model,optimizer=\"adam\", loss='mse')\n\nmodel.optimizer.lr=0.001\nfit(model,list(trainData$cust_id2,trainData$prod_id2),trainData$rating,\n batch_size=128,epochs=40,validation_split = 0.1 )\nTrain on 23119 samples, validate on 2569 samples\nEpoch 1/40\n23119/23119 [==============================] - 1s 31us/step - loss: 10.3551 - val_loss: 9.9817\nEpoch 2/40\n23119/23119 [==============================] - 0s 21us/step - loss: 8.6549 - val_loss: 7.7826\nEpoch 3/40\n23119/23119 [==============================] - 0s 20us/step - loss: 6.0651 - val_loss: 5.2164\n...\n...\n...\nEpoch 37/40\n23119/23119 [==============================] - 0s 19us/step - loss: 0.6674 - val_loss: 0.9575\nEpoch 38/40\n23119/23119 [==============================] - 0s 18us/step - loss: 0.6486 - val_loss: 0.9555\nEpoch 39/40\n23119/23119 [==============================] - 0s 19us/step - loss: 0.6271 - val_loss: 0.9547\nEpoch 40/40\n23119/23119 [==============================] - 0s 20us/step - loss: 0.6023 - val_loss: 0.9508\n```", "```py\n##### model use-case, find products that customers 'should' be purchasing ######\ndf$preds<-predict(model,list(df$cust_id2,df$prod_id2))\n# remove index variables, do not need them anymore\ndf$cust_id2 <- NULL\ndf$prod_id2 <- NULL\nmse<-mean((df$rating-df$preds)^2)\nrmse<-sqrt(mse)\nmae<-mean(abs(df$rating-df$preds))\nprint (sprintf(\"DL Collaborative filtering model: MSE=%1.3f, RMSE=%1.3f, MAE=%1.3f\",mse,rmse,mae))\n[1] \"DL Collaborative filtering model: MSE=0.478, RMSE=0.691, MAE=0.501\"\n\ndf <- df[order(-df$preds),]\nhead(df)\n     prod_id        cust_id rating    preds\n10017 D00003 CUST0000283274      5 5.519783\n4490  D00002 CUST0000283274      5 5.476133\n9060  D00002 CUST0000084449      5 5.452055\n6536  D00002 CUST0000848462      5 5.447111\n10294 D00003 CUST0000578851      5 5.446453\n7318  D00002 CUST0000578851      5 5.442836\n\ndf[df$preds>5,]$preds <- 5\ndf[df$preds<1,]$preds <- 1\nmse<-mean((df$rating-df$preds)^2)\nrmse<-sqrt(mse)\nmae<-mean(abs(df$rating-df$preds))\nprint (sprintf(\"DL Collaborative filtering model (adjusted): MSE=%1.3f, RMSE=%1.3f, MAE=%1.3f\",mse,rmse,mae))\n[1] \"DL Collaborative filtering model (adjusted): MSE=0.476, RMSE=0.690, MAE=0.493\"\n```", "```py\ndf$diff <- df$preds - df$rating\ndf <- df[order(-df$diff),]\nhead(df,20)\n     prod_id        cust_id rating    preds     diff\n3259  D00001 CUST0000375633      1 5.000000 4.000000\n12325 D00003 CUST0000038166      1 4.306837 3.306837\n14859 D00004 CUST0000817991      1 4.025836 3.025836\n15279 D00004 CUST0000620867      1 4.016025 3.016025\n22039 D00008 CUST0000588390      1 3.989520 2.989520\n3370  D00001 CUST0000530875      1 3.969685 2.969685\n22470 D00008 CUST0000209037      1 3.927513 2.927513\n22777 D00008 CUST0000873432      1 3.905162 2.905162\n13905 D00004 CUST0000456347      1 3.877517 2.877517\n18123 D00005 CUST0000026547      1 3.853488 2.853488\n24208 D00008 CUST0000732836      1 3.810606 2.810606\n22723 D00008 CUST0000872856      1 3.746022 2.746022\n22696 D00008 CUST0000549120      1 3.718482 2.718482\n15463 D00004 CUST0000035935      1 3.714494 2.714494\n24090 D00008 CUST0000643072      1 3.679629 2.679629\n21167 D00006 CUST0000454947      1 3.651651 2.651651\n23769 D00008 CUST0000314496      1 3.649187 2.649187\n14294 D00004 CUST0000127124      1 3.625893 2.625893\n22534 D00008 CUST0000556279      1 3.578591 2.578591\n22201 D00008 CUST0000453430      1 3.576008 2.576008\n```", "```py\ndf <- df[order(df$diff),]\nhead(df,20)\n     prod_id        cust_id rating    preds      diff\n21307 D00006 CUST0000555858      5 1.318784 -3.681216\n15353 D00004 CUST0000640069      5 1.324661 -3.675339\n21114 D00006 CUST0000397007      5 1.729860 -3.270140\n23097 D00008 CUST0000388652      5 1.771072 -3.228928\n21551 D00006 CUST0000084985      5 1.804969 -3.195031\n21649 D00007 CUST0000083736      5 1.979534 -3.020466\n23231 D00008 CUST0000917696      5 2.036216 -2.963784\n21606 D00007 CUST0000899988      5 2.050258 -2.949742\n21134 D00006 CUST0000373894      5 2.071380 -2.928620\n14224 D00004 CUST0000541731      5 2.081161 -2.918839\n15191 D00004 CUST0000106540      5 2.162569 -2.837431\n13976 D00004 CUST0000952727      5 2.174777 -2.825223\n21851 D00008 CUST0000077294      5 2.202812 -2.797188\n16545 D00004 CUST0000945695      5 2.209504 -2.790496\n23941 D00008 CUST0000109728      5 2.224301 -2.775699\n24031 D00008 CUST0000701483      5 2.239778 -2.760222\n21300 D00006 CUST0000752292      5 2.240073 -2.759927\n21467 D00006 CUST0000754753      5 2.240705 -2.759295\n15821 D00004 CUST0000006239      5 2.264089 -2.735911\n15534 D00004 CUST0000586590      5 2.272885 -2.727115\n```"]