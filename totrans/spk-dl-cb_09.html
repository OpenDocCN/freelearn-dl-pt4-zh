<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Apple Stock Market Cost with LSTM</h1>
                </header>
            
            <article>
                
<p class="mce-root">Stock market predictions have been going on for many years and it has spawned an entire industry of prognosticators. It shouldn't come as a surprise since it can turn a significant profit if predicted properly. Understanding when is a good time to buy or sell a stock is key to getting the upper hand on Wall Street. This chapter will focus on creating a deep learning model using LSTM on Keras to predict the stock market quote of AAPL.</p>
<p class="mce-root">The following recipes will be covered in this chapter:</p>
<ul>
<li>Downloading stock market data for Apple</li>
<li>Exploring and visualizing stock market data for Apple</li>
<li>Preparing stock data for model performance</li>
<li>Building the LSTM model</li>
<li>Evaluating the LSTM model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading stock market data for Apple</h1>
                </header>
            
            <article>
                
<p>There are many resources for downloading stock market data for Apple. For our purposes, we will be using the Yahoo! Finance website.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require initializing a Spark cluster that will be used for all recipes in this chapter. A Spark notebook can be initialized in the terminal using <kbd>sparknotebook</kbd>, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1489 image-border" src="assets/c9cd9f51-9207-4cc3-8288-355f18c47dff.png" style="width:54.33em;height:8.75em;"/></div>
<p>A <kbd><span><span>SparkSession</span></span></kbd> can be initialized in a Jupyter notebook using the following script:</p>
<pre>spark = SparkSession.builder \<br/>    .master("local") \<br/>    .appName("StockMarket") \<br/>    .config("spark.executor.memory", "6gb") \<br/>    .getOrCreate()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The following section walks through the steps for downloading historical stock market data for Apple.</p>
<ol>
<li>Visit the following website to track the daily historical adjusted closing stock value for Apple, which has a stock ticker value of AAPL: <a href="https://finance.yahoo.com/quote/AAPL/history">https://finance.yahoo.com/quote/AAPL/history</a></li>
<li>Set and apply the following parameters to the <span class="packt_screen">Historical Data</span> tab:
<ol>
<li><span class="packt_screen">Time Period</span><span class="packt_screen">: Jan 01, 2000 - Apr 30, 2018.</span></li>
<li><span class="packt_screen">Show</span><span class="packt_screen">: Historical prices.</span></li>
<li><span class="packt_screen">Frequency</span><span class="packt_screen"><span class="packt_screen">: Daily.</span></span></li>
</ol>
</li>
</ol>
<p> </p>
<ol start="3">
<li>Download the dataset with the specified parameter to a <kbd>.csv</kbd> file by clicking on the <span class="packt_screen">Download Data</span> link, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1490 image-border" src="assets/75a7842c-6816-4485-8815-282c4f04f4c0.png" style="width:54.83em;height:22.17em;"/></div>
<ol start="4">
<li>Download the file, <kbd>AAPL.csv</kbd>, and then upload the same dataset to a Spark dataframe using the following script:</li>
</ol>
<pre style="padding-left: 60px">df =spark.read.format('com.databricks.spark.csv')\<br/>   .options(header='true', inferschema='true')\<br/>   .load('AAPL.csv')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The following section explains how the stock market data is incorporated into a Jupyter notebook.</p>
<ol>
<li>Yahoo! Finance is a great source for stock market quotes for publicly traded companies. The stock quote for Apple, AAPL, is traded on NASDAQ and the historical quotes can be captured for model development and analysis purposes. Yahoo! Finance gives you the option to capture stock quotes on a daily, weekly, or monthly snapshot.</li>
<li>The purpose of this chapter is to forecast stock at a daily level, as that would pull in the most amount of data into our training model. We can do this by tracing data back to January 1, 2000, all the way to April 30, 2018.</li>
</ol>
<p> </p>
<ol start="3">
<li>Once our parameters are set for download, we receive a nicely formatted comma-separated value file from Yahoo! Finance that can be easily converted into a Spark dataframe with minimal issues.</li>
<li>The dataframe will allow us to view the <span class="packt_screen">Date</span>, <span class="packt_screen">Open</span>, <span class="packt_screen">High</span>, <span class="packt_screen">Low</span>, <span class="packt_screen">Close</span>, <span class="packt_screen">Adj Close</span>, and <span class="packt_screen">Volume</span> of the stock on a daily basis. The columns in the dataframe track the opening and closing stock values as well as the highest and lowest values traded during that day. The number of shares traded during the day is also captured. The output of the Spark dataframe, <kbd>df</kbd>, can be shown by executing <kbd>df.show()</kbd>, as you can see in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1491 image-border" src="assets/5f01e2e7-00ae-4314-886f-bec781945611.png" style="width:44.50em;height:38.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Python had stock market APIs that allowed you to automatically connect and pull back stock market quotes for publicly traded companies such as Apple.   You would be required to input parameters and retrieve the data that can be stored in a dataframe. However, as of April 2018, the <em>Yahoo! Finance</em> API is no longer operational and therefore not a reliable solution for extracting data for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p><kbd>Pandas_datareader</kbd> is a very powerful library for extracting data from websites such as Yahoo! Finance. To learn more about the library and how it may connect back to Yahoo! Finance once it is back online, visit the following website: </p>
<p><a href="https://github.com/pydata/pandas-datareader">https://github.com/pydata/pandas-datareader</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring and visualizing stock market data for Apple</h1>
                </header>
            
            <article>
                
<p>Before any modeling and predictions are performed on the data, it is important to first explore and visualize the data at hand for any hidden gems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will perform transformations and visualizations on the dataframe in this section. This will require importing the following libraries in Python:</p>
<ul>
<li><kbd>pyspark.sql.functions</kbd></li>
<li><kbd>matplotlib</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The following section walks through the steps to explore and visualize the stock market data.</p>
<ol start="1">
<li>Transform the<span> </span><kbd>Date</kbd><span> </span>column in the dataframe by removing the timestamp using the following script:</li>
</ol>
<pre style="padding-left: 60px">import pyspark.sql.functions as f<br/>df = df.withColumn('date', f.to_date('Date'))</pre>
<ol start="2">
<li>Create a for-cycle to add three additional columns to the dataframe. The loop breaks apart the <kbd>date</kbd> field into <kbd>year</kbd>, <kbd>month</kbd>, and <kbd>day</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">date_breakdown = ['year', 'month', 'day']<br/>for i in enumerate(date_breakdown):<br/>    index = i[0]<br/>    name = i[1]<br/>    df = df.withColumn(name, f.split('date', '-')[index])</pre>
<ol start="3">
<li>Save a subset of the Spark dataframe to a <kbd>pandas</kbd> dataframe called <kbd>df_plot</kbd> using the following script: <kbd>df_plot = df.select('year', 'Adj Close').toPandas()</kbd>.</li>
<li>Graph and visualize the <kbd>pandas</kbd> dataframe, <kbd>df_plot</kbd>, inside of the notebook using the following script:</li>
</ol>
<pre style="padding-left: 60px">from matplotlib import pyplot as plt<br/>%matplotlib inline<br/><br/>df_plot.set_index('year', inplace=True)<br/>df_plot.plot(figsize=(16, 6), grid=True)<br/>plt.title('Apple stock')<br/>plt.ylabel('Stock Quote ($)')<br/>plt.show()</pre>
<ol start="5">
<li>Calculate the row and column count of our Spark dataframe using the following script: <kbd>df.toPandas().shape</kbd>.</li>
</ol>
<p> </p>
<ol start="6">
<li>Execute the following script to determine null values in the dataframe: <kbd>df.dropna().count()</kbd>.</li>
<li>Execute the following script to pull back statistics on <kbd>Open</kbd>, <kbd>High</kbd>, <kbd>Low</kbd>, <kbd>Close</kbd>, and <kbd>Adj Close</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>df.select('Open', 'High', 'Low', 'Close', 'Adj Close').describe().show()</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The following section explains the techniques used and insights gained from exploratory data analysis.</p>
<ol start="1">
<li>The<span> </span><span class="packt_screen">date</span><span> </span><span>column in the dataframe is more of a date-time column with the time values all ending in</span><span> </span><span class="packt_screen">00:00:00</span><span>. This is unnecessary for what we will need during our modeling and therefore can be removed from the dataset. Luckily for us, PySpark has a</span><span> </span><kbd>to_date</kbd><span> </span><span>function that can do this quite easily. The dataframe, </span><kbd>df</kbd><span>, is transformed using the <kbd>withColumn()</kbd> function and now only shows the date column without the timestamp, as seen in the following screenshot:</span></li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1492 image-border" src="assets/7e4db493-73b8-4a4d-a52d-c918ea13998d.png" style="width:43.92em;height:17.75em;"/></div>
<ol start="2">
<li>For analysis purposes, we want to extract the <kbd>day</kbd>, <kbd>month</kbd>, and <kbd>year</kbd> from the <kbd>date</kbd> column. We can do this by enumerating through a custom list, <kbd>date_breakdown</kbd>, to split the date by a <span class="packt_screen"><kbd>-</kbd></span> and then adding a new column for <span class="packt_screen">the year</span>, <span class="packt_screen">month</span>, and <span class="packt_screen">day</span> using the <kbd>withColumn()</kbd> function. The updated dataframe with the newly added columns can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 30px"><img class="alignnone size-full wp-image-1493 image-border" src="assets/f9a7a8a6-a99e-4f5c-8416-cdab2bc52514.png" style="width:47.42em;height:23.75em;"/></div>
<div class="packt_tip">One important takeaway is that <kbd>PySpark</kbd> also has a SQL function for dates that can extract the day, month, or year from a date timestamp. For example, if we were to add a month column to our dataframe, we would use the following script: <kbd>df.withColumn("month",f.month("date")).show()</kbd>. This is to highlight the fact that there are multiple ways to transform data within Spark.</div>
<ol start="3">
<li>
<p>Spark dataframes are more limited in visualization features than <kbd>pandas</kbd> dataframes. Therefore, we will subset two columns from the Spark dataframe, <kbd>df</kbd>, and convert them into a <kbd>pandas</kbd> dataframe for plotting a line or time-series chart. The y-axis will be the adjusted close of the stock and the x-axis will be the year of the date.</p>
</li>
<li>
<p><span>The</span> <span>pandas</span> <span>dataframe,</span> <span>df_plot</span><span>, is ready to be plotted using</span> <span>matplotlib</span> <span>once some formatting features are set, such as the grid visibility, the figure size of the plot, and the labels for the title and axes. Additionally, we explicitly state that the index of the dataframe needs to point to the year column. Otherwise, the default index will appear on the x-axis and not the year. The final time-series plot can be seen in the following screenshot:</span></p>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1494 image-border" src="assets/27142593-3862-40c6-80df-ce03e2e90c81.png" style="width:43.58em;height:24.42em;"/></div>
<ol start="5">
<li>Apple has experienced extensive growth over the last 18 years. While a few years saw some downward dips, the overall trend has been a steady upward move with the last couple of year's stock quotes hovering between $150 and $175.</li>
<li>We have made some changes to our dataframe so far, so it is important to get an inventory count of the rows and columns total as this will affect how the dataset is broken up for testing and training purposes later on in the chapter. As can be seen in the following screenshot, we have a total of 10 columns and 4,610 rows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1495 image-border" src="assets/8a8ef246-4ce2-414d-8973-3b7d20f45564.png" style="width:17.92em;height:3.83em;"/></div>
<ol start="7">
<li>When executing <kbd>df.dropna().count()</kbd>, we can see that the row count is still 4,610, which is identical to the row count from the previous step, indicating that none of the rows have any null values.</li>
<li>Finally, we can get a good read on the row count, mean, standard deviation, minimum, and maximum values of each of the columns that will be used in the model. This can help to identify whether there are anomalies in the data. One important thing to note is that each of the five fields that will be used in the model has a standard deviation higher than the mean value, indicating that the data is more spread out and not so clustered around the mean. The statistics for <span class="packt_screen">Open</span>, <span class="packt_screen">High</span>, <span class="packt_screen">Low</span>, <span class="packt_screen">Close</span>, and <span class="packt_screen">Adj Close</span> can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1496 image-border" src="assets/2df0f754-2582-417d-8429-2fc79b0cd4dd.png" style="width:46.50em;height:15.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While dataframes in Spark do not have the same native visualization features that are found in <kbd>pandas</kbd> dataframes, there are companies that manage Spark for enterprise solutions that allow for advanced visualization capabilities through notebooks without having to use libraries such as <kbd>matplotlib</kbd>. Databricks is one such company that offers this feature.</p>
<p>The following is an example of a visualization using the built-in features available in notebooks from Databricks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1500 image-border" src="assets/fc9a047b-540b-473d-902e-cd83c02c0743.png" style="width:162.50em;height:63.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about Databricks in general, visit the following website: <a href="https://databricks.com/">https://databricks.com/</a>.<a href="https://databricks.com/"/></p>
<p>To learn more about visualizations in Databricks notebooks, visit the following website: <a href="https://docs.databricks.com/user-guide/visualizations/index.html">https://docs.databricks.com/user-guide/visualizations/index.html</a>.<a href="https://docs.databricks.com/user-guide/visualizations/index.html"/></p>
<p>To learn more about accessing Databricks through a Microsoft Azure subscription, visit the following website:</p>
<p><a href="https://azure.microsoft.com/en-us/services/databricks/">https://azure.microsoft.com/en-us/services/databricks/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing stock data for model performance</h1>
                </header>
            
            <article>
                
<p>We are almost ready to build a prediction algorithm for the stock value performance of Apple. The remaining task at hand is to prepare the data in a manner that ensures the best possible predictive outcome.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will perform transformations and visualizations on the dataframe in this section. This will require importing the following libraries in Python:</p>
<ul>
<li><kbd><span><span>numpy</span></span></kbd></li>
<li><kbd>MinMaxScaler()</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps for preparing the stock market data for our model.</p>
<ol>
<li>Execute the following script to group the year column by the <kbd>Adj Close</kbd> count:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy(['year']).agg({'Adj Close':'count'})\<br/>     .withColumnRenamed('count(Adj Close)', 'Row Count')\<br/>     .orderBy(["year"],ascending=False)\<br/>     .show()</pre>
<ol start="2">
<li>Execute the following script to create two new dataframes for training and testing purposes:</li>
</ol>
<pre style="padding-left: 60px">trainDF = df[df.year &lt; 2017]<br/>testDF = df[df.year &gt; 2016]</pre>
<ol start="3">
<li>Convert the two new dataframes  to <kbd>pandas</kbd> dataframes to get row and column counts with <kbd>toPandas()</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">trainDF.toPandas().shape<br/>testDF.toPandas().shape</pre>
<ol start="4">
<li>As we did previously with <kbd>df</kbd>, we visualize <kbd>trainDF</kbd> and <kbd>testDF</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">trainDF_plot = trainDF.select('year', 'Adj Close').toPandas()<br/>trainDF_plot.set_index('year', inplace=True)<br/>trainDF_plot.plot(figsize=(16, 6), grid=True)<br/>plt.title('Apple Stock 2000-2016')<br/>plt.ylabel('Stock Quote ($)')<br/>plt.show()<br/><br/>testDF_plot = testDF.select('year', 'Adj Close').toPandas()<br/>testDF_plot.set_index('year', inplace=True)<br/>testDF_plot.plot(figsize=(16, 6), grid=True)<br/>plt.title('Apple Stock 2017-2018')<br/>plt.ylabel('Stock Quote ($)')<br/>plt.show()</pre>
<ol start="5">
<li>We create two new arrays, <kbd>trainArray</kbd> and <kbd>testArray</kbd>, based on the dataframes with the exception of the date columns using the following script:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>trainArray = np.array(trainDF.select('Open', 'High', 'Low',                     'Close','Volume', 'Adj Close' ).collect())<br/>testArray = np.array(testDF.select('Open', 'High', 'Low', 'Close','Volume',     'Adj Close' ).collect())</pre>
<ol start="6">
<li>In order to scale the arrays between 0 and 1, import <kbd>MinMaxScaler</kbd> from <kbd>sklearn</kbd> and create a function call, <kbd>MinMaxScale</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.preprocessing import MinMaxScaler<br/>minMaxScale = MinMaxScaler()</pre>
<ol start="7">
<li><kbd>MinMaxScaler</kbd> is then fit on the <kbd>trainArray</kbd> and used to create two new arrays that are scaled to fit using the following script:</li>
</ol>
<pre style="padding-left: 60px">minMaxScale.fit(trainArray)<br/><br/>testingArray = minMaxScale.transform(testArray)<br/>trainingArray = minMaxScale.transform(trainArray)</pre>
<ol start="8">
<li>Split both <kbd>testingArray</kbd> and <kbd>trainingArray</kbd>  into features, <kbd>x</kbd>, and label, <kbd>y</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">xtrain = trainingArray[:, 0:-1]<br/>xtest = testingArray[:, 0:-1]<br/>ytrain = trainingArray[:, -1:]<br/>ytest = testingArray[:, -1:]</pre>
<ol start="9">
<li>Execute the following script to retrieve a final inventory of the shape of all four arrays:</li>
</ol>
<pre style="padding-left: 60px">print('xtrain shape = {}'.format(xtrain.shape))<br/>print('xtest shape = {}'.format(xtest.shape))<br/>print('ytrain shape = {}'.format(ytrain.shape))<br/>print('ytest shape = {}'.format(ytest.shape))</pre>
<ol start="10">
<li>Execute the following script to plot the training array for the quotes <kbd>open</kbd>, <kbd>high</kbd>, <kbd>low</kbd>, and <kbd>close</kbd> :</li>
</ol>
<pre style="padding-left: 60px">plt.figure(figsize=(16,6))<br/>plt.plot(xtrain[:,0],color='red', label='open')<br/>plt.plot(xtrain[:,1],color='blue', label='high')<br/>plt.plot(xtrain[:,2],color='green', label='low')<br/>plt.plot(xtrain[:,3],color='purple', label='close')<br/>plt.legend(loc = 'upper left')<br/>plt.title('Open, High, Low, and Close by Day')<br/>plt.xlabel('Days')<br/>plt.ylabel('Scaled Quotes')<br/>plt.show()</pre>
<ol start="11">
<li>Additionally, we plot the training array for <kbd>volume</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">plt.figure(figsize=(16,6))<br/>plt.plot(xtrain[:,4],color='black', label='volume')<br/>plt.legend(loc = 'upper right')<br/>plt.title('Volume by Day')<br/>plt.xlabel('Days')<br/>plt.ylabel('Scaled Volume')<br/>plt.show()</pre>
<ol start="10"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains the transformations needed on the data to be used in the model.</p>
<ol>
<li>One of the first steps to building a model is splitting the data into a training and test dataset for model evaluation purposes. Our goal is to use all of the stock quotes from 2000 through 2016 to predict stock trends in 2017-2018. We know from previous sections that we have a total of 4,610 days of stock quotes, but we don't know exactly how many fall in each year. We can use the <kbd>groupBy()</kbd> function within the dataframe to get a unique count of stock quotes per year, as can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1497 image-border" src="assets/94a6f06e-e420-4c98-af2d-47734b6fe9b4.png" style="width:35.75em;height:25.92em;"/></div>
<ol start="2">
<li>2016 and 2017's combined data represents approximately 7% of the total data, which is a bit small for a testing dataset. However, for the purposes of this model, it should be sufficient. The remaining 93% of the dataset will be used for training purposes between 2000 and 2016. Therefore, two dataframes are created using a filter to determine whether to include or exclude rows before or after 2016.</li>
<li>We can now see that the test dataset, <kbd>testDF</kbd>, contains 333 rows and that the training dataset, <kbd>trainDF</kbd>, contains 4,277 rows. When both are combined, we reach our total row count from our original dataframe, <kbd>df</kbd>, of 4,610. Finally, we see that <kbd>testDF</kbd> is comprised of 2017 and 2018 data only, which is 251 rows for 2017 and 82 rows for 2018 for a total of 333 rows, as can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1498 image-border" src="assets/f249625c-a120-4cd6-91a0-59562546f47d.png" style="width:17.83em;height:10.08em;"/></div>
<div class="packt_tip">Please note that anytime we are converting a Spark dataframe to a <kbd>pandas</kbd> dataframe it may not always scale for big data.  While it will work for our specific example as we are using a relatively small dataset, the conversion to a <kbd>pandas</kbd> dataframe means that all of the data is loaded into the memory of the driver.  Once this conversion occurs, the data is not stored in the Spark worker nodes but is instead to the main driver node.  This is not optimal and may produce an out of memory error.  If you find that you need to convert to a <kbd>pandas</kbd> dataframe from Spark to visualize data it is recommended to pull a random sample from Spark or to aggregate the spark data to a more manageable dataset and then visualize in <kbd>pandas</kbd>.</div>
<ol start="4">
<li>Both testing and training dataframes can be visualized using <kbd>matplotlib</kbd> once a subset of the data is converted using <kbd>toPandas()</kbd> to leverage the built-in graphing capabilities of <kbd>pandas</kbd>. Visualizing the dataframes side by side showcases how the graphs appear to be similar when the y-axis for adjusted close is not scaled. In reality, we can see that <kbd>trainDF_plot</kbd> starts close to 0, but <kbd>testDF_plot</kbd> starts closer to 110, as seen in the following two screenshots:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bad4a77f-af78-4ec0-8600-1ce08fbd58df.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/d0c2e709-8238-47a5-8624-9875b5aaea6b.png" style="width:44.92em;height:21.58em;"/></div>
<ol start="5">
<li>Our stock values, as they stand, don't lend themselves well to deep learning modeling because there isn't a baseline for normalization or standardization. When working with neural networks, it is best to keep the values between 0 and 1 to match outcomes found in sigmoid or step functions that are used for activation. In order for us to accomplish this, we must first convert our <kbd>pyspark</kbd> dataframes, <kbd>trainDF</kbd> and <kbd>testDF</kbd>, into <kbd>numpy</kbd> arrays, these being <kbd>trainArray</kbd> and <kbd>testArray</kbd>. As these are now arrays and not dataframes, we will not be using the date column as the neural network is only interested in numerical values. The first values in each can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/839b0580-face-47d5-b89e-2d0401361448.png" style="width:45.58em;height:10.50em;"/></div>
<ol start="6">
<li>There are many ways to scale array values to a range between 0 and 1. It involves using the following formula: <kbd>scaled array value = (array value - min array value) / (max array value - min array value)</kbd>. Fortunately, we do not need to manually make this calculation on arrays. We can leverage the <kbd>MinMaxScaler()</kbd> function from <kbd>sklearn</kbd> to scale down both arrays.</li>
</ol>
<p> </p>
<ol start="7">
<li>The <kbd>MinMaxScaler()</kbd> function is fit on the training array, <kbd>trainArray</kbd>, and is then applied to create two brand new arrays, <kbd>trainingArray</kbd> and <kbd>testingArray</kbd>, that are scaled to values between 0 and 1. The first row for each array can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ac5852c-0e03-464c-b5cd-3074bf12f275.png" style="width:38.25em;height:15.83em;"/></div>
<ol start="8">
<li>We are now ready to set our label and feature variables by slicing up the array into x and y for both testing and training purposes. The first five elements in the array are the features or the x values and the last element is the label or y value. The features are composed of the values from <span class="packt_screen">Open</span>, <span class="packt_screen">High</span>, <span class="packt_screen">Low</span>, <span class="packt_screen">Close</span>, and <span class="packt_screen">Volume</span>. The label is composed of <span class="packt_screen">Adj Close</span>. The breakout of the first row for <kbd>trainingArray</kbd> can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a2e0b0e3-f4b8-49a0-83b8-163b4fcb4f1f.png" style="width:39.50em;height:16.25em;"/></div>
<ol start="9">
<li class="mce-root"><span>A final look at the shape of the four arrays that we will be using in the model can be used to confirm that we have<span> </span><span class="packt_screen">4,227</span><span> </span>matrix rows of training data, <span class="packt_screen">333</span><span> </span>matrix rows of test data,<span> </span><span class="packt_screen">5</span><span> </span>elements for features (</span><kbd>x</kbd><span>), and<span> </span><span class="packt_screen">1</span><span> </span>element for the label (</span><kbd>y</kbd><span>), as can be seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black"><img class="alignnone size-full wp-image-1501 image-border" src="assets/b6e38d99-ebe8-48a3-a186-3089a8074a93.png" style="width:42.33em;height:11.58em;"/></div>
<ol start="10">
<li>The values for the training array, <kbd>xtrain</kbd>, for <span class="packt_screen">open</span>, <span class="packt_screen">low</span>, <span class="packt_screen">high</span>, and <span class="packt_screen">close</span> can be plotted using the newly adjusted scales between 0 and 1 for the quotes, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1502 image-border" src="assets/240dd4a3-03f5-48ad-8ec5-079c745060b9.png" style="width:161.83em;height:85.92em;"/></div>
<ol start="11">
<li>Additionally, to <span class="packt_screen">volume</span> can also be plotted with the scaled volume scores between 0 and 1, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0e927f61-bd08-42be-9487-cac94c83908f.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While we did use <kbd>MinMaxScaler</kbd> from <kbd>sklearn</kbd>, it is also important to understand that there is also a <kbd>MinMaxScaler</kbd> function that is available directly through <kbd>pyspark.ml.feature</kbd>. It works exactly the same way by rescaling each feature to a value between 0 and 1. Had we used a machine learning library natively through <span>PySpark</span> in this chapter to make our prediction, we would have used <kbd>MinMaxScaler</kbd> from <kbd>pyspark.ml.feature</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about <kbd>MinMaxScaler</kbd> from <kbd>sklearn</kbd>, visit the following website: </p>
<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.</a></p>
<p>To learn more about <kbd>MinMaxScaler</kbd> from <kbd>pyspark</kbd>, visit the following website: </p>
<p><a href="https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler">https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the LSTM model</h1>
                </header>
            
            <article>
                
<p>The data is now in a format compatible with model development in Keras for LSTM modeling. Therefore, we will spend this section setting up and configuring the deep learning model for predicting stock quotes for Apple in 2017 and 2018.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will perform model management and hyperparameter tuning of our model in this section. This will require importing the following libraries in Python:</p>
<pre><span><span>from keras import models<br/></span></span>from keras import layers</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to setting up and tuning the LSTM model.</p>
<ol>
<li>Import the following libraries from <kbd>keras</kbd> using the following script:</li>
</ol>
<pre style="padding-left: 60px">from keras import models, layers</pre>
<ol start="2">
<li>Build a <kbd>Sequential</kbd> model using the following script:</li>
</ol>
<pre style="padding-left: 60px">model = models.Sequential()<br/>model.add(layers.LSTM(1, input_shape=(1,5)))<br/>model.add(layers.Dense(1))<br/>model.compile(loss='mean_squared_error', optimizer='adam')</pre>
<ol start="3">
<li>Transform the testing and training data sets into three-dimensional arrays using the following script:</li>
</ol>
<pre style="padding-left: 60px">xtrain = xtrain.reshape((xtrain.shape[0], 1, xtrain.shape[1]))<br/>xtest = xtest.reshape((xtest.shape[0], 1, xtest.shape[1]))</pre>
<ol start="4">
<li>Fit the  <kbd>model</kbd> using a variable called <kbd>loss</kbd> with the following script:</li>
</ol>
<pre style="padding-left: 60px">loss = model.fit(xtrain, ytrain, batch_size=10, epochs=100)</pre>
<ol start="5">
<li>Create a new array, <kbd>predicted</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">predicted = model.predict(xtest)</pre>
<ol start="6">
<li>Combine the <kbd>predicted</kbd> and <kbd>ytest</kbd> arrays into a single unified array, <kbd>combined_array</kbd>, using the following script:</li>
</ol>
<pre style="padding-left: 60px">combined_array = np.concatenate((ytest, predicted), axis = 1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This section explains how the LSTM neural network model is configured to train on our dataset.</span></p>
<ol>
<li>Most of the functionality from <kbd>keras</kbd> used to build the LSTM model will come from <kbd>models</kbd> and <kbd>layers</kbd>.</li>
<li>The <kbd>LSTM</kbd> model that has been built will be defined using a <kbd>Sequential</kbd> class that works well with time series that are sequence dependent. The LSTM model has an <kbd>input_shape = (1,5)</kbd> for one dependent variable and five independent variables in our training dataset. Only one <kbd>Dense</kbd> layer will be used to define the neural network as we are looking to keep the model simple. A loss function is required when compiling a model in keras, and since we are performing it on a recurrent neural network, a <kbd>mean_squared_error</kbd> calculation is best to determine how close the predicted value is to the actual value. Finally, an optimizer is also defined when the model is compiled to adjust the weights in the neural network. <kbd>adam</kbd> has given good results, especially when being used with recurrent neural networks.</li>
</ol>
<p> </p>
<ol start="3">
<li>Our current arrays, <kbd>xtrain</kbd> and <kbd>xtest</kbd>, are currently two-dimensional arrays; however, to incorporate them into the LSTM model, they will need to be converted to three-dimensional arrays using <kbd>reshape()</kbd>, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed6bb442-5b2e-4367-b453-589ade0a6c89.png" style="width:36.25em;height:16.67em;"/></div>
<ol start="4">
<li>The LSTM model is fit with <kbd>xtrain</kbd> and <kbd>ytrain</kbd> and the batch size is set to 10 with 100 epochs. The batch size is the setting that defines the number of objects that are trained together. We can go as low or as high as we like in terms of setting the batch size, keeping in mind that the lower the number of batches, the more memory is required. Additionally, an epoch is a measurement of how often the model goes through the entire dataset. Ultimately, these parameters can be tuned based on time and memory allotment.</li>
</ol>
<p style="padding-left: 60px">The <span class="packt_screen">mean squared error</span> loss in each <span class="packt_screen">epoch</span> is captured and visualized. After the fifth or sixth <span class="packt_screen">epoch</span>, we can see that the <span class="packt_screen">loss</span> tapers off, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1503 image-border" src="assets/5227945e-55d2-4d8f-869c-bb319f9a88b8.png" style="width:45.33em;height:35.00em;"/></div>
<ol start="5">
<li> We can now create a new array, <kbd>predicted</kbd>, based on the fitted model applied on <kbd>xtest</kbd> and then combine it with <kbd>ytest</kbd> to compare them side by side for accuracy purposes.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about parameter tuning models within keras, visit the following website: <a href="https://keras.io/models/model/">https://keras.io/models/model/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Here's the moment of truth: we are going to see if our model is able to give us a good prediction for the AAPL stock in 2017 and 2018.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will perform a model evaluation using the mean squared error. Therefore, we will need to import the following library:</p>
<pre>import sklearn.metrics as metrics</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through visualizing and calculating the predicted vs. actual stock quotes for Apple in 2017 and 2018.</p>
<ol>
<li>Plot a side by side comparison of <kbd>Actual</kbd> versus <kbd>Predicted</kbd> stock to compare trends using the following script:</li>
</ol>
<pre style="padding-left: 60px">plt.figure(figsize=(16,6))<br/>plt.plot(combined_array[:,0],color='red', label='actual')<br/>plt.plot(combined_array[:,1],color='blue', label='predicted')<br/>plt.legend(loc = 'lower right')<br/>plt.title('2017 Actual vs. Predicted APPL Stock')<br/>plt.xlabel('Days')<br/>plt.ylabel('Scaled Quotes')<br/>plt.show()</pre>
<ol start="2">
<li>Calculate the mean squared error between the actual <kbd>ytest</kbd> versus <kbd>predicted</kbd> stock using the following script:</li>
</ol>
<pre style="padding-left: 60px">import sklearn.metrics as metrics<br/>np.sqrt(metrics.mean_squared_error(ytest,predicted))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains the results of the LSTM model's evaluation.</p>
<ol>
<li>From a graphical perspective, we can see that our predictions were close to the actual stock quotes from 2017-2018, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/59aa966a-f8d8-4990-83ca-adb1e7eacc6c.png" style="width:67.33em;height:39.33em;"/></div>
<ol start="2">
<li>Our model shows that the predicted values are closer to the actual values earlier on in the days for 2017 and 2018 than later on.  Overall, while it seems that our predicted and actual scores are very close, it would be best to get a mean squared error calculation to understand how much deviation is between the two. As we can see, we have a mean squared error of 0.05841 or approximately 5.8%:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1505 image-border" src="assets/38579afd-997d-47c8-a733-538b06ae4ba8.png" style="width:33.08em;height:4.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In order to learn more about how the mean squared error is calculated within sklearn, visit the following website: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"/></p>
<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html</a>.<a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"/></p>


            </article>

            
        </section>
    </body></html>