- en: Exploring the Gym and its Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a working setup, we will start exploring the various features
    and options provided by the Gym toolkit. This chapter will walk you through some
    of the commonly used environments, the tasks they solve, and what it would take
    for your agent to master a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the various types of Gym environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the structure of the reinforcement learning loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the different observation and action spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the list of environments and nomenclature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by picking an environment and understanding the Gym interface. You
    may already be familiar with the basic function calls to create a Gym environment
    from the previous chapters, where we used them to test our installations. Here,
    we will formally go through them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s activate the `rl_gym_book` conda environment and open a Python prompt.
    The first step is to import the Gym Python module using the following line of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `gym.make` method to create an environment from the available
    list of environments. You may be asking how to find the list of Gym environments
    available on your system. We will create a small utility script to generate the
    list of environments so that you can refer to it later when you need to. Let''s
    create a script named `list_gym_envs.py` under the `~/rl_gym_book/ch4` directory
    with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This script will print the names of all the environments available through
    your Gym installation, sorted alphabetically. You can run this script using the
    following command to see the names of the environments installed and available
    in your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get an output like this. Note that only the first few environment
    names are shown in it. They may be different, depending on the environments you
    installed on your system based on what we discussed in [Chapter 3](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12),
    *Getting Started with OpenAI Gym and Deep Reinforcement Learning*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the list of environment names, you may note that there are similar names,
    with some variations. For example, there are eight different variations for the
    Alien environment. Let's try to understand the nomenclature before we pick one
    and start using it.
  prefs: []
  type: TYPE_NORMAL
- en: Nomenclature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The presence of the word *ram* in the environment name means that the observation
    returned by the environment is the contents of the **Random Access Memory** (**RAM**)
    of the Atari console on which the game was designed to run.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of the word *deterministic* in the environment names means that
    the actions sent to the environment by the agent are performed repeatedly for
    a *deterministic/fixed* duration of four frames, and then the resulting state
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of the word *NoFrameskip* means that the actions sent to the environment
    by the agent are performed once and the resulting state is returned immediately,
    without skipping any frames in-between.
  prefs: []
  type: TYPE_NORMAL
- en: By default, if *deterministic* and *NoFrameskip* are not included in the environment
    name, the action sent to the environment is repeatedly performed for a duration
    of *n* frames, where *n* is uniformly sampled from {2,3,4}.
  prefs: []
  type: TYPE_NORMAL
- en: The letter *v* followed by a number in the environment name represents the version
    of the environment. This is to make sure that any change to the environment implementation
    is reflected in its name so that the results obtained by an algorithm/agent in
    an environment are comparable to the results obtained by another algorithm/agent
    without any discrepancies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this nomenclature by looking at the Atari Alien environment.
    The various options available are listed with a description as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Version name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-ram-v0` | Observation is the RAM contents of the Atari machine with
    a total size of 128 bytes and the action sent to the environment is repeatedly
    performed for a duration of *n* frames, where *n* is uniformly sampled from {2,3,4}.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-ram-v4` | Observation is the RAM contents of the Atari machine with
    a total size of 128 bytes and the action sent to the environment is repeatedly
    performed for a duration of *n* frames, where *n* is uniformly sampled from {2,3,4}.
    There''s some modification in the environment compared to v0. |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-ramDeterministic-v0` | Observation is the RAM contents of the Atari
    machine with a total size of 128 bytes and the action sent to the environment
    is repeatedly performed for a duration of four frames. |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-ramDeterministic-v4` | Observation is the RAM contents of the Atari
    machine with a total size of 128 bytes and the action sent to the environment
    is repeatedly performed for a duration of four frames. There''s some modification
    in the environment compared to v0. |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-ramNoFrameskip-v0` | Observation is the RAM contents of the Atari
    machine with a total size of 128 bytes and the action sent to the environment
    is applied, and the resulting state is returned immediately without skipping any
    frames. |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-v0` | Observation is an RGB image of the screen represented as an
    array of shape (210, 160, 3) and the action sent to the environment is repeatedly
    performed for a duration of *n* frames, where *n* is uniformly sampled from {2,3,4}.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Alien-v4` | Observation is an RGB image of the screen represented as an
    array of shape (210, 160, 3) and the action sent to the environment is repeatedly
    performed for a duration of *n* frames, where *n* is uniformly sampled from {2,3,4}.
    There''s some modification in the environment compared to v0. |'
  prefs: []
  type: TYPE_TB
- en: '| `AlienDeterministic-v0` | Observation is an RGB image of the screen represented
    as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly
    performed for a duration of four frames. |'
  prefs: []
  type: TYPE_TB
- en: '| `AlienDeterministic-v4` | Observation is an RGB image of the screen represented
    as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly
    performed for a duration of four frames. There''s some modification in the environment
    compared to v0. |'
  prefs: []
  type: TYPE_TB
- en: '| `AlienNoFrameskip-v0` | Observation is an RGB image of the screen represented
    as an array of shape (210, 160, 3) and the action sent to the environment is applied,
    and the resulting state is returned immediately without skipping any frames. |'
  prefs: []
  type: TYPE_TB
- en: '| `AlienNoFrameskip-v4` | Observation is an RGB image of the screen represented
    as an array of shape (210, 160, 3) and the action sent to the environment is applied,
    and the resulting state is returned immediately without skipping any frames. any
    frames. There''s some modification in the environment compared to v0. |'
  prefs: []
  type: TYPE_TB
- en: This summary should help you understand the nomenclature of the environments,
    and it applies to all environments in general. The RAM may be specific to the
    Atari environments, but you now have an idea of what to expect when you see several
    related environment names.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Gym environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make it easy for us to visualize what an environment looks like or what
    its task is, we will make use of a simple script that can launch any environment
    and step through it with some randomly sampled actions. You can download the script
    from this book''s code repository under `ch4` or create a file named `run_gym_env.py`
    under `~/rl_gym_book/ch4` with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This script will take the name of the environment supplied as the first command-line
    argument and the number of steps to be run. For example, we can run the script
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command will launch the `Alien-ram-v0` environment and step through it
    2,000 times using random actions sampled from the action space of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a window pop up with the `Alien-ram-v0` environment, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the Gym interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s continue our Gym exploration by understanding the interface between
    the Gym environment and the agents that we will develop. To help us with that,
    let''s have another look at the picture we saw in [Chapter 2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12), *Reinforcement
    Learning and Deep Reinforcement Learning*, when we were discussing the basics
    of reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Did the picture give you an idea about the interface between the agent and the
    environment? We will make your understanding secure by going over the description
    of the interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we `import gym` , we `make` an environment using the following line of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `ENVIRONMENT_NAME` is the name of the environment we want, chosen from
    the list of the environments we found installed on our system. From the previous
    diagram, we can see that the first arrow comes from the environment to the agent,
    and is named Observation. From [Chapter 2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12),
    *Reinforcement Learning and Deep Reinforcement Learning*, we understand the difference
    between partially observable environments and fully observable environments, and
    the difference between state and observation in each case. We get that first observation
    from the environment by calling `env.reset()`. Let''s store the observation in
    a variable named `obs` using the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, the agent has received the observation (the end of the first arrow). It's
    time for the agent to take an action and send the action to the environment to
    see what happens. In essence, this is what the algorithms we develop for the agents
    should figure out! We'll be developing various state-of-the-art algorithms to
    develop agents in the next and subsequent chapters. Let's continue our journey
    towards understanding the Gym interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the action to be taken is decided, we send it to the environment (second
    arrow in the diagram) using the `env.step()` method, which will return four values
    in this order: `next_state`, `reward`, `done`, and `info`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `next_state` is the resulting state of the environment after the action
    was taken in the previous state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some environments may internally run one or more steps using the same action
    before returning the `next_state`. We discussed *deterministic* and *NoFrameskip*
    types in the previous section, which are examples of such environments.
  prefs: []
  type: TYPE_NORMAL
- en: The `reward` (third arrow in the diagram) is returned by the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `done` variable is a Boolean (true or false), which gets a value of true
    if the episode has terminated/finished (therefore, it is time to reset the environment)
    and false otherwise. This will be useful for the agent to know when an episode
    has ended or when the environment is going to be reset to some initial state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `info` variable returned is an optional variable, which some environments
    may return with some additional information. Usually, this is not used by the
    agent to make its decision on which action to take.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a consolidated summary of the four values returned by a Gym environment''s
    `step()` method, together with their types and a concise description about them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Returned value** | **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `next_state` (or observation) | `Object` | Observation returned by the environment.
    The object could be the RGB pixel data from the screen/camera, RAM contents, join
    angles and join velocities of a robot, and so on, depending on the environment.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `reward` | `Float` | Reward for the previous action that was sent to the
    environment. The range of the `Float` value varies with each environment, but
    irrespective of the environment, a higher reward is always better and the goal
    of the agent should be to maximize the total reward. |'
  prefs: []
  type: TYPE_TB
- en: '| `done` | `Boolean` | Indicates whether the environment is going to be reset
    in the next step. When the Boolean value is true, it most likely means that the
    episode has ended (due to loss of life of the agent, timeout, or some other episode
    termination criteria). |'
  prefs: []
  type: TYPE_TB
- en: '| `info` | `Dict` | Some additional information that can optionally be sent
    out by an environment as a dictionary of arbitrary key-value pairs. The agent
    we develop should not rely on any of the information in this dictionary for taking
    action. It may be used (if available) for debugging purposes. |'
  prefs: []
  type: TYPE_TB
- en: Note that the following code is provided to show the general structure and is
    not ready to be executed due to the `ENVIRONMENT_NAME` and the `agent.choose_action()`
    not being defined in this snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put all the pieces together and look at them in one place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'I hope you got a good understanding of one cycle of the interaction between
    the environment and the agent. This process will repeat until we decide to terminate
    the cycle after a certain number of episodes or steps have passed. Let''s now
    have a look at a complete example with the inner loop running for `MAX_STEPS_PER_EPISODE`
    and the outer loop running for `MAX_NUM_EPISODES` in a `Qbert-v0` environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this script, you will notice a Qbert screen pop up and Qbert taking
    random actions and getting a score, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will also see print statements on the console like the following, depending
    on when the episode ended. Note that the step numbers you get might be different
    because the actions are random:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The boilerplate code is available in this book's code repository under the `ch4`
    folder and is named `rl_gym_boilerplate_code.py`. It is indeed boilerplate code,
    because the overall structure of the program will remain the same. When we build
    our intelligent agents in subsequent chapters, we will extend this boilerplate
    code. It is worth taking a while and going through the script line by line to
    make sure you understand it well.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that in the example code snippets provided in this chapter
    and in [Chapter 3](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12), *Getting
    Started with OpenAI Gym and Deep Reinforcement Learning*, we used `env.action_space.sample()`
    in place of `action` in the previous code. `env.action_space` returns the type
    of the action space (`Discrete(18)`, for example, in the case of Alien-v0), and
    the `sample()` method randomly samples a value from that `action_space`. That's
    all it means!
  prefs: []
  type: TYPE_NORMAL
- en: We will now have a closer look at the spaces in the Gym to understand the state
    space and action spaces of environments.
  prefs: []
  type: TYPE_NORMAL
- en: Spaces in the Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can see that each environment in the Gym is different. Every game environment
    under the Atari category is also different from the others. For example, in the
    case of the `VideoPinball-v0` environment, the goal is to keep bouncing a ball
    with two paddles to collect points based on where the ball hits, and to make sure
    that the ball never falls below the paddles, whereas in the case of `Alien-v0`***,***
    which is another Atari game environment, the goal is to move through a maze (the
    rooms in a ship) collecting *dots*, which are equivalent to destroying the eggs
    of the alien. Aliens can be killed by collecting a pulsar dot and the reward/score
    increases when that happens. Do you see the variations in the games/environments?
    How do we know what types of actions are valid in a game?
  prefs: []
  type: TYPE_NORMAL
- en: In the VideoPinball environment, naturally, the actions are to move the paddles
    up or down, whereas in the Alien environment, the actions are to command the player
    to move left, right, up, or down. Note that there is no "move left" or "move right"
    action in the case of VideoPinball. When we look at other categories of environment,
    the variations are even greater. For example, in the case of continuous control
    environments such as recently release robotics environments with the fetch robot
    arms, the action is to vary the continuous valued join positions and joint velocities
    to achieve the task. The same discussion can be had with respect to the values
    of the observations from the environment. We already saw the different observation
    object types in the case of Atari (RAM versus RGB images).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the motivation for why the *spaces* (as in mathematics) for the observation
    and actions are defined for each environment. At the time of the writing of this
    book, there are six spaces (plus one more called `prng` for random seed) that
    are supported by OpenAI Gym. They are listed in this table, with a brief description
    of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Space type** | **Description** | **Usage Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `Box` | A box in the ![](img/00116.jpeg)space (an *n*-dimensional box) where
    each coordinate is bounded to lie in the interval defined by [low,high]. Values
    will be an array of *n* numbers. The shape defines the *n* for the space. | `gym.spaces.Box(low=-100,
    high=100, shape=(2,))` |'
  prefs: []
  type: TYPE_TB
- en: '| `Discrete` | Discrete, integer-value space in the interval [0,n-1]. The argument
    for `Discrete()` defines *n.* | `gym.spaces.Discrete(4)` |'
  prefs: []
  type: TYPE_TB
- en: '| `Dict` | A dictionary of sample space to create arbitrarily complex space.
    In the example, a Dict space is created, which consists of two discrete spaces
    for positions and velocities in three dimensions. | `gym.spaces.Dict({"position":
    gym.spaces.Discrete(3), "velocity": gym.spaces.Discrete(3)})` |'
  prefs: []
  type: TYPE_TB
- en: '| `MultiBinary` | *n*-dimensional binary space. The argument to `MultiBinary()`defines
    *n.* | `gym.spaces.MultiBinary(5)` |'
  prefs: []
  type: TYPE_TB
- en: '| `MultiDiscrete` | Multi-dimensional discrete space. | `gym.spaces.MultiDiscrete([-10,10],
    [0,1])` |'
  prefs: []
  type: TYPE_TB
- en: '| `Tuple` | A product of simpler spaces. | `gym.spaces.Tuple((gym.spaces.Discrete(2),
    spaces.Discrete(2)))` |'
  prefs: []
  type: TYPE_TB
- en: '`Box` and `Discrete` are the most commonly used action spaces. We now have
    a basic understanding of the various space types available in the Gym. Let''s
    look at how to find which observation and action spaces an environment uses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script will print the observation and the action space of a given
    environment, and also optionally print the lower bound and upper bound of the
    values in the case of a `Box Space`. Additionally, it will also print a description/meaning
    of the possible action in the environment if it is provided by the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This script is also available for download in this book''s code repository
    under `ch4`, named `get_observation_action_space.py`. You can run the script using
    the following command, where we supply the name of the environment as the first
    argument to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The script will print an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the script prints that the observation space for the `CartPole-v0`
    environment is `Box(4,)`*, *which corresponds to `cart position`, `cart velocity`,
    `pole angle`, and `pole velocity` at the tip for the four box values.
  prefs: []
  type: TYPE_NORMAL
- en: The action space is printed out to be `Discrete(2)`, which corresponds to *push
    cart to left *and *push cart to right *for the discrete values `0` and `1`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at another example that has a few more complex spaces. This
    time, let''s run the script with the `BipedalWalker-v2`environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'That produces an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A detailed description of the state space of the Bipedal Walker (v2) environment
    is tabulated here for your quick and easy reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Index** | **Name/description** | **Min** | **Max** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | hull_angle | 0 | 2*pi |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | hull_angularVelocity | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | vel_x | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | vel_y | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | hip_joint_1_angle | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | hip_joint_1_speed | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | knee_joint_1_angle | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | knee_joint_1_speed | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | leg_1_ground_contact_flag | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | hip_joint_2_angle | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | hip_joint_2_speed | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | knee_joint_2_angle | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | knee_joint_2_speed | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | leg_2_ground_contact_flag | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 14-23 | 10 lidar readings | -inf | +inf |'
  prefs: []
  type: TYPE_TB
- en: The state space, as you can see, is quite complicated, which is reasonable for
    a complex bipedal walking robot. It more or less resembles an actual bipedal robot
    system and sensor configuration that we can find in the real world, such as Boston
    Dynamics' (part of Alphabet) Atlas bipedal robot, who stole the limelight during
    the DARPA Robotics Challenge in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look into and understand the action space. A detailed description
    of the action space for the Bipedal Walker (v2) environment is tabulated here
    for your quick and easy reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Index | Name/description | Min | Max |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Hip_1 (torque/velocity) | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Knee_1 (torque/velocity) | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Hip_2 (torque/velocity) | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Knee_2 (torque/velocity) | -1 | +1 |'
  prefs: []
  type: TYPE_TB
- en: Action
  prefs: []
  type: TYPE_NORMAL
- en: The torque control is the default control method, which controls the amount
    of torque applied at the joints.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the list of Gym environments available on your
    system, which you installed in the previous chapter, and then understood the naming
    conventions, or nomenclature, of the environments. We then revisited the agent-environment
    interaction (the RL loop) diagram and understood how the Gym environment provides
    the interfaces corresponding to each of the arrows we saw in the image. We then
    looked at a consolidated summary of the four values returned by the Gym environment's
    `step()` method in a tabulated, easy-to-understand format to *reinforce* your
    understanding of what they mean!
  prefs: []
  type: TYPE_NORMAL
- en: We also explored in detail the various types of spaces used in the Gym for the
    observation and action spaces, and we used a script to print out what spaces are
    used by an environment to understand the Gym environment interfaces better. In
    our next chapter, we will consolidate all our learning so far to develop our first
    artificially intelligent agent! Excited?! Flip the page to the next chapter now!
  prefs: []
  type: TYPE_NORMAL
