["```py\nalphago_zero/\n|-- __init__.py\n|-- config.py\n|-- constants.py\n|-- controller.py\n|-- features.py\n|-- go.py\n|-- mcts.py\n|-- alphagozero_agent.py\n|-- network.py\n|-- preprocessing.py\n|-- train.py\n`-- utils.py\n```", "```py\ndef process_tf_records(list_tf_records, shuffle_records=True,\n                       buffer_size=GLOBAL_PARAMETER_STORE.SHUFFLE_BUFFER_SIZE,\n                       batch_size=GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE):\n\n    if shuffle_records:\n        random.shuffle(list_tf_records)\n\n    list_dataset = tf.data.Dataset.from_tensor_slices(list_tf_records)\n\n    tensors_dataset = list_dataset.interleave(map_func=lambda x: tf.data.TFRecordDataset(x, compression_type='ZLIB'),\n                                             cycle_length=GLOBAL_PARAMETER_STORE.CYCLE_LENGTH,\n                                             block_length=GLOBAL_PARAMETER_STORE.BLOCK_LENGTH)\n    tensors_dataset = tensors_dataset.repeat(1).shuffle(buffer_siz=buffer_size).batch(batch_size)\n\n    return tensors_dataset\n```", "```py\ndef parse_batch_tf_example(example_batch):\n    features = {\n        'x': tf.FixedLenFeature([], tf.string),\n        'pi': tf.FixedLenFeature([], tf.string),\n        'z': tf.FixedLenFeature([], tf.float32),\n    }\n    parsed_tensors = tf.parse_example(example_batch, features)\n\n    # Get the board state\n    x = tf.cast(tf.decode_raw(parsed_tensors['x'], tf.uint8), tf.float32)\n    x = tf.reshape(x, [GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE, GOPARAMETERS.N,\n                       GOPARAMETERS.N, FEATUREPARAMETERS.NUM_CHANNELS])\n\n    # Get the policy target, which is the distribution of possible moves\n    # Each target is a vector of length of board * length of board + 1\n    distribution_of_moves = tf.decode_raw(parsed_tensors['pi'], tf.float32)\n    distribution_of_moves = tf.reshape(distribution_of_moves,\n                                       [GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE, GOPARAMETERS.N * GOPARAMETERS.N + 1])\n\n    # Get the result of the game\n    # The result is simply a scalar\n    result_of_game = parsed_tensors['z']\n    result_of_game.set_shape([GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE])\n\n    return (x, {'pi_label': distribution_of_moves, 'z_label': result_of_game})\n```", "```py\ndef get_input_tensors(list_tf_records, buffer_size=GLOBAL_PARAMETER_STORE.SHUFFLE_BUFFER_SIZE):\n    logger.info(\"Getting input data and tensors\")\n    dataset = process_tf_records(list_tf_records=list_tf_records,\n                                 buffer_size=buffer_size)\n    dataset = dataset.filter(lambda input_tensor: tf.equal(tf.shape(input_tensor)[0],\n                                                           GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE))\n    dataset = dataset.map(parse_batch_tf_example)\n    logger.info(\"Finished parsing\")\n    return dataset.make_one_shot_iterator().get_next()\n```", "```py\ndef create_dataset_from_selfplay(data_extracts):\n    return (create_tf_train_example(extract_features(board_state), pi, result)\n            for board_state, pi, result in data_extracts)\n\ndef shuffle_tf_examples(batch_size, records_to_shuffle):\n    tf_dataset = process_tf_records(records_to_shuffle, batch_size=batch_size)\n    iterator = tf_dataset.make_one_shot_iterator()\n    next_dataset_batch = iterator.get_next()\n    sess = tf.Session()\n    while True:\n        try:\n            result = sess.run(next_dataset_batch)\n            yield list(result)\n        except tf.errors.OutOfRangeError:\n            break\n\ndef create_tf_train_example(board_state, pi, result):\n    board_state_as_tf_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[board_state.tostring()]))\n    pi_as_tf_feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[pi.tostring()]))\n    value_as_tf_feature = tf.train.Feature(float_list=tf.train.FloatList(value=[result]))\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        'x': board_state_as_tf_feature,\n        'pi': pi_as_tf_feature,\n        'z': value_as_tf_feature\n    }))\n\n    return tf_example\n\ndef write_tf_examples(record_path, tf_examples, serialize=True):\n    with tf.python_io.TFRecordWriter(record_path, options=TF_RECORD_CONFIG) as tf_record_writer:\n        for tf_example in tf_examples:\n            if serialize:\n                tf_record_writer.write(tf_example.SerializeToString())\n            else:\n                tf_record_writer.write(tf_example)\n```", "```py\nimport numpy as np\n\nfrom config import GOPARAMETERS\n\ndef stone_features(board_state):\n    # 16 planes, where every other plane represents the stones of a particular color\n    # which means we track the stones of the last 8 moves.\n    features = np.zeros([16, GOPARAMETERS.N, GOPARAMETERS.N], dtype=np.uint8)\n\n    num_deltas_avail = board_state.board_deltas.shape[0]\n    cumulative_deltas = np.cumsum(board_state.board_deltas, axis=0)\n    last_eight = np.tile(board_state.board, [8, 1, 1])\n    last_eight[1:num_deltas_avail + 1] -= cumulative_deltas\n    last_eight[num_deltas_avail +1:] = last_eight[num_deltas_avail].reshape(1, GOPARAMETERS.N, GOPARAMETERS.N)\n\n    features[::2] = last_eight == board_state.to_play\n    features[1::2] = last_eight == -board_state.to_play\n    return np.rollaxis(features, 0, 3)\n\ndef color_to_play_feature(board_state):\n    # 1 plane representing which color is to play\n    # The plane is filled with 1's if the color to play is black; 0's otherwise\n    if board_state.to_play == GOPARAMETERS.BLACK:\n        return np.ones([GOPARAMETERS.N, GOPARAMETERS.N, 1], dtype=np.uint8)\n    else:\n        return np.zeros([GOPARAMETERS.N, GOPARAMETERS.N, 1], dtype=np.uint8)\n\ndef extract_features(board_state):\n    stone_feat = stone_features(board_state=board_state)\n    turn_feat = color_to_play_feature(board_state=board_state)\n    all_features = np.concatenate([stone_feat, turn_feat], axis=2)\n    return all_features\n```", "```py\nimport functools\nimport logging\nimport os.path\n\nimport tensorflow as tf\n\nimport features\nimport preprocessing\nimport utils\nfrom config import GLOBAL_PARAMETER_STORE, GOPARAMETERS\nfrom constants import *\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\ndef create_partial_bn_layer(params):\n    return functools.partial(tf.layers.batch_normalization,\n        momentum=params[\"momentum\"],\n        epsilon=params[\"epsilon\"],\n        fused=params[\"fused\"],\n        center=params[\"center\"],\n        scale=params[\"scale\"],\n        training=params[\"training\"]\n    )\n\ndef create_partial_res_layer(inputs, partial_bn_layer, partial_conv2d_layer):\n    output_1 = partial_bn_layer(partial_conv2d_layer(inputs))\n    output_2 = tf.nn.relu(output_1)\n    output_3 = partial_bn_layer(partial_conv2d_layer(output_2))\n    output_4 = tf.nn.relu(tf.add(inputs, output_3))\n    return output_4\n\ndef softmax_cross_entropy_loss(logits, labels):\n return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels['pi_label']))\n\ndef mean_squared_loss(output_value, labels):\n return tf.reduce_mean(tf.square(output_value - labels['z_label']))\n\ndef get_losses(logits, output_value, labels):\n ce_loss = softmax_cross_entropy_loss(logits, labels)\n mse_loss = mean_squared_loss(output_value, labels)\n return ce_loss, mse_loss\n\ndef create_metric_ops(labels, output_policy, loss_policy, loss_value, loss_l2, loss_total):\n return {'accuracy': tf.metrics.accuracy(labels=labels['pi_label'], predictions=output_policy, name='accuracy'),\n 'loss_policy': tf.metrics.mean(loss_policy),\n 'loss_value': tf.metrics.mean(loss_value),\n 'loss_l2': tf.metrics.mean(loss_l2),\n 'loss_total': tf.metrics.mean(loss_total)}\n```", "```py\ndef generate_network_specifications(features, labels, mode, params, config=None):\n batch_norm_params = {\"epsilon\": 1e-5, \"fused\": True, \"center\": True, \"scale\": True, \"momentum\": 0.997,\n \"training\": mode==tf.estimator.ModeKeys.TRAIN\n }\n```", "```py\nwith tf.name_scope(\"shared_layers\"):\n    partial_bn_layer = create_partial_bn_layer(batch_norm_params)\n    partial_conv2d_layer = functools.partial(tf.layers.conv2d,\n        filters=params[HYPERPARAMS.NUM_FILTERS], kernel_size=[3, 3], padding=\"same\")\n    partial_res_layer = functools.partial(create_partial_res_layer, batch_norm=partial_bn_layer,\n                                          conv2d=partial_conv2d_layer)\n\n    output_shared = tf.nn.relu(partial_bn_layer(partial_conv2d_layer(features)))\n\n    for i in range(params[HYPERPARAMS.NUMSHAREDLAYERS]):\n        output_shared = partial_res_layer(output_shared)\n\n# Implement the policy network\nwith tf.name_scope(\"policy_network\"):\n    conv_p_output = tf.nn.relu(partial_bn_layer(partial_conv2d_layer(output_shared, filters=2,\n                                                                          kernel_size=[1, 1]),\n                                                                          center=False, scale=False))\n    logits = tf.layers.dense(tf.reshape(conv_p_output, [-1, GOPARAMETERS.N * GOPARAMETERS.N * 2]),\n                             units=GOPARAMETERS.N * GOPARAMETERS.N + 1)\n    output_policy = tf.nn.softmax(logits,\n                                  name='policy_output')\n\n# Implement the value network\nwith tf.name_scope(\"value_network\"):\n    conv_v_output = tf.nn.relu(partial_bn_layer(partial_conv2d_layer(output_shared, filters=1, kernel_size=[1, 1]),\n        center=False, scale=False))\n    fc_v_output = tf.nn.relu(tf.layers.dense(\n        tf.reshape(conv_v_output, [-1, GOPARAMETERS.N * GOPARAMETERS.N]),\n        params[HYPERPARAMS.FC_WIDTH]))\n    fc_v_output = tf.layers.dense(fc_v_output, 1)\n    fc_v_output = tf.reshape(fc_v_output, [-1])\n    output_value = tf.nn.tanh(fc_v_output, name='value_output')\n\n# Implement the loss functions\nwith tf.name_scope(\"loss_functions\"):\n    loss_policy, loss_value = get_losses(logits=logits,\n                                         output_value=output_value,\n                                         labels=labels)\n    loss_l2 = params[HYPERPARAMS.BETA] * tf.add_n([tf.nn.l2_loss(v)\n        for v in tf.trainable_variables() if not 'bias' in v.name])\n    loss_total = loss_policy + loss_value + loss_l2\n```", "```py\n# Steps and operations for training\nglobal_step = tf.train.get_or_create_global_step()\n\nlearning_rate = tf.train.piecewise_constant(global_step, GLOBAL_PARAMETER_STORE.BOUNDARIES,\n                                            GLOBAL_PARAMETER_STORE.LEARNING_RATE)\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith tf.control_dependencies(update_ops):\n    train_op = tf.train.MomentumOptimizer(learning_rate,\n                params[HYPERPARAMS.MOMENTUM]).minimize(loss_total, global_step=global_step)\n\nmetric_ops = create_metric_ops(labels=labels,\n                               output_policy=output_policy,\n                               loss_policy=loss_policy,\n                               loss_value=loss_value,\n                               loss_l2=loss_l2,\n                               loss_total=loss_total)\n\nfor metric_name, metric_op in metric_ops.items():\n    tf.summary.scalar(metric_name, metric_op[1])\n```", "```py\nreturn tf.estimator.EstimatorSpec(\n    mode=mode,\n    predictions={\n        'policy_output': output_policy,\n        'value_output': output_value,\n    },\n    loss=loss_total,\n    train_op=train_op,\n    eval_metric_ops=metric_ops,\n)\n```", "```py\ndef initialize_random_model(estimator_dir, **kwargs):\n    sess = tf.Session(graph=tf.Graph())\n    params = utils.parse_parameters(**kwargs)\n    initial_model_path = os.path.join(estimator_dir, PATHS.INITIAL_CHECKPOINT_NAME)\n\n    # Create the first model, where all we do is initialize random weights and immediately write them to disk\n    with sess.graph.as_default():\n        features, labels = get_inference_input()\n        generate_network_specifications(features, labels, tf.estimator.ModeKeys.PREDICT, params)\n        sess.run(tf.global_variables_initializer())\n        tf.train.Saver().save(sess, initial_model_path)\n```", "```py\ndef get_estimator(estimator_dir, **kwargs):\n    params = utils.parse_parameters(**kwargs)\n    return tf.estimator.Estimator(generate_network_specifications, model_dir=estimator_dir, params=params)\n```", "```py\ndef train(estimator_dir, tf_records, model_version, **kwargs):\n    \"\"\"\n    Main training function for the PolicyValueNetwork\n    Args:\n        estimator_dir (str): Path to the estimator directory\n        tf_records (list): A list of TFRecords from which we parse the training examples\n        model_version (int): The version of the model\n    \"\"\"\n    model = get_estimator(estimator_dir, **kwargs)\n    logger.info(\"Training model version: {}\".format(model_version))\n    max_steps = model_version * GLOBAL_PARAMETER_STORE.EXAMPLES_PER_GENERATION // \\\n                GLOBAL_PARAMETER_STORE.TRAIN_BATCH_SIZE\n    model.train(input_fn=lambda: preprocessing.get_input_tensors(list_tf_records=tf_records),\n                max_steps=max_steps)\n    logger.info(\"Trained model version: {}\".format(model_version))\n\ndef validate(estimator_dir, tf_records, checkpoint_path=None, **kwargs):\n    model = get_estimator(estimator_dir, **kwargs)\n    if checkpoint_path is None:\n        checkpoint_path = model.latest_checkpoint()\n    model.evaluate(input_fn=lambda: preprocessing.get_input_tensors(\n        list_tf_records=tf_records,\n        buffer_size=GLOBAL_PARAMETER_STORE.VALIDATION_BUFFER_SIZE),\n                   steps=GLOBAL_PARAMETER_STORE.VALIDATION_NUMBER_OF_STEPS,\n                   checkpoint_path=checkpoint_path)\n```", "```py\nclass PolicyValueNetwork():\n\n    def __init__(self, model_path, **kwargs):\n        self.model_path = model_path\n        self.params = utils.parse_parameters(**kwargs)\n        self.build_network()\n\n    def build_session(self):\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        return tf.Session(graph=tf.Graph(), config=config)\n\n    def build_network(self):\n        self.sess = self.build_session()\n\n        with self.sess.graph.as_default():\n            features, labels = get_inference_input()\n            model_spec = generate_network_specifications(features, labels,\n                                                         tf.estimator.ModeKeys.PREDICT, self.params)\n            self.inference_input = features\n            self.inference_output = model_spec.predictions\n            if self.model_path is not None:\n                self.load_network_weights(self.model_path)\n            else:\n                self.sess.run(tf.global_variables_initializer())\n\n    def load_network_weights(self, save_file):\n        tf.train.Saver().restore(self.sess, save_file)\n```", "```py\ndef predict_on_single_board_state(self, position):\n    probs, values = self.predict_on_multiple_board_states([position])\n    prob = probs[0]\n    value = values[0]\n    return prob, value\n\ndef predict_on_multiple_board_states(self, positions):\n    symmetries, processed = utils.shuffle_feature_symmetries(list(map(features.extract_features, positions)))\n    network_outputs = self.sess.run(self.inference_output, feed_dict={self.inference_input: processed})\n    action_probs, value_pred = network_outputs['policy_output'], network_outputs['value_output']\n    action_probs = utils.invert_policy_symmetries(symmetries, action_probs)\n    return action_probs, value_pred\n```", "```py\nimport collections\nimport math\n\nimport numpy as np\n\nimport utils\nfrom config import MCTSPARAMETERS, GOPARAMETERS\n\nclass RootNode(object):\n\n    def __init__(self):\n        self.parent_node = None\n        self.child_visit_counts = collections.defaultdict(float)\n        self.child_cumulative_rewards = collections.defaultdict(float)\n```", "```py\nclass MCTreeSearchNode(object):\n\n    def __init__(self, board_state, previous_move=None, parent_node=None):\n        \"\"\"\n        A node of a MCTS tree. It is primarily responsible with keeping track of its children's scores\n        and other statistics such as visit count. It also makes decisions about where to move next.\n\n        board_state (go.BoardState): The Go board\n        fmove (int): A number which represents the coordinate of the move that led to this board state. None if pass\n        parent (MCTreeSearchNode): The parent node\n        \"\"\"\n        if parent_node is None:\n            parent_node = RootNode()\n        self.parent_node = parent_node\n        self.previous_move = previous_move\n        self.board_state = board_state\n        self.is_visited = False\n        self.loss_counter = 0\n        self.illegal_moves = 1000 * (1 - self.board_state.enumerate_possible_moves())\n        self.child_visit_counts = np.zeros([GOPARAMETERS.N * GOPARAMETERS.N + 1], dtype=np.float32)\n        self.child_cumulative_rewards = np.zeros([GOPARAMETERS.N * GOPARAMETERS.N + 1], dtype=np.float32)\n        self.original_prior = np.zeros([GOPARAMETERS.N * GOPARAMETERS.N + 1], dtype=np.float32)\n        self.child_prior = np.zeros([GOPARAMETERS.N * GOPARAMETERS.N + 1], dtype=np.float32)\n        self.children_moves = {}\n```", "```py\n@property\ndef child_action_score(self):\n    return self.child_mean_rewards * self.board_state.to_play + self.child_node_scores - self.illegal_moves\n\n@property\ndef child_mean_rewards(self):\n    return self.child_cumulative_rewards / (1 + self.child_visit_counts)\n\n@property\ndef child_node_scores(self):\n    # This scores each child according to the UCT scoring system\n    return (MCTSPARAMETERS.c_PUCT * math.sqrt(1 + self.node_visit_count) * self.child_prior / \n            (1 + self.child_visit_counts))\n```", "```py\n@property\ndef node_mean_reward(self):\n    return self.node_cumulative_reward / (1 + self.node_visit_count)\n\n@property\ndef node_visit_count(self):\n    return self.parent_node.child_visit_counts[self.previous_move]\n\n@node_visit_count.setter\ndef node_visit_count(self, value):\n    self.parent_node.child_visit_counts[self.previous_move] = value\n\n@property\ndef node_cumulative_reward(self):\n    return self.parent_node.child_cumulative_rewards[self.previous_move]\n\n@node_cumulative_reward.setter\ndef node_cumulative_reward(self, value):\n    self.parent_node.child_cumulative_rewards[self.previous_move] = value\n\n@property\ndef mean_reward_perspective(self):\n    return self.node_mean_reward * self.board_state.to_play\n```", "```py\ndef choose_next_child_node(self):\n    current = self\n    pass_move = GOPARAMETERS.N * GOPARAMETERS.N\n    while True:\n        current.node_visit_count += 1\n        # We stop searching when we reach a new leaf node\n        if not current.is_visited:\n            break\n        if (current.board_state.recent\n            and current.board_state.recent[-1].move is None\n                and current.child_visit_counts[pass_move] == 0):\n            current = current.record_child_node(pass_move)\n            continue\n\n        best_move = np.argmax(current.child_action_score)\n        current = current.record_child_node(best_move)\n    return current\n\ndef record_child_node(self, next_coordinate):\n    if next_coordinate not in self.children_moves:\n        new_board_state = self.board_state.play_move(\n            utils.from_flat(next_coordinate))\n        self.children_moves[next_coordinate] = MCTreeSearchNode(\n            new_board_state, previous_move=next_coordinate, parent_node=self)\n    return self.children_moves[next_coordinate]\n```", "```py\ndef incorporate_results(self, move_probabilities, result, start_node):\n    if self.is_visited:\n        self.revert_visits(start_node=start_node)\n        return\n    self.is_visited = True\n    self.original_prior = self.child_prior = move_probabilities\n    self.child_cumulative_rewards = np.ones([GOPARAMETERS.N * GOPARAMETERS.N + 1], dtype=np.float32) * result\n    self.back_propagate_result(result, start_node=start_node)\n\ndef back_propagate_result(self, result, start_node):\n    \"\"\"\n    This function back propagates the result of a match all the way to where the search started from\n\n    Args:\n        result (int): the result of the search (1: black, -1: white won)\n        start_node (MCTreeSearchNode): the node to back propagate until\n    \"\"\"\n    # Keep track of the cumulative reward in this node\n    self.node_cumulative_reward += result\n\n    if self.parent_node is None or self is start_node:\n        return\n\n    self.parent_node.back_propagate_result(result, start_node)\n```", "```py\nimport logging\nimport os\nimport random\nimport time\n\nimport numpy as np\n\nimport go\nimport utils\nfrom config import GLOBAL_PARAMETER_STORE, GOPARAMETERS\nfrom mcts import MCTreeSearchNode\nfrom utils import make_sgf\n\nlogger = logging.getLogger(__name__)\n\nclass AlphaGoZeroAgent:\n\n    def __init__(self, network, player_v_player=False, workers=GLOBAL_PARAMETER_STORE.SIMULTANEOUS_LEAVES):\n        self.network = network\n        self.player_v_player = player_v_player\n        self.workers = workers\n        self.mean_reward_store = []\n        self.game_description_store = []\n        self.child_probability_store = []\n        self.root = None\n        self.result = 0\n        self.logging_buffer = None\n        self.conduct_exploration = True\n        if self.player_v_player:\n            self.conduct_exploration = True\n        else:\n            self.conduct_exploration = False\n```", "```py\ndef initialize_game(self, board_state=None):\n    if board_state is None:\n        board_state = go.BoardState()\n    self.root = MCTreeSearchNode(board_state)\n    self.result = 0\n    self.logging_buffer = None\n    self.game_description_store = []\n    self.child_probability_store = []\n    self.mean_reward_store = []\n```", "```py\ndef play_move(self, coordinates):\n    if not self.player_v_player:\n       self.child_probability_store.append(self.root.get_children_as_probability_distributions())\n    self.mean_reward_store.append(self.root.node_mean_reward)\n    self.game_description_store.append(self.root.describe())\n    self.root = self.root.record_child_node(utils.to_flat(coordinates))\n    self.board_state = self.root.board_state\n    del self.root.parent_node.children_moves\n    return True\n\ndef select_move(self):\n    # If we have conducted enough moves and this is single player mode, we turn off exploration\n    if self.root.board_state.n > GLOBAL_PARAMETER_STORE.TEMPERATURE_CUTOFF and not self.player_v_player:\n        self.conduct_exploration = False\n\n    if self.conduct_exploration:\n        child_visits_cum_sum = self.root.child_visit_counts.cumsum()\n        child_visits_cum_sum /= child_visits_cum_sum[-1]\n        coorindate = child_visits_cum_sum.searchsorted(random.random())\n    else:\n        coorindate = np.argmax(self.root.child_visit_counts)\n\n    return utils.from_flat(coorindate)\n```", "```py\ndef search_tree(self):\n    child_node_store = []\n    iteration_count = 0\n    while len(child_node_store) < self.workers and iteration_count < self.workers * 2:\n        iteration_count += 1\n        child_node = self.root.choose_next_child_node()\n        if child_node.is_done():\n            result = 1 if child_node.board_state.score() > 0 else -1\n            child_node.back_propagate_result(result, start_node=self.root)\n            continue\n        child_node.propagate_loss(start_node=self.root)\n        child_node_store.append(child_node)\n    if len(child_node_store) > 0:\n        move_probs, values = self.network.predict_on_multiple_board_states(\n            [child_node.board_state for child_node in child_node_store])\n        for child_node, move_prob, result in zip(child_node_store, move_probs, values):\n            child_node.revert_loss(start_node=self.root)\n            child_node.incorporate_results(move_prob, result, start_node=self.root)\n```", "```py\ndef play_match(black_net, white_net, games, readouts, sgf_dir):\n\n    # Create the players for the game\n    black = AlphaGoZeroAgent(black_net, player_v_player=True, workers=GLOBAL_PARAMETER_STORE.SIMULTANEOUS_LEAVES)\n    white = AlphaGoZeroAgent(white_net, player_v_player=True, workers=GLOBAL_PARAMETER_STORE.SIMULTANEOUS_LEAVES)\n\n    black_name = os.path.basename(black_net.model_path)\n    white_name = os.path.basename(white_net.model_path)\n```", "```py\nfor game_num in range(games):\n    # Keep track of the number of moves made in the game\n    num_moves = 0\n\n    black.initialize_game()\n    white.initialize_game()\n\n    while True:\n        start = time.time()\n        active = white if num_moves % 2 else black\n        inactive = black if num_moves % 2 else white\n\n        current_readouts = active.root.node_visit_count\n        while active.root.node_visit_count < current_readouts + readouts:\n            active.search_tree()\n```", "```py\nlogger.info(active.root.board_state)\n\n# Check whether a player should resign\nif active.should_resign():\n    active.set_result(-1 * active.root.board_state.to_play, was_resign=True)\n    inactive.set_result(active.root.board_state.to_play, was_resign=True)\n\nif active.is_done():\n    sgf_file_path = \"{}-{}-vs-{}-{}.sgf\".format(int(time.time()), white_name, black_name, game_num)\n    with open(os.path.join(sgf_dir, sgf_file_path), 'w') as fp:\n        game_as_sgf_string = make_sgf(active.board_state.recent, active.logging_buffer,\n                          black_name=black_name,\n                          white_name=white_name)\n        fp.write(game_as_sgf_string)\n    print(\"Game Over\", game_num, active.logging_buffer)\n    break\n\nmove = active.select_move()\nactive.play_move(move)\ninactive.play_move(move)\n```", "```py\nimport argparse\nimport logging\nimport os\nimport random\nimport socket\nimport sys\nimport time\n\nimport argh\nimport tensorflow as tf\nfrom tensorflow import gfile\nfrom tqdm import tqdm\n\nimport alphagozero_agent\nimport network\nimport preprocessing\nfrom config import GLOBAL_PARAMETER_STORE\nfrom constants import PATHS\nfrom alphagozero_agent import play_match\nfrom network import PolicyValueNetwork\nfrom utils import logged_timer as timer\nfrom utils import print_flags, generate, detect_model_name, detect_model_version\n\nlogging.basicConfig(\n level=logging.DEBUG,\n handlers=[logging.StreamHandler(sys.stdout)],\n format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n)\n\nlogger = logging.getLogger(__name__)\n\ndef get_models():\n \"\"\"\n Get all model versions\n \"\"\"\n all_models = gfile.Glob(os.path.join(PATHS.MODELS_DIR, '*.meta'))\n model_filenames = [os.path.basename(m) for m in all_models]\n model_versionbers_names = sorted([\n (detect_model_version(m), detect_model_name(m))\n for m in model_filenames])\n return model_versionbers_names\n\ndef get_latest_model():\n \"\"\"\n Get the latest model\n\n Returns:\n Tuple of <int, str>, or <model_version, model_name>\n \"\"\"\n return get_models()[-1]\n```", "```py\ndef initialize_random_model():\n    bootstrap_name = generate(0)\n    bootstrap_model_path = os.path.join(PATHS.MODELS_DIR, bootstrap_name)\n    logger.info(\"Bootstrapping with working dir {}\\n Model 0 exported to {}\".format(\n        PATHS.ESTIMATOR_WORKING_DIR, bootstrap_model_path))\n    maybe_create_directory(PATHS.ESTIMATOR_WORKING_DIR)\n    maybe_create_directory(os.path.dirname(bootstrap_model_path))\n    network.initialize_random_model(PATHS.ESTIMATOR_WORKING_DIR)\n    network.export_latest_checkpoint_model(PATHS.ESTIMATOR_WORKING_DIR, bootstrap_model_path)\n```", "```py\ndef selfplay():\n    _, model_name = get_latest_model()\n    try:\n        games = gfile.Glob(os.path.join(PATHS.SELFPLAY_DIR, model_name, '*.zz'))\n        if len(games) > GLOBAL_PARAMETER_STORE.MAX_GAMES_PER_GENERATION:\n            logger.info(\"{} has enough games ({})\".format(model_name, len(games)))\n            time.sleep(600)\n            sys.exit(1)\n    except:\n        pass\n\n    for game_idx in range(GLOBAL_PARAMETER_STORE.NUM_SELFPLAY_GAMES):\n        logger.info('================================================')\n        logger.info(\"Playing game {} with model {}\".format(game_idx, model_name))\n        logger.info('================================================')\n        model_save_path = os.path.join(PATHS.MODELS_DIR, model_name)\n        game_output_dir = os.path.join(PATHS.SELFPLAY_DIR, model_name)\n        game_holdout_dir = os.path.join(PATHS.HOLDOUT_DIR, model_name)\n        sgf_dir = os.path.join(PATHS.SGF_DIR, model_name)\n\n        clean_sgf = os.path.join(sgf_dir, 'clean')\n        full_sgf = os.path.join(sgf_dir, 'full')\n        os.makedirs(clean_sgf, exist_ok=True)\n        os.makedirs(full_sgf, exist_ok=True)\n        os.makedirs(game_output_dir, exist_ok=True)\n        os.makedirs(game_holdout_dir, exist_ok=True)\n```", "```py\nwith timer(\"Loading weights from %s ... \" % model_save_path):\n    network = PolicyValueNetwork(model_save_path)\n\nwith timer(\"Playing game\"):\n    agent = alphagozero_agent.play_against_self(network, GLOBAL_PARAMETER_STORE.SELFPLAY_READOUTS)\n\n```", "```py\noutput_name = '{}-{}'.format(int(time.time()), socket.gethostname())\ngame_play = agent.extract_data()\nwith gfile.GFile(os.path.join(clean_sgf, '{}.sgf'.format(output_name)), 'w') as f:\n    f.write(agent.to_sgf(use_comments=False))\nwith gfile.GFile(os.path.join(full_sgf, '{}.sgf'.format(output_name)), 'w') as f:\n    f.write(agent.to_sgf())\n\ntf_examples = preprocessing.create_dataset_from_selfplay(game_play)\n\n# We reserve 5% of games played for validation\nholdout = random.random() < GLOBAL_PARAMETER_STORE.HOLDOUT\nif holdout:\n    to_save_dir = game_holdout_dir\nelse:\n    to_save_dir = game_output_dir\ntf_record_path = os.path.join(to_save_dir, \"{}.tfrecord.zz\".format(output_name))\n\npreprocessing.write_tf_examples(tf_record_path, tf_examples)\n```", "```py\ndef aggregate():\n    logger.info(\"Gathering game results\")\n\n    os.makedirs(PATHS.TRAINING_CHUNK_DIR, exist_ok=True)\n    os.makedirs(PATHS.SELFPLAY_DIR, exist_ok=True)\n    models = [model_dir.strip('/')\n              for model_dir in sorted(gfile.ListDirectory(PATHS.SELFPLAY_DIR))[-50:]]\n\n    with timer(\"Finding existing tfrecords...\"):\n        model_gamedata = {\n            model: gfile.Glob(\n                os.path.join(PATHS.SELFPLAY_DIR, model, '*.zz'))\n            for model in models\n        }\n    logger.info(\"Found %d models\" % len(models))\n    for model_name, record_files in sorted(model_gamedata.items()):\n        logger.info(\"    %s: %s files\" % (model_name, len(record_files)))\n\n    meta_file = os.path.join(PATHS.TRAINING_CHUNK_DIR, 'meta.txt')\n    try:\n        with gfile.GFile(meta_file, 'r') as f:\n            already_processed = set(f.read().split())\n    except tf.errors.NotFoundError:\n        already_processed = set()\n\n    num_already_processed = len(already_processed)\n\n    for model_name, record_files in sorted(model_gamedata.items()):\n        if set(record_files) <= already_processed:\n            continue\n        logger.info(\"Gathering files for %s:\" % model_name)\n        for i, example_batch in enumerate(\n                tqdm(preprocessing.shuffle_tf_examples(GLOBAL_PARAMETER_STORE.EXAMPLES_PER_RECORD, record_files))):\n            output_record = os.path.join(PATHS.TRAINING_CHUNK_DIR,\n                                         '{}-{}.tfrecord.zz'.format(model_name, str(i)))\n            preprocessing.write_tf_examples(\n                output_record, example_batch, serialize=False)\n        already_processed.update(record_files)\n\n    logger.info(\"Processed %s new files\" %\n          (len(already_processed) - num_already_processed))\n    with gfile.GFile(meta_file, 'w') as f:\n        f.write('\\n'.join(sorted(already_processed)))\n```", "```py\ndef train():\n    model_version, model_name = get_latest_model()\n    logger.info(\"Training on gathered game data, initializing from {}\".format(model_name))\n    new_model_name = generate(model_version + 1)\n    logger.info(\"New model will be {}\".format(new_model_name))\n    save_file = os.path.join(PATHS.MODELS_DIR, new_model_name)\n\n    try:\n        logger.info(\"Getting tf_records\")\n        tf_records = sorted(gfile.Glob(os.path.join(PATHS.TRAINING_CHUNK_DIR, '*.tfrecord.zz')))\n        tf_records = tf_records[\n                     -1 * (GLOBAL_PARAMETER_STORE.WINDOW_SIZE // GLOBAL_PARAMETER_STORE.EXAMPLES_PER_RECORD):]\n\n        print(\"Training from:\", tf_records[0], \"to\", tf_records[-1])\n\n        with timer(\"Training\"):\n            network.train(PATHS.ESTIMATOR_WORKING_DIR, tf_records, model_version+1)\n            network.export_latest_checkpoint_model(PATHS.ESTIMATOR_WORKING_DIR, save_file)\n\n    except:\n        logger.info(\"Got an error training\")\n        logging.exception(\"Train error\")\n```", "```py\ndef validate(model_version=None, validate_name=None):\n    if model_version is None:\n        model_version, model_name = get_latest_model()\n    else:\n        model_version = int(model_version)\n        model_name = get_model(model_version)\n\n    models = list(\n        filter(lambda num_name: num_name[0] < (model_version - 1), get_models()))\n\n    if len(models) == 0:\n        logger.info('Not enough models, including model N for validation')\n        models = list(\n            filter(lambda num_name: num_name[0] <= model_version, get_models()))\n    else:\n        logger.info('Validating using data from following models: {}'.format(models))\n\n    tf_record_dirs = [os.path.join(PATHS.HOLDOUT_DIR, pair[1])\n                    for pair in models[-5:]]\n\n    working_dir = PATHS.ESTIMATOR_WORKING_DIR\n    checkpoint_name = os.path.join(PATHS.MODELS_DIR, model_name)\n\n    tf_records = []\n    with timer(\"Building lists of holdout files\"):\n        for record_dir in tf_record_dirs:\n            tf_records.extend(gfile.Glob(os.path.join(record_dir, '*.zz')))\n\n    with timer(\"Validating from {} to {}\".format(os.path.basename(tf_records[0]), os.path.basename(tf_records[-1]))):\n        network.validate(working_dir, tf_records, checkpoint_path=checkpoint_name, name=validate_name)\n```", "```py\ndef evaluate(black_model, white_model):\n    os.makedirs(PATHS.SGF_DIR, exist_ok=True)\n\n    with timer(\"Loading weights\"):\n        black_net = network.PolicyValueNetwork(black_model)\n        white_net = network.PolicyValueNetwork(white_model)\n\n    with timer(\"Playing {} games\".format(GLOBAL_PARAMETER_STORE.EVALUATION_GAMES)):\n        play_match(black_net, white_net, GLOBAL_PARAMETER_STORE.EVALUATION_GAMES,\n                   GLOBAL_PARAMETER_STORE.EVALUATION_READOUTS, PATHS.SGF_DIR)\n```", "```py\nimport subprocess\nimport sys\nfrom utils import timer\n\nimport os\n\nfrom constants import PATHS\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef main():\n\n    if not os.path.exists(PATHS.SELFPLAY_DIR):\n        with timer(\"Initialize\"):\n            logger.info('==========================================')\n            logger.info(\"============ Initializing...==============\")\n            logger.info('==========================================')\n            res = subprocess.call(\"python controller.py initialize-random-model\", shell=True)\n\n        with timer('Initial Selfplay'):\n            logger.info('=======================================')\n            logger.info('============ Selplaying...=============')\n            logger.info('=======================================')\n            subprocess.call('python controller.py selfplay', shell=True)\n```", "```py\nwhile True:\n    with timer(\"Aggregate\"):\n        logger.info('=========================================')\n        logger.info(\"============ Aggregating...==============\")\n        logger.info('=========================================')\n        res = subprocess.call(\"python controller.py aggregate\", shell=True)\n        if res != 0:\n            logger.info(\"Failed to gather\")\n            sys.exit(1)\n\n    with timer(\"Train\"):\n        logger.info('=======================================')\n        logger.info(\"============ Training...===============\")\n        logger.info('=======================================')\n        subprocess.call(\"python controller.py train\", shell=True)\n\n    with timer('Selfplay'):\n        logger.info('=======================================')\n        logger.info('============ Selplaying...=============')\n        logger.info('=======================================')\n        subprocess.call('python controller.py selfplay', shell=True)\n\n    with timer(\"Validate\"):\n        logger.info('=======================================')\n        logger.info(\"============ Validating...=============\")\n        logger.info('=======================================')\n        subprocess.call(\"python controller.py validate\", shell=True)\n```", "```py\nif __name__ == '__main__':\n    main()\n```", "```py\n$ python train.py\n```", "```py\n2018-09-14 03:41:27,286 utils INFO Playing game: 342.685 seconds\n2018-09-14 03:41:27,332 __main__ INFO ================================================\n2018-09-14 03:41:27,332 __main__ INFO Playing game 9 with model 000010-pretty-tetra\n2018-09-14 03:41:27,332 __main__ INFO ================================================\nINFO:tensorflow:Restoring parameters from models/000010-pretty-tetra\n2018-09-14 03:41:32,352 tensorflow INFO Restoring parameters from models/000010-pretty-tetra\n2018-09-14 03:41:32,624 utils INFO Loading weights from models/000010-pretty-tetra ... : 5.291 seconds\n```", "```py\n A B C D E F G H J\n 9 . . . . . . . . X 9\n 8 . . . X . . O . . 8\n 7 . . . . X O O . . 7\n 6 O . X X X<. . . . 6\n 5 X . O O . . O X . 5\n 4 . . X X . . . O . 4\n 3 . . X . X . O O . 3\n 2 . . . O . . . . X 2\n 1 . . . . . . . . . 1\n A B C D E F G H J\nMove: 25\\. Captures X: 0 O: 0\n -5.5\n A B C D E F G H J\n 9 . . . . . . . . X 9\n 8 . . . X . . O . . 8\n 7 . . . . X O O . . 7\n 6 O . X X X . . . . 6\n 5 X . O O . . O X . 5\n 4 . . X X . . . O . 4\n 3 . . X . X . O O . 3\n 2 . . . O . . . . X 2\n 1 . . . . . . . . . 1\n A B C D E F G H J\nMove: 26\\. Captures X: 0 O: 0\n```", "```py\npython controller.py evaluate models/{model_name_1} models/{model_name_2}\n```"]