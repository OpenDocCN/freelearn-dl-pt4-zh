["```py\nStart: 14 August 2006\nEnd: 13th August 2015\n```", "```py\nStart: 14 August 2015\nEnd: 14 August 2018\n```", "```py\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\n\n```", "```py\nclass Actor:\n\n  # \"\"\"Actor (policy) Model. \"\"\"\n\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n\n```", "```py\n        self.build_model()\n```", "```py\n    def build_model(self):\n        states = layers.Input(shape=(self.state_size,), name='states')         \n```", "```py\n        net = layers.Dense(units=16,kernel_regularizer=layers.regularizers.l2(1e-6))(states)\n        net = layers.BatchNormalization()(net)\n        net = layers.Activation(\"relu\")(net)\n        net = layers.Dense(units=32,kernel_regularizer=layers.regularizers.l2(1e-6))(net)\n        net = layers.BatchNormalization()(net)\n        net = layers.Activation(\"relu\")(net)\n```", "```py\n        actions = layers.Dense(units=self.action_size, activation='softmax', name = 'actions')(net)\n\n        self.model = models.Model(inputs=states, outputs=actions)\n\n```", "```py\n        action_gradients = layers.Input(shape=(self.action_size,))\n        loss = K.mean(-action_gradients * actions)\n```", "```py\n        optimizer = optimizers.Adam(lr=.00001)\n        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n        self.train_fn = K.function(\n            inputs=[self.model.input, action_gradients, K.learning_phase()],\n            outputs=[],\n            updates=updates_op)\n```", "```py\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\n```", "```py\nclass Critic:\n    \"\"\"Critic (Value) Model.\"\"\"\n\n    def __init__(self, state_size, action_size):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n        \"\"\"\n        self.state_size = state_size\n        self.action_size = action_size\n\n        self.build_model()\n```", "```py\n    def build_model(self):\n        states = layers.Input(shape=(self.state_size,), name='states')\n        actions = layers.Input(shape=(self.action_size,), name='actions')\n```", "```py\n        net_states = layers.Dense(units=16,kernel_regularizer=layers.regularizers.l2(1e-6))(states)\n        net_states = layers.BatchNormalization()(net_states)\n        net_states = layers.Activation(\"relu\")(net_states)\n\n        net_states = layers.Dense(units=32, kernel_regularizer=layers.regularizers.l2(1e-6))(net_states)\n```", "```py\n        net_actions = layers.Dense(units=32,kernel_regularizer=layers.regularizers.l2(1e-6))(actions)\n```", "```py\n        net = layers.Add()([net_states, net_actions])\n        net = layers.Activation('relu')(net)\n```", "```py\n        Q_values = layers.Dense(units=1, name='q_values',kernel_initializer=layers.initializers.RandomUniform(minval=-0.003, maxval=0.003))(net)\n```", "```py\n        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n```", "```py\n        optimizer = optimizers.Adam(lr=0.001)\n        self.model.compile(optimizer=optimizer, loss='mse')\n```", "```py\n        action_gradients = K.gradients(Q_values, actions)\n```", "```py\n        self.get_action_gradients = K.function(\n            inputs=[*self.model.input, K.learning_phase()],\n            outputs=action_gradients)\n```", "```py\nfrom actor import Actor\nfrom critic import Critic\n```", "```py\nimport numpy as np\nfrom numpy.random import choice\nimport random\n\nfrom collections import namedtuple, deque\n```", "```py\nclass ReplayBuffer:\n    #Fixed sized buffer to stay experience tuples\n\n    def __init__(self, buffer_size, batch_size):\n\n    #Initialize a replay buffer object.\n\n    #parameters\n\n    #buffer_size: maximum size of buffer. Batch size: size of each batch\n\n        self.memory = deque(maxlen = buffer_size)  #memory size of replay buffer\n        self.batch_size = batch_size               #Training batch size for Neural nets\n        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])                                           #Tuple containing experienced replay\n\n```", "```py\n    def add(self, state, action, reward, next_state, done):\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n```", "```py\n    def sample(self, batch_size = 32):\n        return random.sample(self.memory, k=self.batch_size)\n```", "```py\n    def __len__(self):\n        return len(self.memory)\n```", "```py\nclass Agent:\n    def __init__(self, state_size, batch_size, is_eval = False):\n        self.state_size = state_size #\n```", "```py\n        self.action_size = 3 \n```", "```py\n        self.buffer_size = 1000000\n        self.batch_size = batch_size\n        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n        self.inventory = []\n```", "```py\n        self.is_eval = is_eval    \n```", "```py\n        self.gamma = 0.99        \n```", "```py\n        self.tau = 0.001   \n```", "```py\n        self.actor_local = Actor(self.state_size, self.action_size) \n        self.actor_target = Actor(self.state_size, self.action_size)    \n```", "```py\n        self.critic_local = Critic(self.state_size, self.action_size)\n```", "```py\n        self.critic_target = Critic(self.state_size, self.action_size)    \n        self.critic_target.model.set_weights(self.critic_local.model.get_weights()) \n```", "```py\n      self.actor_target.model.set_weights(self.actor_local.model.get_weights()\n```", "```py\n def act(self, state):\n        options = self.actor_local.model.predict(state) \n        self.last_state = state\n        if not self.is_eval:\n            return choice(range(3), p = options[0])     \n        return np.argmax(options[0])\n```", "```py\n    def step(self, action, reward, next_state, done):\n```", "```py\n        self.memory.add(self.last_state, action, reward, next_state, \n          done) \n```", "```py\n        if len(self.memory) > self.batch_size:               \n```", "```py\n       experiences = self.memory.sample(self.batch_size)\n```", "```py\n        self.learn(experiences)                                 \n```", "```py\n        self.last_state = next_state                   \n```", "```py\n    def learn(self, experiences):               \n        states = np.vstack([e.state for e in experiences if e is not None]).astype(np.float32).reshape(-1,self.state_size)    \n        actions = np.vstack([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1,self.action_size)\n        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1,1)\n        dones = np.array([e.done for e in experiences if e is not None]).astype(np.float32).reshape(-1,1)\n        next_states = np.vstack([e.next_state for e in experiences if e is not None]).astype(np.float32).reshape(-1,self.state_size) \n\n```", "```py\n        actions_next = self.actor_target.model.predict_on_batch(next_states)    \n```", "```py\n        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])  \n```", "```py\n        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)   \n```", "```py\n        self.critic_local.model.train_on_batch(x = [states, actions], y = Q_targets) \n\n```", "```py\n        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]),(-1, self.action_size))\n```", "```py\n        self.actor_local.train_fn([states, action_gradients, 1])  \n```", "```py\n        self.soft_update(self.actor_local.model, self.actor_target.model)\n```", "```py\n    def soft_update(self, local_model, target_model):\n        local_weights = np.array(local_model.get_weights())\n        target_weights = np.array(target_model.get_weights())\n        assert len(local_weights) == len(target_weights)\n        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n        target_model.set_weights(new_weights)\n```", "```py\nimport numpy as np\nimport math\n```", "```py\ndef formatPrice(n):\n\n    if n>=0:\n        curr = \"$\"\n    else:\n        curr = \"-$\"\n    return (curr +\"{0:.2f}\".format(abs(n)))\n```", "```py\ndef getStockData(key):\n    datavec = []\n    lines = open(\"data/\" + key + \".csv\", \"r\").read().splitlines()\n\n    for line in lines[1:]:\n        datavec.append(float(line.split(\",\")[4]))\n\n    return datavec\n```", "```py\ndef getState(data, t, window):    \n    if t - window >= -1:\n        vec = data[t - window+ 1:t+ 1]\n    else: \n        vec = -(t-window+1)*[data[0]]+data[0: t + 1]\n    scaled_state = []\n    for i in range(window - 1):\n```", "```py\n        scaled_state.append(1/(1 + math.exp(vec[i] - vec[i+1])))  \n    return np.array([scaled_state])\n```", "```py\nfrom agent import Agent\nfrom helper import getStockData, getState\nimport sys\n```", "```py\nwindow_size = 100                         \nbatch_size = 32\n```", "```py\nagent = Agent(window_size, batch_size)  \n```", "```py\ndata = getStockData(\"^GSPC\")\nl = len(data) - 1\n```", "```py\nepisode_count = 300\n```", "```py\nfor e in range(episode_count):\n    print(\"Episode \" + str(e) + \"/\" + str(episode_count))\n```", "```py\n    state = getState(data, 0, window_size + 1)\n    agent.inventory = []\n    total_profit = 0\n    done = False\n```", "```py\n    for t in range(l):\n        action = agent.act(state)\n        action_prob = agent.actor_local.model.predict(state)\n\n        next_state = getState(data, t + 1, window_size + 1)\n        reward = 0\n\n```", "```py\n        if action == 1:\n            agent.inventory.append(data[t])\n            print(\"Buy:\" + formatPrice(data[t]))\n```", "```py\n        elif action == 2 and len(agent.inventory) > 0:  # sell\n            bought_price = agent.inventory.pop(0)\n            reward = max(data[t] - bought_price, 0)\n            total_profit += data[t] - bought_price\n            print(\"sell: \" + formatPrice(data[t]) + \"| profit: \" + \n              formatPrice(data[t] - bought_price))\n\n        if t == l - 1:\n            done = True\n        agent.step(action_prob, reward, next_state, done)\n        state = next_state\n\n        if done:\n            print(\"------------------------------------------\")\n            print(\"Total Profit: \" + formatPrice(total_profit))\n            print(\"------------------------------------------\")\n```", "```py\nsell: $2102.15| profit: $119.30\nsell: $2079.65| profit: $107.36\nBuy:$2067.64\nsell: $2108.57| profit: $143.75\nBuy:$2108.63\nBuy:$2093.32\nBuy:$2099.84\nBuy:$2083.56\nBuy:$2077.57\nBuy:$2104.18\nsell: $2084.07| profit: $115.18\nsell: $2086.05| profit: $179.92\n------------------------------------------\nTotal Profit: $57473.53\n```", "```py\ntest_data = getStockData(\"^GSPC Test\")\nl_test = len(test_data) - 1\nstate = getState(test_data, 0, window_size + 1)\n```", "```py\ntotal_profit = 0\nagent.inventory = []\nagent.is_eval = False\ndone = False\n```", "```py\nfor t in range(l_test):\n    action = agent.act(state)\n```", "```py\n    next_state = getState(test_data, t + 1, window_size + 1)\n    reward = 0\n\n```", "```py\n    if action == 1:  \n\n        agent.inventory.append(test_data[t])\n        print(\"Buy: \" + formatPrice(test_data[t]))\n```", "```py\n    elif action == 2 and len(agent.inventory) > 0: \n        bought_price = agent.inventory.pop(0)\n        reward = max(test_data[t] - bought_price, 0)\n        total_profit += test_data[t] - bought_price\n        print(\"Sell: \" + formatPrice(test_data[t]) + \" | profit: \" + formatPrice(test_data[t] - bought_price))\n\n    if t == l_test - 1:\n        done = True\n    agent.step(action_prob, reward, next_state, done)\n    state = next_state\n\n    if done:\n        print(\"------------------------------------------\")\n        print(\"Total Profit: \" + formatPrice(total_profit))\n        print(\"------------------------------------------\")\n```", "```py\nSell: $2818.82 | profit: $44.80\nSell: $2802.60 | profit: $4.31\nBuy: $2816.29\nSell: $2827.22 | profit: $28.79\nBuy: $2850.40\nSell: $2857.70 | profit: $53.21\nBuy: $2853.58\nBuy: $2833.28\n------------------------------------------\nTotal Profit: $10427.24\n```"]