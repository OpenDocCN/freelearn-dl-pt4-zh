["```py\npip3 install tensorflow\npip3 install numpy\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nimport sys\nimport collections\n```", "```py\ndef get_words(file_name):\n    with open(file_name) as file:\n        all_lines = file.readlines()\n    lines_without_spaces = [x.strip() for x in all_lines]\n    words = []\n    for line in lines_without_spaces:\n        words.extend(line.split())\n    words = np.array(words)\n    return words\n```", "```py\ndef build_dictionary(words):\n    most_common_words = collections.Counter(words).most_common()\n    word2id = dict((word, id) for (id, (word, _)) in enumerate(most_common_words))\n    id2word = dict((id, word) for (id, (word, _)) in enumerate(most_common_words))\n    return most_common_words, word2id, id2word\n```", "```py\nwords = get_words(\"the_hunger_games.txt\")\nmost_common_words, word2id, id2word = build_dictionary(words)\nmost_common_words_length = len(most_common_words)\n```", "```py\nsection_length = 20\n\ndef input_output_values(words):\n    input_values = []\n    output_values = []\n    num_sections = 0\n    for i in range(len(words) - section_length):\n        input_values.append(words[i: i + section_length])\n        output_values.append(words[i + section_length])\n        num_sections += 1\n\n    one_hot_inputs = np.zeros((num_sections, section_length, most_common_words_length))\n    one_hot_outputs = np.zeros((num_sections, most_common_words_length))\n\n    for s_index, section in enumerate(input_values):\n        for w_index, word in enumerate(section):\n            one_hot_inputs[s_index, w_index, word2id[word]] = 1.0\n        one_hot_outputs[s_index, word2id[output_values[s_index]]] = 1.0\n\n    return one_hot_inputs, one_hot_outputs   \n```", "```py\ntraining_X, training_y = input_output_values(words)\n```", "```py\nlearning_rate = 0.001\nbatch_size = 512\nnumber_of_iterations = 100000\nnumber_hidden_units = 1024\n```", "```py\n      X = tf.placeholder(tf.float32, shape=[batch_size, section_length,                                most_common_words_length])\n      y = tf.placeholder(tf.float32, shape=[batch_size, most_common_words_length])\n```", "```py\n      weights = tf.Variable(tf.truncated_normal([num_hidden_units, \n      most_common_words_length]))\n      biases = \n      tf.Variable(tf.truncated_normal([most_common_words_length]))\n```", "```py\n      gru_cell = tf.contrib.rnn.GRUCell(num_units=num_hidden_units)\n```", "```py\n      outputs, state = tf.nn.dynamic_rnn(gru_cell, inputs=X, \n       dtype=tf.float32)\n      outputs = tf.transpose(outputs, perm=[1, 0, 2])\n\n      last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n```", "```py\n      prediction = tf.matmul(last_output, weights) + biases\n\n      loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, \n      logits=prediction)\n      total_loss = tf.reduce_mean(loss)\n```", "```py\n      optimizer = \n     tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n     loss=total_loss)\n```", "```py\n      with tf.Session() as sess:\n          sess.run(tf.global_variables_initializer())\n          iter_offset = 0\n```", "```py\n      saver = tf.train.Saver()\n```", "```py\n      for iter in range(number_of_iterations):\n          length_X = len(training_X)\n\n          if length_X != 0:\n              iter_offset = iter_offset % length_X\n\n          if iter_offset <= length_X - batch_size:\n              training_X_batch = training_X[iter_offset: iter_offset +               \n                batch_size]\n              training_y_batch = training_y[iter_offset: iter_offset + \n               batch_size]\n              iter_offset += batch_size\n          else:\n              add_from_the_beginning = batch_size - (length_X - \n               iter_offset)\n              training_X_batch = \n               np.concatenate((training_X[iter_offset: length_X], X[0:                         \n               add_from_the_beginning]))\n              training_y_batch = \n               np.concatenate((training_y[iter_offset:  \n              length_X], y[0: add_from_the_beginning]))\n              iter_offset = add_from_the_beginning\n```", "```py\n        _, training_loss = sess.run([optimizer, total_loss], feed_dict=\n         {X: training_X_batch, y: training_y_batch})\n        if iter % 10 == 0:\n            print(\"Loss:\", training_loss)\n            saver.save(sess, 'ckpt/model', global_step=iter)\n```", "```py\n      starting_sentence = 'I plan to make the world a better place \n       because I love seeing how people grow and do in their lives '\n```", "```py\n      with tf.Session() as sess:\n          sess.run(tf.global_variables_initializer())\n          model = tf.train.latest_checkpoint('ckpt')\n          saver = tf.train.Saver()\n          saver.restore(sess, model)\n```", "```py\n      generated_text = starting_sentence\n      words_in_starting_sentence = starting_sentence.split()\n      test_X = np.zeros((1, section_length, \n      most_common_words_length))\n\n      for index, word in enumerate(words_in_starting_sentence[:-1]):\n          if index < section_length:\n              test_X[0, index, word2id[word]] = 1\n```", "```py\n        _ = sess.run(prediction, feed_dict={X: test_X})\n\n        test_last_X = np.zeros((1, 1, most_common_words_length))\n        test_last_X[0, 0, word2id[words_in_starting_sentence[-1]]] = 1\n        test_next_X = np.reshape(np.concatenate((test_X[0, 1:], \n        test_last_X[0])), (1, section_length, most_common_words_length)\n```", "```py\n         for i in range(1000):\n             test_prediction = prediction.eval({X: test_next_X})[0]\n             next_word_one_hot = prediction_to_one_hot(test_prediction)\n             next_word = id2word[np.argmax(next_word_one_hot)]\n             generated_text += next_word + \" \"\n             test_next_X = \n              np.reshape(np.concatenate((test_next_X[0,1:],\n                                np.reshape(next_word_one_hot, (1, \n                                most_common_words_length)))),\n                                (1, section_length,   \n                                 most_common_words_length))\n                 print(\"Generated text: \", generated_text)\n```", "```py\n    def prediction_to_one_hot(prediction):\n        zero_array = np.zeros(np.shape(prediction))\n        zero_array[np.argmax(prediction)] = 1\n        return zero_array\n```"]