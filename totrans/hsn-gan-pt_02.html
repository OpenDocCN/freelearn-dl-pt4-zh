<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generative Adversarial Networks Fundamentals</h1>
                </header>
            
            <article>
                
<p><strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) have brought about a revolutionary storm in the <strong>machine learning</strong> <span>(<strong>ML</strong>) </span>community. They, to some extent, have changed the way people solve practical problems in <strong>Computer Vision</strong> (<strong>CV</strong>) and <strong>Natural Language Processing</strong> (<strong>NLP</strong>). Before we dive right into the storm, let's prepare you with the fundamental insights of GANs.</p>
<p>In this chapter, you will understand the idea behind adversarial learning and the basic components of a GAN model. You will also get a brief understanding on how GANs work and how it can be built with NumPy. </p>
<p>Before we start exploiting the new features in PyTorch, we will first learn to build a simple GAN with NumPy to generate sine signals so that you may have a profound understanding of the mechanism beneath GANs. By the end of this chapter, you may relax a little as we walk you through many showcases about how GANs are used to address practical problems in CV and NLP fields.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Fundamentals of machine learning</li>
<li>Generator and discriminator networks</li>
<li>What GAN we do?</li>
<li>References and a useful reading list</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fundamentals of machine learning</h1>
                </header>
            
            <article>
                
<p>To introduce how GANs work, let's use an analogy:</p>
<div class="packt_quote"><span>A long, long time ago, there were two neighboring kingdoms on an island. One was called Netland, and the other was called </span>Ganland<span>. Both kingdoms produced fine wine, armor, </span>and<span> weapons. In Netland, the king demanded that the blacksmiths who specialized in making armor worked at the east corner of the castle, while those who made swords worked at the west side so that the lords and knights could choose the best equipment the kingdom had to offer. The king of </span>Ganland<span>, on the other hand, put all of the blacksmiths in the same corner and demanded that the armor makers and sword makers should test their work against each other every day. If a sword broke through the armor, the sword would sell at a good price and the armor would be melted and reforged. If it didn't, the sword would be remade and men would strive to buy the armor. One day, the two kings were arguing over which kingdom made better wine until the quarrel escalated into war. Though outnumbered, the soldiers of </span>Ganland<span> wore the armor and swords that had been improved for years in the daily adversarial tests, and the Netland soldiers could not break their strong armor nor withstand their sharp swords. In the end, the defeated king of Netland, however reluctant he was, agreed that </span>Ganland<span> had better wine and blacksmiths.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning – classification and generation</h1>
                </header>
            
            <article>
                
<p><span><strong>ML</strong> is the study of recognizing patterns from data without hardcoded rules given by </span>humans<span>. The recognizing of patterns (<strong>Pattern Recognition</strong> or <strong>PR</strong>) is the automatic discovering of the similarities and differences among raw data, which is an essential way to realize <strong>Artificial Intelligence</strong> (<strong>AI</strong>) that only exists in novels and movies. Although it is hard to tell when exactly real AI will come to birth in the future, the development of ML has given us much confidence in recent years. ML has already been vastly used in many fields, such as CV, NLP, recommendation systems, <strong>Intelligent Transportation Systems</strong> (<strong>ITS</strong>), medical diagnoses, robotics, and advertising</span>.</p>
<p><span>A ML model is typically described as a system that takes in data and gives certain outputs based on the parameters it contains. The <strong>learning</strong> of the model is actually adjusting the parameters to get better outputs. As illustrated in the following diagram, we feed training data into the model and get a certain output. We then use one or several criteria to measure the output, to tell how well our model performs. In this step, a set of desired outputs (or ground truth) with respect to the training data would be very helpful. If ground truth data is used in training, this process is often called <strong>supervised learning</strong>. If not, it is often regarded as <strong>unsupervised learning</strong>.</span></p>
<p class="mce-root"/>
<p><span>We constantly adjust the model's parameters based on its performance (in other words, whether it gives us the results we want) so that it yields better results in the future. This process is called <strong>model training</strong>. The training of a model takes as long as it pleases us. Typically, we stop the training after a certain number of iterations or when the performance is good enough. When the training process has finished, we apply the trained model to predict on new data (testing data). This process is called <strong>model testing</strong>. Sometimes, people use different sets of data for training and testing to see how well the model performs on samples it never meets, which is called the <strong>generalization</strong> capability. Sometimes an additional step called <strong>model</strong> <strong>evaluation </strong>is involved, when the parameters of the model are so complicated that we need another set of data to see whether our model or training process has been designed well:</span></p>
<div>
<p class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/8d437726-177b-4779-8b14-90d062d9a810.png" style="width:30.92em;height:18.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A typical machine learning system, with model training and testing</div>
<p class="CDPAlignLeft CDPAlign"><span>What types of problems</span> <span>this</span><span> </span><span>model </span><span>can </span><span>solve is essentially determined by the types of input and output</span><span> </span><span>data we want.</span><span> For example, a classification model takes an input of any number of dimensions (audio, text, image, or video) and gives a 1-dimension output (single values indicating the predicted labels). A generative model typically takes a </span><span>1-dimension input (a latent vector) and generates high-dimension outputs (images, videos, or 3D models). It maps low-dimensional data to high-dimensional data, at the same time, trying to make the output samples look as convincing as possible. However, it is worth pointing out that we'll meet generative models that don't obey this rule in the future chapters. Until <a href="685b2621-6dbb-4157-a258-f3cf2825728c.xhtml"/><a href="685b2621-6dbb-4157-a258-f3cf2825728c.xhtml">Chapter 5</a>, <em>Generating Images Based on Label Information</em>, it's a simple rule to bear in mind.</span></p>
</div>
<div class="packt_infobox">When it comes to <span>AI, there are two groups of believers in the community. The symbolists acknowledge the necessity of human experience and knowledge. They believe the low-level patterns constitute high-level decisions based on explicit rules given by humans. The connectionists believe that AI can be realized by an analogous network similar to human neural systems and adjusting the connections between simple neurons is the key to this system. Apparently, the exploding development of deep learning adds a score to the connectionists' side. What is your opinion?</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing adversarial learning</h1>
                </header>
            
            <article>
                
<div>
<p>Traditionally, generative problems are solved by statistics-based methods such as a <strong>Boltzmann machine</strong>, <strong>Markov chain</strong>, or <strong>variational encoder</strong>. As mathematically profound as they are, the generated samples are as of yet far from perfect. <span>A classification model maps high-dimension data to low-dimension, while a generative model often maps low-dimension data to high-dimension ones. People in both fields have been working hard to improve their models. Let's look back to the little made-up opening story. Can we get the two different models to work against each other and improve themselves at the same time? If we take the output of a generative model as the input of the classification model, we can measure the performance of the generative model (the armor) with the classification model (the sword). At the same time, we can improve the classification model (the sword) by feeding generated samples (the armor) along with real samples, since we can agree that more data is often better for the training of ML models.</span></p>
</div>
<p>The training process where the two models try to weaken each other and, as a result, improve each other is called <strong>adversarial learning</strong>. <span>As demonstrated in the following diagram, the models, <span class="packt_screen">A</span> and <span class="packt_screen">B</span>, have totally opposite agendas (for example, classification and generation). However, during each step of the training, the output of <span class="packt_screen">Model A</span> improves <span class="packt_screen">Model B</span>, and the output of <span class="packt_screen">Model B</span> improves <span class="packt_screen">Model A</span>:</span></p>
<div>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-596 image-border" src="assets/d722cae9-5f24-4757-a699-30cda072c9e3.png" style="width:28.58em;height:15.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A typical adversarial learning system</div>
</div>
<p><strong>GANs</strong> are designed based on this very idea, which was proposed by Goodfellow, Pouget-Abadie, Mirza, et al in 2014. Now, GANs have become the most thriving and popular method to synthesize audio, text, images, video, and 3D models in the ML community. In this book, we will walk you through the basic components and mechanisms of different types of GANs and learn how to use them to address various practical problems. In the next section, we will introduce the basic <span>structure of </span>GANs to show you how and why they work so well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator and discriminator networks</h1>
                </header>
            
            <article>
                
<p>Here, we will show you the basic components of GANs and explain how they work with/against each other to achieve our goal to generate realistic samples. A typical structure of a GAN is shown in the following diagram. It contains two different networks: a generator network and a discriminator network. The <strong>generator</strong> network typically takes random noises as input and generates fake samples. Our goal is to let the fake samples be as close to the real samples as possible. That's where the discriminator comes in. The <strong>discriminator</strong> is, in fact, a classification network, whose job is to tell whether a given sample is fake or real. The generator tries its best to trick and confuse the discriminator to make the wrong decision, while the discriminator tries its best to distinguish the fake samples from the real ones.</p>
<p class="mce-root"/>
<p>In this process, the differences between fake and real samples are used to improve the generator. Therefore, the generator gets better at generating realistic-looking samples while the discriminator gets better at picking them out. Since real samples are used to train the <span>discriminator, the training process is therefore supervised. Even though the generator always gives fake samples without the knowledge of ground truth, the overall training of GAN is still <strong>supervised</strong>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-597 image-border" src="assets/e656f056-7176-4841-8ab2-84e6d1b0207e.png" style="width:42.08em;height:23.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Basic process of a GAN</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical background of GANs</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the math behind this process to get a better understanding of the mechanism. Let <img class="fm-editor-equation" src="assets/e4fdcaef-f6c7-4251-a263-378e85581a51.png" style="width:0.92em;height:1.08em;"/> and <img class="fm-editor-equation" src="assets/a0f5ff27-298f-4194-b834-47e74a4de2b5.png" style="width:1.08em;height:1.17em;"/><span> </span><span>represent the generator and discriminator networks, respectively. Let <img class="fm-editor-equation" src="assets/5358da90-7104-466b-be88-de95b9cb655c.png" style="width:1.00em;height:1.17em;"/> represent the performance criterion of the system. The optimization objective is described as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7629f5ad-ff01-49bf-9ad6-352d182d819a.png" style="width:33.33em;height:2.33em;"/></p>
<p>In this equation, <img class="fm-editor-equation" src="assets/c7c4bd1a-b0be-4fb7-afb3-bde5f3740804.png" style="width:0.83em;height:0.92em;"/> is the real sample, <img class="fm-editor-equation" src="assets/84f608eb-69dc-45b5-826f-6ff15659158b.png" style="width:5.17em;height:1.33em;"/> is the generated sample, and <img class="fm-editor-equation" src="assets/a97b65f1-02ea-45fb-b1d4-5388d6d06533.png" style="width:0.92em;height:1.25em;"/> is the random noise that <img class="fm-editor-equation" src="assets/5e2f7c11-0834-46eb-9358-5696a9e2ab65.png" style="width:0.83em;height:1.00em;"/> uses to generate fake samples. <img class="fm-editor-equation" src="assets/9df998ed-dd13-4c2e-97bf-21c29d75270d.png" style="width:2.33em;height:1.25em;"/> is the expectation over <img class="fm-editor-equation" src="assets/2264cd61-a60b-4641-92ab-0ac367b897ee.png" style="width:0.67em;height:0.75em;"/>, which means the average value of any function, <img class="fm-editor-equation" src="assets/a5f718a9-f8bc-4a3f-b73b-ac07079acc1e.png" style="width:0.83em;height:1.58em;"/>, over all samples.</p>
<p>As mentioned before, the goal of the discriminator, <img class="fm-editor-equation" src="assets/c8f7d82c-acd3-44cc-b621-1349279a4cfb.png" style="width:1.00em;height:1.08em;"/>, is to maximize the prediction confidence of real samples. Therefore, <img class="fm-editor-equation" src="assets/757e3f28-fb7d-427c-b8e1-fc4ee1c135cd.png" style="width:0.92em;height:1.00em;"/> needs to be trained with <strong>gradient ascent</strong> (the <img class="fm-editor-equation" src="assets/7be72e8f-bfb8-4247-8c3b-3d61c34bbbff.png" style="width:2.75em;height:1.00em;"/> operator in the objective). The update rule for, <img class="fm-editor-equation" src="assets/ef174eed-8739-4cb1-87ee-1d477315c13e.png" style="width:0.92em;height:1.00em;"/>, is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/26319362-d37f-4fee-80bc-20ea6964f938.png" style="width:28.33em;height:3.83em;"/></p>
<p>In this formula, <img class="fm-editor-equation" src="assets/2a2c4476-f6b1-4742-823c-987aea9ca695.png" style="width:1.42em;height:1.58em;"/> is the parameter of <img class="fm-editor-equation" src="assets/4126c5f6-8a46-49d0-9773-29c51ddb4bc4.png" style="width:0.83em;height:0.92em;"/> (such as convolution kernels and weights in fully-connected layers), <img class="fm-editor-equation" src="assets/52e8bf3e-64f4-4da7-85a4-24aa6a5f0a18.png" style="width:1.25em;height:0.92em;"/> is the size of the mini batch (or batch size for short), and <img class="fm-editor-equation" src="assets/e24446fd-41d9-41c5-b767-89690d10ec97.png" style="width:0.58em;height:1.50em;"/><span> is the index of the sample in the mini-batch</span>. Here, we assume that we are using mini-batches to feed the training data, which is fairly reasonable since it's the most commonly used and empirically effective strategy. Therefore, the gradients need to be averaged over <img class="fm-editor-equation" src="assets/d9812252-2aa9-4ef4-b0b6-5ec52c08c78b.png" style="width:1.00em;height:0.75em;"/> samples.</p>
<div class="packt_infobox">There are 3 different ways to feed training data into models: (1) one sample at a time, which is often referred to as <strong>stochastic</strong> (for example, <strong>Stochastic Gradient Descent</strong> or<strong> SGD</strong>); (2) a handful of samples at a time, which is called <strong>mini</strong>-<strong>batch</strong>; and (3) all samples at one time, which is, in fact, called <strong>batch</strong>. The stochastic way introduces too much randomness so that one bad sample could jeopardize the good work of several previous training steps. The full batch requires too much memory to calculate. Therefore, we feed data to all of the models by <span>mini-batch</span> in this book, even though we might slothfully refer to it as just batch.</div>
<p>The goal of the generator network, <img class="fm-editor-equation" src="assets/99821001-761d-489e-b507-b024acb31baf.png" style="width:1.00em;height:1.17em;"/>, is to fool the discriminator, <img class="fm-editor-equation" src="assets/1d1ef16d-f059-4fa9-ad79-5f3bc703dd35.png" style="width:1.00em;height:1.08em;"/>, and let <img class="fm-editor-equation" src="assets/0f2bdefe-7641-4334-8ad1-845340116f44.png" style="width:0.92em;height:1.00em;"/> believe that the generated samples are real. Therefore, the training of <img class="fm-editor-equation" src="assets/b23e0a77-9872-4650-b680-cc43b69a698f.png" style="width:0.92em;height:1.08em;"/> is to maximize <img class="fm-editor-equation" src="assets/3e009fdd-496e-490d-96ec-b4e9dd4f056e.png" style="width:3.75em;height:1.17em;"/> or minimize <img class="fm-editor-equation" src="assets/87430d00-7be9-43a2-9378-602ed797e643.png" style="width:6.83em;height:1.42em;"/>. Therefore, <img class="fm-editor-equation" src="assets/71036e04-30c2-49b9-be85-0064aae009be.png" style="width:0.83em;height:1.00em;"/> needs to be trained with <strong>gradient descent</strong> <span>(the <img class="fm-editor-equation" src="assets/32d44fa3-3ea0-44ce-9054-56489723c54d.png" style="width:2.92em;height:1.50em;"/></span><span> operator in the objective). The update rule for <img class="fm-editor-equation" src="assets/251e6d73-6aad-4e01-8d2b-fa4f4e755424.png" style="width:1.00em;height:1.17em;"/> is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/33b9dec3-6746-48c0-8fae-5996ceeedb2a.png" style="width:19.92em;height:3.42em;"/></p>
<p><span>In this formula,</span> <img class="fm-editor-equation" src="assets/4e9f2bc5-1dca-4d79-8165-8ea31c605113.png" style="width:1.42em;height:1.75em;"/> is<span> the parameters of <img class="fm-editor-equation" src="assets/a3e44616-38e4-4983-9b89-275d94dc1edf.png" style="width:0.92em;height:1.00em;"/></span><q><span>,</span></q><span> <img class="fm-editor-equation" src="assets/3417277d-b9e1-4f8a-bbfb-3caa4be5f150.png" style="width:1.25em;height:0.92em;"/> is the size of the mini-batch, and <img class="fm-editor-equation" src="assets/21482551-6613-41e5-aa29-1313c5155c0a.png" style="width:0.58em;height:1.50em;"/></span><span> is the index of the sample in the mini-batch.</span></p>
<p class="mce-root"/>
<div class="packt_tip">If you are unfamiliar with the concept of GD, think of it as a little boy kicking a sticky ball on bumpy terrain. The boy wants the ball to be at the bottom of the lowest pit so that he can call it a day and go home. The ball is sticky so it doesn't roll after it hits the ground, even on a slope. Therefore, where the ball will hit is determined by which direction and how hard the boy kicks it. The amount of force the boy kicks the ball with is described by the step size (or the <strong>learning rate</strong>). <span>The direction of kicking is determined by the characteristics of the terrain under his feet. An efficient choice would be the downhill direction, which is the negative gradient of the loss function with respect to the parameters. Therefore, we often use gradient descent to minimize an objective function. However, the boy is so obsessed with the ball that he only stares at the ball and refuses to look up to find the lowest pit in a wider range. Therefore, the GD method is sometimes inefficient because it takes a very long time to reach the bottom. We will introduce several tips on how to improve the efficiency of GD in <a href="8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml"/><a href="8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml"/><a href="8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml">Chapter 3</a>, <em>Best Practices for Model Design and Training</em>. The <strong>gradient ascent</strong> is the opposite of gradient descent, which is to find the highest peak.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using NumPy to train a sine signal generator</h1>
                </header>
            
            <article>
                
<p>Maybe math is even more confusing than a big chunk of code to some. Now, let's look at some code to digest the equations we've thrown at you. Here, we will use Python to implement a very simple adversarial learning example <span>to generate sine (</span>sin<span>) signals</span>.</p>
<div class="packt_infobox">In the following example, we will be only using NumPy, a powerful <span>linear algebra Python library to implement a GAN model. We will need to calculate the gradients by ourselves so that you can have an in-depth understanding of what might be happening beneath the popular deep learning toolkits such as PyTorch. Rest assured that we won't do this in future chapters because we can use the powerful computational graph provided by PyTorch to calculate the gradients for us!</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing the network architectures</h1>
                </header>
            
            <article>
                
<p>The architecture of the generator network is described in the following diagram. It takes a 1-dimension random value as input and gives a 10-dimension vector as output. It has 2 hidden layers with each containing 10 neurons. The calculation in each layer is a matrix multiplication. Therefore, the network is, in fact, a <span><strong>Multilayer Perceptron</strong> (<strong>MLP</strong>):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/df51118f-249b-4bbd-be3a-df084d96438d.png" style="width:29.75em;height:16.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Structure of the generator network</div>
<p>The architecture of the discriminator network is described in the following diagram. <span>It takes a 10-dimension vector as input and gives a 1-dimension value as output. The output is the prediction label (real or fake) of the input sample. The discriminator network is also an MLP with two hidden layers and each containing 10 neurons:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-599 image-border" src="assets/c299314f-b7d9-44f0-aa27-754e7f165ab5.png" style="width:30.42em;height:17.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Structure of the discriminator network</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining activation functions and the loss function</h1>
                </header>
            
            <article>
                
<p>We will be only using NumPy<span> (<a href="http://www.numpy.org">http://www.numpy.org</a>)</span> to calculate and train our GAN model (and optionally using Matplotlib<span> (<a href="https://matplotlib.org">https://matplotlib.org</a>)</span> to visualize the signals). If you don't already have the Python environment on your machine, please refer to <a href="4459c703-9610-43e7-9eda-496d63a45924.xhtml">Chapter 2</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with PyTorch 1.3</span></span></em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">,</span></span> to learn how to set up a working Python environment. If your Python environment is properly set up, let's move on to the actual code.</p>
<p>All of the following code can be placed in a <kbd>simple<em>.</em>py</kbd> file (such as <kbd>simple_gan.py</kbd>). Let's look at the code step by step:</p>
<ol>
<li>Import the <kbd>NumPy</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np</pre>
<ol start="2">
<li class="mce-root">Define a few constant variables that are needed in our model:</li>
</ol>
<pre style="padding-left: 60px">Z_DIM = 1<br/>G_HIDDEN = 10<br/>X_DIM = 10<br/>D_HIDDEN = 10<br/><br/>step_size_G = 0.01<br/>step_size_D = 0.01<br/>ITER_NUM = 50000<br/><br/>GRADIENT_CLIP = 0.2<br/>WEIGHT_CLIP = 0.25</pre>
<ol start="3">
<li class="mce-root">Define the real sine samples (with <kbd>numpy.sin</kbd>) that we want to estimate:</li>
</ol>
<pre style="padding-left: 60px">def get_samples(random=True):<br/>    if random:<br/>        x0 = np.random.uniform(0, 1)<br/>        freq = np.random.uniform(1.2, 1.5)<br/>        mult = np.random.uniform(0.5, 0.8)<br/>    else:<br/>        x0 = 0<br/>        freq = 0.2<br/>        mult = 1<br/>    signal = [mult * np.sin(x0+freq*i) for i in range(X_DIM)]<br/>    return np.array(signal)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span>In the previous snippet, we use a <kbd>bool</kbd> variable,</span> <kbd>random</kbd>, <span>to introduce randomness into the real samples, as real-life data has. The real samples look like this (50 samples with</span> <kbd>random=True</kbd><span>):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-600 image-border" src="assets/cb9a644e-7dcc-44bf-b223-b2d3a6945b06.png" style="width:33.00em;height:24.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The real sine samples</div>
<ol start="4">
<li>Define the activation functions and their derivatives. If you are not familiar with the concept of activation functions, just remember that their jobs are to adjust the outputs of a layer so that its next layer can have a better understanding of these output values:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>def</span><span> </span><span>ReLU</span><span>(</span><span>x</span><span>):<br/>    return np.maximum(x, 0.)<br/><br/>def dReLU(x):<br/>    return ReLU(x)<br/><br/>def LeakyReLU(x, k=0.2):<br/>    return np.where(x &gt;= 0, x, x * k)<br/><br/>def dLeakyReLU(x, k=0.2):<br/>    return np.where(x &gt;= 0, 1., k)<br/><br/>def Tanh(x):<br/>    return np.tanh(x)<br/><br/>def dTanh(x):<br/>    return 1. - Tanh(x)**2<br/><br/>def Sigmoid(x):<br/>    return 1. / (1. + np.exp(-x))<br/><br/>def dSigmoid(x):<br/>    return Sigmoid(x) * (1. - Sigmoid(x))<br/></span></pre></div>
<ol start="5">
<li>Define a <kbd>helper</kbd> function to initialize the <span>layer </span>parameters:</li>
</ol>
<pre style="padding-left: 60px">def weight_initializer(in_channels, out_channels):<br/>    scale = np.sqrt(2. / (in_channels + out_channels))<br/>    return np.random.uniform(-scale, scale, (in_channels, out_channels))</pre>
<ol start="6">
<li>Define the <kbd>loss</kbd> function (both forward and backward):</li>
</ol>
<pre style="padding-left: 60px">class LossFunc(object):<br/>    def __init__(self):<br/>        self.logit = None<br/>        self.label = None<br/><br/>    def forward(self, logit, label):<br/>        if logit[0, 0] &lt; 1e-7:<br/>            logit[0, 0] = 1e-7<br/>        if 1. - logit[0, 0] &lt; 1e-7:<br/>            logit[0, 0] = 1. - 1e-7<br/>        self.logit = logit<br/>        self.label = label<br/>        return - (label * np.log(logit) + (1-label) * np.log(1-logit))<br/><br/>    def backward(self):<br/>        return (1-self.label) / (1-self.logit) - self.label / self.logit</pre>
<p>This is called <span><strong>binary cross-entropy</strong>, which is typically used in binary classification problems (in which a sample either belongs to class A or class B). Sometimes, one of the networks is trained too well so that the </span><kbd>sigmoid</kbd> <span>output of the discriminator might be either too close to 0 or 1. Both of the scenarios lead to numerical errors of the </span><kbd>log</kbd><span> function. Therefore, we need to restrain the maximum and minimum values of the output value.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working on forward pass and backpropagation</h1>
                </header>
            
            <article>
                
<p>Now, let's create our generator and discriminator networks. We put the code in the same <kbd>simple_gan.py</kbd> file as well:</p>
<ol>
<li>Define the parameters of the generator network:</li>
</ol>
<pre style="padding-left: 60px">class Generator(object):<br/>    def __init__(self):<br/>        self.z = None<br/>        self.w1 = weight_initializer(Z_DIM, G_HIDDEN)<br/>        self.b1 = weight_initializer(1, G_HIDDEN)<br/>        self.x1 = None<br/>        self.w2 = weight_initializer(G_HIDDEN, G_HIDDEN)<br/>        self.b2 = weight_initializer(1, G_HIDDEN)<br/>        self.x2 = None<br/>        self.w3 = weight_initializer(G_HIDDEN, X_DIM)<br/>        self.b3 = weight_initializer(1, X_DIM)<br/>        self.x3 = None<br/>        self.x = None</pre>
<p style="padding-left: 60px">We keep track of the inputs and outputs of all the layers because we need them to calculate the derivatives to update the parameters later.</p>
<ol start="2">
<li>Define the forward calculation (to generate samples based on random noise):</li>
</ol>
<pre>    def forward(self, inputs):<br/>        self.z = inputs.reshape(1, Z_DIM)<br/>        self.x1 = np.matmul(self.z, self.w1) + self.b1<br/>        self.x1 = ReLU(self.x1)<br/>        self.x2 = np.matmul(self.x1, self.w2) + self.b2<br/>        self.x2 = ReLU(self.x2)<br/>        self.x3 = np.matmul(self.x2, self.w3) + self.b3<br/>        self.x = Tanh(self.x3)<br/>        return self.x</pre>
<p style="padding-left: 60px">It's basically the same calculation process repeated 3 times. Each layer calculates its output according to this formula:</p>
<p style="padding-left: 150px"><img src="assets/5b6b6174-39a7-4f27-be1c-2bda2ab5e28d.png" style="width:16.83em;height:1.75em;"/></p>
<p style="padding-left: 60px">In this equation, <img class="fm-editor-equation" src="assets/5daabc50-fcc2-47c3-8f4f-fee2d1f48ec9.png" style="width:0.92em;height:0.92em;"/> represents the output value of a layer, <q>f</q> represents the activation function, and subscript <q>l</q> represents the index of the layer. Here, we use <kbd>ReLU</kbd> in the hidden layers and <kbd>Tanh</kbd> in the output layer.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">Now it's time to define the backward calculation for the generator network (to calculate the derivatives and update the parameters). This part of the code is a bit long. It's really repeating the same process 3 times:</p>
<ol>
<li style="list-style-type: none">
<ol>
<li>Calculate the derivatives of loss with respect to the output of this layer (for example, the derivative with respect to <kbd>output</kbd> or <kbd>x2</kbd>).</li>
<li>Calculate the derivatives of loss with respect to the parameters (for example, the derivative with respect to <kbd>w3</kbd> or <kbd>b3</kbd>).</li>
<li>Update the parameters with the derivatives.</li>
<li>Passing the gradients to the preceding layer. The <span>derivatives are calculated as follows:</span></li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1c4e523a-2f8a-4000-8d35-8e079df29fd3.png" style="width:5.92em;height:3.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/04542316-3e43-4141-944c-723a441a651e.png" style="width:22.92em;height:3.08em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/73bb6f13-fc16-4422-98ad-a4b39b10b11a.png" style="width:15.08em;height:1.67em;"/></p>
<p style="padding-left: 120px"><span>In this process, the derivative of the loss with respect to the output, which is denoted by</span> <kbd>delta</kbd> <span>in the code, is the key to propagate the gradients from layer <q>l+1</q> to layer <q>l</q>. Therefore, this process is called <strong>backpropagation</strong>. The propagation from layer <q>l+1</q> to layer <q>l</q> is described as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/75e53cd1-5243-4db0-82ed-aad2eba959fa.png" style="width:21.75em;height:3.42em;"/></p>
<ol start="3">
<li>Calculate the derivatives with respect to the output:</li>
</ol>
<pre>    def backward(self, outputs):<br/>        # Derivative with respect to output<br/>        delta = outputs<br/>        delta *= dTanh(self.x)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Calculate the derivatives with respect to the parameters in the third layer:</p>
<pre>        # Derivative with respect to w3<br/>        d_w3 = np.matmul(np.transpose(self.x2), delta)<br/>        # Derivative with respect to b3<br/>        d_b3 = delta.copy()</pre>
<p style="padding-left: 60px">Pass the gradients to the second layer:</p>
<pre>        # Derivative with respect to x2<br/>        delta = np.matmul(delta, np.transpose(self.w3))</pre>
<p style="padding-left: 60px">And update the parameters of the third layer:</p>
<pre>        # Update w3<br/>        if (np.linalg.norm(d_w3) &gt; GRADIENT_CLIP):<br/>            d_w3 = GRADIENT_CLIP / np.linalg.norm(d_w3) * d_w3<br/>        self.w3 -= step_size_G * d_w3<br/>        self.w3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP,  <br/>         self.w3))<br/><br/>        # Update b3<br/>        self.b3 -= step_size_G * d_b3<br/>        self.b3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP,  <br/>         self.b3))<br/>        delta *= dReLU(self.x2)</pre>
<ol start="4">
<li>Update the parameters in the second layer and pass the gradients to the first layer:</li>
</ol>
<pre>        # Derivative with respect to w2<br/>        d_w2 = np.matmul(np.transpose(self.x1), delta)<br/>        # Derivative with respect to b2<br/>        d_b2 = delta.copy()<br/><br/>        # Derivative with respect to x1<br/>        delta = np.matmul(delta, np.transpose(self.w2))<br/><br/>        # Update w2<br/>        if (np.linalg.norm(d_w2) &gt; GRADIENT_CLIP):<br/>            d_w2 = GRADIENT_CLIP / np.linalg.norm(d_w2) * d_w2<br/>        self.w2 -= step_size_G * d_w2<br/>        self.w2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>          self.w2))<br/><br/>        # Update b2<br/>        self.b2 -= step_size_G * d_b2<br/>        self.b2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>          self.b2))<br/>        delta *= dReLU(self.x1)</pre>
<ol start="5">
<li>Update the parameters in the first layer:</li>
</ol>
<pre>        # Derivative with respect to w1<br/>        d_w1 = np.matmul(np.transpose(self.z), delta)<br/>        # Derivative with respect to b1<br/>        d_b1 = delta.copy()<br/><br/>        # No need to calculate derivative with respect to z<br/>        # Update w1<br/>        if (np.linalg.norm(d_w1) &gt; GRADIENT_CLIP):<br/>            d_w1 = GRADIENT_CLIP / np.linalg.norm(d_w1) * d_w1<br/>        self.w1 -= step_size_G * d_w1<br/>        self.w1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>          self.w1))<br/><br/>        # Update b1<br/>        self.b1 -= step_size_G * d_b1<br/>        self.b1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>          self.b1))</pre>
<p>You will notice that the following code looks similar to the preceding code. It is only mentioned here to point out that these lines help to keep the data from becoming unstable. You don't have to add these three lines:</p>
<pre>if (np.linalg.norm(d_w3) &gt; GRADIENT_CLIP):<br/>    d_w3 = GRADIENT_CLIP / np.linalg.norm(d_w3) * d_w3<br/>self.w3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, self.w3))</pre>
<p>This code is included because the training of GANs can be very unstable and we need to clip the gradients and the parameters to ensure a stable training process. </p>
<div class="packt_infobox">We will elaborate on the topics of activation functions, loss functions, weight initialization, gradient clipping, weight clipping, and more in <a href="8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml"/><a href="8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml">Chapter 3</a>, <em>Best Practices for Model Design and Training.</em> These are extremely useful for stabilizing and improving the training of GANs.</div>
<p>Now, let's define the discriminator network:</p>
<pre>class Discriminator(object):<br/>    def __init__(self):<br/>        self.x = None<br/>        self.w1 = weight_initializer(X_DIM, D_HIDDEN)<br/>        self.b1 = weight_initializer(1, D_HIDDEN)<br/>        self.y1 = None<br/>        self.w2 = weight_initializer(D_HIDDEN, D_HIDDEN)<br/>        self.b2 = weight_initializer(1, D_HIDDEN)<br/>        self.y2 = None<br/>        self.w3 = weight_initializer(D_HIDDEN, 1)<br/>        self.b3 = weight_initializer(1, 1)<br/>        self.y3 = None<br/>        self.y = None</pre>
<p>And now define its forward calculation (to predict the label based on the input sample):</p>
<pre>    def forward(self, inputs):<br/>        self.x = inputs.reshape(1, X_DIM)<br/>        self.y1 = np.matmul(self.x, self.w1) + self.b1<br/>        self.y1 = LeakyReLU(self.y1)<br/>        self.y2 = np.matmul(self.y1, self.w2) + self.b2<br/>        self.y2 = LeakyReLU(self.y2)<br/>        self.y3 = np.matmul(self.y2, self.w3) + self.b3<br/>        self.y = Sigmoid(self.y3)<br/>        return self.y</pre>
<p>Here, we use LeakyReLU as the activation function for hidden layers and sigmoid for the output layer. Now, let's define the backward calculation for the discriminator network (to calculate the derivatives and update the parameters):</p>
<pre>    def backward(self, outputs, apply_grads=True):<br/>        # Derivative with respect to output<br/>        delta = outputs<br/>        delta *= dSigmoid(self.y)<br/>        # Derivative with respect to w3<br/>        d_w3 = np.matmul(np.transpose(self.y2), delta)<br/>        # Derivative with respect to b3<br/>        d_b3 = delta.copy()<br/>        # Derivative with respect to y2<br/>        delta = np.matmul(delta, np.transpose(self.w3))<br/>        if apply_grads:<br/>            # Update w3<br/>            if np.linalg.norm(d_w3) &gt; GRADIENT_CLIP:<br/>                d_w3 = GRADIENT_CLIP / np.linalg.norm(d_w3) * d_w3<br/>            self.w3 += step_size_D * d_w3<br/>            self.w3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP,  <br/>              self.w3))<br/>            # Update b3<br/>            self.b3 += step_size_D * d_b3<br/>            self.b3 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP,  <br/>              self.b3))<br/>        delta *= dLeakyReLU(self.y2)<br/>        # Derivative with respect to w2<br/>        d_w2 = np.matmul(np.transpose(self.y1), delta)<br/>        # Derivative with respect to b2<br/>        d_b2 = delta.copy()<br/>        # Derivative with respect to y1<br/>        delta = np.matmul(delta, np.transpose(self.w2))<br/>        if apply_grads:<br/>            # Update w2<br/>            if np.linalg.norm(d_w2) &gt; GRADIENT_CLIP:<br/>                d_w2 = GRADIENT_CLIP / np.linalg.norm(d_w2) * d_w2<br/>            self.w2 += step_size_D * d_w2<br/>            self.w2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>              self.w2))<br/>            # Update b2<br/>            self.b2 += step_size_D * d_b2<br/>            self.b2 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>              self.b2))<br/>        delta *= dLeakyReLU(self.y1)<br/>        # Derivative with respect to w1<br/>        d_w1 = np.matmul(np.transpose(self.x), delta)<br/>        # Derivative with respect to b1<br/>        d_b1 = delta.copy()<br/>        # Derivative with respect to x<br/>        delta = np.matmul(delta, np.transpose(self.w1))<br/>        # Update w1<br/>        if apply_grads:<br/>            if np.linalg.norm(d_w1) &gt; GRADIENT_CLIP:<br/>                d_w1 = GRADIENT_CLIP / np.linalg.norm(d_w1) * d_w1<br/>            self.w1 += step_size_D * d_w1<br/>            self.w1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>              self.w1))<br/>            # Update b1<br/>            self.b1 += step_size_D * d_b1<br/>            self.b1 = np.maximum(-WEIGHT_CLIP, np.minimum(WEIGHT_CLIP, <br/>              self.b1))<br/>        return delta</pre>
<p>Please note that the main difference in the backward calculation of the discriminator is that it's trained with gradient ascent. Therefore, to update its parameters, we need to add the gradients. So, in the preceding code, you will see lines like this that take care of it for us:</p>
<pre>self.w3 += step_size_D * d_w3</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training our GAN model</h1>
                </header>
            
            <article>
                
<p>Now that all the necessary components are defined, we can begin the training of our GAN model:</p>
<pre>G = Generator()<br/>D = Discriminator()<br/>criterion = LossFunc()<br/><br/>real_label = 1<br/>fake_label = 0<br/><br/>for itr in range(ITER_NUM):<br/>    # Update D with real data<br/>    x_real = get_samples(True)<br/>    y_real = D.forward(x_real)<br/>    loss_D_r = criterion.forward(y_real, real_label)<br/>    d_loss_D = criterion.backward()<br/>    D.backward(d_loss_D)<br/><br/>    # Update D with fake data<br/>    z_noise = np.random.randn(Z_DIM)<br/>    x_fake = G.forward(z_noise)<br/>    y_fake = D.forward(x_fake)<br/>    loss_D_f = criterion.forward(y_fake, fake_label)<br/>    d_loss_D = criterion.backward()<br/>    D.backward(d_loss_D)<br/><br/>    # Update G with fake data<br/>    y_fake_r = D.forward(x_fake)<br/>    loss_G = criterion.forward(y_fake_r, real_label)<br/>    d_loss_G = D.backward(loss_G, apply_grads=False)<br/>    G.backward(d_loss_G)<br/>    loss_D = loss_D_r + loss_D_f<br/>    if itr % 100 == 0:<br/>        print('{} {} {}'.format(loss_D_r.item((0, 0)), loss_D_f.item((0, 0)), loss_G.item((0, 0))))</pre>
<p>As you can see from the preceding code, the training of the GAN model mainly has 3 steps:</p>
<ol>
<li>Train the discriminator with real data (and recognize it as real).</li>
<li>Train the discriminator with fake data (and recognize it as fake).</li>
<li>Train the generator with fake data (and recognize it as real).</li>
</ol>
<p class="mce-root"/>
<p>The first two steps teach the <span>discriminator how to tell the difference between real and fake data. The third step teaches the generator how to fool the discriminator by generating fake data that is similar to real data.</span> <span>This is the core idea of adversarial learning and the reason why GANs can generate relatively realistic audio, text, images, and videos.</span></p>
<div class="packt_infobox">Here, we use SGD to train the model for 50,000 iterations. If you are interested, feel free to implement a mini-batch GD to see whether it produces better results in a shorter time. You are also welcome to change the network architectures (for example, the number of layers, the number of neurons in each layer, and the data dimension, <kbd>X_DIM</kbd>) to see how results change with the hyperparameters.</div>
<p>Finally, let's use Matplotlib to visualize the generated samples:</p>
<pre>import matplotlib.pyplot as plt<br/>x_axis = np.linspace(0, 10, 10)<br/>for i in range(50):<br/>    z_noise = np.random.randn(Z_DIM)<br/>    x_fake = G.forward(z_noise)<br/>    plt.plot(x_axis, x_fake.reshape(X_DIM))<br/>plt.ylim((-1, 1))<br/>plt.show()</pre>
<p><span>It may take a few seconds to finish the training, depending on how powerful your CPU is. When the training is finished, the samples generated by the generator network</span> may look like this (50 samples):</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-601 image-border" src="assets/d3c5a92f-6bb4-4362-adc7-aa78d6eb8184.png" style="width:27.50em;height:20.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The generated sine samples</div>
<p>Pretty convincing, right? It's amazing to see how it captures the peaks and valleys of the original sine waves. Imagine what GANs are capable of with much more complex structures!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What GAN we do?</h1>
                </header>
            
            <article>
                
<p>GANs can do a lot more than generating sine signals. We can apply GANs to address many different practical problems by altering the input and output dimensions of the generator and combining them with other methods. For example, we can generate text and audio (1-dimension), images (2-dimension), video, and 3D models (3-dimension) based on random input. If we keep the same dimension of input and output, we can perform denoising and translation on these types of data. We can feed real data into the generator and let it output data with larger dimensions, for example, image super-resolution. We can also feed one type of data and let it give another type of data, for example, generate audio based on text, generate images based on text, and so on.</p>
<p>Even though it has only been 4 years since GANs first came out (at the time of writing), people have kept working on improving GANs and new GAN models are coming out almost weekly. If you take a look at <a href="https://github.com/hindupuravinash/the-gan-zoo">https://github.com/hindupuravinash/the-gan-zoo</a>, you can see that there have been at least 500 different GAN models. It's nearly impossible for us to learn and evaluate each one of them. You'll be amazed to see that it is actually common to find several models sharing the same name! Therefore, in this book, we won't even try to introduce you to most of the GAN models <span>out there. We will, however, help you to get familiar with the most typical GAN models in different applications and learn how to use them to address practical problems.</span></p>
<p><span>We will also introduce you to some useful tricks and techniques to improve the performance of GANs. We hope that, by the time you finish this book, you have a wide yet in-depth understanding of the mechanisms of various GAN models so that you will feel confident to design your own GANs to creatively solve the problems you may encounter in the future.</span></p>
<p>Let's take a look at what GANs are capable of and what their advantages are compared to traditional approaches <span>in these fields: image processing, NLP, and 3D modeling.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image processing</h1>
                </header>
            
            <article>
                
<p>In the field of image processing, GANs are applied to many applications, including image synthesis, image translation, and image restoration. These topics are the most common in the study and application of GANs and make up most of the content in this book. Images are one of the easiest to show and spread media form on the internet; therefore, any latest breakthrough in the image-wise application of GANs would receive overwhelming attention in the deep learning community.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image synthesis</h1>
                </header>
            
            <article>
                
<p>Image synthesis is, in short, the creation of new images. Early in 2015, <strong>DCGANs</strong> (<strong><span>Deep Convolutional Generative Adversarial </span>Networks</strong>) came out. It was one of the first well-performing and stable approaches to address the hard-to-train issues presented in earlier GAN models. It generates 64 x 64 images based on a random vector with a length of 100. Some images generated by DCGANs are shown in the following screenshot. You may notice that some of the images are far from being realistic because of the blocky appearance of the pixels. In the paper by <span>Radford, Metz, and Chintala (2015), they present many interesting and inspiring visual experiments and reveal even more potential of GANs. </span>We will talk about the architecture and training procedure of DCGANs later in <a href="3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml">Chapter 4</a>, <em>Building Your First GAN with PyTorch</em>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/69a09d37-57b3-4a02-aa33-b0a2560dee60.png" style="width:21.00em;height:21.00em;"/><img src="assets/d68c77e3-9e2c-498f-942a-86d8a822b817.png" style="width:21.08em;height:21.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Images <span>generated</span> by a DCGAN (left: human faces; right: bedroom)</div>
<p>Now, GANs perform extraordinarily in image synthesis. Take BigGAN, for example. It was proposed in a paper submitted to ICLR <span>2019 </span>(<em>7<sup>th</sup> International Conference on Learning Representations</em>) by Brock, Donahue, and Simonyan. It received a lot of attention on social media even during the open review process. It's capable of generating images as large as 512 x 512 with high quality. </p>
<p>In future chapters, we will also take a look at GAN models that look further into attributes of images, rather than just class conditions. We will talk about Conditional GANs, which allow you to generate images interactively, and Age-cGAN, which generates human faces of any age of your desire. We will also look into how to use GANs to generate adversarial examples that even the best classifiers cannot correctly recognize in <a href="f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml">Chapter 8</a>, <em>Training Your GANs to Break Different Models</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image translation</h1>
                </header>
            
            <article>
                
<p class="mce-root">If we describe image synthesis as a process where we expect the outputs to be 2-dimension images when feeding 1-dimension vector into the models (again, note that there are exceptions since you can generate images based on other types of data if you want), image translation (more precisely, image-to-image translation) would be the process where we feed 2-dimension images into models that also give 2-dimension data as outputs. A lot of interesting things can be done with image translation. For example, pix2pix (<span>Isola, Zhu, Zhou, et al, 2016</span>) transforms label maps into images, including turning edge sketches into colorized images, generating street view photos based on <span>semantic segmentation information</span>, transferring image styles, and so on. We will get to an upgraded version of pix2pix, pix2pixHD, in <a href="209b2357-05d7-48d4-9c91-e061eccf8344.xhtml">Chapter 6</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Image-to-Image Translation and Its Applications</span></span></em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">,</span></span> along with other image-to-image translation methods such as CycleGAN and DiscoGAN.</p>
<p>Image-to-image translation can be used in other computer vision applications and address more traditional problems, such as image restoration, image in-painting, and super-resolution. Image restoration is one of the most important research areas in computer vision. Mathematicians and computer scientists have been trying to figure out how to remove the annoying noises off photos or reveal more information out of blur images for decades. Traditionally, these problems are solved by iterative numerical calculations, which often require profound mathematical backgrounds to master. Now, with GANs at hand, these problems can be solved by image-to-image translation. For example, SRGAN (<span>Ledig, Theis, Huszar, et al, 2016)</span> can upscale images to 4X of size with high quality, which we will talk about in detail in <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, <em>Image Restoration with GANs</em>. <span>Yes, Chen, Lim, et al</span> <span>(2016) proposed using a DCGAN-like model to address human face inpainting problems. More recently, Yu, Lin, Yang, et al (2018) designed a GAN model that fills in an arbitrary shape of blank holes in images and the generated pixels are quite convincing as well.</span></p>
<p class="mce-root"/>
<p>Text-to-image translation is also a good application of GANs, in which new images are generated based on the description text. Reed, Akata, Yan, et al (2016) came up with a procedure that extracts distinguish features from detailed description text and uses the information to generate flower or bird images that match perfectly to the description. Months later, <span>Zhang, Xu, Li, et al (2016) proposed StackGAN to generate 256 x 256 images with high fidelity based on description text. We will talk about text-to-image translation in<a href="464e6361-6a52-4de2-960a-4fa0576f42c7.xhtml"/> <a href="464e6361-6a52-4de2-960a-4fa0576f42c7.xhtml">Chapter 9</a>, <em>Image Generation from Description Text</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Video synthesis and translation</h1>
                </header>
            
            <article>
                
<p>A video is a sequence of images. Therefore, most of the image translation methods can be directly applied to video. However, a crucial performance criterion of video synthesis or translation is the calculation speed. For example, if we want to develop a camera application with different image styles for mobile devices, our users would certainly hope that they can see the processed results in real-time. Take video surveillance systems as another example. It is completely feasible to use GANs to denoise and enhance the video signals (provided that your clients trust your models without reservation). A fast model that processes each frame in milliseconds to keep up with the frame rate would certainly be worth considering.</p>
<p>We'd like to point out an interesting gesture transfer project called <strong><span>Everybody Dance Now</span></strong>. It extracts the movements of the dancer from a source video, then maps the same movements to the person in the target video by image-to-image translation. This way, anyone can use this model to make dancing videos of their own! </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NLP</h1>
                </header>
            
            <article>
                
<p>NLP is the study of how to use computers to process and analyze natural human languages. Other than generating images, GANs can also be used to generate sequential and time-dependent data, such as text and audio. SeqGAN, proposed by Yu, Zhang, Wang, et al (2016), is designed to generate sequential signals, like poems and music. Shortly after, <span>Mogren (2016) proposed C-RNN-GAN, which is designed to generate classical music under acoustic restraints. In 2017, Dong, Hsiao, Yang, et al designed MuseGAN to generate polyphonic music of multiple instruments, including bass, drums, guitar, piano, and strings. Feel free to visit the following web<sup>10</sup> site to enjoy the generated music!</span></p>
<p class="mce-root"/>
<p>Speech enhancement is one of the main research areas in audio signal processing. Traditionally, people use spectral subtraction, Wiener filtering, subspace approaches, and more to remove the noises in audio or speech signals. However, the performances of these methods have only been satisfactory under certain circumstances. <span>Pascual, Bonafonte, and Serrà (2017) designed SEGAN to address this problem and achieved impressive results<sup>11</sup>. We will talk about the applications of GANs in the field of NLP in <a href="e05f7dcc-b1fe-4b9b-a893-124e67718cac.xhtml">Chapter 10</a>, <em>Sequence Synthesis with GANs</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">3D modeling</h1>
                </header>
            
            <article>
                
<p>Now that we know GANs can generate 2D data based on 1D inputs, it's only natural to consider leveling this up to generate 3D data based on 1D or 2D signals with GANs. 3D-GAN (<span>Wu, Zhang, Xue, et al, 2016</span>) is designed exactly for this purpose. It learns the mapping between the latent vector and 3D models to generate 3D objects based on a 1D vector. It is also completely feasible to use GANs to predict 3D models based on 2D silhouettes. <span>Gadelha, Maji, and Wang (2016) designed PrGAN to generate 3D objects based on binary silhouette images from any viewpoint. We will discuss how to generate 3D objects with GANs in detail in <a href="09d087ce-5d7e-4bd4-af48-693ac63d891c.xhtml">Chapter 11</a>, <em>Reconstructing 3D Models with GANs</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We've covered a tremendous amount of information just in this first chapter. You've seen how GANs came about and have a basic grasp of the roles of generators and discriminators. You've even seen a few examples of some of the things that GANs can do. We've even created a GAN program using just NumPy. Not to mention we now know why Ganland has better blacksmiths and wine.</p>
<p>Next, we'll dive into the wondrous world of PyTorch, what it is, and how to install it.</p>
<p><span>The following is a list of references and other helpful links.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References and useful reading list</h1>
                </header>
            
            <article>
                
<ol>
<li><span>Goodfellow I, Pouget-Abadie J, Mirza M, et al. (2014). Generative adversarial nets. NIPS, 2672-2680.</span></li>
<li><span>Wang, J. (2017, Dec 23). <em>Symbolism vs. Connectionism: A Closing Gap in Artificial Intelligence</em>, retrieved from <a href="https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence">https://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence</a>.</span></li>
<li><span>Radford A, Metz L, Chintala S. (2015). <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>. arXiv preprint arXiv:1511.06434.</span></li>
<li>"Dev Nag". (2017, Feb 11). <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) in 50 lines of code (PyTorch), retrieved from <a href="https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f">https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f</a>.</li>
<li>Brock A, Donahue J, Simonyan K. (2018). <em>Large Scale GAN Training for High Fidelity Natural Image Synthesis</em>. <span>arXiv preprint arXiv:</span>1809.11096.</li>
<li>Isola P, Zhu J Y, Zhou T, Efros A. (2016). <em>Image-to-Image Translation with Conditional Adversarial Networks</em>. <span>arXiv preprint arXiv:</span>1611.07004.</li>
<li>Ledig C, Theis L, Huszar F, et al (2016). <em>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</em>. <span>arXiv preprint arXiv:</span>1609.04802.</li>
<li>Yeh R A, Chen C, Lim T Y, et al (2016). <em>Semantic Image Inpainting with Deep Generative Models</em>. <span>arXiv preprint arXiv:</span>1607.07539.</li>
<li>Yu J, Lin Z, Yang J, et al (2018). <em>Free-Form Image Inpainting with Gated Convolution</em>. <span>arXiv preprint arXiv:</span>1806.03589.</li>
<li>Reed S, Akata Z, Yan X, et al (2016). <em>Generative Adversarial Text to Image Synthesis</em>. <span>arXiv preprint arXiv:</span>1605.05396.</li>
<li>Zhang H, Xu T, Li H, et al (2016). <em>StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</em>. <span>arXiv preprint arXiv:</span>1612.03242.</li>
<li>Yu L, Zhang W, Wang J, et al (2016). <em>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</em>. <span>arXiv preprint arXiv:</span>1609.05473.</li>
<li>Mogren O. (2016). <em>C-RNN-GAN: Continuous recurrent neural networks with adversarial training</em>. <span>arXiv preprint arXiv:</span>1611.09904.</li>
<li>Dong H W, Hsiao W Y, Yang L C, et al (2017). <em>MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment</em>. <span>arXiv preprint arXiv:</span>1709.06298.</li>
</ol>
<p> </p>
<ol start="15">
<li>Pascual S, Bonafonte A, Serrà J. (2017). <em>SEGAN: Speech Enhancement Generative Adversarial Network</em>. <span>arXiv preprint arXiv:</span>1703.09452.</li>
<li>Wu J, Zhang C, Xue T, et al (2016). <em>Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</em>. <span>arXiv preprint arXiv:</span>1610.07584.</li>
<li>Gadelha M, Maji S, Wang R. (2016). <em>3D Shape Induction from 2D Views of Multiple Objects</em>. <span>arXiv preprint arXiv:</span>1612.05872.</li>
</ol>


            </article>

            
        </section>
    </body></html>