- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling Auto-GPT for Enterprise-Level Projects with Docker and Advanced Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B21128_02.xhtml#_idTextAnchor028)*,* we covered how to start
    Auto-GPT with Docker; now, we will dive deeper into the utilization of Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Docker has become a dynamic tool in the realm of software development, especially
    in the management and distribution of complex applications such as Auto-GPT. As
    such, this chapter aims to provide you with a comprehensive understanding of how
    Auto-GPT utilizes Docker. Using Docker will make sure we are always on the same
    page. Also, you can read about several cases, such as this GitHub issue I reported
    which has an early issue number [https://github.com/Significant-Gravitas/AutoGPT/issues/666](https://github.com/Significant-Gravitas/AutoGPT/issues/666),
    where my Auto-GPT Agent managed to break out of its own boundaries using an exploit
    it found by itself.
  prefs: []
  type: TYPE_NORMAL
- en: We will also explore the continuous mode functionality in Auto-GPT and discuss
    its implications. The power of Auto-GPT is not only in its ability to generate
    creative and coherent content but also in its capability to operate autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will also delve into one of Auto-GPT’s key features: continuous
    mode. We will explore what it is, its potential applications, and the precautions
    required when using it.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous mode in Auto-GPT allows the program to run without requiring user
    approval at every step. This means that it can generate content, perform tasks,
    and even make decisions independently. This feature is particularly useful for
    automating tasks that would otherwise require constant human intervention. However,
    as with any powerful tool, it is essential to use it responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of how Auto-GPT utilizes Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing potential loopholes or bugs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example run scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is continuous mode?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known use cases of continuous mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safeguards and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential risks and how to mitigate them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gracefully stopping a continuous process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of how AutoGPT utilizes Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker operates on the principle of containerization – a streamlined and isolated
    way to run applications securely and efficiently. Consider these containers as
    self-sufficient units housing all the components required to run an application.
    Auto-GPT’s integration with Docker empowers users to bypass the strenuous process
    of setting up environments manually, turning Auto-GPT’s focus towards customizing
    AI experiences instead.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, Docker encapsulates Auto-GPT’s computational environment. The Docker
    container for Auto-GPT includes the Python environment, the necessary libraries,
    as well as the application itself. The Dockerfile, located in the root directory
    of Auto-GPT, holds the instructions for Docker to build this image.
  prefs: []
  type: TYPE_NORMAL
- en: So, how it works is that Docker creates an isolated environment, or *container*,
    where Auto-GPT runs. Every interaction between Auto-GPT and your system goes through
    Docker, which interprets these interactions and ensures they’re safe and compatible
    with the environment within the container. Isolation also means that if anything
    goes wrong inside the container, your system remains unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: The Dockerfile for this often just contains a few commands, such as which environment
    is supposed to be loaded, which is `python:3.10-slim` on top of poetry in the
    case of Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how this integration works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Auto-GPT’s integration with Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The integration of Docker with Auto-GPT provides several advantages. The encapsulation
    ensures that each user experiences precisely the same computational environment,
    thereby reducing disparities that could arise due to different operating systems
    or Python distributions. This encapsulation also permits seamless sharing and
    easy version control, and eliminates the *It works on my* *machine* problem!
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT had a memory database with SQLite running that contained the actions
    and messages of prior runs. This was first augmented with a few vector database
    solutions at the beginning of 2023, but was completely dropped by the middle of
    2023 because some of them did not work with most users. Since then, a VectorDB
    or any memory apart from saving into a file locally has not been implemented because
    supporting too many solutions was too much work. Instead, the memory is held in
    a JSON file locally, whose position may vary in each release version of Auto-GPT.
    Using Docker was a necessary step here, although many users then (including me)
    complained that Auto-GPT was less supportive of local running instances. Fortunately,
    it changed later, as the memory system is again a very basic JSON file now, which
    is better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Having delved into why Auto-GPT supports Docker, we may now move forward to
    how to start our Auto-GPT Docker instance.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a Docker instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this may sound complex, running a Docker container is remarkably straightforward.
    Firstly, if you haven’t already, you need to make sure Docker is installed on
    your system. Several guides can assist you with this process, such as Docker’s
    official installation instructions ([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)),
    which can guide you through the process for Windows, MacOS, and various Linux
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run an instance of Auto-GPT in a Docker container, you’ll first need to
    build the Docker image using the Dockerfile. Here’s a quick rundown:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the directory that contains the Dockerfile (it should be the root
    directory of Auto-GPT): `cd path/to/Auto-GPT`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the Docker image with `docker build -t auto-gpt .`. (make sure it ends
    with a space and dot; it may not be clearly visible).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The easy way to run the instance is to execute the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you dare, you can also build and run it with vanilla Docker commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you have followed all the steps correctly, you should be greeted with Auto-GPT’s
    chat input. If not, we will go through the common ways it may not work. Also,
    you can refer to the previously mentioned Docker documentation if the functionality
    may have changed.
  prefs: []
  type: TYPE_NORMAL
- en: Although Docker is often easy to just install and go, we may still run into
    some problems.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing potential loopholes or bugs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use Docker on a PC, you have to make sure virtualization is activated.
  prefs: []
  type: TYPE_NORMAL
- en: On macOS, you do not have to change anything. Virtualization is already working
    there; you only have to download and install Docker Desktop.
  prefs: []
  type: TYPE_NORMAL
- en: As each motherboard manufacturer has different BIOS settings, the way you activate
    it may vary, but normally you would go to the BIOS by restarting your machine
    and pressing the *Setup*/*BIOS* button that is displayed. For me, it’s *F2* or
    *DEL*. If no button is displayed, you will have to do some research.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on which CPU manufacturer you have, the setting may be less obvious.
    For example, Intel has a setting named **Intel Virtualization Technology**, while
    AMD has **AMD-V**. Once you have found it, make sure to enable it and save the
    change.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to mention that working with Docker can sometimes lead to bugs
    or issues that are not apparent at first. An example of such is the notorious
    `docker system` `prune` command.
  prefs: []
  type: TYPE_NORMAL
- en: Despite Docker’s advantages, it isn’t without potential challenges. One common
    issue involves insufficient memory allocation. When you start Docker, it reserves
    a certain amount of memory which, if depleted, can cause unexpected behavior or
    crashes. To resolve this, try to increase Docker’s memory allocation in Docker’s
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: Another common issue is when Docker fails to start a container because it claims
    the selected port is already in use. Even if you’re certain it isn’t, a misbehaving
    application could be invisibly binding to that port. In this case, rebooting the
    system or manually killing the process occupying the port can help.
  prefs: []
  type: TYPE_NORMAL
- en: I personally recommend either the Docker Desktop app on Windows and Mac, or
    for Linux, I use Portainer. This is a web-based Docker manager application that
    is just as powerful as the official counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install and run it by executing these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create the volume that the Portainer Server will use to store its database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, download and install the Portainer Server container. Keep in mind that
    this should all be on one line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Docker also creates intermediate images during the build process. These images
    can take up a substantial amount of disk space. Check your Docker system by running
    `docker system df`, and clean up unneeded images with the `docker system` `prune`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is not the project’s fault; sometimes, the software we use may
    experience some issues. Docker is very commonly used, but it can also cause some
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying and fixing potential issues related to Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Utilizing Docker alongside Auto-GPT can be a rewarding experience, but like
    all tech integrations, it comes with a set of challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an in-depth guide to tackling these challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker daemon not running:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sudo systemctl start docker` to initiate the Docker service.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Applications` folder. If Docker is running correctly, you’ll see the Docker
    whale icon in your top menu bar.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Auto-GPT packages or modules missing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cd /app/`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requirements.txt` file that lists all necessary packages. Use `pip` to install
    these requirements:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python -m pip install -``r requirements.txt`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This command will ensure all listed packages in the `requirements.txt` file
    are installed within the container’s environment. If there are any additional
    modules or packages you know are missing, you can install them individually using
    `pip` as well.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exit`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Remember, Docker containers are isolated environments. Any changes you make
    inside a container (such as installing packages) won’t affect your host system.
    However, these changes will be lost if the container is removed. To make persistent
    changes, you might consider creating a new Docker image or using Docker volumes.*   Insufficient
    memory or disk space: Docker may sometimes use up too much space, or maybe you
    just do not have enough storage available. Docker may just shut down and not even
    tell you what was wrong:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker system prune -a` to remove unused data. Note: This will delete unused
    containers, networks, and images. Always backup essential data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust resources (Windows/macOS)**: Open **Docker Desktop** | **Settings**
    | **Resources** to modify memory or disk allocations.*   Networking issues: Sometimes,
    the port forwarding to and from Docker containers may be broken. For example,
    when we want to open the frontend website of our Auto-GPT instance, it may be
    that we cannot access it:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker port <container_name>`, you can check which ports are active'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p` flag, such as `docker run -p 4000:80 <image_name>`, to bind container
    ports to host ports*   Version conflicts:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sudo apt-get update && sudo apt-get upgrade docker-ce` or similar commands
    to update Docker.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows/macOS**: Docker Desktop will notify you of available updates. Ensure
    you keep it up to date.*   For Auto-GPT updates, regularly check the official
    Auto-GPT repository or documentation for the latest versions and update instructions.*   Container
    isolation:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-v` to mount host directories to the container, such as `docker run -v /``host/directory:/container/directory
    <image_name>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker network inspect <network_name>` to check the network configuration
    and ensure containers are correctly connected*   Logs and diagnostics:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker logs <container_name>` command will display the logs of your container.
    These logs are invaluable for diagnosing issues. If an error occurs, it’s likely
    detailed in these logs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, the keys to troubleshooting are patience and systematic exploration.
    With these detailed steps, you’ll be well equipped to handle most Docker-related
    challenges that come your way when working with Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the issues we experience are deeper, and for those, we need to access
    the container that Auto-GPT is running on.
  prefs: []
  type: TYPE_NORMAL
- en: To get an idea of how else you may start Auto-GPT using Docker, I have made
    a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Example run scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To familiarize yourself with run scripts, you can refer to the ones listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run an interactive shell in the Docker container (useful for debugging),
    use the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command starts an interactive shell within the Docker container, allowing
    you to directly access and debug Auto-GPT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To run Auto-GPT on a different port, try this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To forward all traffic from port 80 to Auto-GPT (this requires administrator/`sudo`
    privileges), run the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By utilizing these run scripts, you can enhance your efficiency and productivity
    when working with Auto-GPT in the Docker environment. Feel free to experiment
    with different port mappings or explore the interactive shell for debugging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, brace yourself for an exploration of the intriguing world
    of Auto-GPT’s continuous mode. We will delve deep into its mechanics, understanding
    the potential consequences and benefits it brings to the table, and as the chapters
    unfold, we will also touch upon integrating different LLM models, making the most
    of prompts, and much more. The journey with Auto-GPT is about to get even more
    exciting!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move onto continuous mode and its consequences.
  prefs: []
  type: TYPE_NORMAL
- en: What is continuous mode?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by explaining what continuous mode is. It allows Auto-GPT to run
    without requiring user approval at each step. This enables automation without
    constant human oversight.
  prefs: []
  type: TYPE_NORMAL
- en: We will then look at some known use cases and examples of continuous mode, such
    as automating research tasks, content generation, and code compilation. While
    continuous mode is powerful, note that caution should be exercised given the lack
    of human verification.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will provide tips for using continuous mode safely and effectively.
    This includes setting clearly defined goals, constraints, and limits in the configuration
    to restrict unwanted behavior. Monitoring logs and outputs is also advised.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we will discuss potential risks, such as generating falsities,
    getting stuck in loops, and cost overruns. Mitigation strategies such as kill
    switches, usage limits, and sandboxing will be suggested.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will explain how to gracefully stop a continuous process, such as
    using keyboard interrupts or the `task_complete` command. We will also explore
    active research into enhancements such as pause commands and timed takeovers.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous mode allows Auto-GPT to operate autonomously without requiring user
    approval at each step. By keeping the core loop running uninterrupted, it enables
    the automation of tasks without constant human oversight. However, caution is
    advised. When used judiciously with appropriate safeguards, continuous mode can
    boost efficiency. When deployed carelessly, it can have regrettable consequences.
    Finding the right balance requires an understanding of its capabilities and risks.
    This part of the chapter aims to provide that holistic perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Known use cases of continuous mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My own favorite use case is a personal assistant that only reports or asks when
    input is needed. While most AI chatbots currently work one step at a time, I prefer
    Auto-GPT to run continuously, and I wrote my Sophie-Plugin so that Auto-GPT only
    texts me when it either wants to report something and keeps going (it then adds
    `...` (it’s just three dots; I did not add any other symbols, as it may create
    confusion) at the end of the message), or it asks me to make a certain decision
    or provide feedback.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the assistants’ use case is broader, and you can also give it multiple
    tasks and tell it to get back to you once they are done, in combination with other
    plugins, such as email plugins, discord plugins, and SSH terminal plugins. Your
    AI is even capable of managing communication for you, which could be risky but
    is at the same time very cool. Auto-GPT once even negotiated competitive salaries
    for me for a coding project it was supposed to do as it reached out to companies
    to test the code. On a side note, that was GPT 3.5 before it was Turbo. The new
    models are not as bold or aware of what the context is anymore, which could maybe
    be achieved with alternative LLM models, which were used in my mini-autogpt project
    ([https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt)),
    where I lined up several LLM models and tested their congruence. As OpenAI seemingly
    tries to minimize the cost of running its new LLMs, it has become less and less
    sure about what they do and they are either ambivalent or repetitive. Also, they
    feel like they do not really listen to you and stick to what they said previously.
  prefs: []
  type: TYPE_NORMAL
- en: Automating research and analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most compelling applications of Auto-GPT’s continuous mode is in
    the field of research and analysis. By activating continuous mode, Auto-GPT can
    endlessly mine information from multiple sources, such as academic journals, news
    feeds, and social media platforms. Market researchers can deploy this feature
    to keep an ongoing tab on consumer sentiments, emerging trends, and competitor
    strategies. The feature is also highly beneficial for academic research, as it
    can scan newly published papers and add new insights in real time to an ongoing
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Streamlining content creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Content creators and marketing departments can significantly benefit from Auto-GPT’s
    continuous mode. The AI can be programmed to scan trending topics and produce
    relevant articles, blog posts, or social media updates without human intervention.
    Furthermore, because the model can generate content in various styles and formats,
    it allows a diversified content strategy. Imagine running a news website where
    the AI continually updates content based on emerging global events, freeing up
    human editors to focus on more complex tasks, such as investigative reporting
    or opinion pieces.
  prefs: []
  type: TYPE_NORMAL
- en: Supercharging code compilation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Developers often find themselves waiting for code to compile, which interrupts
    the flow of work. Auto-GPT in continuous mode can manage these mundane tasks.
    It can be set up to automatically compile code, run test cases, and even push
    updates to a repository. This enables a more seamless development process and
    ensures that the latest changes are always integrated and tested. By offloading
    routine tasks to Auto-GPT, developers can focus on problem-solving and innovation.
  prefs: []
  type: TYPE_NORMAL
- en: Always-on customer support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customer support is an area where Auto-GPT’s continuous mode can be transformative.
    By enabling this feature, the chatbot can handle an infinite number of queries
    and concerns without requiring human intervention. This makes for a 24/7 customer
    support system that can handle most issues and only escalates complex or sensitive
    matters to human agents. This not only improves customer satisfaction but also
    significantly reduces the operational costs associated with customer service.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into using continuous mode, let’s first make sure we learn some
    best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Safeguards and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the capabilities of Auto-GPT’s continuous mode are impressive, it is
    essential to approach its deployment with caution. Here are some measures you
    can take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enable confirmation prompts**: For commands that could potentially incur
    costs or are irreversible, enable confirmation prompts. For example, if you set
    up Auto-GPT to handle emails, a confirmation prompt sent to the user before sending
    could prevent unwanted communications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use allowlists and blocklists**: Restrict the model’s capabilities by using
    allowlists for approved actions and blocklists for prohibited ones. For example,
    you could use an allowlist to specify which external databases the AI can access
    for information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradual resource scaling**: Begin with conservative computational and financial
    limits. As you observe the system’s behavior and performance, you can slowly relax
    these constraints. This minimizes the risk of runaway costs or overutilization
    of resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sandboxed testing**: Before rolling out the system in a live environment,
    it is advisable to test it in a sandboxed or isolated setting. This allows you
    to identify and correct any bugs or vulnerabilities without affecting real-world
    operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have discussed ways to ensure good behavior. Now, we can assess how human
    monitoring through user prompts relates to general human oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Regular monitoring and human oversight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While continuous mode aims to minimize human intervention, periodic reviews
    and adjustments are necessary. Monitor logs to detect any anomalies or irregular
    behavior and have a plan in place for immediate human takeover in high-risk scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: However, human oversight is still indispensable. Adjust configurations based
    on feedback, establish approval workflows for high-risk scenarios, and default
    to human takeover when unsure.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to fully trust GPT to make all the decisions itself and are confident
    that it might run perfectly, let’s discuss the downside of having GPT make decisions
    for all actions and shake up your trust towards it a little.
  prefs: []
  type: TYPE_NORMAL
- en: Potential risks and how to mitigate them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As GPT is the core of our Auto-GPT application, it can and will at some point
    make absurd decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, I let Auto-GPT run for a while with instructions to write me a
    web-based browser game whose genre is RPG that should be running with `Three.js`,
    and I got this:'
  prefs: []
  type: TYPE_NORMAL
- en: First, Auto-GPT created some empty files of the code it wanted to write but
    only added TODOs in them that indicate that the code has to be finished.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it wrote some actual unit tests that might even partially work, but it
    got so confident that it finished the whole project and tested it. Then, it started
    searching Google to find companies to test the game and even wrote an email to
    one of them negotiating a price of only $100 per day as I set that as the budget
    for the project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way it wrote emails was scary and interesting at the same time. It found
    a coding website that just runs code you enter into a box, which also allowed
    URL params that accept code that is to be executed. Auto-GPT created a URL with
    the code it wrote as the parameters that sent an email to some company it found
    on Google that offered cheap QA testing.
  prefs: []
  type: TYPE_NORMAL
- en: It also managed to receive the email that was sent back with an answer for the
    price asked by the company employee, although to this day I have no idea how it
    created that email account because it was done by an agent that it created, and
    back then, they did not log what they did.
  prefs: []
  type: TYPE_NORMAL
- en: I was sitting on the couch and listening to the *speak* outputs of Auto-GPT.
    I set it to use a female voice for the main agent and for any other agent, it
    was a male voice, so I just thought it sounded like a conversation in an office
    until the male voice said *finished negotiating the daily price for testing with
    ****, reporting back* *from duty*.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT has changed since then. It is a bit stricter with the agents. Back
    then, the agents did whatever they wanted until they decided they were done with
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite precautions, preparedness to forcibly stop is crucial. But balance
    is key: avoid prematurely halting long-running tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: If we do run into a situation where we want to stop Auto-GPT from going further,
    but we do not want to break it, we will explore how to avoid abruptly killing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Gracefully stopping a continuous process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, we may observe Auto-GPT behaving totally incorrectly and performing
    very wrong actions, or we just get a feeling that it is not doing anything productive,
    or it might even get so off track that it may run into a loop of researching a
    topic over and over without ever finalizing its search.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you see in the logs that Auto-GPT is stuck in a loop of doing
    the same few tasks over and over, you may want to stop it to prevent high running
    costs for using the API.
  prefs: []
  type: TYPE_NORMAL
- en: Keyboard shortcuts such as *Ctrl* + *C* send immediate cancellation requests.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to have the terminal in which Auto-GPT is running active by clicking
    on it if you are not sure.
  prefs: []
  type: TYPE_NORMAL
- en: If this does not work, for example, when Auto-GPT is *thinking* or a plugin
    is doing something and ignores the key interruption, you can always use a Process
    Explorer tool such as **Windows Task Manager**, **Mac OS Activities**, or **htop/top**
    on Linux to find the Python process of Auto-GPT and kill it manually.
  prefs: []
  type: TYPE_NORMAL
- en: When your Auto-GPT instance is running on Docker and continuous mode is enabled,
    always make sure that you have at least the Docker Desktop app open so you can
    shut down the Docker container of Auto-GPT if you see that it does something unexpected
    and does not react to *Ctrl* + *C*.
  prefs: []
  type: TYPE_NORMAL
- en: If you are away from your computer and only communicate to Auto-GPT over Telegram
    or Discord, for example, make sure to have a failsafe that always works to prevent
    Auto-GPT from becoming destructive.
  prefs: []
  type: TYPE_NORMAL
- en: When Auto-GPT decides that it has no other tasks, it also executes the `task_complete`
    command, which cleanly finishes ongoing tasks before stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Pause/resume functionality is being explored to delay Auto-GPT actions without
    disrupting them. These delays could enable periodic human reviews to make sure
    no odd behavior starts happening.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve examined continuous mode in Auto-GPT, a feature that
    allows the AI to operate autonomously without requiring user input at every step.
    This mode is very useful for automating tasks such as research, content creation,
    code compilation, and customer support, providing significant efficiency gains.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed practical use cases, such as personal assistants and automated
    research tools, highlighting how continuous mode can streamline various processes.
    However, using this mode requires careful planning and safeguards. Best practices
    include enabling confirmation prompts, using allowlists and blocklists, starting
    with conservative limits, and conducting sandbox testing.
  prefs: []
  type: TYPE_NORMAL
- en: Regular monitoring and human oversight remain essential, even with continuous
    mode enabled. It’s important to review logs and have a plan for immediate intervention
    if necessary. Potential risks, such as generating false information or getting
    stuck in loops, can be mitigated with strategies such as kill switches and usage
    limits.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we covered how to stop a continuous process gracefully and the ongoing
    research into adding pause and resume functionalities. By understanding and implementing
    these strategies, you can effectively use continuous mode while maintaining control
    and safety.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker provides a powerful platform to easily develop, distribute, and run your
    AI models built with Auto-GPT. Understanding how Docker integrates with Auto-GPT,
    how to begin a Docker instance, and troubleshooting Docker-related issues can
    fast-track your Auto-GPT deployment. Coupled with the power of customization discussed
    in [*Chapter 5*](B21128_05.xhtml#_idTextAnchor063), Docker takes Auto-GPT’s prowess
    a notch higher, consolidating it as a wonderfully adaptable, shareable, and user-friendly
    tool in your AI toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: With more to be learned about Docker, feel free to visit the official page at
    [https://docs.docker.com/](https://docs.docker.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we saw how continuous mode is a double-edged sword requiring thoughtful
    configuration and oversight. Used judiciously, it can automate workflows, enhancing
    productivity. Used carelessly, it can have regrettable outcomes. Find the right
    equilibrium for your use case. But regardless of safeguards, exercise caution,
    start small, and keep humans in the loop, especially for high-risk applications.
    Because beneficial AI, like all powerful technologies, necessitates responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve explored how Docker encapsulates Auto-GPT’s computational environment,
    simplifying setup, sharing, and version control, and ensuring consistent experiences
    across users.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve demystified what continuous mode is and how it allows Auto-GPT to function
    autonomously, highlighting its significance in driving efficiency and productivity
    across various tasks and industries.
  prefs: []
  type: TYPE_NORMAL
- en: Through real-world use cases, we’ve seen the transformative power of continuous
    mode in research, content creation, software development, and customer service,
    showcasing its versatility.
  prefs: []
  type: TYPE_NORMAL
- en: We explored strategies for deploying continuous mode effectively while mitigating
    risks, including the implementation of safeguards, monitoring mechanisms, and
    limits to ensure responsible usage.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve tackled potential pitfalls, from generating inaccuracies to running into
    infinite loops, offering practical solutions to prevent, detect, and correct such
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of knowing how to stop or pause continuous mode operations safely
    was discussed, ensuring that interventions are possible without causing disruption.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, we will dive into using our own LLM models for Auto-GPT
    and see how they compare to GPT-4.
  prefs: []
  type: TYPE_NORMAL
