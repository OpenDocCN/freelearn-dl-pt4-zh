- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining Expressiveness for Graph Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we traded accuracy for scalability. We saw that it
    was instrumental in applications such as recommender systems. However, it raises
    several questions about what makes GNNs “accurate.” Where does this precision
    come from? Can we use this knowledge to design better GNNs?
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will clarify what makes a GNN powerful by introducing the **Weisfeiler-Leman**
    (**WL**) test. This test will give us the framework to understand an essential
    concept in GNNs – **expressiveness**. We will use it to compare different GNN
    layers and see which one is the most expressive. This result will then be used
    to design a more powerful GNN than GCNs, GATs, and GraphSAGE.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will implement it using PyTorch Geometric to perform a new task
    – graph classification. We will implement a new GNN on the `PROTEINS` dataset,
    comprising 1,113 graphs representing proteins. We will compare different methods
    for graph classification and analyze our results.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand what makes a GNN expressive
    and how to measure it. You will be able to implement a new GNN architecture based
    on the WL test and perform graph classification using various techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining expressiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the GIN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying graphs with GIN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter09](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Defining expressiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are used to approximate functions. This is justified by the
    **universal approximation theorem**, which states that a feedforward neural network
    with only one layer can approximate any smooth function. But what about universal
    function approximation on graphs? This is a more complex problem that requires
    the ability to distinguish graph structures.
  prefs: []
  type: TYPE_NORMAL
- en: With GNNs, our goal is to produce the best node embeddings possible. This means
    that different nodes must have different embeddings, and similar nodes must have
    similar embeddings. But how do we know that two nodes are similar? Embeddings
    are computed using node features and connections. Therefore, we have to compare
    their features and neighbors to distinguish nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In graph theory, this is referred to as the graph **isomorphism** problem. Two
    graphs are isomorphic (“the same”) if they have the same connections, and their
    only difference is a permutation of their nodes (see *Figure 9**.1*). In 1968,
    Weisfeiler and Lehman [1] proposed an efficient algorithm to solve this problem,
    now known as the WL test.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – An example of two isomorphic graphs](img/B19153_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – An example of two isomorphic graphs
  prefs: []
  type: TYPE_NORMAL
- en: The WL test aims to build the **canonical form** of a graph. We can then compare
    the canonical form of two graphs to check whether they are isomorphic or not.
    However, this test is not perfect, and non-isomorphic graphs can share the same
    canonical form. This can be surprising, but it is an intricate problem that is
    still not completely understood; for instance, the complexity of the WL algorithm
    is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 'The WL test works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning, each node in the graph receives the same color.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each node aggregates its own color and the colors of its neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result is fed to a hash function that produces a new color.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each node aggregates its new color and the new colors of its neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result is fed to a hash function that produces a new color.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are repeated until no more nodes change color.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following figure summarizes the WL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – An application of the WL algorithm to get the canonical form
    of a graph](img/B19153_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – An application of the WL algorithm to get the canonical form of
    a graph
  prefs: []
  type: TYPE_NORMAL
- en: The resulting colors give us the canonical form of the graph. If two graphs
    do not share the same colors, they are not isomorphic. Conversely, we cannot be
    sure they are isomorphic if they obtain the same colors.
  prefs: []
  type: TYPE_NORMAL
- en: The steps we described should be familiar; they are surprisingly close to what
    GNNs perform. Colors are a form of embeddings, and the hash function is an aggregator.
    But it is not just any aggregator; the hash function is particularly suited for
    this task. Would it still be as efficient if we were to replace it with another
    function, such as a mean or max aggregator (as seen in [*Chapter 8*](B19153_08.xhtml#_idTextAnchor096))?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the result for each operator:'
  prefs: []
  type: TYPE_NORMAL
- en: With the mean aggregator, having 1 blue node and 1 red node, or 10 blue nodes
    and 10 red nodes, results in the same embedding (half blue and half red).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the max aggregator, half of the nodes would be ignored in the previous
    example; the embedding would only consider the blue or red color.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the sum aggregator, however, every node contributes to the final embedding;
    having 1 red node and 1 blue node is different from having 10 blue nodes and 10
    red nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, the sum aggregator can discriminate more graph structures than the other
    two. If we follow this logic, this can only mean one thing – the aggregators we
    have been using so far are suboptimal, since they are strictly less expressive
    than a sum. Can we use this knowledge to build better GNNs? In the next section,
    we will introduce the **Graph Isomorphism Network** (**GIN**) based on this idea.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the GIN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw that the GNNs introduced in the previous chapters
    were less expressive than the WL test. This is an issue because the ability to
    distinguish more graph structures seems to be connected to the quality of the
    resulting embeddings. In this section, we will translate the theoretical framework
    into a new GNN architecture – the GIN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduced in 2018 by Xu et al. in a paper called “*How Powerful are Graph
    Neural Networks?*” [2], the GIN is designed to be as expressive as the WL test.
    The authors generalized our observations on aggregation by dividing it into two
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aggregate**: This function, ![](img/Formula_B19153_09_001.png), selects the
    neighboring nodes that the GNN considers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine**: This function, ![](img/Formula_B19153_09_002.png), combines the
    embeddings from the selected nodes to produce the new embedding of the target
    node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The embedding of the ![](img/Formula_B19153_09_003.png) node can be written
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the case of a GCN, the ![](img/Formula_B19153_09_005.png) function aggregates
    every neighbor of the ![](img/Formula_B19153_09_006.png) node, and ![](img/Formula_B19153_09_007.png)
    applies a specific mean aggregator. In the case of GraphSAGE, the neighborhood
    sampling is the ![](img/Formula_B19153_09_008.png) function, and we saw three
    options for ![](img/Formula_B19153_09_009.png) – the mean, LSTM, and max aggregators.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are these functions in the GIN? Xu et al. argue that they have to be
    **injective**. As shown in *Figure 9**.3*, injective functions map distinct inputs
    to distinct outputs. This is precisely what we want to distinguish graph structures.
    If the functions were not injective, we would end up with the same output for
    different inputs. In this case, our embeddings would be less valuable because
    they would contain less information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A mapping diagram of an injective function](img/B19153_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A mapping diagram of an injective function
  prefs: []
  type: TYPE_NORMAL
- en: 'The GIN’s authors use a clever trick to design these two functions – they simply
    approximate them. In the GAT layer, we learned the self-attention weights. In
    this example, we can learn both functions using a single MLP, thanks to the universal
    approximation theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B19153_09_011.png) is a learnable parameter or a fixed
    scalar, representing the importance of the target node’s embedding compared to
    its neighbors’. The authors also emphasize that the MLP must have more than one
    layer to distinguish specific graph structures.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a GNN that is as expressive as the WL test. Can we do even better?
    The answer is yes. The WL test can be generalized to a hierarchy of higher-level
    tests known as **k-WL**. Instead of considering individual nodes, ![](img/Formula_B19153_09_012.png)-WL
    tests look at ![](img/Formula_B19153_09_013.png)-tuples of nodes. It means that
    they are non-local, since they can look at distant nodes. This is also why ![](img/Formula_B19153_09_014.png)-WL
    tests can distinguish more graph structures than ![](img/Formula_B19153_09_015.png)-WL
    tests for ![](img/Formula_B19153_09_016.png).
  prefs: []
  type: TYPE_NORMAL
- en: Several architectures based on ![](img/Formula_B19153_09_017.png)-WL tests have
    been proposed, such as the **k-GNN** by Morris et al. [3]. While these architectures
    help us better understand how GNNs work, they tend to underperform in practice
    compared to less expressive models, such as GNNs or GATs [4]. But all hope is
    not lost, as we will see in the next section in the particular context of graph
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying graphs using GIN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could directly implement a GIN model for node classification, but this architecture
    is more interesting for performing graph classification. In this section, we will
    see how to transform node embeddings into graph embeddings using `PROTEINS` dataset
    and compare our results using GIN and GCN models.
  prefs: []
  type: TYPE_NORMAL
- en: Graph classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph classification is based on the node embeddings that a GNN produces. This
    operation is often called global pooling or **graph-level readout**. There are
    three simple ways of implementing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean global pooling**: The graph embedding ![](img/Formula_B19153_09_018.png)
    is obtained by averaging the embeddings of every node in the graph:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Max global pooling**: The graph embedding is obtained by selecting the highest
    value for each node dimension:![](img/Formula_B19153_09_020.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sum global pooling**: The graph embedding is obtained by summing the embeddings
    of every node in the graph:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'According to what we saw in the first section, the sum global pooling is strictly
    more expressive than the two other techniques. The GIN’s authors also note that
    to consider all structural information, it is necessary to consider embeddings
    produced by every layer of the GNN. In summary, we concatenate the sum of node
    embeddings produced by each of the *k* layers of our GNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This solution elegantly combines the expressive power of the sum operator with
    the memory of each layer provided by the concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the GIN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now implement a GIN model with the previous graph-level readout function
    on the `PROTEINS [5, 6,` `7]` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset comprises 1,113 graphs representing proteins, where every node
    is an amino acid. An edge connects two nodes when their distance is lower than
    0.6 nanometers. The goal of this dataset is to classify each protein as an **enzyme**.
    Enzymes are a particular type of protein that act as catalysts to speed up chemical
    reactions in a cell. For instance, enzymes called lipases aid in the digestion
    of food. *Figure 9**.4* shows the 3D plot of a protein.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – An example of a protein in 3D](img/B19153_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – An example of a protein in 3D
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a GIN model on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `PROTEINS` dataset using the `TUDataset` class from PyTorch
    Geometric and print the information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We split the data (graphs) into training, validation, and test sets with an
    80/10/10 split respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We convert these splits into mini-batches using the `DataLoader` object with
    a batch size of 64\. This means that each batch will contain up to 64 graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can verify that by printing information about each batch, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s start implementing a GIN model. The first question we have to answer
    is the composition of our GIN layer. We need an MLP with at least two layers.
    Following the authors’ guidelines, we can also introduce batch normalization to
    standardize the inputs of each hidden layer, which stabilizes and speeds up training.
    In summary, our GIN layer has the following composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_09_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, it is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Geometric also offers the GINE layer, a modified version of the GIN
    layer. It was introduced in 2019 by Hu et al. in “*Strategies for Pre-training
    Graph Neural Networks*” [8]. Its major improvement over the previous GIN version
    is the ability to consider edge features during the aggregation process. The `PROTEINS`
    dataset does not have edge features, which is why we will implement the classic
    GIN model instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model is not complete yet. We must not forget that we want to perform graph
    classification. It requires the sum of every node embedding in the graph for each
    layer. In other words, we will need to store one vector of `dim_h` size per layer
    – three, in this example. This is why we add a linear layer with `3*dim_h` size
    before the final linear layer for binary classification (`data.num_classes` =
    2):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We must implement the logic to connect our initialized layers. Each layer produces
    a different embedding tensor – `h1`, `h2`, and `h3`. We sum them using the `global_add_pool()`
    function and then concatenate them with `torch.cat()`. This gives us the input
    to our classifier, which acts as a regular neural network with a dropout layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now implement a regular training loop with mini-batching for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We print the training and validation accuracy every 20 epochs and return the
    trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Unlike the `test` function from the previous chapter, this one must also include
    mini-batching, since our validation and test loaders contain more than one batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the function we will use to calculate the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s instantiate and train our GIN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s test it using the test loader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To better understand this final test score, we can implement a GCN that performs
    graph classification with a simple global mean pooling (`global_mean_pool()` in
    PyTorch Geometric). With the exact same setting, it achieves an average accuracy
    score of 53.72% (± 0.73%) on 100 experiments. This is much lower than the average
    76.56% (± 1.77%) obtained by the GIN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can conclude that the entire GIN architecture is much more suited for this
    graph classification task than GCNs. According to the theoretical framework we
    used, this is explained by the fact that GCNs are strictly less expressive than
    GINs. In other words, GINs can distinguish more graph structures than GCNs, which
    is why they are more accurate. We can verify this assumption by visualizing the
    mistakes made by both models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `matplotlib` and `networkx` libraries to make a 4x4 plot of proteins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each protein, we get the final classification from our GNN (the GIN in
    this case). We give the prediction a green color if it is correct (red otherwise):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We convert our protein into a `networkx` graph for convenience. We can then
    draw it using the `nx.draw_networkx()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We obtain the following plot for the GIN model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Graph classifications produced by the GIN model](img/B19153_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Graph classifications produced by the GIN model
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for the GCN and get the following visualization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Graph classifications produced by the GCN model](img/B19153_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Graph classifications produced by the GCN model
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the GCN model makes more mistakes. Understanding which graph structures
    are not adequately captured would require extensive analysis for each protein
    correctly classified by GIN. However, we can see that the GIN also makes different
    mistakes. This is interesting because it shows that these models can be complementary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating ensembles from models that make different mistakes is a common technique
    in machine learning. We could use different approaches, such as a third model
    trained on our final classifications. As creating ensembles is not the goal of
    this chapter, we will implement a simple model-averaging technique instead:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set the models in evaluation mode and define the variables to store
    accuracy scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the final classifications for each model and combine them to get the
    ensemble’s predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the accuracy scores for the three sets of predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, our ensemble outperforms both models with an accuracy score
    of 81.25% (compared to 72.14% for the GCN and 80.99% for the GIN). This result
    is significant, as it shows the possibilities offered by this kind of technique.
    However, this is not necessarily the case in general; even with this example,
    the ensemble model does not consistently outperform the GIN. We could enrich it
    with embeddings from other architectures, such as `Node2Vec`, and see whether
    it improves the final accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we defined the expressive power of GNNs. This definition is
    based on another algorithm, the WL method, which outputs the canonical form of
    a graph. This algorithm is not perfect, but it can distinguish most graph structures.
    It inspired the GIN architecture, designed to be as expressive as the WL test
    and, therefore, strictly more expressive than GCNs, GATs, or GraphSAGE.
  prefs: []
  type: TYPE_NORMAL
- en: We then implemented this architecture for graph classification. We saw different
    methods to combine node embeddings into graph embeddings. GIN offers a new technique,
    which incorporates a sum operator and the concatenation of graph embeddings produced
    by every GIN layer. It significantly outperformed the classic global mean pooling
    obtained with GCN layers. Finally, we combined predictions made by both models
    into a simple ensemble, which increased the accuracy score even further.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B19153_10.xhtml#_idTextAnchor116)*, Predicting Links with
    Graph Neural Networks*, we will explore another popular task with GNNs – link
    prediction. In fact, this is not entirely new, as previous techniques we saw such
    as `DeepWalk` and `Node2Vec` were already based on this idea. We will explain
    why and introduce two new GNN frameworks – the Graph (Variational) Autoencoder
    and SEAL. Finally, we will implement and compare them on the `Cora` dataset on
    a link prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Weisfeiler and Lehman, A.A. (1968) A Reduction of a Graph to a Canonical
    Form and an Algebra Arising during This Reduction. Nauchno-Technicheskaya Informatsia,
    9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, *How Powerful are Graph Neural
    Networks?* arXiv, 2018\. doi: 10.48550/ARXIV.1810.00826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. Morris et al., *Weisfeiler and Leman Go Neural: Higher-order Graph Neural
    Networks*. arXiv, 2018\. doi: 10.48550/ARXIV.1810.02244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] V. P. Dwivedi et al. *Benchmarking graph neural networks*. arXiv, 2020\.
    doi: 10.48550/ARXIV.2003.00982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] K. M. Borgwardt, C. S. Ong, S. Schoenauer, S. V. N. Vishwanathan, A. J.
    Smola, and H. P. Kriegel. *Protein function prediction via graph kernels*. Bioinformatics,
    21(Suppl 1):i47–i56, Jun 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] P. D. Dobson and A. J. Doig. *Distinguishing enzyme structures from non-enzymes
    without alignments*. J. Mol. Biol., 330(4):771–783, Jul 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Christopher Morris and Nils M. Kriege and Franka Bause and Kristian Kersting
    and Petra Mutzel and Marion Neumann. *TUDataset: A collection of benchmark datasets
    for learning with graphs*. In ICML 2020 Workshop on Graph Representation Learning
    and Beyond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] W. Hu et al., *Strategies for Pre-training Graph Neural Networks*. arXiv,
    2019\. doi: 10.48550/ARXIV.1905.12265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
