["```py\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport utils\n```", "```py\nCUDA = True\nDATA_PATH = '~/Data/mnist'\nOUT_PATH = 'output'\nLOG_FILE = os.path.join(OUT_PATH, 'log.txt')\nBATCH_SIZE = 128\nIMAGE_CHANNEL = 1\nZ_DIM = 100\nG_HIDDEN = 64\nX_DIM = 64\nD_HIDDEN = 64\nEPOCH_NUM = 25\nREAL_LABEL = 1\nFAKE_LABEL = 0\nlr = 2e-4\nseed = 1\n```", "```py\nutils.clear_folder(OUT_PATH)\nprint(\"Logging to {}\\n\".format(LOG_FILE))\nsys.stdout = utils.StdOut(LOG_FILE)\nCUDA = CUDA and torch.cuda.is_available()\nprint(\"PyTorch version: {}\".format(torch.__version__))\nif CUDA:\n    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\nif seed is None:\n    seed = np.random.randint(1, 10000)\nprint(\"Random Seed: \", seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif CUDA:\n    torch.cuda.manual_seed(seed)\ncudnn.benchmark = True\ndevice = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n```", "```py\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # 1st layer\n            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(G_HIDDEN * 8),\n            nn.ReLU(True),\n            # 2nd layer\n            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(G_HIDDEN * 4),\n            nn.ReLU(True),\n            # 3rd layer\n            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(G_HIDDEN * 2),\n            nn.ReLU(True),\n            # 4th layer\n            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(G_HIDDEN),\n            nn.ReLU(True),\n            # output layer\n            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n```", "```py\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n```", "```py\nnetG = Generator().to(device)\nnetG.apply(weights_init)\nprint(netG)\n```", "```py\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # 1st layer\n            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 2nd layer\n            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(D_HIDDEN * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 3rd layer\n            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(D_HIDDEN * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # 4th layer\n            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(D_HIDDEN * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # output layer\n            nn.Conv2d(D_HIDDEN * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1, 1).squeeze(1)\n```", "```py\nnetD = Discriminator().to(device)\nnetD.apply(weights_init)\nprint(netD)\n```", "```py\ncriterion = nn.BCELoss()\n\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n```", "```py\ndataset = dset.MNIST(root=DATA_PATH, download=True,\n                     transform=transforms.Compose([\n                     transforms.Resize(X_DIM),\n                     transforms.ToTensor(),\n                     transforms.Normalize((0.5,), (0.5,))\n                     ]))\nassert dataset\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n                                         shuffle=True, num_workers=4)\n\n```", "```py\nviz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\nfor epoch in range(EPOCH_NUM):\n    for i, data in enumerate(dataloader):\n        x_real = data[0].to(device)\n        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device)\n        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device)\n\n        # Update D with real data\n        netD.zero_grad()\n        y_real = netD(x_real)\n        loss_D_real = criterion(y_real, real_label)\n        loss_D_real.backward()\n\n        # Update D with fake data\n        z_noise = torch.randn(x_real.size(0), Z_DIM, 1, 1, device=device)\n        x_fake = netG(z_noise)\n        y_fake = netD(x_fake.detach())\n        loss_D_fake = criterion(y_fake, fake_label)\n        loss_D_fake.backward()\n        optimizerD.step()\n\n        # Update G with fake data\n        netG.zero_grad()\n        y_fake_r = netD(x_fake)\n        loss_G = criterion(y_fake_r, real_label)\n        loss_G.backward()\n        optimizerG.step()\n\n        if i % 100 == 0:\n            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake: \n              {:.4f} loss_G: {:.4f}'.format(\n                epoch, i, len(dataloader),\n                loss_D_real.mean().item(),\n                loss_D_fake.mean().item(),\n                loss_G.mean().item()\n            ))\n```", "```py\nif i % 100 == 0:\n            ...\n            vutils.save_image(x_real, os.path.join(OUT_PATH, 'real_samples.png'), normalize=True)\n            with torch.no_grad():\n                viz_sample = netG(viz_noise)\n                vutils.save_image(viz_sample, os.path.join(OUT_PATH, 'fake_samples_{}.png'.format(epoch)), normalize=True)\n    torch.save(netG.state_dict(), os.path.join(OUT_PATH, 'netG_{}.pth'.format(epoch)))\n    torch.save(netD.state_dict(), os.path.join(OUT_PATH, 'netD_{}.pth'.format(epoch)))\n```", "```py\n $ conda activate torch\n(torch)$ python dcgan.py\n```", "```py\n$ sudo apt-get install gir1.2-gtop-2.0 gir1.2-networkmanager-1.0 gir1.2-clutter-1.0 gir1.2-clutter-gst-3.0 gir1.2-gtkclutter-1.0\n```", "```py\nwatch -n 0.5 nvidia-smi\n```", "```py\nDATA_PATH = '/media/john/FastData/CelebA'    # Load data from SSD\n```", "```py\nIMAGE_CHANNEL = 3\n```", "```py\ndataset = dset.ImageFolder(root=DATA_PATH,\n                           transform=transforms.Compose([\n                           transforms.Resize(X_DIM),\n                           transforms.CenterCrop(X_DIM),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.5, 0.5, 0.5),  \n                                               (0.5, 0.5, 0.5)),\n                           ]))\n```", "```py\n$ git clone https://github.com/fyu/lsun.git\n$ cd lsun\n$ python download.py -c bedroom\n```", "```py\nDATA_PATH = '/media/john/FastData/lsun'    # Load data from SSD\n```", "```py\nIMAGE_CHANNEL = 3\n```", "```py\ndataset = dset.LSUN(root=DATA_PATH, classes=['bedroom_train'],\n                    transform=transforms.Compose([\n                    transforms.Resize(X_DIM),\n                    transforms.CenterCrop(X_DIM),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.5, 0.5, 0.5), \n                                        (0.5, 0.5, 0.5)),\n                    ]))\n```", "```py\n$ pip install lmdb\n```", "```py\nnetG = Generator().to(device)\nnetG.apply(weights_init)\nprint(netG)\n\n```", "```py\nnetG = Generator()\nnegG.load_state_dict(torch.load(os.path.join(OUT_PATH, 'netG_24.pth')))\nnetG.to(device)\n```", "```py\nif VIZ_MODE == 0:\n    viz_tensor = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\nelif VIZ_MODE == 1:\n    load_vector = np.loadtxt('vec_20190317-223131.txt')\n    xp = [0, 1]\n    yp = np.vstack([load_vector[2], load_vector[9]]) # choose two exemplar vectors\n    xvals = np.linspace(0, 1, num=BATCH_SIZE)\n    sample = interp1d(xp, yp, axis=0)\n    viz_tensor = torch.tensor(sample(xvals).reshape(BATCH_SIZE, Z_DIM, 1, 1), dtype=torch.float32, device=device)\nelif VIZ_MODE == 2:\n    load_vector = np.loadtxt('vec_20190317-223131.txt')\n    z1 = (load_vector[0] + load_vector[6] + load_vector[8]) / 3.\n    z2 = (load_vector[1] + load_vector[2] + load_vector[4]) / 3.\n    z3 = (load_vector[3] + load_vector[4] + load_vector[6]) / 3.\n    z_new = z1 - z2 + z3\n    sample = np.zeros(shape=(BATCH_SIZE, Z_DIM))\n    for i in range(BATCH_SIZE):\n        sample[i] = z_new + 0.1 * np.random.normal(-1.0, 1.0, 100)\n    viz_tensor = torch.tensor(sample.reshape(BATCH_SIZE, Z_DIM, 1, 1), dtype=torch.float32,  device=device)\n```", "```py\nwith torch.no_grad():\n    viz_sample = netG(viz_tensor)\n    viz_vector = utils.to_np(viz_temsor).reshape(BATCH_SIZE, Z_DIM)\n    cur_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    np.savetxt('vec_{}.txt'.format(cur_time), viz_vector)\n    vutils.save_image(viz_sample, 'img_{}.png'.format(cur_time), nrow=10, normalize=True\n```", "```py\nfrom datetime import datetime\n```", "```py\nVIZ_MODE = 0\n```", "```py\nif VIZ_MODE == 0:\n    viz_tensor = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n```", "```py\nelif VIZ_MODE == 1:    \n    load_vector = np.loadtxt('vec_20190317-223131.txt')\n    xp = [0, 1]\n    yp = np.vstack([load_vector[2], load_vector[9]])\n    xvals = np.linspace(0, 1, num=BATCH_SIZE)\n    sample = interp1d(xp, yp, axis=0)\n    viz_tensor = torch.tensor(sample(xvals).reshape(BATCH_SIZE, Z_DIM, 1, 1), dtype=torch.float32, device=device)\n```", "```py\nVIZ_MODE = 1\n```", "```py\nVIZ_MODE = 2 \n```", "```py\nelif VIZ_MODE == 2:    \n    load_vector = np.loadtxt('vec_20190317-223131.txt')\n    z1 = (load_vector[0] + load_vector[6] + load_vector[8]) / 3.\n    z2 = (load_vector[1] + load_vector[2] + load_vector[4]) / 3.\n    z3 = (load_vector[3] + load_vector[4] + load_vector[6]) / 3.\n    z_new = z1 - z2 + z3\n    sample = np.zeros(shape=(BATCH_SIZE, Z_DIM))\n    for i in range(BATCH_SIZE):\n        sample[i] = z_new + 0.1 * np.random.normal(-1.0, 1.0, 100)\n    viz_tensor = torch.tensor(sample.reshape(BATCH_SIZE, Z_DIM, 1, 1), dtype=torch.float32, device=device)\n```"]