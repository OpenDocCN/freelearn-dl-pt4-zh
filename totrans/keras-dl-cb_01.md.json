["```py\nimport tensorflow as tf\n\nvector = tf.constant([[4,5,6]], dtype=tf.float32)\neucNorm = tf.norm(vector, ord=\"euclidean\")\n\nwith tf.Session() as sess:\nprint(sess.run(eucNorm))\n```", "```py\n# convert matrices to tensor objects\nimport numpy as np\nimport tensorflow as tf\n\n# create a 2x2 matrix in various forms\nmatrix1 = [[1.0, 2.0], [3.0, 40]]\nmatrix2 = np.array([[1.0, 2.0], [3.0, 40]], dtype=np.float32)\nmatrix3 = tf.constant([[1.0, 2.0], [3.0, 40]])\n\nprint(type(matrix1))\nprint(type(matrix2))\nprint(type(matrix3))\n\ntensorForM1 = tf.convert_to_tensor(matrix1, dtype=tf.float32)\ntensorForM2 = tf.convert_to_tensor(matrix2, dtype=tf.float32)\ntensorForM3 = tf.convert_to_tensor(matrix3, dtype=tf.float32)\n\nprint(type(tensorForM1))\nprint(type(tensorForM2))\nprint(type(tensorForM3))\n```", "```py\n<class 'list'>\n<class 'numpy.ndarray'>\n<class 'tensorflow.python.framework.ops.Tensor'>\n<class 'tensorflow.python.framework.ops.Tensor'>\n<class 'tensorflow.python.framework.ops.Tensor'>\n<class 'tensorflow.python.framework.ops.Tensor'>\n```", "```py\nimport tensorflow as tf\n\nmat1 = tf.constant([[4, 5, 6],[3,2,1]])\nmat2 = tf.constant([[7, 8, 9],[10, 11, 12]])\n\n*# hadamard product (element wise)* mult = tf.multiply(mat1, mat2)\n\n*# dot product (no. of rows = no. of columns)* dotprod = tf.matmul(mat1, tf.transpose(mat2))\n\nwith tf.Session() as sess:\n    print(sess.run(mult))\n    print(sess.run(dotprod))\n```", "```py\n[[28 40 54][30 22 12]]\n [[122 167][ 46 64]]\n```", "```py\nimport tensorflow as tf\n\nmat = tf.constant([\n [0, 1, 2],\n [3, 4, 5],\n [6, 7, 8]\n], dtype=tf.float32)\n\n *# get trace ('sum of diagonal elements') of the matrix* mat = tf.trace(mat)\n\n with tf.Session() as sess:\n    print(sess.run(mat))\n```", "```py\nimport tensorflow as tf\n\nx = [[1,2,3],[4,5,6]]\nx = tf.convert_to_tensor(x)\nxtrans = tf.transpose(x)\n\ny=([[[1,2,3],[6,5,4]],[[4,5,6],[3,6,3]]])\ny = tf.convert_to_tensor(y)\nytrans = tf.transpose(y, perm=[0, 2, 1])\n\nwith tf.Session() as sess:\n   print(sess.run(xtrans))\n   print(sess.run(ytrans))\n```", "```py\n[[1 4] [2 5] [3 6]]\n```", "```py\nimport tensorflow as tf\n\nmat = tf.constant([\n [0, 1, 2],\n [3, 4, 5],\n [6, 7, 8]\n], dtype=tf.float32)\n\n*# get diagonal of the matrix* diag_mat = tf.diag_part(mat)\n\n*# create matrix with given diagonal* mat = tf.diag([1,2,3,4])\n\nwith tf.Session() as sess:\n   print(sess.run(diag_mat))\n   print(sess.run(mat))\n```", "```py\n[ 0\\.  4\\.  8.]\n[[1 0 0 0][0 2 0 0] [0 0 3 0] [0 0 0 4]]\n```", "```py\nimport tensorflow as tf\n\nidentity = tf.eye(3, 3)\n\nwith tf.Session() as sess:\n   print(sess.run(identity))  \n```", "```py\n[[ 1\\.  0\\.  0.] [ 0\\.  1\\.  0.] [ 0\\.  0\\.  1.]]\n```", "```py\nimport tensorflow as tf\n\nmat = tf.constant([[2, 3, 4], [5, 6, 7], [8, 9, 10]], dtype=tf.float32)\nprint(mat)\n\ninv_mat = tf.matrix_inverse(tf.transpose(mat))\n\nwith tf.Session() as sess:\nprint(sess.run(inv_mat))\n```", "```py\nimport tensorflow as tf\n\n# equation 1\nx1 = tf.constant(3, dtype=tf.float32)\ny1 = tf.constant(2, dtype=tf.float32)\npoint1 = tf.stack([x1, y1])\n\n# equation 2\nx2 = tf.constant(4, dtype=tf.float32)\ny2 = tf.constant(-1, dtype=tf.float32)\npoint2 = tf.stack([x2, y2])\n\n# solve for AX=C\nX = tf.transpose(tf.stack([point1, point2]))\nC = tf.ones((1,2), dtype=tf.float32)\n\nA = tf.matmul(C, tf.matrix_inverse(X))\n\nwith tf.Session() as sess:\n    X = sess.run(X)\n    print(X)\n\n    A = sess.run(A)\n    print(A)\n\nb = 1 / A[0][1]\na = -b * A[0][0]\nprint(\"Hence Linear Equation is: y = {a}x + {b}\".format(a=a, b=b))\n```", "```py\n[[ 3\\. 4.][ 2\\. -1.]]\n [[ 0.27272728 0.09090909]]\nHence Linear Equation is: y = -2.9999999999999996x + 10.999999672174463\n```", "```py\n# canonical circle equation\n# x2+y2+dx+ey+f = 0\n# dx+ey+f=âˆ’(x2+y2) ==> AX = B\n# we have to solve for d, e, f\n\npoints = tf.constant([[2,1], [0,5], [-1,2]], dtype=tf.float64)\nX = tf.constant([[2,1,1], [0,5,1], [-1,2,1]], dtype=tf.float64)\nB = -tf.constant([[5], [25], [5]], dtype=tf.float64)\n\nA = tf.matrix_solve(X,B)\n\nwith tf.Session() as sess:\n    result = sess.run(A)\n    D, E, F = result.flatten()\n    print(\"Hence Circle Equation is: x**2 + y**2 + {D}x + {E}y + {F} = 0\".format(**locals()))\n```", "```py\nHence Circle Equation is: x**2 + y**2 + -2.0x + -6.0y + 5.0 = 0\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plts\n\npath = \"/neuralnetwork-programming/ch01/plots\"\n\ntext = [\"I\", \"like\", \"enjoy\",\n         \"deep\", \"learning\", \"NLP\", \"flying\", \".\"]\nxMatrix = np.array([[0,2,1,0,0,0,0,0],\n              [2,0,0,1,0,1,0,0],\n              [1,0,0,0,0,0,1,0],\n              [0,1,0,0,1,0,0,0],\n              [0,0,0,1,0,0,0,1],\n              [0,1,0,0,0,0,0,1],\n              [0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,1,1,0]], dtype=np.float32)\n\nX_tensor = tf.convert_to_tensor(xMatrix, dtype=tf.float32)\n\n# tensorflow svd\nwith tf.Session() as sess:\n    s, U, Vh = sess.run(tf.svd(X_tensor, full_matrices=False))\n\nfor i in range(len(text)):\n    plts.text(U[i,0], U[i,1], text[i])\n\nplts.ylim(-0.8,0.8)\nplts.xlim(-0.8,2.0)\nplts.savefig(path + '/svd_tf.png')\n\n# numpy svd\nla = np.linalg\nU, s, Vh = la.svd(xMatrix, full_matrices=False)\n\nprint(U)\nprint(s)\nprint(Vh)\n\n# write matrices to file (understand concepts)\nfile = open(path + \"/matx.txt\", 'w')\nfile.write(str(U))\nfile.write(\"\\n\")\nfile.write(\"=============\")\nfile.write(\"\\n\")\nfile.write(str(s))\nfile.close()\n\nfor i in range(len(text)):\n    plts.text(U[i,0], U[i,1], text[i])\n\nplts.ylim(-0.8,0.8)\nplts.xlim(-0.8,2.0)\nplts.savefig(path + '/svd_np.png')\n```", "```py\n[[ -5.24124920e-01  -5.72859168e-01   9.54463035e-02   3.83228481e-01   -1.76963374e-01  -1.76092178e-01  -4.19185609e-01  -5.57702743e-02]\n[ -5.94438076e-01   6.30120635e-01  -1.70207784e-01   3.10038358e-0\n 1.84062332e-01  -2.34777853e-01   1.29535481e-01   1.36813134e-01]\n[ -2.56274015e-01   2.74017543e-01   1.59810841e-01   3.73903001e-16\n  -5.78984618e-01   6.36550903e-01  -3.32297325e-16  -3.05414885e-01]\n[ -2.85637408e-01  -2.47912124e-01   3.54610324e-01  -7.31901303e-02\n  4.45784479e-01   8.36141407e-02   5.48721075e-01  -4.68012422e-01]\n[ -1.93139315e-01   3.38495038e-02  -5.00790417e-01  -4.28462476e-01\n 3.47110212e-01   1.55483231e-01  -4.68663752e-01  -4.03576553e-01]\n[ -3.05134684e-01  -2.93989003e-01  -2.23433599e-01  -1.91614240e-01\n 1.27460942e-01   4.91219401e-01   2.09592804e-01   6.57535374e-01]\n[ -1.82489842e-01  -1.61027774e-01  -3.97842437e-01  -3.83228481e-01\n -5.12923241e-01  -4.27574426e-01   4.19185609e-01  -1.18313827e-01]\n[ -2.46898428e-01   1.57254755e-01   5.92991650e-01  -6.20076716e-01\n -3.21868137e-02  -2.31065080e-01  -2.59070963e-01   2.37976909e-01]]\n[ 2.75726271  2.67824793  1.89221275  1.61803401  1.19154561  0.94833982\n 0.61803401  0.56999218]\n[[ -5.24124920e-01  -5.94438076e-01  -2.56274015e-01  -2.85637408e-01\n -1.93139315e-01  -3.05134684e-01  -1.82489842e-01  -2.46898428e-01]\n[  5.72859168e-01  -6.30120635e-01  -2.74017543e-01   2.47912124e-01\n -3.38495038e-02   2.93989003e-01   1.61027774e-01  -1.57254755e-01]\n[ -9.54463035e-02   1.70207784e-01  -1.59810841e-01  -3.54610324e-01\n 5.00790417e-01   2.23433599e-01   3.97842437e-01  -5.92991650e-01]\n[  3.83228481e-01   3.10038358e-01  -2.22044605e-16  -7.31901303e-02\n -4.28462476e-01  -1.91614240e-01  -3.83228481e-01  -6.20076716e-01]\n[ -1.76963374e-01   1.84062332e-01  -5.78984618e-01   4.45784479e-01\n 3.47110212e-01   1.27460942e-01  -5.12923241e-01  -3.21868137e-02]\n[  1.76092178e-01   2.34777853e-01  -6.36550903e-01  -8.36141407e-02\n -1.55483231e-01  -4.91219401e-01   4.27574426e-01   2.31065080e-01]\n[  4.19185609e-01  -1.29535481e-01  -3.33066907e-16  -5.48721075e-01\n  4.68663752e-01  -2.09592804e-01  -4.19185609e-01   2.59070963e-01]\n[ -5.57702743e-02   1.36813134e-01  -3.05414885e-01  -4.68012422e-01\n -4.03576553e-01   6.57535374e-01  -1.18313827e-01   2.37976909e-01]]\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as FF\nimport pandas as pd\n\npath = \"/neuralnetwork-programming/ch01/plots\"\nlogs = \"/neuralnetwork-programming/ch01/logs\"\n\nxMatrix = np.array([[0,2,1,0,0,0,0,0],\n              [2,0,0,1,0,1,0,0],\n              [1,0,0,0,0,0,1,0],\n              [0,1,0,0,1,0,0,0],\n              [0,0,0,1,0,0,0,1],\n              [0,1,0,0,0,0,0,1],\n              [0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,1,1,0]], dtype=np.float32)\n\ndef pca(mat):\n    mat = tf.constant(mat, dtype=tf.float32)\n    mean = tf.reduce_mean(mat, 0)\n    less = mat - mean\n    s, u, v = tf.svd(less, full_matrices=True, compute_uv=True)\n\n    s2 = s ** 2\n    variance_ratio = s2 / tf.reduce_sum(s2)\n\n    with tf.Session() as session:\n        run = session.run([variance_ratio])\n    return run\n\nif __name__ == '__main__':\n    print(pca(xMatrix))\n```", "```py\n[array([  4.15949494e-01,   2.08390564e-01,   1.90929279e-01,\n         8.36438537e-02,   5.55494241e-02,   2.46047471e-02,\n         2.09326427e-02,   3.57540098e-16], dtype=float32)]\n```", "```py\nmnist dataset and initialize weights and biases:\n```", "```py\nimport tensorflow as tf\n\n# get mnist dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# x represents image with 784 values as columns (28*28), y represents output digit\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# initialize weights and biases [w1,b1][w2,b2]\nnumNeuronsInDeepLayer = 30\nw1 = tf.Variable(tf.truncated_normal([784, numNeuronsInDeepLayer]))\nb1 = tf.Variable(tf.truncated_normal([1, numNeuronsInDeepLayer]))\nw2 = tf.Variable(tf.truncated_normal([numNeuronsInDeepLayer, 10]))\nb2 = tf.Variable(tf.truncated_normal([1, 10]))\n```", "```py\n# non-linear sigmoid function at each neuron\ndef sigmoid(x):\n    sigma = tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n    return sigma\n\n# starting from first layer with wx+b, then apply sigmoid to add non-linearity\nz1 = tf.add(tf.matmul(x, w1), b1)\na1 = sigmoid(z1)\nz2 = tf.add(tf.matmul(a1, w2), b2)\na2 = sigmoid(z2)\n\n# calculate the loss (delta)\nloss = tf.subtract(a2, y)\n\n# derivative of the sigmoid function der(sigmoid)=sigmoid*(1-sigmoid)\ndef sigmaprime(x):\n    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n\n# backward propagation\ndz2 = tf.multiply(loss, sigmaprime(z2))\ndb2 = dz2\ndw2 = tf.matmul(tf.transpose(a1), dz2)\n\nda1 = tf.matmul(dz2, tf.transpose(w2))\ndz1 = tf.multiply(da1, sigmaprime(z1))\ndb1 = dz1\ndw1 = tf.matmul(tf.transpose(x), dz1)\n\n# finally update the network\neta = tf.constant(0.5)\nstep = [\n    tf.assign(w1,\n              tf.subtract(w1, tf.multiply(eta, dw1)))\n    , tf.assign(b1,\n                tf.subtract(b1, tf.multiply(eta,\n                                             tf.reduce_mean(db1, axis=[0]))))\n    , tf.assign(w2,\n                tf.subtract(w2, tf.multiply(eta, dw2)))\n    , tf.assign(b2,\n                tf.subtract(b2, tf.multiply(eta,\n                                             tf.reduce_mean(db2, axis=[0]))))\n]\n\nacct_mat = tf.equal(tf.argmax(a2, 1), tf.argmax(y, 1))\nacct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(10000):\n    batch_xs, batch_ys = data.train.next_batch(10)\n    sess.run(step, feed_dict={x: batch_xs,\n                              y: batch_ys})\n    if i % 1000 == 0:\n        res = sess.run(acct_res, feed_dict=\n        {x: data.test.images[:1000],\n         y: data.test.labels[:1000]})\n        print(res)\n```", "```py\nExtracting MNIST_data\n125.0\n814.0\n870.0\n874.0\n889.0\n897.0\n906.0\n903.0\n922.0\n913.0\n```", "```py\nimport tensorflow as tf\n\n# get mnist dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# x represents image with 784 values as columns (28*28), y represents output digit\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# initialize weights and biases [w1,b1][w2,b2]\nnumNeuronsInDeepLayer = 30\nw1 = tf.Variable(tf.truncated_normal([784, numNeuronsInDeepLayer]))\nb1 = tf.Variable(tf.truncated_normal([1, numNeuronsInDeepLayer]))\nw2 = tf.Variable(tf.truncated_normal([numNeuronsInDeepLayer, 10]))\nb2 = tf.Variable(tf.truncated_normal([1, 10]))\n\n# non-linear sigmoid function at each neuron\ndef sigmoid(x):\n    sigma = tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n    return sigma\n\n# starting from first layer with wx+b, then apply sigmoid to add non-linearity\nz1 = tf.add(tf.matmul(x, w1), b1)\na1 = sigmoid(z1)\nz2 = tf.add(tf.matmul(a1, w2), b2)\na2 = sigmoid(z2)\n\n# calculate the loss (delta)\nloss = tf.subtract(a2, y)\n\n# derivative of the sigmoid function der(sigmoid)=sigmoid*(1-sigmoid)\ndef sigmaprime(x):\n    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n\n# automatic differentiation\ncost = tf.multiply(loss, loss)\nstep = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n\nacct_mat = tf.equal(tf.argmax(a2, 1), tf.argmax(y, 1))\nacct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(10000):\n    batch_xs, batch_ys = data.train.next_batch(10)\n    sess.run(step, feed_dict={x: batch_xs,\n                              y: batch_ys})\n    if i % 1000 == 0:\n        res = sess.run(acct_res, feed_dict=\n        {x: data.test.images[:1000],\n         y: data.test.labels[:1000]})\n        print(res)\n```", "```py\n96.0\n 777.0\n 862.0\n 870.0\n 889.0\n 901.0\n 911.0\n 905.0\n 914.0\n 924.0\n```", "```py\nimport tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\nrndm = numpy.random\n\n# config parameters\nlearningRate = 0.01\ntrainingEpochs = 1000\ndisplayStep = 50\n\n# create the training data\ntrainX = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n                         7.042,10.791,5.313,7.997,5.654,9.27,3.12])\ntrainY = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n                         2.827,3.465,1.65,2.904,2.42,2.94,1.34])\nnSamples = trainX.shape[0]\n\n# tf inputs\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\n\n# initialize weights and bias\nW = tf.Variable(rndm.randn(), name=\"weight\")\nb = tf.Variable(rndm.randn(), name=\"bias\")\n\n# linear model\nlinearModel = tf.add(tf.multiply(X, W), b)\n\n# mean squared error\nloss = tf.reduce_sum(tf.pow(linearModel-Y, 2))/(2*nSamples)\n\n# Gradient descent\nopt = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)\n\n# initializing variables\ninit = tf.global_variables_initializer()\n\n# run\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # fitting the training data\n    for epoch in range(trainingEpochs):\n        for (x, y) in zip(trainX, trainY):\n            sess.run(opt, feed_dict={X: x, Y: y})\n\n        # print logs\n        if (epoch+1) % displayStep == 0:\n            c = sess.run(loss, feed_dict={X: trainX, Y:trainY})\n            print(\"Epoch is:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(c), \"W=\", sess.run(W), \"b=\", sess.run(b))\n\n    print(\"optimization done...\")\n    trainingLoss = sess.run(loss, feed_dict={X: trainX, Y: trainY})\n    print(\"Training loss=\", trainingLoss, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n\n    # display the plot\n    plt.plot(trainX, trainY, 'ro', label='Original data')\n    plt.plot(trainX, sess.run(W) * trainX + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n\n    # Testing example, as requested (Issue #2)\n    testX = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    testY = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n\n    print(\"Testing... (Mean square loss Comparison)\")\n    testing_cost = sess.run(\n        tf.reduce_sum(tf.pow(linearModel - Y, 2)) / (2 * testX.shape[0]),\n        feed_dict={X: testX, Y: testY})\n    print(\"Testing cost=\", testing_cost)\n    print(\"Absolute mean square loss difference:\", abs(trainingLoss - testing_cost))\n\n    plt.plot(testX, testY, 'bo', label='Testing data')\n    plt.plot(trainX, sess.run(W) * trainX + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n```", "```py\nEpoch is: 0050 loss= 0.141912043 W= 0.10565 b= 1.8382\n Epoch is: 0100 loss= 0.134377643 W= 0.11413 b= 1.7772\n Epoch is: 0150 loss= 0.127711013 W= 0.122106 b= 1.71982\n Epoch is: 0200 loss= 0.121811897 W= 0.129609 b= 1.66585\n Epoch is: 0250 loss= 0.116592340 W= 0.136666 b= 1.61508\n Epoch is: 0300 loss= 0.111973859 W= 0.143304 b= 1.56733\n Epoch is: 0350 loss= 0.107887231 W= 0.149547 b= 1.52241\n Epoch is: 0400 loss= 0.104270980 W= 0.15542 b= 1.48017\n Epoch is: 0450 loss= 0.101070963 W= 0.160945 b= 1.44043\n Epoch is: 0500 loss= 0.098239250 W= 0.166141 b= 1.40305\n Epoch is: 0550 loss= 0.095733419 W= 0.171029 b= 1.36789\n Epoch is: 0600 loss= 0.093516059 W= 0.175626 b= 1.33481\n Epoch is: 0650 loss= 0.091553882 W= 0.179951 b= 1.3037\n Epoch is: 0700 loss= 0.089817807 W= 0.184018 b= 1.27445\n Epoch is: 0750 loss= 0.088281371 W= 0.187843 b= 1.24692\n Epoch is: 0800 loss= 0.086921677 W= 0.191442 b= 1.22104\n Epoch is: 0850 loss= 0.085718453 W= 0.194827 b= 1.19669\n Epoch is: 0900 loss= 0.084653646 W= 0.198011 b= 1.17378\n Epoch is: 0950 loss= 0.083711281 W= 0.201005 b= 1.15224\n Epoch is: 1000 loss= 0.082877308 W= 0.203822 b= 1.13198\n optimization done...\n Training loss= 0.0828773 W= 0.203822 b= 1.13198\nTesting... (Mean square loss Comparison)\n Testing cost= 0.0957726\n Absolute mean square loss difference: 0.0128952\n```", "```py\nimport tensorflow as tf\nimport numpy as np\n\nX = tf.Variable(np.random.random_sample(), dtype=tf.float32)\ny = tf.Variable(np.random.random_sample(), dtype=tf.float32)\n\ndef createCons(x):\n    return tf.constant(x, dtype=tf.float32)\n\nfunction = tf.pow(X, createCons(2)) + createCons(2) * X * y + createCons(3) * tf.pow(y, createCons(2)) + createCons(4) * X + createCons(5) * y + createCons(6)\n\n# compute hessian\ndef hessian(func, varbles):\n    matrix = []\n    for v_1 in varbles:\n        tmp = []\n        for v_2 in varbles:\n            # calculate derivative twice, first w.r.t v2 and then w.r.t v1\n            tmp.append(tf.gradients(tf.gradients(func, v_2)[0], v_1)[0])\n        tmp = [createCons(0) if t == None else t for t in tmp]\n        tmp = tf.stack(tmp)\n        matrix.append(tmp)\n    matrix = tf.stack(matrix)\n    return matrix\n\nhessian = hessian(function, [X, y])\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nprint(sess.run(hessian))\n```", "```py\n [[ 2\\.  2.] [ 2\\.  6.]]\n```", "```py\nimport tensorflow as tf\nimport numpy as np\n\nx = np.array([[10.0, 15.0, 20.0], [0.0, 1.0, 5.0], [3.0, 5.0, 7.0]], dtype=np.float32)\n\ndet = tf.matrix_determinant(x)\n\nwith tf.Session() as sess:\n    print(sess.run(det))\n```", "```py\n-15.0\n```", "```py\nimport numpy as np\nimport seaborn\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# input dataset\nxData = np.arange(100, step=.1)\nyData = xData + 20 * np.sin(xData/10)\n\n# scatter plot for input data\nplt.scatter(xData, yData)\nplt.show()\n\n# defining data size and batch size\nnSamples = 1000\nbatchSize = 100\n\n# resize\nxData = np.reshape(xData, (nSamples,1))\nyData = np.reshape(yData, (nSamples,1))\n\n# input placeholders\nx = tf.placeholder(tf.float32, shape=(batchSize, 1))\ny = tf.placeholder(tf.float32, shape=(batchSize, 1))\n\n# init weight and bias\nwith tf.variable_scope(\"linearRegression\"):\n W = tf.get_variable(\"weights\", (1, 1), initializer=tf.random_normal_initializer())\n b = tf.get_variable(\"bias\", (1,), initializer=tf.constant_initializer(0.0))\n\n y_pred = tf.matmul(x, W) + b\n loss = tf.reduce_sum((y - y_pred)**2/nSamples)\n\n# optimizer\nopt = tf.train.AdamOptimizer().minimize(loss)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # gradient descent loop for 500 steps\n    for _ in range(500):\n     # random minibatch\n     indices = np.random.choice(nSamples, batchSize)\n\n     X_batch, y_batch = xData[indices], yData[indices]\n\n     # gradient descent step\n     _, loss_val = sess.run([opt, loss], feed_dict={x: X_batch, y: y_batch})\n```"]