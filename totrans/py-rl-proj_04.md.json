["```py\npip install gym[classic_control]\npip install gym[mujoco]\n```", "```py\nimport gym\natari = gym.make('Acrobot-v1')\natari.reset()\natari.render()\n```", "```py\nimport gym\nimport time\n\ndef start(task_name):\n\n    task = gym.make(task_name)\n    observation = task.reset()\n\n    while True:\n        task.render()\n        action = task.env.action_space.sample()\n        observation, reward, done, _ = task.step(action)\n        print(\"Action {}, reward {}, observation {}\".format(action, reward, observation))\n        if done:\n            print(\"Game finished\")\n            break\n        time.sleep(0.05)\n    task.close()\n\nif __name__ == \"__main__\":\n\n    task_names = ['CartPole-v0', 'MountainCarContinuous-v0', \n                  'Pendulum-v0', 'Acrobot-v1']\n    for task_name in task_names:\n        start(task_name)\n```", "```py\nclass Task:\n\n    def __init__(self, name):\n\n        assert name in ['CartPole-v0', 'MountainCar-v0', \n                        'Pendulum-v0', 'Acrobot-v1']\n        self.name = name\n        self.task = gym.make(name)\n        self.last_state = self.reset()\n\n    def reset(self):\n        state = self.task.reset()\n        self.total_reward = 0\n        return state\n\n    def play_action(self, action):\n\n        if self.name not in ['Pendulum-v0', 'MountainCarContinuous-v0']:\n            action = numpy.fmax(action, 0)\n            action = action / numpy.sum(action)\n            action = numpy.random.choice(range(len(action)), p=action)\n        else:\n            low = self.task.env.action_space.low\n            high = self.task.env.action_space.high\n            action = numpy.fmin(numpy.fmax(action, low), high)\n\n        state, reward, done, _ = self.task.step(action)\n        self.total_reward += reward\n        termination = 1 if done else 0\n\n        return reward, state, termination\n\n    def get_total_reward(self):\n        return self.total_reward\n\n    def get_action_dim(self):\n        if self.name not in ['Pendulum-v0', 'MountainCarContinuous-v0']:\n            return self.task.env.action_space.n\n        else:\n            return self.task.env.action_space.shape[0]\n\n    def get_state_dim(self):\n        return self.last_state.shape[0]\n\n    def get_activation_fn(self):\n        if self.name not in ['Pendulum-v0', 'MountainCarContinuous-v0']:\n            return tf.nn.softmax\n        else:\n            return None\n```", "```py\nInitialize replay memory  to capacity ;\nInitialize the critic network  and actor network  with random weights  and ;\nInitialize the target networks  and  with weights  and ;\nRepeat for each episode:\n    Set time step ;\n    Initialize a random process  for action exploration noise;\n    Receive an initial observation state ;\n    While the terminal state hasn't been reached:\n        Select an action  according to the current policy and exploration noise;\n        Execute action  in the simulator and observe reward  and the next state ;\n        Store transition  into replay memory ;\n        Randomly sample a batch of  transitions  from ;\n        Set  if  is a terminal state or  if  is a non-terminal state;\n        Update critic by minimizing the loss:\n                      ;\n        Update the actor policy using the sampled policy gradient:\n                      ;\n        Update the target networks:\n                      ,\n                      ;\n    End while\n```", "```py\nclass ActorNetwork:\n\n    def __init__(self, input_state, output_dim, hidden_layers, activation=tf.nn.relu):\n\n        self.x = input_state\n        self.output_dim = output_dim\n        self.hidden_layers = hidden_layers\n        self.activation = activation\n\n        with tf.variable_scope('actor_network'):\n            self.output = self._build()\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n\n    def _build(self):\n\n        layer = self.x\n        init_b = tf.constant_initializer(0.01)\n\n        for i, num_unit in enumerate(self.hidden_layers):\n            layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n\n        output = dense(layer, self.output_dim, activation=self.activation, init_b=init_b, name='output')\n        return output\n```", "```py\nclass CriticNetwork:\n\n    def __init__(self, input_state, input_action, hidden_layers):\n\n        assert len(hidden_layers) >= 2\n        self.input_state = input_state\n        self.input_action = input_action\n        self.hidden_layers = hidden_layers\n\n        with tf.variable_scope('critic_network'):\n            self.output = self._build()\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n\n    def _build(self):\n\n        layer = self.input_state\n        init_b = tf.constant_initializer(0.01)\n\n        for i, num_unit in enumerate(self.hidden_layers):\n            if i != 1:\n                layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n            else:\n                layer = tf.concat([layer, self.input_action], axis=1, name='concat_action')\n                layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n\n        output = dense(layer, 1, activation=None, init_b=init_b, name='output')\n        return tf.reshape(output, shape=(-1,))\n```", "```py\nclass ActorCriticNet:\n\n    def __init__(self, input_dim, action_dim, \n                 critic_layers, actor_layers, actor_activation, \n                 scope='ac_network'):\n\n        self.input_dim = input_dim\n        self.action_dim = action_dim\n        self.scope = scope\n\n        self.x = tf.placeholder(shape=(None, input_dim), dtype=tf.float32, name='x')\n        self.y = tf.placeholder(shape=(None,), dtype=tf.float32, name='y')\n\n        with tf.variable_scope(scope):\n            self.actor_network = ActorNetwork(self.x, action_dim, \n                                              hidden_layers=actor_layers, \n                                              activation=actor_activation)\n\n            self.critic_network = CriticNetwork(self.x, \n                                                self.actor_network.get_output_layer(),\n                                                hidden_layers=critic_layers)\n\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n            self._build()\n\n    def _build(self):\n\n        value = self.critic_network.get_output_layer()\n\n        actor_loss = -tf.reduce_mean(value)\n        self.actor_vars = self.actor_network.get_params()\n        self.actor_grad = tf.gradients(actor_loss, self.actor_vars)\n        tf.summary.scalar(\"actor_loss\", actor_loss, collections=['actor'])\n        self.actor_summary = tf.summary.merge_all('actor')\n\n        critic_loss = 0.5 * tf.reduce_mean(tf.square((value - self.y)))\n        self.critic_vars = self.critic_network.get_params()\n        self.critic_grad = tf.gradients(critic_loss, self.critic_vars)\n        tf.summary.scalar(\"critic_loss\", critic_loss, collections=['critic'])\n        self.critic_summary = tf.summary.merge_all('critic')\n```", "```py\nclass ActorCriticNet:\n\n    def get_action(self, sess, state):\n        return self.actor_network.get_action(sess, state)\n\n    def get_value(self, sess, state):\n        return self.critic_network.get_value(sess, state)\n\n    def get_action_value(self, sess, state, action):\n        return self.critic_network.get_action_value(sess, state, action)\n\n    def get_actor_feed_dict(self, state):\n        return {self.x: state}\n\n    def get_critic_feed_dict(self, state, action, target):\n        return {self.x: state, self.y: target, \n                self.critic_network.input_action: action}\n\n    def get_clone_op(self, network, tau=0.9):\n        update_ops = []\n        new_vars = {v.name.replace(network.scope, ''): v for v in network.vars}\n        for v in self.vars:\n            u = (1 - tau) * v + tau * new_vars[v.name.replace(self.scope, '')]\n            update_ops.append(tf.assign(v, u))\n        return update_ops\n```", "```py\nclass DPG:\n\n    def __init__(self, config, task, directory, callback=None, summary_writer=None):\n\n        self.task = task\n        self.directory = directory\n        self.callback = callback\n        self.summary_writer = summary_writer\n\n        self.config = config\n        self.batch_size = config['batch_size']\n        self.n_episode = config['num_episode']\n        self.capacity = config['capacity']\n        self.history_len = config['history_len']\n        self.epsilon_decay = config['epsilon_decay']\n        self.epsilon_min = config['epsilon_min']\n        self.time_between_two_copies = config['time_between_two_copies']\n        self.update_interval = config['update_interval']\n        self.tau = config['tau']\n\n        self.action_dim = task.get_action_dim()\n        self.state_dim = task.get_state_dim() * self.history_len\n        self.critic_layers = [50, 50]\n        self.actor_layers = [50, 50]\n        self.actor_activation = task.get_activation_fn()\n\n        self._init_modules()\n```", "```py\n    def _init_modules(self):\n        # Replay memory\n        self.replay_memory = ReplayMemory(history_len=self.history_len, \n                                          capacity=self.capacity)\n        # Actor critic network\n        self.ac_network = ActorCriticNet(input_dim=self.state_dim, \n                                         action_dim=self.action_dim, \n                                         critic_layers=self.critic_layers, \n                                         actor_layers=self.actor_layers, \n                                         actor_activation=self.actor_activation,\n                                         scope='ac_network')\n        # Target network\n        self.target_network = ActorCriticNet(input_dim=self.state_dim, \n                                             action_dim=self.action_dim, \n                                             critic_layers=self.critic_layers, \n                                             actor_layers=self.actor_layers, \n                                             actor_activation=self.actor_activation,\n                                             scope='target_network')\n        # Optimizer\n        self.optimizer = Optimizer(config=self.config, \n                                   ac_network=self.ac_network, \n                                   target_network=self.target_network, \n                                   replay_memory=self.replay_memory)\n        # Ops for updating target network\n        self.clone_op = self.target_network.get_clone_op(self.ac_network, tau=self.tau)\n        # For tensorboard\n        self.t_score = tf.placeholder(dtype=tf.float32, shape=[], name='new_score')\n        tf.summary.scalar(\"score\", self.t_score, collections=['dpg'])\n        self.summary_op = tf.summary.merge_all('dpg')\n\n    def choose_action(self, sess, state, epsilon=0.1):\n        x = numpy.asarray(numpy.expand_dims(state, axis=0), dtype=numpy.float32)\n        action = self.ac_network.get_action(sess, x)[0]\n        return action + epsilon * numpy.random.randn(len(action))\n\n    def play(self, action):\n        r, new_state, termination = self.task.play_action(action)\n        return r, new_state, termination\n\n    def update_target_network(self, sess):\n        sess.run(self.clone_op)\n```", "```py\n    def train(self, sess, saver=None):\n\n        num_of_trials = -1\n        for episode in range(self.n_episode):\n            frame = self.task.reset()\n            for _ in range(self.history_len+1):\n                self.replay_memory.add(frame, 0, 0, 0)\n\n            for _ in range(self.config['T']):\n                num_of_trials += 1\n                epsilon = self.epsilon_min + \\\n                          max(self.epsilon_decay - num_of_trials, 0) / \\\n                          self.epsilon_decay * (1 - self.epsilon_min)\n                if num_of_trials % self.update_interval == 0:\n                    self.optimizer.train_one_step(sess, num_of_trials, self.batch_size)\n\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, epsilon) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if num_of_trials % self.time_between_two_copies == 0:\n                    self.update_target_network(sess)\n                    self.save(sess, saver)\n\n                if self.callback:\n                    self.callback()\n                if termination:\n                    score = self.task.get_total_reward()\n                    summary_str = sess.run(self.summary_op, feed_dict={self.t_score: score})\n                    self.summary_writer.add_summary(summary_str, num_of_trials)\n                    self.summary_writer.flush()\n                    break\n```", "```py\n    def evaluate(self, sess):\n\n        for episode in range(self.n_episode):\n            frame = self.task.reset()\n            for _ in range(self.history_len+1):\n                self.replay_memory.add(frame, 0, 0, 0)\n\n            for _ in range(self.config['T']):\n                print(\"episode {}, total reward {}\".format(episode, \n                                                           self.task.get_total_reward()))\n\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, self.epsilon_min) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if self.callback:\n                    self.callback()\n                    if termination:\n                        break\n```", "```py\npython train.py -t CartPole-v0 -d cpu\n```", "```py\ntensorboard --logdir=log/CartPole-v0/train\n```", "```py\npython train.py -t Pendulum-v0 -d cpu\n```", "```py\ntensorboard --logdir=log/Pendulum-v0/train\n```", "```py\npython train.py -t Hopper-v0 -d cpu\n```", "```py\nInitialize policy ;\nRepeat for each step :\n    Compute all advantage values ;\n    Solve the following optimization problem:\n    ;\nUntil convergence\n```", "```py\nCUDA_VISIBLE_DEVICES= python train.py -t Swimmer\n```", "```py\ntensorboard --logdir=log/Swimmer\n```", "```py\nCUDA_VISIBLE_DEVICES= python test.py -t Swimmer\n```"]