<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Sequence-to-Sequence Models for Building Chatbots</h1>
                </header>
            
            <article>
                
<p>We're learning a lot and doing some valuable work! In the evolution of our hypothetical business use case, this chapter builds directly on <a href="c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml" target="_blank"/><a href="c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml" target="_blank">C<span><span>hapter 4</span></span></a><span><span>, <em>Building an NLP Pipeline for Building Chatbots</em></span></span>, where we created our <strong>Natural Language Processing</strong> (<strong>NLP</strong>) pipeline. The skills we learned so far in computational linguistics should give us the confidence to expand past the training examples in this book and tackle this next project. We're going to build a more advanced chatbot for our hypothetical restaurant chain to automate the process of fielding call-in orders. </p>
<p>This requirement would mean that we'd have to combine a number of technologies that we've learned so far. But for this project, we'll be interested in learning how to make a chatbot that is more contextually aware and robust, so that we could integrate it into a larger system in this hypothetical. By demonstrating mastery on this training example, we'll have the confidence to execute this in a real situation.</p>
<p>In the previous chapters, we learned about representational learning methods, such as word2vec, and how to use them in combination with a type of deep learning algorithm called a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>). But there are few constraints while using CNNs to build language models, such as the following:</p>
<ul>
<li>The model will not be able to preserve the state information</li>
<li>The length of sentences needs to be of a fixed size for both input values and output values</li>
<li> CNNs are sometimes unable to adequately handle complex sequential contexts</li>
<li> <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>) do better at modeling information in sequence</li>
</ul>
<p class="mce-root"/>
<p>So, to overcome all of these problems, we have an alternative algorithm, which is specially designed to handle input data that comes in the form of sequences (including sequences of words, or of characters). This class of algorithm is called RNN. </p>
<p>In this chapter, we will do the following:</p>
<ul>
<li>Learn about RNN and its various forms</li>
<li>Create a language model implementation using RNN</li>
<li>Build our intuition on the <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) model</li>
<li>Create an LSTM language model implementation and compare it to the RNN model</li>
<li>Implement<span> an encoder-decoder RNN, based on the LSTM unit, for a simple sequence of question-answer tasks</span></li>
</ul>
<div class="packt_tip"><strong>Define the goal</strong>: Build a more robust chatbot with memory to provide more contextually correct responses to questions.</div>
<p class="mce-root">Let's get started!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing RNNs</h1>
                </header>
            
            <article>
                
<p>RNN is a deep learning model architecture specifically designed for sequential data. The purpose of this type of model is to extract relevant features of words and characters of text by using a small window that traverses the corpus.</p>
<p><span>RNN applies a non-linear function to each item in the sequence. This is called the RNN <em>ce</em></span><em>ll</em><span> or s</span><em>tep</em> and, in our case, the items are words or characters in the sequence.<span> The layer's output in an RNN is derived from </span><span>the output of the RNN cell, which is applied to each element in the sequence. With regard to NLP and chatbots that use text data as input, the outputs of the model are successive characters or words.</span></p>
<div class="mce-root packt_tip"><span>Each RNN cell holds an internal memory that summarizes the history of the sequence it has seen so far.</span></div>
<p>This diagram helps us to visualize the RNN model architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1443 image-border" src="assets/9712ff5e-a7f3-4dc4-a097-d20b506f745b.png" style="width:44.33em;height:16.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Vanilla version of RNN model architecture.</div>
<p><span>At the heart of the purpose of an RNN was the idea to introduce a feedback mechanism that enables context modeling through the use of fixed-weight feedback structures. What this does is build a connection between the features in the current mapping to the previous version. Basically, it employs a strategy of using an earlier version of a sequence to instruct a later version.</span></p>
<p>This is quite clever; however, it's not without its challenges. Exploding and vanishing gradients make it extremely frustrating to train these types of modes in instances where the problem is of a complex time series nature.</p>
<div class="packt_infobox"><span>A great reference to dive into that expertly outlines the vanishing and exploding gradient problem, and gives a technical explanation of viable solutions, can be found in </span>Sepp's<span> work from 1998 (</span><a href="https://dl.acm.org/citation.cfm?id=355233">https://dl.acm.org/citation.cfm?id=355233</a><span>).</span></div>
<p>A second problem that was discovered was that RNNs were picking up only one of two temporal structures: either the short-term or long-term structures. But what was needed for the best model performance was a model that was able to learn from both types of features (short-term and long-term) at the same time. The solution came in changing the basic<span> RNN cell for a <strong>Gated Recurrent Unite</strong> (<strong>GRU</strong>) or LSTM cell. </span></p>
<div class="packt_infobox"><span>For additional information on the </span>GRU refer to <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</a> <span>or, to learn more on the </span>LSTM, refer to <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</div>
<p class="mce-root">We'll explore the LSTM architecture in detail later in this chapter. Let's gain some intuition on the value of LSTM that will help us achieve our goal first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNN architectures</h1>
                </header>
            
            <article>
                
<p><span>We will mostly use the LSTM cell, since it has proven better in most NLP tasks. The principle benefit of the </span>LSTM<span> in RNN architectures is that it enables model training over long sequences, while retaining memory. To solve the gradient problem, LSTMs include more gates that effectively control access to the cell state.</span></p>
<div class="packt_infobox"><span>We've found that </span>Colah's blog post (<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>) <span>is a great place to go to obtain a good understand the working of LSTMs.</span></div>
<p class="graf graf--p graf-after--h3">These small LSTM units of RNN can be combined in multiple forms to solve various kinds of use-cases. RNNs are quite flexible in terms of combining the different input and output patterns, as follows:</p>
<ul>
<li><strong>Many to one</strong>: The model takes a complete input sequence to make a single prediction. This is used in sentiment models.</li>
<li><strong>One to many</strong>: This model transforms a single input, such as a numerical date, to generate a sequence string such as "day", "month", or "year".</li>
<li><strong>Many to many</strong>: This is a <strong><span>sequence-to-sequence</span></strong> (<span><strong>seq2seq</strong>) model, which</span> takes the entire sequence <span>as input </span>into a second sequence form, as Q/A systems do.</li>
</ul>
<p>This diagram maps out these relationships nicely:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1385 image-border" src="assets/cbaaa3d4-d06a-4f7c-b719-f2a5a35fac9f.png" style="width:31.83em;height:14.92em;"/></p>
<p class="graf graf--p graf-after--p">In this chapter, we will focus on the <strong>many to many</strong> relationship, also known as seq2seq architecture, to build a question-answer chatbot. The standard RNN approach to solving the seq2seq problem involves three primary components:</p>
<ul class="postList">
<li class="graf graf--li graf-after--p"><strong>Encoders</strong>: These transform<span> </span>the input sentences into some abstract encoded representation</li>
<li class="graf graf--li graf-after--li"><strong>Hidden layer</strong>: Encoded sentence transformation representations are manipulated here</li>
<li class="graf graf--li graf-after--li"><strong>Decoders</strong>: These output a decoded target sequence</li>
</ul>
<p>Let's have a look at the following diagram:</p>
<p class="graf graf--p graf-after--li CDPAlignCenter CDPAlign"><img src="assets/a7479ffd-d841-4e5f-bcce-24455fbaa7c2.png" style="width:40.00em;height:22.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The illustration of building the encode decoder model which takes input text (question) in the encoder, it gets transformed in the intermediate step and gets mapped with the decoder which represents the respective text (answer).</div>
<p>Let's build our intuition on RNNs by first implementing basic forms of RNN models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing basic RNNs</h1>
                </header>
            
            <article>
                
<p>In this section, we will implement a language model, using a basic RNN to perform sentiment classification. Code files for the model can found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing all of the dependencies</h1>
                </header>
            
            <article>
                
<p>This code imports TensorFlow and key dependencies for our RNN:</p>
<pre>from utils import *<br/>import tensorflow as tf<br/>from sklearn.cross_validation import train_test_split<br/>import time</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the dataset</h1>
                </header>
            
            <article>
                
<p>The dataset we'll use in this project is the <em>Movie Review Data</em> from Rotten Tomatoes (<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>). It contains 10,662 example review sentences, with approximately half of them positive and half negative. The dataset has a vocabulary of around 20,000 words. We will use the <kbd>sklearn</kbd> wrapper to load the dataset from a raw file and then a <span><kbd>separate_dataset()</kbd> </span>helper function<em> </em>to clean the dataset and transform it from its raw form to the separate list structure:</p>
<pre><strong>#Helper function</strong><br/>def separate_dataset(trainset,ratio=0.5):<br/>   datastring = []<br/>    datatarget = []<br/>    for i in range(int(len(trainset.data)*ratio)):<br/>        data_ = trainset.data[i].split('\n')<br/>        data_ = list(filter(None, data_))<br/>        for n in range(len(data_)):<br/>            data_[n] = clearstring(data_[n])<br/>        datastring += data_<br/>        for n in range(len(data_)):<br/>            datatarget.append(trainset.target[i])<br/>    return datastring, datatarget</pre>
<p>Here, <kbd>trainset</kbd> is an object that stores all of the text data and the sentiment label data:</p>
<pre>trainset = sklearn.datasets.load_files(container_path = './data', encoding = 'UTF-8')<br/>trainset.data, trainset.target = separate_dataset(trainset,1.0)<br/>print (trainset.target_names)<br/>print ('No of training data' , len(trainset.data))<br/>print ('No. of test data' , len(trainset.target))<br/><br/><strong># Output:<br/></strong>['negative', 'positive']<br/>No of training data 10662
No of test data 10662</pre>
<p class="mce-root"/>
<p>Now we will transform the labels into the one-hot encoding.</p>
<div class="packt_tip">It's important to understand the dimensions of the one-hot encoding vector. Since we have <kbd>10662</kbd> separate sentences, and two sentiments, <kbd>negative</kbd> and <kbd>positive</kbd>, our one-hot vector size will be of a size of [<em>10662, 2</em>]. </div>
<p>We will be using a popular <span><kbd>train_test_split()</kbd> </span>sklearn wrapper to randomly shuffle the data and divide the dataset into two parts: the <kbd>training</kbd> set and the <kbd>test</kbd> set. Further, with another <kbd><span>build_dataset()</span></kbd> helper function, we will create the vocabulary using a word-count-based approach:</p>
<pre>ONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))<br/>ONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0<br/>train_X, test_X, train_Y, test_Y, train_onehot, test_onehot = train_test_split(trainset.data, trainset.target, <br/>ONEHOT, test_size = 0.2)<br/><br/><br/>concat = ' '.join(trainset.data).split()<br/>vocabulary_size = len(list(set(concat)))<br/>data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)<br/>print('vocab from size: %d'%(vocabulary_size))<br/>print('Most common words', count[4:10])<br/>print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])<br/><br/><br/># OUTPUT:<strong><br/></strong>vocab from size: 20465
'Most common words', [(u'the', 10129), (u'a', 7312), (u'and', 6199), (u'of', 6063), (u'to', 4233), (u'is', 3378)]
<br/>'Sample data': <br/>[4, 662, 9, 2543, 8, 22, 4, 3558, 18064, 98] --&gt; <br/>[u'the', u'rock', u'is', u'destined', u'to', u'be', u'the', u'21st', u'centurys', u'new']<em><br/></em></pre>
<div class="packt_tip"><span>You can also try to feed any embedding model in here to make the model more accurate.</span></div>
<p>There are a few important things to remember while preparing the dataset for the RNN models. We need to add explicitly special tags in the vocabulary to keep track of the start of sentences, extra padding, the ends of sentences, and any unknown words. Hence, we have reserved the following positions for special tags in our vocabulary dictionary:</p>
<pre># Tag to mark the beginning of the sentence<br/>'GO' = 0th position<br/># Tag to add extra padding in the sentence<br/>'PAD'= 1st position<br/># Tag to mark the end of the sentence<br/>'EOS'= 2nd position<br/># Tag to mark the unknown word<br/>'UNK'= 3rd position</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameters</h1>
                </header>
            
            <article>
                
<p>We will define some of the hyperparameters for our model, as follows:</p>
<pre>size_layer = 128<br/>num_layers = 2<br/>embedded_size = 128<br/>dimension_output = len(trainset.target_names)<br/>learning_rate = 1e-3<br/>maxlen = 50<br/>batch_size = 128</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining a basic RNN cell model</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now we will create the RNN model, which takes a few input parameters, including the following:</p>
<ul>
<li class="mce-root"><kbd>size_layer</kbd>: The number of units in the RNN cell</li>
<li class="mce-root"><kbd>num_layers</kbd>: The number of hidden layers</li>
<li class="mce-root"><kbd>embedded_size</kbd>: The size of the embedding</li>
<li class="mce-root"><kbd>dict_size</kbd>: The vocabulary size</li>
<li class="mce-root"><kbd>dimension_output</kbd>: The number of classes we need to classify</li>
<li class="mce-root"><kbd>learning_rate</kbd>: The learning rate of the optimization algorithm</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"><span>The architecture of our RNN model consists of the following parts:</span></p>
<ol>
<li class="mce-root"><span>Two placeholders; one to feed sequence data into the model and the second for the output</span></li>
<li>A variable to store the embedding lookup from the dictionary</li>
<li>Then, add the RNN layer with multiple basic RNN cells</li>
<li>Create weight and bias variables</li>
<li>Compute <kbd>logits</kbd> </li>
<li>Compute loss</li>
<li>Add the Adam Optimizer</li>
<li>Calculate prediction and accuracy</li>
</ol>
<p>This model is similar to the CNN model created in the previous chapter, <a href="c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml" target="_blank">Chapter 4</a>, <em>Building an NLP Pipeline for Building Chatbots</em>, except for the RNN cell part:</p>
<pre>class Model:<br/>    def __init__(self, size_layer, num_layers, embedded_size,<br/>                 dict_size, dimension_output, learning_rate):<br/>        <br/>        def cells(reuse=False):<br/>            return tf.nn.rnn_cell.BasicRNNCell(size_layer,reuse=reuse)<br/>        <br/>        self.X = tf.placeholder(tf.int32, [None, None])<br/>        self.Y = tf.placeholder(tf.float32, [None, dimension_output])<br/><br/>        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))<br/>        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)<br/><br/>        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])<br/>        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)<br/><br/>        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())<br/>        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())<br/><br/>        self.logits = tf.matmul(outputs[:, -1], W) + b<br/>        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))<br/>        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)<br/><br/>        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))<br/>        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>
<p>In this model, the data flows from the variables that we created in <em>Step 1</em>. Then, it moves to the embedding layer defined in <em>Step 2</em>, followed by our RNN layer, which performs the computation in two hidden layers of RNN cells. Later, <kbd>logits</kbd> are computed by performing a matrix multiplication of the weight, the output from the RNN layer, and addition of bias. The last step is that we define the <kbd>cost</kbd> function; we will be using the <kbd>softmax_cross_entropy</kbd> function.</p>
<p><span>This is what the complete model looks like after computation:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2c8c39ff-aa88-4f69-8940-f677365ff683.png" style="width:62.33em;height:49.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">TensorBoard graph visualization of the RNN architecture</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following diagram represents the structure of the RNN block from the preceding screenshot. In this architecture, we have two RNN cells incorporated in hidden layers:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7bc55c85-35ba-45fb-a70a-1b9b1b354159.png" style="width:70.83em;height:53.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">TensorBoard visualization of the RNN block containing 2 hidden layers as defined in the code</div>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the RNN Model</h1>
                </header>
            
            <article>
                
<p>Now that we have our model architecture defined, let's train our model. We begin with a TensorFlow graph initialization and execute the training steps as follows:</p>
<pre>tf.reset_default_graph()<br/>sess = tf.InteractiveSession()<br/>model = Model(size_layer,num_layers,embedded_size,vocabulary_size+4,dimension_output,learning_rate)<br/>sess.run(tf.global_variables_initializer())<br/><br/><br/>EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0<br/>while True:<br/>    lasttime = time.time()<br/>    if CURRENT_CHECKPOINT == EARLY_STOPPING:<br/>        print('break epoch:%d\n'%(EPOCH))<br/>        break<br/>        <br/>    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0<br/>    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):<br/>        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)<br/>        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], <br/>                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})<br/>        train_loss += loss<br/>        train_acc += acc<br/>    <br/>    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):<br/>        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)<br/>        acc, loss = sess.run([model.accuracy, model.cost], <br/>                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})<br/>        test_loss += loss<br/>        test_acc += acc<br/>    <br/>    train_loss /= (len(train_X) // batch_size)<br/>    train_acc /= (len(train_X) // batch_size)<br/>    test_loss /= (len(test_X) // batch_size)<br/>    test_acc /= (len(test_X) // batch_size)<br/>    <br/>    if test_acc &gt; CURRENT_ACC:<br/>        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))<br/>        CURRENT_ACC = test_acc<br/>        CURRENT_CHECKPOINT = 0<br/>    else:<br/>        CURRENT_CHECKPOINT += 1<br/>        <br/>    print('time taken:', time.time()-lasttime)<br/>   print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\n'%(EPOCH,train_loss, train_acc,test_loss,test_acc))<br/>    EPOCH += 1<br/><br/></pre>
<p>While the RNN model is being trained, we can see the logs of each epoch, shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/346eb0d5-77eb-4d99-8237-384f178a9ebb.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation of the RNN model</h1>
                </header>
            
            <article>
                
<p class="text_cell_render border-box-sizing rendered_html">Let's look at our results. Once the model is trained, we can feed the test data that we prepared earlier in this chapter and evaluate the predictions. In this case, we will use a few different metrics to evaluate our model: precision, recall, and F1-scores. </p>
<p class="text_cell_render border-box-sizing rendered_html">To evaluate your model, it is important to choose the right kind of metrics—F1-scores are considered more practical compared to the accuracy score.</p>
<p class="text_cell_render border-box-sizing rendered_html">Some key points to help you understand them in simple terms are as follows:</p>
<ul>
<li><strong>Accuracy</strong>: The count of correct predictions, divided by the count of total examples that have been evaluated.</li>
<li><strong>Precision</strong><span>: High precision means you identified nearly all positives appropriately; a low precision score means you often incorrectly predicted a positive when there was none.</span></li>
<li><strong>Recall</strong><span>: High recall means you correctly predicted almost all of the real positives present in the data; a low score means you frequently missed positives that were present.</span></li>
<li><span><strong>F1-score</strong>: The balanced harmonic mean of recall and precision, giving both metrics equal weight. The higher the F-measure, the better.</span></li>
</ul>
<p>Now we will execute the model by feeding the test data with vocabulary and the max length of the text. This will produce the <kbd>logits</kbd> values which we will use to generate the evaluation metrics:</p>
<pre>logits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})<br/>print(metrics.classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))<strong><br/></strong></pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/80fe4448-9a2c-4123-8abc-fbedffae80d0.png" style="width:34.67em;height:9.08em;"/></p>
<p>So here, we can see that our average <kbd>f1-score</kbd> is 66% while using basic RNN cells. Let's see if this can be improved on by using other variations of RNN architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM architecture</h1>
                </header>
            
            <article>
                
<p>The desire to model sequential data more effectively, without the limitations of the gradient problem, led researchers to create the LSTM<span> variant of the previous RNN model architecture. LSTM achieves better performance because it incorporates gates to control the process of memory in the cell. The following diagram shows an LSTM cell:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1386 image-border" src="assets/19f1c280-e0c8-475d-82e1-c2bd83f4cf7b.png" style="width:26.83em;height:28.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>An</span> <span>LSTM unit (source: </span>http://colah.github.io/posts/2015-08-Understanding-LSTMs<span>)</span></div>
<p class="mce-root">LSTM consist of <span>three primary elements, labeled as <strong>1</strong>, <strong>2</strong>, and <strong>3</strong> in the preceding diagram:</span></p>
<ol>
<li><strong>The forget gate f(t)</strong><span>: This gate provides the ability, in the LSTM cell architecture, to forget information that is not needed. The sigmoid activation accepts the inputs </span><em>X(t)</em> <span>and</span> <strong>h(t-1)</strong><span>, and effectively decides to remove pieces of old output information by passing a <em>0</em>. The output of this gate is <em>f(t)*c(t-1)</em>.<br/></span></li>
<li><span>Information from the new input, </span><em>X(t),</em> <span>that is determined to be retained needs to be stored in the next step in the cell state. A sigmoid activation is used in this process to update or ignore parts of the new information. Next, a vector of all possible values for the new input is created by a </span><strong>tanh</strong><span> activation function. The new cell state is the product of these two values, then this new memory is added to the old memory,</span> <strong>c(t-1)</strong>, <span>to give</span> <strong>c(t)</strong><span>.</span></li>
<li><span>The last process of the LSTM cell is to determine the final output. A sigmoid layer decides which parts of the cell state to output. We then put the cell state through a </span><strong>tanh</strong><span> activation to generate all of the possible values, and multiply it by the output of the sigmoid gate, to produce desired outputs according to a non-linear function.<br/></span></li>
</ol>
<p><span>These three steps in the LSTM cell process produce a significant result, that being that the model can be trained to learn which information to retain in long-term memory and which information to forget. Genius!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing an LSTM model</h1>
                </header>
            
            <article>
                
<p>The process that we performed previously, to build the basic RNN model, will remain the same, except for the model definition part. So, let's implement this and check the performance of the new model.</p>
<p>The code for the model can be viewed at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining our LSTM model</h1>
                </header>
            
            <article>
                
<p>Again, most of the code will remain same—the only the major change will be to use <kbd>tf.nn.rnn_cell.LSTMCell()</kbd>, instead of <kbd>tf.nn.rnn_cell.BasicRNNCell()</kbd><em>.</em> While initializing the LSTM cell, we are using an orthogonal initializer that will generate a random orthogonal matrix, which is an <span>effective way of combating exploding and vanishing gradients:</span></p>
<pre>class Model:<br/>    def __init__(self, size_layer, num_layers, embedded_size,<br/>                 dict_size, dimension_output, learning_rate):<br/>        <br/>        def cells(reuse=False):<br/>            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)<br/>        <br/>        self.X = tf.placeholder(tf.int32, [None, None])<br/>        self.Y = tf.placeholder(tf.float32, [None, dimension_output])<br/><br/>        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))<br/>        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)<br/><br/>        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])<br/>        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)<br/><br/>        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())<br/>        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())<br/><br/>        self.logits = tf.matmul(outputs[:, -1], W) + b<br/>        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))<br/>        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)<br/><br/>        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))<br/>        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</pre>
<p>So this is what the architecture of the LSTM model looks like—almost the same, compared to the previous basic model, except with the addition of the LSTM cells in the <strong>RNN Block</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1387 image-border" src="assets/cd5802d3-ee3e-4755-928e-5d33950de711.png" style="width:123.25em;height:46.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the LSTM model</h1>
                </header>
            
            <article>
                
<p>Now that we've established our LSTM intuition and built the model, let's train it as follows:</p>
<pre>EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0<br/>while True:<br/>    lasttime = time.time()<br/>    if CURRENT_CHECKPOINT == EARLY_STOPPING:<br/>        print('break epoch:%d\n'%(EPOCH))<br/>        break<br/>        <br/>    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0<br/>    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):<br/>        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)<br/>        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], <br/>                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})<br/>        train_loss += loss<br/>        train_acc += acc<br/>    <br/>    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):<br/>        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)<br/>        acc, loss = sess.run([model.accuracy, model.cost], <br/>                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})<br/>        test_loss += loss<br/>        test_acc += acc<br/>    <br/>    train_loss /= (len(train_X) // batch_size)<br/>    train_acc /= (len(train_X) // batch_size)<br/>    test_loss /= (len(test_X) // batch_size)<br/>    test_acc /= (len(test_X) // batch_size)<br/>    <br/>    if test_acc &gt; CURRENT_ACC:<br/>        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))<br/>        CURRENT_ACC = test_acc<br/>        CURRENT_CHECKPOINT = 0<br/>    else:<br/>        CURRENT_CHECKPOINT += 1<br/>        <br/>    print('time taken:', time.time()-lasttime)<br/>    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\n'%(EPOCH,train_loss,<br/>                                                                                          train_acc,test_loss,<br/>                                                                                          test_acc))<br/>    EPOCH += 1</pre>
<p><span>While the LSTM model is being trained, we can see the logs of each epoch as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9893e58c-b1e5-4d51-8706-2977a0d6a6d7.png"/></p>
<p>Following is the output:</p>
<pre><strong>('time taken:', 18.061596155166626)</strong><br/><strong>epoch: 10, training loss: 0.015714, training acc: 0.994910, valid loss: 4.252270, valid acc: 0.500000</strong><br/><br/><strong>('time taken:', 17.786305904388428)</strong><br/><strong>epoch: 11, training loss: 0.011198, training acc: 0.995975, valid loss: 4.644272, valid acc: 0.502441</strong><br/><br/><strong>('time taken:', 19.031064987182617)</strong><br/><strong>epoch: 12, training loss: 0.009245, training acc: 0.996686, valid loss: 4.575824, valid acc: 0.499512<br/><br/>('time taken:', 16.996762990951538)<br/>epoch: 13, training loss: 0.006528, training acc: 0.997751, valid loss: 4.449901, valid acc: 0.501953<br/><br/>('time taken:', 17.008245944976807)<br/>epoch: 14, training loss: 0.011770, training acc: 0.995739, valid loss: 4.282045, valid acc: 0.499023<br/><br/>break epoch:15</strong></pre>
<p>You will notice that, even after using the same configurations of the model, the training time required for the LSTM-based model will be greater than the RNN model.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation of the LSTM model</h1>
                </header>
            
            <article>
                
<p>Now, let's again compute the metrics and compare the performance:</p>
<pre>logits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})<br/>print(metrics.classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))<strong><br/></strong></pre>
<p>The computed outputs are shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/de3e11c1-3789-421c-9a7b-4f7ac7e52d61.png" style="width:32.67em;height:7.67em;"/></p>
<p>So, we can clearly see the boost in the performance of the model! Now, with the LSTM, the <kbd>f1-score</kbd> is bumped to 72% whereas, in our previous basic RNN model, it was 66%, which is quite a good improvement of 7%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequence-to-sequence models</h1>
                </header>
            
            <article>
                
<p>In this section, we'll implement a seq2seq model (an encoder-decoder RNN), based on the LSTM unit, for a simple sequence-to-sequence question-answer task. This <span>model</span> <span>can</span> <span>b</span><span>e</span> <span>trained</span> <span>to</span> <span>map</span> <span>an</span> <span>input</span> <span>sequence</span><span class="_ _7"> (questions) </span><span>to</span> <span>an </span>output sequence <span class="_ _8">(answers),</span> which are not necessarily of the same length as each other.</p>
<p>This type of seq2seq model has shown impressive performance in various other tasks such as<span class="_ _7"> </span>speech recognition, machine translation<span class="_ _7">, q</span>uestion answering, <strong>Neural Machine Translation</strong> (<strong>NMT</strong>), and image caption generation.</p>
<p>The following diagram helps us visualize our seq2seq model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa874827-debc-49b6-82a0-42aa8ad6390a.png" style="width:29.83em;height:17.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The illustration of the sequence to sequence (seq2seq) model. Each rectangle box is the RNN cell in which blue ones are the encoders and Red been the Decoders.</div>
<p>In the encoder-decoder structure, one RNN (blue) <strong>encodes</strong> the input sequence. The encoder emits the context <strong>C</strong>, usually as a simple function of its final hidden state. And the second RNN (red) <strong>decoder</strong> calculates the target values and generates the output sequence. One essential step is to let the encoder and decoder communicate. In the simplest approach, you use the last hidden state of the encoder to initialize the decoder. Other approaches let the decoder attend to different parts of the encoded input at different timesteps in the decoding process.</p>
<p>So, let's get started with data preparation, model building, training, tuning, and evaluating our seq2seq model, and see how it performs.</p>
<p>The model file can be found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>Here, we will build our question-answering system. For the project, we need a dataset with question and answer pairs, as shown in the following screenshot. Both of the columns contain sequences of words, which is what we need to feed into our seq2seq model. Also, note that our sentences can be of dynamic length:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1390 image-border" src="assets/953818c9-cf3d-48c3-8466-4a296dd7315b.png" style="width:67.33em;height:63.67em;"/></p>
<p class="CDPAlignCenter CDPAlign">The dataset which we prepared with set of questions and answers</p>
<p class="mce-root"/>
<p><span>Let's load them and perform the same data processing using </span><kbd>build_dataset()</kbd><em>.</em> In the end, we will have a dictionary with words as keys, where the associated values are the counts of the word in the respective corpus. Also, we have four extras values that we talked about before in this chapter:</p>
<pre>import numpy as np<br/>import tensorflow as tf<br/>import collections<br/>from utils import *<br/><br/><br/>file_path = './conversation_data/'<br/><br/>with open(file_path+'from.txt', 'r') as fopen:<br/>    text_from = fopen.read().lower().split('\n')<br/>with open(file_path+'to.txt', 'r') as fopen:<br/>    text_to = fopen.read().lower().split('\n')<br/>print('len from: %d, len to: %d'%(len(text_from), len(text_to)))<br/><br/><br/>concat_from = ' '.join(text_from).split()<br/>vocabulary_size_from = len(list(set(concat_from)))<br/>data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)<br/> <br/><br/>concat_to = ' '.join(text_to).split()<br/>vocabulary_size_to = len(list(set(concat_to)))<br/>data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)<br/> <br/><br/>GO = dictionary_from['GO']<br/>PAD = dictionary_from['PAD']<br/>EOS = dictionary_from['EOS']<br/>UNK = dictionary_from['UNK']</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining a seq2seq model</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will outline the TensorFlow seq2seq model definition. We employed an embedding layer to go from integer representation to the vector representation of the input. This seq2seq model has four major components: the embedding layer, encoders, decoders, and cost/optimizers.</span></p>
<p class="mce-root"/>
<p><span>You can see the model in graphical form in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c6268a0b-a8f8-4242-aede-907665dbf9b7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The TensorBoard visualization of the seq2seq model. This graph shows the connection between the encode and the decoder with other relevent components like the optimizer.</div>
<p><span>The following is a formal outline of the TensorFlow seq2seq model definition:</span></p>
<pre>class Chatbot:<br/> def __init__(self, size_layer, num_layers, embedded_size,<br/> from_dict_size, to_dict_size, learning_rate, batch_size):<br/> <br/> def cells(reuse=False):<br/> return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)<br/> <br/> self.X = tf.placeholder(tf.int32, [None, None])<br/> self.Y = tf.placeholder(tf.int32, [None, None])<br/> self.X_seq_len = tf.placeholder(tf.int32, [None])<br/> self.Y_seq_len = tf.placeholder(tf.int32, [None])<br/><br/> with tf.variable_scope("encoder_embeddings"): <br/> encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))<br/> encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)<br/> main = tf.strided_slice(self.X, [0, 0], [batch_size, -1], [1, 1])<br/> <br/> with tf.variable_scope("decoder_embeddings"): <br/> decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)<br/> decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))<br/> decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)<br/> <br/> with tf.variable_scope("encoder"):<br/> rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])<br/> _, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded,<br/> dtype = tf.float32)<br/> with tf.variable_scope("decoder"):<br/> rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])<br/> outputs, _ = tf.nn.dynamic_rnn(rnn_cells_dec, decoder_embedded, <br/> initial_state = last_state,<br/> dtype = tf.float32)<br/> with tf.variable_scope("logits"): <br/> self.logits = tf.layers.dense(outputs,to_dict_size)<br/> print(self.logits)<br/> masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)<br/> with tf.variable_scope("cost"): <br/> self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.logits,<br/> targets = self.Y,<br/> weights = masks)<br/> with tf.variable_scope("optimizer"): <br/> self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameters</h1>
                </header>
            
            <article>
                
<p>Now that we have our model definition ready, we will define the hyperparameters. We will keep most of the configurations the same as in the previous one:</p>
<pre>size_layer = 128<br/>num_layers = 2<br/>embedded_size = 128<br/>learning_rate = 0.001<br/>batch_size = 32<br/>epoch = 50</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the seq2seq model</h1>
                </header>
            
            <article>
                
<p>Now, let's train the model. We will need some helper functions for the padding of the sentence and to calculate the accuracy of the model:</p>
<pre>def pad_sentence_batch(sentence_batch, pad_int):<br/>    padded_seqs = []<br/>    seq_lens = []<br/>    max_sentence_len = 50<br/>    for sentence in sentence_batch:<br/>        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))<br/>        seq_lens.append(50)<br/>    return padded_seqs, seq_lens<br/><br/>def check_accuracy(logits, Y):<br/>    acc = 0<br/>    for i in range(logits.shape[0]):<br/>        internal_acc = 0<br/>        for k in range(len(Y[i])):<br/>            if Y[i][k] == logits[i][k]:<br/>                internal_acc += 1<br/>        acc += (internal_acc / len(Y[i]))<br/>    return acc / logits.shape[0]</pre>
<p>We initialize our model and iterate the session for the defined number of epochs:</p>
<pre>tf.reset_default_graph()<br/>sess = tf.InteractiveSession()<br/>model = Chatbot(size_layer, num_layers, embedded_size, vocabulary_size_from + 4, <br/>                vocabulary_size_to + 4, learning_rate, batch_size)<br/>sess.run(tf.global_variables_initializer())<br/><br/>for i in range(epoch):<br/> total_loss, total_accuracy = 0, 0<br/> for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):<br/> batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)<br/> batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)<br/> predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer], <br/> feed_dict={model.X:batch_x,<br/> model.Y:batch_y,<br/> model.X_seq_len:seq_x,<br/> model.Y_seq_len:seq_y})<br/> total_loss += loss<br/> total_accuracy += check_accuracy(predicted,batch_y)<br/> total_loss /= (len(text_from) // batch_size)<br/> total_accuracy /= (len(text_from) // batch_size)<br/> print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))<br/><br/><strong>OUTPUT:</strong>
epoch: 47, avg loss: 0.682934, avg accuracy: 0.000000
epoch: 48, avg loss: 0.680367, avg accuracy: 0.000000
epoch: 49, avg loss: 0.677882, avg accuracy: 0.000000
epoch: 50, avg loss: 0.678484, avg accuracy: 0.000000<br/>.<br/>.<br/>.<br/>epoch: 1133, avg loss: 0.000464, avg accuracy: 1.000000<br/>epoch: 1134, avg loss: 0.000462, avg accuracy: 1.000000<br/>epoch: 1135, avg loss: 0.000460, avg accuracy: 1.000000<br/>epoch: 1136, avg loss: 0.000457, avg accuracy: 1.000000</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation of the seq2seq model</h1>
                </header>
            
            <article>
                
<p>So, after running the training process for few hours on a GPU, you can see that the accuracy has reached a value of <kbd>1.0</kbd>, and loss has significantly reduced to <kbd>0.00045</kbd>. Let's see how the model performs when we ask some generic questions.</p>
<p>To make predictions, we will create a <kbd>predict()</kbd> function that will take the raw text of any size as input and return the response to the question that we asked. We did a quick fix to handle the <strong>Out Of Vocab</strong> (<strong>OOV</strong>) words by replacing them with the <kbd>PAD</kbd>:</p>
<pre>def predict(sentence):<br/>    X_in = []<br/>    for word in sentence.split():<br/>        try:<br/>            X_in.append(dictionary_from[word])<br/>        except:<br/>            X_in.append(PAD)<br/>            pass<br/>        <br/>    test, seq_x = pad_sentence_batch([X_in], PAD)<br/>    input_batch = np.zeros([batch_size,seq_x[0]])<br/>    input_batch[0] =test[0] <br/>        <br/>    log = sess.run(tf.argmax(model.logits,2), <br/>                                      feed_dict={<br/>                                              model.X:input_batch,<br/>                                              model.X_seq_len:seq_x,<br/>                                              model.Y_seq_len:seq_x<br/>                                              }<br/>                                      )<br/>    <br/>    result=' '.join(rev_dictionary_to[i] for i in log[0])<br/>    return result</pre>
<p>When the model was trained for the first 50 epochs, we had the following result:</p>
<pre>&gt;&gt; predict('where do you live')<br/>&gt;&gt; i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD<br/><br/>&gt;&gt; print predict('how are you ?')<br/>&gt;&gt; i am PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</pre>
<p><span>When the model was trained for 1,136 epochs:</span></p>
<pre>&gt;&gt; predict('where do you live')<br/>&gt;&gt; miami florida PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD<br/><br/>&gt;&gt; print predict('how are you ?')<br/>&gt;&gt; i am fine thank you PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</pre>
<p>Well! That's impressive, right? Now your model is not just able to understand the context, but can also generate answers word by word.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, <span>we covered basic RNN cells, LSTM cells, and the seq2seq model in building a language model that can be used for multiple NLP tasks. We implemented a chatbot, from scratch, to answer questions by generating a sequence of words from the provided dataset.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The experience in this exercise demonstrates the value of LSTM as an often necessary component of the RNN. With the LSTM, we were able to see the following improvements over past CNN models:</p>
<ul>
<li>The LSTM was able to preserve state information</li>
<li>The length of sentences for both inputs and outputs could be variable and different</li>
<li> The LSTM was able to adequately handle complex context</li>
</ul>
<p><span>Specifically, in this chapter, we did the following:</span></p>
<ul>
<li>Gained an intuition about the RNN and its primary forms</li>
<li>Implemented a language model using RNN</li>
<li>Learned about the LSTM model</li>
<li>Implemented the LSTM language model and compared it to the RNN</li>
<li>Implemented<span> an encoder-decoder RNN based on the LSTM unit for a simple sequence-to-sequence question-answer task</span></li>
</ul>
<p>With the right training data, it would be possible to use this model to achieve the <span>goal of the</span> hypothetical client (the restaurant chain) of building a robust chatbot (in combination with other computational linguistic technologies that we've explored) that could automate the over-the-phone food ordering process.</p>
<p>Well done!</p>


            </article>

            
        </section>
    </body></html>