<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Deep Learning Environments</h1>
                </header>
            
            <article>
                
<p><span>Welcome to the applied AI deep-learning team, and to our first project—<em>Building a Common Deep Learning Environment</em>! We're excited about the projects we've assembled in this book. The foundation of a common working environment will help us work together and learn very cool and powerful <strong>deep learning</strong> (<strong>DL</strong>) technologies, such as <strong>computer vision</strong> (<strong>CV</strong>) and <strong>natural language processing</strong> (<strong>NLP</strong>), that you will be able to use in your professional career as a data scientist.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Components in building a common DL environment</li>
<li>Setting up a local DL environment</li>
<li>Setting up a DL environment in the cloud</li>
<li>Using the cloud for deployment for DL applications</li>
<li>Automating the setup process to reduce errors and get started quickly</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a common DL environment</h1>
                </header>
            
            <article>
                
<p>Our main goal to achieve by the end of this chapter is to standardize the toolsets to work together and achieve consistently accurate results.</p>
<p>In the process of building applications using DL algorithms that can also scale for production, it's very important to have the right kind of setup, whether local or on the cloud, to make things work end to end. So, in this chapter, we will learn how to set up a DL environment that we will be using to run all the experiments and finally take the AI models into production.</p>
<div class="mce-root packt_tip">First, we will discuss the major components required to code, build, and deploy the DL models, then various ways to do this, and finally, look at a few code snippets that will help to automate the whole process.</div>
<p>The following is the list of required components that we need to build DL applications:</p>
<ul>
<li>Ubuntu 16.04 or greater</li>
<li>Anaconda Package </li>
<li>Python 2.x/3.x</li>
<li>TensorFlow/Keras DL packages</li>
<li>CUDA for GPU support</li>
<li>Gunicorn for deployment at scale</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get focused and into the code!</h1>
                </header>
            
            <article>
                
<p>We'll start by setting up your local DL environment. Much of the work that you'll do can be done on local machines. But with large datasets and complex model architectures, processing time slows down dramatically. This is why we are also setting up a DL environment in the cloud, because the processing time for these complex and repetitive calculations just becomes too long to be able to efficiently get things done otherwise. </p>
<p>We will work straight through the preceding list, and by the end (and with the help of a bit of automated script), you'll have everything set up!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL environment setup locally</h1>
                </header>
            
            <article>
                
<p><span>Throughout this book, </span>we will be using Ubuntu OS to run all the experiments, because there is great community support for Linux and mostly any DL application can be set up easily on Linux. For any assistance on installation and setup related to Ubuntu, please refer to the tutorials at <a href="https://tutorials.ubuntu.com/" target="_blank">https://tutorials.ubuntu.com/</a>. On top of that, this book will use the Anaconda package with Python 2.7+ to write our code, train, and test. Anaconda comes with a huge list of pre-installed Python packages, such as <kbd>numpy</kbd>, <kbd>pandas</kbd>, <kbd>sklearn</kbd>, and so on, which are commonly used in all kinds of data science projects. </p>
<div class="mce-root packt_tip"><span class="packt_screen">Why do we need Anaconda? Can't we use Vanilla Python?</span><br/>
Anaconda is a generic bundle that contains iPython Notebook, editor, and lots of Python libraries preinstalled, which saves a lot of time on setting up everything. With Anaconda, we can quickly get started on solving the data science problem, instead of configuring the environment.<br/>
But, yes, you can use the default Python—it's totally the reader's choice, and we will learn at the end of this chapter how to configure <kbd>python env</kbd> using script<q>.</q></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading and installing Anaconda</h1>
                </header>
            
            <article>
                
<p>Anaconda is a very popular data science platform for people using Python to build machine learning and DL models, and deployable applications. The Anaconda marketing team put it best on their <em>What is Anaconda?</em> page, available at <a href="https://www.anaconda.com/what-is-anaconda/" target="_blank">https://www.anaconda.com/what-is-anaconda/</a>. To install Anaconda, perform the following steps:</p>
<ol>
<li>Click <span class="packt_screen">Anaconda</span> on the menu, then click <span class="packt_screen">Downloads</span> to go to the download page at <a href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a></li>
<li>Choose the download suitable for your platform (Linux, OS X, or Windows):
<ol>
<li>Choose Python 3.6 version*</li>
<li>Choose the Graphical Installer</li>
</ol>
</li>
<li>Follow the instructions on the wizard, and in 10 to 20 minutes, your Anaconda environment (Python) setup will be ready</li>
</ol>
<p>Once the installation process is completed, you can use following command to check the Python version on your Terminal:</p>
<pre><strong>python -V</strong></pre>
<p><span>You should see the following output:</span></p>
<div class="crayon-pre">
<pre class="crayon-line"><strong>Python 3.6 :: Anaconda<span class="s1">,Inc.<br/></span></strong></pre>
<p class="crayon-line"><span class="s1"><span>If the command does not work, or returns an error, please check the documentation for help for your platform. </span></span></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing DL libraries</h1>
                </header>
            
            <article>
                
<p>Now, let's install the Python libraries used for DL, specifically, TensorFlow and Keras.</p>
<div class="mce-root packt_tip"><span class="packt_screen">What is TensorFlow?</span><br/>
TensorFlow is a Python library developed and maintained by Google. You can implement many powerful machine learning and DL architectures in custom models and applications using TensorFlow. To find out more, visit <a href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>.</div>
<p>Install the TensorFlow DL library (for all OS except Windows) by typing the following command:</p>
<pre><strong>conda install -c conda-forge tensorflow</strong></pre>
<p>Alternatively, you may choose to install using <kbd>pip</kbd> and a specific version of <span>TensorFlow</span> for your platform, using the following command:</p>
<pre><strong>pip install tensorflow==1.6</strong></pre>
<p>You can find the installation instructions for <span>TensorFlow at <a href="https://www.tensorflow.org/get_started/os_setup#anaconda_installation" target="_blank">https://www.tensorflow.org/get_started/os_setup#anaconda_installation</a></span>. </p>
<p>Now we will install <kbd>keras</kbd> using the following command:</p>
<pre><strong>pip install keras</strong></pre>
<p>To validate the environment and the version of the packages, let's write the following script, which will print the version <span>numbers of each library:</span></p>
<pre># Import the tensorflow library<br/>import tensorflow<br/># Import the keras library<br/>import keras<br/><br/><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'tensorflow: %s'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">tensorflow</span><span class="crayon-sy">.</span><span class="crayon-v">__version__</span><span class="crayon-sy">)</span><br/>print('keras: %s' % keras.__version__)</pre>
<p>Save the script as<span> </span><kbd>dl_versions.py</kbd>. Run the script by typing the following command:</p>
<pre><strong>python dl_version.py</strong></pre>
<p>You should see the following output:</p>
<pre><strong>tensorflow: 1.6.0</strong><br/><strong>Using TensorFlow backend.</strong><br/><strong>keras: 2.1.5</strong></pre>
<p>Voila! Now our <span>Python development environment is ready for us </span>to write some awesome DL applications in our local.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a DL environment in the cloud</h1>
                </header>
            
            <article>
                
<p>All the steps we performed up to now remain the same for the cloud as well, but there are a few additional modules required to configure the cloud virtual machines to make your DL applications servable and scalable. So, before setting up your server, follow the instructions from the preceding section. </p>
<p>To deploy your DL applications in the cloud, you will need a server good enough to train your models and serve at the same time. With huge development in the sphere of DL, the need for cloud servers to practice and deploy projects has increased drastically, and so have the options on the market. The following is a list of some of the best options on offer:</p>
<ul>
<li><span>Paperspace (<a href="https://www.paperspace.com/" target="_blank">https://www.paperspace.com/</a>)</span></li>
<li>FloydHub (<a href="https://www.floydhub.com" target="_blank">https://www.floydhub.com</a>)</li>
<li>Amazon Web Services (<a href="https://aws.amazon.com/" target="_blank">https://aws.amazon.com/</a>)</li>
<li>Google Cloud Platform (<a href="https://cloud.google.com/" target="_blank">https://cloud.google.com/</a>)</li>
<li>DigitalOcean (<a href="https://cloud.digitalocean.com/" target="_blank">https://cloud.digitalocean.com/</a>)</li>
</ul>
<p>All of these options have their own pro and cons, and the final choice totally depends on your use case and preferences, so feel free to explore more. In this book, we will build and deploy our models mostly on <strong>Google Compute Engine</strong> (<strong>GCE</strong>), which is a part of <strong>Google Cloud Platform</strong> (<strong>GCP</strong>). Follow the steps <span>mentioned </span>in this chapter to spin up a VM server and get started.</p>
<div class="packt_tip packt_infobox">Google has released an internal notebook platform, <strong>Google Colab </strong><span>(<a href="https://colab.research.google.com/" target="_blank">https://colab.research.google.com/</a>),</span> which is pre-installed with all the DL packages and other Python libraries. You can write all of your ML/DL applications on the Google Cloud, leveraging free GPUs for 10 hours.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud platforms for deployment </h1>
                </header>
            
            <article>
                
<p>The main idea behind this book is to empower you to build and deploy DL applications. In this section, we will discuss some critical components required to make your applications accessible to millions of users.</p>
<p>The best way to make your application accessible is to expose it as a web service, using REST or SOAP APIs<span>. To do so, we have many P</span>ython web frameworks to choose from,<span> such as <kbd>web.py</kbd>, Flask, Bottle, and many more. These frameworks allow us to easily build web services and deploy them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prerequisites</h1>
                </header>
            
            <article>
                
<p>You should have a Google Cloud (<a href="https://cloud.google.com/" target="_blank">https://cloud.google.com/</a>) account. Google is promoting the usage of its platform right now, and is giving away $300 dollars of credit and 12 months as a free tier user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the GCP</h1>
                </header>
            
            <article>
                
<p>Follow these instructions to set up your GCP:</p>
<ol>
<li class="CDPAlignCenter CDPAlign">
<p><strong>Creating a new project</strong>: Click on the three dots, as shown in the following screenshot, and then click on the + sign to create a new project:</p>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7425a6cf-6f2f-4d79-b59b-b1e947ce3c68.png" style="width:38.67em;height:3.50em;"/></p>
<ol start="2">
<li>
<p><strong><span>Spinning a VM instance</span></strong><span>:</span><strong><span> </span></strong>Click on the three lines on the upper-left corner of the screen, select the <span class="packt_screen">compute</span> option, and click on <span class="packt_screen">Compute Engine</span>. Now choose <span class="packt_screen">Create new instance</span>. Name the VM instance, and select your zone as <span class="packt_screen">us-west2b</span>. Choose the <span class="packt_screen">machine type</span> size. </p>
<p>Choose your boot disk as <span class="packt_screen">Ubuntu 16.04 LTS</span>. In firewall options<span>,</span> choose both the <span class="packt_screen">http</span> and <span class="packt_screen">https</span> option (it's important to make it accessible from the outer world). To opt for GPU options, you can click on <span class="packt_screen">customize</span> button, and find the GPU options. You can choose between two NVIDIA GPUs. Check both <span class="packt_screen">Allow HTTP traffic</span> and <span class="packt_screen">Allow HTTPS traffic</span>.</p>
<p>Now click on <span class="packt_screen">Create</span>. Boom! your new VM is getting ready.</p>
</li>
<li>
<p><strong>Modify the firewall settings</strong>: Now click on the <span class="packt_screen">Firewall rules</span> setting under <span class="packt_screen">Networking</span>. Under Protocols and Ports, we need to select the port that we will use to export our APIs. We have chosen <kbd>tcp:8080</kbd> as our port number. Now click on the <span class="packt_screen">Save</span> button. This will assign a rule in the firewall of your VM to access the applications from the external world.</p>
</li>
<li>
<p><strong>Boot your VM</strong>: Now start your VM instance. When you see the green tick, click on <span class="packt_screen">SSH</span>—this will open a command window, and you are <span>now</span><span> </span>inside the VM. You can also use <kbd>gcloud cli</kbd> to log in and access your VMs.</p>
</li>
<li>
<p><span>Then follow the same steps as we performed to set up the local environment, or read further to learn how to create an automation script that will perform all the setup automatically.</span></p>
</li>
</ol>
<p><span>Now we need a web framework to write our DL applications as web services—again, there are lots of options, but to make it simple, we will be using a combination of</span> <kbd>web.py</kbd> and Gunicorn<span>.</span></p>
<div class="packt_tip"><span>If you want to know which web framework to choose based on memory consumption, CPU utilization, and so on, you can have a look at the comprehensive list of benchmarks at </span><a href="http://klen.github.io/py-frameworks-bench" target="_blank">http://klen.github.io/py-frameworks-bench</a>.</div>
<p>Let's install them using following commands:</p>
<pre><strong>pip install web.py</strong><br/><strong>pip install gunicorn</strong></pre>
<p>Now we are ready to deploy our DL solution as a web service, and scale it to production level.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automating the setup process</h1>
                </header>
            
            <article>
                
<p>Installing<span> of Pyth</span><span>on package</span><span>s and DL libra</span><span>ries can be a tedious process, requiring lots of ti</span><span>me and repetitive effort. So, to ease the job, </span><span>we will create a bash script that can be used to install everything using a single command. </span></p>
<p>The following is a list of components that will get installed and configured:</p>
<ul>
<li>Java 8</li>
<li>Bazel<span> </span>for building</li>
<li>Python and associated dependencies</li>
<li>TensorFlow</li>
<li>Keras</li>
<li>Git</li>
<li>Unzip</li>
<li>Dependencies for all of the aforementioned services (see the script for exact details)</li>
</ul>
<p><span>You can simply download the automation script to your server or locally, execute it, and you're done. Here are the steps to follow:</span></p>
<ol>
<li>Save the script to your home directory, by cloning the code from the repository:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/PacktPublishing/Python-Deep-Learning-Projects<br/></strong></pre>
<ol start="2">
<li class="mce-root">Once you have the copy of the complete repository, move to the <kbd>Chapter01</kbd> folder, which will contain a script file named <kbd>setupDeepLearning.sh</kbd>. This is the script that we will execute to start the setup process, but, before execution, we will have to make it executable using the <kbd>chmod</kbd> command:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd Python-Deep-Learning-Projects/Chapter01/</strong><br/><strong>chmod +x setupDeepLearning.sh</strong></pre>
<ol start="3"/>
<ol start="3">
<li>Once this is done, we are ready to execute it as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>./setupDeepLearning.sh</strong></pre>
<p>Follow any instructions that appear (basically, say <kbd>yes</kbd> to everything and accept Java's license). It should take about 10 to 15 minutes to install everything. Once it has finished, you will see the list of Python packages being installed, as shown in the following screenshot:</p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/79783458-7d6a-48fa-b306-16eaadbc7fe0.png" style="width:42.42em;height:45.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Listed packages with TensorFlow and other Python dependencies</div>
<p>There are a couple of other options, too, such as getting Docker images from TensorFlow and other DL packages, which can set up <span>fully functional DL machines for large-scale and production-ready environments. You can find out more about Docker at <a href="https://www.docker.com/what-docker" target="_blank">https://www.docker.com/what-docker</a>. </span>Also, for a quick-start guide, follow the instructions on this repository for an <span>all-in-one Docker image for DL at <a href="https://github.com/floydhub/dl-docker">https://github.com/floydhub/dl-docker</a></span>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we worked to get the team set up in a common environment with a standardized toolset. We are looking to deploy our project applications by utilizing Gunicorn and CUDA. Those projects will rely on highly advanced and effective DL libraries, such as TensorFlow and Keras running in Python 2.x/3.x. We'll write our code using the resources in the Anaconda package, and all of this will be running on Ubuntu 16.04 or greater.</p>
<p>Now we are all set to perform experiments and deploy our DL models in production!</p>


            </article>

            
        </section>
    </body></html>