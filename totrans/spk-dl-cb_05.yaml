- en: Predicting Fire Department Calls with Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the San Francisco fire department calls dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the target variable of the logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing feature variables for the logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the accuracy of the logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification models are a popular way to predict a defined categorical outcome.
    We use outputs from classification models all the time. Anytime we go to see a
    movie in a theatre, we are interested to know whether the film is considered correct?
    One of the most popular classification models in the data science community is
    a logistic regression. The logistic regression model produces a response that
    is activated by a sigmoid function. The sigmoid function uses the inputs from
    the model and produces an output that is between 0 and 1\. That output is usually
    in a form of a probability score. Many deep learning models are also used for
    classification purposes. It is common to find logistic regression models performed
    in conjunction with deep learning models to help establish a baseline in which
    deep learning models are measured against. The sigmoid activation function is
    one of many activation functions that are also used in deep neural networks within
    deep learning to produce a probability output. We will utilize the built-in machine
    learning libraries within Spark to build a logistic regression model that will
    predict whether an incoming call to the San Francisco Fire department is actually
    related to a fire, rather than another incident.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the San Francisco fire department calls dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The City of San Francisco does a great job of collecting fire department calls
    for services across their area. As it states on their website, each record includes
    the call number, incident number, address, unit identifier, call type, and disposition.
    The official website containing San Francisco fire department call data can be
    found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is some general information regarding the dataset with regards to the
    number of columns and rows, seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc1781db-55cb-4a5e-b3a3-b822a672bbf1.png)'
  prefs: []
  type: TYPE_IMG
- en: This current dataset, updated on 3/26/2018, has roughly 4.61 M rows and 34 columns.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is available in a `.csv` file and can be downloaded locally on to
    your machine, where it can then be imported into Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will walk through the steps to download and import the `.csv` file
    to our Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset from the website by selecting Export and then CSV, as
    seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b1f89e2e-1edf-4936-bb0e-090ddf5bdb0d.png)'
  prefs: []
  type: TYPE_IMG
- en: If not already the case, name the downloaded dataset `Fire_Department_Calls_for_Service.csv`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save the dataset to any local directory, although ideally it should be saved
    to the same folder that contains the Spark notebook that will be used in this
    chapter, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f4654852-1dd6-4f0f-8b99-9d28a57a58cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the dataset has been saved to the same directory as the notebook, execute
    the following `pyspark` script to import the dataset into Spark and create a dataframe
    called `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is saved to the same directory that houses the Jupyter notebook
    for ease of import into the Spark session.
  prefs: []
  type: TYPE_NORMAL
- en: A local `pyspark` session is initialized by importing `SparkSession` from `pyspark.sql`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A dataframe, `df`, is created by reading in the CSV file with the options `header
    = 'true'` and `inferschema = 'true'`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, it is always ideal to run a script to show the data that has been
    imported into Spark through the dataframe to confirm that the data has made its
    way through. The outcome of the script, showing the first two rows of the dataset
    from the San Francisco fire department calls, can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf42a5ae-52b0-4108-9588-afb2deacfb74.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that as we read the file into spark, we are using `.load()` to pull
    the `.csv` file into the Jupyter notebook. This is fine for our purposes as we
    are using a local cluster, but would not work if we were leveraging a cluster
    from Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is accompanied by a data dictionary that defines the headers for
    each of the 34 columns. This data dictionary can be accessed from the same website
    through the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://data.sfgov.org/api/views/nuek-vuh3/files/ddb7f3a9-0160-4f07-bb1e-2af744909294?download=true&filename=FIR-0002_DataDictionary_fire-calls-for-service.xlsx](https://data.sfgov.org/api/views/nuek-vuh3/files/ddb7f3a9-0160-4f07-bb1e-2af744909294?download=true&filename=FIR-0002_DataDictionary_fire-calls-for-service.xlsx)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The San Francisco government website allows for online visualization of the
    data, which can be used to do some quick data profiling. The visualization application
    can be accessed on the website by selecting the Visualize dropdown, as seen in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c564a36-8237-41fc-901c-1d15b50f97c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Identifying the target variable of the logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A logistic regression model operates as a classification algorithm aiming to
    predict a binary outcome. In this section, we will specify the best column within
    the dataset to predict whether an incoming call to the operator is related to
    fire or non-fire incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will visualize many of the data points in this section, which will require
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that `matplotlib` is installed by executing `pip install matplotlib`
    at the command line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running `import matplotlib.pyplot as plt` as well as ensuring graphs are viewed
    within cells by running `%matplotlib inline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, there will be some manipulation of functions within `pyspark.sql`
    that requires `importing functions as F`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will walk through visualizing the data from the San Francisco Fire
    Department.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to get a cursory identification of the unique
    values in the `Call Type Group` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are five main categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Alarm`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Potentially Life-threatening`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Non Life-threatening`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Fire`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`null`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unfortunately, one of those categories is `null` values. It would be useful
    to get a row count of each unique value to identify how many null values there
    are in the dataset. Execute the following script to generate a row count of each
    unique value for the column `Call Type Group`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, there are over 2.8 M rows of data that do not have a `Call Type
    Group` associated with them. That is over 60 percent of the available rows of
    4.6 M. Execute the following script to view the imbalance of null values in a
    bar chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Another indicator may need to be chosen to determine a target variable. Instead,
    we can profile `Call Type` to identify calls associated with fire versus all other
    calls. Execute the following script to profile `Call Type`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There do not appear to be any `null` values, as there were with `Call Type
    Group`. There are 32 unique categories for `Call Type`; therefore, it will be
    used as the target variable for fire incidents. Execute the following script to
    tag the columns containing `Fire` in`Call Type`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to retrieve the distinct counts of `Fire Indicator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to add the `Fire Indicator` column to the original
    dataframe, `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add the `fireIndicator` column has to the dataframe, `df`, and confirm
    by executing the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the key steps to building a successful logistic regression model is
    establishing a binary target variable that will be used as the prediction outcome.
    This section walks through the logic behind selecting our target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data profiling of potential target columns is performed by identifying the
    unique column values of `Call Type Group`. We can view the unique values of the
    `Call Type Group` column, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d3ade658-22c0-4fa4-9a1d-ebae6ad1ecdc.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal is to identify whether there are any missing values within the `Call
    Type Group` column and what can be done with those missing values. Sometimes,
    the missing values in the columns can just be dropped, and other times they are
    manipulated to populate values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows how many null values are present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bca41dd4-acd7-4c95-971a-543d5a0dcbed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we can also plot how many `null` values are present to get a
    better visual sense of the abundance of values, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8834a412-6660-40fe-92be-abf4bcf63aa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Since there are over 2.8 M rows that are missing from `Call Type Group`, as
    seen in the `df.groupBy` script as well as the bar chart, it doesn't make sense
    to drop all of those values, as that is over 60 percent of the total row count
    from the dataset. Therefore, another column will need to be chosen as the target
    indicator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While profiling the `Call Type` column, we find that there aren''t any null
    rows in the 32 unique possible values. This makes `Call Type` a better candidate
    for the target variable for the logistic regression model. The following is a
    screenshot of the `Call Type` column profiled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1977a0ab-f4a3-4174-9ea1-ff28ba2df5dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since logistic regression works best when there is a binary outcome, a new
    column is created using the `withColumn()` operator in the `df` dataframe to capture
    an indicator (0 or 1) as to whether a call is affiliated with a fire-related incident
    or a non-fire-related incident. The new column is called `fireIndicator` and can
    be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/580f9ef9-2299-4aae-8164-c174ba2c31db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can identify how prevalent fire calls are compared to the rest of the calls
    by doing a `groupBy().count()`, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d0b58340-59f9-418d-8d1b-cc62349a62e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is best practice to confirm that the new column has been attached to the
    existing dataframe by executing the `printSchema()` script of the newly modified
    dataframe. The output of the new schema can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b34b4286-dfcf-4827-a2a3-4ce95aeea7ff.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There were a couple of column manipulations done with the `pyspark.sql` module
    in this section. The `withColumn()` operator returns a new dataframe or modifies
    an existing dataframe by adding a new column or modifies an existing column of
    the same name. This is not to be confused with the `withColumnRenamed()` operator,
    which also returns a new dataframe, but by modifying the name of an existing column
    to a new column. Finally, we needed to perform some logical operations to convert
    values associated with `Fire` to 0 and without `Fire` to 1\. This required using
    the `pyspark.sql.functions` module and incorporating the `where` function as an
    equivalent to a case statement used in SQL. The function created a case statement
    equation using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The outcome of the new dataset for both columns, `Call Type` and `fireIndicator`,
    appear as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f49b4cf-6efd-4c04-ab4f-df0623e76d72.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to learn more about the `pyspark.sql` module available within Spark,
    visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing feature variables for the logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we identified our target variable that will be used
    as our predictor for fire calls in our logistic regression model. This section
    will focus on identifying all of the features that will best help the model identify
    what the target should be. This is known as **feature selection**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will require importing `StringIndexer` from `pyspark.ml.feature`.
    In order to ensure proper feature selection, we will need to map string columns
    to columns of indices. This will help generate distinct numeric values for categorical
    variables that will provide ease of computation for the machine learning model
    to ingest the independent variables used to predict the target outcome.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will walk through the steps to prepare the feature variables for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to update the dataframe, `df`, by only selecting
    the fields that are independent of any fire indicators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to identify any null values within the dataframe and remove
    them if they exist. Execute the following script to identify the row count with
    any null values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 16,551 rows with missing values. Execute the following script to
    update the dataframe to remove all rows with null values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to retrieve the updated target count of `fireIndicator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `StringIndexer` class from `pyspark.ml.feature` to assign numeric
    values to each categorical variable for the features, as seen in the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Python list for all the feature variables that will be used in the
    model using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to specify the output column format, `outputcol`,
    that will be `stringIndexed` from the list of features from the input column,
    `inputcol`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to create a `model` that will be used to `fit`
    the input columns and produce the newly defined output columns to the existing
    dataframe, `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to define a final selection of the features in
    the dataframe, `df`, that will be used for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will explain the logic behind the steps in preparing the feature
    variables for our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Only the indicators in the dataframe that are truly independent of an indication
    of fire are selected to contribute to the logistic regression model that will
    predict the outcome. The reason this is performed is to remove any potential bias
    in the dataset that may already reveal the outcome of the prediction. This minimizes
    human interaction with the final outcome. The output of the updated dataframe
    can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9f0de7d8-271d-4563-960e-425ba8cbba19.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that the column `Neighborhooods - Analysis of Boundaries` is originally
    misspelled from the data we extract. We will continue to use the misspelling for
    the rest of the chapter for continuity purposes. However, the column name can
    be renamed using the `withColumnRenamed()` function in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final selection of columns are chosen as the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`fireIndicator`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Zipcode of Incident`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Battalion`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Station Area`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Box`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of Alarms`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Unit sequence in call dispatch`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Neighborhooods - Analysis Boundaries`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fire Prevention District`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Supervisor District`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These columns are selected to avoid data leakage in our modeling. Data leakage
    is common in modeling and can lead to invalid predictive models because it can
    incorporate features that are directly a result of the outcome we are trying to
    predict. Ideally, we wish to incorporate features that are truly independent of
    the outcome. There are several columns that appeared to be leaky and, hence, are
    removed from our dataframe and model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All rows with missing or null values are identified and removed in order to
    get the very best performance out of the model without overstating or understating
    key features. An inventory of the rows with missing values can be calculated and
    shown to be 16,551, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9df0dab7-e80a-4d5c-84fc-67811558ce32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can get a look at the frequency of calls that are fire-related versus those
    that are not, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32117958-6e01-4ba3-9c54-bf04129c5181.png)'
  prefs: []
  type: TYPE_IMG
- en: '`StringIndexer` is imported to help convert several of the categorical or string
    features into numerical values for ease of computation within the logistic regression
    model. The input of the features needs to be in a vector or array format, which
    is ideal for numeric values. A list of all the features that will be used in the
    model can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cb62409e-94dc-4357-a5ac-e7127ff20596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An indexer is built for each of the categorical variables specifying the input
    (`inputCol`) and output (`outputCol`) columns that will be used in the model. Each
    column in the dataframe is adjusted or transformed to rebuild a new output with
    the updated indexing, ranging from 0 to the maximum value of the unique count
    of that specific column. The new column is appended with `_Index` at the end.
    While the updated column is created, the original column is still available in
    the dataframe, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/df67a60c-19da-4691-9903-e81d215ad749.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can look at one of the newly created columns and compare it with the original
    to see how the strings have been converted to numeric categories. The following
    screenshot shows how `Neighborhooods - Analysis Boundaries` compares with `Neighborhooods -
    Analysis Boundaries_Index`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5b9432fe-0906-4632-808a-fd882682c1c5.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataframe is then trimmed down to incorporate only the numerical values
    and remove the original categorical variables that were transformed. The non-numerical
    values no longer serve a purpose from a modeling perspective and are dropped from
    the dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The new columns are printed out to confirm that each value type of the dataframe
    is either double precision or integer, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/61837070-2d38-48e9-92a3-e09a8984a90d.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A final look at the newly modified dataframe will reveal only numerical values,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c63a287-61af-4a3f-a311-d051054791e1.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to learn more about `StringIndexer`, visit the following website: [https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer).
  prefs: []
  type: TYPE_NORMAL
- en: Applying the logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stage is now set to apply the model to the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will focus on applying a very common classification model called
    **logistic regression**, which will involve importing some of the following from
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will walk through the steps of applying our model and evaluating
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to lump all of the feature variables in the dataframe
    in a list called `features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following to import `VectorAssembler` and configure the fields
    that will be assigned to the feature vector by assigning the `inputCols` and `outputCol`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to apply `VectorAssembler` to the dataframe with
    the `transform` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the dataframe to remove all of the columns except for `fireIndicator`
    and `features`, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify the dataframe to rename `fireIndicator` to `label`, as seen in the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the entire dataframe, `df`, into training and test sets in a 75:25 ratio,
    with a random seed set as `12345`, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `LogisticRegression` library from `pyspark.ml.classification` and
    configure to incorporate the `label` and `features` from the dataframe, and then
    fit on the training dataset, `trainDF`, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the test dataframe, `testDF`, to apply the logistic regression model.
    The new dataframe with the scores from the prediction is called `df_predicted`,
    as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the logic behind the steps in applying our model and evaluating
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models work best when all of the features are combined in a single
    vector for training purposes. Therefore, we begin the vectorization process by
    collecting all of the features into a single list called `features`. Since our
    label is the first column of the dataframe, we exclude it and pull in every column
    after as a feature column or feature variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vectorization process continues by converting all of the variables from
    the `features` list into a single vector output to a column called `features`.
    This process requires importing `VectorAssembler` from `pyspark.ml.feature`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Applying `VectorAssembler` transforms the dataframe by creating a newly added
    column called `features`, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dfe3ef28-1df5-4b4a-b749-2b9483542ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, the only columns that are necessary for us to use in the model
    are the label column, `fireIndicator`, and the `features` column. All of the other
    columns can be dropped from the dataframe as they will no longer be needed for
    modeling purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additionally, to help with the logistic regression model, we will change the
    column called `fireIndicator` to `label`. The output of the `df.show()` script
    can be seen in the following screenshot with the newly renamed columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/313f52f6-30e8-4e7a-a968-ff5152d5c2a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to minimize overfitting the model, the dataframe will be split into
    a testing and training dataset to fit the model on the training dataset, `trainDF`,
    and test it on the testing dataset, `testDF`. A random seed of `12345` is set
    to keep the randomness consistent each time the cell is executed. We can identify
    the row counts for the data split, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2d20098a-229b-4a3b-9e7e-d3d6d4efa513.png)'
  prefs: []
  type: TYPE_IMG
- en: A logistic regression model, `LogisticRegression`, is then imported from `pyspark.ml.classification`
    and configured to input the appropriate column names from the dataframe associated
    with the features and the label. Additionally, the logistic regression model is
    assigned to a variable called `logreg` that is then fit to train our data set,
    `trainDF`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A new dataframe, `predicted_df`, is created based on the transformation of
    the test dataframe, `testDF`, once the logistic regression model is scored on
    it. The model creates three additional columns for `predicted_df`, based on the
    scoring. The three additional columns are `rawPrediction`, `probability`, and
    `prediction`, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/889efec5-2794-45f2-af1d-f0fe21429c79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the new columns in `df_predicted` can be profiled, as seen in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8181ca5d-64f1-45b0-9b82-d487ad69dc1d.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important thing to keep in mind because it may initially come off as being
    counter-intuitive is that our probability threshold is set at 50 percent in our
    dataframe. Any call with a probability of 0.500 and above is given a prediction
    of 0.0 and any call with a probability of less than 0.500 is given a prediction of
    1.0\. This was set during the pipeline development process and as long as we are
    aware of what the threshold is along with how the prediction is allocated, we
    are in good shape.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about `VectorAssembler`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/ml-features.html#vectorassembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the accuracy of the logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to evaluate the performance of predicting whether a call was
    correctly classified as a fire incident.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform the model analysis which will require importing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from sklearn import metrics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to evaluate the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a confusion matrix using the `.crosstab()` function, as seen in the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `metrics` from `sklearn` to help measure accuracy using the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create two variables for the `actual` and `predicted` columns from the dataframe
    that will be used to measure accuracy, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the accuracy prediction score using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the model performance is evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compute the accuracy of our model, it is important to be able to
    identify how accurate our predictions were. Often, this is best visualized using
    a confusion matrix cross table that shows correct and incorrect prediction scores.
    We create a confusion matrix using the `crosstab()` function off the `df_predicted` dataframe
    that shows us we have 964,980 true negative predictions for labels that are 0
    and we have 48,034 true positive predictions for labels that are 1, as seen in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3b89bb6d-d661-4cec-92ba-d02a0849e8e4.png)'
  prefs: []
  type: TYPE_IMG
- en: We know from earlier in this section that there are a total of 1,145,589 rows
    from the `testDF` dataframe; therefore, we can calculate the accuracy of the model
    using the following formula: *(TP + TN) / Total*. The accuracy would then be 88.4
    percent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note that not all false scores are created equal. For example,
    it is more detrimental to classify a call as not relating to fire and ultimately
    have it be related to fire than vice-versa from a fire safety perspective. This
    is referred to as a false negative. There is a metric that accounts for a **false
    negative** (**FN**), known as **recall**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While we can work out the accuracy manually, as seen in the last step, it is
    ideal to have the accuracy automatically calculated. This can be easily performed
    by importing `sklearn.metrics`, which is a module that is commonly used for scoring
    and model evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sklearn.metrics` takes in two parameters, the actual results that we have
    for labels and the predicted values we derived from the logistic regression model.
    Therefore, two variables are created, `actual` and `predicted`, and an accuracy
    score is calculated using the `accuracy_score()` function, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3370b494-009d-495f-80ee-685f447efd81.png)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy score is the same as we calculated manually, 88.4 percent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now know that our model was able to accurately predict whether a call coming
    in is related to fire or not at a rate of 88.4 percent. At first, this may sound
    like a strong prediction; however, it''s always important to compare this to a
    baseline score where every call was predicted as a non-fire call. The predicted
    dataframe, `df_predicted`, had the following breakdown of labels `1` and `0`,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6db3def2-822e-4727-bcf2-3ed64295f3a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can run some statistics on that same dataframe to get the mean of label
    occurrences of value `1` using the `df_predicted.describe(''label'').show()` script.
    The output of that script can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66f86b60-b8b9-4a6e-a447-a40fbde76ba4.png)'
  prefs: []
  type: TYPE_IMG
- en: A base model has a prediction value of `1` at a rate of 14.94 percent, or in
    other words, it has a prediction rate of *100 - 14.94* percent, which is 85.06
    percent for a value of 0\. Therefore, since 85.06 percent is less than the model
    prediction rate of 88.4 percent, this model provides an improvement over a blind
    guess as to whether a call is fire-related or not.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about accuracy vs. precision, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mathsisfun.com/accuracy-precision.html](https://www.mathsisfun.com/accuracy-precision.html)'
  prefs: []
  type: TYPE_NORMAL
