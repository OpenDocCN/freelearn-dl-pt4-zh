<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring the Learning Environment Landscape - Roboschool, Gym-Retro, StarCraft-II, DeepMindLab</h1>
                
            
            <article>
                
<p class="calibre2">You have come a long way in your quest to get hands-on experience in building intelligent agents to solve a variety of challenging problems. In the previous chapters, we looked into several environments that are available in OpenAI Gym. In this chapter, we will look beyond the Gym and look at some of the other well developed environments that you can use to train your intelligent agents or run experiments.</p>
<p class="calibre2">Before we look at other open source libraries that provide good learning environments for developing intelligent agents, let's have a look at a recent class of environments added to the OpenAI Gym library. If, like me, you are interested in robotics, you will like this one a lot. Yes! It is the robotics class of environments, which provides very useful environments for<span class="calibre5"> robotic manipulation tasks such as fetching, sliding, pushing, and so on</span><span class="calibre5"> with a robotic arm. These robotics environments are based on the MuJoCo engine and you may recall from <a href="part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 3</a></span><a href="part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">,</a><span class="calibre5"> <em class="calibre13">Getting Started with OpenAI Gym and Deep Reinforcement Learning</em>, that the MuJoCo engine requires a paid license, unless you are a student and using MuJoCo for personal or class use. A summary of these robotics environments is shown in the following screenshot, with the environment names and a brief description for each, so that you can check them out if you are interested in exploring such problems:</span></p>
<div class="cdpaligncenter"><img src="../images/00290.jpeg" class="calibre100"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Gym interface-compatible environments</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will have a deeper look into environments that are compatible with the Gym interface out of the box. You should be able to use any of the agents we developed in the previous chapters in these environments. Let's get started and look at a few very useful and promising learning environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Roboschool</h1>
                
            
            <article>
                
<p class="calibre2">Roboschool<a href="https://github.com/openai/roboschool" class="calibre9"> (https://github.com/openai/roboschool)</a> provides several environments for controlling robots in simulation. It was released by OpenAI and the environments have the same interface as the OpenAI Gym environments that we have been using in this book. The Gym's MuJoCo-based environments offer a rich variety of robotic tasks, but MuJoCo requires a license for use after the free trial. Roboschool provides eight environments that quite closely match the MuJoCo ones, which is a good news as it offers a free alternative. Apart from these eight environments, Roboschool also offers several new and challenging environments.</p>
<p class="calibre2">The following table shows a quick comparison between the MuJoCo Gym environments and the Roboschool environments:</p>
<table border="1" class="calibre101">
<tbody class="calibre36">
<tr class="calibre102">
<td class="calibre103"><strong class="calibre1">Brief description</strong></td>
<td class="calibre104"><strong class="calibre1">MuJoCo environment</strong></td>
<td class="calibre105"><strong class="calibre1">Roboschool environment</strong></td>
</tr>
<tr class="calibre106">
<td class="calibre107">
<p class="calibre2"><span class="calibre5">Make a one-legged </span><span class="calibre5">2D</span><span class="calibre5"> </span><span class="calibre5">robot hop forward as fast as possible</span></p>
</td>
<td class="calibre108">
<p class="calibre2">Hopper-v2</p>
<p class="calibre2"><img src="../images/00291.jpeg" class="calibre109"/></p>
</td>
<td class="calibre110">
<p class="calibre2">RoboschoolHopper-v1</p>
<p class="calibre2"><img src="../images/00292.jpeg" class="calibre111"/></p>
</td>
</tr>
<tr class="calibre112">
<td class="calibre113">
<p class="calibre2"><span class="calibre5">Make a 2D robot walk</span></p>
</td>
<td class="calibre114">
<p class="calibre2">Walker2d-v2</p>
<p class="calibre2"><img src="../images/00293.jpeg" class="calibre115"/></p>
</td>
<td class="calibre116">
<p class="calibre2">RoboschoolWalker2d-v1</p>
<p class="calibre2"><img src="../images/00294.jpeg" class="calibre117"/></p>
</td>
</tr>
<tr class="calibre118">
<td class="calibre119"><span>Make a four-legged </span>3D robot walk<span><br class="title-page-name"/></span>
<p class="calibre2"/>
</td>
<td class="calibre120">
<p class="calibre2">Ant-v2</p>
<p class="calibre2"><img src="../images/00295.jpeg" class="calibre121"/></p>
</td>
<td class="calibre122">
<p class="calibre2">RoboschoolAnt-v1</p>
<p class="calibre2"><img src="../images/00296.jpeg" class="calibre123"/></p>
</td>
</tr>
<tr class="calibre124">
<td class="calibre125">
<p class="calibre2"><span class="calibre5">Make a bipedal 3D robot walk forward as fast as possible without falling</span></p>
</td>
<td class="calibre126">
<p class="calibre2">Humanoid-v2</p>
<p class="calibre2"><img src="../images/00297.jpeg" class="calibre127"/></p>
</td>
<td class="calibre128">
<p class="calibre2">RoboschoolHumanoid-v1</p>
<p class="calibre2"><img src="../images/00298.jpeg" class="calibre129"/></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">A full list of environments that are available as part of the Roboschool library, with their state and action spaces, is provided in the following table <span class="calibre5">for your quick reference</span><span class="calibre5">:</span></p>
<table border="1" class="calibre130">
<thead class="calibre131">
<tr class="calibre37">
<th class="calibre132">
<div class="cdpalignleft">Env ID</div>
</th>
<th class="calibre133">
<div class="cdpalignleft">Roboschool env</div>
</th>
<th class="calibre134">
<div class="cdpalignleft">obs space</div>
</th>
<th class="calibre135">
<div class="cdpalignleft">action space</div>
</th>
</tr>
</thead>
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre136">RoboschoolInvertedPendulum-v1</td>
<td class="calibre137"><img src="../images/00299.jpeg" class="calibre138"/></td>
<td class="calibre139">Box(5,)</td>
<td class="calibre140">Box(1,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolInvertedPendulumSwingup-v1</td>
<td class="calibre137"><img src="../images/00300.jpeg" class="calibre138"/></td>
<td class="calibre139">Box(5,)</td>
<td class="calibre140">Box(1,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolInvertedDoublePendulum-v1</td>
<td class="calibre137"><img src="../images/00301.jpeg" class="calibre141"/></td>
<td class="calibre139">Box(9,)</td>
<td class="calibre140">Box(1,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolReacher-v1</td>
<td class="calibre137"><img src="../images/00302.jpeg" class="calibre142"/></td>
<td class="calibre139">Box(9,)</td>
<td class="calibre140">Box(2,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolHopper-v1</td>
<td class="calibre137"><img src="../images/00303.jpeg" class="calibre143"/></td>
<td class="calibre139">Box(15,)</td>
<td class="calibre140">Box(3,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolWalker2d-v1</td>
<td class="calibre137"><img src="../images/00304.jpeg" class="calibre142"/></td>
<td class="calibre139">Box(22,)</td>
<td class="calibre140">Box(6,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolHalfCheetah-v1</td>
<td class="calibre137"><img src="../images/00305.jpeg" class="calibre144"/></td>
<td class="calibre139">Box(26,)</td>
<td class="calibre140">Box(6,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolAnt-v1</td>
<td class="calibre137"><img src="../images/00306.jpeg" class="calibre144"/></td>
<td class="calibre139">Box(28,)</td>
<td class="calibre140">Box(8,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolHumanoid-v1</td>
<td class="calibre137"><img src="../images/00307.jpeg" class="calibre142"/></td>
<td class="calibre139">Box(44,)</td>
<td class="calibre140">Box(17,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolHumanoidFlagrun-v1</td>
<td class="calibre137"><img src="../images/00308.jpeg" class="calibre142"/></td>
<td class="calibre139">Box(44,)</td>
<td class="calibre140">Box(17,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolHumanoidFlagrunHarder-v1</td>
<td class="calibre137"><img src="../images/00309.jpeg" class="calibre145"/></td>
<td class="calibre139">Box(44,)</td>
<td class="calibre140">Box(17,)</td>
</tr>
<tr class="calibre37">
<td class="calibre136">RoboschoolPong-v1</td>
<td class="calibre137"><img src="../images/00310.jpeg" class="calibre144"/></td>
<td class="calibre139">Box(13,)</td>
<td class="calibre140">Box(2,)</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quickstart guide to setting up and running Roboschool environments</h1>
                
            
            <article>
                
<p class="calibre2">Roboschool environments make use of the open source Bulletphysics engine instead of the proprietary MuJoCo engine. Let's quickly have a look at a Roboschool environment so that you know how to use any environment from the Roboschool library if you happen to find it useful for your work. To get started, we will have to first install the Roboschool Python library in our <kbd class="calibre12">rl_gym_book</kbd> conda environment. Because the library depends on several components, including the Bulletphysics engine, there are several installation steps involved, which are listed in the official Roboschool GitHub repository here: <a href="https://github.com/openai/roboschool" class="calibre9">https://github.com/openai/roboschool</a>. To make things simpler, you can use the script in the book's code repository at <kbd class="calibre12">ch9/setup_roboschool.sh </kbd>to automatically compile and install the <kbd class="calibre12">Roboschool</kbd> library for you. Follow these steps to run the script:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Activate the <kbd class="calibre12">rl_gym_book</kbd> conda environment using <kbd class="calibre12">source activate rl_gym_book</kbd>.</li>
<li value="2" class="calibre11">Navigate to the <kbd class="calibre12">ch9</kbd> folder with <kbd class="calibre12">cd ch9</kbd>.</li>
<li value="3" class="calibre11">Make sure that the script's execution bit is set as <kbd class="calibre12">chmod a+x setup_roboschool.sh</kbd>.</li>
<li value="4" class="calibre11">Run the script with <kbd class="calibre12">sudo</kbd>:<kbd class="calibre12">./setup_roboschool.sh</kbd>.</li>
</ol>
<p class="calibre2">This should install the required system dependencies, fetch and compile a compatible source code of the bullet3 physics engine; pull the Roboschool source code to the <kbd class="calibre12">software</kbd> folder under your home directory; and finally compile, build, and install the Roboschool library in the <kbd class="calibre12">rl_gym_book</kbd> conda environment. If the setup completes successfully, you will see the following message printed on the console:</p>
<pre class="calibre17">Setup completed successfully. You can now import roboschool and use it. If you would like to \test the installation, you can run: python ~/roboschool/agent_zoo/demo_race2.py"</pre>
<p class="calibre2">You can run a quickstart demo script using the following command:</p>
<pre class="calibre17"><strong class="calibre1">`(rl_gym_book) praveen@ubuntu:~$ python ~/roboschool/agent_zoo/demo_race2.py`</strong></pre>
<p class="calibre2">This will launch a funny-to-watch robo-race in which you will see a hopper, half-cheetah, and humanoid running a race! The interesting aspect is that each of the robots is being controlled by a reinforcement learning-based trained policy. The race will look similar to this snapshot:</p>
<div class="cdpaligncenter"><img src="../images/00311.jpeg" class="calibre146"/></div>
<p class="calibre2">Once it has been installed, you can create a Roboschool environment and use one of the agents we developed in an earlier chapter to train and run on these environments. </p>
<p class="calibre2">You can use the <kbd class="calibre12">run_roboschool_env.py</kbd> script in this chapter's code repository at  <a href="https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9" class="calibre9">https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9</a> to check out any of the Roboschool environments. For example, to check out the <kbd class="calibre12"><span>RoboschoolInvertedDoublePendulum-v1</span></kbd> environment, you can run the following script:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$python run_roboschool_env.py --env RoboschoolInvertedDoublePendulum-v1</strong></pre>
<p class="calibre2">You can use any other Roboschool environment name from the previous table, as well as new Roboschool environments when they are made available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Gym retro</h1>
                
            
            <article>
                
<p class="calibre2">Gym Retro (<a href="https://github.com/openai/retro" class="calibre9">https://github.com/openai/retro</a>) is a relatively new (released on May 25, 2018) Python library released by OpenAI (<a href="https://blog.openai.com/gym-retro/" class="calibre9">https://blog.openai.com/gym-retro/</a>) as a research platform for developing reinforcement learning algorithms for game playing. Although the Atari suite of 60+ games was available in OpenAI Gym, the total number of games available was limited. Gym Retro supports the use of games developed for several console/retro gaming platforms, such as Nintendo's NES, SNES, Game Boy consoles, Sega Genesis, and Sega Master System to name a few. This is made possible with the use of emulators using the Libretro API:</p>
<div class="cdpaligncenter"><img src="../images/00312.jpeg" class="calibre147"/></div>
<p class="calibre2">Gym Retro provides convenient wrappers to turn more than 1,000 such video games into Gym interface-compatible learning environments! Isn't that great! Several new learning environments but with the same interface, so that we can easily train and test the agents we have developed so far without any necessary changes to the code...</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">To get a feel for how easy it is to use the environments in Gym Retro, let's put the installation steps aside for a moment and quickly look at the code to create a new Gym R<span class="calibre5">etro</span><span class="calibre5"> </span><span class="calibre5">environment</span> <span class="calibre5">once it is installed:</span></p>
<pre class="calibre17"><span>import</span> retro<br class="title-page-name"/><span>env</span> <span>=</span> <span>retro</span><span>.</span><span>make</span><span>(</span><span>game</span><span>=</span><span>'Airstriker-Genesis'</span><span>,</span> <span>state</span><span>=</span><span>'Level1'</span><span>)</span></pre>
<p class="calibre2">This code snipped will create an <kbd class="calibre12">env</kbd> object that has the same interfaces and methods, such as <kbd class="calibre12">step(...)</kbd>, <kbd class="calibre12">reset()</kbd> and <kbd class="calibre12">render()</kbd>, as all the Gym environments we have seen before.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quickstart guide to setup and run Gym Retro</h1>
                
            
            <article>
                
<p class="calibre2">Let's try out the Gym Retro library quickly by installing the pre-built binaries using pip with the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch9$ pip install gym-retro</strong></pre>
<p class="calibre2">Once the installation is successful, we can have a sneak peek into one of the available Gym Retro environments using the following script:</p>
<pre class="calibre17"><span>#!/usr/bin/env python<br class="title-page-name"/>import</span> <span>retro</span>
<br class="title-page-name"/><span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span><span>:<br class="title-page-name"/></span><span>   env</span> <span>=</span> <span>retro</span><span>.</span><span>make</span><span>(</span><span>game</span><span>=</span><span>'Airstriker-Genesis'</span><span>,</span> <span>state</span><span>=</span><span>'Level1'</span><span>)</span>
    <span>obs</span> <span>=</span> <span>env</span><span>.</span><span>reset</span><span>()</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>obs</span><span>,</span> <span>rew</span><span>,</span> <span>done</span><span>,</span> <span>info</span> <span>=</span> <span>env</span><span>.</span><span>step</span><span>(</span><span>env</span><span>.</span><span>action_space</span><span>.</span><span>sample</span><span>())</span>
        <span>env</span><span>.</span><span>render</span><span>()</span>
        <span>if</span> <span>done</span><span>:</span>
            <span>obs</span> <span>=</span> <span>env</span><span>.</span><span>reset</span><span>()</span></pre>
<p class="calibre2">Running this script will bring up a window with the Airstriker game and show the spaceship taking random actions. The game window will look something like this:</p>
<div class="cdpaligncenter"><img src="../images/00313.jpeg" class="calibre148"/></div>
<p class="calibre2">Before we move on, one thing to note is that the <strong class="calibre4">ROM</strong> (<strong class="calibre4">Read-Only Memory</strong>) file that contains the whole game data is not made freely available for all games. The ROMs for some non-commercial console games such as Airstriker (used in the previous script), Fire, Dekadrive, Automaton, Fire, Lost Marbles, and so on are included with the Gym Retro library and are free to use. Other games, such as the Sonic series (Sonic The Hedgehog, Sonic The Hedgehog 2, Sonic 3 &amp; Knuckles) require ROMs to be purchased for legal use from places such as <a href="https://store.steampowered.com/app/71113/Sonic_The_Hedgehog/" class="calibre9">Steam</a>. This is a barrier for hobbyists, students, and other enthusiasts who wish to develop algorithms using such environments. But at least this barrier is relatively small, as it costs about USD 1.69 on Steam for the Sonic The Hedgehog ROM. Once you have the ROM files for the games, the Gym Retro library provides a script to import them into the library like so:</p>
<pre class="calibre17"><strong class="calibre1"><span>(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch9$ python -m retro.import /PATH/TO/YOUR/ROMs/DIRECTORY<br class="title-page-name"/></span>OpenAI Universe<span><br class="title-page-name"/></span></strong></pre>
<p class="calibre2">Note that when creating a new Gym Retro environment, we need the name of the game as well as the state of the game <kbd class="calibre12"><span>retro</span><span>.</span><span>make</span><span>(</span><span>game</span><span>=</span><span>'NAME_OF_GAME'</span><span>,</span> <span>state</span><span>=</span><span>'NAME_OF_STATE'</span><span>)</span></kbd></p>
<p class="calibre2">To get a list of available Gym Retro environments, you can run the following command:</p>
<pre class="calibre17"><span><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch9$ python -c "import retro; retro.list_games()"</strong><br class="title-page-name"/></span></pre>
<p class="calibre2">And to get a list of available game states, you can run the following Python script:</p>
<pre class="calibre17">#!/usr/bin/evn python<br class="title-page-name"/><span>import</span> retro
<span>for</span> game <span>in</span> retro.list_games():
    <span>print</span>(game, retro.list_states(game))</pre>
<p class="calibre2">So far, we have gotten ourselves familiar with the Gym Retro library. Let's analyze what advantages or new features this library provides us in addition to what we have already seen and used. First, the Gym Retro library utilizes newer games consoles (such as the SEGA Genesis) than the Atari console. For comparison, the SEGA Genesis games console has 500 times as much RAM as the Atari console, enabling better visuals and a greater range of controls. This provides us with learning environments that are relatively sophisticated, and some more complex tasks and challenges for our intelligent agents to learn and solve. Second, several of these console games are progressive in nature, where the complexity of the game typically increases with every level and the levels have several similarities in some aspects (such as the goal, object appearances, physics, and so on) while also providing diversity in other aspects (such as the layout, new objects, and so on). Such a training environment with levels of progressively increasing difficulty help in developing intelligent agents that can learn to solve tasks in general without being very task/environment-specific (such as overfitting in supervised learning). Agents can learn to transfer their skills and learning from one level to another, and then to another game. This area is under active research and is usually known as curriculum learning, staged learning, or incremental evolution. After all, ultimately we are interested in developing intelligent agents that can learn to solve tasks in general and not just the specific tasks the agent was trained on. The Gym Retro library provides some useful, though game-only, environments to facilitate such experiments and research.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Other open source Python-based learning environments</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will discuss recent Python-based learning environments that provide a good platform for intelligent agent development but don't necessarily have a Gym-compatible environment interface. Although they do not provide Gym-compatible interfaces, the environments we will be discussing in this section were carefully selected to make sure that either a Gym wrapper (to make it compatible with the Gym interface) is available, or they are easy to implement in order to use and experiment with the agents we have developed through this book. As you can guess, this list of good Python-based learning environments for developing intelligent agents will grow in the future, as this area is being very actively researched at the moment. The book's code repository will have information and quickstart guides for new environments as they become available in the future. Sign up for update notifications at the book's GitHub repository to get those updates. In the following sub-sections, we will discuss some of the most promising learning environments that are readily available for use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">StarCraft II - PySC2</h1>
                
            
            <article>
                
<p class="calibre2">StarCraft II is very popular, in fact one of the most successful real-time strategy games ever, and is played by millions of people worldwide. It even has a world championship league (<a href="https://wcs.starcraft2.com/en-us/" class="calibre9">https://wcs.starcraft2.com/en-us/</a>)! The environment is quite complex and the main goal is to build an army base, manage an economy, defend the base, and destroy enemies. The player controls the base camp and the army from a third-person view of the scene. If you are not familiar with StarCarft, you should watch a few games online to get a feel for how complex and fast-paced the game is.</p>
<p class="calibre2">For humans to play this real-time strategy game well, it takes a lot of practice <span class="calibre5">(several months even; in fact, professional players train for several years)</span><span class="calibre5">, planning, and quick responses. Although software agents can press several software buttons per frame to make pretty fast moves, speed of action is not the only factor that contributes to victory. The agent has to multi-task and micro-manage </span><span class="calibre5">the army units, and maximize their score, which is several orders of magnitude more complex than the Atari games. </span></p>
<p class="calibre2">Blizzard, the company that made StarCraft II, released the StarCraft II API, which provides the necessary hooks to interface with the StarCraft II game and control it without limitations. That enables several new possibilities, such as the development of the intelligent agents that we are after. They even have a separate <strong class="calibre4">End User License Agreement</strong> (<strong class="calibre4">EULA</strong>) for open use of the environment under an AI and machine learning license! This was a very welcome move by a company such as Blizzard, which is in the business of making and selling games. The open source the <strong class="calibre4">StarCraft2</strong> (<strong class="calibre4">SC2</strong>) client protocol implementation and provides the Linux installation package, along with several accessories such as the map packs, free to download from their GitHub page at <a href="https://github.com/Blizzard/s2client-proto" class="calibre9">https://github.com/Blizzard/s2client-proto</a>. On top of that, Google DeepMind has open sourced their PySC2 library, which exposes the SC2 client interfaces through Python and provides a wrapper that makes it an RL environment. </p>
<p class="calibre2">The following screenshot shows the PySC2 UI, with the feature layers available as observations to the agents shown on the right and a simplified overview of the game scene on the left:</p>
<p class="cdpaligncenter4"><img src="../images/00314.jpeg" class="calibre149"/></p>
<div class="packt_infobox">If you are interested in these types of environment, and especially <span class="packt_screen">if you are a game developer, you might be interested in the Dota 2 environment as well. Dota 2 is a real-time strategy game, like StarCraft II, and is played between two teams of five players with each player controlling a hero character. You can learn more about how OpenAI developed a team of five neural network-based agents that have learned to work in a team, played 180 years' worth of games in a day, learned to overcome several challenges (including high-dimensional and continuous state and action spaces, and long-term horizons), all while playing against themselves using self-play! You can read more about the five-agent team at https://blog.openai.com/openai-five/.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quick start guide to setup and run StarCraft II PySC2 environment</h1>
                
            
            <article>
                
<p class="calibre2">We will look at how you can quickly set up and get started with the StarCraft II environment. As always, use the README files in the code repository for the latest up-to-date instructions, as things as the links and the versions may change. If you have not done so already, star and watch the book's code repository to get notified about changes and updates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Downloading the StarCraft II Linux packages</h1>
                
            
            <article>
                
<p class="calibre2">Download the latest Linux packages for the StarCraft game from https://github.com/Blizzard/s2client-proto#downloads and extract it onto your hard disk at <kbd class="calibre12">~/StarCraftII</kbd>. For example, to download version 4.1.2 to your <kbd class="calibre12">~/StarCraftII/</kbd> folder, you can use the following command:</p>
<pre class="calibre17"><strong class="calibre1">wget http://blzdistsc2-a.akamaihd.net/Linux/SC2.4.1.2.60604_2018_05_16.zip -O ~/StarCraftII/SC2.4.1.2.zip</strong></pre>
<p class="calibre2">Let's unzip and extract the files to the <kbd class="calibre12">~/StarCraftII/</kbd> directory:</p>
<pre class="calibre17"><strong class="calibre1">unzip ~/StarCraftII/SC2.4.1.2.zip -d ~/StarCraftII/</strong></pre>
<p class="calibre2">Note that, as mentioned on the download page, the files are password protected with the password <kbd class="calibre12"><span>'iagreetotheeula</span></kbd>.</p>
<p class="calibre2">By typing that, Blizzard ensures we agree to be bound by the terms of their AI and machine learning license, found on the download page.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Downloading the SC2 maps</h1>
                
            
            <article>
                
<p class="calibre2">We will need the StarCraft II map packs and the mini games pack to get started.</p>
<p class="calibre2">Download the map packs from <a href="https://github.com/Blizzard/s2client-proto#map-packs" class="calibre9">https://github.com/Blizzard/s2client-proto#map-packs</a></p>
<p class="calibre2">Extract them to your <kbd class="calibre12">~/StarCraftII/Maps</kbd> directory.</p>
<p class="calibre2">As an example, let's download the Ladder maps released for 2018 season 2 using the following command:</p>
<pre class="calibre17"><strong class="calibre1">wget http://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2018Season2_Updated.zip -O ~/StarCraftII/Maps/Ladder2018S2.zip</strong></pre>
<p class="calibre2">Let's unzip the maps to the <kbd class="calibre12">~/StarCraftII/Maps</kbd> directory:</p>
<pre class="calibre17"><strong class="calibre1">unzip ~/StarCraftII/Maps/Ladder2018S2.zip -d ~/StarCraftII/Maps/</strong></pre>
<p class="calibre2">Next, we will download and unzip the mini game map files:</p>
<pre class="calibre17"><strong class="calibre1">wget https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip -O ~/StarCraftII/Maps/mini_games1.2.zip</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">unzip ~/StarCraftII/Maps/mini_games1.2.zip -d ~/StarCraftII/Maps</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Installing PySC2</h1>
                
            
            <article>
                
<p class="calibre2">Let's install the PySC2 library for the RL environment<span class="calibre5"> interface, </span>along with the required dependencies. <span class="calibre5">This step is going to be straightforward, as there is a PyPi Python package for the PySC2 library</span>:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ pip install pysc2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Playing StarCraftII yourself or running sample agents</h1>
                
            
            <article>
                
<p class="calibre2">To test whether the installation went fine and to see what the StarCarftII learning environment looks like, you can quickly start up a randomly-acting agent on the Simple64 map or the CollectMineralShards map using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ python -m pysc2.bin.agent --map Simple64</strong></pre>
<p class="calibre2">You can also load another available map for the environment. For example, the following command loads the CollectMineralShards map:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ python -m pysc2.bin.agent --map C<span>ollectMineralShards</span></strong></pre>
<p class="calibre2">This should bring up a UI showing you the actions taken by the random agent, which gives you an idea of what the valid actions are and helps you to visualize what is going on in the environment as the agent is acting.</p>
<p class="calibre2">To play the game yourself, PySC2 offers a human agent interface, which is quite useful for debugging (and, if you are interested, playing!) purposes. The following is the command to run and play the game yourself:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ python -m pysc2.bin.play --map Simple64</strong></pre>
<p class="calibre2">You can also run a sample agent that is scripted to collect mineral shards, which is one of the tasks in the game, using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ python -m pysc2.bin.agent --map CollectMineralShards --agent pysc2.agents.scripted_agent.CollectMineralShards</strong></pre>
<p class="calibre2">Watch the book's code repository for new agent source code and instructions to train and test new agents with advanced skills. You can also customize the agents we developed in the previous chapter to learn to play StarCraftII. If you do, send a pull request to the book's code repository, shoot the author an email, or shout it out so that everyone knows what cools things you have done!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">DeepMind lab</h1>
                
            
            <article>
                
<p class="calibre2">DeepMind Lab (<a href="https://github.com/deepmind/lab" class="calibre9">https://github.com/deepmind/lab</a>) is a 3D learning environment that provides a suite of environments with challenging tasks, such as 3D navigation through mazes and puzzle solving. It is built based on a handful of pieces of open source software, including the famous Quake III Arena.</p>
<p class="calibre2">The environment interface is very similar to the Gym interface that we have used extensively in this book so far. To get a feel for what the environment interface actually looks like, have a look at the following code snippet:</p>
<pre class="calibre17"><span>import deepmind_lab<br class="title-page-name"/>num_steps = 1000<br class="title-page-name"/>config = {  <br class="title-page-name"/>    'width': 640,<br class="title-page-name"/>    'height': 480,<br class="title-page-name"/>    'fps': 30<br class="title-page-name"/>}<br class="title-page-name"/>...<br class="title-page-name"/>env </span><span>=</span><span> deepmind_lab.Lab(level, [</span><span><span>'</span>RGB_INTERLEAVED<span>'</span></span><span>], </span><span>config</span><span>=</span><span>config, renderer='software')<br class="title-page-name"/><br class="title-page-name"/>for step in range(num_steps) <br class="title-page-name"/>if done:<br class="title-page-name"/></span>    env.reset()<br class="title-page-name"/>obs = env.observations()<br class="title-page-name"/>action = agent.get_action(...)<br class="title-page-name"/>reward = env.step(action, num_steps=1)<br class="title-page-name"/>done = not env.is_running()</pre>
<p class="calibre2">This code, though not one-to-one compatible with the OpenAI Gym interface, provides a very similar interface. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">DeepMind Lab learning environment interface</h1>
                
            
            <article>
                
<p class="calibre2">We will briefly discuss the environment interface for <strong class="calibre4">DeepMind Lab (DM Lab)</strong>, so that you are familiar with it, can see the similarities with the OpenAI Gym interface, and can start experimenting with agents in DM Lab environments!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">reset(episode=-1, seed=None)</h1>
                
            
            <article>
                
<p class="calibre2">This is similar to the <kbd class="calibre12">reset()</kbd> method we saw in the Gym interface, but unlike Gym environments, DM Lab's <kbd class="calibre12">reset</kbd> method call does not return the observation. We will see later how to get observations, so for now, we will discuss DM Lab's <kbd class="calibre12">reset(episode=-1, seed=None)</kbd> method. It resets the environment to an initial state and needs to be called at the end of every episode, in order for a new episode to be created. The optional <kbd class="calibre12">episode</kbd> <span class="calibre5">argument </span><span class="calibre5">takes an integer value to specify the level in a specific episode. If the </span><kbd class="calibre12">episode</kbd> <span class="calibre5">value is not set, or is set to </span><kbd class="calibre12">-1</kbd><span class="calibre5">, the levels are loaded in numerical order. The </span><kbd class="calibre12">seed</kbd> <span class="calibre5">argument is also optional and is used to seed the environment's random number generator for reproducibility purposes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">step(action, num_steps=1)</h1>
                
            
            <article>
                
<p class="calibre2">This is similar to the Gym interface's <kbd class="calibre12">step(action)</kbd> method, but like with the <kbd class="calibre12">reset(...)</kbd> method, the call to this method does not return the next observation (or reward, done, and info). Calling this method advances the environment by <kbd class="calibre12">num_steps</kbd> number of frames, executing the action defined by <kbd class="calibre12">action</kbd> in every frame. This action-repeat behavior is useful in cases where we would like the same action to be applied for four or so consecutive frames, which was actually found<span class="calibre5"> </span><span class="calibre5">by several researchers</span><span class="calibre5"> to help with learning. There are Gym environment wrappers that accomplish this action-repeat behavior. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">observations()</h1>
                
            
            <article>
                
<p class="calibre2">This is the method we would use after the call to <kbd class="calibre12">reset(...)</kbd> or <kbd class="calibre12">step(action)</kbd> to receive the observations from DM Lab environments. This method returns a Python dictionary object with every type of observation that we specified from the list of available types for the environment. For example, if we wanted <strong class="calibre4">RGBD</strong> (<strong class="calibre4">Red-Green-Blue-Depth</strong>) information about the environment as the observation type, we can specify that when we initialize the environment using the <kbd class="calibre12">'RGBD'</kbd> key, we can then retrieve this information from the returned observation dictionary using the same <kbd class="calibre12">'RGBD'</kbd> key. A simple example to illustrate this is shown here:</p>
<pre class="calibre17">env = deepmind_lab.Lab('tests/empty_room_test', ['RGBD'])<br class="title-page-name"/>env.reset()<br class="title-page-name"/>obs = env.observations()['RGBD']</pre>
<p class="calibre2">There are also other observation types that are supported by DM Lab environments. We can use <kbd class="calibre12">observation_spec()</kbd> to get a list of supported observation types, which we will discuss very shortly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">is_running()</h1>
                
            
            <article>
                
<p class="calibre2">This method is analogous (in the opposite sense) to the <kbd class="calibre12">done</kbd> Boolean returned by the Gym interface's <kbd class="calibre12">step(action)</kbd> method.</p>
<p class="calibre2">This method will return <kbd class="calibre12">False</kbd> when the environment is done with an episode or stops running. It will return <kbd class="calibre12">True</kbd> as long as the environment is running.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">observation_spec()</h1>
                
            
            <article>
                
<p class="calibre2">This method is similar to <kbd class="calibre12">env.observation_space()</kbd>, which we used with the Gym environments. This method returns a list specifying all the available observations supported by the DM Lab environment. It also includes specifications about level-dependent custom observations.</p>
<p class="calibre2">The specifications contain the name, type, and shape of the tensor or string that will be returned if that specification name is specified in the observation list (such as the <kbd class="calibre12">'RGBD'</kbd> example previously). For example, the following code snippet lists two items in the list that will be returned to give you an idea about what the specs contain:</p>
<div class="title-page-name">
<pre class="calibre17">{<br class="title-page-name"/><span><span>    '</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, ## Array data type<br class="title-page-name"/>    <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGBD<span>'</span></span>,                <span>## Name of observation.</span><br class="title-page-name"/>    <span><span>'</span>shape<span>'</span></span>: (<span>4</span>, <span>180</span>, <span>320</span>)         ## shape of the array. (Heights, Width, Colors)<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>{    <br class="title-page-name"/>    <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGB_INTERLEAVED<span>'</span></span>, <span>## Name of observation.<br class="title-page-name"/></span>    <span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span>## Data type array.</span>     <br class="title-page-name"/>    <span><span>'</span>shape<span>'</span></span>: (<span>180</span>, <span>320</span>, <span>3</span>) <span>## Shape of array. (Height, Width, Colors)<br class="title-page-name"/></span>}</pre></div>
<p class="calibre2">To quickly understand how this method can be used, let's look at the following lines of code and the output:</p>
<pre class="calibre17">import deepmind_lab<br class="title-page-name"/>import pprint<br class="title-page-name"/>env <span>=</span> deepmind_lab.Lab(<span><span>'</span>tests/empty_room_test<span>'</span></span>, [])
observation_spec <span>=</span> env.observation_spec()
pprint.pprint(observation_spec)
<span># Outputs:</span>
[{<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGB_INTERLEAVED<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>180</span>, <span>320</span>, <span>3</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGBD_INTERLEAVED<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>180</span>, <span>320</span>, <span>4</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGB<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>3</span>, <span>180</span>, <span>320</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>RGBD<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>4</span>, <span>180</span>, <span>320</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>BGR_INTERLEAVED<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>180</span>, <span>320</span>, <span>3</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.uint8<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>BGRD_INTERLEAVED<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>180</span>, <span>320</span>, <span>4</span>)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>MAP_FRAME_NUMBER<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>1</span>,)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>VEL.TRANS<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>3</span>,)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>VEL.ROT<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>3</span>,)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>str<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>INSTR<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: ()},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>DEBUG.POS.TRANS<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>3</span>,)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>DEBUG.POS.ROT<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>3</span>,)},
 {<span><span>'</span>dtype<span>'</span></span>: <span>&lt;</span><span>type</span> <span><span>'</span>numpy.float64<span>'</span></span><span>&gt;</span>, <span><span>'</span>name<span>'</span></span>: <span><span>'</span>DEBUG.PLAYER_ID<span>'</span></span>, <span><span>'</span>shape<span>'</span></span>: (<span>1</span>,)},
<span># etc...</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">action_spec()</h1>
                
            
            <article>
                
<p class="calibre2">Similar to the <kbd class="calibre12">observation_spec()</kbd> the <kbd class="calibre12">action_spec()</kbd> method returns a list containing the min, max, and a name for each of the elements in the space.  The <kbd class="calibre12">min</kbd> and <kbd class="calibre12">max</kbd> values respectively represent the minimum and maximum value that the corresponding element in the action space can be set to. The length of this list will equal the dimension/shape of the action space. This is analogous to <kbd class="calibre12">env.action_space</kbd>, which we have been using with the Gym environments.</p>
<p class="calibre2">The following code snippet gives us a quick look into what the return values from a call to this method will look like:</p>
<pre class="calibre17">import deepmind_lab<br class="title-page-name"/>import pprint<br class="title-page-name"/><br class="title-page-name"/>env <span>=</span> deepmind_lab.Lab(<span><span>'</span>tests/empty_room_test<span>'</span></span>, [])
action_spec <span>=</span> env.action_spec()
pprint.pprint(action_spec)
<span># Outputs:</span></pre>
<pre class="calibre17"><span># [{'max': 512, 'min': -512, 'name': 'LOOK_LEFT_RIGHT_PIXELS_PER_FRAME'},</span>
<span>#  {'max': 512, 'min': -512, 'name': 'LOOK_DOWN_UP_PIXELS_PER_FRAME'},</span>
<span>#  {'max': 1, 'min': -1, 'name': 'STRAFE_LEFT_RIGHT'},</span>
<span>#  {'max': 1, 'min': -1, 'name': 'MOVE_BACK_FORWARD'},</span>
<span>#  {'max': 1, 'min': 0, 'name': 'FIRE'},</span>
<span>#  {'max': 1, 'min': 0, 'name': 'JUMP'},</span>
<span>#  {'max': 1, 'min': 0, 'name': 'CROUCH'}]</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">num_steps()</h1>
                
            
            <article>
                
<p class="calibre2">This utility method is like a counter that counts the number of frames executed by the environment since the last <kbd class="calibre12">reset()</kbd> call.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">fps()</h1>
                
            
            <article>
                
<p class="calibre2">This utility method returns the number of frames (or environment steps) executed per actual <span class="calibre5">(wall-clock) second. This is useful to keep track of the environment execution speeds and how fast the agent can sample from the environment.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">events()</h1>
                
            
            <article>
                
<p class="calibre2">This utility method can be useful for debugging as it returns a list of events that have occurred since the last call to <kbd class="calibre12">reset()</kbd> or <kbd class="calibre12">step(...)</kbd>. The returned tuple contains a name and a list of observations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">close()</h1>
                
            
            <article>
                
<p class="calibre2">Like the <kbd class="calibre12">close()</kbd> method available with Gym environments, this method also closes the environment instance and releases the underlying resources, such as the Quake III Arena instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quick start guide to setup and run DeepMind Lab</h1>
                
            
            <article>
                
<p class="calibre2">With the familiarity we got after the brief discussion about the DeepMind Lab environment interface in the previous section, we are ready to get some hands on experience with this learning environment. In the following sub-sections, we will go through the steps to setup DeepMind Lab and  run a sample agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up and installing DeepMind Lab and its dependencies</h1>
                
            
            <article>
                
<p class="calibre2">The DeepMind Lab library uses Bazel as the build tool, which in turn requires Java. The book's code repository has a script that you can run to easily set up DeepMind Lab. You can find the script under the chapter9 folder at <a href="https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9" class="calibre9">https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9</a>. You can run the script using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$./setup_deepmindlab.sh</strong></pre>
<p class="calibre2">This script will take some time to finish, but will automatically install all the necessary packages and libraries, including Bazel and its dependencies, and set everything up for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Playing the game, testing a randomly acting agent, or training your own!</h1>
                
            
            <article>
                
<p class="calibre2">Once the installation is complete, you can test the game using your keyboard inputs by running the following commands:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9$ cd deepmindlab</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9/deepmindlab$ bazel run :game -- --level_script=tests/empty_room_test</strong></pre>
<p class="calibre2">You can also test it with the help of a randomly acting agent using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9/deepmindlab$ bazel run :python_random_agent --define graphics=sdl -- --length=5000</strong></pre>
<p class="calibre2">To get started with your own agent development, you can use the example agent script that was already configured to interact with the DeepMind Lab environment. You can find the script at <kbd class="calibre12">~/HOIAWOG/ch9/deepmindlab/python/random_agent.py</kbd>. To start training that agent, you can use the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch9/deepmindlab$ bazel run :python_random_agent</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we looked at several interesting and valuable learning environments, saw how their interfaces are set up, and even got hands-on with those environments using the quickstart guides for each environment and the setup scripts available in the book's code repository. We first looked at environments that have interfaces compatible with the OpenAI Gym interface that we are now very familiar with. Specifically in this category, we explored the Roboschool and Gym Retro environments.</p>
<p class="calibre2">We also looked at other useful learning environments that did not necessarily have a Gym-compatible environment interface, but had a very similar API and so it was easy to adapt our agent code or implement a wrapper around the learning environment to make it compatible with the Gym API. Specifically, we explored the famous real-time strategy game-based StarCraft II environment and the DeepMind Lab environment. We also very briefly touched upon the DOTA2 environment, which has been used to train both single agents and a team of agents, trained by OpenAI, which successfully defeated <span class="calibre5">amateur</span><span class="calibre5"> </span><span class="calibre5">human players, and even some professional gaming teams, in the DOTA 2 contest.</span></p>
<p class="calibre2">We saw the different sets of tasks and environments available in each of these learning environment libraries and tried out examples to get a feel for the environment, and also to get an idea about how we can use the agents we developed in the previous chapters to train and solve the challenging tasks in these relatively new learning environments. </p>


            </article>

            
        </section>
    </body></html>