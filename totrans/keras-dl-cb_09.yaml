- en: Research in Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at some of the active research areas in neural
    networks. The following problems are analyzed from basic research areas to complex
    real-life problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale video processing with a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition using a twisted neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding overfitting in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand the constituents of overfitting and how to avoid it in neural
    networks. Nitesh Srivastava, Geoffrey Hinton, et al. published a paper, [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf),
    in 2014, which shows cases on how to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks contain nonlinear hidden layers, and this makes them expressive
    models that can learn very complicated relationships between inputs and outputs.
    However, these complicated relationships will be the result of sampling noise.
    These complicated relationships might not exist in test data, leading to overfitting.
    Many techniques and methods have been developed to reduce this noise. These include
    stopping the training as soon as performance on a validation set starts getting
    worse, introducing weight penalties such as L1 and L2 regularization, and soft
    weight sharing (Nowlan and Hinton, 1992).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout is a technique that addresses performance issues of some of the other
    techniques such as averaging across multiple models. It also prevents overfitting
    and provides a way to combine exponentially many different neural network architectures
    efficiently. The term dropout means dropping out units (hidden and visible) in
    a neural network. By dropping a unit out, it means removing it from the network
    and its incoming and outgoing connections, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which units to be dropped is usually random. In a simple case,
    each unit is retained with a probability p independent of other units. The technique
    to choose p can be a validation set or can be set at 0.5; this value is close
    to optimal for a wide range of networks and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For the input units, however, the optimal probability of retention is usually
    closer to 1 than to 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1c5b947-8244-4601-9ba1-2c34e27a6638.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dropout neural net model:'
  prefs: []
  type: TYPE_NORMAL
- en: A standard neural network with two hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thinned neural net produced by applying dropout to the network on the left;
    crossed units have been dropped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example of how Dropout can be applied in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen above a Dropout of 0.5 is applied to the `LSTMCell`, where `output_keep_prob`:
    unit Tensor or float between 0 and 1, output keep probability; if it is constant
    and 1, no output dropout will be added.'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how the dropout strategy affects the accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3933cd47-acda-4760-89ac-019dfaf1d9c8.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, the classification error decreases significantly with dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale video processing with neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this paper, [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf),
    the authors explore how CNNs could be used for large-scale video classification.
    In this use case, the neural networks have access to not only the appearance information
    in single, static images, but also the complex temporal evolution of the image.
    There are several challenges in extending and applying CNNs in this setting.
  prefs: []
  type: TYPE_NORMAL
- en: There are very few (or none) video classification benchmarks that match the
    scale and variety of existing image datasets as videos are significantly more
    challenging to collect, annotate, and store. To obtain sufficient amount of data
    needed to train our CNN architectures, authors collected a new Sports-1M dataset.
    This dataset contains 1 million videos (from YouTube) belonging to a taxonomy
    of 487 classes of sports. Sports-1M is also available to the research community
    to support future work in this area.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, the authors treat every video as a bag of short, fixed-sized clips.
    Each clip contains several contiguous frames in time, hence the connectivity of
    the network can be extended in a time dimension to learning spatio-temporal features.
    The authors describe three broad connectivity pattern categories (**Early Fusion**,
    **Late Fusion**, and **Slow Fusion**). Afterward, we will look at a multiresolution
    architecture to address the computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure explains various techniques for fusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/529a83c9-f956-4c19-905b-cacb2c394ba2.png)'
  prefs: []
  type: TYPE_IMG
- en: Various fusion techniques to combine frames separated by time
  prefs: []
  type: TYPE_NORMAL
- en: Resolution improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors used a multiresolution architecture that aimed to strike a compromise
    by having two separate streams of processing (called Fovea and Context Streams)
    over two spatial resolutions (see the following figure). A 178 × 178 frame video
    clip is the input to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7822995-bf74-4bbe-8995-a213dd0c66f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiresolution CNN
  prefs: []
  type: TYPE_NORMAL
- en: The **context stream** receives the downsampled frames at half the original
    spatial resolution (89 × 89 pixels). The **fovea stream** receives the center
    89 × 89 region at the original resolution. In this way, the total input dimensionality
    is halved.
  prefs: []
  type: TYPE_NORMAL
- en: Feature histogram baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to comparing CNN architectures with each other, the authors also
    report the accuracy of a feature-based approach. A standard bag-of-words pipeline
    was used to extract several types of features at all frames of the videos, followed
    by discretizing them using k-means vector quantization and accumulating words
    into histograms with spatial pyramid encoding and soft quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sports-1M dataset test set results (200,000 videos and 4,000,000 clips) are
    summarized in the following table. The approach of multiple networks consistently
    and significantly outperforms the feature-based baseline. The feature-based approach
    computes visual words densely over the duration of the video and produces predictions
    that are based on the complete video-level feature vector, while the authors''
    networks only see 20 randomly sampled clips individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4133e9d9-37f1-44fd-826e-a4e6890ac9d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Results on the 200,000 videos of the Sports-1M test set. Hit@k values indicate
    the fraction of test samples that
  prefs: []
  type: TYPE_NORMAL
- en: contained at least one of the ground truth labels in the top k predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The approach taken with network topology learns well despite label noise; the
    training videos are subject to some incorrect annotations and even the correctly-labeled
    videos often contain a large amount of artifacts such as text, effects, cuts,
    and logos, none of which we attempted to filter out explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition using a twisted neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this paper, [http://www.cs.cmu.edu/~leili/pubs/lu-baylearn2015-twinet.pdf,](http://www.cs.cmu.edu/~leili/pubs/lu-baylearn2015-twinet.pdf) the
    authors look at the problem of recognizing entities in natural language. This
    is often the first step in question answering, conversations, and a host of other
    NLP use cases. For a sequence of text tokens, a named entity recognizer identifies
    chunks of tokens that belong to a predefined category of persons and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a named entity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IOB  tagging system is one of the convention for NER.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **IOB Tagging** system contains tags of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '`B-{CHUNK_TYPE}`: for the word in the **B**eginning chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I-{CHUNK_TYPE}`: for words **I**nside the chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`O`: **O**utside any chunk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`B-PERSON` : Person Entity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`B-GPE` : Geopolitical Entity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Th following text shows example of a names entities in a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, it is quite challenging due to two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Entity databases are often incomplete (given the number of new organizations
    being established)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same phrase can refer to a different entity (or none entity) depending on
    the context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining Twinet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Twisting RNNs (Twinet) use two parallel branches. Each branch is composed of
    a recurrent network layer, a nonlinear perceptron layer, and a reversed recurrent
    network layer. Branches are *twisted*: the order of the layers is reversed in
    the second branch. The output of all the recurrent layers is collected toward
    the end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, a **recurrent neural network** (**RNN**) takes a sequence of input
    vectors *x1..T*, and recurrently computes hidden states (also called **output
    labels**):'
  prefs: []
  type: TYPE_NORMAL
- en: '*h[t] = σ(U · x[t] + W · ht−1)*'
  prefs: []
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: '*t* is 1..T'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x[t]* is the external signal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* are the weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h[t-1]* is the hidden layer weights for time step *t-1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h[t]* weights being calculated for time step *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* is tan*h* layer which helps in creating weights for time step *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ(·)* is a nonlinear activation function. In the experiments that the authors
    used, we used **rectified linear units** (**RELU**).'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twinet was compared to Stanford NER and Illinois NER and the results were quite
    favorable. Here  NER stands for60; (**Named Entity Recognizer**).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e70dfc38-aaec-400c-bd50-ceeb3c45a3c7.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the preceding figure, the Precision-Recall as well as the
    F1 scores are all higher.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a new neural network topology that is gaining
    momentum in the area of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Schuster and Paliwal have introduced **Bidirectional Recurrent Neural Networks**
    (**BRNN**) in 1997\. BRNNs help increase the amount of input information available
    to the network. **Multilayer perceptrons** (**MLPs**) and **time delay neural
    networks** (**TDNNs**) are known to have limitations on the input data flexibility.
    RNNs also require their input data to be fixed. More advanced topologies like
    RNNs also have restrictions as the future input information cannot be predicted
    from the current state. BRNNs, on the contrary, do not need their input data to
    be fixed. Their future input information is reachable from the current state.
    The idea of BRNNs is to connect two hidden layers of opposite directions to the
    same output. With this structure, the output layer is able to get information
    from past and future states.
  prefs: []
  type: TYPE_NORMAL
- en: BRNNs are useful when the context of the input is needed. As an example, in
    handwriting recognition, the performance can be enhanced by knowledge of the letters
    located before and after the current letter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/807e2fd3-ea0e-4cc3-9554-d7dd4e059fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: This depicts a Bidirectional RNN
  prefs: []
  type: TYPE_NORMAL
- en: BRNN on TIMIT dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how a BRNN provides higher accuracy results
    on the TIMIT Dataset for phoneme text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'TIMIT is a corpus of phonemically and lexically transcribed speeches of American
    English speakers of different sexes and dialects. Each transcribed element has
    been delineated in time. TIMIT was designed to further acoustic-phonetic knowledge
    and automatic speech recognition systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/037a0b0a-62a7-4c7a-a18b-d76acaec0920.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the preceding figure, the BRNN gives higher percent frames
    accurately as compared to MLP, both for the training set and testing set. BLSTM
    gives even higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we addressed some of the areas where research has been done
    on improving accuracy and avoiding overfitting. We also looked at some of the
    newer areas such as video classification. While it is outside the scope of this
    book to cover all the research areas in detail, we sincerely advise you to explore
    the research websites of Google, Facebook, and Baidu, in addition to Tier 1 ACM
    and IEEE conferences, to skim through new research being done.
  prefs: []
  type: TYPE_NORMAL
