["```py\n#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\no\n\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3))\n\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(256, 256, 3))\n\n```", "```py\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\n\n```", "```py\nkeras.layers.convolutional.Conv2D(filters, kernel_size, padding='valid')\n\n```", "```py\nkeras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n\n```", "```py\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD, RMSprop, Adam\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n```", "```py\n#define the ConvNet\nclass LeNet:\n    @staticmethod\n    def build(input_shape, classes):\n         model = Sequential()\n         # CONV => RELU => POOL\n\n```", "```py\nmodel.add(Convolution2D(20, kernel_size=5, padding=\"same\",\ninput_shape=input_shape))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n# CONV => RELU => POOL\n\n```", "```py\nmodel.add(Conv2D(50, kernel_size=5, border_mode=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n```", "```py\n# Flatten => RELU layers\nmodel.add(Flatten())\nmodel.add(Dense(500))\nmodel.add(Activation(\"relu\"))\n# a softmax classifier\nmodel.add(Dense(classes))\nmodel.add(Activation(\"softmax\"))\nreturn model\n\n```", "```py\n# network and training\nNB_EPOCH = 20\nBATCH_SIZE = 128\nVERBOSE = 1\nOPTIMIZER = Adam()\nVALIDATION_SPLIT=0.2\nIMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\nNB_CLASSES = 10 # number of outputs = number of digits\nINPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)\n# data: shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nk.set_image_dim_ordering(\"th\")\n# consider them as float and normalize\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n# we need a 60K x [1 x 28 x 28] shape as input to the CONVNET\nX_train = X_train[:, np.newaxis, :, :]\nX_test = X_test[:, np.newaxis, :, :]\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n# convert class vectors to binary class matrices\ny_train = np_utils.to_categorical(y_train, NB_CLASSES)\ny_test = np_utils.to_categorical(y_test, NB_CLASSES)\n# initialize the optimizer and model\nmodel = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\nmetrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train,\nbatch_size=BATCH_SIZE, epochs=NB_EPOCH,\nverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n```", "```py\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport matplotlib.pyplot as plt\n\n# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\nIMG_CHANNELS = 3\nIMG_ROWS = 32\nIMG_COLS = 32\n\n#constant\nBATCH_SIZE = 128\nNB_EPOCH = 20\nNB_CLASSES = 10\nVERBOSE = 1\nVALIDATION_SPLIT = 0.2\nOPTIM = RMSprop()\n\n#load dataset\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n```", "```py\n# convert to categorical\nY_train = np_utils.to_categorical(y_train, NB_CLASSES)\nY_test = np_utils.to_categorical(y_test, NB_CLASSES)\n\n# float and normalization\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n\n```", "```py\n# network\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\ninput_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n```", "```py\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(NB_CLASSES))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\n```", "```py\n# train\nmodel.compile(loss='categorical_crossentropy', optimizer=OPTIM,\nmetrics=['accuracy'])\nmodel.fit(X_train, Y_train, batch_size=BATCH_SIZE,\nepochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\nverbose=VERBOSE)\nscore = model.evaluate(X_test, Y_test,\nbatch_size=BATCH_SIZE, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])\n\n```", "```py\n#save model\nmodel_json = model.to_json()\nopen('cifar10_architecture.json', 'w').write(model_json)\nAnd the weights learned by our deep network on the training set\nmodel.save_weights('cifar10_weights.h5', overwrite=True)\n\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\ninput_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(NB_CLASSES))\nmodel.add(Activation('softmax'))\n\n```", "```py\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.datasets import cifar10\nimport numpy as np\nNUM_TO_AUGMENT=5\n\n#load dataset\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# augumenting\nprint(\"Augmenting training set images...\")\ndatagen = ImageDataGenerator(\nrotation_range=40,\nwidth_shift_range=0.2,\nheight_shift_range=0.2,\nzoom_range=0.2,\nhorizontal_flip=True,\nfill_mode='nearest')\n\n```", "```py\nxtas, ytas = [], []\nfor i in range(X_train.shape[0]):\nnum_aug = 0\nx = X_train[i] # (3, 32, 32)\nx = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\nfor x_aug in datagen.flow(x, batch_size=1,\nsave_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\nif num_aug >= NUM_TO_AUGMENT:\nbreak\nxtas.append(x_aug[0])\nnum_aug += 1\n\n```", "```py\n#fit the dataget\ndatagen.fit(X_train)\n\n# train\nhistory = model.fit_generator(datagen.flow(X_train, Y_train,\nbatch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0],\nepochs=NB_EPOCH, verbose=VERBOSE)\nscore = model.evaluate(X_test, Y_test,\nbatch_size=BATCH_SIZE, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])\n\n```", "```py\nimport numpy as np\nimport scipy.misc\nfrom keras.models import model_from_json\nfrom keras.optimizers import SGD\n\n#load model\nmodel_architecture = 'cifar10_architecture.json'\nmodel_weights = 'cifar10_weights.h5'\nmodel = model_from_json(open(model_architecture).read())\nmodel.load_weights(model_weights)\n\n#load images\nimg_names = ['cat-standing.jpg', 'dog.jpg']\nimgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32, 32)),\n(1, 0, 2)).astype('float32')\nfor img_name in img_names]\nimgs = np.array(imgs) / 255\n\n# train\noptim = SGD()\nmodel.compile(loss='categorical_crossentropy', optimizer=optim,\nmetrics=['accuracy'])\n\n# predict\npredictions = model.predict_classes(imgs)\nprint(predictions)\n\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\nfrom keras.optimizers import SGD\nimport cv2, numpy as np\n\n# define a VGG16 network\ndef VGG_16(weights_path=None):\nmodel = Sequential()\nmodel.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\nmodel.add(Flatten())\n#top layer of the VGG net\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1000, activation='softmax'))\nif weights_path:\nmodel.load_weights(weights_path)\nreturn model\n\n```", "```py\nim = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)\nim = im.transpose((2,0,1))\nim = np.expand_dims(im, axis=0)\n\n# Test pretrained model\nmodel = VGG_16('/Users/gulli/Keras/codeBook/code/data/vgg16_weights.h5')\noptimizer = SGD()\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy')\nout = model.predict(im)\nprint np.argmax(out)\n\n```", "```py\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.optimizers import SGD\nfrom keras.applications.vgg16 import VGG16\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# prebuild model with pre-trained weights on imagenet\nmodel = VGG16(weights='imagenet', include_top=True)\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=sgd, loss='categorical_crossentropy')\n\n# resize into VGG16 trained images' format\nim = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\nim = np.expand_dims(im, axis=0)\n\n# predict\nout = model.predict(im)\nplt.plot(out.ravel())\nplt.show()\nprint np.argmax(out)\n#this should print 820 for steaming train\n\n```", "```py\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\n# pre-built and pre-trained deep learning VGG16 model\nbase_model = VGG16(weights='imagenet', include_top=True)\nfor i, layer in enumerate(base_model.layers):\n     print (i, layer.name, layer.output_shape)\n\n# extract features from block4_pool block\nmodel =\nModel(input=base_model.input, output=base_model.get_layer('block4_pool').output)\nimg_path = 'cat.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# get the features from this block\nfeatures = model.predict(x)\n\n```", "```py\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n```", "```py\n# layer.name, layer.input_shape, layer.output_shape\n('mixed10', [(None, 8, 8, 320), (None, 8, 8, 768), (None, 8, 8, 768), (None, 8, 8, 192)], (None, 8, 8, 2048))\n('avg_pool', (None, 8, 8, 2048), (None, 1, 1, 2048))\n('flatten', (None, 1, 1, 2048), (None, 2048))\n('predictions', (None, 2048), (None, 1000))\n\n```", "```py\n*# add a global spatial average pooling layer* x = base_model.output\nx = GlobalAveragePooling2D()(x)*# let's add a fully-connected layer as first layer* x = Dense(1024, activation='relu')(x)*# and a logistic layer with 200 classes as last layer* predictions = Dense(200, activation='softmax')(x)*# model to train* model = Model(input=base_model.input, output=predictions)\n\n```", "```py\n*# that is, freeze all convolutional InceptionV3 layers* for layer in base_model.layers: layer.trainable = False\n\n```", "```py\n*# compile the model (should be done *after* setting layers to non-trainable)* model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n*# train the model on the new data for a few epochs* model.fit_generator(...)\n\n```", "```py\n*# we chose to train the top 2 inception blocks, that is, we will freeze* *# the first 172 layers and unfreeze the rest:* for layer in \nmodel.layers[:172]: layer.trainable = False for layer in \nmodel.layers[172:]: layer.trainable = True\n\n```", "```py\n*# we use SGD with a low learning rate* from keras.optimizers\nimport SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n*# we train our model again (this time fine-tuning the top 2 inception blocks)* *# alongside the top Dense layers* model.fit_generator(...)\n\n```"]