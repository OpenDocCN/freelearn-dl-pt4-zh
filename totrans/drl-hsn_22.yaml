- en: '22'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-Agent RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed discrete optimization problems. In this final
    chapter, we will introduce multi-agent reinforcement learning (sometimes abbreviated
    to MARL), a relatively new direction of reinforcement learning (RL) and deep RL,
    which is related to situations when multiple agents communicate in an environment.
    In real life, such problems appear in auctions, broadband communication networks,
    Internet of Things, and other scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will just take a quick glance at MARL and experiment a bit
    with simple environments; but, of course, if you find it interesting, there are
    lots of things you can experiment with. In our experiments, we will use a straightforward
    approach, with agents sharing the policy that we are optimizing, but the observation
    will be given from the agent’s standpoint and include information about the other
    agent’s location. With that simplification, our RL methods will stay the same,
    and just the environment will require preprocessing and must handle the presence
    of multiple agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with an overview of the similarities and differences between the classical
    single-agent RL problem and MARL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the MAgent environment, which was implemented and open sourced by the
    Geek.AI UK/China research group and later adopted by The Farama Foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use MAgent to train models in different environments with several groups of
    agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is multi-agent RL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The multi-agent setup is a natural extension of the familiar RL model that
    we covered in Chapter [1](ch005.xhtml#x1-190001). In the classical RL setup, we
    have one agent communicating with the environment using observations, rewards,
    and actions. But in some problems that often arise in real life, we have several
    agents involved in the environment interaction. To give some concrete examples:'
  prefs: []
  type: TYPE_NORMAL
- en: A chess game, when our program tries to beat the opponent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A market simulation, like product advertisements or price changes, when our
    actions might lead to counter-actions from other participants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplayer games, like Dota 2 or StarCraft II, when the agent needs to control
    several units competing with other players’ units (in this scenario, several units
    controlled by a single player might also cooperate to reach the goal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If other agents are outside of our control, we can treat them as part of the
    environment and still stick to the normal RL model with the single agent. As you
    saw in Chapter [20](ch024.xhtml#x1-36400020), training via self-play is a very
    powerful technique, which might lead to good policies without much sophistication
    on the environment side. But in some situations, that’s too limited and not exactly
    what we want.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as research shows, a group of simple agents might demonstrate collaborative
    behavior that is way more complex than expected. Some examples are the OpenAI
    blog post at [https://openai.com/blog/emergent-tool-use/](https://openai.com/blog/emergent-tool-use/)
    and the paper Emergent tool use from multi-agent autocurricula by Baker et al.
    [[Bak+20](#)] about the “hide-and-seek” game, where a group of agents collaborate
    and develop more and more sophisticated strategies and counter-strategies to win
    against another group of agents, for example, “build a fence from available objects”
    and “use a trampoline to catch agents behind the fence.”
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of different ways that agents might communicate, they can be separated
    into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Competitive: When two or more agents try to beat each other in order to maximize
    their reward. The simplest setup is a two-player game, like chess, backgammon,
    or Atari Pong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaborative: When a group of agents needs to use joint efforts to reach some
    goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are lots of examples that fall into one of these groups, but the most
    interesting and close to real-life scenarios are normally a mixture of both behaviors.
    There are tons of examples, starting from some board games that allow you to form
    allies and going up to modern corporations, where 100% collaboration is assumed,
    but real life is normally much more complicated than that.
  prefs: []
  type: TYPE_NORMAL
- en: From a theoretical point of view, game theory has quite a developed foundation
    for both communication forms, but for the sake of brevity, we’re not going to
    deep dive into the field, which is large and has different terminology. If you’re
    curious, you can find lots of books and courses that explore it in great depth.
    To give an example, the minimax algorithm is a well-known result of game theory,
    and you saw it used in Chapter [20](ch024.xhtml#x1-36400020).
  prefs: []
  type: TYPE_NORMAL
- en: MARL is a relatively young field, but activity has been growing over time; so,
    it might be interesting to keep an eye on it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into our first MARL example, let’s look at the environment we
    can use. If you want to play with MARL, your choice is a bit limited. All the
    environments that come with Gym support only one agent. There are some patches
    for Atari Pong, to switch it into two-player mode, but they are not standard and
    are an exception rather than the rule.
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind, together with Blizzard, has made StarCraft II publicly available (
    [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)) and makes
    for a very interesting and challenging environment for experimentation. However,
    for somebody who is taking their first steps in MARL, it might be too complex.
    In that regard, I have found the MAgent environment, originally developed by Geek.AI,
    to be perfectly suitable; it is simple and fast and has minimal dependency, but
    it still allows you to simulate different multi-agent scenarios for experimentation.
    It doesn’t provide a Gym-compatible API, but we will implement it on our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re interested in MARL, you might also check out the PettingZoo package
    from The Farama Foundation: [https://pettingzoo.farama.org](https://pettingzoo.farama.org).
    It includes more environments and a unified API for communication, but in this
    chapter, we’re focusing only on the MAgent environment.'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of MAgent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at MAgent at a high level. It provides the simulation of a grid world
    that 2D agents inhabit. These agents can observe things around them (according
    to their perception length), move to some distance from their location, and attack
    other agents around them.
  prefs: []
  type: TYPE_NORMAL
- en: There might be different groups of agents with various characteristics and interaction
    parameters. For example, the first environment that we will consider is a predator-prey
    model, where “tigers” hunt “deer” and obtain a reward for that. In the environment
    configuration, you can specify lots of aspects of the group, like perception,
    movement, attack distance, the initial health of every agent in the group, how
    much health they spend on movement and attack, and so on. Aside from the agents,
    the environment might contain walls that are not crossable by the agents.
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about MAgent is that it is very scalable, as it is implemented
    in C++ internally, just exposing the Python interface. This means that the environment
    can have thousands of agents in the group, providing you with observations and
    processing the agents’ actions.
  prefs: []
  type: TYPE_NORMAL
- en: Installing MAgent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As it often happens, the original version of MAgent hasn’t been maintained
    for some time. Luckily for us, The Farama Foundation forked the original repo
    and are currently maintaining it, providing most of the original functionality.
    Their version is called MAgent2 and the documentation can be found here: [https://magent2.farama.org/](https://magent2.farama.org/).
    The GitHub repository is available here: [https://github.com/Farama-Foundation/magent2](https://github.com/Farama-Foundation/magent2).
    To install MAgent2, you need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a random environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To quickly understand the MAgent API and logic, I’ve implemented a simple environment
    with “tiger” and “deer” agents, where both groups are driven by the random policy.
    It might not be very interesting from an RL perspective, but it will allow us
    to quickly learn enough about the API to implement the Gym environment wrapper.
    The example can be found in Chapter22/forest_random.py and we’ll walk through
    it here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with ForestEnv, defined in lib/data.py, which defines the environment.
    This class is inherited from magent_parallel_env (yes, the name of the class is
    in lowercase, in contravention of the Python style guide, but that’s how it has
    been defined in the library), the base class for MAgent environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This class mimics the Gym API, but it is not 100% compatible, so we will need
    to deal with this in our code later.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the constructor, we instantiate the GridWorld class, which works as a Python
    adapter around the low-level MAgent C++ library API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we instantiate the GridWorld class, which implements
    most of the logic of our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GridWorld class is configured by the Config instance returned by the get_config
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the forest_config function, which is imported from the magent.builtin.config.forest
    package and tweaks the configuration, adding the reward for deer on every step.
    This will be important when we start training the deer model, so a reward of 1
    on every step will incentivize the agent to live longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the configuration hasn’t been included here as it is largely unchanged
    and defines lots of details about the environment, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How many groups of agents do we have in the environment? In our case, we have
    two groups: “deer” and “tigers.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the properties of each group – how far can they see from their location?
    An example of this could be that the deer can see as far as one cell, but the
    tigers have can see as far as four. Can they attack others and how far? What is
    the initial health of each agent? How fast can they recover from damage? There
    are lots of parameters you can specify.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can they attack other groups and what damage does it do? There is lots of
    flexibility – for example, you can model the scenario when predators hunt only
    in pairs (we’ll do this experiment later in the chapter). In our current setup,
    the situation is simple – any tiger can attack any deer without restrictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last function in the ForestEnv class is generate_map, which places walls,
    deer, and tigers randomly on the map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s get to the forest_random.py source code. In the beginning, we import
    the lib.data package and the VideoRecorder class from Gymnasium:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In Chapter [2](ch006.xhtml#x1-380002), we used the RecordVideo wrapper to capture
    environment observations automatically, but in the case of MAgent environments,
    it is not possible, due to different return values (all methods are returning
    dictionaries for all agents at once instead of single values). To get around this,
    we’ll use the VideoRecorder class to capture videos and write into the RENDER_DIR
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a ForestEnv instance and video recorder. An environment object
    contains the property agents, which keeps string identifiers for all the agents
    in the environment. In our case, it will be a list of values like deer_12 or tiger_3\.
    With the default configuration, on the map 64 × 64, we have 204 deer agents and
    40 tigers, so the env.agents list has 244 items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We reset the environment using the reset() method, but now it returns one value
    (instead of the two in the Gym API). The returned value is a dict with agent IDs
    as keys and observation tensors as values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The action space contains five mutually exclusive actions for deer (four directions
    + a “do nothing” action). Tigers can do the same, but in addition can attack in
    four directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of observations, every tiger gets a 9 × 9 matrix with five different
    planes of information. Deer are more short-sighted, so their observation is just
    3 × 3\. The observation always contains the agent in the center, so it shows the
    grid around this specific agent. The five planes of information are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Walls: 1 if this cell contains the wall and 0 otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 1 (the group that the agent belongs to): 1 if the cell contains agents
    from the agent’s group and 0 otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 1 health: The relative health of the agent in this cell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 2 (the group with enemy agents): 1 if there is an enemy in this cell
    and 0 otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 2 health: The relative health of the enemy or 0 if nothing is there'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If more groups are configured, the observation will contain more planes in the
    observation tensor. In addition, MAgent has a “minimap” functionality that adds
    the “zoomed-out” location of agents of every group. This minimap feature is disabled
    in my examples, but you can experiment with it to check the effect on the training.
    Without this feature, every agent sees only a limited range of cells around itself,
    but minimap allows them to have a more global view of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Groups 1 and 2 are relative to the agent’s group; so, in the second plane, deer
    have information about other deer and for tigers, this plane includes other tigers.
    This makes observations group-independent and allows us to train a single policy
    for both groups if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Another optional part of the observation is the so-called “extra features,”
    which includes the agent’s ID, last action, last reward, and normalized position.
    Concrete details could be found in the MAgent source code, but we’re not going
    to use this functionality in our examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue describing our code. We have a loop that is repeated until we
    have alive agents in the environment. On every iteration, we sample random actions
    for all the agents and execute them in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'All values returned from the env.step() function are dictionaries with agent_id
    as the key. Another very important detail about the MAgent environment is that
    the set of agents is volatile: agents can disappear from the environment (when
    they die, for example). In our “forest” environment, tigers are losing 0.1 points
    of health every step, which could be increased after eating deer. Deer lose health
    only after an attack and gain it on every step (likely from eating grass).'
  prefs: []
  type: TYPE_NORMAL
- en: When the agent dies (a tiger from starvation or a deer from a tiger’s attack),
    the corresponding entry in the all_dones dict is set to True and on the next iteration,
    the agent disappears from all the dictionaries and the env.agents list. So, after
    the death of one agent, the whole episode continues and we need to take this into
    account during the training.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the loop is executed until no more agents are alive.
    As both tigers and deer are behaving randomly (and tigers are losing health at
    every step), it is very likely that all tigers will die from starvation and the
    surviving deer will live happily infinitely long. But the environment is configured
    to automatically remove all deer when no more tigers are left, so our program
    ends after 30-40 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the loop, we sum up the reward obtained by agents and track the
    amount of steps for which they were alive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After the loop, we show the top 20 agents sorted by their reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this tool might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In my simulation, one agent (tiger_5) was especially lucky and lived longer
    than others. At the end, the program saves a video of the episode. The result
    of my run is available here: [https://youtube.com/shorts/pH-Rz9Q4yrI](https://youtube.com/shorts/pH-Rz9Q4yrI).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [22.1](#x1-416097r1), two different states are shown: at the beginning
    of the game and close to the end. Tigers are shown with blue dots (or the darker
    ones if you’re reading this in grayscale), the red dots are deer, and the gray
    dots are walls (you can refer to the digital version of the book to see the colors
    in the screenshot). The attack direction is shown with small black arrows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.1: Two states of the forest environment: at the beginning of the
    episode (left) and close to the end (right)'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, both groups of agents were behaving randomly, which is not
    very interesting. In the next section, we’ll apply a deep Q-network (DQN) to improve
    the tiger’s hunting skills.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-network for tigers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will apply the DQN model to the tiger group of agents to check whether
    they can learn how to hunt better. All of the agents share the network, so their
    behavior will be the same. The deer group will keep random behavior in this example
    to keep things simple for now; we’ll train them later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The training code can be found in Chapter22/forest_tigers_dqn.py; it doesn’t
    differ much from the other DQN versions from the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make the MAgent environment work with our classes, a specialized version
    of ExperienceSourceFirstLast was implemented to handle the specifics of the environment.
    This class is called MAgentExperienceSourceFirstLast and can be found in lib/data.py.
    Let’s check it out to understand how it fits into the rest of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first class we define is the items produced by our ExperienceSource. As
    we discussed in Chapter [7](ch011.xhtml#x1-1070007), instances of the class ExperienceFirstLast
    contain the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'state: Observations from the environment at the current step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'action: The action we executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'reward: The amount of reward we obtained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'last_state: Observation after executing the action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a multi-agent setup, every agent is producing the same set of data, but
    we also have to be able to identify which group this agent belongs to (in this
    tiger-deer example, does this experience correspond to the tiger or the deer’s
    trajectory?). To retain this information, we define a subclass, ExperienceFirstLastMARL,
    with a new field keeping the group’s name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the constructor of MAgentExperienceSourceFirstLast, we pass the following
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'magent_parallel_env: The MAgent parallel environment (we experimented with
    this in the previous section).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'agents_by_group: The PTAN BaseAgent object for every group of agents. In our
    tiger DQN example, tigers will be controlled by a neural network (ptan.agent.DQNAgent),
    but deer behave randomly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'track_reward_group: The parameter that specifies the group for which we’re
    tracking the episode’s reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'filter_group: An optional filter for the group for which we want to generate
    an experience. In our current example, we need only observations from tigers (as
    we train only tigers), but in the next section, we’ll train a DQN for both tigers
    and deer, so the filter will be disabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the subsequent constructor code, we store arguments and create two useful
    mappings for agents: from the agent ID to the group name and back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We also define a utility method to strip the agent’s numerical ID to get the
    group name (in our case, it will be tiger or deer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the main method of the class: the iterator interface, which produces
    experience items from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we reset the environment in the beginning and create initial states for
    agents (in case our agents will keep some state, but in this chapter’s examples,
    they are stateless).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we iterate the episode until we have living agents (the same way we did
    in the example in the previous section). In this loop, we fill actions in the
    dictionary, mapping the agent ID to the action. To do this, we use PTAN BaseAgent
    instances, which work with batches of observations, so actions will be produced
    very efficiently for the whole group of agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have actions to be executed, we send them to the environment and obtain
    dictionaries with new observations, rewards, and done and truncation flags. Then,
    we generate experience items for every alive agent we currently have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the episode, we remember the number of steps and the average
    reward obtained by the group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With this class at hand, our DQN training code stays almost the same as in
    the single-agent RL case. The full source code of this example can be found in
    forest_tigers_dqn.py. Here, I’m going to show only part of the code where PTAN
    agents and the experience source are created (to illustrate how MAgentExperienceSourceFirstLast
    is being used):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, tigers are controlled by the neural network (which is a very
    simple two-layer convolution plus a two-layer fully connected net). A group of
    deer is controlled by a random number generator. The experience replay buffer
    will be populated only with the tiger experience because of the filter_group="tiger"
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: Training and results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start the training, run ./forest_tigers_dqn.py -n run_name --dev cuda. In
    one hour of training, the tiger’s test reward has reached the best score of 82,
    which is a significant improvement over the random baseline. Acting randomly,
    most tigers die after 20 steps and only a few lucky ones can live longer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate how many deer were eaten to get this score. Initially, each
    tiger has a health of 10 and spends 0.5 of their health on each step. In total,
    we have 40 tigers and 204 deer on the map (you can change this amount with command-line
    arguments). For every eaten deer, the tigers obtain 8 health points, which allows
    them to survive for an extra 16 steps. For every step, each tiger obtains a reward
    of 1, so the “excess reward” from the deer eaten by 40 tigers is 82 ⋅ 40 − 20
    ⋅ 40 = 2480\. Every deer gives 8 health points, which is converted into an extra
    16 steps of a tiger’s life, so the number of deer eaten is 2480∕16 = 155\. So,
    almost 76% of deer were hunted by the best policy we’ve got. Not bad given that
    deer are randomly placed on the map and tigers need to get to them to attack.
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite likely that the policy stopped improving just because of the limited
    view of the tigers. If you are curious, you can enable the minimap in the environment
    settings and experiment. With more information about the food’s location, it’s
    likely that the policy could be improved even more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [22.2](#x1-419002r2), the average reward and number of steps during
    the training is shown. From it, you can see that the main growth was during the
    first 300 episodes and later, the training progress was almost 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.2: Average reward (left) and count of steps (right) from training
    episodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Figure [22.3](#x1-419003r3) shows the plots for the test reward and
    steps, which demonstrate that the policy continued to improve even after 300 episodes
    (≈ 0.4 hours of training):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.3: Average reward (left) and count of steps (right) from test episodes'
  prefs: []
  type: TYPE_NORMAL
- en: The final pair of charts in Figure [22.4](#x1-419005r4) shows the training loss
    and epsilon during the training. Both plots are correlated, which indicates that
    most of the novelty during the training was obtained in the exploration phase
    (as the loss value is high, which means that new situations arise during training).
    This might be an indication that better exploration methods might be beneficial
    for the final policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.4: Average loss (left) and epsilon (right) during the training'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, besides the training, I implemented a tool to check the models in
    action. It’s called forest_tigers_play.py and it loads the trained model and uses
    it during the episode, producing a video recording of the observations. The video
    from the best model (with a test score of 82.89) is available here: [https://www.youtube.com/shorts/ZZf80AHk538](https://www.youtube.com/shorts/ZZf80AHk538).
    As you can see, tigers’ hunting skills are now significantly better than the random
    policy: at the end of the episode, just 53 deer were left from the initial 204.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration by the tigers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second experiment that I implemented was designed to make the tigers’ lives
    more complicated and encourage collaboration between them. The training and play
    code are the same; the only difference is in the MAgent environment’s configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you pass the argument --mode double_attack to the training utility, the
    environment data.DoubleAttackEnv will be used. The only difference is the configuration
    object, which sets additional constraints on tigers’ attacks. In the new setup,
    they can attack deer only in pairs and have to do this at the same time. A single
    tiger’s attack doesn’t have any effect. This definitely complicates the training
    and hunting, as obtaining the reward from eating the deer is now much harder for
    tigers. To start the training, you can run the same train utility, but with an
    extra command-line argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [22.5](#x1-420002r5), the reward and step plots for the training episodes
    are shown. As you can see, even after 2 hours of training, the reward is still
    improving. At the same time, the count of steps in the episode never exceeds 300,
    which might be an indication that tigers just don’t have nearby deer to eat and
    die from starvation (it also might just be an internal limit of steps in the environment).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.5: Average reward (left) and count of steps (right) from training
    episodes in double_attack mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to single-tiger hunting mode, the loss during the training is not
    decreasing (as shown in Figure [22.6](#x1-420003r6)), which might indicate that
    training hyperparameters could be improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.6: Average loss during training'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the model in action, you can use the same utility as before; just pass
    it the --mode double_attack argument. A video recording of the best model I got
    is available here: [https://youtu.be/VjGbzP1r7HY](https://youtu.be/VjGbzP1r7HY).
    As you can see, tigers are now moving in pairs, attacking the deer together.'
  prefs: []
  type: TYPE_NORMAL
- en: Training both tigers and deer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next example is the scenario when both tigers and deer are controlled by
    different DQN models being trained simultaneously. Tigers are rewarded for living
    longer, which stimulates them to eat more deer, as at every step in the simulation,
    they lose health points. Deer are also rewarded on every timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: The code is in forest_both_dqn.py and it is an extension of the previous example.
    For both groups of agents, we have a separate DQNAgent class instance, which uses
    separate neural networks to convert observations into actions. The experience
    source is the same, but now we’re not filtering on a tiger’s group experience
    (with the parameter filter_group=None). Because of this, our replay buffer now
    contains observations from all the agents in the environment, not just from tigers
    as in the previous example. During the training, we sample a batch and split examples
    from deer and tigers into two separate batches to be used for training their networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m not going to include all the code here, as it differs from the previous
    example only in small details. If you are curious, you can take a look at the
    source code in the GitHub repository. Figure [22.7](#x1-421001r7) shows the training
    reward and steps for tigers. You can see that initially, tigers were able to consistently
    increase their reward, but later, the growth stopped:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.7: Average reward for tigers (left) and count of steps (right) from
    training episodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next two plots in Figure [22.8](#x1-421002r8), the reward for tigers
    and deer during the testing is shown. There is no clear trend here; both groups
    are competing and trying to beat their opponent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.8: Test reward for tigers (left) and deer (right)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from Figure [22.8](#x1-421002r8), deer are way more successful
    than tigers, which is not surprising, as the speed of both is the same, so the
    deer just need to move all the time and wait for the tigers to die from starvation.
    If you want, you can experiment with the environment settings by increasing either
    the tigers’ speed or the wall density.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, it is possible to visualize the learned policies with the utility
    forest_both_play.py, but now you need to pass two model files. Here is a video
    comparing the best model for deer and the best model for tigers: [https://youtube.com/shorts/vuVL1e26KqY](https://youtube.com/shorts/vuVL1e26KqY).
    In the video, all deer are just moving to the left of the field. Most likely,
    tigers can exploit this simple policy for their benefit.'
  prefs: []
  type: TYPE_NORMAL
- en: The battle environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the tiger-deer environment, MAgent contains several other predefined
    configurations you can find in the magent2.builtin.config and magent2.environment
    packages. As a final example in this chapter, we’ll take a look at the “battle”
    configuration, where two groups of agents are fighting each other (without eating,
    thank goodness). Both agents have health points of 10 and every attack takes 2
    health points, so 5 consecutive attacks are required to get the reward for the
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code in battel_dqn.py. In this setup, one group is behaving
    randomly and another is using the DQN to improve the policy. Training took two
    hours and the DQN was able to find a decent policy, but at the end, the training
    process diverged. In Figure [22.9](#x1-422002r9), the training and test reward
    plots are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.9: Average reward during training (left) and test (right) in the
    battle scenario'
  prefs: []
  type: TYPE_NORMAL
- en: 'The video recording (produced by the tool battle_play.py) is available here:
    [https://youtube.com/shorts/ayfCa8xGY2k](https://youtube.com/shorts/ayfCa8xGY2k).
    The blue team is random and the red is controlled by the DQN.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we just touched a bit on the very interesting and dynamic field
    of MARL, which has several practical applications in trading simulation, communication
    networks, and others. There are lots of things that you can try on your own using
    the MAgent environment or other environments (like PySC2).
  prefs: []
  type: TYPE_NORMAL
- en: My congratulations on reaching the end of the book! I hope that the book was
    useful and you enjoyed reading it as much as I enjoyed gathering the material
    and writing all the chapters. As a final word, I would like to wish you good luck
    in this exciting and dynamic area of RL. The domain is developing very rapidly,
    but with an understanding of the basics, it will become much simpler for you to
    keep track of the new developments and research in this field.
  prefs: []
  type: TYPE_NORMAL
- en: There are many very interesting topics left uncovered, such as partially observable
    Markov decision processes (where environment observations don’t fulfill the Markov
    property) or recent approaches to exploration, such as the count-based methods.
    There has been a lot of recent activity around multi-agent methods, where many
    agents need to learn how to coordinate to solve a common problem. I also haven’t
    mentioned the memory-based RL approach, where your agent can maintain some sort
    of memory to keep its knowledge and experience. A great deal of effort is being
    put into increasing the RL sample efficiency, which will ideally be close to human
    learning performance one day, but this is still a far-off goal at the moment.
    Of course, it’s not possible to cover the full domain in just one book, because
    new ideas appear almost every day. However, the goal of this book was to give
    you a practical foundation in the field, simplifying your own learning of the
    common methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’d like to end by quoting Volodymyr Mnih’s words from his talk, Recent Advances,
    Frontiers and Future of Deep RL, from the Deep RL Bootcamp, Berkeley, 2017, which
    are still very relevant: “The field of deep RL is very new and everything is still
    exciting. Literally, nothing is solved yet!”'
  prefs: []
  type: TYPE_NORMAL
- en: Leave a Review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoyed
    it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an [Amazon review](https://packt.link/r/1835882714);
    it will only take a minute, but it makes a big difference for readers like you.
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code below to receive a free ebook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*https://packt.link/NzOWQ*'
  prefs: []
  type: TYPE_NORMAL
