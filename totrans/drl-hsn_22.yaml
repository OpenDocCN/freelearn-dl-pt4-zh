- en: '22'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-Agent RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we discussed discrete optimization problems. In this final
    chapter, we will introduce multi-agent reinforcement learning (sometimes abbreviated
    to MARL), a relatively new direction of reinforcement learning (RL) and deep RL,
    which is related to situations when multiple agents communicate in an environment.
    In real life, such problems appear in auctions, broadband communication networks,
    Internet of Things, and other scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will just take a quick glance at MARL and experiment a bit
    with simple environments; but, of course, if you find it interesting, there are
    lots of things you can experiment with. In our experiments, we will use a straightforward
    approach, with agents sharing the policy that we are optimizing, but the observation
    will be given from the agent’s standpoint and include information about the other
    agent’s location. With that simplification, our RL methods will stay the same,
    and just the environment will require preprocessing and must handle the presence
    of multiple agents.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Start with an overview of the similarities and differences between the classical
    single-agent RL problem and MARL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the MAgent environment, which was implemented and open sourced by the
    Geek.AI UK/China research group and later adopted by The Farama Foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use MAgent to train models in different environments with several groups of
    agents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is multi-agent RL?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The multi-agent setup is a natural extension of the familiar RL model that
    we covered in Chapter [1](ch005.xhtml#x1-190001). In the classical RL setup, we
    have one agent communicating with the environment using observations, rewards,
    and actions. But in some problems that often arise in real life, we have several
    agents involved in the environment interaction. To give some concrete examples:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: A chess game, when our program tries to beat the opponent
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A market simulation, like product advertisements or price changes, when our
    actions might lead to counter-actions from other participants
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplayer games, like Dota 2 or StarCraft II, when the agent needs to control
    several units competing with other players’ units (in this scenario, several units
    controlled by a single player might also cooperate to reach the goal)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If other agents are outside of our control, we can treat them as part of the
    environment and still stick to the normal RL model with the single agent. As you
    saw in Chapter [20](ch024.xhtml#x1-36400020), training via self-play is a very
    powerful technique, which might lead to good policies without much sophistication
    on the environment side. But in some situations, that’s too limited and not exactly
    what we want.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as research shows, a group of simple agents might demonstrate collaborative
    behavior that is way more complex than expected. Some examples are the OpenAI
    blog post at [https://openai.com/blog/emergent-tool-use/](https://openai.com/blog/emergent-tool-use/)
    and the paper Emergent tool use from multi-agent autocurricula by Baker et al.
    [[Bak+20](#)] about the “hide-and-seek” game, where a group of agents collaborate
    and develop more and more sophisticated strategies and counter-strategies to win
    against another group of agents, for example, “build a fence from available objects”
    and “use a trampoline to catch agents behind the fence.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of different ways that agents might communicate, they can be separated
    into two groups:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Competitive: When two or more agents try to beat each other in order to maximize
    their reward. The simplest setup is a two-player game, like chess, backgammon,
    or Atari Pong.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaborative: When a group of agents needs to use joint efforts to reach some
    goal.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are lots of examples that fall into one of these groups, but the most
    interesting and close to real-life scenarios are normally a mixture of both behaviors.
    There are tons of examples, starting from some board games that allow you to form
    allies and going up to modern corporations, where 100% collaboration is assumed,
    but real life is normally much more complicated than that.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: From a theoretical point of view, game theory has quite a developed foundation
    for both communication forms, but for the sake of brevity, we’re not going to
    deep dive into the field, which is large and has different terminology. If you’re
    curious, you can find lots of books and courses that explore it in great depth.
    To give an example, the minimax algorithm is a well-known result of game theory,
    and you saw it used in Chapter [20](ch024.xhtml#x1-36400020).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: MARL is a relatively young field, but activity has been growing over time; so,
    it might be interesting to keep an eye on it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the environment
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into our first MARL example, let’s look at the environment we
    can use. If you want to play with MARL, your choice is a bit limited. All the
    environments that come with Gym support only one agent. There are some patches
    for Atari Pong, to switch it into two-player mode, but they are not standard and
    are an exception rather than the rule.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind, together with Blizzard, has made StarCraft II publicly available (
    [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)) and makes
    for a very interesting and challenging environment for experimentation. However,
    for somebody who is taking their first steps in MARL, it might be too complex.
    In that regard, I have found the MAgent environment, originally developed by Geek.AI,
    to be perfectly suitable; it is simple and fast and has minimal dependency, but
    it still allows you to simulate different multi-agent scenarios for experimentation.
    It doesn’t provide a Gym-compatible API, but we will implement it on our own.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re interested in MARL, you might also check out the PettingZoo package
    from The Farama Foundation: [https://pettingzoo.farama.org](https://pettingzoo.farama.org).
    It includes more environments and a unified API for communication, but in this
    chapter, we’re focusing only on the MAgent environment.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: An overview of MAgent
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at MAgent at a high level. It provides the simulation of a grid world
    that 2D agents inhabit. These agents can observe things around them (according
    to their perception length), move to some distance from their location, and attack
    other agents around them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: There might be different groups of agents with various characteristics and interaction
    parameters. For example, the first environment that we will consider is a predator-prey
    model, where “tigers” hunt “deer” and obtain a reward for that. In the environment
    configuration, you can specify lots of aspects of the group, like perception,
    movement, attack distance, the initial health of every agent in the group, how
    much health they spend on movement and attack, and so on. Aside from the agents,
    the environment might contain walls that are not crossable by the agents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about MAgent is that it is very scalable, as it is implemented
    in C++ internally, just exposing the Python interface. This means that the environment
    can have thousands of agents in the group, providing you with observations and
    processing the agents’ actions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Installing MAgent
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As it often happens, the original version of MAgent hasn’t been maintained
    for some time. Luckily for us, The Farama Foundation forked the original repo
    and are currently maintaining it, providing most of the original functionality.
    Their version is called MAgent2 and the documentation can be found here: [https://magent2.farama.org/](https://magent2.farama.org/).
    The GitHub repository is available here: [https://github.com/Farama-Foundation/magent2](https://github.com/Farama-Foundation/magent2).
    To install MAgent2, you need to run the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Setting up a random environment
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To quickly understand the MAgent API and logic, I’ve implemented a simple environment
    with “tiger” and “deer” agents, where both groups are driven by the random policy.
    It might not be very interesting from an RL perspective, but it will allow us
    to quickly learn enough about the API to implement the Gym environment wrapper.
    The example can be found in Chapter22/forest_random.py and we’ll walk through
    it here.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with ForestEnv, defined in lib/data.py, which defines the environment.
    This class is inherited from magent_parallel_env (yes, the name of the class is
    in lowercase, in contravention of the Python style guide, but that’s how it has
    been defined in the library), the base class for MAgent environments:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This class mimics the Gym API, but it is not 100% compatible, so we will need
    to deal with this in our code later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'In the constructor, we instantiate the GridWorld class, which works as a Python
    adapter around the low-level MAgent C++ library API:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we instantiate the GridWorld class, which implements
    most of the logic of our environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'The GridWorld class is configured by the Config instance returned by the get_config
    function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function uses the forest_config function, which is imported from the magent.builtin.config.forest
    package and tweaks the configuration, adding the reward for deer on every step.
    This will be important when we start training the deer model, so a reward of 1
    on every step will incentivize the agent to live longer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the configuration hasn’t been included here as it is largely unchanged
    and defines lots of details about the environment, including the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'How many groups of agents do we have in the environment? In our case, we have
    two groups: “deer” and “tigers.”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the properties of each group – how far can they see from their location?
    An example of this could be that the deer can see as far as one cell, but the
    tigers have can see as far as four. Can they attack others and how far? What is
    the initial health of each agent? How fast can they recover from damage? There
    are lots of parameters you can specify.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can they attack other groups and what damage does it do? There is lots of
    flexibility – for example, you can model the scenario when predators hunt only
    in pairs (we’ll do this experiment later in the chapter). In our current setup,
    the situation is simple – any tiger can attack any deer without restrictions.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last function in the ForestEnv class is generate_map, which places walls,
    deer, and tigers randomly on the map:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s get to the forest_random.py source code. In the beginning, we import
    the lib.data package and the VideoRecorder class from Gymnasium:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In Chapter [2](ch006.xhtml#x1-380002), we used the RecordVideo wrapper to capture
    environment observations automatically, but in the case of MAgent environments,
    it is not possible, due to different return values (all methods are returning
    dictionaries for all agents at once instead of single values). To get around this,
    we’ll use the VideoRecorder class to capture videos and write into the RENDER_DIR
    directory.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a ForestEnv instance and video recorder. An environment object
    contains the property agents, which keeps string identifiers for all the agents
    in the environment. In our case, it will be a list of values like deer_12 or tiger_3\.
    With the default configuration, on the map 64 × 64, we have 204 deer agents and
    40 tigers, so the env.agents list has 244 items:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We reset the environment using the reset() method, but now it returns one value
    (instead of the two in the Gym API). The returned value is a dict with agent IDs
    as keys and observation tensors as values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code produces the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The action space contains five mutually exclusive actions for deer (four directions
    + a “do nothing” action). Tigers can do the same, but in addition can attack in
    four directions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of observations, every tiger gets a 9 × 9 matrix with five different
    planes of information. Deer are more short-sighted, so their observation is just
    3 × 3\. The observation always contains the agent in the center, so it shows the
    grid around this specific agent. The five planes of information are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Walls: 1 if this cell contains the wall and 0 otherwise'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 1 (the group that the agent belongs to): 1 if the cell contains agents
    from the agent’s group and 0 otherwise'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 1 health: The relative health of the agent in this cell'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 2 (the group with enemy agents): 1 if there is an enemy in this cell
    and 0 otherwise'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Group 2 health: The relative health of the enemy or 0 if nothing is there'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If more groups are configured, the observation will contain more planes in the
    observation tensor. In addition, MAgent has a “minimap” functionality that adds
    the “zoomed-out” location of agents of every group. This minimap feature is disabled
    in my examples, but you can experiment with it to check the effect on the training.
    Without this feature, every agent sees only a limited range of cells around itself,
    but minimap allows them to have a more global view of the environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Groups 1 and 2 are relative to the agent’s group; so, in the second plane, deer
    have information about other deer and for tigers, this plane includes other tigers.
    This makes observations group-independent and allows us to train a single policy
    for both groups if needed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Another optional part of the observation is the so-called “extra features,”
    which includes the agent’s ID, last action, last reward, and normalized position.
    Concrete details could be found in the MAgent source code, but we’re not going
    to use this functionality in our examples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue describing our code. We have a loop that is repeated until we
    have alive agents in the environment. On every iteration, we sample random actions
    for all the agents and execute them in the environment:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'All values returned from the env.step() function are dictionaries with agent_id
    as the key. Another very important detail about the MAgent environment is that
    the set of agents is volatile: agents can disappear from the environment (when
    they die, for example). In our “forest” environment, tigers are losing 0.1 points
    of health every step, which could be increased after eating deer. Deer lose health
    only after an attack and gain it on every step (likely from eating grass).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: When the agent dies (a tiger from starvation or a deer from a tiger’s attack),
    the corresponding entry in the all_dones dict is set to True and on the next iteration,
    the agent disappears from all the dictionaries and the env.agents list. So, after
    the death of one agent, the whole episode continues and we need to take this into
    account during the training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the loop is executed until no more agents are alive.
    As both tigers and deer are behaving randomly (and tigers are losing health at
    every step), it is very likely that all tigers will die from starvation and the
    surviving deer will live happily infinitely long. But the environment is configured
    to automatically remove all deer when no more tigers are left, so our program
    ends after 30-40 steps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the loop, we sum up the reward obtained by agents and track the
    amount of steps for which they were alive:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After the loop, we show the top 20 agents sorted by their reward:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output from this tool might look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In my simulation, one agent (tiger_5) was especially lucky and lived longer
    than others. At the end, the program saves a video of the episode. The result
    of my run is available here: [https://youtube.com/shorts/pH-Rz9Q4yrI](https://youtube.com/shorts/pH-Rz9Q4yrI).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [22.1](#x1-416097r1), two different states are shown: at the beginning
    of the game and close to the end. Tigers are shown with blue dots (or the darker
    ones if you’re reading this in grayscale), the red dots are deer, and the gray
    dots are walls (you can refer to the digital version of the book to see the colors
    in the screenshot). The attack direction is shown with small black arrows.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_01.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.1: Two states of the forest environment: at the beginning of the
    episode (left) and close to the end (right)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In this example, both groups of agents were behaving randomly, which is not
    very interesting. In the next section, we’ll apply a deep Q-network (DQN) to improve
    the tiger’s hunting skills.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-network for tigers
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will apply the DQN model to the tiger group of agents to check whether
    they can learn how to hunt better. All of the agents share the network, so their
    behavior will be the same. The deer group will keep random behavior in this example
    to keep things simple for now; we’ll train them later in the chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用DQN模型到老虎代理组，检查它们是否能学会更好地狩猎。所有代理共享网络，因此它们的行为将是相同的。鹿群将在本示例中保持随机行为，以便简化处理；我们将在本章后面训练它们。
- en: The training code can be found in Chapter22/forest_tigers_dqn.py; it doesn’t
    differ much from the other DQN versions from the previous chapters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码可以在Chapter22/forest_tigers_dqn.py中找到；它与前几章的其他DQN版本没有太大区别。
- en: Understanding the code
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解代码
- en: To make the MAgent environment work with our classes, a specialized version
    of ExperienceSourceFirstLast was implemented to handle the specifics of the environment.
    This class is called MAgentExperienceSourceFirstLast and can be found in lib/data.py.
    Let’s check it out to understand how it fits into the rest of the code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使MAgent环境与我们的类兼容，实施了一个专门版本的ExperienceSourceFirstLast来处理环境的特定情况。这个类被称为MAgentExperienceSourceFirstLast，可以在lib/data.py中找到。我们来查看一下它，了解它如何融入代码的其他部分。
- en: 'The first class we define is the items produced by our ExperienceSource. As
    we discussed in Chapter [7](ch011.xhtml#x1-1070007), instances of the class ExperienceFirstLast
    contain the following fields:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的第一个类是由我们的ExperienceSource生成的条目。正如我们在第[7章](ch011.xhtml#x1-1070007)中讨论的，ExperienceFirstLast类的实例包含以下字段：
- en: 'state: Observations from the environment at the current step'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'state: 当前步骤中来自环境的观测数据'
- en: 'action: The action we executed'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'action: 我们执行的动作'
- en: 'reward: The amount of reward we obtained'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: 我们获得的奖励数量'
- en: 'last_state: Observation after executing the action'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'last_state: 执行动作后的观测数据'
- en: 'In a multi-agent setup, every agent is producing the same set of data, but
    we also have to be able to identify which group this agent belongs to (in this
    tiger-deer example, does this experience correspond to the tiger or the deer’s
    trajectory?). To retain this information, we define a subclass, ExperienceFirstLastMARL,
    with a new field keeping the group’s name:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理设置中，每个代理都会生成相同的数据集，但我们还需要能够识别该代理属于哪个组（在这个老虎-鹿的例子中，这个经验是属于老虎还是鹿的轨迹？）。为了保留这一信息，我们定义了一个子类ExperienceFirstLastMARL，并添加了一个新字段来保存组名：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the constructor of MAgentExperienceSourceFirstLast, we pass the following
    arguments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在MAgentExperienceSourceFirstLast的构造函数中，我们传递了以下参数：
- en: 'magent_parallel_env: The MAgent parallel environment (we experimented with
    this in the previous section).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'magent_parallel_env: MAgent并行环境（我们在前一节中进行了实验）。'
- en: 'agents_by_group: The PTAN BaseAgent object for every group of agents. In our
    tiger DQN example, tigers will be controlled by a neural network (ptan.agent.DQNAgent),
    but deer behave randomly.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'agents_by_group: 每个代理组的PTAN BaseAgent对象。在我们的老虎DQN示例中，老虎将由神经网络（ptan.agent.DQNAgent）控制，而鹿则是随机行为。'
- en: 'track_reward_group: The parameter that specifies the group for which we’re
    tracking the episode’s reward.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'track_reward_group: 指定我们要追踪的组的参数，该组用于追踪回合的奖励。'
- en: 'filter_group: An optional filter for the group for which we want to generate
    an experience. In our current example, we need only observations from tigers (as
    we train only tigers), but in the next section, we’ll train a DQN for both tigers
    and deer, so the filter will be disabled.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'filter_group: 可选的组过滤器，用于生成经验的组。在我们当前的示例中，我们只需要来自老虎的观测数据（因为我们只训练老虎），但在下一节中，我们将训练一个同时适用于老虎和鹿的DQN，因此该过滤器将被禁用。'
- en: 'In the subsequent constructor code, we store arguments and create two useful
    mappings for agents: from the agent ID to the group name and back:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的构造函数代码中，我们存储参数并为代理创建两个有用的映射：从代理ID到组名，再从组名回到代理ID：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We also define a utility method to strip the agent’s numerical ID to get the
    group name (in our case, it will be tiger or deer).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个实用方法来提取代理的数字ID，从而获得组名（在我们的例子中，它将是老虎或鹿）。
- en: 'Now comes the main method of the class: the iterator interface, which produces
    experience items from the environment:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是该类的主要方法：迭代器接口，用于从环境中生成经验条目：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we reset the environment in the beginning and create initial states for
    agents (in case our agents will keep some state, but in this chapter’s examples,
    they are stateless).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们在开始时重置环境，并为代理创建初始状态（以防我们的代理会保持某些状态，但在本章的示例中，它们是无状态的）。
- en: 'Then, we iterate the episode until we have living agents (the same way we did
    in the example in the previous section). In this loop, we fill actions in the
    dictionary, mapping the agent ID to the action. To do this, we use PTAN BaseAgent
    instances, which work with batches of observations, so actions will be produced
    very efficiently for the whole group of agents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have actions to be executed, we send them to the environment and obtain
    dictionaries with new observations, rewards, and done and truncation flags. Then,
    we generate experience items for every alive agent we currently have:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At the end of the episode, we remember the number of steps and the average
    reward obtained by the group:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With this class at hand, our DQN training code stays almost the same as in
    the single-agent RL case. The full source code of this example can be found in
    forest_tigers_dqn.py. Here, I’m going to show only part of the code where PTAN
    agents and the experience source are created (to illustrate how MAgentExperienceSourceFirstLast
    is being used):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see, tigers are controlled by the neural network (which is a very
    simple two-layer convolution plus a two-layer fully connected net). A group of
    deer is controlled by a random number generator. The experience replay buffer
    will be populated only with the tiger experience because of the filter_group="tiger"
    argument.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Training and results
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start the training, run ./forest_tigers_dqn.py -n run_name --dev cuda. In
    one hour of training, the tiger’s test reward has reached the best score of 82,
    which is a significant improvement over the random baseline. Acting randomly,
    most tigers die after 20 steps and only a few lucky ones can live longer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate how many deer were eaten to get this score. Initially, each
    tiger has a health of 10 and spends 0.5 of their health on each step. In total,
    we have 40 tigers and 204 deer on the map (you can change this amount with command-line
    arguments). For every eaten deer, the tigers obtain 8 health points, which allows
    them to survive for an extra 16 steps. For every step, each tiger obtains a reward
    of 1, so the “excess reward” from the deer eaten by 40 tigers is 82 ⋅ 40 − 20
    ⋅ 40 = 2480\. Every deer gives 8 health points, which is converted into an extra
    16 steps of a tiger’s life, so the number of deer eaten is 2480∕16 = 155\. So,
    almost 76% of deer were hunted by the best policy we’ve got. Not bad given that
    deer are randomly placed on the map and tigers need to get to them to attack.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite likely that the policy stopped improving just because of the limited
    view of the tigers. If you are curious, you can enable the minimap in the environment
    settings and experiment. With more information about the food’s location, it’s
    likely that the policy could be improved even more.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [22.2](#x1-419002r2), the average reward and number of steps during
    the training is shown. From it, you can see that the main growth was during the
    first 300 episodes and later, the training progress was almost 0:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_02.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.2: Average reward (left) and count of steps (right) from training
    episodes'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Figure [22.3](#x1-419003r3) shows the plots for the test reward and
    steps, which demonstrate that the policy continued to improve even after 300 episodes
    (≈ 0.4 hours of training):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_03.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.3: Average reward (left) and count of steps (right) from test episodes'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The final pair of charts in Figure [22.4](#x1-419005r4) shows the training loss
    and epsilon during the training. Both plots are correlated, which indicates that
    most of the novelty during the training was obtained in the exploration phase
    (as the loss value is high, which means that new situations arise during training).
    This might be an indication that better exploration methods might be beneficial
    for the final policy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.4: Average loss (left) and epsilon (right) during the training'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, besides the training, I implemented a tool to check the models in
    action. It’s called forest_tigers_play.py and it loads the trained model and uses
    it during the episode, producing a video recording of the observations. The video
    from the best model (with a test score of 82.89) is available here: [https://www.youtube.com/shorts/ZZf80AHk538](https://www.youtube.com/shorts/ZZf80AHk538).
    As you can see, tigers’ hunting skills are now significantly better than the random
    policy: at the end of the episode, just 53 deer were left from the initial 204.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration by the tigers
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second experiment that I implemented was designed to make the tigers’ lives
    more complicated and encourage collaboration between them. The training and play
    code are the same; the only difference is in the MAgent environment’s configuration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'If you pass the argument --mode double_attack to the training utility, the
    environment data.DoubleAttackEnv will be used. The only difference is the configuration
    object, which sets additional constraints on tigers’ attacks. In the new setup,
    they can attack deer only in pairs and have to do this at the same time. A single
    tiger’s attack doesn’t have any effect. This definitely complicates the training
    and hunting, as obtaining the reward from eating the deer is now much harder for
    tigers. To start the training, you can run the same train utility, but with an
    extra command-line argument:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s take a look at the results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [22.5](#x1-420002r5), the reward and step plots for the training episodes
    are shown. As you can see, even after 2 hours of training, the reward is still
    improving. At the same time, the count of steps in the episode never exceeds 300,
    which might be an indication that tigers just don’t have nearby deer to eat and
    die from starvation (it also might just be an internal limit of steps in the environment).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_05.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.5: Average reward (left) and count of steps (right) from training
    episodes in double_attack mode'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to single-tiger hunting mode, the loss during the training is not
    decreasing (as shown in Figure [22.6](#x1-420003r6)), which might indicate that
    training hyperparameters could be improved:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_06.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.6: Average loss during training'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the model in action, you can use the same utility as before; just pass
    it the --mode double_attack argument. A video recording of the best model I got
    is available here: [https://youtu.be/VjGbzP1r7HY](https://youtu.be/VjGbzP1r7HY).
    As you can see, tigers are now moving in pairs, attacking the deer together.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Training both tigers and deer
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next example is the scenario when both tigers and deer are controlled by
    different DQN models being trained simultaneously. Tigers are rewarded for living
    longer, which stimulates them to eat more deer, as at every step in the simulation,
    they lose health points. Deer are also rewarded on every timestamp.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The code is in forest_both_dqn.py and it is an extension of the previous example.
    For both groups of agents, we have a separate DQNAgent class instance, which uses
    separate neural networks to convert observations into actions. The experience
    source is the same, but now we’re not filtering on a tiger’s group experience
    (with the parameter filter_group=None). Because of this, our replay buffer now
    contains observations from all the agents in the environment, not just from tigers
    as in the previous example. During the training, we sample a batch and split examples
    from deer and tigers into two separate batches to be used for training their networks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m not going to include all the code here, as it differs from the previous
    example only in small details. If you are curious, you can take a look at the
    source code in the GitHub repository. Figure [22.7](#x1-421001r7) shows the training
    reward and steps for tigers. You can see that initially, tigers were able to consistently
    increase their reward, but later, the growth stopped:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_07.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.7: Average reward for tigers (left) and count of steps (right) from
    training episodes'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next two plots in Figure [22.8](#x1-421002r8), the reward for tigers
    and deer during the testing is shown. There is no clear trend here; both groups
    are competing and trying to beat their opponent:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.8: Test reward for tigers (left) and deer (right)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from Figure [22.8](#x1-421002r8), deer are way more successful
    than tigers, which is not surprising, as the speed of both is the same, so the
    deer just need to move all the time and wait for the tigers to die from starvation.
    If you want, you can experiment with the environment settings by increasing either
    the tigers’ speed or the wall density.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, it is possible to visualize the learned policies with the utility
    forest_both_play.py, but now you need to pass two model files. Here is a video
    comparing the best model for deer and the best model for tigers: [https://youtube.com/shorts/vuVL1e26KqY](https://youtube.com/shorts/vuVL1e26KqY).
    In the video, all deer are just moving to the left of the field. Most likely,
    tigers can exploit this simple policy for their benefit.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The battle environment
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the tiger-deer environment, MAgent contains several other predefined
    configurations you can find in the magent2.builtin.config and magent2.environment
    packages. As a final example in this chapter, we’ll take a look at the “battle”
    configuration, where two groups of agents are fighting each other (without eating,
    thank goodness). Both agents have health points of 10 and every attack takes 2
    health points, so 5 consecutive attacks are required to get the reward for the
    agent.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code in battel_dqn.py. In this setup, one group is behaving
    randomly and another is using the DQN to improve the policy. Training took two
    hours and the DQN was able to find a decent policy, but at the end, the training
    process diverged. In Figure [22.9](#x1-422002r9), the training and test reward
    plots are shown:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_22_09.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22.9: Average reward during training (left) and test (right) in the
    battle scenario'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The video recording (produced by the tool battle_play.py) is available here:
    [https://youtube.com/shorts/ayfCa8xGY2k](https://youtube.com/shorts/ayfCa8xGY2k).
    The blue team is random and the red is controlled by the DQN.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we just touched a bit on the very interesting and dynamic field
    of MARL, which has several practical applications in trading simulation, communication
    networks, and others. There are lots of things that you can try on your own using
    the MAgent environment or other environments (like PySC2).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: My congratulations on reaching the end of the book! I hope that the book was
    useful and you enjoyed reading it as much as I enjoyed gathering the material
    and writing all the chapters. As a final word, I would like to wish you good luck
    in this exciting and dynamic area of RL. The domain is developing very rapidly,
    but with an understanding of the basics, it will become much simpler for you to
    keep track of the new developments and research in this field.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: There are many very interesting topics left uncovered, such as partially observable
    Markov decision processes (where environment observations don’t fulfill the Markov
    property) or recent approaches to exploration, such as the count-based methods.
    There has been a lot of recent activity around multi-agent methods, where many
    agents need to learn how to coordinate to solve a common problem. I also haven’t
    mentioned the memory-based RL approach, where your agent can maintain some sort
    of memory to keep its knowledge and experience. A great deal of effort is being
    put into increasing the RL sample efficiency, which will ideally be close to human
    learning performance one day, but this is still a far-off goal at the moment.
    Of course, it’s not possible to cover the full domain in just one book, because
    new ideas appear almost every day. However, the goal of this book was to give
    you a practical foundation in the field, simplifying your own learning of the
    common methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'I’d like to end by quoting Volodymyr Mnih’s words from his talk, Recent Advances,
    Frontiers and Future of Deep RL, from the Deep RL Bootcamp, Berkeley, 2017, which
    are still very relevant: “The field of deep RL is very new and everything is still
    exciting. Literally, nothing is solved yet!”'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Leave a Review!
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoyed
    it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an [Amazon review](https://packt.link/r/1835882714);
    it will only take a minute, but it makes a big difference for readers like you.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code below to receive a free ebook of your choice.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file3.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '*https://packt.link/NzOWQ*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
