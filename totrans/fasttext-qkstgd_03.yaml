- en: Creating Models Using FastText Command Line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'FastText has a powerful command line. In fact, you can call fastText a command-line-first
    library. Now, a lot of developers and researchers are not comfortable with the
    command line, and I would ask you to go through the examples in this chapter with
    greater attention. My hope is that by the end of this chapter, you will have some
    confidence in command-line file manipulations. The advantages of using the command
    line are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Commands such as `cat`, `grep`, `sed`, and `awk` are quite old and their behavior
    is well-documented on the internet. Chances are high that, for any use case that
    you might have, you will easily get snippets on Stack Overflow/Google (or your
    colleague next door will know it).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since they are generally implemented in the C language, they are very fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The commands are very crisp and concise, which means there is not a lot of code
    to write and maintain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will take a look at how classification and word vector generation works
    in fastText. In this chapter, we will explore how to implement them using the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using fastText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastText word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pretrained word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification using fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To access the command line, open the Terminal on your Linux or macOS machines,
    or the command prompt (by typing `cmd` in Windows + *R* and hitting *Enter*) on
    Windows machines, and then type `fastText`. You should see some output coming
    out. If you are not seeing anything, or getting an error saying that the command
    not found, please take a look at the previous chapter on how to install fastText
    on your computer. If you are able to see some output, the output is a basic description
    of all the options. A description of the command line options for fastText can
    be found in the *Appendix* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: All the methods and command line statements mentioned in this chapter will work
    on Linux and Mac machines. If you are a Windows user, focus more on the description
    and the logic of what is being done and follow the logic of the steps. A helpful
    guide on command line differences between Windows and Linux is mentioned in the
    *Appendix*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fastText, there are two primary use cases for the command line. These are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the core areas of focus for fastText is text classification. Text classification
    is a technique in which we learn to which set of categories the input text belongs.
    This is basically a supervised machine learning problem, so first and foremost,
    you will need a dataset that contains text and the corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Roughly speaking, machine learning algorithms run some kind of optimization
    problem on a set of matrices and vectors. They do not really understand "raw text,"
    which means that you will need to set up a pipeline to convert the raw text into
    numbers. Here are the steps that can be followed to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you need the data and hence for text classification you need a series
    of texts or documents that will be labeled. You convert them into a series of
    text-label pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is called **tokenization**. Tokenization is the process of dividing
    the text into individual pieces or tokens. Tokenization is primarily done by understanding
    the word boundaries in the given text. Many languages in the world are space delimited.
    Examples of these are English and French. In some other cases, the word boundaries
    may not be clear, such as in the case of Mandarin, Tamil, and Urdu.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the tokenization is done, based on the process you may end up with a "bag
    of words," which is essentially a vector for the document/sentence telling you
    whether a specific word is there or not, and how many times. The columns in the
    matrix are all the set of words present, which is called the dictionary, and the
    rows are the count of the particular words in the document. This is called the
    **bag**-**of**-**words** approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the bag of words into a TF-IDF matrix to reduce the weight of the common
    terms. TF-IDF has been used so that the terms that are common in the document
    do not have too much impact on the resultant matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have the matrix, you can pass the matrix as input to a classification
    algorithm, which will essentially *train a model* on this input matrix. General
    algorithms that are quite popular in this stage are logistic regression, as well
    as algorithms such as XGBoost, random forest, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the additional steps that may need to be taken are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Removal of stop words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming or a heurestic removal of end of words. This process works mostly in
    English and related languages due to the prevalence of derivational affixes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addition of n-grams to the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synonymous sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part of speech tagging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the dataset, you may need to do some or all of these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the text into lowercase. This is only required for languages using Latin,
    Greek, Cyrillic, and Armenian scripts. Examples of such languages are English,
    French, German, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strip empty lines and their correspondences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove lines with XML tags (starting with `<`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These steps should be done in both cases, for sentence classification as well
    as the creation of word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: English text and text using other Roman alphabets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will understand text processing using a sample dataset. In this chapter,
    the Yelp dataset is used. This is a popular dataset containing text reviews and
    the ratings given by users. In this dataset, you will find information about businesses
    in 11 metropolitan areas in four countries. If you download the data from the
    Kaggle link where it is shared, [https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data),
    there are various files we will see, but in our case we will only be interested
    in the review text provided by users in the `yelp_review.csv` file. As a challenge,
    we will try to see whether we can correctly predict the ratings or not.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since this information is related to a particular business, and in case you
    are interested in downloading and playing with the data, please take a look at
    these steps before downloading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Please review the Yelp dataset webpage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please review, agree to, and respect Yelp's terms of use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download `yelp_review.csv` from Kaggle. The link for that is here: [https://www.kaggle.com/yelp-dataset/yelp-dataset/data](https://www.kaggle.com/yelp-dataset/yelp-dataset/data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing the Yelp data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the data. Always take a deep look at the data. The first line
    contains the headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you check the other lines, you will see that all the individual values
    are quotes. Also, the text field has new lines in many places. Since the strength
    of fastText is in text processing, we will only be taking the "stars" and the
    "text" fields, and will try to predict the ratings based on what is written in
    the text field.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following Python script to save the text and the ratings to
    another file, since the review text has a lot of new lines and we needed to remove
    the new lines from the text. You can keep them if you want and change the new
    line to another delimiter so that the file is fastText-compatible, but for our
    example we will remove the new lines from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Python code to get only the relevant parts of the `.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Save this in a file named `parse_yelp_dataset.py` and then run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Text normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, will take a look at some text normalization techniques that
    you can use.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The removal of stop words may or may not increase the performance of your model.
    So, keep two files, one with the stop words and one with the stop words stripped
    out. We will talk about how to check model performance in the *Model testing and
    evaluation* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following script to remove the stop words. This is a Python
    script with dependencies such as `nltk`, so use it with your Anaconda installation.
    Please ensure that you have already downloaded the `nltk` `''english''` package
    before running the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the following code in a file named `remove_stop_words.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To run the file, you will need to pass the contents to the Python file. In the
    following explanations though, we are not really removing the stop words for the
    sake of brevity. You are of course encouraged to try both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we are dealing with English, it is recommended to first convert all uppercase
    letters to lowercase as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Languages using the Latin, Greek, Cyrillic, and Armenian scripts are bicameral,
    which means that there are uppercase and lowercase letters. Examples of these
    are English, French, and German. Only in such languages should you be careful
    to convert all the text to lowercase. While processing a corpus for other languages,
    this step is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the start of the files already has all the labels. If we prefix the start
    of all sentences with `__label__`, it will add all the labels with the `__label__`
    text. This prefixing of the labels is necessary as the library takes in the whole
    text as the input, and there is no specific way to specify the input and the labels
    separately, as you might have seen in `scikit-learn` or other libraries. You can
    change the specific label prefix though, as you will see in the *A**ppendix*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to read the file in fastText and enable fastText to differentiate between
    normal text and label text, you will need to append `__label__` to the labels.
    One of the ways you can do that easily in the command line is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Separate out and remove some of the punctuation that may be irrelevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Do not forget to keep checking how the data has been transformed after each
    transformation. On checking the data now, you can see that there is a comma at
    the beginning. There are also a lot of dots (`.`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Remove the commas and the dots. Keep in mind that fastText does not require
    the removal of punctuation and lowercasing all the letters; in fact, in some cases
    these may be important. Remember to take all the advice given here with a grain
    of salt and try all possible options you can think of. The ultimate aim is to
    train the best model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Finally, remove all the consecutive spaces. Please note that in the following
    example, after all these transformations, the files are no longer in the `.csv`
    format, but that is fine for us because at the end of the day, `.csv` files are
    also text files and hence you should be fine with using `.txt` or any other textual
    format. As long as the files are text files with the contents in UTF-8 format,
    you should be good to go.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Shuffling all the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shuffling the data before training the classifier is important. If the labels
    for the data are clustered, then the precision and recall, and hence the performance
    of the resulting model, will be low. This is because fastText uses stochastic
    gradient descent to learn the model. The training data from the files is processed
    in order. In our example, this is not the case as the labels of the same class
    are not together, but still it may be a good idea to keep this in mind and always
    shuffle before training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Nix systems, you have the shuffle command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, the shuffle command is quite slow and you may want to consider using
    the `perl` one-liner in the case of large files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Dividing into training and validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model performance evaluation should always be done on independent data. You
    should always separate out your whole dataset into train and test sets. But, dividing
    too much also reduces the amount of data that you have for training, so 80% is
    a good midpoint. You can divide the file into an 80-20 split using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we take a look at how to go about the steps of model training
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you can start the training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be shown when training, and at the end you should get an output
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you now check the `result/yelp/` directory, you should be able to see two
    files with extensions `.vec` and `.bin`. The `.bin` file is the trained classifier.
    The `.vec` file has all the words with the vectors for the individual words. You
    can open the `.vec` file. It is just a text file. However, take care to open it
    using a lightweight text editor such as Sublime Text or Notepad++, as it will
    be a big file. Or, just use command line tools such as `head` or `tail`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the vectors created in our case. The first line has the dimensions
    of the vectors, which are (`1459620`, `100`) in our case. The next two lines are
    the vectors for `.` and `the`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Model testing and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how to create model files in fastText, you will need to test
    and check the performance of your model, and report its efficacy in real terms.
    This can be done using various performance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the accuracy of a classification model, two parameters that are very
    popular and are supported by fastText are precision and recall. Recall is the
    percentage of labels that are correctly recalled of all the labels that actually
    exist, and precision is the percentage of all the labels that were predicted correctly.
    These two parameter values can be checked using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The precision and recall are currently at 68%. Let's optimize some parameters
    and see if we can make the model better.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a model, you can also see the performance of the model with
    respect to the different labels using the confusion matrix. Along with precision
    and recall, confusion matrices give a good idea of the **true negatives** (**TN**)
    and the **false positives** (**FP**) as well. In an ideal world, all the diagonals
    have high values, while all the remaining cells have negligible values, but in
    a real scenario, you may need to chose if you are OK with having high FP values
    or high **false negative** (**FN**) values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the confusion matrix, you will need to do some post-processing. Separate
    the sentences from the labels. Then, using the predict command, you will be able
    to predict the label for each test line. A Python script is provided, which can
    be used to get the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can download the Python script from this gist: [https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py](https://gist.github.com/loretoparisi/41b918add11893d761d0ec12a3a4e1aa#file-fasttext_confusion_matrix-py).
    Or, you can get it from the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple hyperparameters that can be supplied when training the model
    to improve the model. Take a look at some of the hyperparameters here and their
    effects on model training.
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, fastText takes a look at each data point five times. You can change
    this using the `-epoch` command. In the following example, we change the epoch
    parameter to 25 and see whether there is any improvement in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the model is 68.6% for precision and recall, which is only a
    0.1% improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Learning rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This may be because we already have a huge number of samples. Another important
    hyperparameter that we can change is the learning rate, using the `-lr` argument.
    The learning rate controls how "fast" the model updates during training. This
    parameter controls the size of the update that is applied to the parameters of
    the models. Changing the learning rate from 0.025 to 1.0 means that the updates
    that are applied to the model are 40 times larger. In our model, we could also
    see that the learning rate was becoming `0` at the end. This means that the model
    was not learning at all by the end. Lets try to make the learning rate as `1`
    and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result for this model was the same as before. There was no difference when
    changing the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have one more hyperparameter that may have a huge influence on the performance
    of the model. By default, when creating word vectors for the model, unigrams are
    used. Unigrams are n-grams where *n* is 1\. N-grams can be best explained using
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://stackoverflow.com/a/45477420/5417164](https://stackoverflow.com/a/45477420/5417164)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You also can fix the value of `N` in fastText using the `-wordNgrams` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the precision and recall are 71.8%, which is a 3.2% improvement. Lets
    try for some more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Making N = 3 resulted in a decrease in performance. So, let's keep the value
    of N as 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can combine all the parameters to create the new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Start with pretrained word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the text corpus that you have is not huge, it is generally advised to start
    with some pretrained word vectors for the language that you are training the classifier
    for, or the classification results may be poor. How to create word vectors from
    your corpus is handled in depth in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In our case, there was not much improvement. The precision and recall increased
    marginally and stood at 68.5%.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best fastText hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FastText has a lot of hyperparameters that you can optimize to find the right
    balance for your model. For a classifier, you can start with the loss functions,
    see whether changing the character n-grams makes sense, and see whether changing
    the learning rate and dimensions have any effect.
  prefs: []
  type: TYPE_NORMAL
- en: A popular algorithm for implementing hyperparameter optimization is using the
    grid-search approach. Since your aim is to find a good model, you will have a
    training dataset and a test dataset. Let's say the training data is the `train.txt`
    file and the test data is the `test.txt` file. You are essentially solving an
    optimization problem (P), which is the function of the combination of weights
    in this case *w*^' and the hyperparameters, be in n-grams, learning rate or epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, you understand that solving the optimization problem for a fixed set of
    values for the hyperparameters gives you a specific model. Since the optimal model
    (call it model*) is a function of the hyperparameters, we can write it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, you can use this model* to predict on the training data to get the accuracy.
    Thus, the goal of hyperparameter optimization is to find the set of hyperparameters
    that gives the highest accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this calculation of the best model is going to be quite expensive.
    There is no magic mantra, no magic formula to find the hyperparameters for the
    best model. Just taking one hyperparameter, the learning rate, would make the
    calculation impractical. This is a continuous variable and you would need to feed
    in each specific value, compute the model, and check the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we resort to a grid search: basically, picking a bunch of values
    for the hyperparameters based on a heurestic, and based on all the combinations
    of the values, feeding them into the calculation and picking the set of values
    with the best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: This is called a grid search because the set of values that are considered,
    when plotted on a graph, look like a grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'How you can implement this is by defining an array of values for your hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a global variable where we will save the individual variables,
    as and when we find better models, and initialize them to be 0\. We will also
    have a global performance variable to store the present best performance and set
    it to 0 initially. In this case, since we are experimenting with three hyperparameters,
    we will have the variable as length 3, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the implementation of the `for` loops that will cycle through all
    the combinations of the values. Note that the depth of the `for` loop would be
    based on the number of hyperparameters that you are cycling through:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can probably guess, since we are checking for two values each for the
    three hyperparameters, the number of times training will happen is 2 x 2 x 2 =
    8\. So, if each training step takes, say, 5 minutes, that would mean that the
    total process will take 8 x 5 minutes or 40 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go to the mean. Here is the training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is done, then comes the test phase. We save the test data
    to a file so that we can compare the results later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the comparison and saving the best models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can extend this script to bring in more hyperparameters as well. You
    can find the whole code in the repo in the file.
  prefs: []
  type: TYPE_NORMAL
- en: Model quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the help of model quantization, the fastest models have the ability to
    fit on mobile and on small devices such as Raspberry Pi. Since the code is open
    source, there are Java and Swift libraries that can be used to load the quantized
    models and serve them in Android and iOS apps respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm for compressing the fastText models was created with collaboration
    between fastText and the **Facebook AI Research** (**FAIR**) team. This results
    in the reduction of fastText models by a huge amount. FastText models that are
    of the range of hundreds of MB get reduced to around 1-2 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing quantization can be done using the quantize argument. You will
    need to train a model with the normal route though:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that there is a huge difference between the quantized model and the unquantized
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `.bin` file is about 466 MB, while the quantized model is just 1.6 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly there seems to be a slight increase in precision and recall values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: So you get almost no difference in performance and a good saving in space. You
    can now deploy this model in a smaller device. In the next chapter, we will discuss
    how this model quantization works. Also, in [Chapter 7](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755),
    *Deploying Models to Web and Mobile*, we will discuss how you can package a quantized
    model as part of an Android app.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, quantization only works for supervised models for now, but this
    may change in the future, so keep your fastText installation updated.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is created, you can now see the parameters that were used while
    generating the model. This can be useful later when you are thinking deeper about
    your data and would like to change some model parameters, or for general documentation
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dict` parameter gives information on the dictionary of words that was
    used in training. In the preceding training procedure, 1,459,625 words have been
    used, which can be seen as follows. `was` is used 8,272,495 times, `crinkle-also`
    is used only once in the whole set of sentences, and so on. It also gives information
    on whether the word is used as a word or a label. As you can see, the labels are
    listed at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The rows of the dump of input and output correspond to the parameters of the
    model. In our model, the first 1,459,620 rows of input are the vectors associated
    to the individual words, while the remaining 2 million rows are used to represent
    subwords. Those 2 million subwords were chosen to represent the overall meaning
    and can be understood from the bucket parameter in the output for the dump of
    the args as well. The rows of output are the vectors associated to the context
    or our labels. Usually, when learning unsupervised word representations, these
    are not kept after training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The transformations mentioned in this section can be seen in the `transformations.sh`
    file in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: FastText word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second major focus of fastText is creating word embeddings for the input
    text. During training, fastText looks at the supplied text corpus and forms a
    high-dimensional vector space model, where it tries to encapsulate as much meaning
    as possible. The aim of creating the vectors space is that the vectors of similar
    words should be near to each other. In fastText, these word vectors are then saved
    in two files, similar to what you have seen in text classification: a `.bin` file
    and a `.vec` file.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at the creation and use of word vectors using
    the fastText command line.
  prefs: []
  type: TYPE_NORMAL
- en: Creating word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now take a look at how to go about creating word vectors in fastText.
    You will probably be working with and building a solution for a specific domain,
    and in such a case, my advice would be to generate the raw text from the specific
    domain. But in cases where the raw text is not available to you, then you can
    use the help of Wikipedia, which is a huge collection of raw text in multiple
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start with word vectors, you will need data or a text corpus. If you are
    lucky, you have the text corpus available to you. If you are not so lucky, which
    you will eventually be if you are interested in solving interesting problems in
    NLP, you will not have the data with you. In those cases, Wikipedia is your friend.
    The best thing about Wikipedia is that it is the best source of written text in
    more than 250 languages from around the world. Granted that is a minuscule number
    compared to the number of languages there are, but still that will probably be
    enough for most of your use cases. And if you are working in a language for which
    there are not enough Wikipedia resources, maybe you should raise awareness of
    how important Wikipedia is in your language community and ask the community to
    contribute to Wikipedia more. Once you know your target language, you can download
    the Wikipedia corpus using the `get-wikimedia.sh` file. You can get this file
    from the GitHub repository of fastText. A slightly updated version of the file
    can be copied from the *Appendix*.
  prefs: []
  type: TYPE_NORMAL
- en: Use the `get-wikimedia.sh` file to download the Wikipedia corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the list of all the languages that Wikipedia has articles on at
    this link: [https://meta.wikimedia.org/wiki/List_of_Wikipedias](https://meta.wikimedia.org/wiki/List_of_Wikipedias).
    At this link, the list of languages is given in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is the third column that needs your attention. Note down the third value
    for your language of choice and run `bash get-wikimedia.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You will receive a BZ2 file. To uncompress a BZ2 file, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: If you open the file (be careful while doing this, the file is huge), you will
    find a lot of unnecessary stuff, such as HTML tags and links. So, you will need
    to clean the text with the `wikifil.pl` script, which was written by Matt Mahoney.
    This script is distributed with the fastText GitHub files.
  prefs: []
  type: TYPE_NORMAL
- en: Text normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have downloaded the English corpus, you can use the `wikifil.pl` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we have the Japanese text and hence we will be using WikiExtractor
    ([https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor))
    to extract the text from the BZ2 file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'There are still a lot of tags and English words that are part of the tags.
    You will need to do some more processing and text cleaning to make the corpus
    ready for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can go ahead and start the training process. We will keep the English
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Create word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you can create the word vectors. Create the `result` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two algorithms that are supported by fastText for creating word vectors,
    `skipgram` and `cbow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evaluation for word vectors is largely a manual process. In such cases,
    you can try some samples of the words and see if the model gives adequate results.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the methods that you can use for model evaluation are looking at the
    nearest neighbors of the model and looking at some word analogies. One popular
    method is through t-SNE visualizations. We will look at t-SNE visualizations in
    [Chapter 4](part0098.html#2TEN40-05950c18a75943d0a581d9ddc51f2755),
  prefs: []
  type: TYPE_NORMAL
- en: '*Sentence Classification in FastText*.'
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get the nearest neighbors of a given word, pass the `nn` argument, and then
    you will need to give the path to the BIN file. For brevity, we will show only
    three results here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Similar words to sleep (![](img/00013.jpeg)) give the previous results, which
    mean "return to the mainland"; "Hachinosu," which is a peak in Antarctica; and
    "national election." The results are random in our case and therefore not good.
  prefs: []
  type: TYPE_NORMAL
- en: Word analogies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word analogies are a good way of finding out whether the model is working or
    not. How analogies work is that two groups of words with similar relationships
    should be separated by similar distances in the vector space. So, when you provide
    words for man, woman, and king, the result should be queen, as in a good vector
    space, the distance between word vector denoting "man" and the word vector denoting
    "woman" should be close to the distance between the word vector denoting "king"
    and the word vector denoting "queen." The command shows 10, but here only the
    top three are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The meaning of the symbols in this code block are "cracking," "Monkey King,"
    and "King", which are not very helpful in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Other parameters when training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to supervised learning, you can change various hyperparameters and see
    whether the models that are created work better. So, you can fix the minimal number
    of word occurrences with the `-minCount` parameter and the maximum length of word
    n-grams with the `-wordNgrams` parameter. You can change the number of buckets
    that fastText uses to hash the word and character n-grams to limit memory use.
    If you have a huge memory in your system, you can change this bucket parameter
    by passing a larger value than 2 million to see whether your model performance
    increases using the `-bucket` parameter. You can change the minimum and maximum
    length of character n-grams using the parameters `-minn` and `-maxn`. Change the
    sampling threshold using `-t`, change the learning rate using `-lr`, change the
    rate of updates for the learning rate using `-lrUpdateRate`, change the dimensions
    of the word vectors using `-dim`, change the size of the context window using
    `-ws`, change the number of epochs, which is the number of times each row is looked
    at during training, from the default 5 using `-epoch`, change the number of negatives
    sampled using `-neg`, and change the loss function that is used from the default
    **ns** (**negative sampling**) to softmax or **hs** (**hierarchical softmax**).
    The default number of threads used is 12, but generally people have four cores
    or eight cores, and hence you can change the number of threads to optimally use
    the cores using the `-thread` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Out of vocabulary words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'FastText also supports out of vocabulary words. FastText is able to do that
    because it not only keeps track of word-level N-grams, but also character-level
    n-grams. So, things like "learn," "learns," and "learned" look similar to it.
    To get the vectors for out of vocabulary words, you have to use binary models,
    which means the model files with the `.bin` extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Facebook word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Facebook has released a large number of pretrained word vectors based on Wikipedia
    and common crawling, which you can download from their website and use in your
    projects ([https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)).
    The common crawl models are CBOW models, and the wiki models are skip-gram models.
    Vectors are of dimensions 300 and character n-grams of length 5 are used, and
    a window size of 5 and 10 negatives are used.
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use pretrained word vectors for your supervised learning task. This
    has been discussed briefly under supervised learning. An example command is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: There are some things that need to be taken care of while using pretrained vectors.
    You can find them at [https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification](https://stackoverflow.com/questions/47692906/fasttext-using-pre-trained-word-vector-for-text-classification).
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Facebook has done a lot of work on neural machine translation in the form of
    MUSE. MUSE is a library for multilingual unsupervised and supervised word embeddings.
    MUSE uses and is built on top of fastText word embeddings. The word embeddings
    that you get with fastText are monolingual, and hence the vectors need to be aligned
    to effectively translate from one language to the other. As part of MUSE, see
    the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: fastText Wikipedia supervised word embeddings for 30 languages were released.
    These word embeddings are aligned in a single vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 110 large-scale ground truth bilingual dictionaries were also released so that
    you can train your own models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you took a look at how you can combine the command line text
    transformation capabilities of the *Nix shell and the fastText library to implement
    a training, validation, and prediction pipeline. The commands that you explored
    in this chapter are not only versatile, they are fast as well. Having good mastery
    over the command line, along with a fastText app, should enable you to create
    fast prototypes and deploy them in a fast-paced environment. With this, the first
    part of the book is complete.
  prefs: []
  type: TYPE_NORMAL
- en: The next part of the book is about the theory and algorithms that have gone
    into making the package, with the next chapter being about unsupervised learning
    using fastText.
  prefs: []
  type: TYPE_NORMAL
