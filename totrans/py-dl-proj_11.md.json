["```py\n# Import TensorFlow and enable eager execution\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n```", "```py\nannotation_zip = tf.keras.utils.get_file('captions.zip', \n                                          cache_subdir=os.path.abspath('.'),\n                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n                                          extract = True)\nannotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n\nname_of_zip = 'train2014.zip'\nif not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n  image_zip = tf.keras.utils.get_file(name_of_zip, \n                                      cache_subdir=os.path.abspath('.'),\n                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n                                      extract = True)\n  PATH = os.path.dirname(image_zip)+'/train2014/'\nelse:\n  PATH = os.path.abspath('.')+'/train2014/'\n```", "```py\nDownloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip \n252878848/252872794 [==============================] - 6s 0us/step \nDownloading data from http://images.cocodataset.org/zips/train2014.zip \n13510574080/13510573713 [==============================] - 322s 0us/step\n```", "```py\n# read the json annotation file\nwith open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# storing the captions and the image name in vectors\nall_captions = []\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    image_id = annot['image_id']\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n\n    all_img_name_vector.append(full_coco_image_path)\n    all_captions.append(caption)\n\n# shuffling the captions and image_names together\n# setting a random state\ntrain_captions, img_name_vector = shuffle(all_captions,\n                                          all_img_name_vector,\n                                          random_state=1)\n\n# selecting the first 40000 captions from the shuffled set\nnum_examples = 40000\ntrain_captions = train_captions[:num_examples]\nimg_name_vector = img_name_vector[:num_examples]\n```", "```py\n#Resizing the image to (299, 299)\n#Using the preprocess_input method to place the pixels in the range of -1 to 1.\n\ndef load_image(image_path):\n    img = tf.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize_images(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path\n\n```", "```py\nimage_model = tf.keras.applications.InceptionV3(include_top=False, \n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n```", "```py\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87916544/87910968 [==============================] - 40s 0us/step\n```", "```py\nencode_train = sorted(set(img_name_vector))\n#Load images\nimage_dataset = tf.data.Dataset.from_tensor_slices(\n                                encode_train).map(load_image).batch(16)\n# Extract features\nfor img, path in image_dataset:\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features, \n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n#Dump into disk\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())\n```", "```py\n# Helper func to find the maximum length of any caption in our dataset\n\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)\n\n# Performing tokenization on the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n                                                  oov_token=\"<unk>\", \n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n\n# Converting text into sequence of numbers\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\n\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\n\n# putting <unk> token in the word2idx dictionary\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1\ntokenizer.word_index['<pad>'] = 0\n\n# creating the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\n\n# creating a reverse mapping (index -> word)\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\n\n# padding each vector to the max_length of the captions\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n\n# calculating the max_length \n# used to store the attention weights\nmax_length = calc_max_length(train_seqs)\n\n```", "```py\nimg_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector,test_size=0.2,random_state=0)\n\n# Checking the sample counts\nprint (\"No of Training Images:\",len(img_name_train))\nprint (\"No of Training Caption: \",len(cap_train) )\nprint (\"No of Training Images\",len(img_name_val))\nprint (\"No of Training Caption:\",len(cap_val) )\n\nNo of Training Images: 24000\nNo of Training Caption:  24000\nNo of Training Images 6000\nNo of Training Caption: 6000\n```", "```py\n# Defining parameters\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = len(tokenizer.word_index)\n\n# shape of the vector extracted from Inception-V3 is (64, 2048)\n# these two variables represent that\nfeatures_shape = 2048\nattention_features_shape = 64\n\n# loading the numpy files \ndef map_func(img_name, cap):\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n    return img_tensor, cap\n\n#We use the from_tensor_slices to load the raw data and transform them into the tensors\n\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Using the map() to load the numpy files in parallel\n# NOTE: Make sure to set num_parallel_calls to the number of CPU cores you have\n# https://www.tensorflow.org/api_docs/python/tf/py_func\ndataset = dataset.map(lambda item1, item2: tf.py_func(\n          map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=8)\n\n# shuffling and batching\ndataset = dataset.shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE)\ndataset = dataset.prefetch(1)\n```", "```py\ndef gru(units):\n  if tf.test.is_gpu_available():\n    return tf.keras.layers.CuDNNGRU(units, \n                                    return_sequences=True, \n                                    return_state=True, \n                                    recurrent_initializer='glorot_uniform')\n  else:\n    return tf.keras.layers.GRU(units, \n                               return_sequences=True, \n                               return_state=True, \n                               recurrent_activation='sigmoid', \n                               recurrent_initializer='glorot_uniform')\n```", "```py\nclass BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # we get 1 at the last axis because we are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n\n```", "```py\nclass CNN_Encoder(tf.keras.Model):\n    # Since we have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x\n```", "```py\nclass RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = gru(self.units)\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)\n```", "```py\noptimizer = tf.train.AdamOptimizer()\n\n# We are masking the loss calculated for padding\ndef loss_function(real, pred):\n    mask = 1 - np.equal(real, 0)\n    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n    return tf.reduce_mean(loss_)\n```", "```py\nEPOCHS = 20\nloss_plot = []\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset):\n        loss = 0\n\n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = decoder.reset_state(batch_size=target.shape[0])\n\n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n        with tf.GradientTape() as tape:\n            features = encoder(img_tensor)\n\n            for i in range(1, target.shape[1]):\n                # passing the features through the decoder\n                predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n                loss += loss_function(target[:, i], predictions)\n\n                # using teacher forcing\n                dec_input = tf.expand_dims(target[:, i], 1)\n\n        total_loss += (loss / int(target.shape[1]))\n\n        variables = encoder.variables + decoder.variables\n\n        gradients = tape.gradient(loss, variables) \n\n        optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n\n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, \n                                                          batch, \n                                                          loss.numpy() / int(target.shape[1])))\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / len(cap_vector))\n\n    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, \n                                         total_loss/len(cap_vector)))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n\n```", "```py\nplt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()\n```", "```py\ndef evaluate(image):\n attention_plot = np.zeros((max_length, attention_features_shape))\n\n hidden = decoder.reset_state(batch_size=1)\n\n temp_input = tf.expand_dims(load_image(image)[0], 0)\n img_tensor_val = image_features_extract_model(temp_input)\n img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n features = encoder(img_tensor_val)\n\n dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n result = []\n\n for i in range(max_length):\n predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n predicted_id = tf.argmax(predictions[0]).numpy()\n result.append(index_word[predicted_id])\n\n if index_word[predicted_id] == '<end>':\n return result, attention_plot\n\n dec_input = tf.expand_dims([predicted_id], 0)\n\n attention_plot = attention_plot[:len(result), :]\n return result, attention_plot\n```", "```py\ndef plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()\n\n# captions on the validation set\nrid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nreal_caption = ' '.join([index_word[i] for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot)\n# opening the image\nImage.open(img_name_val[rid])\n```", "```py\ncheckpoint_dir = './my_model'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(\n                                 optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder,\n                                )\n\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n```", "```py\ndef evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n # Extract features from the test image\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n # Feature is fed into the encoder\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n # Prediction loop\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result.append(index_word[predicted_id])\n # Hard stop when end token is predicted\n        if index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot\n```", "```py\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: rahulkumar\n\"\"\"\n\nfrom flask import Flask , request, jsonify\n\nimport time\nfrom inference import evaluate\nimport tensorflow as tf\n\napp = Flask(__name__)\n\n@app.route(\"/wowme\")\ndef AutoImageCaption():\n    image_url=request.args.get('image')\n    print('image_url')\n    image_extension = image_url[-4:]\n    image_path = tf.keras.utils.get_file(str(int(time.time()))+image_extension, origin=image_url)\n    result, attention_plot = evaluate(image_path)\n    data = {'Prediction Caption:': ' '.join(result)}\n\n    return jsonify(data)\n\nif __name__ == \"__main__\":\n    app.run(host = '0.0.0.0',port=8081)\n```", "```py\npython caption_deploy_api.py \n```", "```py\n* Running on http://0.0.0.0:8081/ (Press CTRL+C to quit)\n```", "```py\ncurl 0.0.0.0:8081/wowme?image=https://www.beautifulpeopleibiza.com/images/BPI/img_bpi_destacada.jpg \n```"]