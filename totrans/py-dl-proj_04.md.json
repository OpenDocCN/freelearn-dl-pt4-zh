["```py\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n```", "```py\ntext = u\"\"\"\nDealing with textual data is very crucial so to handle these text data we need some \nbasic text processing steps. Most of the processing steps covered in this section are \ncommonly used in NLP and involve the combination of several steps into a single \nexecutable flow. This is usually referred to as the NLP pipeline. These flow \ncan be a combination of tokenization, stemming, word frequency, parts of \nspeech tagging, etc.\n\"\"\"\n\n# Sentence Tokenization\nsentenses = nltk.sent_tokenize(text)\n\n# Word Tokenization\nwords = [nltk.word_tokenize(s) for s in sentenses]\n\nOUTPUT:\nSENTENCES:\n[u'\\nDealing with textual data is very crucial so to handle these text data we need some \\nbasic text processing steps.', \nu'Most of the processing steps covered in this section are \\ncommonly used in NLP and involve the combination of several steps into a single \\nexecutable flow.', \nu'This is usually referred to as the NLP pipeline.', \nu'These flow \\ncan be a combination of tokenization, stemming, word frequency, parts of \\nspeech tagging, etc.']\n\nWORDS:\n[[u'Dealing', u'with', u'textual', u'data', u'is', u'very', u'crucial', u'so', u'to', u'handle', u'these', u'text', u'data', u'we', u'need', u'some', u'basic', u'text', u'processing', u'steps', u'.'], [u'Most', u'of', u'the', u'processing', u'steps', u'covered', u'in', u'this', u'section', u'are', u'commonly', u'used', u'in', u'NLP', u'and', u'involve', u'the', u'combination', u'of', u'several', u'steps', u'into', u'a', u'single', u'executable', u'flow', u'.'], [u'This', u'is', u'usually', u'referred', u'to', u'as', u'the', u'NLP', u'pipeline', u'.'], [u'These', u'flow', u'can', u'be', u'a', u'combination', u'of', u'tokenization', u',', u'stemming', u',', u'word', u'frequency', u',', u'parts', u'of', u'speech', u'tagging', u',', u'etc', u'.']]\n\n```", "```py\ntagged_wt = [nltk.pos_tag(w)for w in words]\n\n[[('One', 'CD'), ('way', 'NN'), ('to', 'TO'), ('extract', 'VB'), ('meaning', 'VBG'), ('from', 'IN'), ('text', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('analyze', 'VB'), ('individual', 'JJ'), ('words', 'NNS'), ('.', '.')], [('The', 'DT'), ('processes', 'NNS'), ('of', 'IN'), ('breaking', 'VBG'), ('up', 'RP'), ('a', 'DT'), ('text', 'NN'), ('into', 'IN'), ('words', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('tokenization', 'NN'), ('--', ':'), ('the', 'DT'), ('resulting', 'JJ'), ('words', 'NNS'), ('are', 'VBP'), ('referred', 'VBN'), ('to', 'TO'), ('as', 'IN'), ('tokens', 'NNS'), ('.', '.')], [('Punctuation', 'NN'), ('marks', 'NNS'), ('are', 'VBP'), ('also', 'RB'), ('tokens', 'NNS'), ('.', '.')], [('Each', 'DT'), ('token', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('has', 'VBZ'), ('several', 'JJ'), ('attributes', 'IN'), ('we', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('for', 'IN'), ('analysis', 'NN'), ('.', '.')]]\n\npatternPOS= []\nfor tag in tagged_wt:\n  patternPOS.append([v for k,v in tag])\n\n[['CD', 'NN', 'TO', 'VB', 'VBG', 'IN', 'NN', 'VBZ', 'TO', 'VB', 'JJ', 'NNS', '.'], ['DT', 'NNS', 'IN', 'VBG', 'RP', 'DT', 'NN', 'IN', 'NNS', 'VBZ', 'VBN', 'NN', ':', 'DT', 'JJ', 'NNS', 'VBP', 'VBN', 'TO', 'IN', 'NNS', '.'], ['NN', 'NNS', 'VBP', 'RB', 'NNS', '.'], ['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJ', 'IN', 'PRP', 'MD', 'VB', 'IN', 'NN', '.'], ['DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'CD', 'NN', ':', 'NNS', 'VBP', 'DT', 'NN', ',', 'NN', ',', 'CC', 'NN', ':', 'NNS', 'VBP', 'NNS', 'CC', 'NNS', ':', 'NNS', 'VBP', 'NNS', 'IN', 'NN', 'NNS', '.'], ['VBG', 'DT', 'NNS', ',', 'PRP', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'VBG', 'DT', 'RBS', 'JJ', 'NNS', ',', 'NNS', ',', 'CC', 'NNS', '.']]\n\n```", "```py\nnouns = [] \nfor tag in tagged_wt:\nnouns.append([k for k,v in tag if v in ['NN','NNS','NNP','NNPS']])\n\n[['way', 'text', 'words'], ['processes', 'text', 'words', 'tokenization', 'words', 'tokens'], ['Punctuation', 'marks', 'tokens'], ['token', 'sentence', 'analysis'], ['part', 'speech', 'word', 'example', 'nouns', 'person', 'place', 'thing', 'verbs', 'actions', 'occurences', 'adjectives', 'words', 'describe', 'nouns'], ['attributes', 'summary', 'piece', 'text', 'nouns', 'verbs', 'adjectives']]\n```", "```py\nverbs = [] \nfor tag in tagged_wt:\nverbs.append([k for k,v in tag if v in ['VB','VBD','VBG','VBN','VBP','VBZ']])\n\n[['extract', 'meaning', 'is', 'analyze'], ['breaking', 'is', 'called', 'are', 'referred'], ['are'], ['has', 'use'], ['is', 'are', 'are', 'are'], ['Using', \"'s\", 'create', 'counting']]\n\n```", "```py\n! pip install -q spacy \n! pip install -q tabulate\n! python -m spacy download en_core_web_lg\n\nfrom collections import Counter\nimport spacy\nfrom tabulate import tabulate\nnlp = spacy.load('en_core_web_lg')\n\ndoc = nlp(text)\nnoun_counter = Counter(token.lemma_ for token in doc if token.pos_ == 'NOUN')\n\nprint(tabulate(noun_counter.most_common(5), headers=['Noun', 'Count']))\n```", "```py\nNoun         Count \n-----------  ------- \nstep          3 \ncombination   2 \ntext          2 \nprocessing    2 \ndatum         2\n```", "```py\ndoc = nlp(sentenses[2])\nspacy.displacy.render(doc,style='dep', options={'distance' : 140}, jupyter=True)\n```", "```py\ndoc = nlp(u\"My name is Jack and I live in India.\")\nentity_types = ((ent.text, ent.label_) for ent in doc.ents)\nprint(tabulate(entity_types, headers=['Entity', 'Entity Type']))\n\nOutput:\nEntity     Entity Type \n--------   ------------- \nJack       PERSON \nIndia      GPE\n```", "```py\nimport pandas as pd\n\nfilepath = 'sample_data.csv'\ncsv_reader=pd.read_csv(filepath)\n\nquestion_list = csv_reader[csv_reader.columns[0]].values.tolist()\nanswers_list = csv_reader[csv_reader.columns[1]].values.tolist()\n\nquery= 'Can I get an Americano, btw how much it will cost ?'\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df=0, ngram_range=(2, 4), strip_accents='unicode',norm='l2' , encoding='ISO-8859-1')\n```", "```py\n# We create an array for our train data set (questions)\nX_train = vectorizer.fit_transform(np.array([''.join(que) for que in question_list]))\n\n# Next step is to transform the query sent by user to bot (test data)\nX_query=vectorizer.transform(query)\n```", "```py\nXX_similarity=np.dot(X_train.todense(), X_query.transpose().todense())\n```", "```py\nXX_sim_scores= np.array(XX_similarity).flatten().tolist()\n```", "```py\ndict_sim= dict(enumerate(XX_sim_scores))\n\nsorted_dict_sim = sorted(dict_sim.items(), key=operator.itemgetter(1), reverse =True)\n```", "```py\nif sorted_dict_sim[0][1]==0:\n       print(\"Sorry I have no answer, please try asking again in a nicer way :)\")\nelif sorted_dic_sim[0][1]>0:\n       print answer_list [sorted_dic_sim[0][0]]\n```", "```py\n\"I'm looking for an Italian restaurant in the center of town\"\n```", "```py\nintent: search_restaurant\nentities:     \n      - cuisine : Italian    \n     - location : center of town\n```", "```py\npip install rasa_nlu\npip install coloredlogs sklearn_crfsuite spacy\npython -m spacy download en\n```", "```py\n# intent_list : Only intent part\n[\n  {\n    \"text\": \"hey\",\n    \"intent\": \"greet\"\n  },\n  {\n    \"text\": \"hello\",\n    \"intent\": \"greet\"\n  }\n]\n\n# entity_list : Intent with entities\n[{\n  \"text\": \"show me indian restaurants\",\n  \"intent\": \"restaurant_search\",\n  \"entities\": [\n    {\n      \"start\": 8,\n      \"end\": 15,\n      \"value\": \"indian\",\n      \"entity\": \"cuisine\"\n    }\n  ]\n},\n]\n\n```", "```py\n{\n  \"rasa_nlu_data\": {\n    \"entity_examples\": [entity_list],\n    \"intent_examples\": [intent_list]\n  }\n}\n```", "```py\nlanguage: \"en\"\npipeline: \"spacy_sklearn\"\nfine_tune_spacy_ner: true\n```", "```py\npython -m rasa_nlu.train \\\n    --config config_spacy.yml \\\n    --data restaurant.json \\\n    --path projects\n```", "```py\npython -m rasa_nlu.server --path projects\n```", "```py\n2018-05-23 21:34:23+0530 [-] Log opened.\n2018-05-23 21:34:23+0530 [-] Site starting on 5000\n2018-05-23 21:34:23+0530 [-] Starting factory <twisted.web.server.Site instance at 0x1062207e8>\n\n```", "```py\ncurl -X POST localhost:5000/parse -d '{\"q\":\"I am looking for Mexican food\"}' | python -m json.tool\n\nOutput:\n{\n \"entities\": [\n {\n \"confidence\": 0.5348393725109971,\n \"end\": 24,\n \"entity\": \"cuisine\",\n \"extractor\": \"ner_crf\",\n \"start\": 17,\n \"value\": \"mexican\"\n }\n ],\n \"intent\": {\n \"confidence\": 0.7584285478135262,\n \"name\": \"restaurant_search\"\n },\n \"intent_ranking\": [\n {\n \"confidence\": 0.7584285478135262,\n \"name\": \"restaurant_search\"\n },\n {\n \"confidence\": 0.11009204166074991,\n \"name\": \"goodbye\"\n },\n {\n \"confidence\": 0.08219245368495268,\n \"name\": \"affirm\"\n },\n {\n \"confidence\": 0.049286956840770876,\n \"name\": \"greet\"\n }\n ],\n \"model\": \"model_20180523-213216\",\n \"project\": \"default\",\n \"text\": \"I am looking for Mexican food\"\n}\n```", "```py\ncd Chapter04/\n```", "```py\npython -m rasa_nlu.server --path ./rasa_version/projects\n```", "```py\npython chatbot_api.py\n```", "```py\ncurl http://localhost:8080/version1?query=Can I get an Americano\n```", "```py\nhttp://localhost:8080/version2?query=where is Indian cafe\n```"]