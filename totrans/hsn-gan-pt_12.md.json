["```py\nimport sys\nimport platform\nimport os\nimport json\nimport csv\nimport re\n\ndatapath = './data'\nredditfile = 'reddit_jokes.json'\nstupidfile = 'stupidstuff.json'\nwockafile = 'wocka.json'\noutfile = 'jokes.csv'\nheaders = ['row', 'Joke', 'Title', 'Body', 'ID',\n           'Score', 'Category', 'Other', 'Source']\n```", "```py\ndef clean_str(text):\n    fileters = '\"#$%&()*+-/;<=>@[\\\\]^_`{|}~\\t\\n\\r\\\"'\n    trans_map = str.maketrans(fileters, \" \" * len(fileters))\n    text = text.translate(trans_map)\n    re.sub(r'[^a-zA-Z,. ]+', '', text)\n    return text\n```", "```py\ndef get_data(fn):\n    with open(fn, 'r') as f:\n        extracted = json.load(f)\n    return extracted\n```", "```py\ndef handle_reddit(rawdata, startcount):\n    global writer\n    print(f'Reddit file has {len(rawdata)} items...')\n    cntr = startcount\n    with open(outfile, mode='a') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        for d in rawdata:\n            title = clean_str(d['title'])\n            body = clean_str(d['body'])\n            id = d['id']\n            score = d['score']\n            category = ''\n            other = ''\n            dict = {}\n            dict['row'] = cntr\n            dict['Joke'] = title + ' ' + body\n            dict['Title'] = title\n            dict['Body'] = body\n            dict['ID'] = id\n            dict['Category'] = category\n            dict['Score'] = score\n            dict['Other'] = other\n            dict['Source'] = 'Reddit'\n            writer.writerow(dict)\n            cntr += 1\n            if cntr % 10000 == 0:\n                print(cntr)\n    return cntr\n```", "```py\ndef handle_stupidstuff(rawdata, startcount):\n    global writer\n    print(f'StupidStuff file has {len(rawdata)} items...')\n    with open(outfile, mode='a') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        cntr = startcount\n        for d in rawdata:\n            body = clean_str(d['body'])\n            id = d['id']\n            score = d['rating']\n            category = d['category']\n            other = ''\n            dict = {}\n            dict['row'] = cntr\n            dict['Joke'] = body\n            dict['Title'] = ''\n            dict['Body'] = body\n            dict['ID'] = id\n            dict['Category'] = category\n            dict['Score'] = score\n            dict['Other'] = other\n            dict['Source'] = 'StupidStuff'\n            writer.writerow(dict)\n            cntr += 1\n            if cntr % 1000 == 0:\n                print(cntr)\n    return cntr\n\ndef handle_wocka(rawdata, startcount):\n    global writer\n    print(f'Wocka file has {len(rawdata)} items...')\n    with open(outfile, mode='a') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        cntr = startcount\n        for d in rawdata:\n            other = clean_str(d['title'])\n            title = ''\n            body = clean_str(d['body'])\n            id = d['id']\n            category = d['category']\n            score = ''\n            other = ''\n            dict = {}\n            dict['row'] = cntr\n            dict['Joke'] = body\n            dict['Title'] = title\n            dict['Body'] = body\n            dict['ID'] = id\n            dict['Category'] = category\n            dict['Score'] = score\n            dict['Other'] = other\n            dict['Source'] = 'Wocka'\n            writer.writerow(dict)\n            cntr += 1\n            if cntr % 1000 == 0:\n                print(cntr)\n    return cntr\n```", "```py\ndef prep_CVS():\n    global writer\n    with open(outfile, mode='a') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=headers)\n        writer.writeheader()\n```", "```py\ndef main():\n    pv = platform.python_version()\n    print(f\"Running under Python {pv}\")\n    path1 = os.getcwd()\n    print(path1)\n    prep_CVS()\n    print('Dealing with Reddit file')\n    extracted = get_data(datapath + \"/\" + redditfile)\n    count = handle_reddit(extracted, 0)\n    print('Dealing with StupidStuff file')\n    extracted = get_data(datapath + \"/\" + stupidfile)\n    count = handle_stupidstuff(extracted, count)\n    print('Dealing with Wocka file')\n    extracted = get_data(datapath + \"/\" + wockafile)\n    count = handle_wocka(extracted, count)\n    print(f'Finished processing! Total items processed: {count}')\n\nif __name__ == '__main__':\n    main()\n```", "```py\n$ python parse_jokes.py\n```", "```py\n$ pip install torchtext\n```", "```py\nimport torchtext as tt\nimport numpy as np\nimport torch\nfrom datetime import datetime\n\nVOCAB_SIZE = 5000\nMAX_SEQ_LEN = 30\nBATCH_SIZE = 32\n\nsrc = tt.data.Field(tokenize=tt.data.utils.get_tokenizer(\"basic_english\"),\n                    fix_length=MAX_SEQ_LEN,\n                    lower=True)\n\ndatafields = [('row', None),\n              ('Joke', src),\n              ('Title', None),\n              ('Body', None),\n              ('ID', None),\n              ('Score', None),\n              ('Category', None),\n              ('Other', None),\n              ('Source', None)]\n```", "```py\ndataset = tt.data.TabularDataset(path='jokes.csv', format='csv',\n                                 fields=[('id', None), \n                                         ('text', src)])\n\nsrc.build_vocab(dataset, max_size=VOCAB_SIZE)\n```", "```py\nsrc_itr = tt.data.BucketIterator(dataset=dataset,\n                                 batch_size=BATCH_SIZE,\n                                 sort_key=lambda x: len(x.text),\n                                 device=torch.device(\"cuda:0\"))\n```", "```py\nclass BatchLoader:\n    def __init__(self, dl, x_field):\n        self.dl, self.x_field = dl, x_field\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        for batch in self.dl:\n            x = getattr(batch, self.x_field)\n            yield x.t()\n\ntrain_loader = BatchLoader(src_itr, 'text')\n```", "```py\nvocab_max = 0\nfor i, batch in enumerate(train_loader):\n    _max = torch.max(batch)\n    if _max > vocab_max:\n        vocab_max = _max\n\nVOCAB_SIZE = vocab_max.item() + 1\n\ninv_vocab = {v: k for k, v in src.vocab.stoi.items()}\n```", "```py\nsentence = ['a', 'man', 'walks', 'into', 'a', 'bar']\nfor w in sentence:\n    v = src.vocab[w]\n    print(v)\n    print(inv_vocab[v])\n```", "```py\nfor i in inv_vocab:\n    print(f'Counter: {i} inv_vocab: {inv_vocab[i]}')\n```", "```py\n$ python mymain.py\n```", "```py\n$ python main.py\n```", "```py\n$ git clone https://github.com/santi-pdp/segan_pytorch.git\n$ pip install soundfile scipy librosa h5py numba matplotlib pyfftw tensorboardX\n```", "```py\n$ git submodule update --init --recursive\n```", "```py\n$ python train.py --save_path ckpt_segan+ --batch_size 300 --clean_trainset data/clean_trainset_wav --noisy_trainset data/noisy_trainset_wav --cache_dir data/cache\n```", "```py\n$ python clean.py --g_pretrained_ckpt ckpt_segan+/weights_EOE_G-Generator-16301.ckpt --cfg_file ckpt_segan+/train.opts --synthesis_path enhanced_results --test_files data/noisy_testset --soundfile\n```"]