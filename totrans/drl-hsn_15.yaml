- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continous Action Space
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter kicks off the advanced reinforcement learning (RL) part of the
    book by taking a look at a problem that has only been briefly mentioned so far:
    working with environments when our action space is not discrete. Continuous action
    space problems are an important subfield of RL, both theoretically and practically,
    because they have essential applications in robotics, control problems, and other
    fields in which we communicate with physical objects. In this chapter, you will
    become familiar with the challenges that arise in such cases and learn how to
    solve them.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This material might be applicable even in problems and environments we’ve already
    seen. For example, in the previous chapter, when we implemented a mouse clicking
    in the browser environment, the x and y coordinates for the click position could
    be seen as two continuous variables to be predicted as actions. This might look
    a bit artificial, but such representation has a lot of sense from the environment
    perspective: it is much more compact and naturally captures possible click dispersion.
    At the end, clicking at coordinate (x,y) isn’t much different from clicking at
    the (x + 1,y + 1) position for most of the tasks.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Cover the continuous action space, why it is important, how it differs from
    the already familiar discrete action space, and the way it is implemented in the
    Gym API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss the domain of continuous control using RL methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check three different algorithms on the problem of a four-legged robot
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why a continuous space?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the examples that we have seen so far in the book had a discrete action
    space, so you might have the wrong impression that discrete actions dominate the
    field. This is a very biased view, of course, and just reflects the selection
    of domains that we picked our test problems from. Besides Atari games and simple,
    classic RL problems, there are many tasks that require more than just making a
    selection from a small and discrete set of things to do.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: To give you an example, just imagine a simple robot with only one controllable
    joint that can be rotated in some range of degrees. Usually, to control a physical
    joint, you have to specify either the desired position or the force applied. In
    both cases, you need to make a decision about a continuous value. This value is
    fundamentally different from a discrete action space, as the set of values on
    which you can make a decision is potentially infinite. For instance, you could
    ask the joint to move to a 13.5^∘ angle or 13.512^∘ angle, and the results could
    be different. Of course, there are always some physical limitations of the system,
    as you can’t specify the action with infinite precision, but the size of the potential
    values could be very large.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In fact, when you need to communicate with a physical world, a continuous action
    space is much more likely than having a discrete set of actions. As an example,
    different kinds of robots control systems (such as a heating/cooling controller).
    The methods of RL could be applied to this domain, but there are some details
    that you need to take into consideration before using the advantage actor-critic
    (A2C) or deep Q-network (DQN) methods.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to deal with this family of problems. This
    will act as a good starting point for learning about this very interesting and
    important domain of RL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The action space
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fundamental and obvious difference with a continuous action space is its
    continuity. In contrast to a discrete action space, when the action is defined
    as a discrete, mutually exclusive set of options to choose from (for example {left,
    right}, which contains only two elements), the continuous action has a value from
    some range (for instance, [0…1], which includes infinite elements, like 0.5, ![√-
    23-](img/eq49.png), and ![ 3 πe5-](img/eq50.png)). On every time step, the agent
    needs to select the concrete value for the action and pass it to the environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In Gym, a continuous action space is represented as the gym.spaces.Box class,
    which was described, when we talked about the observation space. You may remember
    that Box includes a set of values with a shape and bounds. For example, every
    observation from the Atari emulator was represented as Box(low=0, high=255, shape=(210,
    160, 3)), which means 100,800 values organized as a 3D tensor, with values from
    the 0…255 range. For the action space, it’s unlikely that you’ll work with such
    large numbers of actions. For example, the four-legged robot that we will use
    as a testing environment has eight continuous actions, which correspond to eight
    motors, two in every leg. For this environment, the action space will be defined
    as Box(low=-1, high=1, shape= (8, )), which means eight values from the range
    −1…1 have to be selected at every timestamp to control the robot.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the action passed to the env.step() at every step won’t be an
    integer anymore; it will be a NumPy vector of some shape with individual action
    values. Of course, there could be more complicated cases when the action space
    is a combination of discrete and continuous actions, which may be represented
    with the gym.spaces.Tuple class.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Environments
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the environments that include continuous action spaces are related to
    the physical world, so physics simulations are normally used. There are lots of
    software packages that can simulate physical processes, from very simple open
    source tools to complex commercial packages that can simulate multiphysics processes
    (such as fluid, burning, and strength simulations).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In the case of robotics, one of the most popular packages is MuJoCo, which stands
    for Multi-Joint dynamics with Contact ([https://www.mujoco.org](https://www.mujoco.org)).
    This is a physics engine in which you can define the components of the system
    and their interaction and properties. Then the simulator is responsible for solving
    the system by taking into account your intervention and finding the parameters
    (usually the location, velocities, and accelerations) of the components. This
    makes it ideal as a playground for RL environments, as you can define fairly complicated
    systems (such as multipede robots, robotic arms, or humanoids) and then feed the
    observation into the RL agent, getting actions back.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: For a long time, MuJoCo was a commercial package and required an expensive license
    to be purchased. Trial licenses and education licenses existed, but they limited
    the audience for this software. But in 2022, DeepMind acquired MuJoCo and made
    the source code publicly available for everybody, which was a really great and
    generous move. Farama Gymnasium includes several MuJoCo environments ([https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/))
    out of the box; to get them working, you need to install the gymnasium[mujoco]
    package.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides MuJoCo, there are other physics simulators you can use for RL. One
    of the most popular is PyBullet ([https://pybullet.org/](https://pybullet.org/)),
    which was open source from the very beginning. In this chapter, we’ll use PyBullet
    in our experiments, and later in the book, we’ll take a look at MuJoCo as well.
    To install PyBullet, you need to execute pip install pybullet==3.2.6 in your Python
    environment. As PyBullet wasn’t updated to the Gymnasium API, we also need to
    install OpenAI Gym for compatibility:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use version 0.25.1, as later versions of OpenAI Gym are not compatible with
    the latest version of PyBullet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code (which is available in Chapter15/01_check_env.py) allows
    you to check that PyBullet works. It looks at the action space and renders an
    image of the environment that we will use as a guinea pig in this chapter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After you start the utility, it should open the graphical user interface (GUI)
    window with our four-legged robot, shown in the following figure, that we will
    train to move:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file195.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: The Minitaur environment in the PyBullet GUI (for better visualization,
    refer to https://packt.link/gbp/9781835882702 )'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'This environment provides you with 28 numbers as the observation and they correspond
    to different physical parameters of the robot: velocity, position, and acceleration.
    (You can check the source code of MinitaurBulletEnv-v0 for details.) The action
    space is eight numbers that define the parameters of the motors. There are two
    in every leg (one in every knee). The reward of this environment is the distance
    traveled by the robot minus the energy spent.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The A2C method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first method that we will apply to our walking robot problem is A2C, which
    we experimented with in Part 3 of the book. This choice of method is quite obvious,
    as A2C is very easy to adapt to the continuous action domain. As a quick refresher,
    A2C’s idea is to estimate the gradient of our policy as ∇J = ∇[𝜃] log π[𝜃](a|s)(R
    −V [𝜃](s)). The policy π[𝜃](s) is supposed to provide the probability distribution
    of actions given the observed state. The quantity V [𝜃](s) is called a critic,
    equal to the value of the state, and is trained using the mean squared error (MSE)
    loss between the critic’s return and the value estimated by the Bellman equation.
    To improve exploration, the entropy bonus L[H] = π[𝜃](s)log π[𝜃](s) is usually
    added to the loss.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the value head of the actor-critic will be unchanged for continuous
    actions. The only thing that is affected is the representation of the policy.
    In the discrete cases that we have seen, we had only one action with several mutually
    exclusive discrete values. For such a case, the obvious representation of the
    policy was the probability distribution over all actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In a continuous case, we usually have several actions, each of which can take
    a value from some range. With that in mind, the simplest policy representation
    will be just those values returned for every action. These values should not be
    confused with the value of the state, V (s), which indicates how many rewards
    we can get from the state. To illustrate the difference, let’s imagine a simple
    car steering case in which we can only turn the wheel. The action at every moment
    will be the wheel angle (action value), but the value of every state will be the
    potential discounted reward from the state (for example, the distance the car
    can travel), which is a totally different thing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our action representation options, if you remember what we covered
    in the Policy representation section in Chapter [11](ch015.xhtml#x1-18200011),
    the representation of an action as a concrete value has different disadvantages,
    mostly related to the exploration of the environment. A much better choice will
    be something stochastic, for example, the network returning parameters of the
    Gaussian distribution. For N actions, those parameters will be two vectors of
    size N. The first will be the mean values, μ, and the second vector will contain
    variances, σ². In that case, our policy will be represented as a random N-dimensional
    vector of uncorrelated, normally distributed random variables, and our network
    can make a selection about the mean and the variance of every variable.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: By definition, the probability density function of the Gaussian distribution
    is given by
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![--1-- √2πσ2-](img/eq53.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: We could directly use this formula to get the probabilities, but to improve
    numerical stability, it is worth doing some math and simplifying the expression
    for log π[𝜃](a|s).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The final result will be this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![(x−μ)2 2σ2](img/eq51.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: The entropy of the Gaussian distribution could be obtained using the differential
    entropy definition and will be ![√ ------ 2πeσ2](img/eq52.png). Now we have everything
    we need to implement the A2C method, so let’s do this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The complete source code is in 02_train_a2c.py, lib/model.py, and lib/common.py.
    You will be familiar with most of the code, so the following includes only the
    parts that differ. Let’s start with the model class defined in lib/model.py:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, our network has three heads, instead of the normal two for a
    discrete variant of A2C. The first two heads return the mean value and the variance
    of the actions, while the last is the critic head returning the value of the state.
    The mean value returned has an activation function of a hyperbolic tangent, which
    is the squashed output to the range of −1…1\. The variance is transformed with
    the softplus activation function, which is log(1 + e^x) and has the shape of a
    smoothed rectified linear unit (ReLU) function. This activation helps to make
    our variance positive. The value head, as usual, has no activation function applied.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass is obvious; we apply the common layer first, and then we calculate
    individual heads:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next step is to implement the PTAN Agent class, which is used to convert
    the observation into actions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the discrete case, we used the ptan.agent.DQNAgent and ptan.agent.PolicyAgent
    classes, but for our problem, we need to write our own, which is not complicated:
    you just need to write a class, derived from ptan.agent.BaseAgent, and override
    the __call__ method, which needs to convert observations into actions.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In this class, we get the mean and the variance from the network and sample
    the normal distribution using NumPy functions. To prevent the actions from going
    outside of the environment’s −1…1 bounds, we use np.clip(), which replaces all
    values less than -1 with -1, and values more than 1 with 1\. The agent_states
    argument is not used, but it needs to be returned with the chosen actions, as
    our BaseAgent supports keeping the state of the agent. We don’t need this functionality
    right now, but it will be handy in the next section on deep deterministic policy
    gradients, when we will need to implement a random exploration using the Ornstein-Uhlenbeck
    (OU) process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'With the model and the agent at hand, we can now go to the training process,
    defined in 02_train_a2c.py. It consists of the training loop and two functions.
    The first is used to perform periodical tests of our model on the separate testing
    environment. During the testing, we don’t need to do any exploration; we will
    just use the mean value returned by the model directly, without any random sampling.
    The testing function is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The second function defined in the training module implements the calculation
    of the logarithm of the taken actions’ probabilities given the policy. The function
    is a straightforward implementation of the formula we saw earlier:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![(x−μ2)2 2σ](img/eq54.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![(x−μ2)2 2σ](img/eq54.png)'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The only tiny difference is in using the torch.clamp() function to prevent the
    division on zero when the returned variance is too small.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的微小区别是在使用torch.clamp()函数，以防当返回的方差太小时发生除零错误。
- en: 'The training loop, as usual, creates the network and the agent, and then instantiates
    the two-step experience source and optimizer. The hyperparameters used are given
    as follows. They weren’t tweaked much, so there is plenty of room for optimization:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环照常创建网络和代理，然后实例化两步经验源和优化器。使用的超参数如下所示。它们没有经过太多调整，因此仍有很大的优化空间：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code used to perform the optimization step on the collected batch is very
    similar to the A2C training that we implemented in Chapter [12](ch016.xhtml#x1-20300012).
    The difference is only in using our calc_logprob() function and a different expression
    for the entropy bonus, which is shown next:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对收集到的批次执行优化步骤的代码与我们在第[12章](ch016.xhtml#x1-20300012)中实现的A2C训练非常相似。唯一的区别是使用了我们的calc_logprob()函数以及不同的熵奖励表达式，接下来会展示：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Every TEST_ITERS frames, the model is tested, and in the case of the best reward
    obtained, the model weights are saved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每隔TEST_ITERS帧，模型会进行一次测试，并在获得最佳奖励时保存模型权重。
- en: Results
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: In comparison to other methods that we will look at in this chapter, A2C shows
    the worst results, both in terms of the best reward and convergence speed. That’s
    likely because of the single environment used to gather experience, which is a
    weak point of the policy gradient (PG) methods. So, you may want to check the
    effect of several parallel environments on A2C.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在本章中将要探讨的其他方法相比，A2C的表现最差，无论是在最佳奖励还是收敛速度上。这很可能是因为经验收集仅使用了单一环境，而这是策略梯度（PG）方法的一个弱点。因此，你可能想要检查多个并行环境对A2C的影响。
- en: To start the training, we pass the -n argument with the run name, which will
    be used in TensorBoard and a new directory to save the models. The --dev option
    could be used to enable the GPU usage, but due to the small dimensionality of
    the input and the tiny network size, it gives only a marginal increase in speed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，我们传递-n参数以及运行名称，该名称将在TensorBoard中使用，并且会创建一个新的目录以保存模型。可以使用--dev选项启用GPU，但由于输入的维度较小且网络规模较小，它只会带来微小的速度提升。
- en: 'After 9M frames, which took 16 hours of optimization, the training process
    reached the best score of 0.35 during the testing, which is not very impressive.
    If we leave it running for a week or two, we might be able to achieve a better
    score. The reward and episode steps during the training and testing are shown
    in the following graphs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过9M帧（16小时的优化过程）后，训练过程在测试中达到了最佳得分0.35，虽然不算很出色。如果我们让它运行一两周，可能能获得更好的分数。训练和测试中的奖励以及回合步骤如下图所示：
- en: '![PIC](img/B22150_15_02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_02.png)'
- en: 'Figure 15.2: The reward (left) and steps (right) for training episodes'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：训练回合的奖励（左）和步骤（右）
- en: '![PIC](img/B22150_15_03.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_15_03.png)'
- en: 'Figure 15.3: The reward (left) and steps (right) for testing episodes'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：测试回合的奖励（左）和步骤（右）
- en: The episode steps charts (right plots on both figures) shows the average count
    of steps performed in the episode before the end. The time limit of the environment
    is 1,000 steps, so everything lower than 1,000 indicates that the episode was
    stopped due to environment checks. For most of the PyBullet environments, special
    checks for self-damage are implemented internally, which stop the simulation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该图中的回合步骤图（右侧的图表）显示了回合结束前每个回合执行的平均步数。环境的时间限制为1,000步，因此低于1,000的值表示回合因环境检查而停止。对于大多数PyBullet环境，内部实现了自我损害检查，这会停止仿真。
- en: Using models and recording videos
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型和录制视频
- en: As you have seen before, the physical simulator can render the state of the
    environment, which makes it possible to see how our trained model behaves. To
    do that for our A2C models, there is a utility, 03_play_a2c.py. Its logic is the
    same as in the test_net() function, so its code is not shown here.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前所看到的，物理模拟器可以呈现环境的状态，这使得我们可以观察训练后的模型行为。为了实现这一点，对于我们的A2C模型，提供了一个工具：03_play_a2c.py。其逻辑与test_net()函数相同，因此这里不展示代码。
- en: To start it, you need to pass the -m option with the model file and optional
    parameter -r with a directory name, which will be used to save the video using
    the RecordVideo wrapper we discussed in Chapter [2](ch006.xhtml#x1-380002).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the simulation, the utility shows the number of steps and accumulated
    reward. For example, the best A2C model from my training was able to get the reward
    0.312 and the video is just 2 seconds long (you can find it here: [https://youtu.be/s9BReDUtpQs](https://youtu.be/s9BReDUtpQs)).
    Figure [15.4](#x1-279005r4) shows the last frame of the video and it looks like
    our model had problems keeping the balance.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file200.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: Last frame of A2C model simulation'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Deep deterministic policy gradients
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next method that we will take a look at is called deep deterministic policy
    gradients (DDPG), which is an actor-critic method but has a very nice property
    of being off-policy. The following is a simplified interpretation of the strict
    proofs. If you are interested in understanding the core of this method deeply,
    you can always refer to the article by Silver et al. called Deterministic policy
    gradient algorithms [[Sil+14](#)], published in 2014, and the paper by Lillicrap
    et al. called Continuous control with deep reinforcement learning [[Lil15](#)],
    published in 2015.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to illustrate the method is through comparison with the already
    familiar A2C method. In this method, the actor estimates the stochastic policy,
    which returns the probability distribution over discrete actions or, as we have
    just covered in the previous section, the parameters of normal distribution. In
    both cases, our policy is stochastic, so, in other words, our action taken is
    sampled from this distribution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic policy gradients also belong to the A2C family, but the policy
    is deterministic, which means that it directly provides us with the action to
    take from the state. This makes it possible to apply the chain rule to the Q-value,
    and by maximizing the Q, the policy will be improved as well. To understand this,
    let’s look at how the actor and critic are connected in a continuous action domain.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the actor, as it is the simpler of the two. What we want from
    it is the action to take for every given state. In a continuous action domain,
    every action is a number, so the actor network will take the state as an input
    and return N values, one for every action. This mapping will be deterministic,
    as the same network always returns the same output if the input is the same. (We’re
    not going to use dropout or anything adding stochasticity to the inference; we’re
    just going to use an ordinary feed-forward network.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the critic. The role of the critic is to estimate the Q-value,
    which is a discounted reward of the action taken in some state. However, our action
    is a vector of numbers, so our critic network now accepts two inputs: the state
    and the action. The output from the critic will be the single number, which corresponds
    to the Q-value. This architecture is different from the DQN, when our action space
    was discrete and, for efficiency, we returned values for all actions in one pass.
    This mapping is also deterministic.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下评论员。评论员的作用是估算Q值，即在某个状态下采取的动作的折扣奖励。然而，我们的动作是一个数字向量，所以我们的评论员网络现在接受两个输入：状态和动作。评论员的输出将是一个数字，表示Q值。这个架构不同于DQN，当时我们的动作空间是离散的，并且为了提高效率，我们在一次传递中返回所有动作的值。这个映射也是确定性的。
- en: 'So, we have two functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有两个函数：
- en: The actor, let’s call it μ(s), which converts the state into the action
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员，我们称之为μ(s)，将状态转换为动作
- en: 'The critic, which, through the state and the action, gives us the Q-value:
    Q(s,a)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论员通过状态和动作，给我们Q值：Q(s,a)
- en: 'We can substitute the actor function into the critic and get the expression
    with only one input parameter of our state: Q(s,μ(s)). In the end, neural networks
    are just functions.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将演员函数代入评论员，并得到只有一个输入参数的表达式：Q(s,μ(s))。最终，神经网络只是函数。
- en: 'Now, the output of the critic gives us the approximation of the entity that
    we’re interested in maximizing in the first place: the discounted total reward.
    This value depends not only on the input state but also on the parameters of the
    𝜃[μ] actor and the 𝜃[Q] critic networks. At every step of our optimization, we
    want to change the actor’s weights to improve the total reward that we get. In
    mathematical terms, we want the gradient of our policy.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，评论员的输出给出了我们最初想要最大化的实体的近似值：折扣总奖励。这个值不仅依赖于输入状态，还依赖于𝜃[μ]演员和𝜃[Q]评论员网络的参数。在我们优化的每一步中，我们都希望改变演员的权重，以提高我们获得的总奖励。从数学角度看，我们希望得到我们策略的梯度。
- en: 'In his deterministic policy gradient theorem, Silver et al. proved that the
    stochastic policy gradient is equivalent to the deterministic policy gradient.
    In other words, to improve the policy, we just need to calculate the gradient
    of the Q(s,μ(s)) function. By applying the chain rule, we get the gradient: ∇[a]Q(s,a)∇[𝜃[μ]]μ(s).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的确定性策略梯度定理中，Silver等人证明了随机策略梯度等同于确定性策略梯度。换句话说，想要改进策略，我们只需要计算Q(s,μ(s))函数的梯度。通过应用链式法则，我们得到梯度：∇[a]Q(s,a)∇[𝜃[μ]]μ(s)。
- en: Note that, despite both the A2C and DDPG methods belonging to the A2C family,
    the way that the critic is used is different. In A2C, we use the critic as a baseline
    for a reward from the experienced trajectories, so the critic is an optional piece
    (without it, we will get the REINFORCE method) and is used to improve the stability.
    This happens as the policy in A2C is stochastic, which builds a barrier in our
    backpropagation capabilities (we have no way of differentiating the random sampling
    step).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管A2C和DDPG方法都属于A2C家族，但评论员的使用方式是不同的。在A2C中，我们使用评论员作为经验轨迹中奖励的基准，因此评论员是一个可选部分（没有它，我们将得到REINFORCE方法），并用于提高稳定性。这是因为A2C中的策略是随机的，这在我们的反向传播能力中建立了一个屏障（我们无法区分随机采样步骤）。
- en: In DDPG, the critic is used in a different way. As our policy is deterministic,
    we can now calculate the gradients from Q, which is obtained from the critic network,
    which uses actions produced by the actor (check Figure [15.5](#x1-282003r5)),
    so the whole system is differentiable and could be optimized end to end with stochastic
    gradient descent (SGD). To update the critic network, we can use the Bellman equation
    to find the approximation of Q(s,a) and minimize the MSE objective.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG中，评论员以不同的方式使用。由于我们的策略是确定性的，我们现在可以从Q中计算梯度，Q是从评论员网络获得的，评论员使用由演员产生的动作（见图[15.5](#x1-282003r5)），因此整个系统是可微的，并且可以通过随机梯度下降（SGD）进行端到端优化。为了更新评论员网络，我们可以使用贝尔曼方程来找到Q(s,a)的近似值并最小化均方误差（MSE）目标。
- en: 'All this may look a bit cryptic, but behind it stands a quite simple idea:
    the critic is updated as we did in A2C, and the actor is updated in a way to maximize
    the critic’s output. The beauty of this method is that it is off-policy, which
    means that we can now have a huge replay buffer and other tricks that we used
    in DQN training. Nice, right?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Exploration
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The price we have to pay for all this goodness is that our policy is now deterministic,
    so we have to explore the environment somehow. We can do this by adding noise
    to the actions returned by the actor before we pass them to the environment. There
    are several options here. The simplest method is just to add the random noise
    to the actions: μ(s) + 𝜖𝒩. We will use this in the next method that we will consider
    in this chapter.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'A more advanced (and sometimes giving better results) approach to the exploration
    is to use the previously mentioned Ornstein-Uhlenbeck process, which is very popular
    in the financial world and other domains dealing with stochastic processes. This
    process models the velocity of a massive Brownian particle under the influence
    of friction and is defined by this stochastic differential equation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: ∂x[t] = 𝜃(μ −x[t])∂t + σ∂W,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: where 𝜃, μ, and σ are parameters of the process and W[t] is the Wiener process.
    In a discrete-time case, the OU process could be written as
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: x[t+1] = x[t] + 𝜃(μ −x) + σ𝒩.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: This equation expresses the next value generated by the process via the previous
    value of the noise, adding normal noise, 𝒩. In our exploration, we will add the
    value of the OU process to the action returned by the actor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This example consists of three source files:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: lib/model.py contains the model and the PTAN agent
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lib/common.py has a function used to unpack the batch
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 04_train_ddpg.py has the startup code and the training loop
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, I will show only the significant pieces of the code. The model consists
    of two separate networks for the actor and critic, and it follows the architecture
    from the paper by Lillicrap et al. [[Lil15](#)] mentioned earlier. The actor is
    extremely simple and is a feed-forward network with two hidden layers. The input
    is an observation vector, whereas the output is a vector with N values, one for
    each action. The output actions are transformed with hyperbolic tangent nonlinearity
    to squeeze the values to the −1…1 range.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'The critic is a bit unusual, as it includes two separate paths for the observation
    and the actions, and those paths are concatenated together to be transformed into
    the critic output of one number. Figure [15.5](#x1-282003r5) shows the structure
    of both networks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file201.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: The DDPG actor and critic networks'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the actor includes a three-layer network that produces the action
    value:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Similarly, the following is the code used for the critic:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The forward() function of the critic first transforms the observations with
    its small network and then concatenates the output and given actions to transform
    them into one single value of Q. To use the actor network with the PTAN experience
    source, we need to define the agent class that has to transform the observations
    into actions. This class is the most convenient place to put our OU exploration
    process, but to do this properly, we should use the functionality of the PTAN
    agents that we haven’t used so far: optional statefulness.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is simple: our agent transforms the observations into actions. But
    what if it needs to remember something between the observations? All our examples
    have been stateless so far, but sometimes this is not enough. The issue with OU
    is that we have to track the OU values between the observations.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Another very useful use case for stateful agents is a partially observable Markov
    decision process (POMDP), which was briefly mentioned in Chapter [6](#) and Chapter [14](ch018.xhtml#x1-24700014).
    The POMDP is a Markov decision process when the state observed by the agent doesn’t
    comply with the Markov property and doesn’t include the full information to distinguish
    one state from another. In that case, our agent needs to track the state along
    the trajectory to be able to take the action.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the code for the agent that implements the OU for exploration is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The constructor accepts a lot of parameters, most of which are the default hyperparameters
    of the OU process taken from the paper Continuous Control with Deep Reinforcement
    Learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The initial_state() method is derived from the BaseAgent class and has to return
    the initial state of the agent when a new episode is started. As our initial state
    has to have the same dimension as the actions (we want to have individual exploration
    trajectories for every action of the environment), we postpone the initialization
    by returning None as the initial state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'In the __call__ method, we’ll take this into account:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method is the core of the agent and the purpose of it is to convert the
    observed state and internal agent state into the action. As the first step, we
    convert the observations into the appropriate form and ask the actor network to
    convert them into deterministic actions. The rest of the method is for adding
    the exploration noise by applying the OU process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'In this loop, we iterate over the batch of observations and the list of the
    agent states from the previous call, and we update the OU process value, which
    is a straightforward implementation of the already shown formula:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To finalize the loop, we add the noise from the OU process to our actions and
    save the noise value for the next step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we clip the actions to enforce them to fall into the −1…1 range; otherwise,
    PyBullet will throw an exception:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final piece of the DDPG implementation is the training loop in the 04_train_ddpg.py
    file. To improve the stability, we use the replay buffer with 100,000 transitions
    and target networks for both the actor and the critic (we discussed both in Chapter [6](#)):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We also use two different optimizers to simplify the way that we handle gradients
    for the actor and critic training steps. The most interesting code is inside the
    training loop. On every iteration, we store the experience into the replay buffer
    and sample the training batch:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, two separate training steps are performed. To train the critic, we need
    to calculate the target Q-value using the one-step Bellman equation, with the
    target critic network as the approximation of the next state:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When we have got the reference, we can calculate the MSE loss and ask the critic’s
    optimizer to tweak the critic’s weights. The whole process is similar to the training
    for the DQN, so nothing is really new here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'On the actor’s training step, we need to update the actor’s weights in a direction
    that will increase the critic’s output. As both the actor and critic are represented
    as differentiable functions, what we need to do is just pass the actor’s output
    to the critic and then minimize the negated value returned by the critic:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This negated output of the critic could be used as a loss to backpropagate
    it to the critic network and, finally, the actor. We don’t want to touch the critic’s
    weights, so it’s important to ask only the actor’s optimizer to do the optimization
    step. The weights of the critic will still keep the gradients from this call,
    but they will be discarded on the next optimization step:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As the last step of the training loop, we perform the update of the target
    networks in an unusual way:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Previously, we synced the weights from the optimized network into the target
    periodically. In continuous action problems, such syncing works worse than so-called
    “soft sync.” The soft sync is carried out on every step, but only a small ratio
    of the optimized network’s weights are added to the target network. This makes
    a smooth and slow transition from the old weight to the new ones.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Results and video
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code can be started in the same way as the A2C example: you need to pass
    the run name and optional --dev flag. My experiments have shown ≈ 30% speed increase
    from a GPU, so if you’re in a hurry, using CUDA may be a good idea, but the increase
    is not that dramatic, as we have seen in the case of Atari games.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: After 5M observations, which took about 20 hours, the DDPG algorithm was able
    to reach the mean reward of 4.5 on 10 test episodes, which is an improvement over
    the A2C result. The training dynamics are shown in Figure [15.6](#x1-283002r6)
    and Figure [15.7](#x1-283003r7).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_06.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: The reward (left) and steps (right) for training episodes'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_07.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Actor loss (left) and critic loss (right) during the training'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The “Episode steps” plot shows the mean length of the episodes that we used
    for training. The critic loss is an MSE loss and should be low, but the actor
    loss, as you will remember, is the negated critic’s output, so the smaller it
    is, the better the reward that the actor can (potentially) achieve.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [15.8](#x1-283004r8), the shown values were obtained during the testing
    (which are average values obtained for 10 episodes).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: The reward (left) and steps (right) for testing episodes'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the saved model and record the video the same way we did for the A2C
    model, you can use the utility 05_play_ddpg.py. It uses the same command-line
    options, but is supposed to load DDPG models. In figure Figure [15.9](#x1-283006r9),
    the last frame of my video is shown:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file208.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Last frame of DDPG model simulation'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The score during the testing was 3.033 and the video is avaliable at [https://youtu.be/vVnd0Nu1d9s](https://youtu.be/vVnd0Nu1d9s).
    Now the video is 11 seconds long and the model fails after falling forward.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Distributional policy gradients
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the last method of this chapter, we will take a look at the paper by Barth-Maron
    et al., called Distributed distributional deterministic policy gradients [[Bar+18](#)],
    published in 2018.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The full name of the method is Distributed Distributional Deep Deterministic
    Policy Gradients or D4PG for short. The authors proposed several improvements
    to the DDPG method to improve stability, convergence, and sample efficiency.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: First, they adapted the distributional representation of the Q-value proposed
    in the paper by Bellemare et al. called A distributional perspective on reinforcement
    learning, published in 2017 [[BDM17](#)]. We discussed this approach in Chapter [8](ch012.xhtml#x1-1240008),
    when we talked about DQN improvements, so refer to it or to the original Bellemare
    paper for details. The core idea is to replace a single Q-value from the critic
    with a probability distribution. The Bellman equation is replaced with the Bellman
    operator, which transforms this distributional representation in a similar way.
    The second improvement was the usage of the n-step Bellman equation, unrolled
    to speed up the convergence. We also discussed this in detail in Chapter [8](ch012.xhtml#x1-1240008).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Another difference versus the original DDPG method was the usage of the prioritized
    replay buffer instead of the uniformly sampled buffer. So, strictly speaking,
    the authors took relevant improvements from the paper by Hassel et al., called
    Rainbow: Combining Improvements in Deep Reinforcement Learning, which was published
    in 2017 [[Hes+18](#)], and adapted them to the DDPG method. The result was impressive:
    this combination showed state-of-the-art results on the set of continuous control
    problems. Let’s try to reimplement the method and check it ourselves.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most notable change between D4PG and DDPG is the critic’s output. Instead
    of returning the single Q-value for the given state and the action, it now returns
    N_ATOMS values, corresponding to the probabilities of values from the predefined
    range. In my code, I used N_ATOMS=51 and the distribution range of Vmin=-10 and
    Vmax=10, so the critic returned 51 numbers, representing the probabilities of
    the discounted reward falling into bins with bounds in [−10,−9.6,−9.2,…,9.6,10].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between D4PG and DDPG is the exploration. DDPG used the OU
    process for exploration, but according to the D4PG authors, they tried both OU
    and adding simple random noise to the actions, and the result was the same. So,
    they used a simpler approach for exploration in the paper.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The last significant difference in the code is related to the training, as D4PG
    uses cross-entropy loss to calculate the difference between two probability distributions
    (returned by the critic and obtained as a result of the Bellman operator). To
    make both distributions aligned to the same supporting atoms, distribution projection
    is used in the same way as in the original paper by Bellemare et al.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The complete source code is in 06_train_d4pg.py, lib/model.py, and lib/common.py.
    As before, we start with the model class. The actor class has exactly the same
    architecture as in DDPG, so during the training class, DDPGActor is used. The
    critic has the same size and count of hidden layers; however, the output is not
    a single number, but N_ATOMS:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We also create a helper PyTorch buffer with reward supports, which will be
    used to get from the probability distribution to the single mean Q-value:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, softmax() application is not part of the network’s forward()
    method, as we’re going to use the more stable log_softmax() function during the
    training. Due to this, softmax() needs to be applied when we want to get actual
    probabilities.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent class is much simpler for D4PG and has no state to track:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For every state to be converted to actions, the agent applies the actor network
    and adds Gaussian noise to the actions, scaled by the epsilon value. In the training
    code, we have the following hyperparameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: I used a smaller replay buffer of 100,000, and it worked fine. (In the D4PG
    paper, the authors used 1M transitions in the buffer.) The buffer is prepopulated
    with 10,000 samples from the environment, and then the training starts.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'For every training loop, we perform the same two steps as before: we train
    the critic and the actor. The difference is in the way that the loss for the critic
    is calculated:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As the first step in the critic’s training, we ask it to return the probability
    distribution for the states and actions taken. This probability distribution will
    be used as an input in the cross-entropy loss calculation. To get the target probability
    distribution, we need to calculate the distribution from the last states in the
    batch and then perform the Bellman projection of the distribution:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This projection function is a bit complicated and is exactly the same as the
    implementation explained in detail in Chapter [8](ch012.xhtml#x1-1240008). As
    a quick reminder, it calculates the transformation of the last_states probability
    distribution, which is shifted according to the immediate reward and scaled to
    respect the discount factor. The result is the target probability distribution
    that we want our network to return. As there is no general cross-entropy loss
    function in PyTorch, we calculate it manually by multiplying the logarithm of
    the input probability by the target probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The actor’s training is much simpler, and the only difference from the DDPG
    method is the use of the distr_to_q() method of the model to convert from the
    probability distribution to the single mean Q-value using support atoms:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Results
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The D4PG method showed the best results in both convergence speed and the reward
    obtained. Following 20 hours of training, after about 3.5M observations, it was
    able to reach the mean test reward of 17.912\. Given that “gym environment threshold”
    is 15.0 (which is a score when the environment considered it solved), that is
    a great result. And this result could be improved, as the count of steps is less
    than 1,000 (which is a time limit for the environment). This means that our model
    is being terminated prematurely because of internal environment checks. In Figure [15.10](#x1-287002r10)
    and Figure [15.11](#x1-287003r11), we have the train and test metrics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: The reward (left) and steps (right) for training episodes'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_11.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: The reward (left) and steps (right) for testing episodes'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: To compare the implemented methods, Figure [15.12](#x1-287004r12) contains test
    episode metrics from all three methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_15_12.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: The reward (left) and steps (right) for test episodes'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the model “in action,” you can use the same tool, 05_play_ddpg.py
    (as actor has the same network structure as in DDPG). Now the video produced by
    the best model lasts 33 seconds and the final score was 17.827\. You can watch
    it here: [https://youtu.be/XZdVrGPaI0M](https://youtu.be/XZdVrGPaI0M).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Things to try
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of things you can do to improve your understanding of the topic:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: In the D4PG code, I used a simple replay buffer, which was enough to get good
    improvement over DDPG. You can try to switch the example to the prioritized replay
    buffer in the same way as we did in Chapter [8](ch012.xhtml#x1-1240008).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are lots of interesting and challenging environments around. For example,
    you can start with other PyBullet environments, but there is also the DeepMind
    Control Suite (Tassa et al., DeepMind Control Suite, arXiv abs/1801.00690 (2018)),
    MuJoCo-based environments in Gym, and many others.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can play with the very challenging Learning to Run competition from NIPS-2017
    (which also took place in 2018 and 2019 with more challenging problems), where
    you are given a simulator of the human body and your agent needs to figure out
    how to move it around.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we quickly skimmed through the very interesting domain of
    continuous control using RL methods, and we checked three different algorithms
    on one problem of a four-legged robot. In our training, we used an emulator, but
    there are real models of this robot made by the Ghost Robotics company. (You can
    check out the cool video on YouTube: [https://youtu.be/bnKOeMoibLg](https://youtu.be/bnKOeMoibLg).)
    We applied three training methods to this environment: A2C, DDPG, and D4PG (which
    showed the best results).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will continue exploring the continuous action domain
    and check a different set of improvements: trust region extension.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
