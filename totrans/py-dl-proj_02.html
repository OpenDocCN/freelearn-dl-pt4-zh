<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training NN for Prediction Using Regression</h1>
                </header>
            
            <article>
                
<p>Welcome to our first proper project in Python deep learning! What we'll be doing today is building a classifier to solve the problem of identifying specific handwriting samples from a dataset of images. We've been asked (in this hypothetical use case) to do this by a restaurant chain that has the need to accurately classify handwritten numbers into digits. What they have their customers do is write their phone numbers in a simple iPad application. At the time when they can be seated, the guest will get a text prompting them to come and see the restaurant's host. We need to accurately classify the handwritten numbers, so that the output from the app will be accurately predicted labels for the digits of a phone number. This can then be sent to their (hypothetical) auto dialer service for text messages, and the notice gets to the right hungry customer!</p>
<div class="packt_tip"><strong>Define success</strong>: A good practice is to define the criteria for success at the beginning of a project. What metric should we use for this project? Let's use a global accuracy test as a percentage to measure our performance in this project.</div>
<p>The data science approach to the problem of classification can be configured in a number of ways. In fact, later in this book, we'll look at how to increase accuracy in image classification with convolutional neural networks. </p>
<div class="packt_tip packt_infobox"><strong>Transfer learning</strong>: This means pretraining a deep learning model on a different (but quite similar) dataset to speed up the rate of learning and accuracy on another (often smaller) dataset. In this project and our hypothetical use case, the pretraining of our deep learning <strong>multi-layer perceptron</strong> (<strong>MLP</strong>) on the MNIST dataset would enable the deployment of a production system of handwriting classification, without having a huge period of time where we were collecting data samples in a live but non-functional system. Python deep learning projects are cool!</div>
<p>Let's start with the baseline deep neural network model architecture. We will get our intuition and skills firmly established, and this will prepare us for learning more complex architectures to solve a wider variety of problems as we go progress through the projects in this book.</p>
<p>What we'll learn in this chapter includes the following:</p>
<ul>
<li>What is an MLP?</li>
<li>Exploring a common open source handwriting dataset—the MNIST dataset</li>
<li>Building our intuition and preparations for model architecture</li>
<li>Coding the model and defining hyperparameters</li>
<li>Building the training loop</li>
<li>Testing the model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a regression model for prediction using an MLP deep neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">In any real job working in an AI team, one of the primary goals will be to build regression models that can make predictions in non-linear datasets. Because of the complexity of the real world and the data that you'll be working with, simple linear regression models won't provide the predictive power you're seeking. That is why, in this chapter, we will discuss how to build world-class prediction models using MLP. More information can be found at <a href="http://www.deeplearningbook.org/contents/mlp.html" target="_blank">http://www.deeplearningbook.org/contents/mlp.html</a>, and an example of the MLP architecture is shown here:</p>
<div class="CDPAlignCenter CDPAlign"><span><img src="assets/c57522c1-d374-41bd-a8f7-edcd12debda7.png" style="width:20.25em;height:20.25em;"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An MLP with two hidden layers</div>
<p>We will implement a neural network with a simple architecture of only two layers, using TensorFlow, that will perform regression on the MNIST dataset (<a href="http://yann.lecun.com/exdb/mnist/" target="_blank">http://yann.lecun.com/exdb/mnist/</a>) that we will provide. We can (and will) go deeper in architecture in later projects! <span>We assume that you are already familiar with backpropagation (if not, please read article on backpropagation by </span>Michal Nielsen at <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank">http://neuralnetworksanddeeplearning.com/chap2.html</a><span>). We'll not spend much time on how TensorFlow works, but you can refer to </span>the official tutorial, available at <a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html">https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html</a>,<span> if you are interested in looking under the hood of that technology.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the MNIST dataset</h1>
                </header>
            
            <article>
                
<p>Before we jump into building our awesome neural network, let's first have a look at the famous MNIST dataset. So let's visualize the MNIST dataset in this section.</p>
<div class="packt_infobox"><strong>Words of wisdom</strong>: You must know your data and how it has been preprocessed, in order to know why the models you build perform the way they do. This section reviews the significant work that has been done in preparation on the dataset, to make our current job of building the MLP easier. Always remember: data science begins with DATA!</div>
<p>Let's start therefore by downloading the data, using the following commands: </p>
<pre>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)</pre>
<p>If we examine the <kbd>mnist</kbd> variable content, we can see that it is structured in a specific format, with three major components—<strong>TRAIN</strong>, <strong>TEST</strong>, and <strong>VALIDATION</strong>. Each set has handwritten images and their respective labels. The images are stored in a flattened way as a single vector:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1376 image-border" src="assets/8aa4383e-28f6-4c9d-b7bd-064a4d75da37.png" style="width:32.17em;height:17.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The format of the MNIST dataset</span></div>
<p>Let's extract one image from the dataset and plot it. Since the stored shape of a single image matrix is <kbd>[1,784]</kbd>, we need to reshape these vectors into <kbd>[28,28]</kbd> to visualize the original image:</p>
<pre><span>sample_image = mnist.train.images[0].reshape([28,28])</span></pre>
<p>Once we have the image matrix, we will use <kbd>matplotlib</kbd> to plot it, as follows:</p>
<pre>import matplotlib.pyplot as plt<br/>plt.gray()<br/>plt.imshow(sample_image)</pre>
<p>The output will be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e396298d-51ec-46f8-969d-f86bf1a17a99.png" style="width:23.83em;height:23.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A sample of the MNIST dataset </div>
<p>In the same vein as this image, there are a total of 55,000 similar images of handwritten digits [0-9]. The labels in the MNIST dataset are the true value of the digits present in the image. Our objective, then, is to train a model with this set of images and labels, so that it can predict the labels of any image <span>provided</span> from the MNIST dataset.</p>
<div class="packt_tip"><strong>Be a deep learning explorer</strong>: If you are interested in playing around with the dataset, you can try the Colab Notebook, available at <a href="https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing" target="_blank">https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intuition and preparation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's build our intuition around this project. What we need to do is build a deep learning technology that accurately assigns class labels to an input image. We're using a deep neural network, known as an MLP, to do this. The core of this technology is the mathematics of regression. The specific calculus proofs are outside the scope of this book, but in this section, we provide a foundational basis for your understanding. We also outline the structure of the project, so that it's easy to understand the primary steps needed to create our desired results. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining regression</h1>
                </header>
            
            <article>
                
<p>Our first task is to define the model that will perform regression on the provided MNIST dataset. So, we will create a TensorFlow model with two hidden layers as part of a fully connected neural network. You may also hear it referred to as MLP.</p>
<p>The model will perform the operation that will fit the following equation, where<span> </span><em>y</em><span> </span>is the label,<span> </span><em>x</em><span> </span>is the image,<span> </span><em>W</em><span> </span>is the weight that the model will learn, and<span> </span><em>b</em><span> </span>is the bias, which will also be learned by the model, following is the regression equation for the model:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a9fcc2d-2c3f-4db7-8253-0e83d069b765.png" style="width:14.50em;height:1.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The regression equation for the model</div>
<div class="packt_infobox"><strong>Supervised learning</strong>: When you have data and accurate labels for the training set (that is, you know the answer), you are in a supervised deep learning paradigm. Model training is a mathematical process by which the features of the data are learned and associated with the proper labels, so that when a new data point (test data) is presented, the accurate output class label can be produced. In other words, when you present a new data point and do not have the label (that is, you don't know the answer), your model can produce it for you with a highly reliable class prediction. </div>
<p class="mce-root">Each iteration will try to generalize the values of weight and bias and reduce the error rate. Also, keep in mind that we need to ensure that the model is not overfitting, which may lead to wrong predictions for the unseen dataset. We'll show you how to code this and visualize the progress to aid in your intuition of model performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the project structure</h1>
                </header>
            
            <article>
                
<p>Let's structure our project as shown in the following pattern:</p>
<ul>
<li><kbd>hy_param.py</kbd><span>: A</span><span>ll the hyperparameters and other configurations are defined here</span></li>
<li><kbd>model.py</kbd>:<span> The d</span>efinition and architecture of the model<span> </span>are<span> </span>defined here</li>
<li><span class="s1"><kbd>train.py</kbd>: The code to train the model is written here<br/></span></li>
<li><kbd>inference.py</kbd>: The code to execute the trained model and make predictions is defined here</li>
<li><kbd>/runs</kbd>:<span> </span>This folder will store all of the checkpoints that get created during the training process</li>
</ul>
<p class="mce-root"/>
<p>You can clone the code from the repository—the code for this can be found in the <kbd>Chapter02</kbd> folder, available at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Let's code the implementation!</h1>
                </header>
            
            <article>
                
<p class="mce-root">To code the implementation, we'll start by defining the hyperparameters, then we will define the model, followed by building and executing the training loop. We conclude by checking to see if our model is overfitting and build an <span>inference code that loads the latest checkpoints and then makes predictions on the basis of learned parameters.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining hyperparameters</h1>
                </header>
            
            <article>
                
<p>We will define all of the required hyperparameters in the <kbd>hy_param.py</kbd> file and then import it as a module in our other codes. This makes it easy in deployment, and is good practice to make your code as modular as possible. Let's look into the hyperparameter configurations that we have in our<span> </span><kbd>hy_param.py</kbd><span> </span>file:</p>
<pre>#!/usr/bin/env python2<br/><br/># Hyperparameters and all other kind of params<br/><br/># Parameters<br/>learning_rate = 0.01<br/>num_steps = 100<br/>batch_size = 128<br/>display_step = 1<br/><br/><br/># Network Parameters<br/>n_hidden_1 = 300 # 1st layer number of neurons<br/>n_hidden_2 = 300 # 2nd layer number of neurons<br/>num_input = 784 # MNIST data input (img shape: 28*28)<br/>num_classes = 10 # MNIST total classes (0-9 digits)<br/><br/>#Training Parameters<br/>checkpoint_every = 100<br/>checkpoint_dir = './runs/'</pre>
<p>We will be using these values throughout our code, and they're totally configurable.</p>
<p class="mce-root"/>
<div class="packt_tip">As a Python deep learning projects exploration opportunity, we invite you, our project teammate and reader, to try different values of<span> </span>learning rate and numbers of hidden layers to experiment and build better models!</div>
<p>Since the flat vectors of images shown previously are of a size of [1 x 786], the<span> </span><kbd>num_input=784</kbd><span> </span>is fixed in this case. In addition, the class count in the MNIST dataset is <kbd>10</kbd>. We have digits from 0-9, so obviously we have<span> </span><kbd>num_classes=10</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model definition</h1>
                </header>
            
            <article>
                
<p>First, we will load the Python modules; in this case, the TensorFlow package and the hyperparameters that we defined previously:</p>
<pre>import tensorflow as tf<br/>import hy_param</pre>
<p>Then, we define the placeholders that we will be using to input data into the model. <kbd>tf.placeholder</kbd><span> allows us to feed input data to the computational graph. We can define constraints with the shape of the placeholder to only accept a tensor of a certain shape. Note that it is common to provide <kbd>None</kbd> </span><span>for the first dimension, which allows us to the size of the batch at runtime.</span></p>
<div class="packt_tip"><strong>Master your craft</strong>: Batch size can often have a big impact on the performance of deep learning models. Explore different batch sizes in this project. What changes as a result? What's your intuition? Batch size is another tool in your data science toolkit!</div>
<p><span>We have also assigned names to the placeholders, so that we can use them later on while building our inference code:</span></p>
<pre class="mce-root">X = tf.placeholder("float", [None, hy_param.num_input],name="input_x")<br/>Y = tf.placeholder("float", [None, hy_param.num_classes],name="input_y")</pre>
<p><span>Now we will define variables that will hold values for weights and bias.</span> <kbd>tf.Variable</kbd><span> allows us to store and update tensors in our graph. To initialize our variables with random values from a normal distribution, we will use <kbd>tf.random_normal()</kbd> (more details can be found at <a href="https://www.tensorflow.org/api_docs/python/tf/random_normal">https://www.tensorflow.org/api_docs/python/tf/random_normal</a>). The important thing to notice here is the mapping variable size between layers:</span></p>
<pre class="mce-root">weights = {<br/> 'h1': tf.Variable(tf.random_normal([hy_param.num_input, hy_param.n_hidden_1])),<br/> 'h2': tf.Variable(tf.random_normal([hy_param.n_hidden_1, hy_param.n_hidden_2])),<br/> 'out': tf.Variable(tf.random_normal([hy_param.n_hidden_2, hy_param.num_classes]))<br/> }<br/> biases = {<br/> 'b1': tf.Variable(tf.random_normal([hy_param.n_hidden_1])),<br/> 'b2': tf.Variable(tf.random_normal([hy_param.n_hidden_2])),<br/> 'out': tf.Variable(tf.random_normal([hy_param.num_classes]))<br/> }</pre>
<p><span class="c1">Now, let's set up the operation that we defined in the equation earlier in this chapter. </span><span class="c1">This is the logistic regression operation:</span></p>
<pre>layer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])<br/>layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])<br/>logits = tf.matmul(layer_2, weights['out']) + biases['out']</pre>
<p>The logistic values are converted into the probabilistic values using <kbd>tf.nn.softmax()</kbd>. <span>The softmax activation squashes the output values of each unit to a value between zero and one:</span></p>
<pre>prediction = tf.nn.softmax(logits, name='prediction')</pre>
<p>Next, let's use <span><kbd>tf.nn.softmax_cross_entropy_with_logits</kbd> to define our cost function. We will optimize our performance</span> using the Adam Optimizer. <span>Finally, we can use the built-in <kbd>minimize()</kbd> function </span><span>to calculate the <strong>stochastic gradient descent</strong></span> (<span><strong>SGD</strong>) update rule for each parameter in our network:</span></p>
<pre>loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))<br/>optimizer = tf.train.AdamOptimizer(learning_rate=hy_param.learning_rate)<br/>train_op = optimizer.minimize(loss_op)</pre>
<p><span>Next, we make our prediction. These functions are needed to calculate and capture the accuracy values in a batch:</span></p>
<pre>correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32) ,name='accuracy')</pre>
<p>The complete code is as follows:</p>
<pre>#!/usr/bin/env python2<br/># -*- coding: utf-8 -*-<br/><br/>import tensorflow as tf<br/>import hy_param<br/><br/><br/>## Defining Placeholders which will be used as inputs for the model<br/>X = tf.placeholder("float", [None, hy_param.num_input],name="input_x")<br/>Y = tf.placeholder("float", [None, hy_param.num_classes],name="input_y")<br/><br/><br/># Defining variables for weights &amp; bias<br/>weights = {<br/>    'h1': tf.Variable(tf.random_normal([hy_param.num_input, hy_param.n_hidden_1])),<br/>    'h2': tf.Variable(tf.random_normal([hy_param.n_hidden_1, hy_param.n_hidden_2])),<br/>    'out': tf.Variable(tf.random_normal([hy_param.n_hidden_2, hy_param.num_classes]))<br/>}<br/>biases = {<br/>    'b1': tf.Variable(tf.random_normal([hy_param.n_hidden_1])),<br/>    'b2': tf.Variable(tf.random_normal([hy_param.n_hidden_2])),<br/>    'out': tf.Variable(tf.random_normal([hy_param.num_classes]))<br/>}<br/><br/><br/># Hidden fully connected layer 1 with 300 neurons<br/>layer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])<br/># Hidden fully connected layer 2 with 300 neurons<br/>layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])<br/># Output fully connected layer with a neuron for each class<br/>logits = tf.matmul(layer_2, weights['out']) + biases['out']<br/><br/># Performing softmax operation<br/>prediction = tf.nn.softmax(logits, name='prediction')<br/><br/># Define loss and optimizer<br/>loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<br/>    logits=logits, labels=Y))<br/>optimizer = tf.train.AdamOptimizer(learning_rate=hy_param.learning_rate)<br/>train_op = optimizer.minimize(loss_op)<br/><br/># Evaluate model<br/>correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32) ,name='accuracy')</pre>
<p>Hurray! The heavy lifting part of the code is done. We save the model code in the <kbd>model.py</kbd> file. So, up until now, we've defined the simple two-hidden-layer model architecture, with 300 neurons in each layer, which will try to learn the best weight distribution using the Adam Optimizer and predict the probability of ten classes. These layers are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c0616a53-71bd-456e-8dd2-c56116074037.png" style="width:27.58em;height:23.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An illustration of the model that we created</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the training loop</h1>
                </header>
            
            <article>
                
<p><span>The next step is to utilize the model for training, and record the learned model parameters, which we will accomplish in <kbd>train.py</kbd>.</span></p>
<p>Let's start by importing the dependencies:</p>
<pre>import tensorflow as tf<br/>import hy_param<br/><br/><strong># MLP Model which we defined in previous step</strong><br/>import model</pre>
<p>Then, we define the variables that we require to be fed into our MLP:</p>
<pre><strong># This will feed the raw images</strong><br/>X = model.X<br/><strong># This will feed the labels associated with the image</strong><br/>Y = model.Y</pre>
<p>Let's create the folder to which we will save our checkpoints. Checkpoints are basically the intermediate steps that capture the values of <kbd>W</kbd> and <kbd>b</kbd> in the process of learning. Then, we will use the <kbd>tf.train.Saver()</kbd> function (more details on this function can be found at <a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver">https://www.tensorflow.org/api_docs/python/tf/train/Saver</a>) to save and restore <kbd>checkpoints</kbd>:</p>
<pre>checkpoint_dir = os.path.abspath(os.path.join(hy_param.checkpoint_dir, "checkpoints"))<br/>checkpoint_prefix = os.path.join(checkpoint_dir, "model")<br/>if not os.path.exists(checkpoint_dir):<br/>    os.makedirs(checkpoint_dir)<br/><br/># We only keep the last 2 checkpoints to manage storage<br/>saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)</pre>
<p>In order to begin training, we need to create a new session in TensorFlow. In this session, we'll initialize the graph variables and feed the model operations the valid data:</p>
<pre># Initialize the variables<br/>init = tf.global_variables_initializer()<br/><br/># Start training<br/>with tf.Session() as sess:<br/><br/>    # Run the initializer<br/>    sess.run(init)<br/><br/>    for step in range(1, hy_param.num_steps+1):<br/>        # Extracting <br/>        batch_x, batch_y = mnist.train.next_batch(hy_param.batch_size)<br/>        # Run optimization op (backprop)<br/>        sess.run(model.train_op, feed_dict={X: batch_x, Y: batch_y})<br/>        if step % hy_param.display_step == 0 or step == 1:<br/>            # Calculate batch loss and accuracy<br/>            loss, acc = sess.run([model.loss_op, model.accuracy], feed_dict={X: batch_x,<br/>                                                                 Y: batch_y})<br/>            print("Step " + str(step) + ", Minibatch Loss= " + \<br/>                  "{:.4f}".format(loss) + ", Training Accuracy= " + \<br/>                  "{:.3f}".format(acc))<br/>        if step % hy_param.checkpoint_every == 0:<br/>            path = saver.save(<br/>                        sess, checkpoint_prefix, global_step=step)<br/>            print("Saved model checkpoint to {}\n".format(path))<br/><br/>    print("Optimization Finished!")</pre>
<p>We will extract batches of 128 training image-label pairs from the MNIST dataset and feed them into the model. After subsequent steps or epochs, we will store the checkpoints using the <kbd>saver</kbd> operation:</p>
<pre>#!/usr/bin/env python2<br/># -*- coding: utf-8 -*-<br/><br/><br/>from __future__ import print_function<br/><br/># Import MNIST data<br/>import os<br/>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)<br/><br/>import tensorflow as tf<br/>import model<br/>import hy_param<br/><br/><br/>## tf Graph input<br/>X = model.X<br/>Y = model.Y<br/><br/><br/><br/>checkpoint_dir = os.path.abspath(os.path.join(hy_param.checkpoint_dir, "checkpoints"))<br/>checkpoint_prefix = os.path.join(checkpoint_dir, "model")<br/>if not os.path.exists(checkpoint_dir):<br/>    os.makedirs(checkpoint_dir)<br/>saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)<br/>        <br/>#loss = tf.Variable(0.0)<br/># Initialize the variables<br/>init = tf.global_variables_initializer()<br/>all_loss = []<br/># Start training<br/>with tf.Session() as sess:<br/>    writer_1 = tf.summary.FileWriter("./runs/summary/",sess.graph)<br/>    <br/>    sum_var = tf.summary.scalar("loss", model.accuracy)<br/>    write_op = tf.summary.merge_all()<br/><br/>    # Run the initializer<br/>    sess.run(init)<br/><br/>    for step in range(1, hy_param.num_steps+1):<br/>        # Extracting <br/>        batch_x, batch_y = mnist.train.next_batch(hy_param.batch_size)<br/>        # Run optimization op (backprop)<br/>        sess.run(model.train_op, feed_dict={X: batch_x, Y: batch_y})<br/>        if step % hy_param.display_step == 0 or step == 1:<br/>            # Calculate batch loss and accuracy<br/>            loss, acc, summary = sess.run([model.loss_op, model.accuracy, write_op], feed_dict={X: batch_x,<br/>                                                                 Y: batch_y})<br/>            all_loss.append(loss)<br/>            writer_1.add_summary(summary, step)<br/>            print("Step " + str(step) + ", Minibatch Loss= " + \<br/>                  "{:.4f}".format(loss) + ", Training Accuracy= " + \<br/>                  "{:.3f}".format(acc))<br/>        if step % hy_param.checkpoint_every == 0:<br/>            path = saver.save(<br/>                        sess, checkpoint_prefix, global_step=step)<br/># print("Saved model checkpoint to {}\n".format(path))<br/><br/>    print("Optimization Finished!")<br/><br/>    # Calculate accuracy for MNIST test images<br/>    print("Testing Accuracy:", \<br/>        sess.run(model.accuracy, feed_dict={X: mnist.test.images,<br/>                                      Y: mnist.test.labels}))</pre>
<p>Once we have executed the <kbd>train.py</kbd> file, you will see the progress on your console, as shown in the preceding screenshot. This depicts the loss being reduced after every step, along with accuracy increasing over each step:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d036ba50-4562-4694-95c1-e5c7dd2152a6.png" style="width:46.08em;height:40.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The training epoch's output with minibatch loss and training accuracy parameters</div>
<p><span>Also, you can see in the plot of minibatch loss, shown in the following diagram, that it approaches toward the minima with each </span>step:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f77e584b-d4d2-4679-8406-d44b89911846.png" style="width:60.00em;height:31.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Plotting the loss values computed at each step</div>
<p>It is very important to visualize how your model is performing, so that you can analyze and prevent it from underfitting or overfitting. Overfitting is a very common scenario when you are dealing with the deeper models. Let's spend some time getting to understand them in detail and learning a few tricks to overcome them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overfitting and underfitting </h1>
                </header>
            
            <article>
                
<p>With great power comes great responsibility and with deeper models come deeper problems. A fundamental challenge with deep learning is striking the right balance between generalization and optimization. In the deep learning process, we are tuning hyperparameters and often continuously configuring and tweaking the model to produce the best results, based on the data we have for training. This is <strong>optimization</strong>. The key question is, how well does our model generalize in performing predictions on unseen data?</p>
<p>As professional deep learning engineers, our goal is to build models with good real-world generalization. However, generalization is subjective to the model architecture and the training dataset. We work to guide our model for maximum utility by reducing the likelihood that it learns irrelevant patterns or simple similar patterns found in the data used for training. If this is not done, it can affect the generalization process. A good solution is to provide the model with more information that is likely to have a better (that is, more complete and often complex) signal of what you're trying to actually model, by getting<span> more data to train on and to work to optimize the model architecture. Here are few quick tricks that can improve your model by preventing overfitting:</span></p>
<ul>
<li>Getting more data for training</li>
<li>Reducing network capacity by altering the number of layers or nodes</li>
<li>Employing L2 (and trying L1) weight regularization<span> techniques</span></li>
<li>Adding dropout layers or polling layers in the model</li>
</ul>
<div class="packt_tip">L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients, is also known as <em>L1 norm</em>. L2 regularization, where the cost added is proportional to the square of the value of the weight's coefficients, is also known as <em>L2 norm</em> or <em>weight decay.</em></div>
<p>When the model gets trained completely, its output,<span> as checkpoints,</span> will get dumped into the <kbd>/runs</kbd> folder, which will contain the binary dump of <kbd>checkpoints</kbd>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1a78e26a-0e10-4b5a-ae44-43f3ca99e16f.png" style="width:75.08em;height:17.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The checkpoint folder after the training process is completed</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building inference</h1>
                </header>
            
            <article>
                
<p>Now, we will create an inference code that loads the latest checkpoints and then makes predictions on the basis of learned parameters. For that, we need to create a <kbd>saver</kbd> operation that will pick the latest checkpoints and load the metadata. Metadata contains the information regarding the variables and the nodes that we created in the graph:</p>
<pre># Pointing the model checkpoint<br/>checkpoint_file = tf.train.latest_checkpoint(os.path.join(hy_param.checkpoint_dir, 'checkpoints'))<br/>saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))</pre>
<p>We know the importance of this, because we want to load similar variables and operations back from the stored checkpoint. We load them into memory using <kbd>tf.get_default_graph().get_operation_by_name()</kbd><em>, </em>by passing the operation name in the parameter that we defined in the model:</p>
<pre># Load the input variable from the model<br/>input_x = tf.get_default_graph().get_operation_by_name("input_x").outputs[0]<br/><br/># Load the Prediction operation<br/>prediction = tf.get_default_graph().get_operation_by_name("prediction").outputs[0]</pre>
<p>Now, we need to initialize the session and pass data for a test image to the operation that makes the prediction, as follows:</p>
<pre># Load the test data<br/>test_data = np.array([mnist.test.images[0]])<br/><br/>with tf.Session() as sess:<br/>    # Restore the model from the checkpoint<br/>    saver.restore(sess, checkpoint_file)<br/>    <br/>    # Execute the model to make predictions <br/>    data = sess.run(prediction, feed_dict={input_x: test_data })<br/>    <br/>    print("Predicted digit: ", data.argmax() )<br/></pre>
<p>Following is the full code:</p>
<pre>#!/usr/bin/env python2<br/># -*- coding: utf-8 -*-<br/><br/><br/>from __future__ import print_function<br/><br/><br/>import os<br/>import numpy as np<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt<br/><br/>import hy_param<br/><br/>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)<br/><br/># Pointing the model checkpoint<br/>checkpoint_file = tf.train.latest_checkpoint(os.path.join(hy_param.checkpoint_dir, 'checkpoints'))<br/>saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))<br/><br/># Loading test data<br/>test_data = np.array([mnist.test.images[6]])<br/><br/># Loading input variable from the model<br/>input_x = tf.get_default_graph().get_operation_by_name("input_x").outputs[0]<br/><br/># Loading Prediction operation<br/>prediction = tf.get_default_graph().get_operation_by_name("prediction").outputs[0]<br/><br/><br/>with tf.Session() as sess:<br/>    # Restoring the model from the checkpoint<br/>    saver.restore(sess, checkpoint_file)<br/>    <br/>    # Executing the model to make predictions <br/>    data = sess.run(prediction, feed_dict={input_x: test_data })<br/>    <br/>    print("Predicted digit: ", data.argmax() )<br/><br/><br/># Display the feed image<br/>print ("Input image:")<br/>plt.gray()<br/>plt.imshow(test_data.reshape([28,28]))</pre>
<p class="mce-root"/>
<p>And with that, we are done with our first project that predicts the digits provided in a handwritten image! Here are some of the results that the model predicted when provided with the test image from the MNIST dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c276e6f7-1b28-4bab-9be0-94d4680b1928.png" style="width:40.33em;height:44.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The output of the model, depicting the prediction of the model and the input image</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Concluding the project</h1>
                </header>
            
            <article>
                
<p><span>Today's project was to build a classifier to solve the problem of identifying specific handwriting samples from a dataset of images. Our hypothetical use case was to apply deep learning to enable customers of a restaurant chain to write their phone numbers in a simple iPad application, so that they could get a text notification that their party was ready to be seated. Our specific task was to build the intelligence that would drive this application.</span></p>
<div class="packt_tip"><strong>Revisit our success criteria</strong>: How did we do? Did we succeed? What was the impact of our success? Just as we defined success at the beginning of the project, these are the key questions that we need to ask as deep learning data scientists, as we look to wrap up a project.</div>
<p>Our MLP model accuracy hit 87.42%! Not bad, given the depth of the model and the hyperparameters that we chose at the beginning. See if you can tweak the model to get an even higher test set accuracy.</p>
<p>What are the implications of this accuracy? Let's calculate the incidence of an error occurring that would result in a customer service issue (that is, the customer not getting the text that their table is ready, and getting upset due to an excessively long wait time at the restaurant).</p>
<p>Each customer's phone number is ten digits long. Let's say that our hypothetical restaurant has an average of 30 tables at each location, and those tables turn over two times per night during the rush hour, when the system is likely to be used, and finally, the restaurant chain has 35 locations. This means that each day of operation, there are approximately 21,000 handwritten numbers captured (30 tables x 2 turns/day x 35 locations x 10 digit phone number). </p>
<p>Obviously, all digits must be correctly classified for the text to get to the waiting restaurant patron. So, any single digit misclassification causes a failure. A model accuracy of 87.42% would improperly classify 2,642 digits per day in our example. The worst case for the hypothetical scenario would be if there occurred only one improperly classified digit in each phone number. Since there are only 2,100 patrons and corresponding phone numbers, this would mean that every phone number had an error in classification (a 100% failure rate), and not a single customer would get their text notification that their party could be seated! The best case, in this scenario, would be if all 10 digits were misclassified in each phone number, which would result in 263 wrong phone numbers out of 2,100 (a 12.5% failure rate). This is still not a level of performance that the restaurant chain would be likely be happy with.</p>
<div class="packt_tip"><strong>Words of wisdom</strong>: Model performance may not equal system or app performance. Many factors contribute to a system being robust or fragile in the real world. Model performance is a key factor, but other items with individual fault tolerances definitely play a part. Know how your deep learning models integrate into the larger project so that you can set proper expectations! </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In the project in this chapter, we successfully built an MLP to produce a regression classification prediction, based on handwritten digits. We gained experience with the MNIST dataset and a deep neural network model architecture, which gave us the added opportunity to define some key hyperparameters. Finally, we looked at the model performance in testing and determined whether we succeeded in achieving our goals. </p>


            </article>

            
        </section>
    </body></html>