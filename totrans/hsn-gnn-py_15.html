<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer705">
<h1 class="chapter-number" id="_idParaDest-174"><a id="_idTextAnchor179"/>15</h1>
<h1 id="_idParaDest-175"><a id="_idTextAnchor180"/>Forecasting Traffic Using A3T-GCN</h1>
<p>We<a id="_idIndexMarker847"/> introduced T-GNNs in <a href="B19153_13.xhtml#_idTextAnchor153"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, but we did not elaborate on their main application: <strong class="bold">traffic forecasting</strong>. In recent years, the concept of smart cities has become increasingly popular. This idea refers to cities where data is used to manage and improve operations and services. In this context, one of the main sources of appeal is the creation of intelligent transportation systems. Accurate traffic forecasts can help traffic managers to optimize traffic signals, plan infrastructure, and reduce congestion. However, traffic forecasting is a challenging problem due to complex spatial and <span class="No-Break">temporal dependencies.</span></p>
<p>In this chapter, we will apply T-GNNs to a particular case of traffic forecasting. First, we will explore and process a new dataset to create a temporal graph from raw CSV files. We will then apply a new type of T-GNN to predict future traffic speed. Finally, we will visualize and compare the results to a baseline solution to verify that our architecture <span class="No-Break">is relevant.</span></p>
<p>By the end of this chapter, you will know how to create a temporal graph dataset from tabular data. In particular, we will see how to create a weighted adjacency matrix that will provide us with edge weights. Finally, you will learn how to apply a T-GNN to a traffic forecasting task and evaluate <span class="No-Break">the results.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Exploring the <span class="No-Break">PeMS-M dataset</span></li>
<li>Processing <span class="No-Break">the dataset</span></li>
<li>Implementing a <span class="No-Break">temporal GNN</span></li>
</ul>
<h1 id="_idParaDest-176"><a id="_idTextAnchor181"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter15"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter15</span></a><span class="No-Break">.</span></p>
<p>Installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> of this book. This chapter requires a large amount of GPU. You can lower it by decreasing the size of the training set in <span class="No-Break">the code.</span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor182"/>Exploring the PeMS-M dataset</h1>
<p>In this section, we <a id="_idIndexMarker848"/>will explore our dataset to find patterns and get insights that will be useful to the task <span class="No-Break">of interest.</span></p>
<p>The dataset we will use for this application is the medium variant of the <strong class="source-inline">PeMSD7</strong> dataset [1]. The original dataset was obtained by collecting traffic speed from 39,000 sensor stations on the <a id="_idIndexMarker849"/>weekdays of May and June 2012 using the Caltrans <strong class="bold">Performance Measurement System</strong> (<strong class="bold">PeMS</strong>). We will only consider 228 stations across District 7 of California in the medium variant. These stations output 30-second speed measurements that are aggregated into 5-minute intervals in this dataset. For example, the following figure shows the Caltrans PeMS (<a href="https://pems.dot.ca.gov">pems.dot.ca.gov</a>) with various <span class="No-Break">traffic speeds:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer670">
<img alt="Figure 15.1 – Traffic data from Caltrans PeMS with high speed (&gt;60 mph) in green and low speed (&lt;35 mph) in red" height="747" src="image/B19153_15_001.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1 – Traffic data from Caltrans PeMS with high speed (&gt;60 mph) in green and low speed (&lt;35 mph) in red</p>
<p>We <a id="_idIndexMarker850"/>can directly load the dataset from GitHub and <span class="No-Break">unzip it:</span></p>
<pre class="source-code">
from io import BytesIO
from urllib.request import urlopen
from zipfile import ZipFile
url = 'https://github.com/VeritasYin/STGCN_IJCAI-18/raw/master/data_loader/PeMSD7_Full.zip'
with urlopen(url) as zurl:
    with ZipFile(BytesIO(zurl.read())) as zfile:
        zfile.extractall('.')</pre>
<p>The resulting folder contains two files: <strong class="source-inline">V_228.csv</strong> and <strong class="source-inline">W_228.csv</strong>. The <strong class="source-inline">V_228.csv</strong> file contains the traffic speed collected by the 228 sensor stations, and <strong class="source-inline">W_228.csv</strong> stores the distances between <span class="No-Break">these stations.</span></p>
<p>Let’s load them using <strong class="source-inline">pandas</strong>. We will rename the columns using <strong class="source-inline">range()</strong> for <span class="No-Break">easy access:</span></p>
<pre class="source-code">
import pandas as pd
speeds = pd.read_csv('PeMSD7_V_228.csv', names=range(0,228))
distances = pd.read_csv('PeMSD7_W_228.csv.csv', names=range(0,228))</pre>
<p>The first thing we want to do with this dataset is to visualize the evolution of traffic speed. This is a classic in time series forecasting since characteristics such as seasonality can be extremely helpful. On the other hand, non-stationary time series might need further processing before they can <span class="No-Break">be used.</span></p>
<p>Let’s plot the traffic speed over time <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">:</span></p>
<ol>
<li>We import <strong class="source-inline">NumPy</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">:</span><pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt</pre></li>
<li>We <a id="_idIndexMarker851"/>use <strong class="source-inline">plt.plot()</strong> to create a line plot for every row in <span class="No-Break">our DataFrame:</span><pre class="source-code">
plt.figure(figsize=(10,5))
plt.plot(speeds)
plt.grid(linestyle=':')
plt.xlabel('Time (5 min)')
plt.ylabel('Traffic speed')</pre></li>
<li>We obtain the <span class="No-Break">following plot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer671">
<img alt="Figure 15.2 – Traffic speed over time for each of the 228 sensor stations" height="864" src="image/B19153_15_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2 – Traffic speed over time for each of the 228 sensor stations</p>
<p>Unfortunately, the data is too noisy to give us any insight using this approach. Instead, we could plot the data corresponding to a few sensor stations. However, it might not be representative of the entire dataset. There is another option: we can plot the mean traffic speed with standard deviation. That way, we can visualize a summary of <span class="No-Break">the dataset.</span></p>
<p>In practice, we would use both approaches, but let’s try the second <span class="No-Break">option now:</span></p>
<ol>
<li>We <a id="_idIndexMarker852"/>calculate the mean traffic speed with the corresponding standard deviation for each column (<span class="No-Break">time step):</span><pre class="source-code">
mean = speeds.mean(axis=1)
std = speeds.std(axis=1)</pre></li>
<li>We plot the mean values in black with a <span class="No-Break">solid line:</span><pre class="source-code">
plt.plot(mean, 'k-')</pre></li>
<li>We plot the standard deviation around the mean values using <strong class="source-inline">plt.fill_between()</strong> in <span class="No-Break">light red:</span><pre class="source-code">
plt.fill_between(mean.index, mean-std, mean+std, color='r', alpha=0.1)
plt.grid(linestyle=':')
plt.xlabel('Time (5 min)')
plt.ylabel('Traffic speed')</pre></li>
<li>The code generates the <span class="No-Break">following plot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer672">
<img alt="Figure 15.3 – Mean traffic speed over time with a standard deviation" height="864" src="image/B19153_15_003.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.3 – Mean traffic speed over time with a standard deviation</p>
<p>This figure is much more comprehensible. We can see a clear seasonality (pattern) in the time series data, except around the 5,800<span class="superscript">th</span> data sample. The traffic speed has a lot of variability with important spikes. This is understandable because the sensor stations are spread throughout District 7 of California: traffic might be jammed for some sensors but not <span class="No-Break">for others.</span></p>
<p>We can <a id="_idIndexMarker853"/>verify that by plotting the correlation between the speed values from every sensor. In addition to that, we can compare it with the distances between each station. Stations close to each other should display similar values more often than <span class="No-Break">distant ones.</span></p>
<p>Let’s compare these two plots on the <span class="No-Break">same figure:</span></p>
<ol>
<li>We create a figure with two horizontal subplots and some padding <span class="No-Break">between them:</span><pre class="source-code">
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8))
fig.tight_layout(pad=3.0)</pre></li>
<li>First, we use the <strong class="source-inline">matshow()</strong> function to plot the <span class="No-Break">distance matrix:</span><pre class="source-code">
ax1.matshow(distances)
ax1.set_xlabel("Sensor station")
ax1.set_ylabel("Sensor station")
ax1.title.set_text("Distance matrix")</pre></li>
<li>Then, we calculate the Pearson correlation coefficients for each sensor station. We must transpose the speed matrix or we will get the correlation coefficients for each time step instead. Finally, we invert them, so the two plots are easier <span class="No-Break">to compare:</span><pre class="source-code">
ax2.matshow(-np.corrcoef(speeds.T))
ax2.set_xlabel("Sensor station")
ax2.set_ylabel("Sensor station")
ax2.title.set_text("Correlation matrix")</pre></li>
<li>We obtain the <span class="No-Break">following plot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer673">
<img alt="Figure 15.4 – Distance and correlation matrices with darker colors representing short distances and high correlation, while brighter colors represent long distances and low correlation" height="834" src="image/B19153_15_004.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.4 – Distance and correlation matrices with darker colors representing short distances and high correlation, while brighter colors represent long distances and low correlation</p>
<p>Interestingly, long<a id="_idIndexMarker854"/> distances between stations do not mean they are not highly correlated (and vice versa). This is particularly important if we only consider a subset of this dataset: close stations might have very different outputs, making traffic forecasting more difficult. In this chapter, we will take into account every sensor station in <span class="No-Break">the dataset.</span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor183"/>Processing the dataset</h1>
<p>Now that we have more information about this dataset, it is time to process it before we can feed it to <span class="No-Break">a T-GNN.</span></p>
<p>The first <a id="_idIndexMarker855"/>step consists of transforming the tabular dataset into a temporal graph. So, first, we need to create a graph from the raw data. In other words, we must connect the different sensor stations in a meaningful way. Fortunately, we have access to the distance matrix, which should be a good way to connect <span class="No-Break">the stations.</span></p>
<p>There are several options to compute the adjacency matrix from the distance matrix. For example, we could assign a link when the distance between two stations is inferior to the mean distance. Instead, we will perform a more advanced processing introduced in [2] to calculate a weighted adjacency matrix. Instead of binary values, we calculate weights between 0 (no connection) and 1 (strong connection) using the <span class="No-Break">following formula:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer674">
<img alt="" height="112" src="image/Formula_B19153_15_001.jpg" width="1108"/>
</div>
</div>
<p>Here, <img alt="" height="47" src="image/Formula_B19153_15_002.png" width="67"/> represents<a id="_idIndexMarker856"/> the weight of the edge from node <img alt="" height="34" src="image/Formula_B19153_15_003.png" width="15"/> to node <img alt="" height="39" src="image/Formula_B19153_15_004.png" width="21"/>, <img alt="" height="43" src="image/Formula_B19153_15_005.png" width="44"/> is the distance between these two nodes, and <img alt="" height="36" src="image/Formula_B19153_15_006.png" width="40"/> and <img alt="" height="21" src="image/Formula_B19153_15_007.png" width="19"/> are two thresholds to control the distribution and sparsity of the adjacency matrix. The official implementation of [2] is available on GitHub (<a href="https://github.com/VeritasYin/STGCN_IJCAI-18">https://github.com/VeritasYin/STGCN_IJCAI-18</a>). We will reuse the same threshold values <img alt="" height="37" src="image/Formula_B19153_15_008.png" width="144"/> <span class="No-Break">and <img alt="" height="32" src="image/Formula_B19153_15_009.png" width="121"/>.</span></p>
<p>Let’s implement it in Python and plot the resulting <span class="No-Break">adjacency matrix:</span></p>
<ol>
<li>We create a function to compute the adjacency matrix that takes three parameters: the distance matrix and the two thresholds <img alt="" height="42" src="image/Formula_B19153_15_010.png" width="48"/> and <img alt="" height="27" src="image/Formula_B19153_15_011.png" width="23"/>. Like in the official implementation, we divide the distances by 10,000 and <span class="No-Break">calculate <img alt="" height="44" src="image/Formula_B19153_15_012.png" width="45"/>:</span><pre class="source-code">
def compute_adj(distances, sigma2=0.1, epsilon=0.5):
    d = distances.to_numpy() / 10000.
    d2 = d * d</pre></li>
<li>Here, we <a id="_idIndexMarker857"/>want weights when their values are greater than or equal to <img alt="" height="26" src="image/Formula_B19153_15_013.png" width="23"/> (otherwise, they should be equal to zero). When we test whether the weights are greater than or equal to <img alt="" height="27" src="image/Formula_B19153_15_014.png" width="23"/>, the results are <strong class="source-inline">True</strong> or <strong class="source-inline">False</strong> statements. This is why we need a mask of ones (<strong class="source-inline">w_mask</strong>) to convert it back into 0 and 1 values. We multiply it a second time so that we only obtain the real values of weights that are greater than or equal <span class="No-Break">to <img alt="" height="26" src="image/Formula_B19153_15_015.png" width="23"/>:</span><pre class="source-code">
    n = distances.shape[0]
    w_mask = np.ones([n, n]) - np.identity(n)
    return np.exp(-d2 / sigma2) * (np.exp(-d2 / sigma2) &gt;= epsilon) * w_mask</pre></li>
<li>Let’s compute our adjacency matrix and print the result for <span class="No-Break">one line:</span><pre class="source-code">
adj = compute_adj(distances)
adj[0]
<strong class="bold">array([0.       , 0.       , 0.        , 0.       , 0.  ,</strong>
<strong class="bold">       0.       , 0.       , 0.61266012, 0.       , ...</strong></pre></li>
</ol>
<p>We can see a value of <strong class="source-inline">0.61266012</strong>, representing the weight of the edge from node 1 to <span class="No-Break">node 2.</span></p>
<ol>
<li value="4">A more<a id="_idIndexMarker858"/> efficient way to visualize this matrix is to use <strong class="source-inline">matplotlib</strong>’s <span class="No-Break"><strong class="source-inline">matshow</strong></span><span class="No-Break"> again:</span><pre class="source-code">
plt.figure(figsize=(8, 8))
cax = plt.matshow(adj, False)
plt.colorbar(cax)
plt.xlabel("Sensor station")
plt.ylabel("Sensor station")</pre></li>
</ol>
<p>We get the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer689">
<img alt="Figure 15.5 – The PeMS-M dataset’s weighted adjacency matrix" height="1164" src="image/B19153_15_005.jpg" width="1214"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.5 – The PeMS-M dataset’s weighted adjacency matrix</p>
<p>This is a great way to summarize this first processing step. We can compare it to the distance matrix we previously plotted to <span class="No-Break">find similarities.</span></p>
<ol>
<li value="5">We can also <a id="_idIndexMarker859"/>directly plot it as a graph using <strong class="source-inline">networkx</strong>. In this case, connections are binary, so we can simply consider every weight higher than 0. We could display these values using edge labels, but the graph would be extremely difficult <span class="No-Break">to read:</span><pre class="source-code">
import networkx as nx
def plot_graph(adj):
    plt.figure(figsize=(10,5))
    rows, cols = np.where(adj &gt; 0)
    edges = zip(rows.tolist(), cols.tolist())
    G = nx.Graph()
    G.add_edges_from(edges)
    nx.draw(G, with_labels=True)
    plt.show()</pre></li>
<li>Even without labels, the resulting graph is not easy <span class="No-Break">to read:</span><pre class="source-code">
plot_graph(adj)</pre></li>
</ol>
<p>It gives us the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer690">
<img alt="Figure 15.6 – The PeMS-M dataset as a graph (every node represents a sensor station)" height="746" src="image/B19153_15_006.jpg" width="1423"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.6 – The PeMS-M dataset as a graph (every node represents a sensor station)</p>
<p>Indeed, many nodes are<a id="_idIndexMarker860"/> interconnected because they are very close to each other. Yet, despite that, we can distinguish several branches that could correspond to <span class="No-Break">actual roads.</span></p>
<p>Now that we have a graph, we can focus on the time series aspect of this problem. The first step consists of normalizing the speed values so they can be fed to a neural network. In the traffic forecasting literature, many authors choose a z-score normalization (or standardization), which we will <span class="No-Break">implement here:</span></p>
<ol>
<li>We create a function to <span class="No-Break">calculate z-scores:</span><pre class="source-code">
def zscore(x, mean, std):
    return (x - mean) / std</pre></li>
<li>We apply it to our dataset to create a normalized version <span class="No-Break">of it:</span><pre class="source-code">
speeds_norm = zscore(speeds, speeds.mean(axis=0), speeds.std(axis=0))</pre></li>
<li>We can check <span class="No-Break">the result:</span><pre class="source-code">
speeds_norm.head(1)</pre></li>
</ol>
<p>We get the <span class="No-Break">following output:</span></p>
<table class="No-Table-Style" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">0</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">1</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">2</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">3</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">4</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">5</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">6</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">…</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">0</strong></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.950754</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.548255</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.502211</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.831672</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.793696</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1.193806</span></p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.7 – An example of standardized speed values</p>
<p>These values are correctly standardized. Now, we can use them to create time series for each node. We want <img alt="" height="23" src="image/Formula_B19153_15_016.png" width="27"/> input data samples at each time step, <img alt="" height="30" src="image/Formula_B19153_15_017.png" width="17"/>, to predict the speed value at <img alt="" height="34" src="image/Formula_B19153_15_018.png" width="92"/>. A high number of input data samples also increases the memory footprint of the dataset. The <a id="_idIndexMarker861"/>value for <img alt="" height="31" src="image/Formula_B19153_15_019.png" width="23"/>, also called the horizon, depends on the task we want to perform: short-term or long-term <span class="No-Break">traffic forecasting.</span></p>
<p>In this example, let’s take a high value of 48 to predict the traffic speed in <span class="No-Break">4 hours:</span></p>
<ol>
<li>We initialize the variables: the number of <strong class="source-inline">lags</strong> (number of input data samples), <strong class="source-inline">horizon</strong>, the input matrix, and the <span class="No-Break">ground-truth matrix:</span><pre class="source-code">
lags = 24
horizon = 48
xs = []
ys = []</pre></li>
<li>For each time step <img alt="" height="32" src="image/Formula_B19153_15_020.png" width="18"/>, we store the 12 (lags) previous values in <strong class="source-inline">xs</strong> and the value at <img alt="" height="32" src="image/Formula_B19153_15_021.png" width="86"/> <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">ys</strong></span><span class="No-Break">:</span><pre class="source-code">
for i in range(lags, speeds_norm.shape[0]-horizon):
    xs.append(speeds_norm.to_numpy()[i-lags:i].T)
    ys.append(speeds_norm.to_numpy()[i+horizon-1])</pre></li>
<li>Finally, we <a id="_idIndexMarker862"/>can create the temporal graph using PyTorch Geometric Temporal. We need to give the edge index in COO format and the edge weight from the weighted <span class="No-Break">adjacency matrix:</span><pre class="source-code">
from torch_geometric_temporal.signal import StaticGraphTemporalSignal
edge_index = (np.array(adj) &gt; 0).nonzero()
edge_weight = adj[adj &gt; 0]
dataset = StaticGraphTemporalSignal(edge_index, adj[adj &gt; 0], xs, ys)</pre></li>
<li>Let’s print information about the first graph to see whether everything <span class="No-Break">looks good:</span><pre class="source-code">
dataset[0]
<strong class="bold">Data(x=[228, 12], edge_index=[2, 1664], edge_attr=[1664], y=[228])</strong></pre></li>
<li>Let’s not forget the train/test split to finalize <span class="No-Break">our dataset:</span><pre class="source-code">
from torch_geometric_temporal.signal import temporal_signal_split
train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)</pre></li>
</ol>
<p>The final temporal graph <a id="_idIndexMarker863"/>has 228 nodes with 12 values and 1,664 connections. We are now ready to apply a T-GNN to <span class="No-Break">predict traffic.</span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor184"/>Implementing the A3T-GCN architecture</h1>
<p>In this section, we will train an <strong class="bold">Attention Temporal Graph Convolutional Network</strong> (<strong class="bold">A3T-GCN</strong>), designed <a id="_idIndexMarker864"/>for traffic forecasting. This architecture allows us to consider complex spatial and <span class="No-Break">temporal dependencies:</span></p>
<ul>
<li>Spatial dependencies refer to the fact that the traffic condition of a location can be influenced by the traffic condition of nearby locations. For example, traffic jams often spread to <span class="No-Break">neighboring roads.</span></li>
<li>Temporal dependencies refer to the fact that the traffic condition of a location at a time can be influenced by the traffic condition of the same location at previous times. For example, if a road is congested during the morning peak, it is likely to remain congested until the <span class="No-Break">evening peak.</span></li>
</ul>
<p>A3T-GCN is an improvement over the <strong class="bold">temporal GCN</strong> (<strong class="bold">TGCN</strong>) architecture. The TGCN is a combination of a GCN and GRU that <a id="_idIndexMarker865"/>produces hidden vectors from each input time series. The combination of these two layers captures spatial and temporal information from the input. An attention model is then used to calculate weights and output a context vector. The final prediction is based on the resulting context vector. The addition of this attention model is motivated by the need to understand <span class="No-Break">global trends.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer697">
<img alt="Figure 15.8 – The A3T-GCN framework" height="476" src="image/B19153_15_007.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.8 – The A3T-GCN framework</p>
<p>We will now implement it using the PyTorch Geometric <span class="No-Break">Temporal library:</span></p>
<ol>
<li>First, we import the <span class="No-Break">required libraries:</span><pre class="source-code">
import torch
from torch_geometric_temporal.nn.recurrent import A3TGCN</pre></li>
<li>We <a id="_idIndexMarker866"/>create a T-GNN with an <strong class="source-inline">A3TGCN</strong> layer and a linear layer with 32 hidden dimensions. The <strong class="source-inline">edge_attr</strong> parameter will store our <span class="No-Break">edge weights:</span><pre class="source-code">
class TemporalGNN(torch.nn.Module):
    def __init__(self, dim_in, periods):
        super().__init__()
        self.tgnn = A3TGCN(in_channels=dim_in, out_channels=32, periods=periods)
        self.linear = torch.nn.Linear(32, periods)
    def forward(self, x, edge_index, edge_attr):
        h = self.tgnn(x, edge_index, edge_attr).relu()
        h = self.linear(h)
        return h</pre></li>
<li>We instantiate the T-GNN and the <strong class="source-inline">Adam</strong> optimizer with a learning rate of <strong class="source-inline">0.005</strong>. Due to implementation details, we will train this model using a CPU instead of a GPU, which is faster in <span class="No-Break">this case:</span><pre class="source-code">
model = TemporalGNN(lags, 1).to('cpu')
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)</pre></li>
<li>We train the model for 30 epochs using<a id="_idIndexMarker867"/> the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) as the <strong class="source-inline">loss</strong> function. The <strong class="source-inline">loss</strong> value is backpropagated after <span class="No-Break">each epoch:</span><pre class="source-code">
model.train()
for epoch in range(30):
    loss = 0
    step = 0
    for i, snapshot in enumerate(train_dataset):
        y_pred = model(snapshot.x.unsqueeze(2), snapshot.edge_index, snapshot.edge_attr)
        loss += torch.mean((y_pred-snapshot.y)**2)
        step += 1
    loss = loss / (step + 1)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if epoch % 10 == 0:
        print(f"Epoch {epoch+1:&gt;2} | Train MSE: {loss:.4f}")</pre></li>
<li>We <a id="_idIndexMarker868"/>obtain the <span class="No-Break">following output:</span><pre class="source-code">
<strong class="bold">Epoch  1 | Train MSE: 1.0209</strong>
<strong class="bold">Epoch 10 | Train MSE: 0.9625</strong>
<strong class="bold">Epoch 20 | Train MSE: 0.9143</strong>
<strong class="bold">Epoch 30 | Train MSE: 0.8905</strong></pre></li>
</ol>
<p>Now that our model is trained, we have to evaluate it. Beyond classic<a id="_idIndexMarker869"/> metrics such as <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) and <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>), it is particularly <a id="_idIndexMarker870"/>helpful to compare our model to a baseline solution with time series data. In the following list, we will introduce <span class="No-Break">two methods:</span></p>
<ul>
<li>Using <strong class="bold">Random Walk</strong> (<strong class="bold">RW</strong>) as a<a id="_idIndexMarker871"/> naïve forecaster. In this case, RW refers to using the last <a id="_idIndexMarker872"/>observation as the predicted value. In other words, the value at <img alt="" height="31" src="image/Formula_B19153_15_022.png" width="18"/> is the same one as <span class="No-Break">at <img alt="" height="32" src="image/Formula_B19153_15_023.png" width="86"/>.</span></li>
<li>Using <strong class="bold">Historical Average</strong> (<strong class="bold">HA</strong>) as <a id="_idIndexMarker873"/>a slightly more evolved solution. In this case, we calculate the mean traffic speed of <img alt="" height="31" src="image/Formula_B19153_15_024.png" width="22"/> previous samples as the value at <img alt="" height="31" src="image/Formula_B19153_15_025.png" width="86"/>. In this example, we will use the number of lags as our <img alt="" height="30" src="image/Formula_B19153_15_026.png" width="20"/> value, but we could also take the overall <span class="No-Break">historical average.</span></li>
</ul>
<p>Let’s start by evaluating the model’s predictions on the <span class="No-Break">test set:</span></p>
<ol>
<li>We create a function to invert the z-score and get back to the <span class="No-Break">original values:</span><pre class="source-code">
def inverse_zscore(x, mean, std):
    return x * std + mean</pre></li>
<li>We use it to recalculate the speeds we want to predict from their normalized values. The following loop is not very efficient, but it is clearer to understand than more <span class="No-Break">optimized code:</span><pre class="source-code">
y_test = []
for snapshot in test_dataset:
    y_hat = snapshot.y.numpy()
    y_hat = inverse_zscore(y_hat, speeds.mean(axis=0), speeds.std(axis=0))
    y_test = np.append(y_test, y_hat)</pre></li>
<li>We apply the same strategy to the predictions made by <span class="No-Break">the GNN:</span><pre class="source-code">
gnn_pred = []
model.eval()
for snapshot in test_dataset:
    y_hat = model(snapshot.x.unsqueeze(2), snapshot.edge_index, snapshot.edge_weight).squeeze().detach().numpy()
    y_hat = inverse_zscore(y_hat, speeds.mean(axis=0), speeds.std(axis=0))
    gnn_pred = np.append(gnn_pred, y_hat)</pre></li>
<li>We do<a id="_idIndexMarker874"/> the same thing for the RW and <span class="No-Break">HA techniques:</span><pre class="source-code">
rw_pred = []
for snapshot in test_dataset:
    y_hat = snapshot.x[:,-1].squeeze().detach().numpy()
    y_hat = inverse_zscore(y_hat, speeds.mean(axis=0), speeds.std(axis=0))
    rw_pred = np.append(rw_pred, y_hat)
ha_pred = []
for i in range(lags, speeds_norm.shape[0]-horizon):
    y_hat = speeds_norm.to_numpy()[i-lags:i].T.mean(axis=1)
    y_hat = inverse_zscore(y_hat, speeds.mean(axis=0), speeds.std(axis=0))
    ha_pred.append(y_hat)
ha_pred = np.array(ha_pred).flatten()[-len(y_test):]</pre></li>
<li>We create <a id="_idIndexMarker875"/>functions to calculate the <a id="_idIndexMarker876"/>MAE, RMSE, and the <strong class="bold">Mean Absolute Percentage </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MAPE</strong></span><span class="No-Break">):</span><pre class="source-code">
def MAE(real, pred):
    return np.mean(np.abs(pred - real))
def RMSE(real, pred):
    return np.sqrt(np.mean((pred - real) ** 2))
def MAPE(real, pred):
    return np.mean(np.abs(pred - real) / (real + 1e-5))</pre></li>
<li>We evaluate the GNN’s predictions in the following block and repeat this process for <span class="No-Break">every technique:</span><pre class="source-code">
print(f'GNN MAE  = {MAE(gnn_pred, y_test):.4f}')
print(f'GNN RMSE = {RMSE(gnn_pred, y_test):.4f}')
print(f'GNN MAPE = {MAPE(gnn_pred, y_test):.4f}')</pre></li>
</ol>
<p>In the end, we obtain the <span class="No-Break">following table:</span></p>
<table class="No-Table-Style" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">RMSE</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">MAE</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">MAPE</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">A3T-GCN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">11.9396</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">8.3293</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">14.95%</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Random Walk</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">17.6501</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">11.0469</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">29.99%</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Historical Average</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">17.9009</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">11.7308</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28.93%</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.9 – Output table of predictions</p>
<p>We see that the <a id="_idIndexMarker877"/>baseline techniques are outperformed by the A3T-GCN model in every metric. This is an important result because baselines can often be difficult to beat. It would be interesting to compare these metrics to the predictions provided by LSTM or GRU networks to measure the importance of the <span class="No-Break">topological information.</span></p>
<p>Finally, we can plot the mean predictions to obtain a visualization that is similar to <span class="No-Break"><em class="italic">Figure 15</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
<ol>
<li>We obtain the mean predictions using a list comprehension that is a little faster than the previous method (but harder <span class="No-Break">to read):</span><pre class="source-code">
y_preds = [inverse_zscore(model(snapshot.x.unsqueeze(2), snapshot.edge_index, snapshot.edge_weight).squeeze().detach().numpy(), speeds.mean(axis=0), speeds.std(axis=0)).mean() for snapshot in test_dataset]</pre></li>
<li>We calculate the mean and standard deviation of the <span class="No-Break">original dataset:</span><pre class="source-code">
mean = speeds.mean(axis=1)
std = speeds.std(axis=1)</pre></li>
<li>We plot the mean traffic speed with standard deviation and compare it to the predicted values (<img alt="" height="33" src="image/Formula_B19153_15_027.png" width="91"/> <span class="No-Break">hours):</span><pre class="source-code">
plt.figure(figsize=(10,5), dpi=300)
plt.plot(mean, 'k-', label='Mean')
plt.plot(range(len(speeds)-len(y_preds), len(speeds)), y_preds, 'r-', label='Prediction')
plt.grid(linestyle=':')
plt.fill_between(mean.index, mean-std, mean+std, color='r', alpha=0.1)
plt.axvline(x=len(speeds)-len(y_preds), color='b', linestyle='--')
plt.xlabel('Time (5 min)')
plt.ylabel('Traffic speed to predict')
plt.legend(loc='upper right')</pre></li>
</ol>
<p>We <a id="_idIndexMarker878"/>obtain the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer704">
<img alt="Figure 15.10 – Mean traffic speeds predicted by the A3T-GCN model on the test set" height="694" src="image/B19153_15_008.jpg" width="1326"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.10 – Mean traffic speeds predicted by the A3T-GCN model on the test set</p>
<p>The T-GNN<a id="_idIndexMarker879"/> correctly predicts spikes and follows the general trend. However, the predicted speeds are closer to the overall average value, as it is more costly for the model to make serious mistakes due to the MSE loss. Despite that, the GNN is quite accurate and could be fine-tuned to output more <span class="No-Break">extreme values.</span></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor185"/>Summary</h1>
<p>This chapter focused on a traffic forecasting task using T-GNNs. First, we explored the PeMS-M dataset and converted it from tabular data into a static graph dataset with a temporal signal. In practice, we created a weighted adjacency matrix based on the input distance matrix and converted the traffic speeds into time series. Finally, we implemented an A3T-GCN model, a T-GNN designed for traffic forecasting. We compared the results to two baselines and validated the predictions made by <span class="No-Break">our model.</span></p>
<p>In <a href="B19153_16.xhtml#_idTextAnchor187"><span class="No-Break"><em class="italic">Chapter 16</em></span></a><em class="italic">, Building a Recommender System Using LightGCN</em>, we will see the most popular application of GNNs. We will implement a lightweight GNN on a massive dataset and evaluate it using techniques from <span class="No-Break">recommender systems.</span></p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor186"/>Further reading</h1>
<ul>
<li>[1] B. Yu, H. Yin, and Z. Zhu. <em class="italic">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</em>. Jul. 2018. doi: 10.24963/ijcai.2018/505. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1709.04875"><span class="No-Break">https://arxiv.org/abs/1709.04875</span></a><span class="No-Break">.</span></li>
<li>[2] Y. Li, R. Yu, C. Shahabi, and Y. Liu. <em class="italic">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</em>. arXiv, 2017. doi: 10.48550/ARXIV.1707.01926. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1707.01926"><span class="No-Break">https://arxiv.org/abs/1707.01926</span></a><span class="No-Break">.</span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer706">
</div>
</div>
</div></body></html>