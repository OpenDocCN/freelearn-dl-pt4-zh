- en: Tuning and Optimizing Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last two chapters, we trained deep learning models for classification,
    regression, and image recognition tasks. In this chapter, we will discuss some
    important issues in regard to managing deep learning projects. While this chapter
    may seem somewhat theoretical, if any of the issues discussed are not correctly
    managed, it can derail your deep learning project. We will look at how to choose
    evaluation metrics and how to create an estimate of how well a deep learning model
    will perform before you begin modeling. Next, we will move onto data distribution and
    the mistakes often made in splitting data into correct partitions for training. Many
    machine learning projects fail in production use because the data distribution
    is different to what the model was trained with. We will look at data augmentation,
    a valuable method to enhance your model's accuracy. Finally, we will discuss hyperparameters
    and learn how to tune them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be looking at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics and evaluating performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pre-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case—interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation metrics and evaluating performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will discuss how to set up a deep learning project and what evaluation
    metrics to select. We will look at how to select evaluation criteria and how to
    decide when the model is approaching optimal performance. We will also discuss
    how all deep learning models tend to overfit and how to manage the bias/variance
    tradeoff. This will give guidelines on what to do when models have low accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Types of evaluation metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different evaluation metrics are used for categorization and regression tasks.
    For categorization, accuracy is the most commonly used evaluation metric. However,
    accuracy is only valid if the cost of errors is the same for all classes, which
    is not always the case. For example, in medical diagnosis, the cost of a false
    negative will be much higher than the cost of a false positive. A false negative
    in this case says that the person is not sick when they are, and a delay in diagnosis
    can have serious, perhaps fatal, consequences. On the other hand, a false positive is
    saying that the person is sick when they are not, which is upsetting for that
    person but is not life threatening.
  prefs: []
  type: TYPE_NORMAL
- en: 'This issue is compounded when you have imbalanced datasets, that is, when one
    class is much more common than the other. Going back to our medical diagnosis
    example, if only 1% of people who get tested actually have the disease, then a
    machine learning algorithm can get 99% accuracy by just declaring that nobody
    has the disease. In this case, you can look at other metrics rather than accuracy.
    One such metric that is useful for imbalanced datasets is the F1 evaluation metric,
    which is a weighted average of precision and recall. The formula for the F1 score
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F1 = 2 * (precision * recall) / (precision + recall)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formulas for precision and recall are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*precision = true_positives / (true_positives + false_positives)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*recall = true_positives / (true_positives + false_negatives)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression, you have a choice of evaluation metrics: MAE, MSE, and RMSE.
    **MAE**, or **Mean Absolute Error**, is the simplest; it is just the average of
    the absolute difference between the actual value and the predicted value. The
    advantage of MAE is that it is easily understood; if MAE is 3.5, then the difference
    between the predicted value and the actual value is 3.5 on average. **MSE**, or
    **Mean Squared Error**, is the average of the squared error, that is, it takes
    the difference between the actual value and the predicted value, squares it, and
    then takes the average of those values. The advantage of using MSE over MAE is
    that it penalizes errors according to their severity. If the difference between
    the actual value and the predicted value for two rows was 2 and 5, then the MSE
    would put more weight on the second example because the error is larger. **RMSE**,
    or **Root Mean Squared Error**, is the square root of MSE. The advantage of using
    MSE is that it puts the error term back into units that are comparable to the
    actual values. For regression tasks, RMSE is usually the preferred metric.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on metrics in MXNet, see [https://mxnet.incubator.apache.org/api/python/metric/metric.html](https://mxnet.incubator.apache.org/api/python/metric/metric.html).
  prefs: []
  type: TYPE_NORMAL
- en: For more information on metrics in Keras, see [https://keras.io/metrics/](https://keras.io/metrics/).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have explored a few deep learning models in earlier chapters. We got an accuracy
    rate of 98.36% in our image classification task on the `MNIST` dataset in [Chapter
    5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification Using Convolutional
    Neural Networks.* For the binary classification task (predicting which customers
    will return in the next 14 days) in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),* Training
    Deep Prediction Models*, we got an accuracy rate of 77.88%. But what does this
    actually mean and how do we evaluate the performance of a deep learning model?
  prefs: []
  type: TYPE_NORMAL
- en: The obvious starting point in evaluating whether your deep learning model has
    good predictive capability is by comparing it to other models. The `MNIST` dataset
    is used in a lot of benchmarks for deep learning research, so we know that there
    are models that achieve 99.5% accuracy. Therefore, our model is OK, but not great.
    In the *D**ata augmentation* section in this chapter, we will improve our model
    significantly, from 98.36% accuracy to 98.95% accuracy, by augmenting our data
    with new images created by making changes to the existing image data. In general,
    for image classification tasks anything less than 95% accuracy probably indicates
    a problem with your deep learning model. Either the model is not designed correctly
    or you do not have enough data for your task.
  prefs: []
  type: TYPE_NORMAL
- en: Our binary classification model only had 77.54% accuracy, which is much less
    than the image classification task. So, is it a terrible model? Not really; it
    is still a useful model. We also have some benchmarks from other machine learning
    models such as random forest and xgboost that we ran on a small section of the
    data. We also saw that we got an increase in accuracy when we moved from a model
    with 3,900 rows to a deeper model with 390,000 rows. This highlights that deep
    learning models improve with more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One step you can do to evaluate your model''s performance is to see if more
    data will increase accuracy significantly. The data can be acquired from more
    training data, or from data augmentation, which we will see later. You can use learning
    curves to evaluate if this will help with performance. To create a learning curve,
    you train a series of machine learning models with increasing sizes, for example,
    10,000 rows to 200,000 rows in steps of 1,000 rows. For each step, run `5` different
    machine learning models to smooth the results and plot average accuracy by the
    sample size. Here is the pseudocode to perform this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of a learning curve plot for similar task to the churn problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5c76faa-0ab1-4117-a821-8d943a43ec49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: An example of a learning curve which plots accuracy by data size'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, accuracy is in a very narrow range and stabilizes as the # instances
    increase. Therefore, for this algorithm and hyperparameter choice, adding more
    data will not increase accuracy significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: If we get a learning curve that is flat like in this example, then adding more
    data to the existing model will not increase accuracy. We could try to improve
    our performance by either changing the model architecture or by adding more features.
    We discussed some options for this in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our binary classification model, let''s consider how we could
    we use it in production. Recall that this model is trying to predict if customers
    will return in the next *x* days. Here is the confusion matrix from that model
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at how the model performs for each class, we get a different accuracy
    rates:'
  prefs: []
  type: TYPE_NORMAL
- en: For `Actual=0`, we get *10714 / (10714 + 4756) = 69.3%* values correct. This
    is called specificity or the true negative rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `Actual=1`, we get *19649 / (3466** + 19649) = 85.0%* values correct. This
    is called sensitivity or the true positive rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this use case, sensitivity is probably more of a concern than specificity.
    If I were a senior manager, I would be more interested in knowing which customers
    were predicted to return but did not. This group could be sent offers to entice
    them back. Here is how a senior manager might use this model, assuming that the
    model is built to predict whether a person comes in from September 1 to September
    14\. On September 15, we get the preceding confusion matrix. How should a manager
    allocate his/her limited marketing budget?
  prefs: []
  type: TYPE_NORMAL
- en: I can see that I got 4,756 customers who were predicted not to return but actually
    did. This is good, but I cannot really act on this. I can attempt to send offers
    to the 10,135 who did not return, but since my model already predicted that they
    would not return, I would expect the response rate to be low.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 3,870 customers who were predicted to return but did not are more interesting.
    These people should be sent offers to entice them back before their change in
    behavior becomes permanent. This represents only 9.9% of my customer base, so
    by only sending offers to these customers, I am not diluting my budget by sending
    offers to a large contingent of my customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction model should not be used in isolation; other metrics should be
    combined with it to develop a marketing strategy. For example, **customer lifetime
    value** (**CLV**), which measures the expected future revenue for a customer minus
    the cost to re-acquire that customer, could be combined with the prediction model.
    By using a prediction model and CLV together, we can prioritize customers that
    are likely to return by their predicted future value.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize this section, it is all too easy to get obsessed with optimizing
    evaluation metrics, especially if you are new to the field. As a data scientist,
    you should always remember that optimizing evaluation metrics on a machine learning
    task is not the ultimate goal—it is just a proxy for improving some part of the
    business. You must be able to link the results of your machine learning model
    back to a business use case. In some cases, for example, digit recognition in
    the `MNIST` dataset, there is a direct link between your evaluation metrics and
    your business case. But sometimes it is not so obvious, and you need help to work
    with the business in finding out how to use the results of your analysis to maximize
    the benefits to the company.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is about training a model to generalize on the cases it sees
    so that it can make predictions on unseen data. Therefore, the data used to train
    the deep learning model should be similar to the data that the model sees in production.
    However, at an early product stage, you may have little or no data to train a
    model, so what can you do? For example, a mobile app could include a machine learning
    model that predicts the subject of image taken by the mobile camera. When the
    app is being written, there may not be enough data to train the model using a
    deep learning network. One approach would be to augment the dataset with images
    from other sources to train the deep learning network. However, you need to know
    how to manage this and how to deal with the uncertainty it introduces. Another
    approach is transfer learning, which we will cover in [Chapter 11](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml),
    *The Next Level in Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between deep learning and traditional machine learning is
    the size of the datasets. This can affect the ratios used to split data between train/test—the
    recommended guidelines for splitting data into 70/30 or 80/20 splits for machine
    learning need to be revised for training deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Different data distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we used the MNIST dataset for classification tasks. While
    this dataset contains handwritten digits, the data is not representative of real-life
    data. In [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification
    Using Convolutional Neural Networks,* we visualized some of the digits, if you
    go back and look at these images, it is clear that these images are in a standard
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: There are all grayscale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images are all 28 x 28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images all appear to have at border of at least 1 pixel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images are all of the same scale, that is, each image takes up most of the
    image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is very little distortion, since the border is black and the foreground
    is white
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images are the *right way up*, that is, we do not have any major rotations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original use case for the MNIST dataset is to recognize 5 digit postcodes
    on letters. Let''s suppose we train a model on the 60,000 images in the MNIST
    dataset and wish to use it in a production environment to recognize postcodes
    from letters and packages. Here are the steps a production system must go through
    before any deep learning can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan the letters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the postcode section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the postcode digits into 5 different regions (one per digit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In any one of these data transformation steps, additional data bias could occur.
    If we used the *clean* MNIST data to train a model and then tried to predict the
    *biased* transformed data, then our model may not work that well. Examples of
    how bias could affect the production data include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly locating the postcode is a difficult problem in itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The letters will have backgrounds and foregrounds of different colors and contrasts,
    and so converting them to grayscale may not be consistent depending on the type
    of letter and pen used on the letter / package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results from the scanning processes may vary because of different hardware
    and software being used—this is a ongoing problem in applying deep learning to
    medical image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the difficultly in splitting the postcode into 5 different regions
    depends on the letter and pen used, as well as the quality of the preceding steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, the distribution of the data used to train and the estimate
    model's performance is different from the production data. If a data scientist
    had promised to deliver 99% accuracy before the model is deployed, then senior
    managers are very likely to be disappointed when the application runs in production!
    When creating a new model, we split data into train and test splits, so the main
    purpose of the test dataset is to estimate model accuracy. But if the data in
    the test dataset is different to what the model will be see in production, then the
    evaluation metrics on the test dataset cannot give a good guide to how the model
    will perform in production.
  prefs: []
  type: TYPE_NORMAL
- en: If the problem is that there is little or no actual labeled dataset to begin
    with, then one of the first steps to consider before any model training is to
    investigate if more data can be acquired. Acquiring data may involve setting up
    a mini production environment, partnering with a client or using a combination
    of semi-supervised and manual labelling. In the use case we just saw, I would
    consider it more important to set up the process to extract the digitized images
    before looking at any machine learning. Once this is set up, I would look to build
    up some training data—it still may not be enough to build a model, but it could
    be used but a proper test set to create evaluation metrics that would reflect
    real-life performance. This may appear obvious, as over optimistic expectations
    based on flawed evaluation metrics are probably one of the top three problems
    in data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: One example of a very large scale project that managed this problem very well
    is this use case in Airbnb: [https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3](https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3).
    They had a huge number of photos of house interiors, but these were not labeled
    with the room type. They took their existing labeled data and also performed quality
    assurance to check how accurate the labels were. It is often said in data science
    that creating machine learning models may only be 20% of the actual work involved—acquiring
    an accurate large labeled dataset that is representative of what the model will
    see in production is often the hardest task in a deep learning project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a dataset in place, you need to split your data into train and
    test splits before modeling. If you have experience in traditional machine learning,
    you may start with a 70/30 split, that is, 70% for training the model and 30%
    for evaluating the model. However, this rule is less valid in the world of large
    datasets and training deep learning models. Again, the only reason to split data
    into train and test sets is to have a holdout set to estimate the model''s performance.
    Therefore, you only need enough records in this dataset so that the accuracy estimate
    you get is reliable and has the precision you require. If you have a large dataset
    to begin with, then a smaller percentage might be adequate for the test dataset.
    Let me explain this with an example, where you want to improve on an existing
    machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: A prior machine learning model has 99.0% accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a labeled dataset with 1,000,000 records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a new machine learning model is to be trained, then it should get at least
    99.1% accuracy for you to be confident that it is an improvement on the existing
    model. How many records do you need when evaluating the existing model? You only
    need enough records so that you are fairly sure that the accuracy on the new model
    is accurate to 0.1%. Therefore 50,000 records in the test set, which is 5% of
    the dataset, would be sufficient to evaluate your model. If the accuracy on these
    50,000 records was 99.1%, that would be 49,550 records. This represents 50 more
    correctly classified records than the benchmark model, which would strongly suggest
    that the second model is a better model—it would be unlikely that the difference
    would be simply down to chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may get resistance to the suggestion you use only 5% of data for model
    evaluation. However, the idea of splitting data into 70/30 splits goes back to
    the days of small datasets, such as the iris dataset with 150 records. We previously
    saw the following graph in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),* Training
    Deep Prediction Models*, which showed how accuracy on machine learning algorithms
    tends to stagnate as the data size increases. Therefore, there was less of an
    incentive to maximize the amount of data that was available for training. Deep
    learning models can take advantage of more data, so if we can use less data for
    the test set, we should get a better model overall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/272cf949-b665-4241-8b57-34c1419ee8c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: How model accuracy increases by dataset size for deep learning
    models versus other machine learning models'
  prefs: []
  type: TYPE_NORMAL
- en: Data partition between training, test, and validation sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section highlighted the importance of acquiring some data at an
    early stage in the project. But if you do not have enough data to train a deep
    learning model, it is possible to train on other data and apply it to your data.
    For example, you can use a model trained on ImageNet data for image classification
    tasks. In this scenario, you need to use the real data that has been collected
    wisely. This section discusses some good practices on that subject.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever wondered why big companies such as Google, Apple, Facebook,
    Amazon, and so on have such a head start in AI, this is the reason why. While
    they have some of the best AI people in the world working for them, their chief
    advantage is that they have access to tons of *labeled* data that they can use
    to build their machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we said that the sole purpose of a test set is to
    evaluate the model. But if that data is not from the same distribution as the
    data the model will see in prediction tasks, then the evaluation will be misleading.
    One of the most important project priorities should be to acquire labeled data
    that is as similar to real-life data as soon as possible. Once you have that data,
    you need to be clever on how you use this valuable asset. The best use of this
    data, in order of priority, would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Can I use some of this data to create more training data? This could be through
    augmentation, or implementing an early prototype that users can interact with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are building several models (which you should be), use some of the data
    in the validation set to tune the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the data in the test set to evaluate the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the data in the train set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these suggestions may be contentious—especially when suggesting that
    you should use the data for the validation set before the test set. Remember that
    the sole purpose of a test set is that it should only be used once to evaluate
    the model, so you only get one shot at using this data. If I have only a small
    amount of realistic data, then I prefer to use it to tune the model and have a
    less precise evaluation metric than having a poorly performing model with a very
    precise evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is risky, and ideally you want your validation dataset and your
    test dataset to be from the same distribution and be representative of the data
    that the model will see in production. Unfortunately, when you are at the early
    stages in machine learning projects with limited real-life data, then you have
    to make decisions on how best to use this data, and in this case it is better
    to use the limited data in the validation dataset rather than the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important step in data preparation is standardizing data. In the previous
    chapter, for the MNIST data, all pixel values were divided by 255 so that the
    input data was between 0.0 and 1.0\. In our case, we applied min-max normalization,
    which transforms the data linearly using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*xnew = (x-min(x))/(max(x)-min(x))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already know that *min(x) = 0* and *max(x)=255*, this reduces to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*xnew = x / 255.0*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other most popular form of standardization scales the feature so that the
    mean is 0 and the standard deviation from the mean is 1\. This is also known as
    **z-scores**, and the formula for it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*xnew = (x - mean(x)) / std.dev(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three reasons why we need to perform standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: It is especially important to normalize our input features if the features are
    in different scales. A common example often cited in machine learning is predicting
    house prices from the number of bedrooms and the square foot. The number of bedrooms
    ranges from 1 to 10, while the square feet can range from 500 sq feet to 20,000 sq
    feet. Deep learning models expect features to be in the same range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if all of our features are already in the same range, it is still advisable
    to normalize the input features. Recall from [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml), *Deep
    Learning Fundamentals*, that we looked at initializing the weights before model
    training. Any benefit from initializing weights will be cancelled if our features
    are not normalized. We also spoke about the problem of exploding and vanishing
    gradients. When features are on different scales this is more likely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if we avoid both of the preceding problem, if we do not apply normalization,
    the model will take longer to train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the churn model in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, all of the columns were monetary spent, so are already
    on the same scale. When we applied the log to each of these variables, it will
    have shrunk them down to values between -4.6 to 11, so there was no need to scale
    them to values between 0 and 1\. When correctly applied, standardization has no
    negative consequences and so should be one of first steps applied to data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data leakage** is where a feature used to train the model has values that
    could not exist if the model was used in production. It occurs most frequently
    in time series data. For example, in our churn use case in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, there were a number of categorical variables in the data
    that indicated customer segmentation. A data modeler may assume that these are
    good predictor variables, but it is not known how and when these variables were
    set. They could be based on customer'' spend, which means that if they are used
    in the prediction algorithm, there is a circular reference—an external process
    calculates the segment based on the spend and then this variable is used to predict
    spend!'
  prefs: []
  type: TYPE_NORMAL
- en: When extracting data to build a model, you should be wary of categorical attributes
    and question when these variables could have been created and modified. Unfortunately,
    most database systems are poor at tracking the data lineage, so if in doubt you
    may consider omitting the variable from your model.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of data leakage in image classification tasks is when attribute
    information within the image is used in the model. For example, if we build a
    model where the filenames were included as attributes, these names may hint at
    the class name. When the model is used in production, theses hints will not exist,
    so this is also seen as data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: We will see an example of data leakage in practice in the the *Use case—interpretability*
    section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One approach to increasing the accuracy in a model regardless of the amount
    of data you have is to create artificial examples based on existing data. This
    is called **data augmentation**. Data augmentation can also be used at test time
    to improve prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Using data augmentation to increase the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to apply data augmentation to the `MNIST` dataset that we used
    in previous chapters. The code for this section is in `Chapter6/explore.Rmd` if
    you want to follow along. In [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks, *we plotted some examples
    from the MNIST data, so we won''t repeat the code again. It is included in the
    code file, and you can also refer back to the image in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaa6ddab-8aa7-4016-a047-fbe2740489df.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3: The first 9 images in the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We described data augmentation as creating new data from an existing dataset.
    This means creating a new instance that is sufficiently different from the original
    instance but not so much that it no longer represents the data label. For image
    data, this might mean performing the following functions on the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Zooming**: By zooming into the center of the image, your model may be better
    able to handle images at different scales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shifting**: Moving the image up, down, left, or right can make the deep learning
    model more aware of examples of images taken off-center.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotation**: By rotating images, the model will be able to recognize data
    that is off-center.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flipping**: For many objects, flipping the images 90 degrees is valid. For
    example, a picture of a car from the left side can be flipped to show a similar
    image of the car from the right side. A deep model can take advantage of this
    new perspective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding noise**: Sometimes, deliberately adding noise to images can force
    the deep learning model to find deeper meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying color**: By adding filters to the image, you can simulate different
    lighting conditions. For example, you can change an image taken in bright light
    so that it appears to be taken in poor lighting conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of this task is to increase accuracy on the test dataset. However,
    the important rule of data augmentation is that the new data should attempt to
    simulate the data your model will use in production rather than trying to increase
    model accuracy on existing data. I cannot stress that enough. Getting 99% accuracy
    on a hold-out set means nothing if a model fails to work in a production environment
    because the data used to train and evaluate the model was not representative of
    real-life data. In our case, we can see that the MNIST images are grayscale and
    neatly centered, and so on. In a production use case, images are off-center and
    with different backgrounds and foregrounds (for example, with a brown background
    and blue writing), and so will not be classified correctly. You can attempt to
    pre-process the images so that you can format them to a similar manner (28 x 28
    grayscale image with black background and data centered with a 2 x 2 margin),
    but a better solution is to train the model on typical data it will encounter
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the previous image, we can see that most of these data augmentation
    tasks are not applicable to the MNIST data. All of the images appear to be at
    the same zoom level already, so creating artificial examples at increased zoom
    will not help. Similarly, shifting is unlikely to work, since the images are already
    centered. Flipping images is definitely not valid, since most digits are not valid
    when flipped, example *7*. There is no evidence of existing random noise in our
    data, so this will not work either.
  prefs: []
  type: TYPE_NORMAL
- en: 'One technique that we can try is to rotate the images. We will create two new
    artificial images for each existing image, the first artificial image will be
    rotated 15 degrees left and the second artificial image will be rotated 15 degrees
    right. Here are some of the artificial images after we have rotated the original
    images 15 degrees left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9a813a3-c0c5-4e64-a1ec-dce8d29c14be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: MNIST data rotated 15 degrees left'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the the preceding screenshot, one strange anomaly exists. We
    have 10 classes, and using this approach may increase overall accuracy, but one
    class will not get as much uplift. The zero digit is the odd one out because rotating
    a zero still looks like a zero—we may still get an increase in accuracy for this
    class, but probably not as much as for the other classes. The function to rotate 
    image data is in `Chapter6/img_ftns.R`. It uses the `rotateImage` function from
    the `OpenImageR` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There is actually two types of data augmentation that we can apply to our dataset.
    The first type creates new training data from existing examples. But we can also
    use a technique called **test time augmentation** (**TTA**), which can be used
    during model evaluation. It makes copies of each test row and then uses these
    copies and the originals to vote for the category. We will see an example of this
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to create datasets for the data augmentation is in `Chapter6/augment.R`.
    Note that this takes a long time to run, maybe 6-10 hours depending on your machine.
    It also needs approx. 300 MB of free space on the drive to create the new datasets.
    The code is not difficult; it loads in the data, and splits it into train and
    test sets. For the train data, it creates two new instances: one rotated 15 degrees
    left and one rotated 15 degrees right. It is important that the data used to evaluate
    the model performance is not included in the data augmentation process, that is,
    split the data into a train dataset first and only apply data augmentation to
    the train split.'
  prefs: []
  type: TYPE_NORMAL
- en: When the data augmentation is complete, there will be a new file in the data
    folder called `train_augment.csv`. This file should have 113,400 rows. Our original
    dataset for `MNIST` had 42,000 rows; we took 10% of that for test purposes (that
    is, to validate our model) and were left with 37,800 rows. We then made two copies
    of these rows, meaning that we now have 3 rows for each previous row. This means
    that we have *37,800 x 3 = **113,400* rows in our training data file. `augment.R`
    also outputs the test data (4,200 rows) as `test0.csv` and an augmented test set
    (`test_augment.csv`), which we will cover later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to run the neural network is in `Chapter6/mnist.Rmd`. The first part
    which uses the augmented data for training is almost identical to the code in
    [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification
    Using Convolutional Neural Networks*. The only change is that it loads the data
    files created in `augment.R` (`train_augment.csv` and `test0.csv`), so we we will
    not repeat all of the code for the model here again. Here is the confusion matrix
    and the final accuracy on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This compares to an accuracy of `0.9821429` from our model in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*, so this is a significant
    improvement. We have reduced our error rate by over 30% *(0.9885714-0.9835714**)
    / (1.0-0.9835714)*.
  prefs: []
  type: TYPE_NORMAL
- en: Test time augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also use data augmentation during test time. In the `augment.R` file,
    it created a file with the original test set of 4,200 rows (`data/test0.csv`),
    which was used that to evaluate the model. The `augment.R` file also created a
    file called `test_augment.csv`, which has the original 4,200 rows and 2 copies
    for each image. The copies are similar to what we did to augment the training
    data, that is, a row with data rotated 15 degrees left and a row with data rotated
    15 degrees right. The three rows are outputted sequentially and we will use these
    3 rows to *vote* for the winner. We need to take 3 records at a time from `test_augment.csv`
    and calculate the prediction value as the average of these three values. Here
    is the code that performs test time augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Doing this, we get predictions for 12,600 rows (*4,200 x 3*). The for loop
    runs through 4,200 times and takes 3 records at a time, calculating the average
    accuracy. The increase in accuracy over the accuracy using augmented training
    data is small, from `0.9885714` to `0.9895238`, which is approx. 0.1% (4 rows).
    We can look at the effect of TTA in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This table shows the 9 rows where the test time augmentation was correct and
    the previous model was wrong. We can see three cases where the previous model
    (`pred.model`) predicted `9` and the test time augmentation model correctly predicted
    `4`. Although test time augmentation did not significantly increase our accuracy
    in this case, it can make a difference in other computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using data augmentation in deep learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We implemented data augmentation using R packages and it took a long time to
    generate our augmented data. It was useful for demonstration purposes, but MXNet
    and Keras support data augmentation functions. In MXNet, there are a range of
    functions in `mx.image.*` to do this ([https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html](https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html)).
    In Keras, this is in `keras.preprocessing.*` ([https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)),
    which applies these automatically to your models. In [Chapter 11](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml),
    *The Next Level in Deep Learning*, we show how to apply data augmentation using
    Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All machine learning algorithms have hyper-parameters or settings that can change
    how they operate. These hyper-parameters can improve the accuracy of a model or
    reduce the training time. We have seen some of these hyper-parameters in previous
    chapters, particularly [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),
    *Deep Learning Fundamentals, *where we looked at the hyper-parameters that can
    be set in the `mx.model.FeedForward.create` function. The techniques in this section
    can help us find better values for the hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting hyper-parameters is not a magic bullet; if the raw data quality is
    poor or if there is not enough data to support training, then tuning hyper-parameters
    will only get you so far. In these cases, either acquiring additional variables/features
    that can be used as predictors and/or additional cases may be required.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on tuning hyper-parameters, see Bengio, Y. (2012), particularly
    Section 3, *Hyperparameters*, which discusses the selection and characteristics
    of various hyper-parameters. Aside from manual trial and error, two other approaches
    for improving hyper-parameters are grid searches and random searches. In a grid
    search, several values for hyper-parameters are specified and all possible combinations
    are tried. This is perhaps easiest to see. In R, we can use the `expand.grid()`
    function to create all possible combinations of variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Grid searching is effective when there are only a few values for a few hyper-parameters.
    However, when there are many values for some or many hyper-parameters, it quickly
    becomes unfeasible. For example, even with only two values for each of eight hyper-parameters,
    there are *2⁸ = 256* combinations, which quickly becomes computationally impracticable.
    Also, if the interactions between hyper-parameters and model performance are small,
    then using grid search is an inefficient approach.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative approach to hyper-parameter selection is searching through random
    sampling. Rather than pre­-specifying all of the values to try and create all
    possible combinations, one can randomly sample values for the parameters, fit
    a model, store the results, and repeat. To get a very large sample size, this
    too would be computationally demanding, but you can specify just how many different
    models you are willing to run. Therefore this approach gives you a spread over
    the combination of hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For random sampling, all that need to be specified are values to randomly sample,
    or distributions to randomly draw from. Typically, some limits would also be set.
    For example, although a model could theoretically have any integer number of layers,
    some reasonable number (such as 1 to 10) is used rather than sampling integers
    from 1 to a billion.
  prefs: []
  type: TYPE_NORMAL
- en: To perform random sampling, we will write a function that takes a seed and then
    randomly samples a number of hyper-parameters, stores the sampled parameters,
    runs the model, and returns the results. Even though we are doing a random search
    to try and find better values, we are not sampling from every possible hyper-parameter.
    Many remain fixed at values we specify or their defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some hyper-parameters, specifying how to randomly sample values can take
    a bit of work. For example, when using dropout for regularization, it is common
    to have a relatively smaller amount of dropout for early hidden layers (0%-20%)
    and a higher amount for later hidden layers (50-80%). Choosing the right distributions
    allows us to encode this prior information into our random search. The following
    code plots the density of two beta distributions, and the results are shown in
    *Figure 6.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By sampling from these distributions, we can ensure that our search focuses
    on small proportions of dropout for the early hidden layers, and in the **0**
    to **0.50** range for the hidden neurons with a tendency to over­sample from values closer
    to **0.50**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dccb2eb7-acbb-40e0-885d-a59a081ad55e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Using the beta distribution to select hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: Use case—using LIME for interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning models are known to be difficult to interpret. Some approaches
    to model interpretability, including LIME, allow us to gain some insights into
    how the model came to its conclusions. Before we demonstrate LIME, I will show
    how different data distributions and / or data leakage can cause problems when
    building deep learning models. We will reuse the deep learning churn model from
    [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training Deep Prediction
    Models*, but we are going to make one change to the data. We are going to introduce
    a bad variable that is highly correlated to the *y* value. We will only include
    this variable in the data used to train and evaluate the model. A separate test
    set from the original data will be kept to represent the data the model will see
    in production, this will not have the bad variable in it. The creation of this
    bad variable could simulate two possible scenarios we spoke about earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Different data distributions**: The bad variable does exist in the data that
    the model sees in production, but it has a different distribution which means
    the model does not perform as expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data leakage**: Our bad variable is used to train and evaluate the model,
    but when the model is used in production, this variable is not available, so we
    assign it a zero value, which also means the model does not perform as expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this example is in `Chapter6/binary_predict_lime.R`. We will not
    cover the deep learning model in depth again, so go back to [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, if you need a refresher on how it works. We are going
    to make two changes to the model code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will split the data into three parts: a train, validate, and test set. The
    train split is used to train the model, the validate set is used to evaluate the
    model when it is trained, and the test set represents the data that the model
    sees in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will create the `bad_var` variable, and include it in the train and validation set,
    but not in the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code to split the data and create the `bad_var` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new variable is highly correlated with our `y` variable at `0.958`. We
    also created a bar plot of the most highly correlated features to the `y` variable,
    and we can see that correlation between this new variable and the `y` variable is
    much higher than correlation between the other variables and the `y` variable.
    If a feature is very highly correlated to the `y` variable, then this is usually
    a sign that something is wrong in the data preparation. It also indicates that
    a machine learning solution is not required because a simple mathematical formula
    will be able to predict the outcome variable. For a real project, this variable
    should not be included in the model. Here is the graph with the features that
    are most highly correlated with the `y` variable, the correlation of the `bad_var`
    variable is over `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c51ab854-c930-482d-948e-83aa0e701d6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: The top 10 correlations from feature to target variable'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go ahead and build the model, notice how we set this new feature
    to zero for the test set. Our test set in this example actually represents the
    data that the model will see when it is production, so we set it to zero to represent
    either a different data distribution or a data leakage problem. Here is the code
    that shows how the model performs on the validation set and on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The validation set here represents the data used to evaluate the model when
    it is being built, while the test set represents the future production data. The
    accuracy on the validation set is over 90%, but the accuracy on the test set is
    less than 70%. This shows how different data distributions and/or data leakage
    problems can cause over-estimations of model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability with LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LIME **stands for **Local Interpretable Model-Agnostic Explanations**. LIME
    can explain the predictions of any machine learning classifier, not just deep
    learning models. It works by making small changes to the input for each instance
    and trying to map the local decision boundary for that instance. By doing so,
    it can see which variable has the most influence for that instance. It is explained
    in the following paper: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin*.
    Why should I trust you?: Explaining the predictions of any classifier. Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining. ACM, 2016*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at using LIME to analyze the model from the previous section. We
    have to set up some boilerplate code to interface the MXNet and LIME structures,
    and then we can create LIME objects based on our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then can pass in the first 10 records in the test set and create a plot
    to show feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following plot, which shows the features that were most
    influential in the model predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6bbe7d7-bf00-4481-ba69-1aa00dfaf64d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7: Feature importance using LIME
  prefs: []
  type: TYPE_NORMAL
- en: 'Note how, in each case, the `bad_var` variable is the most important variable
    and its scale is much larger than the other features. This matches what we saw
    in *Figure 6.6*. The following graph shows the heatmap visualization for feature
    combinations for the 10 test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/765662ae-1728-4f81-8d55-1f1cb1634153.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8: Feature heatmap using LIME
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how to apply LIME to an existing deep learning model trained
    with MXNet to visualize which features were the most important for some of the
    predictions using the model. We can see in Figures 6.7 and 6.8 that a single feature
    was almost completely responsible for predicting the `y` variable, which is an
    indication that there is an issue with different data distributions and/or data
    leakage problems. In practice, such a variable should be excluded from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a comparison, if we train a model without this field, and plot the feature
    importance again, we see that one feature does not dominate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a3b160d-5582-4943-81b7-ceafba51f828.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9: Feature importance using LIME (without the `bad_var` feature)
  prefs: []
  type: TYPE_NORMAL
- en: 'There is not one feature that is a number 1 feature, the explanation fit is
    0.05 compared to 0.18 in *Figure 6.7,* and the significance bars for the three
    variables are on a similar scale. The following graph shows the feature heatmap
    using LIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b25a6e33-f61c-4d77-95cb-78c0b82f0ad1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10: Feature heatmap using LIME (without the `bad_var` feature)
  prefs: []
  type: TYPE_NORMAL
- en: Again, this plot shows us that more than one feature is being used. We can see
    that the scale of legend for the feature weights in the preceding graph is from
    0.01 - 0.02\. In *Figure 6.8*, the scale of legend for the feature weights was
    -0.2 - 0.2, indicating that some features (just one, actually) are dominating
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered topics that are critical to success in deep learning projects.
    These included the different types of evaluation metric that can be used to evaluate
    the model. We looked at some issues that can come up in data preparation, including
    if you only have a small amount of data to train on and how to create different
    splits in the data, that is, how to create proper train, test, and validation
    datasets. We looked at two important issues that can cause the model to perform
    poorly in production, different data distributions, and data leakage. We saw how
    data augmentation can be used to improve an existing model by creating artificial
    data and looked at tuning hyperparameters in order to improve the performance
    of a deep learning model. We closed the chapter by examining a use case where
    we simulated a problem with different data distributions/data leakage and used
    LIME to interpret an existing deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the concepts in this chapter may seem somewhat theoretical; however
    they are absolutely critical to the success of machine learning projects! Many
    books cover this material toward the end, but it is included in this book at a
    relatively early stage to signify its importance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to look at using deep learning for **Natural
    Language Processing** (**NLP**), or text data. Using deep learning for text data
    is more efficient, simpler, and often outperforms traditional NLP approaches.
  prefs: []
  type: TYPE_NORMAL
