- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diffusion Models and AI Art
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In prior chapters, we’ve looked at examples of how generative models can be
    used to create novel images; we’ve also seen how language models can be used to
    author answers to questions or create entirely new creative text like poems. In
    this chapter, we bring together these two concepts by showing how user prompts
    can be translated into images, allowing you to author “AI art” using natural language.
    In addition to creating novel images, we can perform some useful functions like
    extending an image beyond its current boundaries (“outfilling”) and defining features
    for safety screening in our results. We’ll also look at one of the foundational
    ideas underlying this image generation methodology, the *diffusion model*, which
    uses the concept of heat transfer to represent how an input of random numbers
    is “decoded” into an image. To illustrate these ideas, we’ll primarily work with
    *Stable Diffusion*, an open-source generative model, but similar concepts apply
    to closed-source models such as *Midjourney* and *DALL-E.* Topics we’ll cover
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: How diffusion models relate to other kinds of image-generating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the Stable Diffusion model combines **Variational Autoencoders** (**VAEs**)
    and diffusion models to create extremely efficient image sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some examples of using the Stable Diffusion model in the Hugging Face pipelines
    library, where we:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate key parameters that impact the output of the image generation task
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Walk through how the pieces of the Hugging Face pipeline implement each step
    of the image generation task to create a picture from a user prompt:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing the user prompt as a byte string
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding the byte string prompt as a vector
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating random number vectors
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the encoded prompt and random input to run multiple denoising steps to
    generate a compressed form of the new image
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncompressing the new image using the decoder arm of a VAE
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A walk through image generation: Why we need diffusion models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Diffusion models are among the latest and most popular methods for image generation,
    particularly based on user-provided natural language prompts. The conceptual challenge
    of this class of image generation model is to create a method that is:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalable to train and execute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to generate a diversity of images, including with user-guided prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to generate natural-looking images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has stable training behavior that is possible to replicate easily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One approach to this problem is “autoregressive” models, where the image is
    generated pixel by pixel, using the prior-generated pixels as successive inputs1\.
    The inputs to these models could be both a set of image pixels and natural language
    instructions from the user that are encoded into an embedding vector. This approach
    is slow, as it makes each pixel dependent upon prior steps in the model output.
    As we’ve seen in prior chapters, **Generative Adversarial Networks** (**GANs**)
    can also be used to synthesize images, but they have unstable training behavior
    that is tricky to replicate and have a tendency to get stuck in local “modes,”
    rather than generating a broader distribution of natural images². As we saw with
    VAEs in [*Chapter 11*](Chapter_11.xhtml), the objective function based on pixel-wise
    approximation may not create the most realistic images. Recently, *diffusion models*
    have arisen as a promising alternative. What are they, and how do they solve some
    of the challenges we’ve mentioned?
  prefs: []
  type: TYPE_NORMAL
- en: 'Pictures from noise: Using diffusion to model natural image variability'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core idea of diffusion models is that we can represent images as a set of
    pixels, which are like a cloud in high-dimensional space. That cloud is highly
    structured, representing colored patches and objects. If we add noise – such as
    random normal noise – to that structure, it becomes a spherical cloud. However,
    if we had a recipe for how to reverse that “blurring” of the image, we could create
    new images from a set of random points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how to write this out mathematically. We start with our “forward
    process,” which takes input data, such as an image, ![](img/B22333_15_001.png),
    and applies stepwise noise to turn it into a vector of random normals. We will
    label this forward “blurring” process ![](img/B22333_11_022.png)*,* and we can
    represent it as a *Markov* process where each step depends only on the prior step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_003.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the image at the end composed of random pixels is created by
    repetitively applying a function ![](img/B22333_11_022.png)to step ![](img/B22333_15_005.png),
    dependent on the prior value of ![](img/B22333_11_001.png). This function ![](img/B22333_11_022.png)
    defines a transition process that follows a Gaussian distribution parameterized
    by ![](img/B22333_15_008.png), which controls the variance³. The value of ![](img/B22333_15_008.png)
    determines the level of noise applied at each step – smaller values (low ![](img/B22333_15_008.png))
    result in a gradual increase in noise, while larger values (high ![](img/B22333_15_008.png))
    accelerate the transition, causing the image to degrade into a noisy set of random
    pixels more quickly. Once we’ve applied this “blurring” transformation enough
    times, the data will be in a distribution such as a random normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we now want to recover an image from this blurred cloud? We just apply
    a “reverse” transformation, ![](img/B22333_11_021.png), using a similar formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_013.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that ![](img/B22333_11_021.png) and ![](img/B22333_11_022.png) are
    reverses of each other, but that ![](img/B22333_11_021.png) also represents a
    recipe for taking random data and generating images from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1: The diffusion process for noising and denoising images4](img/B22333_15_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: The diffusion process for noising and denoising images⁴'
  prefs: []
  type: TYPE_NORMAL
- en: This design seems promising conceptually, but it’s not clear how we would guarantee
    that ![](img/B22333_11_021.png) and ![](img/B22333_11_022.png) are sufficiently
    close that they would result in high-quality samples when we apply them. In other
    words, we need a method to optimize the parameters of ![](img/B22333_11_021.png)
    and ![](img/B22333_11_022.png) so that they are tuned to generate high-quality
    reconstructions of an input image once it has been blurred and recovered through
    ![](img/B22333_11_021.png). It’s perhaps not surprising, given the familiar ![](img/B22333_11_021.png)
    and ![](img/B22333_11_022.png) distributions from our discussion of VAEs in [*Chapter
    11*](Chapter_11.xhtml), that this problem can be solved through variational inference⁴.
    Let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: Using variational inference to generate high-quality diffusion models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that the **Evidence Lower Bound** (**ELBO**) gives an expression for
    the log-likelihood of a difficult-to-calculate distribution p in terms of an approximating,
    easy-to-calculate distribution ![](img/B22333_11_022.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of directly maximizing the likelihood of ![](img/B22333_11_021.png),
    we can maximize the right-hand side, which is a lower bound on the likelihood
    of ![](img/B22333_11_021.png), in terms of the divergence with an approximating
    distribution ![](img/B22333_11_022.png). For convenience purposes, we often minimize
    the negative log-likelihood (as many computational packages take the minima of
    a function), which gives the following equation for the diffusion model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_029.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that this equation can be evaluated at multiple steps t in the noising/denoising
    process. We can write this out more explicitly as a loss function with beginning,
    intermediate, and final values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, DKL is the Kullback–Leibler divergence, as we saw in [*Chapter 11*](Chapter_11.xhtml).
    Recall that the forward noising process *q* has a variance ![](img/B22333_15_031.png).
    We could try to learn this value, but as it’s often small, we can treat it as
    fixed. Thus, the last term in this equation, at time *T*, drops out since it is
    a constant. What about the values from *t*=1 to *T-*1? We already described that
    *q* doesn’t have learnable parameters, so we are interested in learning the parameters
    of *p*, the reverse process that converts random noise into an image. In the expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will typically keep the variance as a fixed value, so we just need to learn
    a function to predict the mean – and that function could be a neural network that
    takes the input pixels at a given step and outputs a slightly less noisy image.
    However, we can reparameterize this equation to make it easier to optimize. Using
    the normal distribution, we can write this intermediate likelihood ![](img/B22333_15_033.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'C is a constant and falls out of the minimization. We can calculate the value
    of the mean at a given point in time using the average variance per timestep.
    Let:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, at each timestep, *x* can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we’ll optimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This expression shows how the function predicting the mean of *x* can be represented
    as an equation in which the unknown is a function predicting the noise *e* as
    a function of *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This finally leads us to the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_040.png)'
  prefs: []
  type: TYPE_IMG
- en: Given fixed values for ![](img/B22333_15_041.png), ![](img/B22333_15_042.png),
    and ![](img/B22333_15_043.png), and input data ![](img/B22333_11_037.png), we
    are optimizing a function to predict the noise we should subtract at each step
    of the reverse process ![](img/B22333_11_032.png) to obtain an image ![](img/B22333_11_032.png)
    from a sample of random noise. Like ![](img/B22333_15_047.png), that ![](img/B22333_15_048.png)
    will be a neural network, and that is what we will see implemented in the Stable
    Diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the term L[o] in the diffusion equation on the previous page (i.e., ![](img/B22333_15_049.png)),
    in practice, it has not been found to be needed to train a probabilistic diffusion
    function, so it is dropped. We can make one more improvement; if the sample already
    has low noise (after we’ve run the reverse process for many steps), we can down-weight
    subsequent samples when we subtract the model-predicted noise. We do this by incorporating
    the simulation step t explicitly as a term in our noise-predicting neural network
    *e*, and drop the multipliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22333_15_050.png)'
  prefs: []
  type: TYPE_IMG
- en: As we’ll see later, how we execute e at each step of the simulation to remove
    noise successively from a random image is an important design choice in diffusion
    models, known as the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we have one last challenge to resolve; we can optimize the likelihood
    function above efficiently, but the actual generation step will be costly since
    we could be working with large images, and the size of *x* remains fixed throughout
    the simulation steps. This is where Stable Diffusion comes in: it leverages the
    VAE models we saw in [*Chapter 11*](Chapter_11.xhtml)to perform the forward and
    reverse processes we describe above in a latent space that is much smaller than
    the original image, meaning it is considerably faster for training and inference.
    Let’s take a look.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stable Diffusion: Generating images in latent space'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we described, the major insight for the Stable Diffusion model was that instead
    of performing the forward process *q* and reverse process *p* that we’ve trained
    through variation inference in the image space, we do so using a VAE to compress
    the images, making the calculation much faster than the slower diffusion calculation
    that can be executed in the original pixel space; this process is shown in *Figure
    15**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2: The Stable Diffusion model5](img/B22333_15_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: The Stable Diffusion model⁵'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through some of the elements of this workflow. On the far right,
    the input image *x* is “blurred” using a VAE into a latent vector *z*. Thus, the
    forward step *q* is executed through one pass through the VAE! Then, we can incorporate
    “conditioning information” (such as a textual prompt from the user) using an embedding
    method on the far right. Now, to run the “reverse process,” *p*, we execute a
    time-varying U-Net⁶ to predict the noise, e, that we should remove from a random
    image at each time step. The U-Net (*Figure 15**.2*) is made up of a number of
    transformer layers, which compress the latent vector z generated by the VAE (or
    sampled randomly) into a smaller length before expanding it, in order to enhance
    the salient features of the latent vector. The “U” in the name comes from the
    fact that if the layers are arranged visually with the largest, outermost layers
    at the top and the innermost, narrowest layers at the bottom of a graph of the
    network, it resembles the letter U. Due to this architecture, the U-Net is well
    suited to extract features/details in images (through the forward, encoding path)
    that are then labeled/highlighted at a pixel level (through the reverse, decoding
    path that expands the image to its original dimensions). In our example, where
    we use latent vectors instead of the original image, each pass of the latent vector
    through this U-Net represents one “step” of the denoising process ![](img/B22333_11_021.png).
    You’ll also notice we’ve added residual connections in this U-Net to enable the
    efficient flow of information through the network. We then decode the “denoised”
    latent vector with the VAE in reverse.
  prefs: []
  type: TYPE_NORMAL
- en: In the training phase of this model, we would take pairs of images and prompts
    describing them, embed them in the model, and optimize the lower bound given above.
    If we are not training the model, we don’t even need to run the VAE forward to
    create random vectors; we just sample them. Now that we’ve seen how Stable Diffusion
    is set up, and the details of how it evolved from earlier ideas for image generation,
    let’s see how to put it into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Running Stable Diffusion in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, let’s quickly set up our own instance of the Stable Diffusion model
    in Python code and run an example. For this purpose, we’ll be using Google Colab
    ([https://colab.research.google.com/](https://colab.research.google.com/)), a
    cloud environment that allows you to utilize high-performance **Graphics Processing
    Unit** (**GPU**) computing and large memory resources from your laptop. Colab
    is free, but you can also pay for higher-availability resources if you desire.
    The interface resembles the Python Jupyter notebooks ([https://jupyter.org/](https://jupyter.org/))
    that you’ve likely used in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Installing dependencies and running an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve set up your Colab account, you just need to install the diffusers
    package and a few dependencies. Diffusers is a library created by the company
    Hugging Face ([https://huggingface.co/docs/diffusers/index](https://huggingface.co/docs/diffusers/index))
    that provides easy access to a set of state-of-the-art diffusion models (including
    Stable Diffusion). It utilizes the pipelines API, also developed by Hugging Face,
    which abstracts many of the complexities of these models to a simple interface.
    *Figure 15**.3* demonstrates the commands you would provide in a Colab notebook
    to install diffusers and its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3: Dependencies for diffusers](img/B22333_15_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Dependencies for diffusers'
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you’ll want to make sure you have a GPU-enabled runtime, which
    you can choose by going to *Runtime* and then *Change runtime type* on the top
    ribbon on the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4: Runtime for the diffusers example](img/B22333_15_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: Runtime for the diffusers example'
  prefs: []
  type: TYPE_NORMAL
- en: From there, we’ll initialize the Stable Diffusion 1.4 model using a series of
    simple commands. First, we’ll load the model, then initialize a pipeline on the
    GPU on our runtime. Then we merely need to supply a text prompt to the pipeline;
    the model will be run interactively and we can display the result directly in
    the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll use an example from the Stable Diffusion paper⁵. The user
    prompt is “a zombie in the style of Picasso,” and the Stable Diffusion model translates
    this prompt into an image representing an undead monster in the abstract, cubist
    style of the famous 20th-century artist Pablo Picasso:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5: An example output using the Picasso zombie prompt](img/B22333_15_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: An example output using the Picasso zombie prompt'
  prefs: []
  type: TYPE_NORMAL
- en: However, remember that this is not a deterministic output like a typical machine
    learning prediction, where we get the same output for a given input. Rather, we’re
    sampling from possible model outputs from a distribution, so we’re not limited
    to generating a single output. Indeed, if we modify the `num_images_per_prompt`
    parameter, we can generate a set of images all from the same prompt by printing
    each element of the `images` list.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.6: Generating alternative images from the zombie prompt](img/B22333_15_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Generating alternative images from the zombie prompt'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at a basic example, let’s modify some of the parameters
    to see how they impact the output.
  prefs: []
  type: TYPE_NORMAL
- en: Key parameters for Stable Diffusion text-to-image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides generating multiple images, what other parameters could we modify in
    this example? One would be to remove the prompt (provide a blank prompt) and see
    what output we would get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.7: Running Stable Diffusion with a blank prompt](img/B22333_15_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Running Stable Diffusion with a blank prompt'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, as you can see in *Figure 15**.7,* the result is a set of seemingly
    random images, but not blank images or completely random noise. The reason for
    this can be explained by one of the components of the pipeline, the VAE we covered
    in [*Chapter 11*](Chapter_11.xhtml)and the data used to develop it, as we’ll see
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also modify how much importance we give to the prompt in generating
    images, using the *guidance scale* parameter. As we saw in our overview of the
    Stable Diffusion model, we can think of the image generation step as modeling
    the pixels in the image as particles that drift in a multi-dimensional space.
    The motion of those particles can either be pushed in a particular direction in
    correlation with the input prompt from the user or move randomly according to
    their initial configuration. The default for the guidance scale is 7.5 – let’s
    see what happens if we change it to alternative values between 0 and 10:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figu\uFEFFre 15.8: Modifying the guidance scale from 0 to 10](img/B22333_15_08.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: Modifying the guidance scale from 0 to 10'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that as the guidance scale increases from 0 to 10, the generated
    image more clearly resembles the prompt. The image at 0 looks very much like the
    output from the blank prompt examples in *Figure 15**.6* – indeed, under this
    setting, the model is using a blank input. At 0, the model will pay no attention
    to the prompt, as we’ll see later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The impact of this parameter is perhaps more notable when using a more complex
    prompt, such as the one we used in the last chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A zombie in the style of Monet. The zombie is dressed in a farmer’s outfit and
    holds a paintbrush and canvas. The sun is setting, and there are mountains in
    the distance. The hay in the field in which the zombie is standing comes up to
    its waist. There are red flowers in the field
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Figure 15**.9* shows a comparison of applying a guidance scale of 0 to 10;
    you can see in the final image that the zombie figure begins to appear.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.9: Image generated with guidance scales 0, 7.5, and 10 using a
    complex prompt](img/B22333_15_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: Image generated with guidance scales 0, 7.5, and 10 using a complex
    prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the guidance scale, the number of diffusion steps in the image
    generation process also impacts how crisp the output is. As we’ve seen, the image
    generation by the model can be represented by pixels behaving as particles moving
    in space. The longer they are able to move, the farther they can transition from
    an initial, random arrangement to a new configuration that resembles an image.
    The default in this pipeline is 50 steps: let’s see what happens if we modify
    that to `1`, `3`, and `10` in *Figure 15**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.\uFEFF10: Images generated with 1, 3, and 10 simulation steps](img/B22333_15_10.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: Images generated with 1, 3, and 10 simulation steps'
  prefs: []
  type: TYPE_NORMAL
- en: As the number of simulation steps increases, the generated image goes from blurry
    to resembling our initial examples – at 3 steps, we see output that resembles
    our prompt but without the simplified cubist lines that become clearer with more
    simulation steps. We’ll see later in this chapter how each simulation attempts
    to subtract “noise” from the image, and thus makes it more clear as more steps
    are run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way we can modify the input is by introducing “negative” prompts, which
    cancel out part of the initial prompt. Let’s see how this works by providing `zombie`,
    `Picasso`, or `Cubist` as the negative prompt in *Figure 15**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.11: \uFEFFImage generated with negative prompts](img/B22333_15_11.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Image generated with negative prompts'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that if we provide elements of the prompt (`zombie` or `Picasso`),
    we can cancel out either the subject matter or the style of the image. We don’t
    even need to use the exact words; as you can see using `Cubist` (a term closely
    associated with the art style of Picasso) produces a similar output to a negative
    prompt using the artist’s name explicitly. This is because of how the prompts
    are encoded as numerical vectors when they are passed to the model, which allows
    the model to compare the similarity of terms, as we’ll see later when we discuss
    the embedding step.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to modifying the content of the image, we can also easily change
    its size, as you can see in *Figure 15**.12.*
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.12: Imag\uFEFFe generated with varying dimensions](img/B22333_15_12.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: Image generated with varying dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the resulting image is easily changed by modifying the size of the
    ultimate decoder layer in the last step of the pipeline, as we’ll see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the risks of generating images in an application is that the output
    could be offensive; fortunately, the pipeline in this example has a built-in feature,
    a safety checker, to screen such potentially inappropriate content. We can see
    the effect of this by modifying the prompt (*Figure 15**.13*):'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.13: Image\uFEFF generated with a toxic/offensive prompt](img/B22333_15_13.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: Image generated with a toxic/offensive prompt'
  prefs: []
  type: TYPE_NORMAL
- en: The safety checker is a model that classifies features of the produced image
    as **Not Safe for Work** (**NSFW**) and blocks them. The features it uses to produce
    this classification are quite similar to the embeddings used to feed the prompt
    into the model to generate the image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen numerous ways that we can tweak the output of the model
    through various parameters, let’s explore how each of these parameters appears
    step by step as we walk through each of the components underlying the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into the text-to-image pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we produced all the examples by providing the prompt
    and various arguments to the pipeline directly. The pipeline consists of several
    components that act in sequence to produce images from your prompt. These components
    are contained in a Python dictionary that is part of the `Pipeline` class, and
    so, like any Python dictionary, you can print the key names of the fields to inspect
    the components (*Figure 15**.14*).
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.14: Co\uFEFFmponents of the Stable Diffusion pipeline](img/B22333_15_14.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: Components of the Stable Diffusion pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen each of these in action in the prior examples, as will become clearer
    as we walk through the execution of each:'
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer takes our prompt and turns it into a byte representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text encoder takes that byte representation and turns it into a numerical
    vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The U-Net, which takes a vector of random numbers and the encoded prompt and
    merges them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduler, which runs diffusion steps to “denoise” this merged vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VAE, which converts the merged vector into one or more generated images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature extractor, which extracts elements from the generated image that
    might be labeled as offensive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The safety checker, which scores those extracted elements to see whether the
    image might be censored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s step through each component and see how the parameters we looked at earlier
    come into play in the execution.
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step in this pipeline is to convert the prompt into a set of *tokens*,
    or individual elements to be passed into the textual embedding step. You can access
    a lot of information about the tokenizer by printing this pipeline component to
    the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.15: T\uFEFFhe tokenizer properties](img/B22333_15_15.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: The tokenizer properties'
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion uses a **Contrastive Language Image Processing** (**CLIP**)
    model to compute embeddings, which is trained on a joint dataset of images and
    their captions². The tokenizer provides the raw input to compute the textual vectors
    used in the image generation process. You may have encountered tokenization in
    the past in one-hot encoding for natural language processing, in which a word
    (or character) is indexed by a number (for example, each letter in the English
    alphabet can be indexed with the number 0 to 25). Stable Diffusion and similar
    state-of-the-art models use a more efficient embedding than simply mapping each
    word to an index – instead, they map the text to bytes (using an encoding such
    as UTF-8) and represent commonly occurring byte pairs as a single byte, a technique
    called **Byte Pair Encoding** (**BPE**)⁸.
  prefs: []
  type: TYPE_NORMAL
- en: 'BPE is based on the idea that we can compress strings by looking for common
    recurring patterns. Let’s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: abcabcabde
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first pass, we notice that the most commonly occurring pair of characters
    is *ab*; we can convert this to a new character, *f*:'
  prefs: []
  type: TYPE_NORMAL
- en: fcfcfde
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, *fc* is the most commonly occurring pair. Convert this to *g*:'
  prefs: []
  type: TYPE_NORMAL
- en: ggfde
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, convert *gg* to *h*:'
  prefs: []
  type: TYPE_NORMAL
- en: hfde
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now compressed the input string from 10 characters to 4, which is much
    more efficient to work with computationally. If we need to recover the original
    string, we just to store a lookup table of the pairs and their corresponding character
    to reverse this operation, which we can run recursively.
  prefs: []
  type: TYPE_NORMAL
- en: One additional detail is that while this example used characters, in practice
    we use bytes. This is because special characters like emojis would break a fixed-vocabulary
    character pair compressor since the special characters might not be in the lookup
    table, but all text can be represented uniformly as bytes, making it more robust.
  prefs: []
  type: TYPE_NORMAL
- en: So, to summarize, the tokenizer converts the words in the prompt into bytes
    and uses a pre-computed lookup table of frequently occurring byte pairs to index
    those bytes with a set of IDs. You can see this in action by running just the
    `tokenizer` on the input prompt, as shown in *Figure 15**.16:*
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.16:\uFEFF Converting the prompt to byte token IDs](img/B22333_15_16.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: Converting the prompt to byte token IDs'
  prefs: []
  type: TYPE_NORMAL
- en: You can access the encoding map that Stable Diffusion’s encoder uses through
    the `encoder` property and verify that “320” corresponds to the pair of bytes
    for the letter “a” and whitespace. Similarly, “49406” is a placeholder character
    representing the start of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.17: T\uFEFFhe tokenizer encoding map](img/B22333_15_17_A.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: The tokenizer encoding map'
  prefs: []
  type: TYPE_NORMAL
- en: Generating text embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step in the pipeline is to transfer the byte-indexed prompt into numerical
    vectors that can be used as inputs to the image generation step of the model.
    This embedding is performed by the CLIP neural network, whose properties you can
    examine in the notebook, as shown in *Figure 15**.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.18: T\uFEFFhe embedding model](img/B22333_15_18.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: The embedding model'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the tokenizer, which was a lookup table, this component is a neural network
    that produces embedding vectors of size 768\. You can see that the layers in this
    network are a stack of 12 transformer modules, followed by a final layer of normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we execute this model on the output from our prior step (cast as a tensor,
    the input type needed for the embedding model, and sent to the GPU with the `to`
    command), we’ll get an output of size 768 (for each token) representing the embedded
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.19: G\uFEFFenerating the embedding from the prompt](img/B22333_15_19.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.19: Generating the embedding from the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dissect what is happening in the code block in *Figure 15**.19.* The prompt
    (`"a zombie in the style of Picasso"`) is first passed to the tokenizer in the
    pipeline, with a maximum length of 77 (the maximum number of embeddable tokens).
    As we saw above, this function will return a byte-pair-encoded representation
    of the prompt. These tokens are then mapped to a numerical vector of length 768
    each, which you can verify by examining the shape of the model output.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to encoding the prompt itself as a numerical vector, we also encode
    a blank prompt ( ““). This is because when we later pass the embedded prompt to
    the image generation step, we want to control how much importance we assign to
    the prompt in generating the image (using the *guidance scale* parameter we’ll
    see later). To provide a reference, we need to also provide the embedding using
    no prompt at all, and the difference between the two will provide information
    to the image generation model on how to modify the generated image at each step
    of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the latent image using the VAE decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create an image based on your prompt, Stable Diffusion starts with a matrix
    of normally distributed random numbers. This is because, as we mentioned earlier,
    the model was developed using the random vectors (*latent* vectors) generated
    by VAE that we saw in [*Chapter 11*](Chapter_11.xhtml), which consists of an *encoder*
    and a *decoder*. As a reminder, the encoder is a neural network that takes as
    input an image and as output generates a (usually lower dimensional) vector or
    matrix of random numbers. This random number matrix is a kind of “barcode” for
    the image, which allows the important information to be compressed into a lower-dimensional
    space that takes up less memory on your computer – the fact that these vectors
    are smaller than the original image is one of the key optimizations that make
    the Stable Diffusion algorithm work so well. The decoder is a second neural network
    that is used to reverse this compression, turning a set of random numbers into
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: To see how this works, you can input an image into the `vae` component of the
    Stable Diffusion pipeline, as shown in *Figure 15**.20\.* First, you need to convert
    an input image into a tensor using the `torchvision to_tensor` function, then
    pass it through the encoder to create a 4 x 64 x 64 output – the `half()` command
    is to convert the input to float16\. In this example, you can see we have compressed
    a 512-by-512 RGB image into a 4-by-64-by-64 vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.20: Generating the latent vector using the VAE](img/B22333_15_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.20: Generating the latent vector using the VAE'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can run the decoder to verify that you can turn this latent vector back
    into an image (which is the final step of the Stable Diffusion algorithm you’ll
    see in a bit), as shown in *Figure 15**.21.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.21: Decoding the latent vector](img/B22333_15_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.21: Decoding the latent vector'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are able to generate samples from a latent vector and encode our
    prompt, we’re ready to generate images using the U-Net, the final network in the
    Stable Diffusion pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last element of the Stable Diffusion pipeline is U-Net, which takes the
    encoded prompt and a vector of random noise that is the same shape as an encoded
    image from the VAE (*Figure 15**.2)*. The U-Net, similar to the VAE, performs
    an encoding operation through a set of neural network layers and then decodes
    that output into a vector the same size as the random input. Each time we pass
    the latent vector through the U-Net, we are predicting how much noise, *e*, to
    subtract from the latent vector in the last step. Running this operation multiple
    times constitutes the “reverse” process for the Stable Diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: Since there was no original image – we supplied a random vector – the encoded
    prompt provides the model with the context of what image to generate.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.\uFEFF22: The U-Net image generation process](img/B22333_15_22.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.22: The U-Net image generation process'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the steps of generating an image. Our first step is to generate
    a random input of the same dimension as the VAE output, using `torch.randn`. We
    set a fixed seed (manual seed) so that we can make this process repeatable by
    generating the same random vector each time we call the code – this will make
    it easy to debug.
  prefs: []
  type: TYPE_NORMAL
- en: The component of the pipeline that will run the diffusion process – moving a
    random vector to a generated image – is called the *scheduler*. It specifies a
    number of timesteps to run this diffusion process and what properties each of
    those timesteps has. For the Stable Diffusion pipeline we are using, the default
    scheduler is the *PNDMScheduler*⁹. It specifies a set of differential equations
    to use to update the noise prediction at each step of the simulation; the amount
    of noise is determined by a parameter (`init_noise_sigma`) to scale our simple
    random input. Some schedulers apply different scaling/noise at each step of the
    simulation, but the PNDM scheduler does not, so we do not have to call the `scale_model_input`
    function of the scheduler at each step.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice we also concatenate the blank embedding and prompt; this is more
    efficient than processing them sequentially and comparing the output and allows
    us to perform those calculations in parallel. Finally, we set the *guidance scale*
    parameter, which defaults to 7.5\. Lower values assign less importance to the
    input prompt, and will lead to an image that less resembles the prompt. Greater
    values will place more importance on the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: At each step of the diffusion process, we duplicate the latent vector so that
    it can be compared with the blank embedding and the prompt. We then pass the textual
    embedding and the latent image vector to the U-Net, which returns a prediction
    of what the latent vector would be without noise. We split this output into two
    parts; one where that output has been conditioned using the embedded prompt and
    one that receives the blank embedding.
  prefs: []
  type: TYPE_NORMAL
- en: We then create the final U-Net output, `noise_pred`, at each step of the diffusion
    process by adding in a weighted difference between the prompt-conditioned and
    unconditional outputs, with the importance of that difference provided by the
    `guidance_scale`. Then we run the scheduler diffusion equation to generate the
    input for the next pass.
  prefs: []
  type: TYPE_NORMAL
- en: After several rounds (here, 50) of passing the random vector through the U-Net,
    we decode it with the VAE to get the final output. The code in *Figure 15**.23*
    shows how this happens.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure\uFEFF 15.23: Decoding the U-Net output with the VAE](img/B22333_15_23.png)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.23: Decoding the U-Net output with the VAE'
  prefs: []
  type: TYPE_NORMAL
- en: We need to undo the noise scaling we applied at the beginning of the scheduler
    (`init_sigma_noise`) by dividing by the *random* variable we had used as a multiplier
    earlier when we began the diffusion process, then use the decoder arm of the VAE
    to obtain the image from the latent vector. We recenter the output and then bind
    it between 0 and 1 so that the colors will show up correctly in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how the Stable Diffusion algorithm was developed
    and how it is implemented through the Hugging Face pipeline API. In the process,
    we saw how a diffusion model addresses conceptual problems with autoregressive
    transformer and GAN models by modeling the distribution of natural pixels. We
    also saw how this generative diffusion process can be represented as a reversible
    Markov process, and how we can train the parameters of a diffusion model using
    a variational bound, similar to a VAE.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we saw how the efficiency of a diffusion model is improved by executing
    the forward and reverse process in latent space in the Stable Diffusion model.
    We also illustrated how natural language user prompts are represented as byte
    encodings and transformed into numerical vectors. Finally, we looked at the role
    of the VAE in generating compressed image vectors, and how the U-Net of Stable
    Diffusion uses the embedded user prompt and a vector of random numbers to generate
    images by predicting the amount of noise that should be removed in each step of
    the reverse process.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ramesh, Aditya et al. “*Zero-Shot Text-to-Image Generation.*” *ArXiv* abs/2102.12092
    (2021).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Brock, Andrew; Donahue, Jeff; and Simonyan, Karen. “*Large scale GAN training
    for high fidelity natural image synthesis.*” *arXiv preprint arXiv:1809.11096*
    (2018).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; and Ganguli, Surya
    (2015-06-01). `"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"`
    (PDF). *Proceedings of the 32nd International Conference on Machine Learning*.
    37\. PMLR: 2256–2265.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ho, Jonathan; Jain, Ajay; and Abbeel, Pieter. “*Denoising diffusion probabilistic
    models.*” *Advances in neural information processing systems* 33 (2020): 6840-6851.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rombach, Robin et al. “*High-Resolution Image Synthesis with Latent Diffusion
    Models.*” *2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)* (2021): 10674-10685.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ronneberger, Olaf; Fischer, Philipp; and Brox, Thomas. Unet: Convolutional
    networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture
    Notes in Computer Science, pages 234–241\. Springer, 2015.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Radford, Alec et al. “*Learning transferable visual models from natural language
    supervision.*” *International conference on machine learning*. PmLR, 2021.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/2202.09778.pdf](https://arxiv.org/pdf/2202.09778.pdf)
    Liu, Luping et al. “*Pseudo numerical methods for diffusion models on manifolds.*”
    *arXiv preprint arXiv:2202.09778* (2022).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our communities on Discord and Reddit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](https://packt.link/I1tSU)
    and our Reddit channel at [https://packt.link/rmYYs](https://packt.link/rmYYs)
    to connect, share, and collaborate with like-minded AI professionals.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Discord QR** | **Reddit QR** |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/Discord_Babcock_1.png) | ![](img/Reddit_Babcock.png) |'
  prefs: []
  type: TYPE_TB
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[www.packtpub.com](https://www.packtpub.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/9781836200079.jpg)](https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLM Engineer’s Handbook**'
  prefs: []
  type: TYPE_NORMAL
- en: Paul Iusztin, Maxime Labonne
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-83620-007-9'
  prefs: []
  type: TYPE_NORMAL
- en: Implement robust data pipelines and manage LLM training cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create your own LLM and refine it with the help of hands-on examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get started with LLMOps by diving into core MLOps principles such as orchestrators
    and prompt monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform supervised fine-tuning and LLM evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy end-to-end LLM solutions using AWS and other tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design scalable and modularLLM systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about RAG applications by building a feature and inference pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9781803247281.jpg)](https://www.packtpub.com/en-us/product/generative-ai-with-amazon-bedrock-9781804618585)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative AI with Amazon Bedrock**'
  prefs: []
  type: TYPE_NORMAL
- en: Shikhar Kwatra, Bunny Kaushik
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80324-728-1'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the generative AI landscape and foundation models in Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tune generative models to improve their performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore several architecture patterns for different business use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain insights into ethical AI practices, model governance, and risk mitigation
    strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhance your skills in employing agents to develop intelligence and orchestrate
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor and understand metrics and Amazon Bedrock model response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore various industrial use cases and architectures to solve real-world business
    problems using RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay on top of architectural best practices and industry standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Generative AI with Python and PyTorch*, *Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1835884458)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
