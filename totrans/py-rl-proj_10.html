<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Looking Ahead</h1>
                </header>
            
            <article>
                
<p>Over the past few hundred pages, we have faced numerous challenges, to which we applied reinforcement and deep learning algorithms. To conclude our <strong>reinforcement learning</strong> (<strong>RL</strong>) journey, this chapter will look at several aspects of the field that we have not covered yet. We will start by looking at several of the drawbacks of reinforcement learning, which any practitioner or researcher should be aware of. To end on a positive note, we will follow up by describing numerous exciting academic developments and achievements the field has seen in recent years.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The shortcomings of reinforcement learning</h1>
                </header>
            
            <article>
                
<p>So far, we have only covered what reinforcement learning algorithms can do. To the reader, reinforcement learning may seem like the panacea for all kinds of problems. But why do we not see a ubiquitous application of reinforcement learning algorithms in real-life situations? The reality is that the field has a myriad of shortcomings that hinder commercial adoption.</p>
<p>Why is it necessary to talk about the field's flaws? We think this will help you build a more holistic, less biased view of reinforcement learning. Moreover, understanding the weaknesses of reinforcement learning and machine learning is an important quality of a good machine learning researcher or practitioner. In the following subsections, we will discuss a few of the most important limitations that reinforcement learning is currently facing.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resource efficiency</h1>
                </header>
            
            <article>
                
<p>Current deep reinforcement learning algorithms require vast amounts of time, training data, and computational resources in order to reach a desirable level of proficiency. For algorithms such as AlphaGo Zero, where our reinforcement learning algorithm learns to play Go with zero prior knowledge and experience, resource efficiency becomes a major bottleneck for taking such algorithms to commercial scales. Recall that when DeepMind implemented AlphaGo Zero, they needed to train the agent on tens of millions of games using hundreds of GPUs and thousands of CPUs. For AlphaGo Zero to reach a reasonable proficiency, it needs to play a number of games, equivalent to what hundreds of thousands of humans would play in their lifetimes.</p>
<p>Unless, in the future, the average consumer can readily leverage vast amounts of computational power that only the likes of Google and Nvidia can offer today, the ability to develop superhuman reinforcement learning algorithms will continue to be way beyond the public's reach. This means that powerful, resource-hungry reinforcement learning algorithms will be monopolized by a small consortium of institutions, which is probably not a great thing.</p>
<p>Thus, making reinforcement learning algorithms trainable under limited resources will continue to be an important issue that the community must address.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reproducibility</h1>
                </header>
            
            <article>
                
<p>In numerous fields of scientific research, a prevalent problem has been the inability to reproduce the experimental results claimed in academic papers and journals. In a 2016 survey conducted by Nature, the world's most renowned scientific journal, 70% of respondents claimed that they have failed to reproduce their own or another researcher's experimental results. Moreover, the attitude toward the inability to reproduce experimental results was a stark one, with 90% of researchers thinking that there is indeed a reproducibility crisis.</p>
<div class="packt_infobox">The original work reported by nature can be found here: <a href="https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970" target="_blank">https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970</a>.</div>
<p>While this survey targeted researchers across a number of disciplines, including biology and chemistry, reinforcement learning is also facing a similar problem. In the paper <em>Deep Reinforcement Learning Matters</em> (reference at the end of this chapter; you can view it at <a href="https://arxiv.org/pdf/1709.06560.pdf">https://arxiv.org/pdf/1709.06560.pdf</a> for the online version), Peter Henderson et al. study the effects of different configurations of a deep reinforcement learning algorithm on experimental outcomes. These configurations include hyperparameters, seeds for the random number generator, and network architecture.</p>
<p>In extreme cases, they found that, when training the same model on two sets of five different random seed configurations, the resulting average return for the two sets of models diverged  significantly. Moreover, changing other settings, such as the architecture of the CNN, activation functions, and learning rates, have profound effects on the outcome.</p>
<p>What are the implications of inconsistent, unreproducible results? As the adoption and popularity of reinforcement learning and machine learning continues to grow at near exponential rates, the number of implementations of reinforcement learning algorithms freely available on the internet also increases. If those implementations cannot reproduce the results they claim to be able to achieve, this would cause major issues and potential danger in real-life applications. Certainly, no one would want their self-driving car to be implemented so that it cannot produce consistent decisions!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explainability/accountability</h1>
                </header>
            
            <article>
                
<p>We have seen how an agent's policy can return either a single action or a probability distribution over a set of possible actions and how its value function can return how desirable a certain state is. But how can a model explain how it arrived at such predictions? As reinforcement learning becomes more popular and potentially more prevalent in real-life applications, there will be an ever-increasing need to be able to explain the output of reinforcement learning algorithms.</p>
<p>Today, most advanced reinforcement learning algorithms incorporate deep neural networks, which, as of now, can only be represented as a set of weights and a sequence of non-linear functions. Moreover, due to its high dimensional nature, neural networks are not able to provide any meaningful, intuitive relationships between input and their corresponding output that can be understood easily by humans. Hence, deep learning algorithms are often referred to as black boxes, for it is difficult for us to understand what is really going on inside a neural network.</p>
<p>Why is it important for a reinforcement learning algorithm to be explainable? Suppose an autonomous car is involved in a car accident (let's assume it was just an innocuous bump between two cars and the drivers are not hurt). Human drivers would be able to explain what led to the crash; they can give reasons for why they performed a particular maneuver and what exactly happened when the accident occurred. This would help law enforcement ascertain the cause of the accident and potentially determine who or what was accountable. However, even if we create an agent that can drive cars sufficiently well using algorithms available today, this is simply not possible.</p>
<p>Without the ability to explain predictions, it will be difficult for users and the general public to trust software that uses any kind of machine learning, especially in use cases where the algorithms are accountable for making important decisions. This is a serious impediment to the adoption of reinforcement learning algorithms in practical applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Susceptibility to attacks</h1>
                </header>
            
            <article>
                
<p>Deep learning algorithms have shown incredible results across numerous tasks, including computer vision, natural language processing, and speech recognition. In several tasks, deep learning has already surpassed human capabilities. However, recent work has shown that these algorithms are incredibly vulnerable to attacks. By attacks, we mean attempts to make imperceptible modifications to the input which causes the model to behave differently. Take the following example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-685 image-border" src="assets/e6104b4d-4113-4c78-9833-54ef67989b8e.png" style="width:104.08em;height:36.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">An illustration of adversarial attacks. By adding imperceptible perturbations to an image, an attacker can easily fool deep learning image classifiers.</div>
<p class="mce-root"/>
<p>The rightmost image is the result of adding the left image, which is the original image, and the middle image, which represents the perturbations added to the original image. Even the most accurate, well-performing deep neural network image classifier fails to identify the right image as a goat and instead predicts it to be a toaster.</p>
<p>These examples have shocked many in the research community, for people did not expect that deep learning algorithms can be incredibly brittle and susceptible to such attacks. This field is now called <strong>adversarial machine learning</strong> and has been rapidly increasing in prominence and importance as more researchers around the world are investigating the robustness and vulnerabilities of deep learning algorithms.</p>
<p>Reinforcement learning algorithms are also no stranger to these results and attacks. According to the paper titled <em>Robust Deep Reinforcement Learning with Adversarial Attacks</em> (<a href="https://arxiv.org/abs/1712.03632">https://arxiv.org/abs/1712.03632</a>) by Anay Pattanaik et. al., adversarial attacks to reinforcement learning algorithms can be defined as any possible perturbation that leads the agent into an increased probability of taking the worst possible action in that state. For example, we can add noise to the screen of an Atari game with the intention of tricking the RL agent playing the game to make a poor decision, which leads to a lower score.</p>
<p>More serious applications include adding noise to street signs to trick a self-driving car into thinking that a STOP sign is a speed sign, making an ATM recognize a $100 check as a $1,000,000 one, or even fooling a facial-recognition system to identify an attacker's face as that of another user.</p>
<p>Needless to say, these vulnerabilities further add to the risks of adopting deep learning algorithms in practical, safety-critical use cases. While there are numerous ongoing efforts to countervail adversarial attacks, there is still a long way to go for deep learning algorithms to become robust enough for such use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upcoming developments in reinforcement learning</h1>
                </header>
            
            <article>
                
<p>The past few sections may have painted a stark outlook for deep learning and reinforcement learning. However, there is no need to feel entirely discouraged; this is, in fact, an exciting time for DL and RL, where many significant advances in research are continuing to shape the field and cause it to evolve at a rapid pace. With increasing availability of computational resources and data, the possibilities of expanding and improving deep learning and reinforcement learning algorithms continue to expand.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Addressing the limitations</h1>
                </header>
            
            <article>
                
<p>For one, the issues raised in the preceding section are recognized and acknowledged by the research community. There are several efforts being made to address them. In the work by Pattanaik et. al., not only do the authors demonstrate that current deep reinforcement learning algorithms are susceptible to adversarial attacks, they also propose techniques that can make the same algorithms more robust toward such attacks. In particular, by training deep RL algorithms on examples that were adversarially perturbed, the model can improve its robustness against similar attacks. This technique is commonly referred to as adversarial training.</p>
<p>Moreover, the research community is actively taking actions to solve the reproducibility problem. ICLR and ICML, two of the biggest conferences in machine learning, have hosted challenges where participants are invited to reimplement and re-run experiments conducted by submitted papers to reproduce the reported results. Participants are then required to critique the original work by writing a reproducibility report that describes the problem statement, experimental methodology, implementation details, analyses, and the reproducibility of the original paper. Organized by Joelle Pineau and McGill University, this challenge aims to promote transparency in experiments and academic work as well as to ensure the reproducibility and integrity of results.</p>
<div class="packt_infobox">More information on the ICLR 2018 reproducibility challenge can be found here: <a href="https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html" target="_blank">https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html</a>. Similarly, the original ICML workshop on reproducibility can be found here: <a href="https://sites.google.com/view/icml-reproducibility-workshop/home" target="_blank">https://sites.google.com/view/icml-reproducibility-workshop/home</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p>Another important topic that is increasing in importance and attention is transfer learning. Transfer learning is a paradigm in machine learning, where a model trained on one task is fine-tuned to accomplish another.</p>
<p>For example, we can train a model to recognize images of cars and use the weights of that model to initialize an identical model that learns to recognize trucks. The main intuition is that certain abstract concepts and features learned by training on one task are transferable to other similar tasks. This idea is applicable to many reinforcement learning problems as well. An agent that learns to play a particular Atari game should be able to play other Atari games proficiently without training entirely from scratch, much like how a human can.</p>
<p>Demis Hassabis, the founder of DeepMind and a pioneer in deep reinforcement learning, said in a recent talk that transfer learning is the key to general intelligence. <span>And I think the key to doing transfer learning will be the acquisition of conceptual knowledge that is abstracted away from perceptual details of where you learned it from.</span></p>
<div class="packt_infobox">The Demis Hassabis quote and the talk in which this was mentioned can be found here: <a href="https://www.youtube.com/watch?v=YofMOh6_WKo" target="_blank">https://www.youtube.com/watch?v=YofMOh6_WKo</a></div>
<p>There have already been several advances in computer vision and natural language processing, where models initialized with knowledge and priors from one domain are used to learn about data from another domain.</p>
<p>This is especially useful when the second domain lacks data. Called <strong>few-shot</strong> or <strong>one-shot</strong> learning, these techniques allow models to learn to perform tasks well, even when the dataset is small, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-644 image-border" src="assets/13a45856-a815-4b92-9a54-68b401f7db6a.png" style="width:44.50em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">An illustration of a few-shot learning classifier learning good decision boundaries for classes with small volumes of data</div>
<p>Few-shot learning for reinforcement learning would involve having the agent learn to achieve high proficiency on a given task without a high dependence on time, data, and computational resources. Imagine a generalized game-playing agent that can easily be fine-tuned to perform well on any other video game using readily-available computational resources; this would make training RL algorithms a lot more efficient and thus more accessible to a wider audience.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-agent reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Another promising area making significant strides is multi-agent reinforcement learning. Contrary to the problems we've seen where only one agent makes decisions, this topic involves having multiple agents make decisions simultaneously and cooperatively in order to achieve a common objective. One of the most significant works related to this has been OpenAI's Dota2-playing system, called <strong>OpenAI Five</strong>. Dota2 is one of the world's most popular <strong>Massively Multiplayer Online Role Playing Game</strong> (<strong>MMORPGs</strong>). Compared to traditional RL games such as Go and Atari, Dota2 is more complex for the following reasons:</p>
<ul>
<li><strong>Multiple agents</strong>: Dota2 games involve two teams of five players, each fighting to destroy the other team's base. Hence there are multiple agents, not just one, making decisions simultaneously.</li>
<li><strong>Observability</strong>: The screen only shows the proximity of the agent's character instead of the whole map. This means that the whole game state, including the locations of opponents and what they are doing, is not observable. In reinforcement learning, we call this a <em>partially-observable</em> state.</li>
<li><strong>High dimensionality</strong>: A Dota2 agent's observations can include 20,000 points, each depicting what a human player may observe on the screen, including health, the location of the controlling character, the location of enemies, and any attacks. Go, on the other hand, requires fewer data points to construct an observation (19 x 19 board, past moves). Hence, observations have high dimensionality and complexity. This also goes for decisions, where a Dota2 AI's action space consists of 170,000 possibilities, which includes decisions on movement, casting spells, and using items.</li>
</ul>
<div class="packt_infobox">For more information on OpenAI's Dota2 AI, check out their blogs on the project at <a href="https://blog.openai.com/openai-five/">https://blog.openai.com/openai-five/</a>.</div>
<p class="CDPAlignLeft CDPAlign">Moreover, by using novel upgrades on traditional reinforcement learning algorithms, each agent in OpenAI Five was able to learn to cooperate with one another in order to reach the common objective of destroying the enemy's base. They were even able to learn several team strategies that experienced human players employ. The following is a screenshot from a game being played between a team of Dota players and OpenAI Five:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-648 image-border" src="assets/c596eab2-e204-41ac-930a-2206475bec90.png" style="width:67.25em;height:37.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">OpenAI versus human players (source: <a href="https://www.youtube.com/watch?v=eaBYhLttETw">https://www.youtube.com/watch?v=eaBYhLttETw</a>)</div>
<p class="mce-root">Despite the extreme levels of resource requirements (240 GPUs, 120,000 CPU cores, ~200 human years of gameplay in a single day), this project demonstrates that current AI algorithms are indeed able to cooperate with one another to reach a common objective in a vastly complex environment. This work symbolizes another significant advancement in AI and RL research and demonstrates what the current technology is capable of.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This concludes our introductory journey into reinforcement learning. Over the course of this book, we learned how to implement agents that can play Atari games, navigate Minecraft, predict stock market prices, play the complex board game of Go, and even generate other neural networks to train on <kbd>CIFAR-10</kbd> data. In doing so, you acquired and became accustomed to some of the fundamental and state-of-the-art deep learning and reinforcement learning algorithms. In short, you have achieved a lot!</p>
<p>But the journey does not and should not end here. We hope that, with your newfound skills and knowledge, you will continue to utilize deep learning and reinforcement learning algorithms to tackle problems that you face outside of this book. More importantly, we hope that this guide motivates you to explore other fields of machine learning and further develop your knowledge and experience.</p>
<p>There are many obstacles for the reinforcement learning community to overcome. However, there is much to look forward to. With the increasing popularity and development of the field, we can't wait to see what new developments and milestones the field will achieve. We hope the reader, upon completing this guide, will feel more equipped and ready to build reinforcement learning algorithms and make significant contributions to the field.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Open Science Collaboration. (2015). <em>Estimating the reproducibility of psychological science</em>. </span>Science<span>, </span>349<span>(6251), aac4716.</span></p>
<p><span>Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2017). <em>Deep reinforcement learning that matters</em>. </span>arXiv preprint arXiv:1709.06560<span>.</span></p>
<p><span>Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., and Chowdhary, G. (2018, July). <em>Robust deep reinforcement learning with adversarial attacks</em>. In </span>Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems<span> (pp. 2040-2042). International Foundation for Autonomous Agents and Multiagent Systems.</span></p>


            </article>

            
        </section>
    </body></html>