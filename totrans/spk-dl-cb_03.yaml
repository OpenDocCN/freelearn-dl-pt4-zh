- en: Pain Points of Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #1: Importing MNIST images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain Point #2: Visualizing MNIST images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain Point #3: Exporting MNIST images as files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain Point #4: Augmenting MNIST images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain Point #5: Utilizing alternate sources for trained images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain Point #6: Prioritizing high-level libraries for CNNs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) have been enjoying a bit of resurgence
    in the last couple of years. They have shown great success when it comes to image
    recognition. This is quite relevant these days with the advent of modern smartphones
    as anyone now has the ability to take large volumes of pictures of objects and
    post them on social media sites. Just due to this phenomenon, convolutional neural
    networks are in high demand these days.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several features that make a CNN optimally perform. They require
    the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: A high volume of training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual and spatial data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An emphasis on filtering (pooling), activation, and convoluting as opposed to
    a fully connected layer that is more apparent in a traditional neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While CNNs have gained great popularity, there are some limitations in working
    with them primarily due to their computational needs as well as the volume of
    training data required to get a well-performing model. We will focus on techniques
    that can be applied to the data that will ultimately assist with the development
    of a convolutional neural network while addressing these limitations. In later
    chapters, we will apply some of these techniques when we develop models for image
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #1: Importing MNIST images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most common datasets used for image classification is the `MNIST` dataset,
    which is composed of thousands of samples of handwritten digits. The **Modified
    National Institute of Standards and Technology** (**MNIST**) is, according to
    Yann LeCun, Corinna Cortes, and Christopher J.C. Burges, useful for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a good database for people who want to try learning techniques and pattern
    recognition methods on real-world data while spending minimal efforts on preprocessing
    and formatting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several methods to import the MNIST images into our Jupyter notebook.
    We will cover the following two methods in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Directly through the TensorFlow library
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manually through the MNIST website
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One thing to note is that we will be primarily using MNIST images as our example
    of how to improve performance within a convolutional neural network. All of these
    techniques that will be applied on MNIST images can be applied to any image that
    will be used to train a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only requirement needed is to install `TensorFlow`. It will likely not
    come pre-installed with the anaconda3 packages; therefore, a simple `pip` install
    will either confirm the availability of `TensorFlow` or install it if not currently
    available. `TensorFlow` can be easily installed in the Terminal, as seen in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32eb9853-862f-45e7-a53a-c61008231b04.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `TensorFlow` library has a conveniently built-in set of examples that can
    be used directly. One of those example datasets is `MNIST`. This section will
    walk through the steps of accessing those images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `TensorFlow` into the library with an alias of `tf` using the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Download and extract images from the library and save to a local folder using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve a final count of the training and testing datasets that will be used
    to evaluate the accuracy of the image classification using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains the process used to access the MNIST datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Once we receive a confirmation that the `TensorFlow` library has been properly
    installed, it is imported into the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can confirm the version of `TensorFlow` as well as extract the images to
    our local folder of `MNIST/`. The extraction process is visible in the output
    of the notebook, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe736aad-311b-4ecf-ba55-204923aff582.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The four extracted files are named the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`t10k-images-idx3-ubyte.gz`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`t10k-labels-idx1-ubyte.gz`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train-images-idx3-ubyte.gz`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train-labels-idx1-ubyte.gz`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They have been downloaded to the `MNIST/` subfolder as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/eae6887f-d725-4376-ae25-a3876d69a939.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, the four files can be viewed in our notebook, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5d23979c-afee-413f-8834-7d6c3f5f3f45.png)'
  prefs: []
  type: TYPE_IMG
- en: The four files are the testing and training images along with the accompanying
    testing and training labels identifying each image in the testing and training
    datasets. Additionally, the `one_hot = True` feature is explicitly defined. This indicates
    that one-hot encoding is active with the labels, which assists with feature selection
    within modeling as each column value will be either 0 or 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A subclass of the library is also imported that stores the handwritten images
    of MNIST to the specified local folder. The folder containing all of the images
    should be approximately 12 MB in size for 55,000 training images and 10,000 testing
    images, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/102e5c7e-5458-4ada-9747-9346ffea364f.png)'
  prefs: []
  type: TYPE_IMG
- en: The 10,000 images will be used to test the accuracy of our model that will be
    trained on the 55,000 images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Occasionally, there may be errors or warnings when trying to access the MNIST
    datasets directly through `TensorFlow`. As was seen earlier on in the section,
    we received the following warning when importing MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: 'WARNING:tensorflow:From <ipython-input-3-ceaef6f48460>:2: read_data_sets (from
    tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be
    removed in a future version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions for updating:'
  prefs: []
  type: TYPE_NORMAL
- en: Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset may become deprecated in a future release of `TensorFlow` and therefore,
    no longer be directly accessible. Sometimes we may just encounter a typical *HTTP 403
    error* when extracting the MNIST images through `TensorFlow`. This may be due
    to the website being temporarily unavailable. Have no fear in either case, there
    is a manual approach to downloading the four `.gz` files using the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The files are located on the website, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a65f6b81-fc22-476b-9ebb-bf7ab594063c.png)'
  prefs: []
  type: TYPE_IMG
- en: Download the files and save them to an accessible local folder similar to what
    was done with the files that came directly from `TensorFlow`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about the `MNIST` database of handwritten digits, visit the following
    website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about one-hot encoding, visit the following website: [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #2: Visualizing MNIST images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Plotting images is often a major pain point when dealing with graphics within
    a Jupyter notebook. Displaying the handwritten images from the training dataset
    is critical, especially when comparing the actual value of the label that is associated
    with the handwritten image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only Python libraries that will be imported to visualize the handwritten
    images are `numpy` and `matplotlib`. Both should already be available through
    the packages in Anaconda. If for some reason they are not available, they can
    both be `pip` installed at the Terminal using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will walk through the steps to visualize the MNIST handwritten
    images in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries, `numpy` and `matplotlib`, and configure `matplotlib`
    to plot `inline` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the first two sample images using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will walk through the process of how the MNIST handwritten images
    are viewed in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: A loop is generated in Python that will sample two images from the training
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initially, the images are just a series of values in float format between 0
    and 1 that are stored in a `numpy` array. The value of the array is a labeled
    image called `image`. The `image` array is then reshaped into a 28 x 28 matrix
    called `pixels` that has a black color for any value at 0 and a gray shade color
    for any color that is not 0\. The higher the value, the lighter the gray shade
    of color. An example can be seen in the following screenshot for the digit 8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe88bf0f-e733-4d79-b8d4-6e4619614ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the loop produces two handwritten images for the numbers 7 and
    3 along with their labels, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe1e9692-b9e0-4732-b6b2-ebe49448854a.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition to the images being plotted, the label from the training dataset
    is also printed above the image. The label is an array of length 10, with values
    of 0 or 1 only for all 10 digits. For digit 7, the 8th element in the array is
    of value 1 and for digit 3, the 4th element in the array is of value 1\. All other
    values are 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may not be immediately obvious what the numeric value of the image is. While
    most will be able to identify that the first image is a 7 and the second image
    is a 3, it would be helpful to have confirmation from the label array.
  prefs: []
  type: TYPE_NORMAL
- en: There are 10 elements in the array, each referencing a value for labels 0 through
    9 in numeric order. Since the first array has a positive or 1 value in the 8th
    slot, that is an indication that the value of the image is a 7, as 7 in the 8th
    index in the array. All other values should be 0\. Additionally, the second image
    has a value of 1 in the 4th spot, indicating a positive value for 3.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Leun, Cortes, and Burges discuss why the image pixelations were set at 28 x
    28 in the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: he original black and white (bilevel) images from NIST were size normalized
    to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting
    images contain grey levels as a result of the anti-aliasing technique used by
    the normalization algorithm. The images were centered in a 28x28 image by computing
    the center of mass of the pixels, and translating the image so as to position
    this point at the center of the 28x28 field.
  prefs: []
  type: TYPE_NORMAL
- en: --Leun, Cortes, and Burges from [http://yann.lecun.com/exdb/mnist/.](http://yann.lecun.com/exdb/mnist/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #3: Exporting MNIST images as files'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often need to work within the image directly and not as an array vector.
    This section will guide us through converting our arrays to `.png` images.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Exporting the vectors to images requires importing the following library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import image from matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to convert a sample of MNIST arrays to
    files in a local folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a subfolder to save our images to our main folder of `MNIST/` using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop through the first 10 samples of MNIST arrays and convert them to `.png`
    files using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to see the list of images from `image_no_1.png` to `image_no_9.png`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the MNIST arrays are converted to images and saved
    to a local folder.
  prefs: []
  type: TYPE_NORMAL
- en: We create a subfolder called `MNIST/images` to help us store our temporary `.png`
    images and separate them from the MNIST arrays and labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once again we loop through `data.train` images and obtain nine arrays that can
    be used for sampling. The images are then saved as `.png` files to our local directory
    with the following format: `'image_no_{}.png'.format(i), pixels, cmap = 'gray'`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the nine images can be seen in our local directory, as seen in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/755def3b-f925-46cd-9f1a-eaed32ab2651.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to seeing the list of images in our directory, we can also view
    the image in our directory within Linux, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36415522-ca99-46de-b043-a9d84796217d.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about `image.imsave` from `matplotlib` visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imsave.html](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imsave.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #4: Augmenting MNIST images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main drawbacks of working with image recognition is the lack of variety
    in some of the images available. This may cause the convolutional neural network
    to not operate as optimally as we would like, and return less than ideal results
    due to the lack of variety in the training data. There are techniques available
    to bypass that shortcoming and we discuss one of them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once again much of the heavy lifting is already done for us. We will use a popular
    Python package, `augmentor`, that is frequently used with machine learning and
    deep learning modeling to generate additional versions of existing images distorted
    and augmented for variety.
  prefs: []
  type: TYPE_NORMAL
- en: 'The package will first have to be `pip` installed using the following script:
    `pip install augmentor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should then have confirmation that the package is installed, as seen in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48cf4d3f-6f9f-43d8-b2dc-d607f1cf2794.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then need to import the pipeline class from augmentor:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from Augmentor import Pipeline`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to increase the frequency and augmentation
    of our nine sample images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the `augmentor` function using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script so that the `augmentor` function can `rotate`
    our images with the following specifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script so that each image is augmented through two iterations
    10 times each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how our nine images are used to create additional images
    that are distorted.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a `Pipeline` for our image transformation and specify the
    location of the images that will be used.  This ensures the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The source location of the images
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of images that will be transformed
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The destination location of the images
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can see that our destination location is created with a subfolder called
    `/output/` as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b894e720-39ad-4b5d-8811-934591624989.png)'
  prefs: []
  type: TYPE_IMG
- en: The `augmentor` function is configured to rotate each image up to 25 degrees
    to the right or 25 degrees to the left with a 90 percent probability. Basically,
    the probability configuration determines how often an augmentation takes place.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A loop is created to go through each image twice and apply two transformations
    to each image; however, since we did add a probability to each transformation
    some images may not get transformed and others may get transformed more than twice.
    Once the transformations are complete, we should get a message indicating so,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a3d8549c-a738-41f1-949e-96d894837ede.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the augmentations complete, we can visit the `/output/` subdirectory
    and see how each digit is slightly altered, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ba5e0a68-9854-4537-bfd1-7147786af584.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that we have several variations of the digits 3, 1, 8, 0, and 9 all
    with varying degrees of rotation. We now have tripled our sample data set and
    added more variety without having to go out and extract more images for training
    and testing purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We only applied the `rotate` transformation; however, there are several transformation
    and augmentation features available to apply to images:'
  prefs: []
  type: TYPE_NORMAL
- en: Perspective skewing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic distortions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shearing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cropping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirroring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While not all of these transformations will be necessary when looking to increase
    frequency and variety of a training dataset, it may be beneficial to use some
    combination of features and evaluate model performance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about `augmentor` visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://augmentor.readthedocs.io/en/master/](https://augmentor.readthedocs.io/en/master/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #5: Utilizing alternate sources for trained images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes there are just not enough resources available to perform a convolutional
    neural network. The resources could be limited from a computational perspective
    or a data collection perspective. In situations like these, we rely on other sources
    to help us with classifying our images.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technique for utilizing pre-trained models as the source for testing outcomes
    on other datasets is referred to as transfer learning. The advantage here is that
    much of the CPU resources allotted for training images is outsourced to a pre-trained
    model. Transfer learning has become a common extension of deep learning more recently.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the process of transfer learning works.
  prefs: []
  type: TYPE_NORMAL
- en: Collect a series of datasets or images that you are interested in classifying, just
    as you would with traditional machine learning or deep learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into a training and testing split such as 75/25 or 80/20.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify a pre-trained model that will be used to identify the patterns and
    recognition of the images you are looking to classify.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a deep learning pipeline that connects the training data to the pre-trained
    model and develops the weights and parameters needed to identify the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, evaluate the model performance on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the process of transfer learning when applied to the MNIST
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We are definitely taking a shortcut approach with transfer learning as we are
    either limited in resources, time, or both as we are taking prior work that has
    already been done and hoping that it will help us solve something new.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since we are dealing with an image classification problem, we should use a
    pre-trained model that has worked with classifying common images in the past.
    There are many common ones out there but two that stand out are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ResNet model developed at Microsoft.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The Inception model developed at Google.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Both models are useful for image classification because both Microsoft and Google
    have a wide spectrum of images that are available to them to train a robust model
    that can extract features at a more detailed level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Directly within Spark, there is the ability to build a deep learning pipeline
    and to call about a class called `DeepImageFeaturizer` and apply the `InceptionV3`
    model to a set of features collected from training data. The trained dataset is
    then evaluated on the testing data using some type of binary or multiclassification
    evaluator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A pipeline within deep learning or machine learning is simply the workflow process
    used to get from an initial environment of data collection to a final evaluation
    or classification environment on the collected data by applying a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with everything, there are pros and cons to using transfer learning. As we
    discussed earlier on in the section, transfer learning is ideal when you are limited
    in resources to perform your own modeling on a large dataset. There is always
    the chance that the source data at hand does not exhibit many of the features
    unique to it in the pre-trained models leading to poor model performance. There
    is always the option to switch from one pre-trained model to another and evaluate
    model performance. Again, transfer learning is a fail fast approach that can be
    taken when other options are not available.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about ResNet at Microsoft, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://resnet.microsoft.com/](https://resnet.microsoft.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about Inception at Google, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tutorials/image_recognition](https://www.tensorflow.org/tutorials/image_recognition)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more specifically about InceptionV3, you can read the following paper
    titled <q>Rethinking the Inception Architecture for Computer Vision</q> at Cornell
    University:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pain Point #6: Prioritizing high-level libraries for CNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many libraries available to perform convolutional neural networks.
    Some of them are considered low-level such as TensorFlow, where much of the configuration
    and setup requires extensive coding. This can be considered a major pain point
    for an inexperienced developer. There are other libraries, such as Keras, that
    are high-level frameworks built on top of libraries such as TensorFlow. These
    libraries require much less code to get up and running with building a convolutional
    neural network. Often times developers getting started with building a neural
    network will try and implement a model with TensorFlow and run into several issues
    along the way. This section will propose initially building a convolutional neural
    network with Keras instead to predict the hand-written images from the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be working with Keras to train a model for recognizing
    handwritten images from MNIST. You can install Keras by executing the following
    command at the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to build a model to recognize handwritten
    images from MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create testing and training images and labels based on the MNIST dataset from
    the following variables using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the testing and training arrays using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the following from `keras` to build the convolutional neural network
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the image ordering using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `Sequential` `model` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add layers to the `model` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the `model` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the `model` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the `model` performance using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the convolutional neural network is built on Keras
    to identify handwritten images from MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: For any model development, we need to identify our testing and training datasets
    as well as the features and the labels. In our case, it is pretty straightforward
    as the MNIST data from TensorFlow is already broken up into `data.train.images`
    for the features and `data.train.labels` for the labels. Additionally, we want
    to convert the labels into arrays, so we utilize `np.asarray()` for `ytest` and
    `ytrain`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The arrays for `xtrain`, `xtest`, `ytrain`, and `ytest` are currently not in
    the proper shape to be used for a convolutional neural network within Keras. As
    we identified early on in the chapter, the features for the MNIST images represent
    28 x 28-pixel images and the labels indicate one of ten values from 0 through
    9\. The x-arrays will be reshaped to (,28,28,1) and the y-arrays will be reshaped
    to (,10). The `shape` of the new arrays can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d8995f0-b3cd-4f67-ac86-77e4906fd698.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned previously, Keras is a high-level library; therefore, it does not
    perform tensor or convolutional operations without the assistance of a lower level
    library such as TensorFlow. In order to configure these operations, we set the
    `backend` to be `K` for `Keras` with the image dimensional ordering, `image_dim_ordering`,
    set to `tf` for TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please note that the backend could also be set to other low-level libraries,
    such as `Theano`. Instead of `tf`, we would set the dimensional ordering to `th`.
    Additionally, we would need to reconstruct the shaping of the features. However,
    in the past few years, `Theano` has not garnered the same adoption rate `TensorFlow`
    has.
  prefs: []
  type: TYPE_NORMAL
- en: Once we import the necessary libraries to build the CNN model, we can begin
    constructing the sequences or layers, `Sequential()`, of the model. For demonstration
    purposes, we will keep this model as simple as possible with only 4 layers to
    prove that we can still gain a high accuracy with minimal complexity. Each layer
    is added using the `.add()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first layer is set to build a 2-Dimensional (`Conv2D`) convolution layer,
    which is common for spatial images such as the MNIST data. Since it is the first
    layer, we must explicitly define the `input_shape` of the incoming data. Additionally,
    we specify a `kernel_size` that is used to set the height and width of the window
    filter used for convolution. Usually, this is either a 3x3 window or 5x5 window
    for the 32 filters. Additionally, we have to set an activation function for this
    layer and rectified linear units, `relu`, are a good option here for efficiency
    purposes, especially early on in the neural network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the second layer flattens the first layer inputs to retrieve a classification
    that we can use to determine whether the image is one of a possible 10 digits.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, we pass the outputs from the second layer into a `dense` layer that has
    128 hidden layers with another `relu` activation function. The function within
    a densely connected layer incorporates the `input_shape` and `kernel_size` as
    well as the bias to create the output for each of the 128 hidden layers.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The final layer is the output that will determine what the predicted value will
    be for the MNIST image. We add another `dense` layer with a `sigmoid` function
    to output probabilities for each of the 10 possible scenarios our MNIST image
    could be. Sigmoid functions are useful for binary or multiclass classification
    outcomes.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to compile the model using `adam` for the `optimizer` and evaluating
    `accuracy` for the `metrics`. The `adam` optimizer is common for CNN models as
    is using `categorical_crossentropy` as a loss function when dealing with multiclassification
    scenarios for 10 possible outcomes as is our case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We train the model using a `batch_size` of `512` images at a time over `5`
    runs or `epochs`. The loss and accuracy of each epoch are captured and can be
    seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/523622b0-32c1-4a34-a0b7-37358cd01f84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We calculate the accuracy and the loss rate by evaluating the trained model
    on the test dataset as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2bcbcfc2-85d3-453b-8acd-9cb6a5f7b1bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Our model seems to be performing well with a 98.6% accuracy rate and a 5% loss
    rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We built a simple convolutional neural network in Keras using five lines of
    code for the actual model design. Keras is a great way to get a model up and running
    in little time and code. Once you are ready to move onto more sophisticated model
    development and control, it may make more sense to build a convolutional neural
    network in TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to retrieving the accuracy of the model we can also produce the
    shapes within each layer of the CNN modeling process by executing the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `model.summary()` can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aac3bf89-2b4a-407a-a159-a8ae8df4bf62.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the output shape of the first layer (None, 24, 24, 32) was flattened
    out into a shape of (None, 18432) by multiplying 24 x 24 x 32 within the second
    layer. Additionally, we see our third and fourth layers have the shape that we
    assigned them using the Dense layer function
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about 2D convolutional layer development in Keras, visit the
    following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/layers/convolutional/#conv2d](https://keras.io/layers/convolutional/#conv2d)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn how to build a convolutional neural network in TensorFlow with MNIST
    images, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros](https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros)'
  prefs: []
  type: TYPE_NORMAL
