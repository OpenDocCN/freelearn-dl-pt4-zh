<html><head></head><body>
  <div id="_idContainer155">
   <h1 class="chapter-number" id="_idParaDest-60">
    <a id="_idTextAnchor063">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     3
    </span>
   </h1>
   <h1 id="_idParaDest-61">
    <a id="_idTextAnchor064">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Graph Representation Learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     Having explained why applying deep learning techniques to graph data is a worthy endeavor, let’s jump right into the thick of things.
    </span>
    <span class="koboSpan" id="kobo.3.2">
     In this chapter, we’ll introduce
    </span>
    <a id="_idIndexMarker207">
    </a>
    <span class="koboSpan" id="kobo.4.1">
     you to
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.5.1">
      graph
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.6.1">
       representation learning
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.7.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.8.1">
     First, we’ll examine representation learning from the perspective of traditional (tabular data-based)
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.9.1">
      machine learning
     </span>
    </strong>
    <span class="koboSpan" id="kobo.10.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.11.1">
      ML
     </span>
    </strong>
    <span class="koboSpan" id="kobo.12.1">
     ) and
    </span>
    <a id="_idIndexMarker208">
    </a>
    <span class="koboSpan" id="kobo.13.1">
     then extend the idea to the graph data space.
    </span>
    <span class="koboSpan" id="kobo.13.2">
     Following this, we’ll talk about the initial challenges that need to be addressed when you’re trying to learn features within graph data.
    </span>
    <span class="koboSpan" id="kobo.13.3">
     Next, you’ll be introduced to a few simple graph representation learning algorithms, namely,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.14.1">
      Node2Vec
     </span>
    </strong>
    <span class="koboSpan" id="kobo.15.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.16.1">
      DeepWalk
     </span>
    </strong>
    <span class="koboSpan" id="kobo.17.1">
     , and
    </span>
    <a id="_idIndexMarker209">
    </a>
    <span class="koboSpan" id="kobo.18.1">
     understand the differences between
    </span>
    <a id="_idIndexMarker210">
    </a>
    <span class="koboSpan" id="kobo.19.1">
     them.
    </span>
    <span class="koboSpan" id="kobo.19.2">
     Finally, we’ll discuss the limitations of such
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.20.1">
      shallow encoding
     </span>
    </strong>
    <span class="koboSpan" id="kobo.21.1">
     techniques
    </span>
    <a id="_idIndexMarker211">
    </a>
    <span class="koboSpan" id="kobo.22.1">
     and why we need algorithms with more firepower to capture more complex relationships
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.23.1">
      in graphs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.24.1">
     We’ll also introduce implementations of relevant algorithms in Python.
    </span>
    <span class="koboSpan" id="kobo.24.2">
     We’ll use Python as our language of choice and primarily
    </span>
    <a id="_idIndexMarker212">
    </a>
    <span class="koboSpan" id="kobo.25.1">
     use the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.26.1">
      PyTorch Geometric
     </span>
    </strong>
    <span class="koboSpan" id="kobo.27.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.28.1">
      PyG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.29.1">
     ) library to implement our algorithms.
    </span>
    <span class="koboSpan" id="kobo.29.2">
     Other libraries are popular as well (such
    </span>
    <a id="_idIndexMarker213">
    </a>
    <span class="koboSpan" id="kobo.30.1">
     as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.31.1">
      Tensorflow-Graph Neural Networks (GNNs)
     </span>
    </strong>
    <span class="koboSpan" id="kobo.32.1">
     ), but PyG seems to be the most established in the industry at the time
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      of writing.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.34.1">
     In this chapter, we’ll cover the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.35.1">
      following topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.36.1">
      Representation learning – what
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.37.1">
       is it?
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.38.1">
      Graph
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.39.1">
       representation learning
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.40.1">
      A framework for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.41.1">
       graph learning
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.42.1">
       DeepWalk
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.43.1">
       Node2Vec
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.44.1">
      Limitations of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.45.1">
       shallow encodings
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-62">
    <a id="_idTextAnchor065">
    </a>
    <span class="koboSpan" id="kobo.46.1">
     Representation learning – what is it?
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.47.1">
     Modern ML-related
    </span>
    <a id="_idIndexMarker214">
    </a>
    <span class="koboSpan" id="kobo.48.1">
     tasks and experiments have settled into a standardized workflow pipeline.
    </span>
    <span class="koboSpan" id="kobo.48.2">
     Here’s a quick and simplified overview of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.49.1">
      the steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.50.1">
      Convert the business/domain-specific problem into an ML problem (supervised or unsupervised, what metric is being optimized, baseline levels of metrics, and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.51.1">
       so on).
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.52.1">
      Get
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.53.1">
       the data.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.54.1">
      Pamper the data (by introducing new columns based on existing ones, imputing missing values,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.55.1">
       and more).
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.56.1">
      Train an ML model on the data and evaluate its performance on the test set.
     </span>
     <span class="koboSpan" id="kobo.56.2">
      Iterate on this step with new models until a satisfactory performance
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.57.1">
       is achieved.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.58.1">
     One of the most important and time-consuming steps in this list is deciding how new columns can be created from the existing ones to add to the knowledge being specified in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.59.1">
      the data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.60.1">
     To understand this, let’s understand the meaning of what a dataset is.
    </span>
    <span class="koboSpan" id="kobo.60.2">
     A row in a dataset is effectively just a record of an event.
    </span>
    <span class="koboSpan" id="kobo.60.3">
     The different columns (features) in a row represent the different variables (or dimensions, gauges, and so on), whose values were recorded at that event.
    </span>
    <span class="koboSpan" id="kobo.60.4">
     Now, for ML models to learn useful information, the values of the primarily
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.61.1">
      useful
     </span>
    </em>
    <span class="koboSpan" id="kobo.62.1">
     features must be recorded in the dataset.
    </span>
    <span class="koboSpan" id="kobo.62.2">
     When we refer to features as useful, we mean those whose values, when changed, significantly alter the overall outcome of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.63.1">
      the event.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.64.1">
     Let’s try to understand this with an example.
    </span>
    <span class="koboSpan" id="kobo.64.2">
     An extremely popular problem in the domain of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.65.1">
      natural language processing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.66.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.67.1">
      NLP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.68.1">
     ) is the task of predicting what the next word would be, given the previous words.
    </span>
    <span class="koboSpan" id="kobo.68.2">
     We won’t get into the nitty-gritty of this, instead just concentrating on a few scenarios where different feature choices have been made.
    </span>
    <span class="koboSpan" id="kobo.68.3">
     The different features here are essentially the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.69.1">
      previous words:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.70.1">
Feature_1 (F1): The last word of the unfinished sentence.
</span><span class="koboSpan" id="kobo.70.2">Feature_2 (F2): The 2</span><span class="superscript"><span class="koboSpan" id="kobo.71.1">nd</span></span><span class="koboSpan" id="kobo.72.1"> last word of the unfinished sentence.
</span><span class="koboSpan" id="kobo.72.2">Feature_3 (F3): The 3</span><span class="superscript"><span class="koboSpan" id="kobo.73.1">rd</span></span><span class="koboSpan" id="kobo.74.1"> last word of the unfinished sentence.
</span><span class="koboSpan" id="kobo.74.2">… and so on.</span></pre>
   <p>
    <span class="koboSpan" id="kobo.75.1">
     Take the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.76.1">
      unfinished sentence:
     </span>
    </span>
   </p>
   <p>
    <em class="italic">
     <span class="koboSpan" id="kobo.77.1">
      As the sun set over the horizon, the mountains cast
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.78.1">
       elongated ___.
      </span>
     </em>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.79.1">
     The features (along
    </span>
    <a id="_idIndexMarker215">
    </a>
    <span class="koboSpan" id="kobo.80.1">
     with the expected word, given a training dataset) would be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.81.1">
      as follows:
     </span>
    </span>
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table001-2">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.82.1">
          F11
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.83.1">
          F10
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.84.1">
          F9
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.85.1">
          F8
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.86.1">
          F7
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.87.1">
          F6
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.88.1">
          F5
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.89.1">
          F4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.90.1">
          F3
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.91.1">
          F2
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.92.1">
          F1
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.93.1">
         P
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.94.1">
          As
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.95.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.96.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.97.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.98.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.99.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.100.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.101.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.102.1">
          mountains
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.103.1">
          cast
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.104.1">
          elongated
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.105.1">
          shadows
         </span>
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.106.1">
     Table 3.1 – The features for predicting the next word
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.107.1">
     With this understanding of what features are, let’s look at different subsets of features that can be taken as
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.108.1">
      eligible features
     </span>
    </em>
    <span class="koboSpan" id="kobo.109.1">
     for model training.
    </span>
    <span class="koboSpan" id="kobo.109.2">
     Let’s take a look at three
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.110.1">
      different cases:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.111.1">
Case 1: All features with a lookback window of 10.
</span><span class="koboSpan" id="kobo.111.2">{F1, F2, F3, …, F8, F9, F10}
Case 2: The 10</span><span class="superscript"><span class="koboSpan" id="kobo.112.1">th</span></span><span class="koboSpan" id="kobo.113.1"> last word to the 5</span><span class="superscript"><span class="koboSpan" id="kobo.114.1">th</span></span><span class="koboSpan" id="kobo.115.1"> last word
{F5, F6, F7, F8, F9, F10}
Case 3: All even positioned last words till the 10</span><span class="superscript"><span class="koboSpan" id="kobo.116.1">th</span></span><span class="koboSpan" id="kobo.117.1"> position
{F2, F4, F6, F8, F10}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.118.1">
     In this instance, we have the following training set (the words in parentheses indicate the word to be predicted by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.119.1">
      the model):
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.120.1">
R1: As the sun set over the horizon, the sky turned a fiery (orange).
</span><span class="koboSpan" id="kobo.120.2">R2: As the sun set over the horizon, the clouds glowed with a golden (hue).
</span><span class="koboSpan" id="kobo.120.3">R3: As the sun set over the horizon, the ocean shimmered with reflected (light).
</span><span class="koboSpan" id="kobo.120.4">R4: As the sun set over the horizon, the landscape transformed into a (silhouette).</span></pre>
   <p>
    <span class="koboSpan" id="kobo.121.1">
     Now, let’s see what the dataset would look like once feature transformation has been performed for the
    </span>
    <a id="_idIndexMarker216">
    </a>
    <span class="koboSpan" id="kobo.122.1">
     three
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.123.1">
      use cases.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.124.1">
     Here’s
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.125.1">
       Case 1
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.126.1">
      :
     </span>
    </span>
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table002">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.127.1">
         R
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.128.1">
          F10
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.129.1">
          F9
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.130.1">
          F8
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.131.1">
          F7
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.132.1">
          F6
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.133.1">
          F5
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.134.1">
          F4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.135.1">
          F3
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.136.1">
          F2
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.137.1">
          F1
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.138.1">
          R1
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.139.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.140.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.141.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.142.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.143.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.144.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.145.1">
          sky
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.146.1">
          turned
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.147.1">
         a
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.148.1">
          fiery
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.149.1">
          R2
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.150.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.151.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.152.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.153.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.154.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.155.1">
          clouds
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.156.1">
          glowed
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.157.1">
          with
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.158.1">
         a
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.159.1">
          golden
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.160.1">
          R3
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.161.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.162.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.163.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.164.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.165.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.166.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.167.1">
          ocean
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.168.1">
          shimmed
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.169.1">
          with
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.170.1">
          reflected
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.171.1">
          R4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.172.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.173.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.174.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.175.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.176.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.177.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.178.1">
          landscape
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.179.1">
          transformed
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.180.1">
          into
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.181.1">
         a
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.182.1">
     Table 3.2 – Features with a lookback window of 10 (Case 1)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.183.1">
     This is
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.184.1">
       Case 2
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.185.1">
      :
     </span>
    </span>
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table003">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.186.1">
         R
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.187.1">
          F10
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.188.1">
          F9
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.189.1">
          F8
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.190.1">
          F7
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.191.1">
          F6
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.192.1">
          F5
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.193.1">
          R1
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.194.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.195.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.196.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.197.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.198.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.199.1">
          the
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.200.1">
          R2
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.201.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.202.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.203.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.204.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.205.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.206.1">
          clouds
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.207.1">
          R3
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.208.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.209.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.210.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.211.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.212.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.213.1">
          the
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.214.1">
          R4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.215.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.216.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.217.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.218.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.219.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.220.1">
          the
         </span>
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.221.1">
     Table 3.3 – Features with 10
    </span>
    <span class="superscript">
     <span class="koboSpan" id="kobo.222.1">
      th
     </span>
    </span>
    <span class="koboSpan" id="kobo.223.1">
     -last to 5
    </span>
    <span class="superscript">
     <span class="koboSpan" id="kobo.224.1">
      th
     </span>
    </span>
    <span class="koboSpan" id="kobo.225.1">
     -last window (Case 2)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.226.1">
     Finally, we have
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.227.1">
       Case 3
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.228.1">
      :
     </span>
    </span>
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table004">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.229.1">
         R
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.230.1">
          F10
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.231.1">
          F8
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.232.1">
          F6
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.233.1">
          F4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.234.1">
          F2
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.235.1">
          R1
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.236.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.237.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.238.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.239.1">
          sky
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.240.1">
         a
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.241.1">
          R2
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.242.1">
          set
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.243.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.244.1">
          the
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.245.1">
          glowed
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="koboSpan" id="kobo.246.1">
         a
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.247.1">
          R3
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.248.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.249.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.250.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.251.1">
          ocean
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.252.1">
          with
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.253.1">
          R4
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.254.1">
          sun
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.255.1">
          over
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.256.1">
          horizon
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.257.1">
          landscape
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.258.1">
          into
         </span>
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.259.1">
     Table 3.4 – Features with even indexed words (Case 3)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.260.1">
     In which case do you think the model would learn
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.261.1">
      something useful?
     </span>
    </span>
   </p>
   <p>
    <em class="italic">
     <span class="koboSpan" id="kobo.262.1">
      Case 1
     </span>
    </em>
    <span class="koboSpan" id="kobo.263.1">
     , without a doubt.
    </span>
    <span class="koboSpan" id="kobo.263.2">
     This example might be trivial, but it shows how decisions regarding feature selection heavily affect model performance down the pipeline.
    </span>
    <span class="koboSpan" id="kobo.263.3">
     Feature selection is a step that involves taking the raw data and deciding on the useful truths within it; a mistake in this step can
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.264.1">
      be devastating.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.265.1">
     With this understanding as to why feature selection is an important step, let’s go back to our discussion on representation learning.
    </span>
    <span class="koboSpan" id="kobo.265.2">
     Representation learning is encountered in almost all subdomains
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.266.1">
      of ML.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.267.1">
     In the domain of images, representation learning was initially explored along the lines of traditional statistical approaches
    </span>
    <a id="_idIndexMarker217">
    </a>
    <span class="koboSpan" id="kobo.268.1">
     such as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.269.1">
      principal component analysis
     </span>
    </strong>
    <span class="koboSpan" id="kobo.270.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.271.1">
      PCA
     </span>
    </strong>
    <span class="koboSpan" id="kobo.272.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.272.2">
     However, given that the data to be learned from was in the form of pixel data, or equivalently, a 2D matrix of float values, other ideas that were more optimized for the task were explored.
    </span>
    <span class="koboSpan" id="kobo.272.3">
     The most successful idea in this domain was the idea of
    </span>
    <a id="_idIndexMarker218">
    </a>
    <span class="koboSpan" id="kobo.273.1">
     using
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.274.1">
      convolutional filters
     </span>
    </strong>
    <span class="koboSpan" id="kobo.275.1">
     to extract meaningful patterns from the data matrix.
    </span>
    <span class="koboSpan" id="kobo.275.2">
     This was the fundamental driving force behind most of the ML tasks that were conducted on images in the
    </span>
    <a id="_idIndexMarker219">
    </a>
    <span class="koboSpan" id="kobo.276.1">
     modern day.
    </span>
    <span class="koboSpan" id="kobo.276.2">
     The
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.277.1">
      convolutional neural network
     </span>
    </strong>
    <span class="koboSpan" id="kobo.278.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.279.1">
      CNN
     </span>
    </strong>
    <span class="koboSpan" id="kobo.280.1">
     ) has its initial layers
    </span>
    <a id="_idIndexMarker220">
    </a>
    <span class="koboSpan" id="kobo.281.1">
     reliant on finding the filters with the right values so that meaningful patterns can be extracted from the image when they’re filtered
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.282.1">
      through it.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.283.1">
     In the domain of natural language, the key breakthrough in feature learning was understanding the dependence of every token on its previous tokens.
    </span>
    <span class="koboSpan" id="kobo.283.2">
     A representation needed to be formulated that learned as we passed each token while maintaining a state of the learned knowledge
    </span>
    <a id="_idIndexMarker221">
    </a>
    <span class="koboSpan" id="kobo.284.1">
     thus far.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.285.1">
      Recurrent neural networks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.286.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.287.1">
      RNNs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.288.1">
     ) maintain the concept of memory where, as the tokens are passed sequentially, the memory vector gets updated based on the memory state and the new token.
    </span>
    <span class="koboSpan" id="kobo.288.2">
     Other architectures, such as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.289.1">
      long short-term memory
     </span>
    </strong>
    <span class="koboSpan" id="kobo.290.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.291.1">
      LSTM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.292.1">
     ), improve
    </span>
    <a id="_idIndexMarker222">
    </a>
    <span class="koboSpan" id="kobo.293.1">
     upon the RNN model to support longer-range dependencies and interactions between tokens.
    </span>
    <span class="koboSpan" id="kobo.293.2">
     Other traditional methods have included algorithms
    </span>
    <a id="_idIndexMarker223">
    </a>
    <span class="koboSpan" id="kobo.294.1">
     such
    </span>
    <a id="_idIndexMarker224">
    </a>
    <span class="koboSpan" id="kobo.295.1">
     as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.296.1">
      GloVe
     </span>
    </strong>
    <span class="koboSpan" id="kobo.297.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.298.1">
      Word2Vec
     </span>
    </strong>
    <span class="koboSpan" id="kobo.299.1">
     , which are of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.300.1">
      shallow variant.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.301.1">
     In tabular data, there are several tried and tested approaches to feature manipulation that can be used to denoise and extract meaningful information from the data.
    </span>
    <span class="koboSpan" id="kobo.301.2">
     Common dimensionality reduction approaches such as
    </span>
    <a id="_idIndexMarker225">
    </a>
    <span class="koboSpan" id="kobo.302.1">
     PCA and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.303.1">
      encoder-decoder networks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.304.1">
     have proven effective in the industry.
    </span>
    <span class="koboSpan" id="kobo.304.2">
     Using matrix factorization methods over incomplete and sparse interaction matrices to generate embeddings has been a very effective first step in building business-facing technologies such as search and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.305.1">
      recommendation engines.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.306.1">
     The list of innovations in the field of representation learning is endless.
    </span>
    <span class="koboSpan" id="kobo.306.2">
     The key takeaway is that representation learning is strongly tied to the domain of the problem being tackled.
    </span>
    <span class="koboSpan" id="kobo.306.3">
     Effectively, it’s a way to guide the algorithm so that it uses more effective techniques instead of a generic architecture, and it doesn’t exploit the invariant patterns of the
    </span>
    <a id="_idIndexMarker226">
    </a>
    <span class="koboSpan" id="kobo.307.1">
     underlying data it’s trying to learn from.
    </span>
   </p>
   <h1 id="_idParaDest-63">
    <a id="_idTextAnchor066">
    </a>
    <span class="koboSpan" id="kobo.308.1">
     Graph representation learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.309.1">
     In the previous
    </span>
    <a id="_idIndexMarker227">
    </a>
    <span class="koboSpan" id="kobo.310.1">
     section, we talked about the need to perform representation learning on different types of data, such as images, tabular, and text.
    </span>
    <span class="koboSpan" id="kobo.310.2">
     In this section, we’ll try to extend this idea to graph data.
    </span>
    <span class="koboSpan" id="kobo.310.3">
     Graph data, theoretically, is more expressive than all the other data representation methods we’ve dealt with so far (such as matrices for images, word tokens, and tables).
    </span>
    <span class="koboSpan" id="kobo.310.4">
     With this expressivity comes the added challenge of finding a representation framework that captures relevant information, even though fewer constraints are enforced in the data representation itself.
    </span>
    <span class="koboSpan" id="kobo.310.5">
     Words in text are sequential, pixels in images are represented as 2D matrices, and tabular data assumes independence of rows (most of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.311.1">
      the time).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.312.1">
     Such inherent patterns in data allow us to exploit it during the representation learning step (think skip-grams for words and convolutional filters for images).
    </span>
    <span class="koboSpan" id="kobo.312.2">
     However, the constraints in graphs are very loose – so loose in fact that there is no obvious pattern to be exploited.
    </span>
    <span class="koboSpan" id="kobo.312.3">
     There are two primary challenges that graph data poses compared to other forms
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.313.1">
      of data:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.314.1">
       Increased computational costs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.315.1">
      : One of the goals of representation learning is to output
     </span>
     <a id="_idIndexMarker228">
     </a>
     <span class="koboSpan" id="kobo.316.1">
      features that make the patterns more obvious for the model to learn from.
     </span>
     <span class="koboSpan" id="kobo.316.2">
      As discussed in
     </span>
     <a href="B22118_01.xhtml#_idTextAnchor014">
      <span class="No-Break">
       <em class="italic">
        <span class="koboSpan" id="kobo.317.1">
         Chapter 1
        </span>
       </em>
      </span>
     </a>
     <span class="koboSpan" id="kobo.318.1">
      , there are several useful properties in graphs that can be exploited to make quick inferences.
     </span>
     <span class="koboSpan" id="kobo.318.2">
      Understanding the shortest path between two nodes can be a useful feature to add to the representation, but computing that takes at least
     </span>
     <span class="koboSpan" id="kobo.319.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/82.png" style="vertical-align:-0.195em;height:0.897em;width:2.956em"/>
     </span>
     <span class="koboSpan" id="kobo.320.1">
      time on average.
     </span>
     <span class="koboSpan" id="kobo.320.2">
      Pattern mining using traditional techniques is a difficult process to undertake when we try to apply ML
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.321.1">
       to graphs.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.322.1">
       It’s difficult to attack the problem in a distributed manner
      </span>
     </strong>
     <span class="koboSpan" id="kobo.323.1">
      : Modern ML pipelines are performant because of their ability to horizontally scale as the volume of data grows.
     </span>
     <span class="koboSpan" id="kobo.323.2">
      Traditional data representations of images, texts, and tabular data can be easily parallelized owing to their inherent independence within data elements.
     </span>
     <span class="koboSpan" id="kobo.323.3">
      Graph data is difficult to break down into smaller chunks, where one chunk has no dependence on another (essentially, this is graph partitioning).
     </span>
     <span class="koboSpan" id="kobo.323.4">
      This is the biggest bottleneck and is a point of active research for making graph-based ML solutions feasible for production use cases.
     </span>
     <span class="koboSpan" id="kobo.323.5">
      One great resource on this is
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.324.1">
       Graph neural networks meet with distributed graph partitioners and reconciliations
      </span>
     </em>
     <span class="koboSpan" id="kobo.325.1">
      by Mu et al.
     </span>
     <span class="koboSpan" id="kobo.325.2">
      (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.326.1">
       2023,
      </span>
     </span>
     <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222011894">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.327.1">
        https://www.sciencedirect.com/science/article/abs/pii/S0925231222011894
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.328.1">
       ).
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.329.1">
     Given these issues, it was
    </span>
    <a id="_idIndexMarker229">
    </a>
    <span class="koboSpan" id="kobo.330.1">
     quickly understood that traditional representation learning approaches that worked on other kinds of data wouldn’t be useful for graph data.
    </span>
    <span class="koboSpan" id="kobo.330.2">
     There needed to be a fresh approach tailored toward graph data.
    </span>
    <span class="koboSpan" id="kobo.330.3">
     Even though the differences that we’ve mentioned concerning the problems in representation learning in graph data versus traditional data exist, graph representation learning maintains a partially common objective regarding representation learning in other domains.
    </span>
    <span class="koboSpan" id="kobo.330.4">
     The objective of graph representation learning includes finding a representation mechanism that will reduce noise in the truth data, as well as highlight patterns that exist in the graph.
    </span>
    <span class="koboSpan" id="kobo.330.5">
     The representation space also needs to be friendly toward modern-day ML algorithms so that gradients and scalar products can be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.331.1">
      easily computed.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.332.1">
     Keeping all these objectives and constraints in mind, graph representation learning tries to find a transformation so that the following two goals can
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.333.1">
      be achieved:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.334.1">
       The representations capture the relationship between the nodes
      </span>
     </strong>
     <span class="koboSpan" id="kobo.335.1">
      .
     </span>
     <span class="koboSpan" id="kobo.335.2">
      If two nodes are connected by an edge, the distance between the representations of these two nodes should be small.
     </span>
     <span class="koboSpan" id="kobo.335.3">
      This concept should also hold to higher orders.
     </span>
     <span class="koboSpan" id="kobo.335.4">
      So, if a subgraph is densely connected, the representations of the nodes of that subgraph should also form a dense cluster in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.336.1">
       representation space.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.337.1">
       The representations are optimized so that they solve the inference problem being tackled
      </span>
     </strong>
     <span class="koboSpan" id="kobo.338.1">
      .
     </span>
     <span class="koboSpan" id="kobo.338.2">
      Popular graph problems include node classification, link prediction, and important node identification.
     </span>
     <span class="koboSpan" id="kobo.338.3">
      Without this goal, the graph representation would remain too generic to solve interesting problems that lead to a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.339.1">
       business lift.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.340.1">
     In conclusion, graph representation learning involves finding embeddings (or vectors) for every node of the graph so that vital information about the graph structure and information relevant
    </span>
    <a id="_idIndexMarker230">
    </a>
    <span class="koboSpan" id="kobo.341.1">
     to the inference problem can be captured in the embedding space.
    </span>
    <span class="koboSpan" id="kobo.341.2">
     The first goal is often called the reconstruction constraint in this domain, whereby you can (to a certain degree) reconstruct the graph data, given the node embeddings.
    </span>
    <span class="koboSpan" id="kobo.341.3">
     Before we delve deeper into various graph representation learning approaches, we must clarify
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.342.1">
      two things:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.343.1">
      Graph data is rarely just a set of
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.344.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.345.1">
      (
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.346.1">
       V
      </span>
     </em>
     <span class="koboSpan" id="kobo.347.1">
      ) and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.348.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.349.1">
      (
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.350.1">
       E
      </span>
     </em>
     <span class="koboSpan" id="kobo.351.1">
      ).
     </span>
     <span class="koboSpan" id="kobo.351.2">
      Often, all the nodes (and possibly the edges) contain side information, meaning additional fields are used to describe the nature of the nodes and edges.
     </span>
     <span class="koboSpan" id="kobo.351.3">
      Modern representation techniques must also have a way to incorporate this information in the embeddings since they’re often pivotal in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.352.1">
       inference task.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.353.1">
      Graph representation learning is a ubiquitous step in graph learning.
     </span>
     <span class="koboSpan" id="kobo.353.2">
      As with traditional ML approaches, some approaches involve distinct steps of learning the representation embeddings, followed by learning the model that’s being used for inference.
     </span>
     <span class="koboSpan" id="kobo.353.3">
      Other approaches combine both steps, where the embedding step is handled internally within the model that’s responsible for outputting the inference output.
     </span>
     <span class="koboSpan" id="kobo.353.4">
      In the world of graph learning, the same two
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.354.1">
       approaches apply.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.355.1">
     Now, let’s dive into some graph representation learning approaches.
    </span>
    <span class="koboSpan" id="kobo.355.2">
     There has been substantial work on graph representation learning, and different approaches have tried to tackle different aspects of the problem.
    </span>
    <span class="koboSpan" id="kobo.355.3">
     Without diving too deep into the chronological development of algorithms in this field, we’ll discuss the most relevant approaches in the
    </span>
    <a id="_idIndexMarker231">
    </a>
    <span class="koboSpan" id="kobo.356.1">
     industry today.
    </span>
    <span class="koboSpan" id="kobo.356.2">
     All
    </span>
    <a id="_idIndexMarker232">
    </a>
    <span class="koboSpan" id="kobo.357.1">
     approaches can be segregated into two concepts:
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.358.1">
      shallow encodings
     </span>
    </strong>
    <span class="koboSpan" id="kobo.359.1">
     and
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.360.1">
       deep encodings
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.361.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.362.1">
     To explain the difference in a single statement, encodings are
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.363.1">
      shallow
     </span>
    </em>
    <span class="koboSpan" id="kobo.364.1">
     when only the fields of the node embeddings need to be estimated; if more parameters need to be estimated alongside the fields of the node embeddings, then the encoding method is called
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.365.1">
       deep
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.366.1">
      encoding.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.367.1">
     The remainder of this chapter will primarily focus on the most popular shallow encoding algorithms in graph
    </span>
    <a id="_idIndexMarker233">
    </a>
    <span class="koboSpan" id="kobo.368.1">
     representation learning: DeepWalk and Node2Vec.
    </span>
    <span class="koboSpan" id="kobo.368.2">
     The different GNN architectures that will be mentioned in the following chapters are examples of deep
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.369.1">
      encoding techniques.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-64">
    <a id="_idTextAnchor067">
    </a>
    <span class="koboSpan" id="kobo.370.1">
     A framework for graph learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.371.1">
     If we take a holistic
    </span>
    <a id="_idIndexMarker234">
    </a>
    <span class="koboSpan" id="kobo.372.1">
     view of the approaches that are followed for learning inference models on graphs, we’ll notice a pattern.
    </span>
    <span class="koboSpan" id="kobo.372.2">
     Every solution can be divided into three
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.373.1">
      distinct steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.374.1">
      The first step involves coming up with a mechanism to find a
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.375.1">
       local subgraph
      </span>
     </strong>
     <span class="koboSpan" id="kobo.376.1">
      , given a node in the graph.
     </span>
     <span class="koboSpan" id="kobo.376.2">
      This term needs to be defined here.
     </span>
     <span class="koboSpan" id="kobo.376.3">
      For example, the graph containing all the nodes that are directly connected to an edge of the concerned node can be a local subgraph.
     </span>
     <span class="koboSpan" id="kobo.376.4">
      Another example can be the set of nodes that have a first or second-degree connection to the concerned node.
     </span>
     <span class="koboSpan" id="kobo.376.5">
      This local subgraph is often called the receptive field of the concerned node in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.377.1">
       academic literature.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.378.1">
      The second step involves a mechanism that takes input from the concerned node and its receptive field and outputs the node embedding.
     </span>
     <span class="koboSpan" id="kobo.378.2">
      The node embedding is simply a vector of real values of a certain dimension.
     </span>
     <span class="koboSpan" id="kobo.378.3">
      It’s important to have a similarity metric defined in this metric space.
     </span>
     <span class="koboSpan" id="kobo.378.4">
      A low similarity score between two vectors suggests that they are close to each other in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.379.1">
       this space.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.380.1">
      The final step involves defining a learning objective.
     </span>
     <span class="koboSpan" id="kobo.380.2">
      A learning objective is a function that tries to emulate what the learned node embeddings need to be optimized for.
     </span>
     <span class="koboSpan" id="kobo.380.3">
      This can involve trying to mimic the graph structure, where if nodes are connected by edges, their embeddings will be more similar to each other.
     </span>
     <span class="koboSpan" id="kobo.380.4">
      Alternatively, it could involve some graph inference tasks, such as optimizing the embeddings so that the nodes can be
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.381.1">
       classified correctly.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.382.1">
     This framework should always be in the back of your mind while you go through the rest of this book.
    </span>
    <span class="koboSpan" id="kobo.382.2">
     Several algorithms and learning models will be outlined in the following few chapters, and you should always be looking to answer the question of how different components of the
    </span>
    <a id="_idIndexMarker235">
    </a>
    <span class="koboSpan" id="kobo.383.1">
     algorithm conform to this framework.
    </span>
    <span class="koboSpan" id="kobo.383.2">
     With this knowledge at hand, let’s look at the DeepWalk algorithm and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.384.1">
      its varieties.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-65">
    <a id="_idTextAnchor068">
    </a>
    <span class="koboSpan" id="kobo.385.1">
     DeepWalk
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.386.1">
     DeepWalk is a learning
    </span>
    <a id="_idIndexMarker236">
    </a>
    <span class="koboSpan" id="kobo.387.1">
     technique that falls under the subcategory of algorithms where a random walk must be performed over the graph to find optimal embeddings.
    </span>
    <span class="koboSpan" id="kobo.387.2">
     To make sense of random walk-based learning techniques, let’s rewind and pick up from where the previous section left off.
    </span>
    <span class="koboSpan" id="kobo.387.3">
     Remember that the objective of this exercise is to come up with an embedding for every node in the graph so that pairs of embeddings are very similar in the vector space if – and only if – the nodes these embeddings represent are also very similar in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.388.1">
      the graph.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.389.1">
     To achieve this goal, we must define what
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.390.1">
      similar
     </span>
    </em>
    <span class="koboSpan" id="kobo.391.1">
     means in both the vector space and the graph.
    </span>
    <span class="koboSpan" id="kobo.391.2">
     Similarity in the vector space is often defined using the cosine similarity function (other similarity functions can also be used, such as L1-similarity, but for the graph use case, cosine similarity remains the most popular).
    </span>
    <span class="koboSpan" id="kobo.391.3">
     Let’s start by defining
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.392.1">
      cosine similarity.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.393.1">
     Let’s say we have the embeddings of two nodes,
    </span>
    <span class="koboSpan" id="kobo.394.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/83.png" style="vertical-align:-0.333em;height:0.781em;width:0.803em"/>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.395.1">
      and
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.396.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/84.png" style="vertical-align:-0.333em;height:0.781em;width:0.803em"/>
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.397.1">
      :
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.398.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/85.png" style="vertical-align:-0.528em;height:1.563em;width:9.588em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.399.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/86.png" style="vertical-align:-0.528em;height:1.563em;width:9.464em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.400.1">
     In this case, the cosine similarity function,
    </span>
    <span class="koboSpan" id="kobo.401.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/87.png" style="vertical-align:-0.578em;height:1.242em;width:4.372em"/>
    </span>
    <span class="koboSpan" id="kobo.402.1">
     , is defined
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.403.1">
      as follows:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.404.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;cos&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/88.png" style="vertical-align:-1.171em;height:2.607em;width:18.151em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.405.1">
     The interpretation is well-studied in the field of ML, so not much has been elaborated here.
    </span>
    <span class="koboSpan" id="kobo.405.2">
     In short, you can think of cosine similarity as the score of how similarly oriented the two vectors are in the vector space.
    </span>
    <span class="koboSpan" id="kobo.405.3">
     Vectors that point almost in a similar direction will have a cosine similarity score closer to 1, while two vectors that are perpendicular to each other will have a score closer
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.406.1">
      to 0.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.407.1">
     The easy part was defining the similarity score between the embeddings.
    </span>
    <span class="koboSpan" id="kobo.407.2">
     Now, we need to come up with
    </span>
    <a id="_idIndexMarker237">
    </a>
    <span class="koboSpan" id="kobo.408.1">
     a similarity score between the nodes of the graph.
    </span>
   </p>
   <h2 id="_idParaDest-66">
    <a id="_idTextAnchor069">
    </a>
    <span class="koboSpan" id="kobo.409.1">
     Random walk – the what and the why
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.410.1">
     How do we define the
    </span>
    <a id="_idIndexMarker238">
    </a>
    <span class="koboSpan" id="kobo.411.1">
     similarity between two nodes in a graph?
    </span>
    <span class="koboSpan" id="kobo.411.2">
     Could it be as simple as saying, assign a score of 1 if they are directly connected by an edge; if they are not directly connected but share common neighbors, assign a score equal to the natural logarithm of the number of common neighbors; if neither condition is met, assign a score
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.412.1">
      of 0.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.413.1">
     A lot of thought can be put into finding the best definition of similarity between the two nodes, and you can imagine how this definition would need to approximate a key structural property of the graph.
    </span>
    <span class="koboSpan" id="kobo.413.2">
     All heuristics that would try to define the similarity score between the nodes would have advantages and disadvantages.
    </span>
    <span class="koboSpan" id="kobo.413.3">
     However, there’s one idea we can implement that’s simple and mathematically
    </span>
    <a id="_idIndexMarker239">
    </a>
    <span class="koboSpan" id="kobo.414.1">
     elegant:
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.415.1">
       random walks
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.416.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.417.1">
     The idea of random walks is quite self-explanatory.
    </span>
    <span class="koboSpan" id="kobo.417.2">
     This algorithm is used to find a neighborhood for some node in the graph, say
    </span>
    <span class="koboSpan" id="kobo.418.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/89.png" style="vertical-align:-0.012em;height:0.460em;width:0.446em"/>
    </span>
    <span class="koboSpan" id="kobo.419.1">
     .
    </span>
    <span class="koboSpan" id="kobo.419.2">
     Based on some strategy, say
    </span>
    <span class="koboSpan" id="kobo.420.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/90.png" style="vertical-align:-0.008em;height:0.656em;width:0.598em"/>
    </span>
    <span class="koboSpan" id="kobo.421.1">
     (we’ll explain what strategies mean in a bit), we try to find the elements that would be part of the neighborhood of
    </span>
    <span class="koboSpan" id="kobo.422.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/91.png" style="vertical-align:-0.012em;height:0.460em;width:0.444em"/>
    </span>
    <span class="koboSpan" id="kobo.423.1">
     ; here, the neighborhood is
    </span>
    <span class="koboSpan" id="kobo.424.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/92.png" style="vertical-align:-0.338em;height:1.184em;width:2.402em"/>
    </span>
    <span class="koboSpan" id="kobo.425.1">
     .
    </span>
    <span class="koboSpan" id="kobo.425.2">
     The following steps explain
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.426.1">
      the algorithm:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.427.1">
      Starting from
     </span>
     <span class="koboSpan" id="kobo.428.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/89.png" style="vertical-align:-0.012em;height:0.460em;width:0.465em"/>
     </span>
     <span class="koboSpan" id="kobo.429.1">
      , we ask the strategy,
     </span>
     <span class="koboSpan" id="kobo.430.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/4.png" style="vertical-align:-0.008em;height:0.656em;width:0.620em"/>
     </span>
     <span class="koboSpan" id="kobo.431.1">
      , to decide (with some degree of randomness) which node that’s connected to
     </span>
     <span class="koboSpan" id="kobo.432.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/95.png" style="vertical-align:-0.012em;height:0.460em;width:0.433em"/>
     </span>
     <span class="koboSpan" id="kobo.433.1">
      we should jump to.
     </span>
     <span class="koboSpan" id="kobo.433.2">
      Let that node be called
     </span>
     <span class="koboSpan" id="kobo.434.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/96.png" style="vertical-align:-0.333em;height:0.781em;width:0.770em"/>
     </span>
     <span class="koboSpan" id="kobo.435.1">
      .
     </span>
     <span class="koboSpan" id="kobo.435.2">
      Add
     </span>
     <span class="koboSpan" id="kobo.436.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/97.png" style="vertical-align:-0.333em;height:0.781em;width:0.770em"/>
     </span>
     <span class="koboSpan" id="kobo.437.1">
      to the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.438.1">
       set,
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.439.1">
       <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/92.png" style="vertical-align:-0.338em;height:1.184em;width:2.402em"/>
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.440.1">
       .
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.441.1">
      Repeat
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.442.1">
       Step 1
      </span>
     </em>
     <span class="koboSpan" id="kobo.443.1">
      , but start from
     </span>
     <span class="koboSpan" id="kobo.444.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/99.png" style="vertical-align:-0.333em;height:0.781em;width:0.812em"/>
     </span>
     <span class="koboSpan" id="kobo.445.1">
      instead of
     </span>
     <span class="koboSpan" id="kobo.446.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/89.png" style="vertical-align:-0.012em;height:0.460em;width:0.450em"/>
     </span>
     <span class="koboSpan" id="kobo.447.1">
      .
     </span>
     <span class="koboSpan" id="kobo.447.2">
      The output of the strategy would be
     </span>
     <span class="koboSpan" id="kobo.448.1">
      <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/101.png" style="vertical-align:-0.333em;height:0.781em;width:0.812em"/>
     </span>
     <span class="koboSpan" id="kobo.449.1">
      , and that would be appended to
     </span>
     <span class="koboSpan" id="kobo.450.1">
      <img alt="&lt;math &gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/102.png" style="vertical-align:-0.338em;height:1.184em;width:2.685em"/>
     </span>
     <span class="koboSpan" id="kobo.451.1">
      .
     </span>
     <span class="koboSpan" id="kobo.451.2">
      Repeat this for a fixed number of steps until you have enough entries in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.452.1">
       your set.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.453.1">
     That’s it!
    </span>
    <span class="koboSpan" id="kobo.453.2">
     We’re pretty much done with the random walk step of the algorithm.
    </span>
    <span class="koboSpan" id="kobo.453.3">
     Before we consider how random walks generate neighborhoods, let’s discuss strategies.
    </span>
    <span class="koboSpan" id="kobo.453.4">
     The strategy,
    </span>
    <span class="koboSpan" id="kobo.454.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/4.png" style="vertical-align:-0.008em;height:0.656em;width:0.612em"/>
    </span>
    <span class="koboSpan" id="kobo.455.1">
     , decides which node should we jump to from the previous node.
    </span>
    <span class="koboSpan" id="kobo.455.2">
     The simplest strategy is a random choice where, given all the connected nodes, you choose a node at random with the same probability as all others.
    </span>
    <span class="koboSpan" id="kobo.455.3">
     Other strategies can be employed as well, where, for example, the unvisited nodes can be given more bias, instead of us adding the same node to the neighborhood repeatedly.
    </span>
    <span class="koboSpan" id="kobo.455.4">
     The different choice of strategies often ends up being the differentiating factor between different shallow embedding learning algorithms in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.456.1">
      this class.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.457.1">
     Why random walks?
    </span>
    <span class="koboSpan" id="kobo.457.2">
     Random walks are a good way to sample the important features of the graph efficiently.
    </span>
    <span class="koboSpan" id="kobo.457.3">
     First, if we notice that a node occurs within the random walk neighborhood of another node with high probability, we can probably conclude they’re supposed to be very similar to each other.
    </span>
    <span class="koboSpan" id="kobo.457.4">
     Such a technique doesn’t rely on hacky heuristics, which are often limited by the degree of connections.
    </span>
    <span class="koboSpan" id="kobo.457.5">
     Random walk-based sampling doesn’t need to worry about the degree of connections to figure out the best candidates to be part of the neighborhood at a statistical level.
    </span>
    <span class="koboSpan" id="kobo.457.6">
     Second, it’s a quick and efficient way to sample.
    </span>
    <span class="koboSpan" id="kobo.457.7">
     The training step doesn’t need to evaluate across all the nodes in the graph; it just
    </span>
    <a id="_idIndexMarker240">
    </a>
    <span class="koboSpan" id="kobo.458.1">
     needs to concern itself with the ones in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.459.1">
      the
     </span>
    </span>
    <span class="No-Break">
     <a id="_idIndexMarker241">
     </a>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.460.1">
      neighborhood.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.461.1">
     In the next subsection, we’ll build on our understanding of random walks and use it to come up with a method of estimating the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.462.1">
      embedding components.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-67">
    <a id="_idTextAnchor070">
    </a>
    <span class="koboSpan" id="kobo.463.1">
     Estimating the node embeddings
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.464.1">
     Now that
    </span>
    <a id="_idIndexMarker242">
    </a>
    <span class="koboSpan" id="kobo.465.1">
     we’ve
    </span>
    <a id="_idIndexMarker243">
    </a>
    <span class="koboSpan" id="kobo.466.1">
     learned the neighborhood set
    </span>
    <span class="koboSpan" id="kobo.467.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/104.png" style="vertical-align:-0.338em;height:1.184em;width:2.388em"/>
    </span>
    <span class="koboSpan" id="kobo.468.1">
     for the node,
    </span>
    <span class="koboSpan" id="kobo.469.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/95.png" style="vertical-align:-0.012em;height:0.460em;width:0.434em"/>
    </span>
    <span class="koboSpan" id="kobo.470.1">
     , let’s understand how it’s relevant to the task of learning node embeddings.
    </span>
    <span class="koboSpan" id="kobo.470.2">
     Recall that before we started talking about random walks, our subproblem of concern was to find a way to estimate the similarity of two nodes in a graph.
    </span>
    <span class="koboSpan" id="kobo.470.3">
     Coming up with a similarity function over the nodes of a graph is a tough ask, but can we make some guided assumptions about this similarity score?
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.471.1">
     An important assumption could be as follows: “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.472.1">
      When two nodes are similar, one node likely lies within the neighborhood of the other.
     </span>
    </em>
    <span class="koboSpan" id="kobo.473.1">
     ” If we assume this statement to be true, we’ve pretty much solved our problem.
    </span>
    <span class="koboSpan" id="kobo.473.2">
     Now, we can leverage this assumption to come up with the node embeddings.
    </span>
    <span class="koboSpan" id="kobo.473.3">
     Without delving deep into probability magic (such as that of likelihood functions, and so on), the crux of the idea is that if this assumption is true in the graph space, then it must also be true in the vector space where the embeddings are defined.
    </span>
    <span class="koboSpan" id="kobo.473.4">
     If that’s the case, we need to find the embedding fields so that the likelihood function is maximized whenever one node is within the neighborhood of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.474.1">
      the other.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.475.1">
     So, if
    </span>
    <span class="koboSpan" id="kobo.476.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/106.png" style="vertical-align:-0.340em;height:0.802em;width:0.635em"/>
    </span>
    <span class="koboSpan" id="kobo.477.1">
     is an embedding vector of node
    </span>
    <span class="koboSpan" id="kobo.478.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/45.png" style="vertical-align:-0.012em;height:0.460em;width:0.438em"/>
    </span>
    <span class="koboSpan" id="kobo.479.1">
     , we can find the components of
    </span>
    <span class="koboSpan" id="kobo.480.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/108.png" style="vertical-align:-0.340em;height:0.802em;width:0.699em"/>
    </span>
    <span class="koboSpan" id="kobo.481.1">
     so that the following value is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.482.1">
      being maximized:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.483.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/109.png" style="vertical-align:-0.962em;height:1.953em;width:10.087em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.484.1">
     The negative log of the probability is a form of the log-likelihood function.
    </span>
    <span class="koboSpan" id="kobo.484.2">
     Now, since this must be true across all the nodes and their embeddings, we must optimize the components while keeping all the nodes in mind,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.485.1">
      essentially maximizing:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.486.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/110.png" style="vertical-align:-0.889em;height:1.953em;width:14.053em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.487.1">
     The final step is to
    </span>
    <a id="_idIndexMarker244">
    </a>
    <span class="koboSpan" id="kobo.488.1">
     tie the probability function to the similarity
    </span>
    <a id="_idIndexMarker245">
    </a>
    <span class="koboSpan" id="kobo.489.1">
     function we defined previously.
    </span>
    <span class="koboSpan" id="kobo.489.2">
     A common way of creating a probability distribution within an embedding space is to use a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.490.1">
      softmax
     </span>
    </strong>
    <span class="koboSpan" id="kobo.491.1">
     function, which converts a function into a probability
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.492.1">
      density function:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.493.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/111.png" style="vertical-align:-0.993em;height:2.513em;width:21.133em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.494.1">
     Here,
    </span>
    <span class="koboSpan" id="kobo.495.1">
     <img alt="&lt;math &gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/112.png" style="vertical-align:-0.390em;height:1.030em;width:4.144em"/>
    </span>
    <span class="koboSpan" id="kobo.496.1">
     is simply the cosine similarity function we defined previously.
    </span>
    <span class="koboSpan" id="kobo.496.2">
     By plugging this definition back into our optimization metric, we get the final parameterized form of the loss function that needs to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.497.1">
      be optimized:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.498.1">
     <img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/113.png" style="vertical-align:-0.889em;height:2.556em;width:16.626em"/>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.499.1">
     Using gradient descent, we’ll find the embeddings,
    </span>
    <span class="koboSpan" id="kobo.500.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/114.png" style="vertical-align:-0.020em;height:0.482em;width:0.415em"/>
    </span>
    <span class="koboSpan" id="kobo.501.1">
     , so that
    </span>
    <span class="koboSpan" id="kobo.502.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/115.png" style="vertical-align:-0.000em;height:0.648em;width:0.529em"/>
    </span>
    <span class="koboSpan" id="kobo.503.1">
     is maximized.
    </span>
    <span class="koboSpan" id="kobo.503.2">
     This will ensure we find the embeddings that satisfy
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.504.1">
      our criterion.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.505.1">
     Note that there are a few other steps in the optimization process that make the process computationally feasible, with one of the most important being the concept of negative sampling.
    </span>
    <span class="koboSpan" id="kobo.505.2">
     Here, instead of calculating the normalization component (the denominator) of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.506.1">
      softmax
     </span>
    </strong>
    <span class="koboSpan" id="kobo.507.1">
     function across all nodes on each iteration, we take a few random nodes that aren’t in the neighborhood of the concerned node and calculate the sum over that.
    </span>
    <span class="koboSpan" id="kobo.507.2">
     This type of optimization problem is
    </span>
    <a id="_idIndexMarker246">
    </a>
    <span class="koboSpan" id="kobo.508.1">
     called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.509.1">
      noise contrastive estimations
     </span>
    </strong>
    <span class="koboSpan" id="kobo.510.1">
     , and it’s a popular technique in NLP learning tasks.
    </span>
    <span class="koboSpan" id="kobo.510.2">
     It’s often
    </span>
    <a id="_idIndexMarker247">
    </a>
    <span class="koboSpan" id="kobo.511.1">
     called the
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.512.1">
       skip-gram model
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.513.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.514.1">
     As we conclude this section, it might be a good time to mention that the preceding algorithm in its entirety is termed
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.515.1">
      DeepWalk
     </span>
    </em>
    <span class="koboSpan" id="kobo.516.1">
     , as stated in the original paper, which was published in 2014 (
    </span>
    <a href="https://dl.acm.org/doi/10.1145/2623330.2623732">
     <span class="koboSpan" id="kobo.517.1">
      https://dl.acm.org/doi/10.1145/2623330.2623732
     </span>
    </a>
    <span class="koboSpan" id="kobo.518.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.518.2">
     The DeepWalk algorithm is an efficient process of estimating shallow encodings.
    </span>
    <span class="koboSpan" id="kobo.518.3">
     However, the simplicity of the approach is one of its major shortcomings: the random unbiased nature of the random walk strategy often wanders too far away from the concerned node, so it samples neighborhoods that aren’t very local to the node.
    </span>
    <span class="koboSpan" id="kobo.518.4">
     As a result, the embeddings aren’t optimized
    </span>
    <a id="_idIndexMarker248">
    </a>
    <span class="koboSpan" id="kobo.519.1">
     based on the most local information of
    </span>
    <a id="_idIndexMarker249">
    </a>
    <span class="koboSpan" id="kobo.520.1">
     the node.
    </span>
    <span class="koboSpan" id="kobo.520.2">
     Several other algorithms build on the work that was done in this paper.
    </span>
    <span class="koboSpan" id="kobo.520.3">
     We’ll talk about one such prominent improvement in the next section, known
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.521.1">
      as
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.522.1">
       Node2Vec.
      </span>
     </em>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.523.1">
     Here’s the pseudocode
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.524.1">
      for DeepWalk:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.525.1">
function DeepWalk(Graph G, walk_length L, num_walks R, dimensions d):
    Initialize walks = []
    for each node v in G:
        for i = 1 to R:
            walk = RandomWalk(G, v, L)
            append walk to walks
    model = Word2Vec(walks, dimensions=d)
    return model
function RandomWalk(Graph G, start_node v, length L):
    walk = [v]
    for i = 1 to L:
        neighbors = GetNeighbors(G, v)
        next_node = RandomChoice(neighbors)
        append next_node to walk
        v = next_node
    return walk</span></pre>
   <p>
    <span class="koboSpan" id="kobo.526.1">
     The preceding pseudocode outlines two
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.527.1">
      main functions:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.528.1">
      First,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.529.1">
       DeepWalk
      </span>
     </strong>
     <span class="koboSpan" id="kobo.530.1">
      generates multiple random walks for each node in the graph and uses Word2Vec to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.531.1">
       create embeddings
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.532.1">
      Second,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.533.1">
       RandomWalk
      </span>
     </strong>
     <span class="koboSpan" id="kobo.534.1">
      performs
     </span>
     <a id="_idIndexMarker250">
     </a>
     <span class="koboSpan" id="kobo.535.1">
      a single
     </span>
     <a id="_idIndexMarker251">
     </a>
     <span class="koboSpan" id="kobo.536.1">
      random walk, starting from a given node for a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.537.1">
       specified length
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-68">
    <a id="_idTextAnchor071">
    </a>
    <span class="koboSpan" id="kobo.538.1">
     Node2Vec
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.539.1">
     The DeepWalk algorithm uses
    </span>
    <a id="_idIndexMarker252">
    </a>
    <span class="koboSpan" id="kobo.540.1">
     unbiased randomized walks to generate the neighborhood of any concerned node.
    </span>
    <span class="koboSpan" id="kobo.540.2">
     Its unbiased nature ensures the graph structure is captured in the best possible manner statistically, but, in practice, this is often the less optimal choice.
    </span>
    <span class="koboSpan" id="kobo.540.3">
     The premise of Node2Vec is that we introduce bias in the random walk strategy to ensure that sampling is done in such a way that both the local and global structures of the graph are represented in the neighborhood.
    </span>
    <span class="koboSpan" id="kobo.540.4">
     Most of the other concepts in Node2Vec are the same as those for DeepWalk, including the learning objective and the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.541.1">
      optimization step.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.542.1">
     Before we delve into the nitty-gritty of the algorithm, let’s do a quick recap of graph
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.543.1">
      traversal approaches.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-69">
    <a id="_idTextAnchor072">
    </a>
    <span class="koboSpan" id="kobo.544.1">
     Graph traversal approaches
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.545.1">
     As we covered
    </span>
    <a id="_idIndexMarker253">
    </a>
    <span class="koboSpan" id="kobo.546.1">
     briefly in
    </span>
    <a href="B22118_01.xhtml#_idTextAnchor014">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.547.1">
        Chapter 1
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.548.1">
     , the two most popular
    </span>
    <a id="_idIndexMarker254">
    </a>
    <span class="koboSpan" id="kobo.549.1">
     graph traversal approaches
    </span>
    <a id="_idIndexMarker255">
    </a>
    <span class="koboSpan" id="kobo.550.1">
     are
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.551.1">
      breadth-first search
     </span>
    </strong>
    <span class="koboSpan" id="kobo.552.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.553.1">
      BFS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.554.1">
     ) and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.555.1">
      depth-first search
     </span>
    </strong>
    <span class="koboSpan" id="kobo.556.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.557.1">
      DFS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.558.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.558.2">
     BFS is the local first approach to graph
    </span>
    <a id="_idIndexMarker256">
    </a>
    <span class="koboSpan" id="kobo.559.1">
     exploration where, given a starting node, all first-degree connections are explored before we venture away from the starting node.
    </span>
    <span class="koboSpan" id="kobo.559.2">
     DFS, on the other hand, takes a global first approach to graph exploration, where the impetus is to explore as deep into the graph as possible before backtracking upon reaching a leaf node.
    </span>
    <span class="koboSpan" id="kobo.559.3">
     The random walk strategy that’s employed in the DeepWalk algorithm is statistically closer to the DFS approach than the BFS approach, which is why the local structure is under-represented in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.560.1">
      the neighborhood.
     </span>
    </span>
   </p>
   <p>
    <em class="italic">
     <span class="koboSpan" id="kobo.561.1">
      How can a random walk strategy emulate BFS
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.562.1">
       and DFS?
      </span>
     </em>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.563.1">
     First, let’s consider the case for DFS.
    </span>
    <span class="koboSpan" id="kobo.563.2">
     In a random walk process, when the current position is on some node, we have two choices: visit the last node that was visited or visit some other node.
    </span>
    <span class="koboSpan" id="kobo.563.3">
     If we can ensure that the first option happens only when the current node has no other connected nodes, then we can ensure the random walk follows
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.564.1">
      DFS traversal.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.565.1">
     For a random walk to emulate BFS, a bit more consideration is needed.
    </span>
    <span class="koboSpan" id="kobo.565.2">
     First, the random walk entity needs to keep track of which node it came from in the last step.
    </span>
    <span class="koboSpan" id="kobo.565.3">
     Now, from the current node, we have three options: go back to the previous node, go to a node that’s further away from the previous node, or go to a node that’s equidistant from the previous node as the current node is.
    </span>
    <span class="koboSpan" id="kobo.565.4">
     If we minimize the chances of the second option happening, we’re effectively left with a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.566.1">
      BFS traversal.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.567.1">
     So, we can add biases to the random walk algorithm to emulate the BFS and DFS traversal patterns.
    </span>
    <span class="koboSpan" id="kobo.567.2">
     With this knowledge, we can enforce finer control over the random walk so that the neighborhoods contain local structure and global structure representations with the ratio we’re
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.568.1">
      interested in.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-70">
    <a id="_idTextAnchor073">
    </a>
    <span class="koboSpan" id="kobo.569.1">
     Finalizing the random walk strategy
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.570.1">
     Let’s formalize
    </span>
    <a id="_idIndexMarker257">
    </a>
    <span class="koboSpan" id="kobo.571.1">
     the strategy mentioned
    </span>
    <a id="_idIndexMarker258">
    </a>
    <span class="koboSpan" id="kobo.572.1">
     previously.
    </span>
    <span class="koboSpan" id="kobo.572.2">
     For this, we’ll use two hyperparameters:
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.573.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.574.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.575.1">
      q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.576.1">
     .
    </span>
    <span class="koboSpan" id="kobo.576.2">
     The first hyperparameter,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.577.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.578.1">
     , is related to a weight that decides how likely the random walk will go back to the node it came from in the last step.
    </span>
    <span class="koboSpan" id="kobo.578.2">
     The second hyperparameter,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.579.1">
      q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.580.1">
     , decides how likely the walk will venture off to a node that’s further away from the previous node or not.
    </span>
    <span class="koboSpan" id="kobo.580.2">
     It can also be interpreted as a parameter that decides how much preference the BFS strategy gets over the DFS strategy.
    </span>
    <span class="koboSpan" id="kobo.580.3">
     The example in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.581.1">
       Figure 3
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.582.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.583.1">
     can
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.584.1">
      clarify this:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer154">
     <span class="koboSpan" id="kobo.585.1">
      <img alt="Figure 3.1 – A small graph showing the effect of the Node2Vec hyperparameters" src="image/B22118_03_1.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.586.1">
     Figure 3.1 – A small graph showing the effect of the Node2Vec hyperparameters
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.587.1">
     Take a look at this graph.
    </span>
    <span class="koboSpan" id="kobo.587.2">
     Here, the random walk has migrated from node
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.588.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.589.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.590.1">
     to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.591.1">
      n
     </span>
    </strong>
    <span class="koboSpan" id="kobo.592.1">
     in the last step.
    </span>
    <span class="koboSpan" id="kobo.592.2">
     In the current step, it needs to decide which node it should migrate to, with its options being
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.593.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.594.1">
      1
     </span>
    </span>
    <span class="koboSpan" id="kobo.595.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.596.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.597.1">
      2
     </span>
    </span>
    <span class="koboSpan" id="kobo.598.1">
     , or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.599.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.600.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.601.1">
     .
    </span>
    <span class="koboSpan" id="kobo.601.2">
     The probabilities of the next migration are decided by the hyperparameters,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.602.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.603.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.604.1">
      q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.605.1">
     .
    </span>
    <span class="koboSpan" id="kobo.605.2">
     Let
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.606.1">
      c
     </span>
    </strong>
    <span class="koboSpan" id="kobo.607.1">
     be the probability that the next migration is to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.608.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.609.1">
      1
     </span>
    </span>
    <span class="koboSpan" id="kobo.610.1">
     .
    </span>
    <span class="koboSpan" id="kobo.610.2">
     Then, the probability of migration to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.611.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.612.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.613.1">
     is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.614.1">
      c/p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.615.1">
     , while the probability of migration to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.616.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.617.1">
      2
     </span>
    </span>
    <span class="koboSpan" id="kobo.618.1">
     is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.619.1">
      c/q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.620.1">
     .
    </span>
    <span class="koboSpan" id="kobo.620.2">
     Here,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.621.1">
      c
     </span>
    </strong>
    <span class="koboSpan" id="kobo.622.1">
     should be such that the sum of all probabilities adds up
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.623.1">
      to 1.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.624.1">
     To clarify this, the values of the probabilities are the way they are because of what each node represents;
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.625.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.626.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.627.1">
     is the last visited node, which is why its visit is weighed additionally by the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.628.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.629.1">
     value.
    </span>
    <span class="koboSpan" id="kobo.629.2">
     Here,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.630.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.631.1">
      1
     </span>
    </span>
    <span class="koboSpan" id="kobo.632.1">
     is the BFS option since it is at the same distance from
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.633.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.634.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.635.1">
     as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.636.1">
      n
     </span>
    </strong>
    <span class="koboSpan" id="kobo.637.1">
     is, while
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.638.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.639.1">
      2
     </span>
    </span>
    <span class="koboSpan" id="kobo.640.1">
     is the DFS option since it is further away from
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.641.1">
      n
     </span>
    </strong>
    <span class="subscript">
     <span class="koboSpan" id="kobo.642.1">
      L
     </span>
    </span>
    <span class="koboSpan" id="kobo.643.1">
     .
    </span>
    <span class="koboSpan" id="kobo.643.2">
     This is why the ratio of their visits
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.644.1">
      is
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.645.1">
       q
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.646.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.647.1">
     With this strategy
    </span>
    <a id="_idIndexMarker259">
    </a>
    <span class="koboSpan" id="kobo.648.1">
     of assigning biases to each step of
    </span>
    <a id="_idIndexMarker260">
    </a>
    <span class="koboSpan" id="kobo.649.1">
     neighborhood creation, we can ensure the neighborhood consists of representatives from both the local and global context of the concerned node.
    </span>
    <span class="koboSpan" id="kobo.649.2">
     Note that this random walk strategy is called a random walk strategy of the second order since we need to maintain a state – that is, the knowledge of the previous state from where the walk
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.650.1">
      has migrated.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.651.1">
     Here’s the pseudocode
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.652.1">
      for Node2Vec:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.653.1">
function Node2Vec(Graph G, walk_length L, num_walks R, dimensions d, p, q):
    Initialize walks = []
    for each node v in G:
        for i = 1 to R:
            walk = BiasedRandomWalk(G, v, L, p, q)
            append walk to walks
    model = Word2Vec(walks, dimensions=d)
    return model
function BiasedRandomWalk(Graph G, start_node v, length L, p, q):
    walk = [v]
    for i = 1 to L:
        current = walk[-1]
        previous = walk[-2] if len(walk) &gt; 1 else None
        next_node = SampleNextNode(G, current, previous, p, q)
        append next_node to walk
    return walk</span></pre>
   <p>
    <span class="koboSpan" id="kobo.654.1">
     Node2Vec extends DeepWalk by introducing biased random walks that are controlled by the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.655.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.656.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.657.1">
      q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.658.1">
     parameters.
    </span>
    <span class="koboSpan" id="kobo.658.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.659.1">
      BiasedRandomWalk
     </span>
    </strong>
    <span class="koboSpan" id="kobo.660.1">
     function uses these parameters to balance between
    </span>
    <a id="_idIndexMarker261">
    </a>
    <span class="koboSpan" id="kobo.661.1">
     exploring local neighborhoods (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.662.1">
      q
     </span>
    </strong>
    <span class="koboSpan" id="kobo.663.1">
     ) and
    </span>
    <a id="_idIndexMarker262">
    </a>
    <span class="koboSpan" id="kobo.664.1">
     reaching farther nodes (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.665.1">
      p
     </span>
    </strong>
    <span class="koboSpan" id="kobo.666.1">
     ), allowing for a more flexible exploration of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.667.1">
      graph structure.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-71">
    <a id="_idTextAnchor074">
    </a>
    <span class="koboSpan" id="kobo.668.1">
     Node2Vec versus DeepWalk
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.669.1">
     The steps that
    </span>
    <a id="_idIndexMarker263">
    </a>
    <span class="koboSpan" id="kobo.670.1">
     follow
    </span>
    <a id="_idIndexMarker264">
    </a>
    <span class="koboSpan" id="kobo.671.1">
     are the same as the ones that were mentioned for DeepWalk.
    </span>
    <span class="koboSpan" id="kobo.671.2">
     We try to maximize the likelihood that the embeddings within the neighborhood of the concerned node are most similar to the concerned node.
    </span>
    <span class="koboSpan" id="kobo.671.3">
     This optimization step, when performed across all the nodes, gives us the optimal embeddings.
    </span>
    <span class="koboSpan" id="kobo.671.4">
     The difference from DeepWalk is in terms of what the embeddings are being optimized for.
    </span>
    <span class="koboSpan" id="kobo.671.5">
     In DeepWalk, the choice of neighbors for a node was different than it was for the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.672.1">
      Node2Vec scenario.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.673.1">
     With that, we’ve covered the two most popular shallow graph representation learning algorithms.
    </span>
    <span class="koboSpan" id="kobo.673.2">
     We’ve learned how DeepWalk and Node2Vec, two similar algorithms, can elegantly employ the random walk approach to generate shallow node embeddings.
    </span>
    <span class="koboSpan" id="kobo.673.3">
     However, we need to understand the limitations of such approaches since such restrictions will act
    </span>
    <a id="_idIndexMarker265">
    </a>
    <span class="koboSpan" id="kobo.674.1">
     as motivation for the topics that will be discussed
    </span>
    <a id="_idIndexMarker266">
    </a>
    <span class="koboSpan" id="kobo.675.1">
     later in this book to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.676.1">
      be used.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-72">
    <a id="_idTextAnchor075">
    </a>
    <span class="koboSpan" id="kobo.677.1">
     Limitations of shallow encodings
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.678.1">
     Shallow embeddings have
    </span>
    <a id="_idIndexMarker267">
    </a>
    <span class="koboSpan" id="kobo.679.1">
     the advantage of being easy to understand and relatively easy to implement.
    </span>
    <span class="koboSpan" id="kobo.679.2">
     However, they have several disadvantages, especially compared to deep
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.680.1">
      encoding techniques:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.681.1">
       This method can’t incorporate node or link-level features
      </span>
     </strong>
     <span class="koboSpan" id="kobo.682.1">
      .
     </span>
     <span class="koboSpan" id="kobo.682.2">
      When using graph data, it’s common to have auxiliary information attached to every node or every edge, to describe further properties.
     </span>
     <span class="koboSpan" id="kobo.682.3">
      By default, the random walk approaches aren’t capable of incorporating such information in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.683.1">
       their embeddings.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.684.1">
       They can be computationally expensive
      </span>
     </strong>
     <span class="koboSpan" id="kobo.685.1">
      .
     </span>
     <span class="koboSpan" id="kobo.685.2">
      How?
     </span>
     <span class="koboSpan" id="kobo.685.3">
      If we’re interested in embeddings of some dimension,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.686.1">
       d
      </span>
     </strong>
     <span class="koboSpan" id="kobo.687.1">
      , and the number of nodes is
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.688.1">
       v
      </span>
     </strong>
     <span class="koboSpan" id="kobo.689.1">
      , then we need to learn a total of
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.690.1">
       v.d
      </span>
     </strong>
     <span class="koboSpan" id="kobo.691.1">
      values.
     </span>
     <span class="koboSpan" id="kobo.691.2">
      Deep approaches with hidden layers would likely have much lower parameters to learn, making the process more
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.692.1">
       computationally efficient.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.693.1">
      Building from the previous point, since there are so many parameters to learn in this approach,
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.694.1">
       we often can’t utilize the advantages that come with making the representations denser
      </span>
     </strong>
     <span class="koboSpan" id="kobo.695.1">
      .
     </span>
     <span class="koboSpan" id="kobo.695.2">
      Denser representations (which use fewer parameters) can often reduce the amount of noise that’s learned.
     </span>
     <span class="koboSpan" id="kobo.695.3">
      Obviously, below a minimum threshold, the number of parameters would become too low to be able to effectively represent the complexity of the graph data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.696.1">
       being learned.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.697.1">
      Finally,
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.698.1">
       the learning approaches for shallow encodings provide no provision to incorporate the graph inference tasks within the learning problem
      </span>
     </strong>
     <span class="koboSpan" id="kobo.699.1">
      .
     </span>
     <span class="koboSpan" id="kobo.699.2">
      The embeddings are learned based on graph structure and are to be used for generic purposes, instead being optimized for one
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.700.1">
       inference task.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.701.1">
     Many of these limitations
    </span>
    <a id="_idIndexMarker268">
    </a>
    <span class="koboSpan" id="kobo.702.1">
     can be overcome with more sophisticated architectures that are optimized to learn
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.703.1">
      node embeddings.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-73">
    <a id="_idTextAnchor076">
    </a>
    <span class="koboSpan" id="kobo.704.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.705.1">
     In this chapter, we introduced graph representation learning, a fundamental concept in the domain of using ML on graph data.
    </span>
    <span class="koboSpan" id="kobo.705.2">
     First, we discussed what representation learning is, in the general sense in ML.
    </span>
    <span class="koboSpan" id="kobo.705.3">
     When concentrating solely on graphs, you learned that the primary objective of representation learning is to find embeddings that can emulate the structure of the graph, as well as learn important concepts that are necessary for the inference task,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.706.1">
      if any.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.707.1">
     We also explored DeepWalk and Node2Vec, two popular graph representation learning approaches, which comprise a class of algorithms that use random walks to generate a neighborhood for a node.
    </span>
    <span class="koboSpan" id="kobo.707.2">
     Based on this neighborhood, you can optimize the embedding values so that the embeddings of the nodes in the neighborhood are highly similar to those of the embeddings of the concerned node.
    </span>
    <span class="koboSpan" id="kobo.707.3">
     Finally, we looked at the drawbacks of using these approaches
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.708.1">
      in practice.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.709.1">
     In the next chapter, we’ll concentrate on the most popular deep learning architectures that can be used to learn graph node embeddings.
    </span>
    <span class="koboSpan" id="kobo.709.2">
     You’ll learn how such architectures exploit patterns in graphs while maintaining the invariants in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.710.1">
      graph data.
     </span>
    </span>
   </p>
  </div>
 

  <div class="Content" id="_idContainer156">
   <h1 id="_idParaDest-74" lang="en-US" xml:lang="en-US">
    <a id="_idTextAnchor077">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     Part 2: Advanced Graph Learning Techniques
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.2.1">
     In this part of the book, you will explore advanced concepts in graph learning, including deep learning architectures for graphs, common challenges in the field, and the integration of large language models.
    </span>
    <span class="koboSpan" id="kobo.2.2">
     You will learn about state-of-the-art approaches, technical challenges, and emerging solutions in graph-based
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.3.1">
      AI systems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.4.1">
     This part has the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.5.1">
      following chapters:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <a href="B22118_04.xhtml#_idTextAnchor078">
      <em class="italic">
       <span class="koboSpan" id="kobo.6.1">
        Chapter 4
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.7.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.8.1">
       Deep Learning Models for Graphs
      </span>
     </em>
    </li>
    <li>
     <a href="B22118_05.xhtml#_idTextAnchor093">
      <em class="italic">
       <span class="koboSpan" id="kobo.9.1">
        Chapter 5
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.10.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.11.1">
       Graph Deep Learning Challenges
      </span>
     </em>
    </li>
    <li>
     <a href="B22118_06.xhtml#_idTextAnchor118">
      <em class="italic">
       <span class="koboSpan" id="kobo.12.1">
        Chapter 6
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.13.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.14.1">
       Harnessing Large Language Models for Graph Learning
      </span>
     </em>
    </li>
   </ul>
  </div>
  <div>
   <div id="_idContainer157">
   </div>
  </div>
  <div>
   <div id="_idContainer158">
   </div>
  </div>
  <div>
   <div id="_idContainer159">
   </div>
  </div>
  <div>
   <div id="_idContainer160">
   </div>
  </div>
  <div>
   <div id="_idContainer161">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer162">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer163">
   </div>
  </div>
  <div>
   <div id="_idContainer164">
   </div>
  </div>
  <div>
   <div id="_idContainer165">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer166">
   </div>
  </div>
 </body></html>