["```py\n$ python \n>>> import torch \n>>> import numpy as np \n>>> a = torch.FloatTensor(3, 2) \n>>> a \ntensor([[0., 0.], \n       [0., 0.], \n       [0., 0.]])\n```", "```py\n>>> torch.zeros(3, 4) \ntensor([[0., 0., 0., 0.], \n       [0., 0., 0., 0.], \n       [0., 0., 0., 0.]])\n```", "```py\n>>> a.zero_() \ntensor([[0., 0.], \n       [0., 0.], \n       [0., 0.]])\n```", "```py\n>>> torch.FloatTensor([[1,2,3],[3,2,1]]) \ntensor([[1., 2., 3.], \n       [3., 2., 1.]])\n```", "```py\n>>> n = np.zeros(shape=(3, 2)) \n>>> n \narray([[0., 0.], \n      [0., 0.], \n      [0., 0.]]) \n>>> b = torch.tensor(n) \n>>> b \ntensor([[0., 0.], \n       [0., 0.], \n       [0., 0.]], dtype=torch.float64)\n```", "```py\n>>> n = np.zeros(shape=(3, 2), dtype=np.float32) \n>>> torch.tensor(n) \ntensor([[0., 0.], \n       [0., 0.], \n       [0., 0.]])\n```", "```py\n>>> n = np.zeros(shape=(3,2)) \n>>> torch.tensor(n, dtype=torch.float32) \ntensor([[0., 0.], \n       [0., 0.], \n       [0., 0.]])\n```", "```py\n>>> a = torch.tensor([1,2,3]) \n>>> a \ntensor([1, 2, 3]) \n>>> s = a.sum() \n>>> s \ntensor(6) \n>>> s.item() \n6 \n>>> torch.tensor(1) \ntensor(1)\n```", "```py\n>>> a = torch.FloatTensor([2,3]) \n>>> a \ntensor([2., 3.]) \n>>> ca = a.to(’cuda’) \n>>> ca \ntensor([2., 3.], device=’cuda:0’)\n```", "```py\n>>> a+1 \ntensor([3., 4.]) \n>>> ca + 1 \ntensor([3., 4.], device=’cuda:0’) \n>>> ca.device \ndevice(type=’cuda’, index=0)\n```", "```py\n>>> v1 = torch.tensor([1.0, 1.0], requires_grad=True) \n>>> v2 = torch.tensor([2.0, 2.0])\n```", "```py\n>>> v_sum = v1 + v2 \n>>> v_sum \ntensor([3., 3.], grad_fn=<AddBackward0>) \n>>> v_res = (v_sum*2).sum() \n>>> v_res \ntensor(12., grad_fn=<SumBackward0>)\n```", "```py\n>>> v1.is_leaf, v2.is_leaf \n(True, True) \n>>> v_sum.is_leaf, v_res.is_leaf \n(False, False) \n>>> v1.requires_grad \nTrue \n>>> v2.requires_grad \nFalse \n>>> v_sum.requires_grad \nTrue \n>>> v_res.requires_grad \nTrue\n```", "```py\n>>> v_res.backward() \n>>> v1.grad \ntensor([2., 2.])\n```", "```py\n>>> v2.grad\n```", "```py\n>>> l = nn.Linear(2, 5) \n>>> v = torch.FloatTensor([1, 2]) \n>>> l(v) \ntensor([-0.1039, -1.1386,  1.1376, -0.3679, -1.1161], grad_fn=<ViewBackward0>)\n```", "```py\n>>> s = nn.Sequential( \n... nn.Linear(2, 5), \n... nn.ReLU(), \n... nn.Linear(5, 20), \n... nn.ReLU(), \n... nn.Linear(20, 10), \n... nn.Dropout(p=0.3), \n... nn.Softmax(dim=1)) \n>>> s \nSequential( \n  (0): Linear(in_features=2, out_features=5, bias=True) \n  (1): ReLU() \n  (2): Linear(in_features=5, out_features=20, bias=True) \n  (3): ReLU() \n  (4): Linear(in_features=20, out_features=10, bias=True) \n  (5): Dropout(p=0.3, inplace=False) \n  (6): Softmax(dim=1) \n)\n```", "```py\n>>> s(torch.FloatTensor([[1,2]])) \ntensor([[0.0847, 0.1145, 0.1063, 0.1458, 0.0873, 0.1063, 0.0864, 0.0821, 0.0894, \n        0.0971]], grad_fn=<SoftmaxBackward0>)\n```", "```py\nclass OurModule(nn.Module): \n    def __init__(self, num_inputs, num_classes, dropout_prob=0.3): \n        super(OurModule, self).__init__() \n        self.pipe = nn.Sequential( \n            nn.Linear(num_inputs, 5), \n            nn.ReLU(), \n            nn.Linear(5, 20), \n            nn.ReLU(), \n            nn.Linear(20, num_classes), \n            nn.Dropout(p=dropout_prob), \n            nn.Softmax(dim=1) \n        )\n```", "```py\n def forward(self, x): \n        return self.pipe(x)\n```", "```py\nif __name__ == \"__main__\": \n    net = OurModule(num_inputs=2, num_classes=3) \n    print(net) \n    v = torch.FloatTensor([[2, 3]]) \n    out = net(v) \n    print(out) \n    print(\"Cuda’s availability is %s\" % torch.cuda.is_available()) \n    if torch.cuda.is_available(): \n        print(\"Data from cuda: %s\" % out.to(’cuda’))\n```", "```py\nChapter03$ python 01_modules.py \nOurModule( \n  (pipe): Sequential( \n   (0): Linear(in_features=2, out_features=5, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=5, out_features=20, bias=True) \n   (3): ReLU() \n   (4): Linear(in_features=20, out_features=3, bias=True) \n   (5): Dropout(p=0.3, inplace=False) \n   (6): Softmax(dim=1) \n  ) \n) \ntensor([[0.3297, 0.3854, 0.2849]], grad_fn=<SoftmaxBackward0>) \nCuda’s availability is False\n```", "```py\nfor batch_x, batch_y in iterate_batches(data, batch_size=N): \n    batch_x_t = torch.tensor(batch_x) \n    batch_y_t = torch.tensor(batch_y) \n    out_t = net(batch_x_t) \n    loss_t = loss_function(out_t, batch_y_t). \n    loss_t.backward() \n    optimizer.step() \n    optimizer.zero_grad()\n```", "```py\nimport math \nfrom torch.utils.tensorboard.writer import SummaryWriter \n\nif __name__ == \"__main__\": \n    writer = SummaryWriter() \n    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n```", "```py\n for angle in range(-360, 360): \n        angle_rad = angle * math.pi / 180 \n        for name, fun in funcs.items(): \n            val = fun(angle_rad) \n            writer.add_scalar(name, val, angle) \n\n    writer.close()\n```", "```py\nChapter03$ tensorboard --logdir runs \nTensorFlow installation not found - running with reduced feature set. \nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all \nTensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n```", "```py\nclass InputWrapper(gym.ObservationWrapper): \n    \"\"\" \n    Preprocessing of input numpy array: \n    1\\. resize image into predefined size \n    2\\. move color channel axis to a first place \n    \"\"\" \n    def __init__(self, *args): \n        super(InputWrapper, self).__init__(*args) \n        old_space = self.observation_space \n        assert isinstance(old_space, spaces.Box) \n        self.observation_space = spaces.Box( \n            self.observation(old_space.low), self.observation(old_space.high), \n            dtype=np.float32 \n        ) \n\n    def observation(self, observation: gym.core.ObsType) -> gym.core.ObsType: \n        # resize image \n        new_obs = cv2.resize( \n            observation, (IMAGE_SIZE, IMAGE_SIZE)) \n        # transform (w, h, c) -> (c, w, h) \n        new_obs = np.moveaxis(new_obs, 2, 0) \n        return new_obs.astype(np.float32)\n```", "```py\ndef iterate_batches(envs: tt.List[gym.Env], \n                    batch_size: int = BATCH_SIZE) -> tt.Generator[torch.Tensor, None, None]: \n    batch = [e.reset()[0] for e in envs] \n    env_gen = iter(lambda: random.choice(envs), None) \n\n    while True: \n        e = next(env_gen) \n        action = e.action_space.sample() \n        obs, reward, is_done, is_trunc, _ = e.step(action) \n        if np.mean(obs) > 0.01: \n            batch.append(obs) \n        if len(batch) == batch_size: \n            batch_np = np.array(batch, dtype=np.float32) \n            # Normalising input to [-1..1] \n            yield torch.tensor(batch_np * 2.0 / 255.0 - 1.0) \n            batch.clear() \n        if is_done or is_trunc: \n            e.reset()\n```", "```py\nif __name__ == \"__main__\": \n    parser = argparse.ArgumentParser() \n    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\") \n    args = parser.parse_args() \n\n    device = torch.device(args.dev) \n    envs = [ \n        InputWrapper(gym.make(name)) \n        for name in (’Breakout-v4’, ’AirRaid-v4’, ’Pong-v4’) \n    ] \n    shape = envs[0].observation_space.shape\n```", "```py\n net_discr = Discriminator(input_shape=shape).to(device) \n    net_gener = Generator(output_shape=shape).to(device) \n\n    objective = nn.BCELoss() \n    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, \n                               betas=(0.5, 0.999)) \n    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, \n                               betas=(0.5, 0.999)) \n    writer = SummaryWriter()\n```", "```py\n gen_losses = [] \n    dis_losses = [] \n    iter_no = 0 \n\n    true_labels_v = torch.ones(BATCH_SIZE, device=device) \n    fake_labels_v = torch.zeros(BATCH_SIZE, device=device) \n    ts_start = time.time()\n```", "```py\n for batch_v in iterate_batches(envs): \n        # fake samples, input is 4D: batch, filters, x, y \n        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1) \n        gen_input_v.normal_(0, 1) \n        gen_input_v = gen_input_v.to(device) \n        batch_v = batch_v.to(device) \n        gen_output_v = net_gener(gen_input_v)\n```", "```py\n dis_optimizer.zero_grad() \n        dis_output_true_v = net_discr(batch_v) \n        dis_output_fake_v = net_discr(gen_output_v.detach()) \n        dis_loss = objective(dis_output_true_v, true_labels_v) + \\ \n                   objective(dis_output_fake_v, fake_labels_v) \n        dis_loss.backward() \n        dis_optimizer.step() \n        dis_losses.append(dis_loss.item())\n```", "```py\n gen_optimizer.zero_grad() \n        dis_output_v = net_discr(gen_output_v) \n        gen_loss_v = objective(dis_output_v, true_labels_v) \n        gen_loss_v.backward() \n        gen_optimizer.step() \n        gen_losses.append(gen_loss_v.item())\n```", "```py\n iter_no += 1 \n        if iter_no % REPORT_EVERY_ITER == 0: \n            dt = time.time() - ts_start \n            log.info(\"Iter %d in %.2fs: gen_loss=%.3e, dis_loss=%.3e\", \n                     iter_no, dt, np.mean(gen_losses), np.mean(dis_losses)) \n            ts_start = time.time() \n            writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no) \n            writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no) \n            gen_losses = [] \n            dis_losses = [] \n        if iter_no % SAVE_IMAGE_EVERY_ITER == 0: \n            img = vutils.make_grid(gen_output_v.data[:64], normalize=True) \n            writer.add_image(\"fake\", img, iter_no) \n            img = vutils.make_grid(batch_v.data[:64], normalize=True) \n            writer.add_image(\"real\", img, iter_no)\n```", "```py\nfrom ignite.engine import Engine, Events \n\ndef training(engine, batch): \n    optimizer.zero_grad() \n    x, y = prepare_batch() \n    y_out = model(x) \n    loss = loss_fn(y_out, y) \n    loss.backward() \n    optimizer.step() \n    return loss.item() \n\nengine = Engine(training) \nengine.run(data)\n```", "```py\nfrom ignite.engine import Engine, Events \nfrom ignite.handlers import Timer \nfrom ignite.metrics import RunningAverage \nfrom ignite.contrib.handlers import tensorboard_logger as tb_logger\n```", "```py\n def process_batch(trainer, batch): \n        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1) \n        gen_input_v.normal_(0, 1) \n        gen_input_v = gen_input_v.to(device) \n        batch_v = batch.to(device) \n        gen_output_v = net_gener(gen_input_v) \n\n        # train discriminator \n        dis_optimizer.zero_grad() \n        dis_output_true_v = net_discr(batch_v) \n        dis_output_fake_v = net_discr(gen_output_v.detach()) \n        dis_loss = objective(dis_output_true_v, true_labels_v) + \\ \n                   objective(dis_output_fake_v, fake_labels_v) \n        dis_loss.backward() \n        dis_optimizer.step() \n\n        # train generator \n        gen_optimizer.zero_grad() \n        dis_output_v = net_discr(gen_output_v) \n        gen_loss = objective(dis_output_v, true_labels_v) \n        gen_loss.backward() \n        gen_optimizer.step() \n\n        if trainer.state.iteration % SAVE_IMAGE_EVERY_ITER == 0: \n            fake_img = vutils.make_grid(gen_output_v.data[:64], normalize=True) \n            trainer.tb.writer.add_image(\"fake\", fake_img, trainer.state.iteration) \n            real_img = vutils.make_grid(batch_v.data[:64], normalize=True) \n            trainer.tb.writer.add_image(\"real\", real_img, trainer.state.iteration) \n            trainer.tb.writer.flush() \n        return dis_loss.item(), gen_loss.item()\n```", "```py\n engine = Engine(process_batch) \n    tb = tb_logger.TensorboardLogger(log_dir=None) \n    engine.tb = tb \n    RunningAverage(output_transform=lambda out: out[1]).\\ \n        attach(engine, \"avg_loss_gen\") \n    RunningAverage(output_transform=lambda out: out[0]).\\ \n        attach(engine, \"avg_loss_dis\") \n\n    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=[’avg_loss_gen’, ’avg_loss_dis’]) \n    tb.attach(engine, log_handler=handler, event_name=Events.ITERATION_COMPLETED) \n\n    timer = Timer() \n    timer.attach(engine)\n```", "```py\n @engine.on(Events.ITERATION_COMPLETED) \n    def log_losses(trainer): \n        if trainer.state.iteration % REPORT_EVERY_ITER == 0: \n            log.info(\"%d in %.2fs: gen_loss=%f, dis_loss=%f\", \n                     trainer.state.iteration, timer.value(), \n                     trainer.state.metrics[’avg_loss_gen’], \n                     trainer.state.metrics[’avg_loss_dis’]) \n            timer.reset() \n\n    engine.run(data=iterate_batches(envs))\n```"]