["```py\npip install torch\n```", "```py\nconda install pytorch torchvision -c pytorch\n```", "```py\nimport torch\nprint(torch.__version__)\n```", "```py\nimport torch\n```", "```py\nt1 = torch.tensor([1, 2, 3])\nprint(t1)\nt2 = torch.tensor([[1, 2], [3, 4]])\nprint(t2)\n```", "```py\nimport numpy as np\nnp_array = np.array([5, 6, 7])\nt3 = torch.from_numpy(np_array)\nprint(t3)\n```", "```py\nt4 = torch.zeros((3, 3))\nprint(t4)\nt5 = torch.ones((3, 3))\nprint(t5)\nt6 = torch.eye(3)\nprint(t6)\n```", "```py\nresult = t1 + t3\nprint(result)\nresult = t3 - t1\nprint(result)\nresult = t1 * t3\nprint(result)\nresult = t3 / t1\nprint(result)\n```", "```py\nt7 = torch.arange(9) # Creates a 1D tensor [0, 1, 2, ..., 8]\nt8 = t7.reshape((3, 3)) # Reshapes the tensor to a 3x3 matrix\nprint(t8)\n```", "```py\nprint(t1)\nprint(t2)\n```", "```py\ndot_product = torch.dot(t1, t3)\nprint(dot_product)\n```", "```py\nmatrix_product = torch.mm(t2, t5)\nprint(matrix_product)\n```", "```py\nt_transposed = t2.T\nprint(t_transposed)\n```", "```py\ndet = torch.det(t2)\nprint(det)\ninverse = torch.inverse(t2)\nprint(inverse)\n```", "```py\nX = torch.randn(100, 10)\ny = torch.randn(100, 1)\n```", "```py\ninput_size = 10\nhidden_size = 5\noutput_size = 1\nW1 = torch.randn(hidden_size, input_size).requires_grad_()\nb1 = torch.zeros(hidden_size, requires_grad=True)\nW2 = torch.randn(output_size, hidden_size).requires_grad_()\nb2 = torch.zeros(output_size, requires_grad=True)\ndef simple_neural_net(x, W1, b1, W2, b2):\n    z1 = torch.mm(x, W1.t()) + b1\n    a1 = torch.sigmoid(z1)\n    z2 = torch.mm(a1, W2.t()) + b2\n    return z2\n```", "```py\nlr = 0.01\nepochs = 100\nloss_fn = torch.nn.MSELoss()\nfor epoch in range(epochs):\n    y_pred = simple_neural_net(X, W1, b1, W2, b2)\n    loss = loss_fn(y_pred.squeeze(), y)\n    loss.backward()\n    with torch.no_grad():\n        W1 -= lr * W1.grad\n        b1 -= lr * b1.grad\n        W2 -= lr * W2.grad\n        b2 -= lr * b2.grad\n    W1.grad.zero_()\n    b1.grad.zero_()\n    W2.grad.zero_()\n    b2.grad.zero_()\n    if epoch % 10 == 0:\n        print(f'Epoch: {epoch} \\t Loss: {loss.item()}')\n```", "```py\nimport torch\nimport torch.nn as nn\n```", "```py\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 1)\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\nnet = Net()\nprint(net)\n```", "```py\nX = torch.randn(100, 10)\nY = torch.randn(100, 1)\n```", "```py\nloss_fn = nn.MSELoss()\n```", "```py\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n```", "```py\nfor epoch in range(100):\n    output = net(X)\n    loss = loss_fn(output, Y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n```", "```py\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.rnn(x, h0)  # get RNN output\n        out = self.fc(out[:, -1, :])\n        return out\nrnn = RNN(10, 20, 1)\nprint(rnn)\n```", "```py\nX = torch.randn(100, 5, 10)\nY = torch.randn(100, 1)\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)\nfor epoch in range(100):\n    output = rnn(X)\n    loss = loss_fn(output, Y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n```", "```py\nclass LSTM(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n     super(LSTM, self).__init__()\n     self.hidden_size = hidden_size\n     self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n     self.fc = nn.Linear(hidden_size, output_size)\n  def forward(self, x):\n     h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n     c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n     out, _ = self.lstm(x, (h0, c0))  # get LSTM output\n     out = self.fc(out[:, -1, :])\n     return out\nlstm = LSTM(10, 20, 1) # 10 features, 20 hidden units, 1 output\nprint(lstm)\n```", "```py\nX = torch.randn(100, 5, 10)\nY = torch.randn(100, 1)\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(lstm.parameters(), lr=0.01)\nfor epoch in range(100):\n      output = lstm(X)\n      loss = loss_fn(output, Y)\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n```", "```py\nclass ConvNet(nn.Module):\n    def __init__(self,\n        input_size,\n        hidden_size,\n        output_size,\n        kernel_size,\n        seq_length):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size)\n        self.fc = nn.Linear(hidden_size*(seq_length-kernel_size+1),\n            output_size)\n    def forward(self, x):\n        x = x.transpose(1, 2)\n        out = torch.relu(self.conv1(x))\n        out = out.view(out.size(0), -1)  # flatten the tensor\n        out = self.fc(out)\n        return out\nconvnet = ConvNet(5, 20, 1, 3, 10)\nprint(convnet)\n```", "```py\nX = torch.randn(100, 10, 5)\nY = torch.randn(100, 1)\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(convnet.parameters(), lr=0.01)\nfor epoch in range(100):\n    output = convnet(X)\n    loss = loss_fn(output, Y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n```"]