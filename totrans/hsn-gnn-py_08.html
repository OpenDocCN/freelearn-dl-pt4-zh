<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer352">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor096"/>8</h1>
<h1 id="_idParaDest-93"><a id="_idTextAnchor097"/>Scaling Up Graph Neural Networks  with GraphSAGE</h1>
<p><strong class="bold">GraphSAGE</strong> is a<a id="_idIndexMarker430"/> GNN architecture designed to handle large graphs. In the tech industry, <strong class="bold">scalability</strong> is a <a id="_idIndexMarker431"/>key driver for growth. As a result, systems are inherently designed to accommodate millions of users. This ability requires a fundamental shift in how the GNN model works compared to GCNs and GATs. Thus, it is no surprise that GraphSAGE is the architecture of choice for tech companies such as Uber Eats <span class="No-Break">and Pinterest.</span></p>
<p>In this chapter, we<a id="_idIndexMarker432"/> will learn about the two main ideas behind GraphSAGE. First, we will describe its <strong class="bold">neighbor sampling</strong> technique, which is at the core of its performance in terms of scalability. We will then explore three aggregation operators used to produce node embeddings. Besides the original approach, we will also detail the variants proposed by Uber Eats <span class="No-Break">and Pinterest.</span></p>
<p>Moreover, GraphSAGE offers new possibilities in terms of training. We will implement two ways of training a GNN for two tasks – node classification with <strong class="source-inline">PubMed</strong> and <strong class="bold">multi-label classification</strong> for <strong class="bold">protein-protein interactions</strong>. Finally, we will discuss the benefits of a new <strong class="bold">inductive</strong> approach and how to <span class="No-Break">use it.</span></p>
<p>By the end of this chapter, you will understand how and why the neighbor sampling algorithm works. You will be able to implement it to create mini-batches and speed up training on most GNN architectures using a GPU. Furthermore, you will master inductive learning and multi-label classification <span class="No-Break">on graphs.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li><span class="No-Break">Introducing GraphSAGE</span></li>
<li>Classifying nodes <span class="No-Break">on PubMed</span></li>
<li>Inductive learning on <span class="No-Break">protein-protein interactions</span></li>
</ul>
<h1 id="_idParaDest-94"><a id="_idTextAnchor098"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter08</span><span class="No-Break">.</span></p>
<p>The installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> chapter of <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor099"/>Introducing GraphSAGE</h1>
<p>Hamilton et al. introduced GraphSAGE in 2017 (see item [1] of the <em class="italic">Further reading</em> section) as a <a id="_idIndexMarker433"/>framework for inductive representation learning on large graphs (with over 100,000 nodes). Its goal is to generate node embeddings for downstream tasks, such as node classification. In addition, it solves two issues with GCNs and GATs – scaling to large graphs and efficiently generalizing to unseen data. In this section, we will explain how to implement it by describing the two main components <span class="No-Break">of GraphSAGE:</span></p>
<ul>
<li><span class="No-Break">Neighbor sampling</span></li>
<li><span class="No-Break">Aggregation</span></li>
</ul>
<p>Let’s take a look <span class="No-Break">at them.</span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor100"/>Neighbor sampling</h2>
<p>So far, we <a id="_idIndexMarker434"/>haven’t discussed an essential concept in traditional neural <a id="_idIndexMarker435"/>networks – <strong class="bold">mini-batching</strong>. It consists of dividing our dataset into smaller fragments, called batches. They are used in <strong class="bold">gradient descent</strong>, the<a id="_idIndexMarker436"/> optimization algorithm that finds the best weights and biases during training. There are three types of <span class="No-Break">gradient descent:</span></p>
<ul>
<li><strong class="bold">Batch gradient descent</strong>: Weights and <a id="_idIndexMarker437"/>biases are<a id="_idIndexMarker438"/> updated after a whole dataset has been processed (every epoch). This is the technique we have implemented so far. However, it is a slow process that requires the dataset to fit <span class="No-Break">in memory.</span></li>
<li><strong class="bold">Stochastic gradient descent</strong>: Weights<a id="_idIndexMarker439"/> and <a id="_idIndexMarker440"/>biases are updated for each training example in the dataset. This is a noisy process because the errors are not averaged. However, it can be used to perform <span class="No-Break">online training.</span></li>
<li><strong class="bold">Mini-batch gradient descent</strong>: Weights <a id="_idIndexMarker441"/>and biases are updated at the end of every <a id="_idIndexMarker442"/>mini-batch of <img alt="" height="24" src="image/Formula_B19153_08_001.png" width="27"/> training examples. This technique is faster (mini-batches can be processed in parallel using a GPU) and leads to more stable convergence. In addition, the dataset can exceed the available memory, which is essential for handling <span class="No-Break">large graphs.</span></li>
</ul>
<p>In practice, we <a id="_idIndexMarker443"/>use more advanced optimizers<a id="_idIndexMarker444"/> such as <strong class="bold">RMSprop</strong> or <strong class="source-inline">Adam</strong>, which also <span class="No-Break">implement </span><span class="No-Break"><a id="_idIndexMarker445"/></span><span class="No-Break">mini-batching.</span></p>
<p>Dividing a tabular dataset is straightforward; it simply consists of selecting <img alt="" height="22" src="image/Formula_B19153_08_002.png" width="25"/> samples (rows). However, this is an issue regarding graph datasets – how do we choose <img alt="" height="21" src="image/Formula_B19153_08_003.png" width="24"/> nodes without breaking essential connections? If we’re not careful, we could end up with a collection of isolated nodes where we cannot perform <span class="No-Break">any aggregation.</span></p>
<p>We have to think about how GNNs use datasets. Every GNN layer computes node embeddings based on their neighbors. This means that computing an embedding only requires the direct neighbors of this node (<strong class="bold">1 hop</strong>). If our GNN has two GNN layers, we need these neighbors and their own neighbors (<strong class="bold">2 hops</strong>), and so on (see <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em>). The rest of the network is irrelevant to computing these individual <span class="No-Break">node embeddings.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer334">
<img alt="Figure 8.1 – A graph with node 0 as the target node and the 1-hop and 2-hop neighbors" height="641" src="image/B19153_08_001.jpg" width="674"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A graph with node 0 as the target node and the 1-hop and 2-hop neighbors</p>
<p>This<a id="_idIndexMarker446"/> technique allows us to fill batches with computation <a id="_idIndexMarker447"/>graphs, which describe the entire sequence of operations for calculating a node embedding. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em> shows the computation graph of node <strong class="source-inline">0</strong> in a more <span class="No-Break">intuitive representation.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer335">
<img alt="Figure 8.2 – A computation graph for node 0" height="628" src="image/B19153_08_002.jpg" width="834"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – A computation graph for node 0</p>
<p>We need<a id="_idIndexMarker448"/> to aggregate 2-hop neighbors in order to compute the <a id="_idIndexMarker449"/>embedding of 1-hop neighbors. These embeddings are then aggregated to obtain the embedding of node 0. However, there are two problems with <span class="No-Break">this design:</span></p>
<ul>
<li>The computation graph becomes exponentially large with respect to the number <span class="No-Break">of hops</span></li>
<li>Nodes with very high degrees of connectivity (such as celebrities on an online social network a social network), also<a id="_idIndexMarker450"/> called <strong class="bold">hub nodes</strong>, create enormous <span class="No-Break">computation graphs</span></li>
</ul>
<p>To solve these issues, we have to limit the size of our computation graphs. In GraphSAGE, the authors propose a technique called neighbor sampling. Instead of adding every neighbor in the computation graph, we sample a predefined number of them. For instance, we choose only to keep (at most) three neighbors during the first hop and five neighbors during the second hop. Hence, the computation graph cannot exceed <img alt="" height="29" src="image/Formula_B19153_08_004.png" width="188"/> nodes in <span class="No-Break">this case.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer337">
<img alt="Figure 8.3 – A computation graph with neighbor sampling to keep two 1-hop neighbors and two 2-hop neighbors" height="628" src="image/B19153_08_003.jpg" width="834"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – A computation graph with neighbor sampling to keep two 1-hop neighbors and two 2-hop neighbors</p>
<p>A low sampling <a id="_idIndexMarker451"/>number is more efficient but makes the training more random (higher variance). Additionally, the number of GNN layers (hops) must stay<a id="_idIndexMarker452"/> low to avoid exponentially large computation graphs. Neighbor sampling can handle large graphs, but it causes a trade-off by pruning important information, which can negatively impact performance such as accuracy. Note that computation graphs involve a lot of redundant calculations, which makes the entire process computationally <span class="No-Break">less efficient.</span></p>
<p>Nonetheless, this random sampling is not the only technique we can use. Pinterest has its own version of GraphSAGE, called PinSAGE, to power its recommender system (see <em class="italic">Further reading</em> [2]). It implements another sampling solution using random walks. PinSAGE keeps the idea of a fixed number of neighbors but implements random walks to see which nodes are the most frequently encountered. This frequency determines their relative importance. PinSAGE’s sampling strategy allows it to select the most critical nodes and proves more efficient <span class="No-Break">in practice.</span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor101"/>Aggregation</h2>
<p>Now<a id="_idIndexMarker453"/> that we’ve seen how to select the neighboring nodes, we still need to <a id="_idIndexMarker454"/>compute embeddings. This is performed by the aggregation operator (or aggregator). In GraphSAGE, the authors have proposed <span class="No-Break">three solutions:</span></p>
<ul>
<li>A <span class="No-Break">mean aggregator</span></li>
<li>A <strong class="bold">long short-term memory</strong> (<span class="No-Break"><strong class="bold">LSTM</strong></span><span class="No-Break">) aggregator</span></li>
<li>A <span class="No-Break">pooling aggregator</span></li>
</ul>
<p>We will focus on the <a id="_idIndexMarker455"/>mean aggregator, as it is the easiest to understand. First, the <a id="_idIndexMarker456"/>mean aggregator<a id="_idIndexMarker457"/> takes the embeddings of target nodes and their sampled neighbors to average them. Then, a linear transformation with a weight matrix, <img alt="" height="34" src="image/Formula_B19153_08_005.png" width="44"/>, is applied to <span class="No-Break">this result:</span></p>
<p>The mean aggregator can be summarized by the following formula, where <img alt="" height="23" src="image/Formula_B19153_08_006.png" width="27"/> is a non-linear function such as ReLU <span class="No-Break">or tanh:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer340">
<img alt="" height="71" src="image/Formula_B19153_08_007.jpg" width="500"/>
</div>
</div>
<p>In the case of PyG’s and Uber Eats’ implementation of GraphSAGE [3], we use two weight matrices instead of one; the first one is dedicated to the target node, and the second to the neighbors. This aggregator can be written <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer341">
<img alt="" height="67" src="image/Formula_B19153_08_008.jpg" width="648"/>
</div>
</div>
<p>The<a id="_idIndexMarker458"/> LSTM aggregator<a id="_idIndexMarker459"/> is based on LSTM architecture, a popular recurrent neural network type. Compared to the mean aggregator, the LSTM aggregator can, in theory, discriminate between more graph structures and, thus, produce better embeddings. The issue is that recurrent neural networks only consider <a id="_idIndexMarker460"/>sequences of inputs, such as a sentence with a beginning and an end. However, nodes do not have any sequence. Therefore, we perform random permutations of the node’s neighbors to address this problem. This solution allows us to use the LSTM architecture without relying on any sequence <span class="No-Break">of inputs.</span></p>
<p>Finally, the<a id="_idIndexMarker461"/> pooling aggregator <a id="_idIndexMarker462"/>works in two steps. First, every <a id="_idIndexMarker463"/>neighbor’s embedding is fed to an MLP to produce a new vector. Secondly, an elementwise max operation is performed to only keep the highest value for <span class="No-Break">each feature.</span></p>
<p>We are not limited to these three options and could implement other aggregators in the GraphSAGE framework. Indeed, the main idea behind GraphSAGE resides in its efficient neighbor sampling. In the next section, we will use it to perform node classification on a <span class="No-Break">new dataset.</span></p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor102"/>Classifying nodes on PubMed</h1>
<p>In this section, we <a id="_idIndexMarker464"/>will implement a GraphSAGE architecture to perform node classification on the <strong class="source-inline">PubMed</strong> dataset (available under the MIT license from <a href="https://github.com/kimiyoung/planetoid"><span class="No-Break">https://github.com/kimiyoung/planetoid</span></a><span class="No-Break">) [4].</span></p>
<p>Previously, we<a id="_idIndexMarker465"/> saw two other citation network datasets from the same Planetoid family – <strong class="source-inline">Cora</strong> and <strong class="source-inline">CiteSeer</strong>. The <strong class="source-inline">PubMed</strong> dataset displays a similar but larger graph, with 19,717 nodes and 88,648 edges. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em> shows a visualization of this dataset as created by <span class="No-Break">Gephi (</span><a href="https://gephi.org/"><span class="No-Break">https://gephi.org/</span></a><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer342">
<img alt="Figure 8.4 – A visualization of the PubMed dataset" height="915" src="image/B19153_08_004.jpg" width="882"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – A visualization of the PubMed dataset</p>
<p>Node <a id="_idIndexMarker466"/>features are TF-IDF-weighted word vectors with 500 dimensions. The<a id="_idIndexMarker467"/> goal is to correctly classify nodes into three categories – diabetes mellitus experimental, diabetes mellitus type 1, and diabetes mellitus type 2. Let’s implement it step by step <span class="No-Break">using PyG:</span></p>
<ol>
<li>We load the <strong class="source-inline">PubMed</strong> dataset from the <strong class="source-inline">Planetoid</strong> class and print some information about <span class="No-Break">the graph:</span><pre class="source-code">
from torch_geometric.datasets import Planetoid
dataset = Planetoid(root='.', name="Pubmed")
data = dataset[0]
print(f'Dataset: {dataset}')
print('-------------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of nodes: {data.x.shape[0]}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')
print('Graph:')
print('------')
print(f'Training nodes: {sum(data.train_mask).item()}')
print(f'Evaluation nodes: {sum(data.val_mask).item()}')
print(f'Test nodes: {sum(data.test_mask).item()}')
print(f'Edges are directed: {data.is_directed()}')
print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')
print(f'Graph has loops: {data.has_self_loops()}')</pre></li>
<li>This <a id="_idIndexMarker468"/>produces<a id="_idIndexMarker469"/> the <span class="No-Break">following output:</span><pre class="source-code">
<strong class="bold">Dataset: Pubmed()</strong>
<strong class="bold">-------------------</strong>
<strong class="bold">Number of graphs: 1</strong>
<strong class="bold">Number of nodes: 19717</strong>
<strong class="bold">Number of features: 500</strong>
<strong class="bold">Number of classes: 3</strong>
<strong class="bold">Graph:</strong>
<strong class="bold">------</strong>
<strong class="bold">Training nodes: 60</strong>
<strong class="bold">Evaluation nodes: 500</strong>
<strong class="bold">Test nodes: 1000</strong>
<strong class="bold">Edges are directed: False</strong>
<strong class="bold">Graph has isolated nodes: False</strong>
<strong class="bold">Graph has loops: False</strong></pre></li>
</ol>
<p>As you<a id="_idIndexMarker470"/> can see, there are only 60 training nodes for 1,000 test nodes, which is quite challenging (a 6/94 split). Fortunately for us, with only 19,717 nodes, <strong class="source-inline">PubMed</strong> will be extremely fast to process <span class="No-Break">with GraphSAGE.</span></p>
<ol>
<li value="3">The first <a id="_idIndexMarker471"/>step in the GraphSAGE framework is neighbor sampling. PyG implements the <strong class="source-inline">NeighborLoader</strong> class to perform it. Let’s keep 10 neighbors of our target node and 10 of their own neighbors. We will group our 60 target nodes into batches of 16 nodes, which should result in <span class="No-Break">four batches:</span><pre class="source-code">
from torch_geometric.loader import NeighborLoader
train_loader = NeighborLoader(
    data,
    num_neighbors=[10,10],
    batch_size=16,
    input_nodes=data.train_mask,
)</pre></li>
<li>By printing<a id="_idIndexMarker472"/> their information, let’s verify that we obtained <a id="_idIndexMarker473"/>four <span class="No-Break">subgraphs (batches):</span><pre class="source-code">
for i, subgraph in enumerate(train_loader):
    print(f'Subgraph {i}: {subgraph}')
<strong class="bold">Subgraph 0: Data(x=[400, 500], edge_index=[2, 455], y=[400], train_mask=[400], val_mask=[400], test_mask=[400], batch_size=16)</strong>
<strong class="bold">Subgraph 1: Data(x=[262, 500], edge_index=[2, 306], y=[262], train_mask=[262], val_mask=[262], test_mask=[262], batch_size=16)</strong>
<strong class="bold">Subgraph 2: Data(x=[275, 500], edge_index=[2, 314], y=[275], train_mask=[275], val_mask=[275], test_mask=[275], batch_size=16)</strong>
<strong class="bold">Subgraph 3: Data(x=[194, 500], edge_index=[2, 227], y=[194], train_mask=[194], val_mask=[194], test_mask=[194], batch_size=12)</strong></pre></li>
<li>These subgraphs contain more than 60 nodes, which is normal, since any neighbor can be sampled. We can even plot them like graphs using <span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">’s subplots:</span><pre class="source-code">
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.utils import to_networkx
fig = plt.figure(figsize=(16,16))
for idx, (subdata, pos) in enumerate(zip(train_loader, [221, 222, 223, 224])):
    G = to_networkx(subdata, to_undirected=True)
    ax = fig.add_subplot(pos)
    ax.set_title(f'Subgraph {idx}', fontsize=24)
    plt.axis('off')
    nx.draw_networkx(G,
                    pos=nx.spring_layout(G, seed=0),
                    with_labels=False,
                    node_color=subdata.y,
                    )
plt.show()</pre></li>
<li>We<a id="_idIndexMarker474"/> obtain <a id="_idIndexMarker475"/>the <span class="No-Break">following plot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer343">
<img alt="Figure 8.5 – A plot of the subgraphs obtained with neighbor sampling" height="909" src="image/B19153_08_005.jpg" width="907"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – A plot of the subgraphs obtained with neighbor sampling</p>
<p>Most <a id="_idIndexMarker476"/>of these nodes have a degree of 1 because of <a id="_idIndexMarker477"/>the way neighbor sampling works. In this case, it’s not an issue, since their embeddings are only used once in the computation graph to calculate the embeddings of the <span class="No-Break">second layer.</span></p>
<ol>
<li value="7">We implement the following function to evaluate the accuracy of <span class="No-Break">our model:</span><pre class="source-code">
def accuracy(pred_y, y):
    return ((pred_y == y).sum() / len(y)).item()</pre></li>
<li>Let’s create a <strong class="source-inline">GraphSAGE</strong> class using two <strong class="source-inline">SAGEConv</strong> layers (the mean aggregator is selected <span class="No-Break">by default):</span><pre class="source-code">
import torchmport torch.nn.functional as F
from torch_geometric.nn import SAGEConv
class GraphSAGE(torch.nn.Module):
    def __init__(self, dim_in, dim_h, dim_out):
        super().__init__()
        self.sage1 = SAGEConv(dim_in, dim_h)
        self.sage2 = SAGEConv(dim_h, dim_out)</pre></li>
<li>Embeddings <a id="_idIndexMarker478"/>are computed using two mean aggregators. We <a id="_idIndexMarker479"/>also use a nonlinear function (<strong class="source-inline">ReLU</strong>) and a <span class="No-Break">dropout layer:</span><pre class="source-code">
   def forward(self, x, edge_index):
        h = self.sage1(x, edge_index)
        h = torch.relu(h)
        h = F.dropout(h, p=0.5, training=self.training)
        h = self.sage2(h, edge_index)
        return F.log_softmax(h, dim=1)</pre></li>
<li>Now that we have to consider batches, the <strong class="source-inline">fit()</strong> function has to change to loop through epochs and then through batches. The metrics we want to measure have to be reinitialized at <span class="No-Break">every epoch:</span><pre class="source-code">
    def fit(self, data, epochs):
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)
        self.train()
        for epoch in range(epochs+1):
            total_loss, val_loss, acc, val_acc = 0, 0, 0, 0</pre></li>
<li>The <a id="_idIndexMarker480"/>second <a id="_idIndexMarker481"/>loop trains the model on <span class="No-Break">every batch:</span><pre class="source-code">
            for batch in train_loader:
                optimizer.zero_grad()
                out = self(batch.x, batch.edge_index)
                loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])
                total_loss += loss
                acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])
                loss.backward()
                optimizer.step()
                # Validation
                val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])
                val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])</pre></li>
<li>We also want to print our metrics. They must be divided by the number of batches to represent <span class="No-Break">an epoch:</span><pre class="source-code">
       if epoch % 20 == 0:
                print(f'Epoch {epoch:&gt;3} | Train Loss: {loss/len(train_loader):.3f} | Train Acc: {acc/len(train_loader)*100:&gt;6.2f}% | Val Loss: {val_loss/len(train_loader):.2f} | Val Acc: {val_acc/len(train_loader)*100:.2f}%')</pre></li>
<li>The <strong class="source-inline">test()</strong> function<a id="_idIndexMarker482"/> does not change, since we don’t use batches for the <span class="No-Break">test set:</span><pre class="source-code">
    @torch.no_grad()
    def test(self, data):
        self.eval()
        out = self(data.x, data.edge_index)
        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])
        return acc</pre></li>
<li>Let’s <a id="_idIndexMarker483"/>create a model with a hidden dimension of 64 and train it for <span class="No-Break">200 epochs:</span><pre class="source-code">
graphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes)
print(graphsage)
graphsage.fit(data, 200)</pre></li>
<li>This gives us the <span class="No-Break">following output:</span><pre class="source-code">
<strong class="bold">GraphSAGE(</strong>
<strong class="bold">  (sage1): SAGEConv(500, 64, aggr=mean)</strong>
<strong class="bold">  (sage2): SAGEConv(64, 3, aggr=mean)</strong>
<strong class="bold">)</strong>
<strong class="bold">Epoch 0 | Train Loss: 0.317 | Train Acc: 28.77% | Val Loss: 1.13 | Val Acc: 19.55%</strong>
<strong class="bold">Epoch 20 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 75.07%</strong>
<strong class="bold">Epoch 40 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.55 | Val Acc: 80.56%</strong>
<strong class="bold">Epoch 60 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.35 | Val Acc: 86.11%</strong>
<strong class="bold">Epoch 80 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 73.58%</strong>
<strong class="bold">Epoch 100 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.79 | Val Acc: 74.72%</strong>
<strong class="bold">Epoch 120 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.71 | Val Acc: 76.75%</strong>
<strong class="bold">Epoch 140 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 67.50%</strong>
<strong class="bold">Epoch 160 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 73.54%</strong>
<strong class="bold">Epoch 180 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.47 | Val Acc: 86.11%</strong>
<strong class="bold">Epoch 200 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 78.37%</strong></pre></li>
</ol>
<p>Note that <a id="_idIndexMarker484"/>the mean aggregator was automatically <a id="_idIndexMarker485"/>selected for both <span class="No-Break">SAGEConv layers.</span></p>
<ol>
<li value="16">Finally, let’s test it on the <span class="No-Break">test set:</span><pre class="source-code">
acc = graphsage.test(data)
print(f'GraphSAGE test accuracy: {acc*100:.2f}%')
<strong class="bold">GraphSAGE test accuracy: 74.70%</strong></pre></li>
</ol>
<p>Considering this dataset’s unfavorable train/test split, we obtain a decent test accuracy of 74.70%. However, GraphSAGE gets a lower average accuracy than a GCN (-0.5%) or a GAT (-1.4%) on <strong class="source-inline">PubMed</strong>. So why should we <span class="No-Break">use it?</span></p>
<p>The answer is<a id="_idIndexMarker486"/> evident when you train the three models – GraphSAGE is <a id="_idIndexMarker487"/>extremely fast. On a consumer GPU, it is 4 times faster than a GCN and 88 times faster than a GAT. Even if GPU memory was not an issue, GraphSAGE could handle larger graphs, producing better results than <span class="No-Break">small networks.</span></p>
<p>To complete this deep-dive into GraphSAGE’s architecture, we must discuss one more feature – its <span class="No-Break">inductive capabilities.</span></p>
<h1 id="_idParaDest-99"><a id="_idTextAnchor103"/>Inductive learning on protein-protein interactions</h1>
<p>In GNNs, we distinguish two types of learning – <strong class="bold">transductive</strong> and <strong class="bold">inductive</strong>. They can be summarized <span class="No-Break">as follows:</span></p>
<ul>
<li>In inductive learning, the<a id="_idIndexMarker488"/> GNN only sees data from the training set during training. This is the typical supervised learning setting in machine learning. In this situation, labels are used to tune the <span class="No-Break">GNN’s parameters.</span></li>
<li>In transductive learning, the <a id="_idIndexMarker489"/>GNN sees data from the training and test sets during training. However, it only learns data from the training set. In this situation, the labels are used for <span class="No-Break">information diffusion.</span></li>
</ul>
<p>The transductive situation<a id="_idIndexMarker490"/> should<a id="_idIndexMarker491"/> be familiar, since it is the only one we have covered so far. Indeed, you can see in the previous example that GraphSAGE makes predictions using the whole graph during training (<strong class="source-inline">self(batch.x, batch.edge_index)</strong>). We then mask part of these predictions to calculate the loss and train the model only using training data (<span class="No-Break"><strong class="source-inline">criterion(out[batch.train_mask], batch.y[batch.train_mask])</strong></span><span class="No-Break">).</span></p>
<p>Transductive learning can only generate embeddings for a fixed graph; it does not generalize for unseen nodes or graphs. However, thanks to neighbor sampling, GraphSAGE is designed to make predictions at a local level with pruned computation graphs. It is considered an inductive framework, since it can be applied to any computation graph with the same <span class="No-Break">feature schema.</span></p>
<p>Let’s <a id="_idIndexMarker492"/>apply it to a new dataset – the<a id="_idIndexMarker493"/> protein-protein interaction (<strong class="source-inline">PPI</strong>) network, described by Agrawal et al. [5]. This dataset is a collection of 24 graphs, where nodes (21,557) are human proteins and edges (342,353) are physical interactions between proteins in a human cell. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em> shows a representation of <strong class="source-inline">PPI</strong> made <span class="No-Break">with Gephi.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer344">
<img alt="Figure 8.6 – A visualization of the protein-protein interaction network" height="933" src="image/B19153_08_006.jpg" width="941"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – A visualization of the protein-protein interaction network</p>
<p>The goal of the dataset is to perform multi-label classification with 121 labels. This means that every node can range from 0 to 121 labels. This differs from a multi-class classification, where every node would only have <span class="No-Break">one class.</span></p>
<p>Let’s<a id="_idIndexMarker494"/> implement a new GraphSAGE model <a id="_idIndexMarker495"/><span class="No-Break">using PyG:</span></p>
<ol>
<li>We load the <strong class="source-inline">PPI</strong> dataset with three different splits – train, validation, <span class="No-Break">and test:</span><pre class="source-code">
from torch_geometric.datasets import PPI
train_dataset = PPI(root=".", split='train')
val_dataset = PPI(root=".", split='val')
test_dataset = PPI(root=".", split='test')</pre></li>
<li>The <a id="_idIndexMarker496"/>training set comprises 20 graphs, while the validation and test sets only have two. We want to apply neighbor sampling to the training set. For convenience, let’s unify all the training graphs in a single set, using <strong class="source-inline">Batch.from_data_list()</strong>, and then apply <span class="No-Break">neighbor sampling:</span><pre class="source-code">
from torch_geometric.data import Batch
from torch_geometric.loader import NeighborLoader
train_data = Batch.from_data_list(train_dataset)
loader = NeighborLoader(train_data, batch_size=2048, shuffle=True, num_neighbors=[20, 10], num_workers=2, persistent_workers=True)</pre></li>
<li>The training set is ready. We can create our batches using the <strong class="source-inline">DataLoader</strong> class. We define a <strong class="source-inline">batch_size</strong> value of <strong class="source-inline">2</strong>, corresponding to the number of graphs in <span class="No-Break">each batch:</span><pre class="source-code">
from torch_geometric.loader import DataLoader
train_loader = DataLoader(train_dataset, batch_size=2)
val_loader = DataLoader(val_dataset, batch_size=2)
test_loader = DataLoader(test_dataset, batch_size=2)</pre></li>
<li>One of <a id="_idIndexMarker497"/>the main benefits of these batches is that they can be processed on a GPU. We can get a GPU if one is available, or take a <span class="No-Break">CPU otherwise:</span><pre class="source-code">
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</pre></li>
<li>Instead<a id="_idIndexMarker498"/> of implementing GraphSAGE by ourselves, we can directly use PyTorch Geometric’s implementation from <strong class="source-inline">torch_geometric.nn</strong>. We initialize it with two layers<a id="_idIndexMarker499"/> and a hidden dimension of 512. In addition, we need to place the model on the same device as our data, <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">to(device)</strong></span><span class="No-Break">:</span><pre class="source-code">
from torch_geometric.nn import GraphSAGE
model = GraphSAGE(
    in_channels=train_dataset.num_features,
    hidden_channels=512,
    num_layers=2,
    out_channels=train_dataset.num_classes,
).to(device)</pre></li>
<li>The <strong class="source-inline">fit()</strong> function is similar to the one we used in the previous section, with two exceptions. First, we want to move the data to a GPU when possible. Secondly, we have two graphs per batch, so we multiply the individual loss by <span class="No-Break">two (</span><span class="No-Break"><strong class="source-inline">data.num_graphs</strong></span><span class="No-Break">):</span><pre class="source-code">
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
def fit():
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = criterion(out, data.y)
        total_loss += loss.item() * data.num_graphs
        loss.backward()
        optimizer.step()
    return total_loss / len(train_loader.dataset)</pre></li>
</ol>
<p>In the <strong class="source-inline">test()</strong> function, we<a id="_idIndexMarker500"/> take advantage of the fact that <strong class="source-inline">val_loader</strong> and <strong class="source-inline">test_loader</strong> have two graphs and a <strong class="source-inline">batch_size</strong> value of 2. That means that the two graphs are in the same batch; we don’t need to loop through the loaders like <span class="No-Break">during training.</span></p>
<ol>
<li value="7">Instead<a id="_idIndexMarker501"/> of accuracy, let’s use another metric – the F1 score. It corresponds to the harmonic mean of <a id="_idIndexMarker502"/>precision and recall. However, our predictions are 121-dim vectors of real numbers. We need to transform them into binary vectors, using <strong class="source-inline">out &gt; 0</strong> to compare them <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">data.y</strong></span><span class="No-Break">:</span><pre class="source-code">
from sklearn.metrics import f1_score
@torch.no_grad()
def test(loader):
    model.eval()
    data = next(iter(loader))
    out = model(data.x.to(device), data.edge_index.to(device))
    preds = (out &gt; 0).float().cpu()
    y, pred = data.y.numpy(), preds.numpy()
    return f1_score(y, pred, average='micro') if pred.sum() &gt; 0 else 0</pre></li>
<li>Let’s<a id="_idIndexMarker503"/> train our model for<a id="_idIndexMarker504"/> 300 epochs <a id="_idIndexMarker505"/>and print the validation F1 score <span class="No-Break">during training:</span><pre class="source-code">
for epoch in range(301):
    loss = fit()
    val_f1 = test(val_loader)
    if epoch % 50 == 0:
        print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Val F1 score: {val_f1:.4f}')
<strong class="bold">Epoch 0 | Train Loss: 0.589 | Val F1-score: 0.4245</strong>
<strong class="bold">Epoch 50 | Train Loss: 0.194 | Val F1-score: 0.8400</strong>
<strong class="bold">Epoch 100 | Train Loss: 0.143 | Val F1-score: 0.8779</strong>
<strong class="bold">Epoch 150 | Train Loss: 0.123 | Val F1-score: 0.8935</strong>
<strong class="bold">Epoch 200 | Train Loss: 0.107 | Val F1-score: 0.9013</strong>
<strong class="bold">Epoch 250 | Train Loss: 0.104 | Val F1-score: 0.9076</strong>
<strong class="bold">Epoch 300 | Train Loss: 0.090 | Val F1-score: 0.9154</strong></pre></li>
<li>Finally, we calculate the F1 score on the <span class="No-Break">test set:</span><pre class="source-code">
print(f'Test F1 score: {test(test_loader):.4f}')
<strong class="bold">Test F1 score: 0.9360</strong></pre></li>
</ol>
<p>We obtain <a id="_idIndexMarker506"/>an excellent F1 score of 0.9360 in an inductive setting. This value dramatically changes when you increase or decrease the size of the hidden channels. You can try it for yourself with different values, such as 128 and 1,024 instead <span class="No-Break">of 512.</span></p>
<p>If you<a id="_idIndexMarker507"/> look carefully at the code, there is no masking involved. Indeed, inductive learning is forced by the <strong class="source-inline">PPI</strong> dataset; the training, validation, and test data are in different graphs and loaders. Naturally, we could merge them using <strong class="source-inline">Batch.from_data_list()</strong> and fall back into a <span class="No-Break">transductive situation.</span></p>
<p>We could<a id="_idIndexMarker508"/> also train GraphSAGE without labels using unsupervised learning. This is particularly useful when labels are scarce or provided by downstream applications. However, it requires a new loss function to encourage nearby nodes to have similar representations while ensuring that distant nodes have <span class="No-Break">distant embeddings:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer345">
<img alt="" height="73" src="image/Formula_B19153_08_009.jpg" width="918"/>
</div>
</div>
<p>Here, <img alt="" height="38" src="image/Formula_B19153_08_014.png" width="20"/> is a neighbor of <img alt="" height="25" src="image/Formula_B19153_08_011.png" width="28"/> in a random walk, <img alt="" height="26" src="image/Formula_B19153_08_012.png" width="28"/> is the sigmoid function, <img alt="" height="43" src="image/Formula_B19153_08_013.png" width="89"/> is the negative sampling distribution for <img alt="" height="38" src="image/Formula_B19153_08_014.png" width="20"/>, and <img alt="" height="40" src="image/Formula_B19153_08_015.png" width="29"/> is the number of <span class="No-Break">negative samples:</span></p>
<p>Finally, PinSAGE and Uber Eats’ versions of GraphSAGE are recommender systems. They combine the unsupervised setting with a different loss because of this application. Their objective is to rank<a id="_idIndexMarker509"/> the most relevant <a id="_idIndexMarker510"/>entities (food, restaurants, pins, and so on) for each user, which is an entirely different task. To perform that, they implement a max-margin ranking loss that considers pairs <span class="No-Break">of embeddings.</span></p>
<p>If you need to <a id="_idIndexMarker511"/>scale up GNNs, other solutions can be considered. Here are short descriptions of two <span class="No-Break">standard techniques:</span></p>
<ul>
<li><strong class="bold">Cluster-GCN</strong> [6] provides a <a id="_idIndexMarker512"/>different answer to the question of how to create mini-batches. Instead of neighbor sampling, it divides the graph into isolated communities. These communities are then processed as independent graphs, which can negatively impact the quality of the <span class="No-Break">resulting embeddings.</span></li>
<li>Simplifying GNNs<a id="_idIndexMarker513"/> can decrease training and inference times. In practice, simplification consists of discarding nonlinear activation functions. Linear layers can then be compressed into one matrix multiplication using linear algebra. Naturally, these simplified versions are not as accurate as real GNNs on small datasets but are efficient for large graphs, such as <span class="No-Break">Twitter [7].</span></li>
</ul>
<p>As you can see, GraphSAGE is a flexible framework that can be tweaked and fine-tuned to suit your goals. Even if you don’t reuse its exact formulation, it introduces key concepts that greatly influence GNN architectures <span class="No-Break">in general.</span></p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor104"/>Summary</h1>
<p>This chapter introduced the GraphSAGE framework and its two components – the neighbor sampling algorithm and three aggregation operators. Neighbor sampling is at the core of GraphSAGE’s ability to process large graphs in a short amount of time. It is also responsible for its inductive setting, which allows it to generalize predictions to unseen nodes and graphs. We tested a transductive situation on <strong class="source-inline">PubMed</strong> and an inductive one to perform a new task on the <strong class="source-inline">PPI</strong> dataset – multi-label classification. While not as accurate as a GCN or a GAT, GraphSAGE is a popular and efficient framework for processing massive amounts <span class="No-Break">of data.</span></p>
<p>In <a href="B19153_09.xhtml#_idTextAnchor106"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Defining Expressiveness for Graph Classification</em>, we will try to define what makes a GNN powerful in terms of representation. We will introduce a famous graph algorithm called the Weisfeiler-Lehman isomorphism test. It will act as a benchmark to evaluate the theoretical performance of numerous GNN architectures, including the graph isomorphism network. We will apply this GNN to perform a new prevalent task – <span class="No-Break">graph classification.</span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor105"/>Further reading</h1>
<ul>
<li>[1] W. L. Hamilton, R. Ying, and J. Leskovec. <em class="italic">Inductive Representation Learning on Large Graphs</em>. arXiv, 2017. <span class="No-Break">DOI: 10.48550/ARXIV.1706.02216.</span></li>
<li>[2] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec. <em class="italic">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</em>. Jul. 2018. <span class="No-Break">DOI: 10.1145/3219819.3219890.</span></li>
<li>[3] Ankit Jain. <em class="italic">Food Discovery with Uber Eats: Using Graph Learning to Power </em><span class="No-Break"><em class="italic">Recommendations</em></span><span class="No-Break">: </span><a href="https://www.uber.com/en-US/blog/uber-eats-graph-learning/"><span class="No-Break">https://www.uber.com/en-US/blog/uber-eats-graph-learning/</span></a><span class="No-Break">.</span></li>
<li>[4]  Galileo Mark Namata, Ben London, Lise Getoor, and Bert Huang. <em class="italic">Query-Driven Active Surveying for Collective Classification</em>. International Workshop on Mining and Learning with <span class="No-Break">Graphs. 2012.</span></li>
<li>[5] M. Agrawal, M. Zitnik, and J. Leskovec. <em class="italic">Large-scale analysis of disease pathways in the human interactome</em>. Nov. 2017. <span class="No-Break">DOI: 10.1142/9789813235533_0011.</span></li>
<li>[6] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. <em class="italic">Cluster-GCN</em>. Jul. 2019. <span class="No-Break">DOI: 10.1145/3292500.3330925.</span></li>
<li>[7] F. Frasca, E. Rossi, D. Eynard, B. Chamberlain, M. Bronstein, and F. Monti. <em class="italic">SIGN: Scalable Inception Graph Neural Networks</em>. arXiv, 2020. <span class="No-Break">DOI: 10.48550/ARXIV.2004.11198.</span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer353">
</div>
</div>
</div></body></html>