- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: DQN Extensions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN扩展
- en: Since DeepMind published its paper on the deep Q-network (DQN) model in 2015,
    many improvements have been proposed, along with tweaks to the basic architecture,
    which, significantly, have improved the convergence, stability, and sample efficiency
    of DeepMind’s basic DQN. In this chapter, we will take a deeper look at some of
    those ideas.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自从DeepMind在2015年发布其深度Q网络（DQN）模型的论文以来，许多改进方案已经被提出，并对基础架构进行了调整，显著提高了DeepMind基础DQN的收敛性、稳定性和样本效率。本章将深入探讨其中的一些思想。
- en: 'In October 2017, Hessel et al. from DeepMind published a paper called Rainbow:
    Combining improvements in deep reinforcement learning [[Hes+18](#)], which presented
    the six most important improvements to DQN; some were invented in 2015, but others
    are relatively recent. In this paper, state-of-the-art results on the Atari games
    suite were reached, just by combining those six methods.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '2017年10月，DeepMind的Hessel等人发布了一篇名为《Rainbow: Combining improvements in deep reinforcement
    learning》的论文[[Hes+18](#)]，介绍了对DQN的六个最重要的改进；其中一些是在2015年发明的，但其他一些则较为近期。在这篇论文中，通过简单地结合这六个方法，达到了Atari游戏套件上的最先进成果。'
- en: Since 2017, more papers have been published and state-of-the-art results have
    been pushed further, but all the methods presented in the paper are still relevant
    and widely used in practice. For example, in 2023, Marc Bellemare published the
    book Distributional reinforcement learning [[BDR23](#)] about one of the paper’s
    methods. In addition, the improvements described are relatively simple to implement
    and understand, so I have not made any major modifications to this chapter in
    this edition.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年以来，更多的论文被发表，并且最先进的结果被进一步推动，但论文中介绍的所有方法仍然是相关的，并在实践中广泛使用。例如，在2023年，Marc
    Bellemare出版了《Distributional reinforcement learning》一书[[BDR23](#)]，书中讨论了论文中的一种方法。此外，所描述的改进相对简单易于实现和理解，因此在本版中我没有对这一章做重大修改。
- en: 'The DQN extensions that we will become familiar with are the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将熟悉的DQN扩展如下：
- en: 'N-step DQN: How to improve convergence speed and stability with a simple unrolling
    of the Bellman equation, and why it’s not an ultimate solution'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N步DQN：如何通过简单地展开贝尔曼方程提高收敛速度和稳定性，以及为什么它不是终极解决方案
- en: 'Double DQN: How to deal with DQN overestimation of the values of the actions'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双DQN：如何处理DQN对动作值的高估
- en: 'Noisy networks: How to make exploration more efficient by adding noise to the
    network weights'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声网络：如何通过给网络权重添加噪声来提高探索效率
- en: 'Prioritized replay buffer: Why uniform sampling of our experience is not the
    best way to train'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先回放缓冲区：为什么均匀采样我们的经验不是训练的最佳方式
- en: 'Dueling DQN: How to improve convergence speed by making our network’s architecture
    more closely represent the problem that we are solving'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗DQN：如何通过使我们的网络架构更紧密地反映我们正在解决的问题，来提高收敛速度
- en: 'Categorical DQN: How to go beyond the single expected value of the action and
    work with full distributions'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类DQN：如何超越单一的期望动作值，处理完整的分布
- en: This chapter will go through all these methods. We will analyze the ideas behind
    them, alongside how they can be implemented and compared to the classic DQN performance.
    Finally, we will analyze how the combined system with all the methods performs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍所有这些方法。我们将分析这些方法背后的思想，以及如何实现它们，并与经典的DQN性能进行比较。最后，我们将分析结合所有方法的系统表现。
- en: Basic DQN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础DQN
- en: To get started, we will implement the same DQN method as in Chapter [6](#),
    but leveraging the high-level primitives described in Chapter [7](ch011.xhtml#x1-1070007).
    This will make our code much more compact, which is good, as non-relevant details
    won’t distract us from the method’s logic. At the same time, the purpose of this
    book is not to teach you how to use the existing libraries but rather how to develop
    intuition about RL methods and, if necessary, implement everything from scratch.
    From my perspective, this is a much more valuable skill, as libraries come and
    go, but true understanding of the domain will allow you to quickly make sense
    of other people’s code and apply it consciously.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，我们将实现与第[6](#)章相同的DQN方法，但利用第[7](ch011.xhtml#x1-1070007)章中描述的高级原语。这将使我们的代码更加简洁，这是好的，因为无关的细节不会使我们偏离方法的逻辑。同时，本书的目的并非教你如何使用现有的库，而是如何培养对强化学习方法的直觉，必要时，从零开始实现一切。从我的角度来看，这是一个更有价值的技能，因为库会不断变化，但对领域的真正理解将使你能够迅速理解他人的代码，并有意识地应用它。
- en: 'In the basic DQN implementation, we have three modules in the Chapter08 folder
    of the GitHub repository for this book:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本的 DQN 实现中，我们在本书的 GitHub 仓库中的 Chapter08 文件夹中有三个模块：
- en: 'Chapter08/lib/dqn_model.py: The DQN neural network (NN), which is the same
    as in Chapter [6](#), so I won’t repeat it'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/lib/dqn_model.py: DQN 神经网络（NN），与第[6](#)章相同，因此我不会重复它。'
- en: 'Chapter08/lib/common.py: Common functions and declarations shared by the code
    in this chapter'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/lib/common.py: 本章代码共享的常用函数和声明。'
- en: 'Chapter08/01_dqn_basic.py: 77 lines of code leveraging the PTAN and Ignite
    libraries, implementing the basic DQN method'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/01_dqn_basic.py: 77 行代码，利用 PTAN 和 Ignite 库实现基本的 DQN 方法。'
- en: Common library
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公共库
- en: 'Let’s start with the contents of lib/common.py. First of all, we have hyperparameters
    for our Pong environment from the previous chapter. The hyperparameters are stored
    in the dataclass object, which is a standard way to store a bunch of data fields
    with their type annotations. This makes it easy to add another configuration set
    for different, more complicated Atari games and allows us to experiment with hyperparameters:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 lib/common.py 的内容开始。首先，我们有上一章中为 Pong 环境设置的超参数。这些超参数存储在一个数据类对象中，这是存储一组数据字段及其类型注释的标准方式。这样，我们可以轻松为不同、更复杂的
    Atari 游戏添加另一个配置集，并允许我们对超参数进行实验：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next function from lib/common.py has the name unpack_batch, and it takes
    the batch, of transitions and converts it into the set of NumPy arrays suitable
    for training. Every transition from ExperienceSourceFirstLast has a type of ExperienceFirstLast,
    which is a dataclass with the following fields:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: lib/common.py 中的下一个函数名为 unpack_batch，它接收转移的批次并将其转换为适合训练的 NumPy 数组集合。来自 ExperienceSourceFirstLast
    的每个转移都属于 ExperienceFirstLast 类型，这是一个数据类，包含以下字段：
- en: 'state: Observation from the environment.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'state: 来自环境的观测值。'
- en: 'action: Integer action taken by the agent.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'action: 代理执行的整数动作。'
- en: 'reward: If we have created ExperienceSourceFirstLast with the attribute steps_count=1,
    it’s just the immediate reward. For larger step counts, it contains the discounted
    sum of rewards for this number of steps.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: 如果我们创建了 ExperienceSourceFirstLast 并设置了属性 steps_count=1，那么它只是即时奖励。对于更大的步数计数，它包含了这个步数内的奖励的折扣总和。'
- en: 'last_state: If the transition corresponds to the final step in the environment,
    then this field is None; otherwise, it contains the last observation in the experience
    chain.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'last_state: 如果转移对应于环境中的最后一步，那么这个字段为 None；否则，它包含经验链中的最后一个观测值。'
- en: 'The code of unpack_batch is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: unpack_batch 的代码如下：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note how we handle the final transitions in the batch. To avoid the special
    handling of such cases, for terminal transitions, we store the initial state in
    the last_states array. To make our calculations of the Bellman update correct,
    we have to mask such batch entries during the loss calculation using the dones
    array. Another solution would be to calculate the value of the last states only
    for non-terminal transitions, but it would make our loss function logic a bit
    more complicated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们如何处理批次中的最终转移。为了避免对这种情况的特殊处理，对于终止转移，我们将初始状态存储在 last_states 数组中。为了使我们的 Bellman
    更新计算正确，我们必须在损失计算时使用 dones 数组对这些批次条目进行掩码。另一种解决方案是仅对非终止转移计算最后状态的值，但这会使我们的损失函数逻辑稍微复杂一些。
- en: 'Calculation of the DQN loss function is provided by the calc_loss_dqn function,
    and the code is almost the same as in Chapter [6](#). One small addition is torch.no_grad(),
    which stops the PyTorch calculation graph from being recorded for the target net:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 损失函数的计算由 calc_loss_dqn 函数提供，代码几乎与第[6](#)章相同。唯一的小改动是 torch.no_grad()，它阻止了
    PyTorch 计算图被记录到目标网络中：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Besides those core DQN functions, common.py provides several utilities related
    to our training loop, data generation, and TensorBoard tracking. The first such
    utility is a small class that implements epsilon decay during the training. Epsilon
    defines the probability of taking the random action by the agent. It should be
    decayed from 1.0 in the beginning (fully random agent) to some small number, like
    0.02 or 0.01\. The code is trivial but is needed in almost any DQN, so it is provided
    by the following little class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心的 DQN 函数外，common.py 还提供了与训练循环、数据生成和 TensorBoard 跟踪相关的多个实用工具。第一个这样的工具是一个小类，它在训练过程中实现了
    epsilon 衰减。Epsilon 定义了代理执行随机动作的概率。它应从 1.0 开始（完全随机的代理），逐渐衰减到某个小值，比如 0.02 或 0.01。这个代码非常简单，但几乎在任何
    DQN 中都需要，因此通过以下小类提供：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Another small function is batch_generator, which takes ExperienceReplayBuffer
    (the PTAN class described in Chapter [7](ch011.xhtml#x1-1070007)) and infinitely
    generates training batches sampled from the buffer. In the beginning, the function
    ensures that the buffer contains the required amount of samples:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个小函数是 batch_generator，它接收 ExperienceReplayBuffer（PTAN 类，在第[7](ch011.xhtml#x1-1070007)章中描述）并无限次生成从缓冲区中采样的训练批次。开始时，函数确保缓冲区包含所需数量的样本：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, a lengthy, but nevertheless very useful, function called setup_ignite
    attaches the needed Ignite handlers, showing the training progress and writing
    metrics to TensorBoard. Let’s look at this function piece by piece:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个冗长但非常有用的函数叫做 setup_ignite，它附加了所需的 Ignite 处理器，显示训练进度并将度量写入 TensorBoard。让我们一块儿看这个函数：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initially, setup_ignite attaches two Ignite handlers provided by PTAN:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，setup_ignite 附加了 PTAN 提供的两个 Ignite 处理器：
- en: EndOfEpisodeHandler, which emits the Ignite event every time a game episode
    ends. It can also fire an event when the averaged reward for episodes crosses
    some boundary. We use this to detect when the game is finally solved.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EndOfEpisodeHandler，每当游戏回合结束时，它会触发 Ignite 事件。当回合的平均奖励超过某个边界时，它还可以触发事件。我们用它来检测游戏何时最终解决。
- en: EpisodeFPSHandler, a small class that tracks the time the episode has taken
    and the amount of interactions that we have had with the environment. From this,
    we calculate frames per second (FPS), which is an important performance metric
    to track.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EpisodeFPSHandler，这是一个小类，跟踪每个回合所花费的时间以及我们与环境交互的次数。根据这些信息，我们计算每秒帧数（FPS），它是一个重要的性能度量指标。
- en: 'Then, we install two event handlers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们安装两个事件处理器：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One of the event handlers is called at the end of an episode. It will show information
    about the completed episode on the console. Another function will be called when
    the average reward grows above the boundary defined in the hyperparameters (18.0
    in the case of Pong). This function shows a message about the solved game and
    stops the training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个事件处理器会在回合结束时被调用。它将在控制台上显示有关已完成回合的信息。另一个函数会在平均奖励超过超参数中定义的边界时被调用（在 Pong 的情况下是
    18.0）。此函数显示关于已解决游戏的消息，并停止训练。
- en: 'The rest of the function is related to the TensorBoard data that we want to
    track. First, we create a TensorboardLogger:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的其余部分与我们想要跟踪的 TensorBoard 数据有关。首先，我们创建一个 TensorboardLogger：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a special class provided by Ignite to write into TensorBoard. Our processing
    function will return the loss value, so we attach the RunningAverage transformation
    (also provided by Ignite) to get a smoothed version of the loss over time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Ignite 提供的一个特殊类，用于写入 TensorBoard。我们的处理函数将返回损失值，因此我们附加了 RunningAverage 转换（同样由
    Ignite 提供），以获取随时间平滑的损失版本。
- en: 'Next, we attach the metrics we want to track to the Ignite events:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将要跟踪的度量值附加到 Ignite 事件：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'TensorboardLogger can track two groups of values from Ignite: outputs (values
    returned by the transformation function) and metrics (calculated during the training
    and kept in the engine state). EndOfEpisodeHandler and EpisodeFPSHandler provide
    metrics, which are updated at the end of every game episode. So, we attach OutputHandler,
    which will write into TensorBoard information about the episode every time it
    is completed.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TensorboardLogger 可以跟踪来自 Ignite 的两组值：输出（由转换函数返回的值）和度量（在训练过程中计算并保存在引擎状态中）。EndOfEpisodeHandler
    和 EpisodeFPSHandler 提供度量，这些度量在每个游戏回合结束时更新。因此，我们附加了 OutputHandler，每当回合完成时，它将把有关该回合的信息写入
    TensorBoard。
- en: 'Next, we track another group of values, metrics from the training process:
    loss, FPS, and, possibly, some custom metrics relevant to the specific extension’s
    logic:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们跟踪训练过程中的另一组值，训练过程中的度量值：损失、FPS，以及可能与特定扩展逻辑相关的自定义度量：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Those values are updated every training iteration, but we are going to do millions
    of iterations, so we will store values in TensorBoard every 100 training iterations;
    otherwise, the data files will be huge. All this functionality might look too
    complicated, but it provides us with the unified set of metrics gathered from
    the training process. In fact, Ignite is not very tricky, given the flexibility
    it provides. That’s it for common.py.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值会在每次训练迭代时更新，但我们将进行数百万次迭代，因此我们每进行 100 次训练迭代就将值存储到 TensorBoard；否则，数据文件会非常大。所有这些功能看起来可能很复杂，但它为我们提供了从训练过程中收集的统一度量集。事实上，Ignite
    并不复杂，考虑到它所提供的灵活性。common.py 就到这里。
- en: Implementation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'Now, let’s take a look at 01_dqn_basic.py, which creates the needed classes
    and starts the training. I’m going to omit non-relevant code and focus only on
    important pieces (the full version is available in the GitHub repo). First, we
    create the environment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 01_dqn_basic.py，它创建了所需的类并开始训练。我将省略不相关的代码，只关注重要部分（完整版本可以在 GitHub 仓库中找到）。首先，我们创建环境：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we apply a set of standard wrappers. We discussed them in Chapter [6](#)
    and will also touch upon them in the next chapter, when we optimize the performance
    of the Pong solver. Then, we create the DQN model and the target network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用一组标准包装器。我们在第[6](#)章中讨论了这些包装器，并且在下一章中，当我们优化 Pong 求解器的性能时，还会再次涉及到它们。然后，我们创建
    DQN 模型和目标网络。
- en: 'Next, we create the agent, passing it an epsilon-greedy action selector:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建代理，并传入一个 epsilon-greedy 动作选择器：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: During the training, epsilon will be decreased by the EpsilonTracker class that
    we have already discussed. This will decrease the amount of randomly selected
    actions and give more control to our NN.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，epsilon 将由我们之前讨论过的 EpsilonTracker 类进行减少。这将减少随机选择的动作数量，并给予我们的神经网络更多的控制权。
- en: 'The next two very important objects are ExperienceSourceFirstLast and ExperienceReplayBuffer:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，两个非常重要的对象是 ExperienceSourceFirstLast 和 ExperienceReplayBuffer：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ExperienceSourceFirstLast takes the agent and environment and provides transitions
    over game episodes. Those transitions will be kept in the experience replay buffer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ExperienceSourceFirstLast 接收代理和环境，并在游戏回合中提供过渡。这些过渡将被保存在经验回放缓冲区中。
- en: 'Then we create an optimizer and define the processing function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建优化器并定义处理函数：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The processing function will be called for every batch of transitions to train
    the model. To do this, we call the common.calc_loss_dqn function and then backpropagate
    on the result. This function also asks EpsilonTracker to decrease the epsilon
    and does periodical target network synchronization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 处理函数将在每批过渡时被调用以训练模型。为此，我们调用 common.calc_loss_dqn 函数，然后对结果进行反向传播。该函数还会要求 EpsilonTracker
    减少 epsilon，并进行定期的目标网络同步。
- en: 'And, finally, we create the Ignite Engine object:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建 Ignite Engine 对象：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We configure it using a function from common.py, and run our training process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 common.py 的函数进行配置，并运行训练过程。
- en: Hyperparameter tuning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: To make our comparison of DQN extensions fair, we also need to tune hyperparameters.
    This is essential because even for the same game (Pong), using the fixed set of
    training parameters might give less optimal results when we change the details
    of the method.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们对 DQN 扩展的比较更加公平，我们还需要调优超参数。这一点至关重要，因为即使对于相同的游戏（Pong），使用固定的训练参数集可能在我们改变方法细节时给出较差的结果。
- en: 'In principle, every explicit or implicit constant in our code could be tuned,
    such as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，我们代码中的每个显式或隐式常量都可以进行调优，例如：
- en: 'Network configuration: Amount and size of layers, activation function, dropout,
    etc.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络配置：层的数量和大小，激活函数，dropout 等
- en: 'Optimization parameters: Method (vanilla SGD, Adam, AdaGrad, etc.), learning
    rate, and other optimizer parameters'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化参数：方法（原生 SGD、Adam、AdaGrad 等）、学习率和其他优化器参数
- en: 'Exploration parameters: Decay rate of 𝜖, final 𝜖 value'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索参数：𝜖 的衰减率，最终 𝜖 值
- en: Discount factor γ in Bellman equation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman 方程中的折扣因子 γ
- en: But every new parameter we tune has a multiplicative effect on the amount of
    trial training we need to perform, so having too many hyperparameters might require
    hundreds or even thousands of trainings. Large companies like Google and Meta
    have access to a much larger amount of GPUs than individual researchers like us,
    so we need to keep the balance there.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们调整的每个新参数都会对所需的试验训练量产生乘法效应，因此调节过多的超参数可能需要进行数百次甚至上千次训练。像 Google 和 Meta 这样的大公司拥有比我们这些个人研究者更多的
    GPU 资源，所以我们需要在这里保持平衡。
- en: 'In my case, I’m going to demonstrate how hyperparameter tuning is done in general,
    but we’ll do the search only on a few values:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的例子中，我将演示如何进行超参数调优，但我们只会在少数几个值上进行搜索：
- en: Learning rate
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Discount factor γ
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣因子 γ
- en: Parameters specific to the DQN extension we’re considering
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在考虑的 DQN 扩展特定的参数
- en: 'There are several libraries that might be helpful with hyperparameter tuning.
    Here, I’m using Ray Tune ([https://docs.ray.io/en/latest/tune/index.xhtml](https://docs.ray.io/en/latest/tune/index.xhtml)),
    which is a part of the Ray project — a distributed computing framework for ML
    and DL. At a high level, you need to define:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个库可能对超参数调整有所帮助。这里，我使用的是Ray Tune（[https://docs.ray.io/en/latest/tune/index.xhtml](https://docs.ray.io/en/latest/tune/index.xhtml)），它是Ray项目的一部分——一个用于机器学习和深度学习的分布式计算框架。从高层次来看，你需要定义：
- en: The hyperparameter space you want to explore (boundaries for values to sample
    from or an explicit list of values to try)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望探索的超参数空间（值的边界或显式列出的尝试值列表）
- en: The function that performs the training with specific values of hyperparameters
    and returns the metric you want to optimize with the tuning
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该函数执行使用特定超参数值的训练，并返回你想要优化的度量。
- en: 'This might look very similar to ML problems, and in fact it is — this is also
    an optimization problem. But there are substantial differences: the function we’re
    optimizing is not differentiable (so you cannot perform the gradient descent to
    push your hyperparameters towards the desired direction of the metric) and the
    optimization space might be discrete (you cannot train the network with the number
    of layers equal to 2.435, for example, since we cannot take the derivative of
    a non-smooth function).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来与机器学习问题非常相似，事实上它确实是——这也是一个优化问题。但它有一些显著的不同：我们正在优化的函数是不可微分的（因此无法执行梯度下降来推动超参数朝向期望的度量方向），而且优化空间可能是离散的（例如，你无法用2.435层的神经网络进行训练，因为我们无法对一个不平滑的函数求导）。
- en: In later chapters, we’ll touch on this problem slightly in the context of black-box
    optimization methods (Chapter [17](ch021.xhtml#x1-31100017)) and RL in discrete
    optimizations (Chapter [21](ch025.xhtml#x1-39100021)), but for now, we’ll use
    the simplest approach — a random search of hyperparameters. In this case, the
    ray.tune library randomly samples concrete parameters several times and calls
    the function to obtain the metric. The smallest (or highest) metric corresponds
    to the best hyperparameter combination found in this run.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们会稍微触及一下这个问题，讨论黑箱优化方法（第[17章](ch021.xhtml#x1-31100017)）和离散优化中的强化学习（第[21章](ch025.xhtml#x1-39100021)），但现在我们将使用最简单的方法——超参数的随机搜索。在这种情况下，`ray.tune`库会随机多次采样具体的参数，并调用函数以获得度量。最小（或最大）的度量值对应于在此次运行中找到的最佳超参数组合。
- en: In this chapter, our metric (optimization objective) will be the number of games
    the agent needs to play before solving the game (reaching a mean score of greater
    than 18 for Pong).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们的度量（优化目标）将是代理需要玩多少局游戏才能解决游戏（即在Pong中达到大于18的平均得分）。
- en: To illustrate the effect of tuning, for every DQN extension, we check the training
    dynamics using a fixed set of parameters (the same as in Chapter [6](#)), and
    the dynamics using the best hyperparameters found after 20-30 rounds of tuning.
    If you wish, you can do your own experiments, optimizing more hyperparameters.
    Most likely, this will allow you to find a better configuration for the training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明调整的效果，对于每个DQN扩展，我们使用一组固定的参数（与第[6章](#)相同）检查训练动态，并使用在20-30轮调整后找到的最佳超参数进行训练。如果你愿意，你可以做自己的实验，优化更多的超参数。最有可能的是，这将使你能够找到一个更好的训练配置。
- en: 'The core of the process is implemented in the common.tune_params function.
    Let’s take a look at its code. We start with the type declaration and hyperparameter
    space:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的核心实现是在`common.tune_params`函数中。让我们看看它的代码。我们从类型声明和超参数空间开始：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we first define the type for the training function, which takes the Hyperparams
    dataclass torch.device to use and a dictionary with extra parameters (as some
    DQN extensions we’re going to present might require extra parameters besides those
    declared in Hyperparams).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先定义训练函数的类型，它接收一个`Hyperparams`数据类、一个要使用的`torch.device`，以及一个包含额外参数的字典（因为我们即将介绍的某些DQN扩展可能需要除了在`Hyperparams`中声明的参数以外的额外参数）。
- en: The result of the function is either the int value, which will be the amount
    of games we played before reaching the score of 18, or None if we decided to stop
    the training early. This is required, as some hyperparameter combinations might
    fail to converge or converge too slowly, so to save time we stop the training
    without waiting for too long.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的结果可以是一个整数值，表示在达到18分的得分之前我们玩了多少局游戏，或者是None，如果我们决定提前停止训练。这是必需的，因为某些超参数组合可能无法收敛或收敛得太慢，因此为了节省时间，我们会在不等待太久的情况下停止训练。
- en: Then we define the hyperparameter search space — which is a dict with string
    keys (parameter name) and the tune declaration of possible values to explore.
    It could be a probability distribution (uniform, loguniform, normal, etc.) or
    an explicit list of values to try. You can also use tune.grid_search declaration
    with a list of values. In that case, all the values will be tried.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义超参数搜索空间——这是一个具有字符串键（参数名）和可能值探索的 `tune` 声明的字典。它可以是一个概率分布（均匀、对数均匀、正态等）或要尝试的显式值列表。你还可以使用
    `tune.grid_search` 声明，提供一个值列表。在这种情况下，将尝试所有值。
- en: In our case, we sample the learning rate from the loguniform distribution and
    gamma from the list of 6 values ranging from 0.9 to 0.995.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们从对数均匀分布中采样学习率，并从一个包含 6 个值（范围从 0.9 到 0.995）的列表中采样 gamma。
- en: 'Next, we have the tune_params function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有 `tune_params` 函数：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This function is given the following arguments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数给定以下参数：
- en: Basic set of hyperparameters that will be used for training
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的基础超参数集
- en: Training function
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练函数
- en: Torch device to use
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的 Torch 设备
- en: Amount of samples to perform during the round
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回合中执行的样本数量
- en: Additional dictionary with search space
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有搜索空间的附加字典
- en: Inside this function, we have an objective function, which creates the Hyperparameters
    object from the sampled dict, calls the training function, and returns the dictionary
    (which is a requirement of the ray.tune library).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数中，我们有一个目标函数，它从采样的字典中创建 `Hyperparameters` 对象，调用训练函数，并返回字典（这是 ray.tune 库的要求）。
- en: 'The rest of the tune_params function is simple:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`tune_params` 函数的其余部分很简单：'
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we wrap the objective function to pass the torch device and take into
    account GPU resources. This is needed to allow Ray to properly parallelize the
    tuning process. If you have multiple GPUs installed on the machine, it will run
    several trainings in parallel. Then, we just create the Tuner object and ask it
    to perform the hyperparameter search.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们包装目标函数，以传递 Torch 设备并考虑 GPU 资源。这是为了让 Ray 能够正确地并行化调优过程。如果你机器上安装了多个 GPU，它将并行运行多个训练。然后，我们只需创建
    `Tuner` 对象，并要求它执行超参数搜索。
- en: 'The final piece relevant to hyperparameter tuning is in the setup_ignite function.
    It checks for situations when the training process is not converging, so we stop
    the training to avoid infinite waiting. To do this, we install the Ignite event
    handler if we’re in the hyperparameter tuning mode:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与超参数调优相关的最后一部分代码在 `setup_ignite` 函数中。它检查训练过程是否没有收敛，如果没有收敛，则停止训练以避免无限等待。为此，我们在超参数调优模式下安装
    Ignite 事件处理程序：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we check for two conditions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们检查两个条件：
- en: If the mean reward is lower than tuner_reward_min (which is an argument to the
    setup_ignite function and equal to -19 by default) after 100 games (provided in
    the tuner_reward_episode argument). This means that it’s quite unlikely that we’ll
    converge at all.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果平均奖励低于 `tuner_reward_min`（这是 `setup_ignite` 函数的一个参数，默认为 -19），并且在 100 局游戏后（由
    `tuner_reward_episode` 参数提供），这意味着我们几乎不可能收敛。
- en: We played more than max_episodes amount of games and still haven’t solved the
    game. In the default config, we set this limit to 500 games.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经玩了超过 `max_episodes` 局游戏，仍然没有解决游戏。在默认配置中，我们将此限制设置为 500 局游戏。
- en: In both cases, we stop the training and set the solved attribute to False, which
    will return a high constant metric value in our tuning process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们都会停止训练并将 `solved` 属性设置为 `False`，这将在调优过程中返回一个较高的常数指标值。
- en: That’s it for the hyperparameter tuning code. Before we run it and check the
    results, let’s first start a single training using the parameters we used in Chapter [6](#).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是超参数调优代码的全部内容。在运行并检查结果之前，让我们首先使用我们在第 [6](#) 章中使用的参数开始一次单次训练。
- en: Results with common parameters
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用常见参数的结果
- en: If we run the training with the argument --params common, we’ll train the Pong
    game using hyperparameters from the common.py module. As an option, you can use
    the --params best command line to train on the best values for this particular
    DQN extension.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用参数 `--params common` 运行训练，我们将使用来自 `common.py` 模块的超参数训练 Pong 游戏。作为选项，你可以使用
    `--params best` 命令行来训练该 DQN 扩展的最佳值。
- en: 'Okay, let’s start the training using the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们使用以下命令开始训练：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Every line in the output is written at the end of the game episode, showing
    the episode reward, a count of steps, the speed, and the total training time.
    For the basic DQN version and common hyperparameters, it usually takes about 700K
    frames and about 400 games to reach the mean reward of 18, so be patient. During
    the training, we can check the dynamics of the training process in TensorBoard,
    which shows charts for epsilon, raw reward values, average reward, and speed.
    The following charts show the reward and the number of steps for episodes (the
    bottom x axis shows the wall clock time, and the top axis is the episode number):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的每一行都是在游戏回合结束时写入的，显示回合奖励、步数、速度和总训练时间。对于基础的DQN版本和常见的超参数，通常需要大约70万帧和约400局游戏才能达到18的平均奖励，因此需要耐心。在训练过程中，我们可以在TensorBoard中查看训练过程的动态，里面显示了ε值、原始奖励值、平均奖励和速度的图表。以下图表显示了每回合的奖励和步数（底部x轴表示墙钟时间，顶部x轴表示回合数）：
- en: '![PIC](img/B22150_08_01.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_01.png)'
- en: 'Figure 8.1: Plots with reward (left) and count of steps per episode (right)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：奖励图（左）和每回合步数图（右）
- en: '![PIC](img/B22150_08_02.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_02.png)'
- en: 'Figure 8.2: Plots with training speed (left) and average training loss (right)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：训练速度图（左）和平均训练损失图（右）
- en: It is also worth noting how the count of steps per episode changes during the
    training. Initially, it increases, as our network starts winning more and more
    games, but after a certain level, the count of steps decreases 2x and stays almost
    constant. This is driven by our γ parameter, which discounts the agent’s reward
    over time, so it tries not just to accumulate as much of a reward as possible,
    but also to do it efficiently.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是每回合步数在训练过程中是如何变化的。最开始时，步数增加，因为我们的网络开始赢得越来越多的游戏，但在达到某个水平后，步数减少了2倍并几乎保持不变。这是由我们的γ参数驱动的，它会随着时间的推移折扣智能体的奖励，所以它不仅仅是尽可能多地积累奖励，还要高效地完成任务。
- en: Tuned baseline DQN
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整过的基准 DQN
- en: 'After running the baseline DQN with the command-line argument --tune 30 (which
    took about a day on one GPU), I was able to find the following parameters, which
    solves Pong in 340 episodes (instead of 360):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用命令行参数--tune 30（这在一块GPU上花费了大约一天）运行基准DQN之后，我找到了以下参数，这可以在340回合内解决Pong问题（而不是360回合）：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, the learning rate is almost the same as before (10^(−4)), but
    gamma is lower (0.98 versus 0.99). This might be an indication that Pong has relatively
    short subtrajectories with action-reward causality, so decreasing the γ has a
    stabilizing effect on the training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，学习率几乎与之前一样（10^(−4)），但γ值较低（0.98对比0.99）。这可能表明Pong有相对较短的子轨迹与动作-奖励因果关系，因此减少γ对训练有稳定作用。
- en: 'In the following figure, you can see a comparison of the reward and steps per
    episode for both tuned and untuned versions (and the difference is quite minor):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到调整过和未调整版本的奖励与每个回合步数的比较（区别非常小）：
- en: '![PIC](img/B22150_08_03.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_03.png)'
- en: 'Figure 8.3: Plots with reward (left) and count of steps per episode (right)
    for tuned and untuned hyperparameters'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：调整过的和未调整超参数的奖励图（左）和每回合步数图（右）
- en: Now we have our baseline DQN version and are ready to explore method modifications
    proposed by Hessel et al.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了基准DQN版本，并准备探索Hessel等人提出的改进方法。
- en: N-step DQN
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N步DQN
- en: 'The first improvement that we will implement and evaluate is quite an old one.
    It was first introduced by Sutton in the paper Learning to Predict by the Methods
    of Temporal Differences [[Sut88](#)]. To get the idea, let’s look at the Bellman
    update used in Q-learning once again:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实现并评估的第一个改进是一个比较老的方法。它最早由Sutton在论文《通过时间差分方法学习预测》[[Sut88](#)]中提出。为了理解这个方法，我们再看一遍Q-learning中使用的Bellman更新：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq26.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq26.png)'
- en: 'This equation is recursive, which means that we can express Q(s[t+1],a[t+1])
    in terms of itself, which gives us this result:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是递归的，这意味着我们可以用自身来表示Q(s[t+1],a[t+1])，从而得到这个结果：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq27.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq27.png)'
- en: 'Value r[a,t+1] means local reward at time t + 1, after issuing action a. However,
    if we assume that action a at step t + 1 was chosen optimally, or close to optimally,
    we can omit the max[a] operation and obtain this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 值r[a,t+1]表示在时间t + 1时发出动作a后的局部奖励。然而，如果我们假设在t + 1步时的动作a是最优选择或接近最优选择，我们可以省略max[a]操作，得到以下结果：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq28.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq28.png)'
- en: 'This value can be unrolled again and again any number of times. As you may
    guess, this unrolling can be easily applied to our DQN update by replacing one-step
    transition sampling with longer transition sequences of n-steps. To understand
    why this unrolling will help us to speed up training, let’s consider the example
    illustrated in Figure [8.4](#x1-131004r4). Here, we have a simple environment
    of four states (s[1], s[2], s[3], s[4]) and the only action available at every
    state, except s[4], which is a terminal state:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值可以反复展开，次数不限。正如你可能猜到的，这种展开可以轻松应用到我们的 DQN 更新中，通过用更长的 n 步转移序列替换一步转移采样。为了理解为什么这种展开可以帮助我们加速训练，让我们考虑图
    [8.4](#x1-131004r4) 中的示例。这里，我们有一个简单的四状态环境（s[1]、s[2]、s[3]、s[4]），除了终止状态 s[4] 外，每个状态都有唯一可执行的动作：
- en: '![ssssararar1234123 ](img/B22150_08_04.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![ssssararar1234123 ](img/B22150_08_04.png)'
- en: 'Figure 8.4: A transition diagram for a simple environment'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：一个简单环境的转移图
- en: 'So, what happens in a one-step case? We have three total updates possible (we
    don’t use max, as there is only one action available):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一步情况下会发生什么呢？我们总共有三个更新是可能的（我们不使用 max，因为只有一个可执行动作）：
- en: Q(s[1],a) ←r[1] + γQ(s[2],a)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[1],a) ←r[1] + γQ(s[2],a)
- en: Q(s[2],a) ←r[2] + γQ(s[3],a)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[2],a) ←r[2] + γQ(s[3],a)
- en: Q(s[3],a) ←r[3]
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[3],a) ←r[3]
- en: Let’s imagine that, at the beginning of the training, we complete the preceding
    updates in this order. The first two updates will be useless, as our current Q(s[2],a)
    and Q(s[3],a) are incorrect and contain initial random values. The only useful
    update will be update 3, which will correctly assign reward r[3] to the state
    s[3] prior to the terminal state.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在训练开始时，我们按照这个顺序完成之前的更新。前两个更新将没有用，因为我们当前的 Q(s[2],a) 和 Q(s[3],a) 是不正确的，且包含初始的随机值。唯一有用的更新是更新
    3，它会将奖励 r[3] 正确地分配给终止状态之前的状态 s[3]。
- en: Now let’s perform the updates over and over again. On the second iteration,
    the correct value will be assigned to Q(s[2],a), but the update of Q(s[1],a) will
    still be noisy. Only on the third iteration will we get the valid values for every
    Q. So, even in a one-step case, it takes three steps to propagate the correct
    values to all the states.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一次次执行这些更新。在第二次迭代时，Q(s[2],a) 会被赋予正确的值，但 Q(s[1],a) 的更新仍然会带有噪声。直到第三次迭代，我们才会为每个
    Q 获得有效的值。所以，即使是在一步情况下，也需要三步才能将正确的值传播到所有状态。
- en: 'Now let’s consider a two-step case. This situation again has three updates:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个两步的情况。这个情况同样有三个更新：
- en: Q(s[1],a) ←r[1] + γr[2] + γ²Q(s[3],a)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[1],a) ←r[1] + γr[2] + γ²Q(s[3],a)
- en: Q(s[2],a) ←r[2] + γr[3]
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[2],a) ←r[2] + γr[3]
- en: Q(s[3],a) ←r[3]
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[3],a) ←r[3]
- en: In this case, on the first loop over the updates, the correct values will be
    assigned to both Q(s[2],a) and Q(s[3],a). On the second iteration, the value of
    Q(s[1],a) will also be properly updated. So, multiple steps improve the propagation
    speed of values, which improves convergence. You may be thinking, “If it’s so
    helpful, let’s unroll the Bellman equation, say, 100 steps ahead. Will it speed
    up our convergence 100 times?” Unfortunately, the answer is no. Despite our expectations,
    our DQN will fail to converge at all.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，在第一次更新循环中，正确的值将分别分配给 Q(s[2],a) 和 Q(s[3],a)。在第二次迭代中，Q(s[1],a) 的值也将得到正确更新。因此，多步操作提高了值的传播速度，从而改善了收敛性。你可能会想，“如果这样这么有帮助，那我们不妨将
    Bellman 方程展开 100 步。这样会让我们的收敛速度加快 100 倍吗？”不幸的是，答案是否定的。尽管我们有所期待，我们的 DQN 完全无法收敛。
- en: To understand why, let’s again return to our unrolling process, especially where
    we dropped the max[a]. Was it correct? Strictly speaking, no. We omitted the max
    operation at the intermediate step, assuming that our action selection during
    experience gathering (or our policy) was optimal. What if it wasn’t, for example,
    at the beginning of the training, when our agent acted randomly? In that case,
    our calculated value for Q(s[t],a[t]) may be smaller than the optimal value of
    the state (as some steps have been taken randomly, but not following the most
    promising paths by maximizing the Q-value). The more steps on which we unroll
    the Bellman equation, the more incorrect our update could be.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么如此，我们再次回到我们的展开过程，特别是我们省略了 max[a]。这样做对吗？严格来说，答案是否定的。我们在中间步骤省略了 max 操作，假设我们在经验收集过程中（或者我们的策略）是最优的。假如不是呢？例如，在训练初期，我们的智能体是随机行为的。在这种情况下，我们计算出的
    Q(s[t],a[t]) 值可能小于该状态的最优值（因为某些步骤是随机执行的，而不是通过最大化 Q 值来遵循最有希望的路径）。我们展开 Bellman 方程的步数越多，我们的更新可能就越不准确。
- en: Our large experience replay buffer will make the situation even worse, as it
    will increase the chance of getting transitions obtained from the old bad policy
    (dictated by old bad approximations of Q). This will lead to a wrong update of
    the current Q approximation, so it can easily break our training progress. This
    problem is a fundamental characteristic of RL methods, as was briefly mentioned
    in Chapter [4](ch008.xhtml#x1-740004), when we talked about RL methods’ taxonomy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大型经验回放缓冲区将使情况变得更糟，因为它会增加从旧的糟糕策略（由旧的糟糕Q近似所决定）获得过渡的机会。这将导致当前Q近似的错误更新，从而很容易破坏我们的训练进程。这个问题是强化学习方法的一个基本特征，正如我们在第[4](ch008.xhtml#x1-740004)章简要提到的，当时我们讨论了强化学习方法的分类。
- en: 'There are two large classes of methods:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有两大类方法：
- en: 'Off-policy methods: The first class of off-policy methods doesn’t depend on
    the “freshness of data.” For example, a simple DQN is off-policy, which means
    that we can use very old data sampled from the environment several million steps
    ago, and this data will still be useful for learning. That’s because we are just
    updating the value of the action, Q(s[t],a[t]), with the immediate reward, plus
    the discounted current approximation of the best action’s value. Even if action
    a[t] was sampled randomly, it doesn’t matter because for this particular action
    a[t], in the state s[t], our update will be correct. That’s why in off-policy
    methods, we can use a very large experience buffer to make our data closer to
    being independent and identically distributed (iid) .'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于非策略的方法：第一类基于非策略的方法不依赖于“数据的新鲜度”。例如，简单的DQN就是基于非策略的，这意味着我们可以使用几百万步之前从环境中采样的非常旧的数据，这些数据仍然对学习有用。这是因为我们只是用即时奖励加上最佳行动价值的当前折扣近似来更新动作的价值Q(s[t],a[t])。即使动作a[t]是随机采样的，也无关紧要，因为对于这个特定的动作a[t]，在状态s[t]下，我们的更新是正确的。这就是为什么在基于非策略的方法中，我们可以使用一个非常大的经验缓冲区，使我们的数据更接近独立同分布（iid）。
- en: 'On-policy methods: On the other hand, on-policy methods heavily depend on the
    training data to be sampled according to the current policy that we are updating.
    That happens because on-policy methods try to improve the current policy indirectly
    (as in the previous n-step DQN) or directly (all of Part 3 of the book is devoted
    to such methods).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略的方法：另一方面，基于策略的方法严重依赖于根据我们正在更新的当前策略来采样的训练数据。这是因为基于策略的方法试图间接（如之前的n步DQN）或直接（本书第三部分的内容完全是关于这种方法）改进当前策略。
- en: So, which class of methods is better? Well, it depends. Off-policy methods allow
    you to train on the previous large history of data or even on human demonstrations,
    but they usually are slower to converge. On-policy methods are typically faster,
    but require much more fresh data from the environment, which can be costly. Just
    imagine a self-driving car trained with the on-policy method. It will cost you
    a lot of crashed cars before the system learns that walls and trees are things
    that it should avoid!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪种方法更好呢？嗯，这取决于。基于非策略的方法允许你在先前的大量数据历史上进行训练，甚至在人工示范上进行训练，但它们通常收敛较慢。基于策略的方法通常更快，但需要更多来自环境的新鲜数据，这可能会很昂贵。试想一下，使用基于策略的方法训练一个自动驾驶汽车。在系统学会避开墙壁和树木之前，你得花费大量的撞车成本！
- en: 'You may have a question: why are we talking about an n-step DQN if this “n-stepness”
    turns it into an on-policy method, which will make our large experience buffer
    useless? In practice, this is usually not black and white. You may still use an
    n-step DQN if it will help to speed up DQNs, but you need to be modest with the
    selection of n. Small values of two or three usually work well, because our trajectories
    in the experience buffer are not that different from one-step transitions. In
    such cases, convergence speed usually improves proportionally, but large values
    of n can break the training process. So, the number of steps should be tuned,
    but convergence speeding up usually makes it worth doing.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有一个问题：为什么我们要讨论一个n步DQN，如果这个“n步性”会使它变成一个基于策略的方法，这将使我们的大型经验缓冲区变得没用？实际上，这通常不是非黑即白的。你仍然可以使用n步DQN，如果它有助于加速DQN的训练，但你需要在选择n时保持谨慎。小的值，如二或三，通常效果很好，因为我们在经验缓冲区中的轨迹与一步过渡差别不大。在这种情况下，收敛速度通常会成比例地提高，但n值过大可能会破坏训练过程。因此，步数应该进行调优，但加速收敛通常使得这样做是值得的。
- en: Implementation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'As the ExperienceSourceFirstLast class already supports the multi-step Bellman
    unroll, our n-step version of a DQN is extremely simple. There are only two modifications
    that we need to make to the basic DQN to turn it into an n-step version:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Pass the count of steps that we want to unroll on ExperienceSourceFirstLast
    creation in the steps_count parameter.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the correct gamma to the calc_loss_dqn function. This modification is really
    easy to overlook, which could be harmful to convergence. As our Bellman is now
    n-steps, the discount coefficient for the last state in the experience chain will
    no longer be just γ, but γ^n.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the whole example in Chapter08/02_dqn_n_steps.py, with only the
    modified lines shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The n_steps value is a count of steps passed in command-line arguments; the
    default is to use four steps.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Another modification is in gamma passed to the calc_loss_dqn function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Results
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training module Chapter08/02_dqn_n_steps.py can be started as before, with
    the additional command-line option -n, which gives a count of steps to unroll
    the Bellman equation. These are charts for our baseline and n-step DQN (using
    a common set of parameters), with n being equal to 2 and 3\. As you can see, the
    Bellman unroll has given a significant convergence speedup:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_05.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The reward and number of steps for basic (one-step) DQN and n-step
    versions'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the diagram, the three-step DQN converges significantly faster
    than the simple DQN, which is a nice improvement. So, what about a larger n? Figure [8.6](#x1-133004r6)
    shows the reward dynamics for n = 3…6:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_06.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Reward dynamics for cases with n = 3…6 with common hyperparameters'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, going from three steps to four has given some improvement, but
    it is much less than before. The variant with n = 5 is worse and very close to
    n = 2\. The same is true for n = 6\. So, in our case, n = 3 looks optimal.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this extension, hyperparameter tuning was done individually for every n
    from 2 to 7\. The following table shows the best parameters and number of games
    they require to solve the game:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '| n | Learning rate | γ | Games |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3.97 ⋅ 10^(−5) | 0.98 | 293 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| 3 | 7.82 ⋅ 10^(−5) | 0.98 | 260 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| 4 | 6.07 ⋅ 10^(−5) | 0.98 | 290 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| 5 | 7.52 ⋅ 10^(−5) | 0.99 | 268 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| 6 | 6.78 ⋅ 10^(−5) | 0.995 | 261 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| 7 | 8.59 ⋅ 10^(−5) | 0.98 | 284 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: 'Table 8.1: The best hyperparameters (learning rate and gamma) for every n'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This table also confirms the conclusions of the untuned version comparison —
    unrolling the Bellman equation for two and three steps improves the convergence,
    but a further increase of n produces worse results. n = 6 gives us a comparable
    result to n = 3, but the outcomes for n = 4 and n = 5 are worse, so we should
    stop at n = 3.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Figure [8.7](#x1-134003r7) compares the training dynamics of tuned versions
    of the baseline and N-step DQN with n = 2 and n = 3.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_07.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: The reward and number of steps after hyperparameter tuning'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：超参数调整后的奖励和步数
- en: Double DQN
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Double DQN
- en: The next fruitful idea on how to improve a basic DQN came from DeepMind researchers
    in the paper titled Deep reinforcement learning with double Q-learning [[VGS16](#)].
    In the paper, the authors demonstrated that the basic DQN tends to overestimate
    values for Q, which may be harmful to training performance and sometimes can lead
    to suboptimal policies. The root cause of this is the max operation in the Bellman
    equation, but the strict proof is a bit complicated (you can find the full explanation
    in the paper). As a solution to this problem, the authors proposed modifying the
    Bellman update a bit.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如何改进基本DQN的下一个富有成效的想法来自DeepMind研究人员在标题为深度强化学习中的双重Q学习的论文中[[VGS16](#)]。在论文中，作者证明了基本DQN倾向于高估Q值，这可能对训练性能有害，并且有时可能导致次优策略。这背后的根本原因是Bellman方程中的max操作，但其严格证明有点复杂（您可以在论文中找到完整的解释）。作为解决这个问题的方法，作者建议稍微修改贝尔曼更新。
- en: 'In the basic DQN, our target value for Q looked like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本DQN中，我们的Q的目标值看起来像这样：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq29.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq29.png)'
- en: 'Q′(s[t+1],a) was Q-values calculated using our target network, the weights
    of which are copied from the trained network every n steps. The authors of the
    paper proposed choosing actions for the next state using the trained network,
    but taking values of Q from the target network. So, the new expression for target
    Q-values will look like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Q′(s[t+1],a) 是使用我们的目标网络计算的Q值，其权重每隔n步从训练网络复制一次。论文的作者建议选择使用训练网络为下一个状态选择动作，但从目标网络获取Q值。因此，目标Q值的新表达式如下所示：
- en: '![π (a |s) = P[At = a|St = s] ](img/eq30.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![π (a |s) = P[At = a|St = s] ](img/eq30.png)'
- en: The authors proved that this simple tweak fixes overestimation completely, and
    they called this new architecture double DQN.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作者证明了这个简单的小改进完全修复了高估问题，并称这种新架构为双重DQN。
- en: Implementation
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施
- en: The core implementation is very simple. What we need to do is slightly modify
    our loss function. But let’s go a step further and compare action values produced
    by basic DQN and double DQN. According to the paper author’s our baseline DQN
    should have consistently higher values predicted for the same states than the
    double DQN version. To do this, we store a random held-out set of states and periodically
    calculate the mean value of the best action for every state in the evaluation
    set.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 核心实现非常简单。我们需要做的是稍微修改我们的损失函数。但是让我们再进一步，比较基本DQN和双重DQN生成的动作值。根据论文作者的说法，我们的基线DQN应该对于相同状态的预测值始终较高。为了做到这一点，我们存储一组随机保留的状态，并周期性地计算评估集中每个状态的最佳动作的均值。
- en: 'The complete example is in Chapter08/03_dqn_double.py. Let’s first take a look
    at the loss function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例位于Chapter08/03_dqn_double.py中。让我们先看一下损失函数：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will use this function instead of common.calc_loss_dqn and they both share
    lots of code. The main difference is in the next Q-values estimation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个函数而不是common.calc_loss_dqn，它们都共享大量代码。主要区别在于下一个Q值的估计：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code snippet calculates the loss in a slightly different way.
    In the double DQN version, we calculate the best action to take in the next state
    using our main trained network, but values corresponding to this action come from
    the target network.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段以稍微不同的方式计算损失。在双重DQN版本中，我们使用我们的主训练网络计算下一个状态中要采取的最佳动作，但与此动作对应的值来自目标网络。
- en: This part could be implemented in a faster way, by combining next_states_v with
    states_v and calling our main network only once, but it will make the code less
    clear.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分可以通过将next_states_v与states_v合并，并仅调用我们的主网络一次来更快地实现，但这会使代码不太清晰。
- en: 'The rest of the function is the same: we mask completed episodes and compute
    the mean squared error (MSE) loss between Q-values predicted by the network and
    approximated Q-values.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的其余部分与之相同：我们遮盖已完成的剧集，并计算网络预测的Q值与近似Q值之间的均方误差（MSE）损失。
- en: 'The last function that we consider calculates the values of our held-out state:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的最后一个函数计算了我们保留状态的值：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'There is nothing complicated here: we just split our held-out states array
    into equal chunks and pass every chunk to the network to obtain action values.
    From those values, we choose the action with the largest value (for every state)
    and calculate the mean of such values. As our array with states is fixed for the
    whole training process, and this array is large enough (in the code, we store
    1,000 states), we can compare the dynamics of this mean value in both DQN variants.
    The rest of the 03_dqn_double.py file is almost the same; the two differences
    are usage of our tweaked loss function and keeping a randomly sampled 1,000 states
    for periodical evaluation. This happens in the process_batch function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Results
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My experiments show that with common hyperparameters, double DQN has a negative
    effect on reward dynamics. Sometimes, double DQN leads to better initial dynamics
    and the trained agent learns how to win more games faster, but reaching the end
    reward boundary takes longer. You can perform your own experiment on other games
    or try parameters from the original paper.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are reward charts from the experiment where double DQN was a
    bit better than the baseline version:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_08.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Reward dynamics for double and baseline DQN'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Besides the standard metrics, the example also outputs the mean value for the
    held-out set of states, which are shown in Figure [8.9](#x1-137004r9).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_09.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Values predicted by the network for held-out states'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the basic DQN does an overestimation of values, so values decrease
    after a certain level. In contrast, the double DQN grows more consistently. In
    my experiments, the double DQN has only a small effect on the training time, but
    this doesn’t necessarily mean that the double DQN is useless, as Pong is a simple
    environment. In more complicated games, the double DQN could give better results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tuning of hyperparameters also wasn’t very successful for the double DQN.
    After 30 trials, the best values for the learning rate and gamma were able to
    solve Pong in 412 games, which is worse than the baseline DQN.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Noisy networks
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next improvement that we are going to look at addresses another RL problem:
    exploration of the environment. The paper that we will draw from is called Noisy
    networks for exploration [[For+17](#)] and it has a very simple idea for learning
    exploration characteristics during training instead of having a separate schedule
    related to exploration.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: A classical DQN achieves exploration by choosing random actions with a specially
    defined hyperparameter 𝜖, which is slowly decreased over time from 1.0 (fully
    random actions) to some small ratio of 0.1 or 0.02\. This process works well for
    simple environments with short episodes, without much non-stationarity during
    the game; but even in such simple cases, it requires tuning to make the training
    processes efficient.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In the Noisy Networks paper, the authors proposed a quite simple solution that,
    nevertheless, works well. They add noise to the weights of fully connected layers
    of the network and adjust the parameters of this noise during training using backpropagation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: This method shouldn’t be confused with “the network decides where to explore
    more,” which is a much more complex approach that also has widespread support
    (for example, see articles about intrinsic motivation and count-based exploration
    methods [[Ost+17](#)], [[Mar+17](#)]). We will discuss advanced exploration techniques
    in Chapter [21](ch025.xhtml#x1-39100021).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors proposed two ways of adding the noise, both of which work according
    to their experiments, but they have different computational overheads:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Independent Gaussian noise: For every weight in a fully connected layer, we
    have a random value that we draw from the normal distribution. Parameters of the
    noise, μ and σ, are stored inside the layer and get trained using backpropagation
    in the same way that we train weights of the standard linear layer. The output
    of such a “noisy layer” is calculated in the same way as in a linear layer.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Factorized Gaussian noise: To minimize the number of random values to be sampled,
    the authors proposed keeping only two random vectors: one with the size of the
    input and another with the size of the output of the layer. Then, a random matrix
    for the layer is created by calculating the outer product of the vectors.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In PyTorch, both methods can be easily implemented in a very straightforward
    way. What we need to do is create our own custom nn.Linear layer with weights
    calculated as w[i,j] = μ[i,j] + σ[i,j] ⋅𝜖[i,j], where μ and σ are trainable parameters
    and 𝜖 ∼𝒩(0,1) is random noise sampled from the normal distribution after every
    optimization step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous editions of the book used my implementation of both methods, but now
    we’ll simply use the implementation from the popular TorchRL library I mentioned
    in Chapter [7](ch011.xhtml#x1-1070007). Let’s take a look at relevant parts of
    the implementation (the full code can be found in torchrl/modules/models/exploration.py
    in the TorchRL repository). The following is the constructor of the NoisyLinear
    class, which creates all the parameters we need to optimize:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the constructor, we create matrices for μ and σ. This implementation inherits
    from torch.nn.Linear, but calls the nn.Module.__init__() method, so normal Linear
    weights and bias buffers are not created.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: To make new matrices trainable, we need to wrap their tensors in an nn.Parameter.
    The register_buffer method creates a tensor in the network that won’t be updated
    during backpropagation, but will be handled by the nn.Module machinery (for example,
    it will be copied to the GPU with the cuda() call). An extra parameter and buffer
    are created for the bias of the layer. At the end, we call the reset_parameters()
    and reset_noise() methods, which perform the initialization of the created trainable
    parameters and the buffer with the epsilon value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following three methods, we initialize the trainable parameters μ and
    σ according to the paper:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The matrix for μ is initialized with uniform random values. The initial value
    for σ is constant depending on the count of neurons in the layer.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'For the noise initialization, factorized Gaussian noise is used – we sample
    two random vectors and calculate the outer product to get the matrix for 𝜖. The
    outer product is a linear algebra operation when two vectors of the same size
    are producing the square matrix filled with product of all combination of each
    vector’s element. The rest is simple: we redefine the weight and bias properties,
    which are expected in nn.Linear layer, so NoisyLinear could be used everywhere
    nn.Linear is used:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This implementation is simple, but has one very subtle nuance — the 𝜖 values
    are not updated after every optimization step (and it is not mentioned in the
    documentation). This issue is already reported in the TorchRL repo, but for the
    current stable release, we have to call the reset_noise() method explicitly. Hopefully,
    it will be fixed and the NoisyLinear layer will update the noise automatically.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: From the implementation point of view, that’s it. What we now need to do to
    turn the classic DQN into a noisy network variant is just replace nn.Linear (which
    are the two last layers in our DQN network) with the NoisyLinear layer. Of course,
    you have to remove all the code related to the epsilon-greedy strategy.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: To check the internal noise level during training, we can monitor the signal-to-noise
    ratio (SNR) of our noisy layers, which is RMS(μ)∕RMS(σ), where RMS is the root
    mean square of the corresponding weights. In our case, the SNR shows how many
    times the stationary component of the noisy layer is larger than the injected
    noise.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the training, the TensorBoard charts show much better training dynamics.
    The model was able to reach the mean score of 18 after 250 games, which is an
    improvement in comparison to 350 for the baseline DQN. But because of extra operations
    required for noisy networks, their training is a bit slower (194 FPS versus 240
    FPS for the baseline), so, time-wise, the difference is less impressive. But still,
    the results look good:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_10.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Noisy networks compared to the baseline DQN'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: After checking the SNR chart (Figure [8.11](#x1-141004r11)), you may notice
    that both layers’ noise levels have decreased very quickly.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_11.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: SNR change in layer 1 (left) and layer 2 (right)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The first layer has gone from ![1 2](img/eq31.png) to almost ![-1- 2.6](img/eq32.png)
    ratio of noise. The second layer is even more interesting, as its noise level
    decreased from ![1 4](img/eq33.png) in the beginning to ![1- 16](img/eq34.png),
    but after 450K frames (roughly the same time as when raw rewards climbed close
    to the 20 score), the level of noise in the last layer started to increase again,
    pushing the agent to explore the environment more. This makes a lot of sense,
    as after reaching high score levels, the agent basically knows how to play at
    a good level, but still needs to “polish” its actions to improve the results even
    more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the tuning, the best set of parameters was able to solve the game after
    273 rounds, which is an improvement over the baseline:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following are charts comparing the reward dynamics and steps for tuned
    baseline DQN and tuned noisy networks:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_12.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Comparison of tuned baseline DQN and tuned noisy network'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'On both charts, we see improvements introduced by noisy networks: it takes
    fewer games to reach a score of 21 and during the training, games have a smaller
    amount of steps.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Prioritized replay buffer
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next very useful idea on how to improve DQN training was proposed in 2015
    in the paper Prioritized experience replay [[Sch+15](#)]. This method tries to
    improve the efficiency of samples in the replay buffer by prioritizing those samples
    according to the training loss.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The basic DQN used the replay buffer to break the correlation between immediate
    transitions in our episodes. As we discussed in Chapter [6](#), the examples we
    experience during the episode will be highly correlated, as most of the time,
    the environment is ”smooth” and doesn’t change much according to our actions.
    However, the stochastic gradient descent (SGD) method assumes that the data we
    use for training has an iid property. To solve this problem, the classic DQN method
    uses a large buffer of transitions, randomly and uniformly sampled to get the
    next training batch.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper questioned this uniform random sample policy and proved
    that by assigning priorities to buffer samples, according to training loss and
    sampling the buffer proportional to those priorities, we can significantly improve
    convergence and the policy quality of the DQN. This method’s basic idea could
    be explained as “train more on data that surprises you.” The tricky point here
    is to keep the balance of training on an “unusual” sample and training on the
    rest of the buffer. If we focus only on a small subset of the buffer, we can lose
    our i.i.d. property and simply overfit on this subset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: From the mathematical point of view, the priority of every sample in the buffer
    is calculated as ![ pα ∑kipα- k](img/eq35.png), where p[i] is the priority of
    the i-th sample in the buffer and α is the number that shows how much emphasis
    we give to the priority. If α = 0, our sampling will become uniform as in the
    classic DQN method. Larger values for α put more stress on samples with higher
    priority. So, it’s another hyperparameter to tune, and the starting value of α
    proposed by the paper is 0.6.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: There were several options proposed in the paper for how to define the priority,
    and the most popular is to make it proportional to the loss for this particular
    example in the Bellman update. New samples added to the buffer need to be assigned
    a maximum value of priority to be sure that they will be sampled soon.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting the priorities for the samples, we are introducing bias into our
    data distribution (we sample some transitions much more frequently than others),
    which we need to compensate for if SGD is to work. To get this result, the authors
    of the study used sample weights, which needed to be multiplied by the individual
    sample loss. The value of the weight for each sample is defined as w[i] = (N ⋅P(i))^(−β),
    where β is another hyperparameter that should be between 0 and 1.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: With β = 1, the bias introduced by the sampling is fully compensated for, but
    the authors showed that it’s good for convergence to start with β between 0 and
    1 and slowly increase it to 1 during the training.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement this method, we have to introduce certain changes in our code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need a new replay buffer that will track priorities, sample
    a batch according to them, calculate weights, and let us update priorities after
    the loss has become known.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second change will be the loss function itself. Now we not only need to
    incorporate weights for every sample, but we need to pass loss values back to
    the replay buffer to adjust the priorities of the sampled transitions.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the main module, Chapter08/05_dqn_prio_replay.py, we have all those changes
    implemented. For the sake of simplicity, the new priority replay buffer class
    uses a very similar storage scheme to our previous replay buffer. Unfortunately,
    new requirements for prioritization make it impossible to implement sampling in
    𝒪(1) time (in other words, sampling time will grow with an increase in buffer
    size). If we are using simple lists, every time that we sample a new batch, we
    need to process all the priorities, which makes our sampling have 𝒪(N) time complexity
    in proportion to the buffer size. It’s not a big deal if our buffer is small,
    such as 100k samples, but may become an issue for real-life large buffers of millions
    of transitions. There are other storage schemes that support efficient sampling
    in 𝒪(log N) time, for example, using the segment tree data structure. There are
    different versions of such optimized buffers available in various libraries –
    for example, in TorchRL.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The PTAN library also provides an efficient prioritized replay buffer in the
    class ptan.experience.PrioritizedReplayBuffer. You can update the example to use
    the more efficient version and check the effect on training performance.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: But, for now, let’s take a look at the naïve version, whose source code you
    will find in lib/dqn_extra.py.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'In the beginning, we define parameters for the β increase rate:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Our beta will be changed from 0.4 to 1.0 during the first 100k frames.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the prioritized replay buffer class:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The class for the priority replay buffer inherits from the simple replay buffer
    in PTAN, which stores samples in a circular buffer (it allows us to keep a fixed
    amount of entries without reallocating the list). Our subclass uses a NumPy array
    to keep priorities.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'The update_beta() method needs to be called periodically to increase beta according
    to a schedule. The populate() method needs to pull the given number of transitions
    from the ExperienceSource object and store them in the buffer:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As our storage for the transitions is implemented as a circular buffer, we
    have two different situations with this buffer:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: When our buffer hasn’t reached the maximum capacity, we just need to append
    a new transition to the buffer.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the buffer is already full, we need to overwrite the oldest transition, which
    is tracked by the pos class field, and adjust this position modulo buffer’s size.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the sample method, we need to convert priorities to probabilities using
    our α hyperparameter:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, using those probabilities, we sample our buffer to obtain a batch of
    samples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As the last step, we calculate weights for samples in the batch:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This returns three objects: the batch, indices, and weights. Indices for batch
    samples are required to update priorities for sampled items.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'The last function of the priority replay buffer allows us to update new priorities
    for the processed batch:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It’s the responsibility of the caller to use this function with the calculated
    losses for the batch.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The next custom function that we have in our example is the loss calculation.
    As the MSELoss class in PyTorch doesn’t support weights (which is understandable,
    as MSE is loss used in regression problems, but weighting of the samples is commonly
    utilized in classification losses), we need to calculate the MSE and explicitly
    multiply the result on the weights:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the last part of the loss calculation, we implement the same MSE loss but
    write our expression explicitly, rather than using the library. This allows us
    to take into account the weights of samples and keep individual loss values for
    every sample. Those values will be passed to the priority replay buffer to update
    priorities. A small value is added to every loss to handle the situation of zero
    loss value, which will lead to zero priority for an entry in the replay buffer.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main section of our program, we have only two updates: the creation
    of the replay buffer and our processing function. Buffer creation is straightforward,
    so we will only take a look at a new processing function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'There are several changes here:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Our batch now contains three entities: the batch of data, indices of sampled
    items, and samples’ weights.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We call our new loss function, which accepts weights and returns the additional
    items’ priorities. They are passed to the buffer.update_priorities() function
    to reprioritize items that we have sampled.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We call the update_beta() method of the buffer to change the beta parameter
    according to the schedule.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This example can be trained as usual. According to my experiments, the prioritized
    replay buffer took almost the same absolute time to solve the environment: almost
    an hour. But it took fewer training iterations and fewer episodes. So, wall clock
    time is the same mostly due to the less efficient replay buffer, which, of course,
    could be resolved by proper 𝒪(log N) implementation of the buffer.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the comparison of reward dynamics of the baseline and prioritized replay
    buffer (right). The x axis is the game episodes:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_13.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Reward dynamics for prioritized replay buffer in comparison to
    basic DQN'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Another difference to note on the TensorBoard charts is a much lower loss for
    the prioritized replay buffer. The following chart shows the comparison:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_14.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: The comparison of loss during the training'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Lower loss value is also expected and is a good sign that our implementation
    works. The idea of prioritization is to train more on samples with high loss value,
    so training becomes more efficient. But there is a danger here: loss value during
    the training is not the primary objective to optimize; we can have very low loss,
    but due to a lack of exploration, the final policy learned could be far from being
    optimal.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning for the prioritized replay buffer was done with an additional
    parameter for α, which was sampled from a fixed list of values ranging from 0.3
    to 0.9 (with steps of 0.1). The best combination was able to solve Pong after
    330 episodes and had α = 0.6 (the same as in the paper):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following are charts comparing the tuned baseline DQN with the tuned prioritized
    replay buffer:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_15.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: Comparison of tuned baseline DQN and tuned prioritized replay
    buffer'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see the prioritized replay buffer had faster gameplay improvement,
    but it took almost the same amount of games to reach score 21\. On the right chart
    (with the amount of game steps), the prioritized replay buffer was also a bit
    better.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Dueling DQN
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This improvement to DQN was proposed in 2015, in the paper called Dueling network
    architectures for deep reinforcement learning [[Wan+16](#)]. The core observation
    of this paper is that the Q-values, Q(s,a), that our network is trying to approximate
    can be divided into quantities: the value of the state, V (s), and the advantage
    of actions in this state, A(s,a).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: You have seen the quantity V (s) before, as it was the core of the value iteration
    method from Chapter [5](ch009.xhtml#x1-820005). It is just equal to the discounted
    expected reward achievable from this state. The advantage A(s,a) is supposed to
    bridge the gap from V (s) to Q(s,a), as, by definition, Q(s,a) = V (s) + A(s,a).
    In other words, the advantage A(s,a) is just the delta, saying how much extra
    reward some particular action from the state brings us. The advantage could be
    positive or negative and, in general, could have any magnitude. For example, at
    some tipping point, the choice of one action over another can cost us a lot of
    the total reward.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dueling paper’s contribution was an explicit separation of the value and
    the advantage in the network’s architecture, which brought better training stability,
    faster convergence, and better results on the Atari benchmark. The architecture
    difference from the classic DQN network is shown in the following illustration.
    The classic DQN network (top) takes features from the convolution layer and, using
    fully connected layers, transforms them into a vector of Q-values, one for each
    action. On the other hand, dueling DQN (bottom) takes convolution features and
    processes them using two independent paths: one path is responsible for V (s)
    prediction, which is just a single number, and another path predicts individual
    advantage values, having the same dimension as Q-values in the classic case. After
    that, we add V (s) to every value of A(s,a) to obtain Q(s,a), which is used and
    trained as normal. Figure [8.16](#x1-147004r16) (from the paper) compares the
    basic DQN and dueling DQN:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file58.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: A basic DQN (top) and dueling architecture (bottom)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'These changes in the architecture are not enough to make sure that the network
    will learn V (s) and A(s,a) as we want it to. Nothing prevents the network, for
    example, from predicting some state, V (s) = 0, and A(s) = [1,2,3,4], which is
    completely wrong, as the predicted V (s) is not the expected value of the state.
    We have yet another constraint to set: we want the mean value of the advantage
    of any state to be zero. In that case, the correct prediction for the preceding
    example will be V (s) = 2.5 and A(s) = [−1.5,−0.5,0.5,1.5].'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'This constraint could be enforced in various ways, for example, via the loss
    function; but in the Dueling paper, the authors proposed the very elegant solution
    of subtracting the mean value of the advantage from the Q expression in the network,
    which effectively pulls the mean for the advantage to zero:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq36.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'This keeps the changes that need to be made in the classic DQN very simple:
    to convert it to the double DQN, you need to change only the network architecture,
    without affecting other pieces of the implementation.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete example is available in Chapter08/06_dqn_dueling.py. All the changes
    sit in the network architecture, so here, I’ll only show the network class (which
    is in the lib/dqn_extra.py module).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution part is exactly the same as before:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Instead of defining a single path of fully connected layers, we create two
    different transformations: one for advantages and one for value prediction:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Also, to keep the number of parameters in the model comparable to the original
    network, the inner dimension in both paths is decreased from 512 to 256\. The
    changes in the forward() function are also very simple, thanks to PyTorch’s expressiveness:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, we calculate the value and advantage for our batch of samples and add
    them together, subtracting the mean of the advantage to obtain the final Q-values.
    A subtle, but important, difference lies in calculating the mean along the second
    dimension of the tensor, which produces a vector of the mean advantage for every
    sample in our batch.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training a dueling DQN, we can compare it to the classic DQN convergence
    on our Pong benchmark. Dueling architecture has faster convergence in comparison
    to the basic DQN version:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_17.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: The reward dynamic of dueling DQN compared to the baseline version'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example also outputs the advantage and value for a fixed set of states,
    shown in the following charts. They meet our expectations: the advantage is not
    very different from zero, but the value improves over time (and resembles the
    value from the Double DQN section):'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_18.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Mean advantage (left) and value (right) on a fixed set of states'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tuning of the hyperparameters was not very fruitful. After 30 tuning iterations,
    there were no combinations of learning rate and gamma that were able to converge
    faster than the common set of parameters.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Categorical DQN
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last, and the most complicated, method in our DQN improvements toolbox is
    from the paper published by DeepMind in June 2017, called A distributional perspective
    on reinforcement learning [[BDM17](#)]. Although this paper is a few years old
    now, it remains highly relevant, and active research is still ongoing in this
    area. The book Distributional reinforcement learning was published in 2023, where
    the same authors describe the method in greater detail [[BDR23](#)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors questioned the fundamental pieces of Q-learning —
    Q-values — and tried to replace them with a more generic Q-value probability distribution.
    Let’s try to understand the idea. Both the Q-learning and value iteration methods
    work with the values of the actions or states represented as simple numbers and
    showing how much total reward we can achieve from a state, or an action and a
    state. However, is it practical to squeeze all future possible rewards into one
    number? In complicated environments, the future could be stochastic, giving us
    different values with different probabilities.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine the commuter scenario when you regularly drive from home
    to work. Most of the time, the traffic isn’t that heavy, and it takes you around
    30 minutes to reach your destination. It’s not exactly 30 minutes, but on average
    it’s 30\. From time to time, something happens, like road repairs or an accident,
    and due to traffic jams, it takes you three times longer to get to work. The probability
    of your commute time can be represented as a distribution of the “commute time”
    random variable, and it is shown in the following chart:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_19.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: The probability distribution of commute time'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine that you have an alternative way to get to work: the train. It
    takes a bit longer, as you need to get from home to the train station and from
    the station to the office, but they are much more reliable than traveling by car
    (in some contries, like Germany, it might not be the case, but let’s consider
    Swiss trains for our example). Say, for instance, that the train commute time
    is 40 minutes on average, with a small chance of train disruption, which adds
    20 minutes of extra time to the journey. The distribution of the train commute
    is shown in the following graph:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_20.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.20: The probability distribution of train commute time'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that now we want to make the decision on how to commute. If we know
    only the mean time for both car and train, a car looks more attractive, as on
    average it takes 35.43 minutes to travel, which is better than 40.54 minutes for
    the train.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: However, if we look at full distributions, we may decide to go by train, as
    even in the worst-case scenario, it will be one hour of commuting versus one hour
    and 30 minutes. Switching to statistical language, the car distribution has much
    higher variance, so in situations when you really have to be at the office in
    60 minutes max, the train is better.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: The situation becomes even more complicated in the Markov decision process (MDP)
    scenario, when the sequence of decisions needs to be made and every decision might
    influence the future situation. In the commute example, it might be the time of
    an important meeting that you need to arrange given the way that you are going
    to commute. In that case, working with mean reward values might mean losing lots
    of information about the underlying environment dynamics.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Exactly the same idea was proposed by the authors of Distributional Perspective
    on Reinforcement Learning [9]. Why do we limit ourselves by trying to predict
    an average value for an action, when the underlying value may have a complicated
    underlying distribution? Maybe it will help us to work with distributions directly.
    The results presented in the paper show that, in fact, this idea could be helpful,
    but at the cost of introducing a more complicated method. I’m not going to put
    a strict mathematical definition here, but the overall idea is to predict the
    distribution of value for every action, similar to the distributions for our car/train
    example. As the next step, the authors showed that the Bellman equation can be
    generalized for a distribution case, and it will have the form Z(x,a)![D =](img/eq37.png)R(x,a)
    + γZ(x′,a′), which is very similar to the familiar Bellman equation, but now Z(x,a)
    and R(x,a) are the probability distributions and are not single numbers. The notation
    A![ D =](img/eq37.png)B indicates eqality of distributions A and B.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The resulting distribution can be used to train our network to give better predictions
    of value distribution for every action of the given state, exactly in the same
    way as with Q-learning. The only difference will be in the loss function, which
    now has to be replaced with something suitable for distribution comparison. There
    are several alternatives available, for example, Kullback-Leibler (KL) divergence
    (or cross-entropy loss), which is used in classification problems, or the Wasserstein
    metric. In the paper, the authors gave theoretical justification for the Wasserstein
    metric, but when they tried to apply it in practice, they faced limitations. So,
    in the end, the paper used KL divergence.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, the method is quite complex, so it took me a while to implement
    it and make sure it was working. The complete code is in Chapter08/07_dqn_distrib.py,
    which uses the distr_projection function in lib/dqn_extra.py to perform distribution
    projection. Before we check it, I need to say a few words about the implementation
    logic.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: The central part of the method is the probability distribution, which we are
    approximating. There are lots of ways to represent the distribution, but the authors
    of the paper chose a quite generic parametric distribution, which is basically
    a fixed number of values placed regularly on a values range. The range of values
    should cover the range of possible accumulated discounted reward. In the paper,
    the authors did experiments with various numbers of atoms, but the best results
    were obtained with the range split on N_ATOMS=51 intervals in the range of values
    from Vmin=-10 to Vmax=10.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'For every atom (we have 51 of them), our network predicts the probability that
    the future discounted value will fall into this atom’s range. The central part
    of the method is the code, which performs the contraction of distribution of the
    next state’s best action using gamma, adds local reward to the distribution, and
    projects the results back into our original atoms. This logic is implemented in
    the dqn_extra.distr_projection function. In the beginning, we allocate the array
    that will keep the result of the projection:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This function expects the batch of distributions with a shape (batch_size,
    N_ATOMS), the array of rewards, flags for completed episodes, and our hyperparameters:
    Vmin, Vmax, N_ATOMS, and gamma. The delta_z variable is the width of every atom
    in our value range.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we iterate over every atom in the original distribution
    that we have and calculate the place that this atom will be projected to by the
    Bellman operator, taking into account our value bounds:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: For example, the very first atom, with index 0, corresponds with the value Vmin=-10,
    but for the sample with reward +1 will be projected into the value −10 ⋅ 0.99
    + 1 = −8.9\. In other words, it will be shifted to the right (assume gamma=0.99).
    If the value falls beyond our value range given by Vmin and Vmax, we clip it to
    the bounds.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next line, we calculate the atom numbers that our samples have projected:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Of course, samples can be projected between atoms. In such situations, we spread
    the value in the original distribution at the source atom between the two atoms
    that it falls between. This spreading should be carefully handled, as our target
    atom can land exactly at some atom’s position. In that case, we just need to add
    the source distribution value to the target atom.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code handles the situation when the projected atom lands exactly
    on the target atom. Otherwise, b_j won’t be the integer value and variables l
    and u (which correspond to the indices of atoms below and above the projected
    point):'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'When the projected point lands between atoms, we need to spread the probability
    of the source atom between the atoms below and above. This is carried out by two
    lines in the following code:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Of course, we need to properly handle the final transitions of episodes. In
    that case, our projection shouldn’t take into account the next distribution and
    should just have a 1 probability corresponding to the reward obtained.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we again need to take into account our atoms and properly distribute
    this probability if the reward value falls between atoms. This case is handled
    by the following code branch, which zeroes the resulting distribution for samples
    with the done flag set and then calculates the resulting projection:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: To give you an illustration of what this function does, let’s look at artificially
    made distributions processed by this function (Figure [8.21](#x1-152038r21)).
    I used them to debug the function and make sure that it worked as intended. The
    code for these checks is in Chapter08/adhoc/distr_test.py.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_21.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.21: The sample of the probability distribution transformation applied
    to a normal distribution'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: The top chart of Figure [8.21](#x1-152038r21) (named Source) is a normal distribution
    with μ = 0 and σ = 3\. The second chart (named Projected) is obtained from distribution
    projection with γ = 0.9 and is shifted to the right with reward=2.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: In the situation where we pass done=True with the same data, the result will
    be different and is shown in Figure [8.22](#x1-152040r22). In such cases, the
    source distribution will be ignored completely, and the result will have only
    the reward projected.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_22.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.22: The projection of distribution for the final step in the episode'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of this method is in Chapter08/07_dqn_distrib.py, which has
    an optional command-line parameter, --img-path. If this option is given, it has
    to be a directory where plots with a probability distribution from a fixed set
    of states will be stored during the training. This is useful to monitor how the
    model converges from uniform probability in the beginning of the training to a
    more spiked weight of probability masses. Sample images from my experiments are
    shown in Figure [8.24](#x1-153003r24) and Figure [8.25](#x1-153005r25).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: I’m going to show only essential pieces of the implementation here. The core
    of the method, the distr_projection function, was already covered, and it is the
    most complicated piece. What is still missing is the network architecture and
    modified loss function, which we will describe here.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the network, which is in lib/dqn_extra.py, in the DistributionalDQN
    class:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The main difference is the output of the fully connected layer. Now it outputs
    the vector of n_actions * N_ATOMS values, which is 6 × 51 = 306 for Pong. For
    every action, it needs to predict the probability distribution on 51 atoms. Every
    atom (called support) has a value, which corresponds to a particular reward. Those
    atoms’ rewards are evenly distributed from -10 to 10, which gives a grid with
    step 0.4\. Those supports are stored in the network’s buffer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward() method returns the predicted probability distribution as a 3D
    tensor (batch, actions, and supports):'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Besides forward(), we define the both() method, which calculates the probability
    distribution for atoms and Q-values in one call.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'The network also defines several helper functions to simplify the calculation
    of Q-values and apply softmax on the probability distribution:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The final change is the new loss function that has to apply distribution projection
    instead of the Bellman equation, and calculate KL divergence between predicted
    and projected distributions:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code is not very complicated; it just prepares to call distr_projection
    and KL divergence, which is defined as:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq38.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: To calculate the logarithm of probability, we use the PyTorch log_softmax function,
    which combines both log and softmax in a numerically stable way.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From my experiments, the distributional version of DQN converged a bit slower
    and less stably than the original DQN, which is not surprising, as the network
    output is now 51 times larger and the loss function has changed. Without hyperparameter
    tuning (which will be described in the next subsection), the distributional version
    requires 20% more episodes to solve the game.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Another factor that might be important here is that Pong is just too simple
    a game to draw conclusions. In the A Distributional Perspective paper, the authors
    reported state-of-the-art scores (at the time of publishing in 2017) for more
    than half of the games from the Atari benchmark (Pong was not among them).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are charts comparing reward dynamics and loss for the distributional
    DQN. As you can see, the reward dynamics for the distributional method is worse
    than the baseline DQN:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_23.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.23: Reward dynamics (left) and loss decrease (right)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'It might be interesting to look into the dynamics of the probability distribution
    during the training. If you start the training with the --img-path parameter (providing
    the directory name), the training process will save plots with the probability
    distribution for a fixed set of states. For example, the following figure shows
    the probability distribution for all six actions for one state at the beginning
    of the training (after 30k frames):'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file68.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.24: Probability distribution at the beginning of training'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'All the distributions are very wide (as the network hasn’t converged yet),
    and the peak in the middle corresponds to the negative reward that the network
    expects to get from its actions. The same state after 500k frames of training
    is shown in the following figure:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file69.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.25: Probability distribution produced by the trained network'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Now we can see that different actions have different distributions. The first
    action (which corresponds to the NOOP, the do nothing action) has its distribution
    shifted to the left, so doing nothing in this state usually leads to losing. The
    fifth action, which is RIGHTFIRE, has the mean value shifted to the right, so
    this action leads to a better score.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tuning of hyperparameters was not very fruitful. After 30 tuning iterations,
    there were no combinations of learning rate and gamma that were able to converge
    faster than the common set of parameters.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Combining everything
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have now seen all the DQN improvements mentioned in the paper Rainbow:
    Combining Improvements in Deep Reinforcement Learning, but it was done in an incremental
    way, which (I hope) was helpful to understand the idea and implementation of every
    improvement. The main point of the paper was to combine those improvements and
    check the results. In the final example, I’ve decided to exclude categorical DQN
    and double DQN from the final system, as they haven’t shown too much improvement
    on our guinea pig environment. If you want, you can add them and try using a different
    game. The complete example is available in Chapter08/08_dqn_rainbow.py.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we need to define our network architecture and the methods that
    have contributed to it:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Dueling DQN: Our network will have two separate paths for the value of the
    state distribution and advantage distribution. On the output, both paths will
    be summed together, providing the final value probability distributions for actions.
    To force the advantage distribution to have a zero mean, we will subtract the
    distribution with the mean advantage in every atom.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noisy networks: Our linear layers in the value and advantage paths will be
    noisy variants of nn.Linear.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to network architecture changes, we will use the prioritized replay
    buffer to keep environment transitions and sample them proportionally to the MSE
    loss.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will unroll the Bellman equation to n-steps.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: I’m not going to repeat all the code, as individual methods have already been
    given in the preceding sections, and it should be obvious what the final result
    of combining the methods will look like. If you have any trouble, you can find
    the code on GitHub.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are charts comparing the smoothed reward and count of steps with
    the baseline DQN. In both, we can see significant improvement in terms of the
    amount of games played:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_26.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.26: Comparison of baseline DQN with combined system'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the averaged reward, it is worth checking the raw reward chart,
    which is even more dramatic than the smoothed reward. It shows that our system
    was able to jump from the negative outcome to the positive very quickly – after
    just 100 games, it won almost every game. So, it took us another 100 games to
    make the smoothed reward reach +18:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_27.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.27: Raw reward for combined system'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'As a downside, the combined system is slower than the baseline, as we have
    a more complicated NN architecture and prioritized replay buffer. The FPS chart
    shows that the combined system starts at 170 FPS and degrades to 130 FPS due to
    the 𝒪(n) buffer complexity:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_28.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.28: Performance comparison (in frames per second)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tuning was done as before and was able to further improve the combined system
    training in terms of games played before solving the game. The following are charts
    comparing the tuned baseline DQN with the tuned combined system:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_29.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.29: Comparison of tuned baseline DQN with tuned combined system'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'Another chart showing the effect of the tuning is the comparison of raw game
    rewards before and after the tuning. The tuned system starts to get the maximum
    score even earlier — just after 40 games, which is quite impressive:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_08_30.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.30: Raw reward for untuned and tuned combined DQN'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have walked through and implemented a lot of DQN improvements
    that have been discovered by researchers since the first DQN paper was published
    in 2015\. This list is far from complete. First of all, for the list of methods,
    I used the paper Rainbow: Combining improvements in deep reinforcement learning
    [[Hes+18](#)], which was published by DeepMind, so the list of methods is definitely
    biased to DeepMind papers. Secondly, RL is so active nowadays that new papers
    come out almost every day, which makes it very hard to keep up, even if we limit
    ourselves to one kind of RL model, such as a DQN. The goal of this chapter was
    to give you a practical view of different ideas that the field has developed.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue discussing practical DQN applications
    from an engineering perspective by talking about ways to improve DQN performance
    without touching the underlying method.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
