["```py\nmkdir -p data/librispeech\ncd data/librispeech\nwget http://www.openslr.org/resources/12/train-clean-100.tar.gz\nwget http://www.openslr.org/resources/12/dev-clean.tar.gz\nwget http://www.openslr.org/resources/12/test-clean.tar.gz\nmkdir audio\ncd audio\ntar xvzf ../train-clean-100.tar.gz LibriSpeech/train-clean-100 --strip-components=1\ntar xvzf ../dev-clean.tar.gz LibriSpeech/dev-clean --strip-components=1\ntar xvzf ../test-clean.tar.gz LibriSpeech/test-clean --strip-components=1\n```", "```py\npip install pysoundfile\n```", "```py\nimport soundfile as sf\nimport matplotlib.pyplot as plt\n\ndef plot_audio(audio):\n    fig, axs = plt.subplots(4, 1, figsize=(20, 7))\n    axs[0].plot(audio[0]);\n    axs[0].set_title('Raw Audio Signals')\n    axs[1].plot(audio[1]);\n    axs[2].plot(audio[2]);\n    axs[3].plot(audio[3]);\n\naudio_list =[]\nfor i in xrange(4): \n    file_path = 'data/128684/911-128684-000{}.flac'.format(i+1)\n    a, sample_rate = sf.read(file_path)\n    audio_list.append(a) \nplot_audio(audio_list)\n```", "```py\npip install python_speech_features\n```", "```py\nfrom python_speech_features import mfcc\n\ndef compute_mfcc(audio_data, sample_rate):\n    ''' Computes the MFCCs.\n    Args:\n        audio_data: time series of the speech utterance.\n        sample_rate: sampling rate.\n    Returns:\n        mfcc_feat:[num_frames x F] matrix representing the mfcc.\n    '''\n\n    audio_data = audio_data - np.mean(audio_data)\n    audio_data = audio_data / np.max(audio_data)\n    mfcc_feat = mfcc(audio_data, sample_rate, winlen=0.025, winstep=0.01,\n                     numcep=13, nfilt=26, nfft=512, lowfreq=0, highfreq=None,\n                     preemph=0.97, ceplifter=22, appendEnergy=True)\n    return mfcc_feat\n```", "```py\naudio, sample_rate = sf.read(file_path)\nfeats[audio_file] = compute_mfcc(audio, sample_rate)\nplot_audio(audio,feats[audio_file])\n```", "```py\nif os.path.basename(partition) == 'train-clean-100':\n    # Create multiple TFRecords based on utterance length for training\n    writer = {}\n    count = {}\n    print('Processing training files...')\n    for i in range(min_t, max_t+1):\n        filename = os.path.join(write_dir, 'train' + '_' + str(i) +\n                                '.tfrecords')\n        writer[i] = tf.python_io.TFRecordWriter(filename)\n        count[i] = 0\n\n    for utt in tqdm(sorted_utts):\n        example = make_example(utt_len[utt], feats[utt].tolist(),\n                               transcripts[utt])\n        index = int(utt_len[utt]/100)\n        writer[index].write(example)\n        count[index] += 1\n\n    for i in range(min_t, max_t+1):\n        writer[i].close()\n    print(count)\n\n    # Remove bins which have fewer than 20 utterances\n    for i in range(min_t, max_t+1):\n        if count[i] < 20:\n            os.remove(os.path.join(write_dir, 'train' +\n                                   '_' + str(i) + '.tfrecords'))\nelse:\n    # Create single TFRecord for dev and test partition\n    filename = os.path.join(write_dir, os.path.basename(write_dir) +\n                            '.tfrecords')\n    print('Creating', filename)\n    record_writer = tf.python_io.TFRecordWriter(filename)\n    for utt in sorted_utts:\n        example = make_example(utt_len[utt], feats[utt].tolist(),\n                               transcripts[utt])\n        record_writer.write(example)\n    record_writer.close()\n    print('Processed '+str(len(sorted_utts))+' audio files')\n```", "```py\npython preprocess_LibriSpeech.py\n```", "```py\ntraining set X =  {(x(1), y(1)), (x(2), y(2)), . . .}\nutterance =  x(i)\nlabel = y(i)\n```", "```py\n    with tf.variable_scope('conv1') as scope:\n        kernel = _variable_with_weight_decay(\n            'weights',\n            shape=[11, feat_len, 1, params.num_filters],\n            wd_value=None, use_fp16=params.use_fp16)\n\n        feats = tf.expand_dims(feats, dim=-1)\n        conv = tf.nn.conv2d(feats, kernel,\n                            [1, params.temporal_stride, 1, 1],\n                            padding='SAME')\n\n        biases = _variable_on_cpu('biases', [params.num_filters],\n                                  tf.constant_initializer(-0.05),\n                                  params.use_fp16)\n        bias = tf.nn.bias_add(conv, biases)\n        conv1 = tf.nn.relu(bias, name=scope.name)\n        _activation_summary(conv1)\n\n        # dropout\n        conv1_drop = tf.nn.dropout(conv1, params.keep_prob)\n```", "```py\n    with tf.variable_scope('rnn') as scope:\n\n        # Reshape conv output to fit rnn input\n        rnn_input = tf.reshape(conv1_drop, [params.batch_size, -1,\n                                            feat_len*params.num_filters])\n        # Permute into time major order for rnn\n        rnn_input = tf.transpose(rnn_input, perm=[1, 0, 2])\n        # Make one instance of cell on a fixed device,\n        # and use copies of the weights on other devices.\n        cell = rnn_cell.CustomRNNCell(\n            params.num_hidden, activation=tf.nn.relu6,\n            use_fp16=params.use_fp16)\n        drop_cell = tf.contrib.rnn.DropoutWrapper(\n            cell, output_keep_prob=params.keep_prob)\n        multi_cell = tf.contrib.rnn.MultiRNNCell(\n            [drop_cell] * params.num_rnn_layers)\n\n        seq_lens = tf.div(seq_lens, params.temporal_stride)\n        if params.rnn_type == 'uni-dir':\n            rnn_outputs, _ = tf.nn.dynamic_rnn(multi_cell, rnn_input,\n                                               sequence_length=seq_lens,\n                                               dtype=dtype, time_major=True,\n                                               scope='rnn',\n                                               swap_memory=True)\n        else:\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                multi_cell, multi_cell, rnn_input,\n                sequence_length=seq_lens, dtype=dtype,\n                time_major=True, scope='rnn',\n                swap_memory=True)\n            outputs_fw, outputs_bw = outputs\n            rnn_outputs = outputs_fw + outputs_bw\n        _activation_summary(rnn_outputs)\n```", "```py\n    with tf.variable_scope('softmax_linear') as scope:\n        weights = _variable_with_weight_decay(\n            'weights', [params.num_hidden, NUM_CLASSES],\n            wd_value=None,\n            use_fp16=params.use_fp16)\n        biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                                  tf.constant_initializer(0.0),\n                                  params.use_fp16)\n        logit_inputs = tf.reshape(rnn_outputs, [-1, cell.output_size])\n        logits = tf.add(tf.matmul(logit_inputs, weights),\n                        biases, name=scope.name)\n        logits = tf.reshape(logits, [-1, params.batch_size, NUM_CLASSES])\n        _activation_summary(logits)\n```", "```py\nconda create -n 'SpeechProject' python=3.5.0\nsource activate SpeechProject\n```", "```py\n(SpeechProject)$ pip install python-Levenshtein\n(SpeechProject)$ pip install python_speech_features\n(SpeechProject)$ pip install pysoundfile\n(SpeechProject)$ pip install scipy\n(SpeechProject)$ pip install tqdm\n```", "```py\n(SpeechProject)$ conda install tensorflow-gpu\n```", "```py\n(SpeechProject)$ sudo apt-get install libsndfile1\n```", "```py\n(SpeechRecog)$ git clone https://github.com/FordSpeech/deepSpeech.git\n(SpeechRecog)$ cd deepSpeech\n```", "```py\ncp -r ./data/librispeech/audio /home/deepSpeech/data/librispeech\ncp -r ./data/librispeech/processed /home/deepSpeech/librispeech\n```", "```py\n(SpeechRecog)$python deepSpeech_train.py --num_rnn_layers 3 --rnn_type 'bi-dir' --initial_lr 3e-4 --max_steps 30000 --train_dir ./logs/ \n```", "```py\n# Learning rate set up from the hyper-param.\nlearning_rate, global_step = set_learning_rate()\n\n# Create an optimizer that performs gradient descent.\noptimizer = tf.train.AdamOptimizer(learning_rate)\n\n# Fetch a batch worth of data for each tower to train.\ndata = fetch_data()\n\n# Construct loss and gradient ops.\nloss_op, tower_grads, summaries = get_loss_grads(data, optimizer)\n\n# Calculate the mean of each gradient. Note that this is the synchronization point across all towers.\ngrads = average_gradients(tower_grads)\n\n# Apply the gradients to adjust the shared variables.\napply_gradient_op = optimizer.apply_gradients(grads,\n                                              global_step=global_step)\n\n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    ARGS.moving_avg_decay, global_step)\nvariables_averages_op = variable_averages.apply(\n    tf.trainable_variables())\n\n# Group all updates to into a single train op.\ntrain_op = tf.group(apply_gradient_op, variables_averages_op)\n\n# Build summary op.\nsummary_op = add_summaries(summaries, learning_rate, grads)\n\n# Create a saver.\nsaver = tf.train.Saver(tf.all_variables(), max_to_keep=100)\n\n# Start running operations on the Graph with allow_soft_placement set to True \n# to build towers on GPU.\nsess = tf.Session(config=tf.ConfigProto(\n    allow_soft_placement=True,\n    log_device_placement=ARGS.log_device_placement))\n\n# Initialize vars depending on the checkpoints.\nif ARGS.checkpoint is not None:\n    global_step = initialize_from_checkpoint(sess, saver)\nelse:\n    sess.run(tf.initialize_all_variables())\n\n# Start the queue runners.\ntf.train.start_queue_runners(sess)\n\n# Run training loop.\nrun_train_loop(sess, (train_op, loss_op, summary_op), saver)\n```", "```py\n(SpeechRecog)$python deepSpeech_test.py --eval_data 'test' --checkpoint_dir ./logs/\n```"]