["```py\n(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch4$ python get_observation_action_space.py 'MountainCar-v0'\n```", "```py\nObservation Space:\nBox(2,)\n\n space.low: [-1.20000005 -0.07 ]\n\n space.high: [ 0.60000002 0.07 ]\nAction Space:\nDiscrete(3)\n```", "```py\n#!/usr/bin/env python\nimport gym\nenv = gym.make(\"Qbert-v0\")\nMAX_NUM_EPISODES = 10\nMAX_STEPS_PER_EPISODE = 500\nfor episode in range(MAX_NUM_EPISODES):\n    obs = env.reset()\n    for step in range(MAX_STEPS_PER_EPISODE):\n        env.render()\n        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms\n        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not\n        obs = next_state\n\n        if done is True:\n            print(\"\\n Episode #{} ended in {} steps.\".format(episode, step+1))\n            break\n```", "```py\n#!/usr/bin/env python\nimport gym\nenv = gym.make(\"MountainCar-v0\")\nMAX_NUM_EPISODES = 5000\n\nfor episode in range(MAX_NUM_EPISODES):\n    done = False\n    obs = env.reset()\n    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n    step = 0\n    while not done:\n        env.render()\n        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms\n        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not\n        total_reward += reward\n        step += 1\n        obs = next_state\n\n    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1, total_reward))\nenv.close()\n```", "```py\nEPSILON_MIN = 0.005\nmax_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\nEPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\nALPHA = 0.05  # Learning rate\nGAMMA = 0.98  # Discount factor\nNUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim\n```", "```py\nclass Q_Learner(object):\n    def __init__(self, env):\n        self.obs_shape = env.observation_space.shape\n        self.obs_high = env.observation_space.high\n        self.obs_low = env.observation_space.low\n        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n        self.action_shape = env.action_space.n\n        # Create a multi-dimensional array (aka. Table) to represent the\n        # Q-values\n        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n                          self.action_shape))  # (51 x 51 x 3)\n        self.alpha = ALPHA  # Learning rate\n        self.gamma = GAMMA  # Discount factor\n        self.epsilon = 1.0\n```", "```py\n(obs - self.obs_low) / self.bin_width)\n```", "```py\n((obs - self.obs_low) / self.bin_width).astype(int)\n```", "```py\ndef discretize(self, obs):\n        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n```", "```py\n def get_action(self, obs):\n        discretized_obs = self.discretize(obs)\n        # Epsilon-Greedy action selection\n        if self.epsilon > EPSILON_MIN:\n            self.epsilon -= EPSILON_DECAY\n        if np.random.random() > self.epsilon:\n            return np.argmax(self.Q[discretized_obs])\n        else:  # Choose a random action\n            return np.random.choice([a for a in range(self.action_shape)])\n```", "```py\n def learn(self, obs, action, reward, next_obs):\n        discretized_obs = self.discretize(obs)\n        discretized_next_obs = self.discretize(next_obs)\n        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n        td_error = td_target - self.Q[discretized_obs][action]\n        self.Q[discretized_obs][action] += self.alpha * td_error\n```", "```py\nself.Q[discretized_obs][action] += self.alpha * (reward + self.gamma * np.max(self.Q[discretized_next_obs] - self.Q[discretized_obs][action]\n```", "```py\nEPSILON_MIN = 0.005\nmax_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\nEPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\nALPHA = 0.05  # Learning rate\nGAMMA = 0.98  # Discount factor\nNUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim\n\nclass Q_Learner(object):\n    def __init__(self, env):\n        self.obs_shape = env.observation_space.shape\n        self.obs_high = env.observation_space.high\n        self.obs_low = env.observation_space.low\n        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n        self.action_shape = env.action_space.n\n        # Create a multi-dimensional array (aka. Table) to represent the\n        # Q-values\n        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n                           self.action_shape))  # (51 x 51 x 3)\n        self.alpha = ALPHA  # Learning rate\n        self.gamma = GAMMA  # Discount factor\n        self.epsilon = 1.0\n\n    def discretize(self, obs):\n        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n\n    def get_action(self, obs):\n        discretized_obs = self.discretize(obs)\n        # Epsilon-Greedy action selection\n        if self.epsilon > EPSILON_MIN:\n            self.epsilon -= EPSILON_DECAY\n        if np.random.random() > self.epsilon:\n            return np.argmax(self.Q[discretized_obs])\n        else:  # Choose a random action\n            return np.random.choice([a for a in range(self.action_shape)])\n\n    def learn(self, obs, action, reward, next_obs):\n        discretized_obs = self.discretize(obs)\n        discretized_next_obs = self.discretize(next_obs)\n        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n        td_error = td_target - self.Q[discretized_obs][action]\n        self.Q[discretized_obs][action] += self.alpha * td_error\n```", "```py\ndef train(agent, env):\n    best_reward = -float('inf')\n    for episode in range(MAX_NUM_EPISODES):\n        done = False\n        obs = env.reset()\n        total_reward = 0.0\n        while not done:\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            agent.learn(obs, action, reward, next_obs)\n            obs = next_obs\n            total_reward += reward\n        if total_reward > best_reward:\n            best_reward = total_reward\n        print(\"Episode#:{} reward:{} best_reward:{} eps:{}\".format(episode,\n                                     total_reward, best_reward, agent.epsilon))\n    # Return the trained policy\n    return np.argmax(agent.Q, axis=2)\n```", "```py\ndef test(agent, env, policy):\n    done = False\n    obs = env.reset()\n    total_reward = 0.0\n    while not done:\n        action = policy[agent.discretize(obs)]\n        next_obs, reward, done, info = env.step(action)\n        obs = next_obs\n        total_reward += reward\n    return total_reward\n1,000 episodes and save the recorded agent's action in the environment as video files in the gym_monitor_path directory:\n```", "```py\nif __name__ == \"__main__\":\n    env = gym.make('MountainCar-v0')\n    agent = Q_Learner(env)\n    learned_policy = train(agent, env)\n    # Use the Gym Monitor wrapper to evalaute the agent and record video\n    gym_monitor_path = \"./gym_monitor_output\"\n    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n    for _ in range(1000):\n        test(agent, env, learned_policy)\n    env.close()\n```", "```py\n#!/usr/bin/env/ python\nimport gym\nimport numpy as np\n\nMAX_NUM_EPISODES = 50000\nSTEPS_PER_EPISODE = 200 #  This is specific to MountainCar. May change with env\nEPSILON_MIN = 0.005\nmax_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\nEPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\nALPHA = 0.05  # Learning rate\nGAMMA = 0.98  # Discount factor\nNUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim\n\nclass Q_Learner(object):\n    def __init__(self, env):\n        self.obs_shape = env.observation_space.shape\n        self.obs_high = env.observation_space.high\n        self.obs_low = env.observation_space.low\n        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n        self.action_shape = env.action_space.n\n        # Create a multi-dimensional array (aka. Table) to represent the\n        # Q-values\n        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n                           self.action_shape))  # (51 x 51 x 3)\n        self.alpha = ALPHA  # Learning rate\n        self.gamma = GAMMA  # Discount factor\n        self.epsilon = 1.0\n\n    def discretize(self, obs):\n        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n\n    def get_action(self, obs):\n        discretized_obs = self.discretize(obs)\n        # Epsilon-Greedy action selection\n        if self.epsilon > EPSILON_MIN:\n            self.epsilon -= EPSILON_DECAY\n        if np.random.random() > self.epsilon:\n            return np.argmax(self.Q[discretized_obs])\n        else:  # Choose a random action\n            return np.random.choice([a for a in range(self.action_shape)])\n\n    def learn(self, obs, action, reward, next_obs):\n        discretized_obs = self.discretize(obs)\n        discretized_next_obs = self.discretize(next_obs)\n        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n        td_error = td_target - self.Q[discretized_obs][action]\n        self.Q[discretized_obs][action] += self.alpha * td_error\n\ndef train(agent, env):\n    best_reward = -float('inf')\n    for episode in range(MAX_NUM_EPISODES):\n        done = False\n        obs = env.reset()\n        total_reward = 0.0\n        while not done:\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            agent.learn(obs, action, reward, next_obs)\n            obs = next_obs\n            total_reward += reward\n        if total_reward > best_reward:\n            best_reward = total_reward\n        print(\"Episode#:{} reward:{} best_reward:{} eps:{}\".format(episode,\n                                     total_reward, best_reward, agent.epsilon))\n    # Return the trained policy\n    return np.argmax(agent.Q, axis=2)\n\ndef test(agent, env, policy):\n    done = False\n    obs = env.reset()\n    total_reward = 0.0\n    while not done:\n        action = policy[agent.discretize(obs)]\n        next_obs, reward, done, info = env.step(action)\n        obs = next_obs\n        total_reward += reward\n    return total_reward\n\nif __name__ == \"__main__\":\n    env = gym.make('MountainCar-v0')\n    agent = Q_Learner(env)\n    learned_policy = train(agent, env)\n    # Use the Gym Monitor wrapper to evalaute the agent and record video\n    gym_monitor_path = \"./gym_monitor_output\"\n    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)\n    for _ in range(1000):\n        test(agent, env, learned_policy)\n    env.close()\n```"]