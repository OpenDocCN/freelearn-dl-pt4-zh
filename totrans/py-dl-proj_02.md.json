["```py\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n```", "```py\nsample_image = mnist.train.images[0].reshape([28,28])\n```", "```py\nimport matplotlib.pyplot as plt\nplt.gray()\nplt.imshow(sample_image)\n```", "```py\n#!/usr/bin/env python2\n\n# Hyperparameters and all other kind of params\n\n# Parameters\nlearning_rate = 0.01\nnum_steps = 100\nbatch_size = 128\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 300 # 1st layer number of neurons\nn_hidden_2 = 300 # 2nd layer number of neurons\nnum_input = 784 # MNIST data input (img shape: 28*28)\nnum_classes = 10 # MNIST total classes (0-9 digits)\n\n#Training Parameters\ncheckpoint_every = 100\ncheckpoint_dir = './runs/'\n```", "```py\nimport tensorflow as tf\nimport hy_param\n```", "```py\nX = tf.placeholder(\"float\", [None, hy_param.num_input],name=\"input_x\")\nY = tf.placeholder(\"float\", [None, hy_param.num_classes],name=\"input_y\")\n```", "```py\nweights = {\n 'h1': tf.Variable(tf.random_normal([hy_param.num_input, hy_param.n_hidden_1])),\n 'h2': tf.Variable(tf.random_normal([hy_param.n_hidden_1, hy_param.n_hidden_2])),\n 'out': tf.Variable(tf.random_normal([hy_param.n_hidden_2, hy_param.num_classes]))\n }\n biases = {\n 'b1': tf.Variable(tf.random_normal([hy_param.n_hidden_1])),\n 'b2': tf.Variable(tf.random_normal([hy_param.n_hidden_2])),\n 'out': tf.Variable(tf.random_normal([hy_param.num_classes]))\n }\n```", "```py\nlayer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])\nlayer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\nlogits = tf.matmul(layer_2, weights['out']) + biases['out']\n```", "```py\nprediction = tf.nn.softmax(logits, name='prediction')\n```", "```py\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=hy_param.learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n```", "```py\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32) ,name='accuracy')\n```", "```py\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport hy_param\n\n## Defining Placeholders which will be used as inputs for the model\nX = tf.placeholder(\"float\", [None, hy_param.num_input],name=\"input_x\")\nY = tf.placeholder(\"float\", [None, hy_param.num_classes],name=\"input_y\")\n\n# Defining variables for weights & bias\nweights = {\n    'h1': tf.Variable(tf.random_normal([hy_param.num_input, hy_param.n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([hy_param.n_hidden_1, hy_param.n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([hy_param.n_hidden_2, hy_param.num_classes]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([hy_param.n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([hy_param.n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([hy_param.num_classes]))\n}\n\n# Hidden fully connected layer 1 with 300 neurons\nlayer_1 = tf.add(tf.matmul(X, weights['h1']), biases['b1'])\n# Hidden fully connected layer 2 with 300 neurons\nlayer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n# Output fully connected layer with a neuron for each class\nlogits = tf.matmul(layer_2, weights['out']) + biases['out']\n\n# Performing softmax operation\nprediction = tf.nn.softmax(logits, name='prediction')\n\n# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=hy_param.learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32) ,name='accuracy')\n```", "```py\nimport tensorflow as tf\nimport hy_param\n\n# MLP Model which we defined in previous step\nimport model\n```", "```py\n# This will feed the raw images\nX = model.X\n# This will feed the labels associated with the image\nY = model.Y\n```", "```py\ncheckpoint_dir = os.path.abspath(os.path.join(hy_param.checkpoint_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# We only keep the last 2 checkpoints to manage storage\nsaver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n```", "```py\n# Initialize the variables\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n\n    # Run the initializer\n    sess.run(init)\n\n    for step in range(1, hy_param.num_steps+1):\n        # Extracting \n        batch_x, batch_y = mnist.train.next_batch(hy_param.batch_size)\n        # Run optimization op (backprop)\n        sess.run(model.train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % hy_param.display_step == 0 or step == 1:\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([model.loss_op, model.accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n        if step % hy_param.checkpoint_every == 0:\n            path = saver.save(\n                        sess, checkpoint_prefix, global_step=step)\n            print(\"Saved model checkpoint to {}\\n\".format(path))\n\n    print(\"Optimization Finished!\")\n```", "```py\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\n# Import MNIST data\nimport os\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nimport tensorflow as tf\nimport model\nimport hy_param\n\n## tf Graph input\nX = model.X\nY = model.Y\n\ncheckpoint_dir = os.path.abspath(os.path.join(hy_param.checkpoint_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\nsaver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n\n#loss = tf.Variable(0.0)\n# Initialize the variables\ninit = tf.global_variables_initializer()\nall_loss = []\n# Start training\nwith tf.Session() as sess:\n    writer_1 = tf.summary.FileWriter(\"./runs/summary/\",sess.graph)\n\n    sum_var = tf.summary.scalar(\"loss\", model.accuracy)\n    write_op = tf.summary.merge_all()\n\n    # Run the initializer\n    sess.run(init)\n\n    for step in range(1, hy_param.num_steps+1):\n        # Extracting \n        batch_x, batch_y = mnist.train.next_batch(hy_param.batch_size)\n        # Run optimization op (backprop)\n        sess.run(model.train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % hy_param.display_step == 0 or step == 1:\n            # Calculate batch loss and accuracy\n            loss, acc, summary = sess.run([model.loss_op, model.accuracy, write_op], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            all_loss.append(loss)\n            writer_1.add_summary(summary, step)\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n        if step % hy_param.checkpoint_every == 0:\n            path = saver.save(\n                        sess, checkpoint_prefix, global_step=step)\n# print(\"Saved model checkpoint to {}\\n\".format(path))\n\n    print(\"Optimization Finished!\")\n\n    # Calculate accuracy for MNIST test images\n    print(\"Testing Accuracy:\", \\\n        sess.run(model.accuracy, feed_dict={X: mnist.test.images,\n                                      Y: mnist.test.labels}))\n```", "```py\n# Pointing the model checkpoint\ncheckpoint_file = tf.train.latest_checkpoint(os.path.join(hy_param.checkpoint_dir, 'checkpoints'))\nsaver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n```", "```py\n# Load the input variable from the model\ninput_x = tf.get_default_graph().get_operation_by_name(\"input_x\").outputs[0]\n\n# Load the Prediction operation\nprediction = tf.get_default_graph().get_operation_by_name(\"prediction\").outputs[0]\n```", "```py\n# Load the test data\ntest_data = np.array([mnist.test.images[0]])\n\nwith tf.Session() as sess:\n    # Restore the model from the checkpoint\n    saver.restore(sess, checkpoint_file)\n\n    # Execute the model to make predictions \n    data = sess.run(prediction, feed_dict={input_x: test_data })\n\n    print(\"Predicted digit: \", data.argmax() )\n\n```", "```py\n#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport hy_param\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# Pointing the model checkpoint\ncheckpoint_file = tf.train.latest_checkpoint(os.path.join(hy_param.checkpoint_dir, 'checkpoints'))\nsaver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n\n# Loading test data\ntest_data = np.array([mnist.test.images[6]])\n\n# Loading input variable from the model\ninput_x = tf.get_default_graph().get_operation_by_name(\"input_x\").outputs[0]\n\n# Loading Prediction operation\nprediction = tf.get_default_graph().get_operation_by_name(\"prediction\").outputs[0]\n\nwith tf.Session() as sess:\n    # Restoring the model from the checkpoint\n    saver.restore(sess, checkpoint_file)\n\n    # Executing the model to make predictions \n    data = sess.run(prediction, feed_dict={input_x: test_data })\n\n    print(\"Predicted digit: \", data.argmax() )\n\n# Display the feed image\nprint (\"Input image:\")\nplt.gray()\nplt.imshow(test_data.reshape([28,28]))\n```"]