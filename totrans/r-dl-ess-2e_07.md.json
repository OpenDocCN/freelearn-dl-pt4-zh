["```py\nlibrary(keras)\n?dataset_reuters\n```", "```py\nlibrary(keras)\n\n# the reuters dataset is in Keras\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_reuters()\nword_index <- dataset_reuters_word_index()\n\n# convert the word index into a dataframe\nidx<-unlist(word_index)\ndfWords<-as.data.frame(idx)\ndfWords$word <- row.names(dfWords)\nrow.names(dfWords)<-NULL\ndfWords <- dfWords[order(dfWords$idx),]\n\n# create a dataframe for the train data\n# for each row in the train data, we have a list of index values\n# for words in the dfWords dataframe\ndfTrain <- data.frame(y_train)\ndfTrain$sentence <- \"\"\ncolnames(dfTrain)[1] <- \"y\"\nfor (r in 1:length(x_train))\n{\n  row <- x_train[r]\n  line <- \"\"\n  for (i in 1:length(row[[1]]))\n  {\n     index <- row[[1]][i]\n     if (index >= 3)\n       line <- paste(line,dfWords[index-3,]$word)\n  }\n  dfTrain[r,]$sentence <- line\n  if ((r %% 100) == 0)\n    print (r)\n}\nwrite.table(dfTrain,\"../data/reuters.train.tab\",sep=\"\\t\",row.names = FALSE)\n\n```", "```py\n# create a dataframe for the test data\n# for each row in the train data, we have a list of index values\n# for words in the dfWords dataframe\ndfTest <- data.frame(y_test)\ndfTest$sentence <- \"\"\ncolnames(dfTest)[1] <- \"y\"\nfor (r in 1:length(x_test))\n{\n  row <- x_test[r]\n  line <- \"\"\n  for (i in 1:length(row[[1]]))\n  {\n    index <- row[[1]][i]\n    if (index >= 3)\n      line <- paste(line,dfWords[index-3,]$word)\n  }\n  dfTest[r,]$sentence <- line\n  if ((r %% 100) == 0)\n    print (r)\n}\nwrite.table(dfTest,\"../data/reuters.test.tab\",sep=\"\\t\",row.names = FALSE)\n```", "```py\n> table(y_train)\n 0   1   2    3    4   5   6   7   8   9  10  11  12  13  14  15  16  17 \n  67 537  94 3972 2423  22  62  19 177 126 154 473  62 209  28  29 543  51 \n\n 18   19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35 \n  86  682 339 127  22  53  81 123  32  19  58  23  57  52  42  16  57  16 \n\n 36  37  38  39  40  41  42  43  44  45 \n  60  21  22  29  46  38  16  27  17  19 \n```", "```py\ny_train[y_train!=3] <- 0\ny_train[y_train==3] <- 1\ntable(y_train)\n 0    1 \n7256 3972 \n```", "```py\nlibrary(tm)\nrequire(nnet)\nrequire(kernlab)\nlibrary(randomForest)\nlibrary(e1071)\noptions(digits=4)\n\nTextClassification <-function (w,stem=0,stop=0,verbose=1)\n{\n  df <- read.csv(\"../data/reuters.train.tab\", sep=\"\\t\", stringsAsFactors = FALSE)\n  df2 <- read.csv(\"../data/reuters.test.tab\", sep=\"\\t\", stringsAsFactors = FALSE)\n  df <- rbind(df,df2)\n\n  # df <- df[df$y %in% c(3,4),]\n  # df$y <- df$y-3\n  df[df$y!=3,]$y<-0\n  df[df$y==3,]$y<-1\n  rm(df2)\n\n  corpus <- Corpus(DataframeSource(data.frame(df[, 2])))\n  corpus <- tm_map(corpus, content_transformer(tolower))\n\n  # hyperparameters\n  if (stop==1)\n    corpus <- tm_map(corpus, function(x) removeWords(x, stopwords(\"english\")))\n  if (stem==1)\n    corpus <- tm_map(corpus, stemDocument)\n  if (w==\"tfidf\")\n    dtm <- DocumentTermMatrix(corpus,control=list(weighting=weightTfIdf))\n  else if (w==\"tf\")\n    dtm <- DocumentTermMatrix(corpus,control=list(weighting=weightTf))\n  else if (w==\"binary\")\n    dtm <- DocumentTermMatrix(corpus,control=list(weighting=weightBin))\n\n  # keep terms that cover 95% of the data\n  dtm2<-removeSparseTerms(dtm, 0.95)\n  m <- as.matrix(dtm2)\n  remove(dtm,dtm2,corpus)\n\n  data<-data.frame(m)\n  data<-cbind(df[, 1],data)\n  colnames(data)[1]=\"y\"\n\n  # create train, test sets for machine learning\n  seed <- 42 \n  set.seed(seed) \n  nobs <- nrow(data)\n  sample <- train <- sample(nrow(data), 0.8*nobs)\n  validate <- NULL\n  test <- setdiff(setdiff(seq_len(nrow(data)), train), validate)\n```", "```py\n\n  # create Naive Bayes model\n  nb <- naiveBayes(as.factor(y) ~., data=data[sample,])\n  pr <- predict(nb, newdata=data[test, ])\n  # Generate the confusion matrix showing counts.\n  tab<-table(na.omit(data[test, ])$y, pr,\n             dnn=c(\"Actual\", \"Predicted\"))\n  if (verbose) print (tab)\n  nb_acc <- 100*sum(diag(tab))/length(test)\n  if (verbose) print(sprintf(\"Naive Bayes accuracy = %1.2f%%\",nb_acc))\n\n  # create SVM model\n  if (verbose) print (\"SVM\")\n  if (verbose) print (Sys.time())\n  ksvm <- ksvm(as.factor(y) ~ .,\n               data=data[sample,],\n               kernel=\"rbfdot\",\n               prob.model=TRUE)\n  if (verbose) print (Sys.time())\n  pr <- predict(ksvm, newdata=na.omit(data[test, ]))\n  # Generate the confusion matrix showing counts.\n  tab<-table(na.omit(data[test, ])$y, pr,\n             dnn=c(\"Actual\", \"Predicted\"))\n  if (verbose) print (tab)\n  svm_acc <- 100*sum(diag(tab))/length(test)\n  if (verbose) print(sprintf(\"SVM accuracy = %1.2f%%\",svm_acc))\n\n  # create Neural Network model\n  rm(pr,tab)\n  set.seed(199)\n  if (verbose) print (\"Neural Network\")\n  if (verbose) print (Sys.time())\n  nnet <- nnet(as.factor(y) ~ .,\n               data=data[sample,],\n               size=10, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)\n  if (verbose) print (Sys.time())\n  pr <- predict(nnet, newdata=data[test, ], type=\"class\")\n  # Generate the confusion matrix showing counts.\n  tab<-table(data[test, ]$y, pr,\n             dnn=c(\"Actual\", \"Predicted\"))\n  if (verbose) print (tab)\n  nn_acc <- 100*sum(diag(tab))/length(test)\n  if (verbose) print(sprintf(\"Neural Network accuracy = %1.2f%%\",nn_acc))\n\n  # create Random Forest model\n  rm(pr,tab)\n  if (verbose) print (\"Random Forest\")\n  if (verbose) print (Sys.time())\n  rf_model<-randomForest(as.factor(y) ~., data=data[sample,])\n  if (verbose) print (Sys.time())\n  pr <- predict(rf_model, newdata=data[test, ], type=\"class\")\n  # Generate the confusion matrix showing counts.\n  tab<-table(data[test, ]$y, pr,\n             dnn=c(\"Actual\", \"Predicted\"))\n  if (verbose) print (tab)\n  rf_acc <- 100*sum(diag(tab))/length(test)\n  if (verbose) print(sprintf(\"Random Forest accuracy = %1.2f%%\",rf_acc))\n\n  dfParams <- data.frame(w,stem,stop)\n  dfParams$nb_acc <- nb_acc\n  dfParams$svm_acc <- svm_acc\n  dfParams$nn_acc <- nn_acc\n  dfParams$rf_acc <- rf_acc\n\n  return(dfParams)\n}\n```", "```py\ndfResults <- TextClassification(\"tfidf\",verbose=1) # tf-idf, no stemming\ndfResults<-rbind(dfResults,TextClassification(\"tf\",verbose=1)) # tf, no stemming\ndfResults<-rbind(dfResults,TextClassification(\"binary\",verbose=1)) # binary, no stemming\n\ndfResults<-rbind(dfResults,TextClassification(\"tfidf\",1,verbose=1)) # tf-idf, stemming\ndfResults<-rbind(dfResults,TextClassification(\"tf\",1,verbose=1)) # tf, stemming\ndfResults<-rbind(dfResults,TextClassification(\"binary\",1,verbose=1)) # binary, stemming\n\ndfResults<-rbind(dfResults,TextClassification(\"tfidf\",0,1,verbose=1)) # tf-idf, no stemming, remove stopwords\ndfResults<-rbind(dfResults,TextClassification(\"tf\",0,1,verbose=1)) # tf, no stemming, remove stopwords\ndfResults<-rbind(dfResults,TextClassification(\"binary\",0,1,verbose=1)) # binary, no stemming, remove stopwords\n\ndfResults<-rbind(dfResults,TextClassification(\"tfidf\",1,1,verbose=1)) # tf-idf, stemming, remove stopwords\ndfResults<-rbind(dfResults,TextClassification(\"tf\",1,1,verbose=1)) # tf, stemming, remove stopwords\ndfResults<-rbind(dfResults,TextClassification(\"binary\",1,1,verbose=1)) # binary, stemming, remove stopwords\n\ndfResults[, \"best_acc\"] <- apply(dfResults[, c(\"nb_acc\",\"svm_acc\",\"nn_acc\",\"rf_acc\")], 1, max)\ndfResults <- dfResults[order(-dfResults$best_acc),]\ndfResults\n\nstrResult <- sprintf(\"Best accuracy score was %1.2f%%. Hyper-parameters: \",dfResults[1,\"best_acc\"])\nstrResult <- paste(strResult,dfResults[1,\"w\"],\",\",sep=\"\")\nstrResult <- paste(strResult,\n                   ifelse(dfResults[1,\"stem\"] == 0,\"no stemming,\",\"stemming,\"))\nstrResult <- paste(strResult,\n                   ifelse(dfResults[1,\"stop\"] == 0,\"no stop word processing,\",\"removed stop words,\"))\nif (dfResults[1,\"best_acc\"] == dfResults[1,\"nb_acc\"]){\n  strResult <- paste(strResult,\"Naive Bayes model\")\n} else if (dfResults[1,\"best_acc\"] == dfResults[1,\"svm_acc\"]){\n  strResult <- paste(strResult,\"SVM model\")\n} else if (dfResults[1,\"best_acc\"] == dfResults[1,\"nn_acc\"]){\n  strResult <- paste(strResult,\"Neural Network model\")\n}else if (dfResults[1,\"best_acc\"] == dfResults[1,\"rf_acc\"]){\n  strResult <- paste(strResult,\"Random Forest model\")\n}\n\nprint (strResult)\n```", "```py\n> dfResults\n w stem stop nb_acc svm_acc nn_acc rf_acc best_acc\n12 binary    1    1   86.06   95.24   90.52   94.26     95.24\n9  binary    0    1   87.71   95.15   90.52   93.72     95.15\n10 tfidf     1    1   91.99   95.15   91.05   94.17     95.15\n3  binary    0    0   85.98   95.01   90.29   93.99     95.01\n6  binary    1    0   84.59   95.01   90.34   93.63     95.01\n7  tfidf     0    1   91.27   94.43   94.79   93.54     94.79\n11 tf        1    1   77.47   94.61   92.30   94.08     94.61\n4  tfidf     1    0   92.25   94.57   90.96   93.99     94.57\n5  tf        1    0   75.11   94.52   93.46   93.90     94.52\n1  tfidf     0    0   91.54   94.26   91.59   93.23     94.26\n2  tf        0    0   75.82   94.03   91.54   93.59     94.03\n8  tf        0    1   78.14   94.03   91.63   93.68     94.03\n\n> print (strResult)\n[1] \"Best accuracy score was 95.24%. Hyper-parameters: binary, stemming, removed stop words, SVM model\"\n```", "```py\nlibrary(keras)\n\nset.seed(42)\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 250\nskip_top = 0\n\nreuters <- dataset_reuters(num_words = max_features,skip_top = skip_top)\nc(c(x_train, y_train), c(x_test, y_test)) %<-% reuters\nx_train <- pad_sequences(x_train, maxlen = maxlen)\nx_test <- pad_sequences(x_test, maxlen = maxlen)\nx_train <- rbind(x_train,x_test)\ny_train <- c(y_train,y_test)\ntable(y_train)\n\ny_train[y_train!=3] <- 0\ny_train[y_train==3] <- 1\ntable(y_train)\n```", "```py\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %>%\n  layer_flatten() %>%\n  layer_dropout(rate = 0.25) %>% \n  layer_dense(units = 16, activation = 'relu') %>%\n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 16, activation = 'relu') %>%\n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"acc\")\n)\nsummary(model)\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 5,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/5\n8982/8982 [==============================] - 3s 325us/step - loss: 0.4953 - acc: 0.7674 - val_loss: 0.2332 - val_acc: 0.9274\nEpoch 2/5\n8982/8982 [==============================] - 3s 294us/step - loss: 0.2771 - acc: 0.9235 - val_loss: 0.1990 - val_acc: 0.9394\nEpoch 3/5\n8982/8982 [==============================] - 3s 297us/step - loss: 0.2150 - acc: 0.9414 - val_loss: 0.1975 - val_acc: 0.9497\nEpoch 4/5\n8982/8982 [==============================] - 3s 282us/step - loss: 0.1912 - acc: 0.9515 - val_loss: 0.2118 - val_acc: 0.9461\nEpoch 5/5\n8982/8982 [==============================] - 3s 280us/step - loss: 0.1703 - acc: 0.9584 - val_loss: 0.2490 - val_acc: 0.9466\n```", "```py\nlibrary(keras)\n\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmax_features\n[1] 30979\n.......\n\nmodel <- keras_model_sequential() %>%\n layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %>%\n.......\n\nsummary(model)\n_______________________________________________________________________________________\nLayer (type)                Output Shape         Param # \n=======================================================================================\nembedding_1 (Embedding)     (None, 150, 16)      495664\n.......\n```", "```py\nlibrary(corpus)\ntext <- \"love loving lovingly loved lover lovely love\"\ntext_tokens(text, stemmer = \"en\") # english stemmer\n[[1]]\n[1] \"love\" \"love\" \"love\" \"love\" \"lover\" \"love\" \"love\" \n```", "```py\nlibrary(tm)\n> stopwords()\n [1] \"i\" \"me\" \"my\" \"myself\" \"we\" \"our\" \n [7] \"ours\" \"ourselves\" \"you\" \"your\" \"yours\" \"yourself\" \n [13] \"yourselves\" \"he\" \"him\" \"his\" \"himself\" \"she\"\n [19] \"her\" \"hers\" \"herself\" \"it\" \"its\" \"itself\" \n [25] \"they\" \"them\" \"their\" \"theirs\" \"themselves\" \"what\" \n.........\n```", "```py\nlibrary(tm)\ndf <- read.csv(\"../data/reuters.train.tab\", sep=\"\\t\", stringsAsFactors = FALSE)\ndf2 <- read.csv(\"../data/reuters.test.tab\", sep=\"\\t\", stringsAsFactors = FALSE)\ndf <- rbind(df,df2)\n\ndf[df$y!=3,]$y<-0\ndf[df$y==3,]$y<-1\nrm(df2)\n\ncorpus <- Corpus(DataframeSource(data.frame(df[, 2])))\ncorpus <- tm_map(corpus, content_transformer(tolower))\n\ndtm <- DocumentTermMatrix(corpus,control=list(weighting=weightBin))\n\n# keep terms that cover 95% of the data\ndtm2<-removeSparseTerms(dtm, 0.95)\n\ndtm\n<<DocumentTermMatrix (documents: 11228, terms: 30538)>>\nNon-/sparse entries: 768265/342112399\nSparsity : 100%\nMaximal term length: 24\nWeighting : binary (bin)\n\ndtm2\n<<DocumentTermMatrix (documents: 11228, terms: 230)>>\nNon-/sparse entries: 310275/2272165\nSparsity : 88%\nMaximal term length: 13\nWeighting : binary (bin)\n```", "```py\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_conv_1d(64,5, activation = \"relu\") %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_max_pooling_1d() %>%\n  layer_flatten() %>%\n  layer_dense(units = 50, activation = 'relu') %>%\n  layer_dropout(rate = 0.6) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/5\n8982/8982 [==============================] - 13s 1ms/step - loss: 0.3020 - acc: 0.8965 - val_loss: 0.1909 - val_acc: 0.9470\nEpoch 2/5\n8982/8982 [==============================] - 13s 1ms/step - loss: 0.1980 - acc: 0.9498 - val_loss: 0.1816 - val_acc: 0.9537\nEpoch 3/5\n8982/8982 [==============================] - 12s 1ms/step - loss: 0.1674 - acc: 0.9575 - val_loss: 0.2233 - val_acc: 0.9368\nEpoch 4/5\n8982/8982 [==============================] - 12s 1ms/step - loss: 0.1587 - acc: 0.9606 - val_loss: 0.1787 - val_acc: 0.9573\nEpoch 5/5\n8982/8982 [==============================] - 12s 1ms/step - loss: 0.1513 - acc: 0.9628 - val_loss: 0.2186 - val_acc: 0.9408\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmax_features <- 4000\nmaxlen <- 100\nskip_top = 100\n\n........\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_spatial_dropout_1d(rate = 0.25) %>%\n  layer_simple_rnn(64,activation = \"relu\", dropout=0.2) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n........\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 4s 409us/step - loss: 0.5289 - acc: 0.7848 - val_loss: 0.3162 - val_acc: 0.9078\nEpoch 2/10\n8982/8982 [==============================] - 4s 391us/step - loss: 0.2875 - acc: 0.9098 - val_loss: 0.2962 - val_acc: 0.9305\nEpoch 3/10\n8982/8982 [==============================] - 3s 386us/step - loss: 0.2496 - acc: 0.9267 - val_loss: 0.2487 - val_acc: 0.9234\nEpoch 4/10\n8982/8982 [==============================] - 3s 386us/step - loss: 0.2395 - acc: 0.9312 - val_loss: 0.2709 - val_acc: 0.9332\nEpoch 5/10\n8982/8982 [==============================] - 3s 381us/step - loss: 0.2259 - acc: 0.9336 - val_loss: 0.2360 - val_acc: 0.9270\nEpoch 6/10\n8982/8982 [==============================] - 3s 381us/step - loss: 0.2182 - acc: 0.9348 - val_loss: 0.2298 - val_acc: 0.9341\nEpoch 7/10\n8982/8982 [==============================] - 3s 383us/step - loss: 0.2129 - acc: 0.9380 - val_loss: 0.2114 - val_acc: 0.9390\nEpoch 8/10\n8982/8982 [==============================] - 3s 382us/step - loss: 0.2128 - acc: 0.9341 - val_loss: 0.2306 - val_acc: 0.9359\nEpoch 9/10\n8982/8982 [==============================] - 3s 378us/step - loss: 0.2053 - acc: 0.9382 - val_loss: 0.2267 - val_acc: 0.9368\nEpoch 10/10\n8982/8982 [==============================] - 3s 385us/step - loss: 0.2031 - acc: 0.9389 - val_loss: 0.2204 - val_acc: 0.9368\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 150\nskip_top = 0\n\n.........\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_lstm(128,dropout=0.2) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n.........\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 25s 3ms/step - loss: 0.3238 - acc: 0.8917 - val_loss: 0.2135 - val_acc: 0.9394\nEpoch 2/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.2465 - acc: 0.9206 - val_loss: 0.1875 - val_acc: 0.9470\nEpoch 3/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1815 - acc: 0.9493 - val_loss: 0.2577 - val_acc: 0.9408\nEpoch 4/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1691 - acc: 0.9521 - val_loss: 0.1956 - val_acc: 0.9501\nEpoch 5/10\n8982/8982 [==============================] - 25s 3ms/step - loss: 0.1658 - acc: 0.9507 - val_loss: 0.1850 - val_acc: 0.9537\nEpoch 6/10\n8982/8982 [==============================] - 25s 3ms/step - loss: 0.1658 - acc: 0.9508 - val_loss: 0.1764 - val_acc: 0.9510\nEpoch 7/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1659 - acc: 0.9522 - val_loss: 0.1884 - val_acc: 0.9466\nEpoch 8/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1548 - acc: 0.9556 - val_loss: 0.1900 - val_acc: 0.9479\nEpoch 9/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1562 - acc: 0.9548 - val_loss: 0.2035 - val_acc: 0.9461\nEpoch 10/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1508 - acc: 0.9567 - val_loss: 0.2052 - val_acc: 0.9470\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 250\nskip_top = 0\n\n...........\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_gru(128,dropout=0.2) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n...........\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.3231 - acc: 0.8867 - val_loss: 0.2068 - val_acc: 0.9372\nEpoch 2/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.2084 - acc: 0.9381 - val_loss: 0.2065 - val_acc: 0.9421\nEpoch 3/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1824 - acc: 0.9454 - val_loss: 0.1711 - val_acc: 0.9501\nEpoch 4/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1656 - acc: 0.9515 - val_loss: 0.1719 - val_acc: 0.9550\nEpoch 5/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1569 - acc: 0.9551 - val_loss: 0.1668 - val_acc: 0.9541\nEpoch 6/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1477 - acc: 0.9570 - val_loss: 0.1667 - val_acc: 0.9555\nEpoch 7/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1441 - acc: 0.9605 - val_loss: 0.1612 - val_acc: 0.9581\nEpoch 8/10\n8982/8982 [==============================] - 36s 4ms/step - loss: 0.1361 - acc: 0.9611 - val_loss: 0.1593 - val_acc: 0.9590\nEpoch 9/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1361 - acc: 0.9620 - val_loss: 0.1646 - val_acc: 0.9568\nEpoch 10/10\n8982/8982 [==============================] - 35s 4ms/step - loss: 0.1306 - acc: 0.9634 - val_loss: 0.1660 - val_acc: 0.9559\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 250\nskip_top = 0\n\n..................\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_dropout(rate = 0.25) %>%\n  bidirectional(layer_lstm(units=128,dropout=0.2)) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n..................\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 82s 9ms/step - loss: 0.3312 - acc: 0.8834 - val_loss: 0.2166 - val_acc: 0.9377\nEpoch 2/10\n8982/8982 [==============================] - 87s 10ms/step - loss: 0.2487 - acc: 0.9243 - val_loss: 0.1889 - val_acc: 0.9457\nEpoch 3/10\n8982/8982 [==============================] - 86s 10ms/step - loss: 0.1873 - acc: 0.9464 - val_loss: 0.1708 - val_acc: 0.9519\nEpoch 4/10\n8982/8982 [==============================] - 82s 9ms/step - loss: 0.1685 - acc: 0.9537 - val_loss: 0.1786 - val_acc: 0.9577\nEpoch 5/10\n8982/8982 [==============================] - 83s 9ms/step - loss: 0.1634 - acc: 0.9531 - val_loss: 0.2094 - val_acc: 0.9310\nEpoch 6/10\n8982/8982 [==============================] - 82s 9ms/step - loss: 0.1567 - acc: 0.9571 - val_loss: 0.1809 - val_acc: 0.9475\nEpoch 7/10\n8982/8982 [==============================] - 83s 9ms/step - loss: 0.1499 - acc: 0.9575 - val_loss: 0.1652 - val_acc: 0.9555\nEpoch 8/10\n8982/8982 [==============================] - 83s 9ms/step - loss: 0.1488 - acc: 0.9586 - val_loss: 0.1795 - val_acc: 0.9510\nEpoch 9/10\n8982/8982 [==============================] - 83s 9ms/step - loss: 0.1513 - acc: 0.9567 - val_loss: 0.1758 - val_acc: 0.9555\nEpoch 10/10\n8982/8982 [==============================] - 83s 9ms/step - loss: 0.1463 - acc: 0.9571 - val_loss: 0.1731 - val_acc: 0.9550\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 250\nskip_top = 0\n\n..................\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_dropout(rate = 0.25) %>%\n  bidirectional(layer_lstm(units=32,dropout=0.2,return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units=32,dropout=0.2)) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n..................\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.2854 - acc: 0.9006 - val_loss: 0.1945 - val_acc: 0.9372\nEpoch 2/10\n8982/8982 [==============================] - 66s 7ms/step - loss: 0.1795 - acc: 0.9511 - val_loss: 0.1791 - val_acc: 0.9484\nEpoch 3/10\n8982/8982 [==============================] - 69s 8ms/step - loss: 0.1586 - acc: 0.9557 - val_loss: 0.1756 - val_acc: 0.9492\nEpoch 4/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1467 - acc: 0.9607 - val_loss: 0.1664 - val_acc: 0.9559\nEpoch 5/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1394 - acc: 0.9614 - val_loss: 0.1775 - val_acc: 0.9533\nEpoch 6/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1347 - acc: 0.9636 - val_loss: 0.1667 - val_acc: 0.9519\nEpoch 7/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1344 - acc: 0.9618 - val_loss: 0.2101 - val_acc: 0.9332\nEpoch 8/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1306 - acc: 0.9647 - val_loss: 0.1893 - val_acc: 0.9479\nEpoch 9/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1286 - acc: 0.9646 - val_loss: 0.1663 - val_acc: 0.9550\nEpoch 10/10\n8982/8982 [==============================] - 70s 8ms/step - loss: 0.1254 - acc: 0.9669 - val_loss: 0.1687 - val_acc: 0.9492\n```", "```py\nword_index <- dataset_reuters_word_index()\nmax_features <- length(word_index)\nmaxlen <- 250\nskip_top = 0\n\n..................\n\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %>%\n  layer_spatial_dropout_1d(rate = 0.25) %>%\n  layer_conv_1d(64,3, activation = \"relu\") %>%\n  layer_max_pooling_1d() %>%\n  bidirectional(layer_gru(units=64,dropout=0.2)) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n..................\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_split = 0.2\n)\n```", "```py\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.2891 - acc: 0.8952 - val_loss: 0.2226 - val_acc: 0.9319\nEpoch 2/10\n8982/8982 [==============================] - 25s 3ms/step - loss: 0.1712 - acc: 0.9505 - val_loss: 0.1601 - val_acc: 0.9586\nEpoch 3/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1651 - acc: 0.9548 - val_loss: 0.1639 - val_acc: 0.9541\nEpoch 4/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1466 - acc: 0.9582 - val_loss: 0.1699 - val_acc: 0.9550\nEpoch 5/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1391 - acc: 0.9606 - val_loss: 0.1520 - val_acc: 0.9586\nEpoch 6/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1347 - acc: 0.9626 - val_loss: 0.1626 - val_acc: 0.9550\nEpoch 7/10\n8982/8982 [==============================] - 27s 3ms/step - loss: 0.1332 - acc: 0.9638 - val_loss: 0.1572 - val_acc: 0.9604\nEpoch 8/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1317 - acc: 0.9629 - val_loss: 0.1693 - val_acc: 0.9470\nEpoch 9/10\n8982/8982 [==============================] - 26s 3ms/step - loss: 0.1259 - acc: 0.9654 - val_loss: 0.1531 - val_acc: 0.9599\nEpoch 10/10\n8982/8982 [==============================] - 28s 3ms/step - loss: 0.1233 - acc: 0.9665 - val_loss: 0.1653 - val_acc: 0.9573\n```"]