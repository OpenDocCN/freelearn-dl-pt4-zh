<html><head></head><body>
  <div id="_idContainer296">
   <h1 class="chapter-number" id="_idParaDest-135">
    <a id="_idTextAnchor138">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     8
    </span>
   </h1>
   <h1 id="_idParaDest-136">
    <a id="_idTextAnchor139">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Graph Deep Learning for Natural Language Processing
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     Language, by its very nature, is inherently structured and relational.
    </span>
    <span class="koboSpan" id="kobo.3.2">
     Words form sentences, and sentences form documents, which contain concepts that interlink in complex ways to convey meaning.
    </span>
    <span class="koboSpan" id="kobo.3.3">
     Graph structures provide an ideal framework to capture these intricate relationships, going beyond the traditional models.
    </span>
    <span class="koboSpan" id="kobo.3.4">
     By representing text as graphs, we can leverage the rich expressiveness of graph theory and the computational power of deep learning to tackle
    </span>
    <a id="_idIndexMarker497">
    </a>
    <span class="koboSpan" id="kobo.4.1">
     challenging
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.5.1">
      natural language processing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.6.1">
     (
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.7.1">
       NLP
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.8.1">
      ) problems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.9.1">
     In this chapter, we will delve into the fundamental concepts of graph representations in NLP, exploring various types of linguistic graphs such as dependency trees, co-occurrence networks, and knowledge graphs.
    </span>
    <span class="koboSpan" id="kobo.9.2">
     We’ll then build upon this foundation to examine the architectures and
    </span>
    <a id="_idIndexMarker498">
    </a>
    <span class="koboSpan" id="kobo.10.1">
     mechanisms of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.11.1">
      graph neural networks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.12.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.13.1">
      GNNs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.14.1">
     ) that have been specifically adapted for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.15.1">
      language tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.16.1">
     We’ll cover the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.17.1">
      following topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.18.1">
      Graph structures
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.19.1">
       in NLP
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.20.1">
      Graph-based
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.21.1">
       text summarization
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.22.1">
       Information extraction
      </span>
     </strong>
     <span class="koboSpan" id="kobo.23.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.24.1">
       IE
      </span>
     </strong>
     <span class="koboSpan" id="kobo.25.1">
      )
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.26.1">
       using GNNs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.27.1">
      Graph-based
     </span>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.28.1">
        dialogue systems
       </span>
      </strong>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-137">
    <a id="_idTextAnchor140">
    </a>
    <span class="koboSpan" id="kobo.29.1">
     Graph structures in NLP
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.30.1">
     NLP has seen significant
    </span>
    <a id="_idIndexMarker499">
    </a>
    <span class="koboSpan" id="kobo.31.1">
     advancements in
    </span>
    <a id="_idIndexMarker500">
    </a>
    <span class="koboSpan" id="kobo.32.1">
     recent years, with graph-based approaches emerging as a powerful paradigm for representing and processing linguistic information.
    </span>
    <span class="koboSpan" id="kobo.32.2">
     In this section, we introduce the concept of graph structures in NLP, highlighting their importance and exploring various types of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      linguistic graphs.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-138">
    <a id="_idTextAnchor141">
    </a>
    <span class="koboSpan" id="kobo.34.1">
     Importance of graph representations in language
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.35.1">
     Graph representations
    </span>
    <a id="_idIndexMarker501">
    </a>
    <span class="koboSpan" id="kobo.36.1">
     play a crucial role in capturing the inherent structure and relationships within language.
    </span>
    <span class="koboSpan" id="kobo.36.2">
     They offer several advantages over traditional sequential or
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.37.1">
      bag-of-word
     </span>
    </strong>
    <span class="koboSpan" id="kobo.38.1">
     modeling, which
    </span>
    <a id="_idIndexMarker502">
    </a>
    <span class="koboSpan" id="kobo.39.1">
     is a simple text representation technique that converts a document into a vector by counting the frequency of each word, disregarding grammar and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.40.1">
      word order:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.41.1">
       Structural information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.42.1">
      : Graphs can explicitly represent the hierarchical and relational nature of language, preserving important linguistic structures that may be lost in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.43.1">
       linear representations.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.44.1">
       Contextual relationships
      </span>
     </strong>
     <span class="koboSpan" id="kobo.45.1">
      : By connecting related elements, graphs capture long-range dependencies and contextual information more effectively than
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.46.1">
       sequential models.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.47.1">
       Flexibility
      </span>
     </strong>
     <span class="koboSpan" id="kobo.48.1">
      : Graph structures can represent various levels of linguistic information, from word-level relationships to document-level connections and even
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.49.1">
       cross-document links.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.50.1">
       Interpretability
      </span>
     </strong>
     <span class="koboSpan" id="kobo.51.1">
      : Graph representations often align with human intuition about language structure, making them more interpretable
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.52.1">
       and analyzable.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.53.1">
       Integration of external knowledge
      </span>
     </strong>
     <span class="koboSpan" id="kobo.54.1">
      : Graphs facilitate the incorporation of external knowledge sources, such as ontologies or knowledge bases, into
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.55.1">
       NLP models.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.56.1">
       Multi-modal integration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.57.1">
      : Graph structures can naturally represent relationships between different modalities, such as text, images,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.58.1">
       and speech.
      </span>
     </span>
    </li>
   </ul>
   <h2 id="_idParaDest-139">
    <a id="_idTextAnchor142">
    </a>
    <span class="koboSpan" id="kobo.59.1">
     Types of linguistic graphs
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.60.1">
     Several types of graph
    </span>
    <a id="_idIndexMarker503">
    </a>
    <span class="koboSpan" id="kobo.61.1">
     structures are
    </span>
    <a id="_idIndexMarker504">
    </a>
    <span class="koboSpan" id="kobo.62.1">
     commonly used in NLP, each capturing different aspects of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.63.1">
      linguistic information:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.64.1">
       Dependency trees
      </span>
     </strong>
     <span class="koboSpan" id="kobo.65.1">
      represent grammatical relationships between words in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.66.1">
       a sentence:
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.67.1">
         Nodes
        </span>
       </strong>
       <span class="koboSpan" id="kobo.68.1">
        are words and
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.69.1">
         edges
        </span>
       </strong>
       <span class="koboSpan" id="kobo.70.1">
        indicate
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.71.1">
         syntactic dependencies.
        </span>
       </span>
      </li>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.72.1">
         Example
        </span>
       </strong>
       <span class="koboSpan" id="kobo.73.1">
        : In “
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.74.1">
         The cat chased the mouse
        </span>
       </em>
       <span class="koboSpan" id="kobo.75.1">
        ,” “
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.76.1">
         chased
        </span>
       </em>
       <span class="koboSpan" id="kobo.77.1">
        ” would be the root, with “
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.78.1">
         cat
        </span>
       </em>
       <span class="koboSpan" id="kobo.79.1">
        ” and “
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.80.1">
         mouse
        </span>
       </em>
       <span class="koboSpan" id="kobo.81.1">
        ” as its dependents.
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.82.1">
         Figure 8.1
        </span>
       </em>
       <span class="koboSpan" id="kobo.83.1">
        illustrates a dependency tree representation of this sentence and shows how dependency trees represent grammatical relationships between words in
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.84.1">
         a sentence:
        </span>
       </span>
      </li>
     </ul>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer287">
     <span class="koboSpan" id="kobo.85.1">
      <img alt="Figure 8.1 – Dependency tree representation of the sentence “The cat chased the mouse”" src="image/B22118_08_01.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.86.1">
     Figure 8.1 – Dependency tree representation of the sentence “The cat chased the mouse”
    </span>
   </p>
   <p class="list-inset">
    <span class="koboSpan" id="kobo.87.1">
     The diagram depicts “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.88.1">
      chased
     </span>
    </em>
    <span class="koboSpan" id="kobo.89.1">
     ” as the root verb, with arrows indicating syntactic dependencies to other words such as “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.90.1">
      cat
     </span>
    </em>
    <span class="koboSpan" id="kobo.91.1">
     ” (subject) and “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.92.1">
      mouse
     </span>
    </em>
    <span class="koboSpan" id="kobo.93.1">
     ” (object).
    </span>
    <span class="koboSpan" id="kobo.93.2">
     It also includes part-of-speech tags for each word, such as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.94.1">
      DET
     </span>
    </strong>
    <span class="koboSpan" id="kobo.95.1">
     (determiner),
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.96.1">
      NOUN
     </span>
    </strong>
    <span class="koboSpan" id="kobo.97.1">
     ,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.98.1">
      and
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.99.1">
       VERB
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.100.1">
      .
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.101.1">
       Co-occurrence graphs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.102.1">
      capture word associations based on their co-occurrence in a corpus.
     </span>
     <span class="koboSpan" id="kobo.102.2">
      They are useful for tasks such as word sense disambiguation and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.103.1">
       semantic similarity.
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.104.1">
         Nodes
        </span>
       </strong>
       <span class="koboSpan" id="kobo.105.1">
        represent words and
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.106.1">
         edges
        </span>
       </strong>
       <span class="koboSpan" id="kobo.107.1">
        indicate how often they
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.108.1">
         appear together.
        </span>
       </span>
      </li>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.109.1">
         Example
        </span>
       </strong>
       <span class="koboSpan" id="kobo.110.1">
        : Let’s say we have the
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.111.1">
         following sentences:
        </span>
       </span>
       <ul>
        <li>
         <span class="koboSpan" id="kobo.112.1">
          “
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.113.1">
           The cat and
          </span>
         </em>
         <span class="No-Break">
          <em class="italic">
           <span class="koboSpan" id="kobo.114.1">
            dog play.
           </span>
          </em>
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.115.1">
           ”
          </span>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.116.1">
          “
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.117.1">
           The dog chases
          </span>
         </em>
         <span class="No-Break">
          <em class="italic">
           <span class="koboSpan" id="kobo.118.1">
            the cat.
           </span>
          </em>
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.119.1">
           ”
          </span>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.120.1">
          “
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.121.1">
           The cat sleeps on
          </span>
         </em>
         <span class="No-Break">
          <em class="italic">
           <span class="koboSpan" id="kobo.122.1">
            the mat.
           </span>
          </em>
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.123.1">
           ”
          </span>
         </span>
        </li>
       </ul>
      </li>
     </ul>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.124.1">
       In this example, we’ll create a co-occurrence graph where words that appear in the same
      </span>
      <a id="_idIndexMarker505">
      </a>
      <span class="koboSpan" id="kobo.125.1">
       sentence
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.126.1">
        are
       </span>
      </span>
      <span class="No-Break">
       <a id="_idIndexMarker506">
       </a>
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.127.1">
        connected:
       </span>
      </span>
     </p>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer288">
     <span class="koboSpan" id="kobo.128.1">
      <img alt="Figure 8.2 – Word co-occurrence graph" src="image/B22118_08_02.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.129.1">
     Figure 8.2 – Word co-occurrence graph
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.130.1">
       Knowledge graphs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.131.1">
      represent factual information and relationships
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.132.1">
       between entities:
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.133.1">
         Nodes
        </span>
       </strong>
       <span class="koboSpan" id="kobo.134.1">
        are entities (e.g., people, places, concepts) and
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.135.1">
         edges
        </span>
       </strong>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.136.1">
         are relationships.
        </span>
       </span>
      </li>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.137.1">
         Example
        </span>
       </strong>
       <span class="koboSpan" id="kobo.138.1">
        : Some
       </span>
       <a id="_idIndexMarker507">
       </a>
       <span class="koboSpan" id="kobo.139.1">
        examples
       </span>
       <a id="_idIndexMarker508">
       </a>
       <span class="koboSpan" id="kobo.140.1">
        include ConceptNet, Wikidata, and
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.141.1">
         domain-specific ontologies.
        </span>
       </span>
      </li>
     </ul>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer289">
     <span class="koboSpan" id="kobo.142.1">
      <img alt="Figure 8.3 – Movie industry knowledge graph" src="image/B22118_08_03.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.143.1">
     Figure 8.3 – Movie industry knowledge graph
    </span>
   </p>
   <p class="list-inset">
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.144.1">
       Figure 8
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.145.1">
      .3
     </span>
    </em>
    <span class="koboSpan" id="kobo.146.1">
     illustrates relationships between various elements of the film industry, including directors, movies, genres, and awards.
    </span>
    <span class="koboSpan" id="kobo.146.2">
     The
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.147.1">
      nodes
     </span>
    </em>
    <span class="koboSpan" id="kobo.148.1">
     represent entities such as filmmakers (e.g.,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.149.1">
      Christopher Nolan
     </span>
    </strong>
    <span class="koboSpan" id="kobo.150.1">
     ,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.151.1">
      Quentin Tarantino
     </span>
    </strong>
    <span class="koboSpan" id="kobo.152.1">
     ), films (e.g.,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.153.1">
      Interstellar
     </span>
    </strong>
    <span class="koboSpan" id="kobo.154.1">
     ,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.155.1">
      Pulp Fiction
     </span>
    </strong>
    <span class="koboSpan" id="kobo.156.1">
     ), genres (e.g.,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.157.1">
      sci-fi
     </span>
    </strong>
    <span class="koboSpan" id="kobo.158.1">
     ,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.159.1">
      crime
     </span>
    </strong>
    <span class="koboSpan" id="kobo.160.1">
     ), and awards (e.g.,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.161.1">
      Oscar
     </span>
    </strong>
    <span class="koboSpan" id="kobo.162.1">
     ,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.163.1">
      BAFTA
     </span>
    </strong>
    <span class="koboSpan" id="kobo.164.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.164.2">
     The connections between these nodes demonstrate the complex interplay of creative talent, film categories, and industry recognition, offering insights into the multifaceted nature of cinema and its
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.165.1">
      key players.
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.166.1">
       Semantic graphs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.167.1">
      represent the meaning of sentences or documents.
     </span>
     <span class="koboSpan" id="kobo.167.2">
      They are used in tasks such as semantic parsing and abstract meaning representation.
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.168.1">
       Semantic parsing
      </span>
     </strong>
     <span class="koboSpan" id="kobo.169.1">
      in NLP is the task of converting natural language expressions into formal, structured representations of their meaning.
     </span>
     <span class="koboSpan" id="kobo.169.2">
      It involves mapping words and phrases to concepts, identifying relationships, and generating logical forms or executable code that capture the underlying semantics of the input text.
     </span>
     <span class="koboSpan" id="kobo.169.3">
      This process enables machines to understand and act upon human language in a more precise and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.170.1">
       actionable manner.
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.171.1">
         Nodes
        </span>
       </strong>
       <span class="koboSpan" id="kobo.172.1">
        can
       </span>
       <a id="_idIndexMarker509">
       </a>
       <span class="koboSpan" id="kobo.173.1">
        be concepts, events, or
       </span>
       <a id="_idIndexMarker510">
       </a>
       <span class="koboSpan" id="kobo.174.1">
        propositions, with
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.175.1">
         edges
        </span>
       </strong>
       <span class="koboSpan" id="kobo.176.1">
        showing
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.177.1">
         semantic relationships.
        </span>
       </span>
      </li>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.178.1">
         Example
        </span>
       </strong>
       <span class="koboSpan" id="kobo.179.1">
        : The semantic graph in
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.180.1">
         Figure 8.4
        </span>
       </em>
       <span class="koboSpan" id="kobo.181.1">
        illustrates how the sentence “
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.182.1">
         John read a book in the library
        </span>
       </em>
       <span class="koboSpan" id="kobo.183.1">
        ” can be broken down into its constituent parts and relationships.
       </span>
       <span class="koboSpan" id="kobo.183.2">
        The graph consists of nodes representing concepts and edges showing the semantic relationships
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.184.1">
         between them:
        </span>
       </span>
      </li>
     </ul>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer290">
     <span class="koboSpan" id="kobo.185.1">
      <img alt="Figure 8.4 – Semantic graphical representation" src="image/B22118_08_04.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.186.1">
     Figure 8.4 – Semantic graphical representation
    </span>
   </p>
   <p class="list-inset">
    <span class="koboSpan" id="kobo.187.1">
     Specifically, the graph
    </span>
    <a id="_idIndexMarker511">
    </a>
    <span class="koboSpan" id="kobo.188.1">
     shows
    </span>
    <a id="_idIndexMarker512">
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.189.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.190.1">
      “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.191.1">
       John
      </span>
     </em>
     <span class="koboSpan" id="kobo.192.1">
      ” is connected to “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.193.1">
       Read
      </span>
     </em>
     <span class="koboSpan" id="kobo.194.1">
      ” with the label
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.195.1">
       agent_of
      </span>
     </strong>
     <span class="koboSpan" id="kobo.196.1">
      , indicating John is the one performing
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.197.1">
       the action.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.198.1">
      “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.199.1">
       Book
      </span>
     </em>
     <span class="koboSpan" id="kobo.200.1">
      ” is connected to “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.201.1">
       Read
      </span>
     </em>
     <span class="koboSpan" id="kobo.202.1">
      ” with the label
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.203.1">
       object_of
      </span>
     </strong>
     <span class="koboSpan" id="kobo.204.1">
      , showing that the book is what’s
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.205.1">
       being read.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.206.1">
      “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.207.1">
       Library
      </span>
     </em>
     <span class="koboSpan" id="kobo.208.1">
      ” is connected to “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.209.1">
       Read
      </span>
     </em>
     <span class="koboSpan" id="kobo.210.1">
      ” with the label
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.211.1">
       location_of
      </span>
     </strong>
     <span class="koboSpan" id="kobo.212.1">
      , indicating where the reading
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.213.1">
       took place.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.214.1">
      “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.215.1">
       John
      </span>
     </em>
     <span class="koboSpan" id="kobo.216.1">
      ” is also connected to “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.217.1">
       Person
      </span>
     </em>
     <span class="koboSpan" id="kobo.218.1">
      ” with an
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.219.1">
       is_a
      </span>
     </strong>
     <span class="koboSpan" id="kobo.220.1">
      relationship, classifying John as
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.221.1">
       a person.
      </span>
     </span>
    </li>
   </ul>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.222.1">
       Discourse graphs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.223.1">
      represent the structure of longer texts or conversations and are used in tasks such as dialogue understanding and text
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.224.1">
       coherence analysis.
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.225.1">
         Nodes
        </span>
       </strong>
       <span class="koboSpan" id="kobo.226.1">
        can be sentences or utterances, with
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.227.1">
         edges
        </span>
       </strong>
       <span class="koboSpan" id="kobo.228.1">
        showing
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.229.1">
         discourse relations.
        </span>
       </span>
      </li>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.230.1">
         Example
        </span>
       </strong>
       <span class="koboSpan" id="kobo.231.1">
        :
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.232.1">
         Figure 8.5
        </span>
       </em>
       <span class="koboSpan" id="kobo.233.1">
        depicts a discourse graph representing a simple conversation.
       </span>
       <span class="koboSpan" id="kobo.233.2">
        The graph consists of seven nodes (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.234.1">
         U0
        </span>
       </strong>
       <span class="koboSpan" id="kobo.235.1">
        to
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.236.1">
         U6
        </span>
       </strong>
       <span class="koboSpan" id="kobo.237.1">
        ), each representing an utterance in the conversation, connected by directed edges that show the flow and relationships between the utterances.
       </span>
       <span class="koboSpan" id="kobo.237.2">
        The edges are labeled with discourse relations such as
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.238.1">
         greeting-response
        </span>
       </strong>
       <span class="koboSpan" id="kobo.239.1">
        ,
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.240.1">
         acknowledgment
        </span>
       </strong>
       <span class="koboSpan" id="kobo.241.1">
        ,
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.242.1">
         question
        </span>
       </strong>
       <span class="koboSpan" id="kobo.243.1">
        ,
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.244.1">
         response
        </span>
       </strong>
       <span class="koboSpan" id="kobo.245.1">
        , and
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.246.1">
         acknowledgment-farewell
        </span>
       </strong>
       <span class="koboSpan" id="kobo.247.1">
        .
       </span>
       <span class="koboSpan" id="kobo.247.2">
        The conversation begins with a greeting (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.248.1">
         U0
        </span>
       </strong>
       <span class="koboSpan" id="kobo.249.1">
        ), followed by a response (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.250.1">
         U1
        </span>
       </strong>
       <span class="koboSpan" id="kobo.251.1">
        ), which then branches out to an acknowledgment (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.252.1">
         U2
        </span>
       </strong>
       <span class="koboSpan" id="kobo.253.1">
        ) and a question (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.254.1">
         U3
        </span>
       </strong>
       <span class="koboSpan" id="kobo.255.1">
        ).
       </span>
       <span class="koboSpan" id="kobo.255.2">
        The dialogue continues with further responses and concludes with a farewell (
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.256.1">
         U6
        </span>
       </strong>
       <span class="koboSpan" id="kobo.257.1">
        ).
       </span>
       <span class="koboSpan" id="kobo.257.2">
        The legend provides brief snippets of each utterance, giving context
       </span>
       <a id="_idIndexMarker513">
       </a>
       <span class="koboSpan" id="kobo.258.1">
        to the
       </span>
       <a id="_idIndexMarker514">
       </a>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.259.1">
         conversation flow:
        </span>
       </span>
      </li>
     </ul>
    </li>
   </ul>
   <div>
    <div class="IMG---Figure" id="_idContainer291">
     <span class="koboSpan" id="kobo.260.1">
      <img alt="Figure 8.5 – Discourse graph: simple conversation" src="image/B22118_08_05.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.261.1">
     Figure 8.5 – Discourse graph: simple conversation
    </span>
   </p>
   <p class="list-inset">
    <span class="koboSpan" id="kobo.262.1">
     This visual representation effectively illustrates how discourse graphs can be used to analyze the structure, coherence, and progression of a conversation, making it a valuable tool for
    </span>
    <a id="_idIndexMarker515">
    </a>
    <span class="koboSpan" id="kobo.263.1">
     tasks such as dialogue understanding and text
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.264.1">
      coherence
     </span>
    </span>
    <span class="No-Break">
     <a id="_idIndexMarker516">
     </a>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.265.1">
      analysis.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.266.1">
     Now, let’s examine a few real-world use cases of graph learning
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.267.1">
      in NLP.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-140">
    <a id="_idTextAnchor143">
    </a>
    <span class="koboSpan" id="kobo.268.1">
     Graph-based text summarization
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.269.1">
     Graph-based
    </span>
    <a id="_idIndexMarker517">
    </a>
    <span class="koboSpan" id="kobo.270.1">
     approaches have become increasingly popular in text summarization due to their ability to capture complex relationships between textual elements.
    </span>
    <span class="koboSpan" id="kobo.270.2">
     Here, we will explore two main categories of graph-based summarization: extractive
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.271.1">
      and abstractive.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-141">
    <a id="_idTextAnchor144">
    </a>
    <span class="koboSpan" id="kobo.272.1">
     Extractive summarization using graph centrality
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.273.1">
      Extractive summarization
     </span>
    </strong>
    <span class="koboSpan" id="kobo.274.1">
     involves
    </span>
    <a id="_idIndexMarker518">
    </a>
    <span class="koboSpan" id="kobo.275.1">
     selecting
    </span>
    <a id="_idIndexMarker519">
    </a>
    <span class="koboSpan" id="kobo.276.1">
     and arranging the most important sentences from the original text to form a concise summary.
    </span>
    <span class="koboSpan" id="kobo.276.2">
     Graph-based methods for extractive summarization typically follow
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.277.1">
      these steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.278.1">
      Construct a graph representation of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.279.1">
       the text.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.280.1">
      Apply centrality measures to identify important
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.281.1">
       nodes (sentences).
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.282.1">
      Extract top-ranked sentences to form
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.283.1">
       the summary.
      </span>
     </span>
    </li>
   </ol>
   <h3>
    <span class="koboSpan" id="kobo.284.1">
     Graph construction
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.285.1">
     The text is represented
    </span>
    <a id="_idIndexMarker520">
    </a>
    <span class="koboSpan" id="kobo.286.1">
     as a graph where
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.287.1">
      nodes
     </span>
    </em>
    <span class="koboSpan" id="kobo.288.1">
     are sentences and
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.289.1">
      edges
     </span>
    </em>
    <span class="koboSpan" id="kobo.290.1">
     represent similarities between sentences.
    </span>
    <span class="koboSpan" id="kobo.290.2">
     Common similarity measures include
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.291.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.292.1">
      Cosine similarity
     </span>
     <a id="_idIndexMarker521">
     </a>
     <span class="koboSpan" id="kobo.293.1">
      of
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.294.1">
       term frequency-inverse document frequency
      </span>
     </strong>
     <span class="koboSpan" id="kobo.295.1">
      (
     </span>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.296.1">
        TF-IDF
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.297.1">
       ) vectors
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.298.1">
       Jaccard similarity
      </span>
     </strong>
     <span class="koboSpan" id="kobo.299.1">
      of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.300.1">
       word sets
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.301.1">
       Semantic similarity
      </span>
     </strong>
     <span class="koboSpan" id="kobo.302.1">
      using
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.303.1">
       word embeddings
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.304.1">
     Centrality measures
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.305.1">
     Several
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.306.1">
      graph centrality
     </span>
    </strong>
    <span class="koboSpan" id="kobo.307.1">
     measures
    </span>
    <a id="_idIndexMarker522">
    </a>
    <span class="koboSpan" id="kobo.308.1">
     can be used to rank the importance
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.309.1">
      of sentences:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.310.1">
       Degree centrality
      </span>
     </strong>
     <span class="koboSpan" id="kobo.311.1">
      measures the number of connections a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.312.1">
       node has.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.313.1">
       Eigenvector centrality
      </span>
     </strong>
     <span class="koboSpan" id="kobo.314.1">
      considers the importance of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.315.1">
       neighboring nodes.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.316.1">
       PageRank
      </span>
     </strong>
     <span class="koboSpan" id="kobo.317.1">
      is a variant of eigenvector centrality, originally used by Google for ranking
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.318.1">
       web pages.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.319.1">
       Hyperlink-Induced Topic Search
      </span>
     </strong>
     <span class="koboSpan" id="kobo.320.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.321.1">
       HITS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.322.1">
      ) computes 	hub and authority scores
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.323.1">
       for nodes.
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.324.1">
     Example – TextRank algorithm
    </span>
   </h3>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.325.1">
      TextRank
     </span>
    </strong>
    <span class="koboSpan" id="kobo.326.1">
     , proposed
    </span>
    <a id="_idIndexMarker523">
    </a>
    <span class="koboSpan" id="kobo.327.1">
     by
    </span>
    <a id="_idIndexMarker524">
    </a>
    <span class="koboSpan" id="kobo.328.1">
     Mihalcea
    </span>
    <a id="_idIndexMarker525">
    </a>
    <span class="koboSpan" id="kobo.329.1">
     and Tarau in 2004 (
    </span>
    <a href="https://aclanthology.org/W04-3252/">
     <span class="koboSpan" id="kobo.330.1">
      https://aclanthology.org/W04-3252/
     </span>
    </a>
    <span class="koboSpan" id="kobo.331.1">
     ), is a popular graph-based extractive summarization method inspired
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.332.1">
      by PageRank.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.333.1">
     Let’s look at a simplified
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.334.1">
      Python implementation:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.335.1">
      First, we import the necessary libraries –
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.336.1">
       NetworkX
      </span>
     </strong>
     <span class="koboSpan" id="kobo.337.1">
      for graph operations,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.338.1">
       NumPy
      </span>
     </strong>
     <span class="koboSpan" id="kobo.339.1">
      for numerical computations, and
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.340.1">
       scikit-learn
      </span>
     </strong>
     <span class="koboSpan" id="kobo.341.1">
      for TF-IDF vectorization and cosine
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.342.1">
       similarity calculation:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.343.1">
import networkx as nx
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.344.1">
      Then, we define the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.345.1">
       textrank
      </span>
     </strong>
     <span class="koboSpan" id="kobo.346.1">
      function, which takes a list of sentences and the number of top sentences to return.
     </span>
     <span class="koboSpan" id="kobo.346.2">
      It creates a TF-IDF matrix from the sentences and computes a similarity matrix using
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.347.1">
       cosine similarity:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.348.1">
def textrank(sentences, top_n=2):
    tfidf = TfidfVectorizer().fit_transform(sentences)
    similarity_matrix = cosine_similarity(tfidf)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.349.1">
      We create a graph from the similarity matrix and compute
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.350.1">
       pagerank
      </span>
     </strong>
     <span class="koboSpan" id="kobo.351.1">
      on this graph.
     </span>
     <span class="koboSpan" id="kobo.351.2">
      Each sentence is a
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.352.1">
       node
      </span>
     </em>
     <span class="koboSpan" id="kobo.353.1">
      , and the similarities are
     </span>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.354.1">
        edge weights
       </span>
      </em>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.355.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.356.1">
    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.357.1">
      Finally, we sort the sentences based on their
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.358.1">
       pagerank
      </span>
     </strong>
     <span class="koboSpan" id="kobo.359.1">
      scores and return the top
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.360.1">
       n
      </span>
     </strong>
     <span class="koboSpan" id="kobo.361.1">
      sentences as
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.362.1">
       the summary:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.363.1">
    ranked_sentences = sorted(((
        scores[i], s) for i, s in enumerate(sentences)
    ), reverse=True)
    return [s for _, s in ranked_sentences[:top_n]]</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.364.1">
     Here is an example usage of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.365.1">
      textrank
     </span>
    </strong>
    <span class="koboSpan" id="kobo.366.1">
     function.
    </span>
    <span class="koboSpan" id="kobo.366.2">
     In this case, we define a sample text, split it into
    </span>
    <a id="_idIndexMarker526">
    </a>
    <span class="koboSpan" id="kobo.367.1">
     sentences, apply
    </span>
    <a id="_idIndexMarker527">
    </a>
    <span class="koboSpan" id="kobo.368.1">
     the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.369.1">
      textrank
     </span>
    </strong>
    <span class="koboSpan" id="kobo.370.1">
     algorithm, and print
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.371.1">
      the summary:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.372.1">
text = """
Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. </span><span class="koboSpan" id="kobo.372.2">The ultimate objective of NLP is to read, decipher, understand, and make sense of human languages in a valuable way. </span><span class="koboSpan" id="kobo.372.3">NLP is used in many applications, including machine translation, speech recognition, and chatbots.
</span><span class="koboSpan" id="kobo.372.4">"""
sentences = text.strip().split('.')
summary = textrank(sentences)
print("Summary:", ' '.join(summary))</span></pre>
   <h2 id="_idParaDest-142">
    <a id="_idTextAnchor145">
    </a>
    <span class="koboSpan" id="kobo.373.1">
     Abstractive summarization with graph-to-sequence models
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.374.1">
      Abstractive summarization
     </span>
    </strong>
    <span class="koboSpan" id="kobo.375.1">
     aims
    </span>
    <a id="_idIndexMarker528">
    </a>
    <span class="koboSpan" id="kobo.376.1">
     to generate new sentences that capture the essence of the original text.
    </span>
    <span class="koboSpan" id="kobo.376.2">
     Graph-to-sequence models have shown promising results in this area by leveraging the structural information of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.377.1">
      input text.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.378.1">
     For abstractive summarization, graphs are often constructed to represent more
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.379.1">
      fine-grained relationships:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.380.1">
       Nodes
      </span>
     </strong>
     <span class="koboSpan" id="kobo.381.1">
      : Words, phrases,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.382.1">
       or concepts
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.383.1">
       Edges
      </span>
     </strong>
     <span class="koboSpan" id="kobo.384.1">
      : Syntactic dependencies, semantic relations,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.385.1">
       or co-occurrence
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.386.1">
     A typical graph-to-sequence model for abstractive summarization consists of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.387.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.388.1">
      A
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.389.1">
       graph encoder
      </span>
     </strong>
     <span class="koboSpan" id="kobo.390.1">
      uses GNNs (e.g.,
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.391.1">
       graph convolutional networks
      </span>
     </strong>
     <span class="koboSpan" id="kobo.392.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.393.1">
       GCNs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.394.1">
      ) or
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.395.1">
       graph attention network
      </span>
     </strong>
     <span class="koboSpan" id="kobo.396.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.397.1">
       GATs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.398.1">
      )) to
     </span>
     <a id="_idIndexMarker529">
     </a>
     <span class="koboSpan" id="kobo.399.1">
      encode
     </span>
     <a id="_idIndexMarker530">
     </a>
     <span class="koboSpan" id="kobo.400.1">
      the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.401.1">
       input graph.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.402.1">
      A
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.403.1">
       sequence decoder
      </span>
     </strong>
     <span class="koboSpan" id="kobo.404.1">
      generates the summary text, often using attention mechanisms to focus on relevant parts of the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.405.1">
       encoded graph.
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.406.1">
     Abstract meaning representation (AMR)-to-text summarization
    </span>
   </h3>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.407.1">
      Abstract meaning representation
     </span>
    </strong>
    <span class="koboSpan" id="kobo.408.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.409.1">
      AMR
     </span>
    </strong>
    <span class="koboSpan" id="kobo.410.1">
     ) is a
    </span>
    <a id="_idIndexMarker531">
    </a>
    <span class="koboSpan" id="kobo.411.1">
     semantic graph
    </span>
    <a id="_idIndexMarker532">
    </a>
    <span class="koboSpan" id="kobo.412.1">
     representation of text.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.413.1">
      AMR-to-text summarization
     </span>
    </strong>
    <span class="koboSpan" id="kobo.414.1">
     is an example of graph-to-sequence abstractive summarization.
    </span>
    <span class="koboSpan" id="kobo.414.2">
     Let’s
    </span>
    <a id="_idIndexMarker533">
    </a>
    <span class="koboSpan" id="kobo.415.1">
     consider a conceptual example
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.416.1">
      using
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.417.1">
       PyTorch
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.418.1">
      :
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.419.1">
      We import the necessary libraries –
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.420.1">
       PyTorch
      </span>
     </strong>
     <span class="koboSpan" id="kobo.421.1">
      for deep learning operations,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.422.1">
       PyTorch Geometric
      </span>
     </strong>
     <span class="koboSpan" id="kobo.423.1">
      for GNNs, and specific modules for neural network layers
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.424.1">
       and functions:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.425.1">
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.426.1">
      Then, we define the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.427.1">
       AMRToTextSummarizer
      </span>
     </strong>
     <span class="koboSpan" id="kobo.428.1">
      class, which is a neural network module.
     </span>
     <span class="koboSpan" id="kobo.428.2">
      It initializes
     </span>
     <a id="_idIndexMarker534">
     </a>
     <span class="koboSpan" id="kobo.429.1">
      a GCN layer, a
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.430.1">
       gated recurrent unit
      </span>
     </strong>
     <span class="koboSpan" id="kobo.431.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.432.1">
       GRU
      </span>
     </strong>
     <span class="koboSpan" id="kobo.433.1">
      ) layer, and a fully
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.434.1">
       connected layer:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.435.1">
class AMRToTextSummarizer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.graph_conv = GCNConv(input_dim, hidden_dim)
        self.gru = nn.GRU(
            hidden_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.436.1">
      This is the forward pass of the network.
     </span>
     <span class="koboSpan" id="kobo.436.2">
      We first apply graph convolution to encode the graph structure, then use a GRU for sequence decoding, and finally apply a fully
     </span>
     <a id="_idIndexMarker535">
     </a>
     <span class="koboSpan" id="kobo.437.1">
      connected layer to produce
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.438.1">
       the output:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.439.1">
    def forward(self, data):
        # Graph encoding
        x, edge_index = data.x, data.edge_index
        h = self.graph_conv(x, edge_index)
        h = F.relu(h)
        # Sequence decoding
        h = h.unsqueeze(0)  # Add batch dimension
        output, _ = self.gru(h)
        output = self.fc(output)
        return output</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.440.1">
      We set up the model parameters and create a sample graph for demonstration.
     </span>
     <span class="koboSpan" id="kobo.440.2">
      Here, we define the dimensions for the input, hidden layer, and output and create a graph with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.441.1">
       three nodes:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.442.1">
input_dim = 100  # Dimension of input node features
hidden_dim = 256
output_dim = 10000  # Vocabulary size
edge_index = torch.tensor([
    [0, 1, 2], [1, 2, 0]], dtype=torch.long)
x = torch.randn(3, input_dim)
data = Data(x=x, edge_index=edge_index)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.443.1">
      Finally, we instantiate the model, run a forward pass with the sample data, and print the shape of the output logits.
     </span>
     <span class="koboSpan" id="kobo.443.2">
      The output shape would be (1, 3, 10000), representing a batch size of 1, 3 nodes, and logits over a vocabulary of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.444.1">
       10,000 words:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.445.1">
model = AMRToTextSummarizer(input_dim, hidden_dim, output_dim)
summary_logits = model(data)
print("Summary logits shape:", summary_logits.shape)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.446.1">
     This example demonstrates the basic structure of a graph-to-sequence model for abstractive
    </span>
    <a id="_idIndexMarker536">
    </a>
    <span class="koboSpan" id="kobo.447.1">
     summarization.
    </span>
    <span class="koboSpan" id="kobo.447.2">
     In practice, more sophisticated architectures, attention mechanisms, and training procedures would
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.448.1">
      be employed.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-143">
    <a id="_idTextAnchor146">
    </a>
    <span class="koboSpan" id="kobo.449.1">
     Information extraction (IE) using GNNs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.450.1">
     IE is a crucial task in NLP that
    </span>
    <a id="_idIndexMarker537">
    </a>
    <span class="koboSpan" id="kobo.451.1">
     involves automatically extracting structured information from unstructured text.
    </span>
    <span class="koboSpan" id="kobo.451.2">
     GNNs have shown promising results in this domain, particularly in event extraction and open IE.
    </span>
    <span class="koboSpan" id="kobo.451.3">
     In this section, we explore how GNNs are applied to these
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.452.1">
      IE tasks.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-144">
    <a id="_idTextAnchor147">
    </a>
    <span class="koboSpan" id="kobo.453.1">
     Event extraction
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.454.1">
      Event extraction
     </span>
    </strong>
    <span class="koboSpan" id="kobo.455.1">
     is the task
    </span>
    <a id="_idIndexMarker538">
    </a>
    <span class="koboSpan" id="kobo.456.1">
     of identifying and categorizing events mentioned
    </span>
    <a id="_idIndexMarker539">
    </a>
    <span class="koboSpan" id="kobo.457.1">
     in text, along with their participants and attributes.
    </span>
    <span class="koboSpan" id="kobo.457.2">
     GNNs have proven effective in this task due to their ability to capture complex relationships between entities and events in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.458.1">
      a document.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.459.1">
     Graph construction for event extraction
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.460.1">
     In event extraction, a
    </span>
    <a id="_idIndexMarker540">
    </a>
    <span class="koboSpan" id="kobo.461.1">
     document is typically represented as a graph where
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.462.1">
      nodes
     </span>
    </strong>
    <span class="koboSpan" id="kobo.463.1">
     represent entities, events, and tokens, while
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.464.1">
      edges
     </span>
    </strong>
    <span class="koboSpan" id="kobo.465.1">
     represent various relationships such as syntactic dependencies, coreference links, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.466.1">
      temporal order.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.467.1">
     Consider the following sentence: “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.468.1">
      John Smith resigned as CEO of TechCorp
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.469.1">
       on Monday
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.470.1">
      .”
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.471.1">
     The graph representation might include
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.472.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.473.1">
       Nodes
      </span>
     </strong>
     <span class="koboSpan" id="kobo.474.1">
      : John Smith (person), TechCorp (organization), CEO (role), Monday (time),
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.475.1">
       Resignation (event)
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.476.1">
       Edges
      </span>
     </strong>
     <span class="koboSpan" id="kobo.477.1">
      : (John Smith) -[AGENT]-&gt; (Resignation), (Resignation) -[ROLE]-&gt; (CEO), (Resignation) -[ORG]-&gt; (TechCorp), (Resignation) -[
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.478.1">
       TIME]-&gt; (Monday)
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.479.1">
     GNN-based event extraction models
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.480.1">
     GNN models for
    </span>
    <a id="_idIndexMarker541">
    </a>
    <span class="koboSpan" id="kobo.481.1">
     event extraction typically involve
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.482.1">
      the following:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.483.1">
      Encoding the initial node features using pre-trained word embeddings or
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.484.1">
       contextual embeddings.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.485.1">
      Applying multiple layers of graph convolution to propagate information across
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.486.1">
       the graph.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.487.1">
      Using the final node representations to classify events and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.488.1">
       their arguments.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.489.1">
       Figure 8
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.490.1">
      .6
     </span>
    </em>
    <span class="koboSpan" id="kobo.491.1">
     shows an
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.492.1">
      example architecture:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer292">
     <span class="koboSpan" id="kobo.493.1">
      <img alt="Figure 8.6 – GNN-based event extraction model architecture and process flow" src="image/B22118_08_06.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.494.1">
     Figure 8.6 – GNN-based event extraction model architecture and process flow
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.495.1">
     A recent study (
    </span>
    <a href="https://aclanthology.org/2021.acl-long.274/">
     <span class="koboSpan" id="kobo.496.1">
      https://aclanthology.org/2021.acl-long.274/
     </span>
    </a>
    <span class="koboSpan" id="kobo.497.1">
     ) has shown that GNN-based models can outperform traditional sequence-based models, especially in capturing long-range dependencies and handling multiple events in a single document.
    </span>
    <span class="koboSpan" id="kobo.497.2">
     Specifically, graph-based methods showed significant improvements in handling cross-sentence events and multiple event scenarios through a heterogeneous graph interaction network that captured global context and a
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.498.1">
      Tracker
     </span>
    </em>
    <span class="koboSpan" id="kobo.499.1">
     module that modeled interdependencies between events.
    </span>
    <span class="koboSpan" id="kobo.499.2">
     The model’s effectiveness was particularly evident when extracting events that involved many scattered arguments across different sentences, validating
    </span>
    <a id="_idIndexMarker542">
    </a>
    <span class="koboSpan" id="kobo.500.1">
     GNNs’ superior ability to capture long-range dependencies compared to traditional
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.501.1">
      sequence models.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-145">
    <a id="_idTextAnchor148">
    </a>
    <span class="koboSpan" id="kobo.502.1">
     Open IE
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.503.1">
      Open information extraction
     </span>
    </strong>
    <span class="koboSpan" id="kobo.504.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.505.1">
      OpenIE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.506.1">
     ) aims to
    </span>
    <a id="_idIndexMarker543">
    </a>
    <span class="koboSpan" id="kobo.507.1">
     extract relational triples (subject, relation, object) from
    </span>
    <a id="_idIndexMarker544">
    </a>
    <span class="koboSpan" id="kobo.508.1">
     text without being confined to a predefined set of relations.
    </span>
    <span class="koboSpan" id="kobo.508.2">
     GNNs have been successfully applied to this task, leveraging the inherent graph-like structure
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.509.1">
      of sentences.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.510.1">
     Graph-based OpenIE approach
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.511.1">
     In a graph-based OpenIE
    </span>
    <a id="_idIndexMarker545">
    </a>
    <span class="koboSpan" id="kobo.512.1">
     system, sentences are typically converted into dependency parse trees, which are then used as the input graph for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.513.1">
      a GNN.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.514.1">
     For example, in the sentence “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.515.1">
      Einstein developed the theory of relativity
     </span>
    </em>
    <span class="koboSpan" id="kobo.516.1">
     ,” the dependency parse might look like
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.517.1">
      the following:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer293">
     <span class="koboSpan" id="kobo.518.1">
      <img alt="Figure 8.7 – Graph-based OpenIE approach using dependency parsing" src="image/B22118_08_07.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.519.1">
     Figure 8.7 – Graph-based OpenIE approach using dependency parsing
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.520.1">
     GNN processing for OpenIE
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.521.1">
     The GNN processes
    </span>
    <a id="_idIndexMarker546">
    </a>
    <span class="koboSpan" id="kobo.522.1">
     this graph in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.523.1">
      several steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.524.1">
       Node encoding
      </span>
     </strong>
     <span class="koboSpan" id="kobo.525.1">
      : Each word is encoded using
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.526.1">
       contextual embeddings.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.527.1">
       Graph convolution
      </span>
     </strong>
     <span class="koboSpan" id="kobo.528.1">
      : Information is propagated along the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.529.1">
       dependency edges.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.530.1">
       Relation prediction
      </span>
     </strong>
     <span class="koboSpan" id="kobo.531.1">
      : The model predicts potential relations between pairs
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.532.1">
       of nodes.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.533.1">
       Triple formation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.534.1">
      : Valid subject-relation-object triples are constructed based on
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.535.1">
       the predictions.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.536.1">
     Using our previous example, the output would look like this: (Einstein, developed, theory
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.537.1">
      of relativity).
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-146">
    <a id="_idTextAnchor149">
    </a>
    <span class="koboSpan" id="kobo.538.1">
     Advantages of GNN-based IE
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.539.1">
     GNN-based approaches
    </span>
    <a id="_idIndexMarker547">
    </a>
    <span class="koboSpan" id="kobo.540.1">
     to IE offer
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.541.1">
      several advantages:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.542.1">
      Capturing long-range dependencies that may be missed by
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.543.1">
       sequential models
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.544.1">
      Integrating various types of linguistic information (syntactic, semantic, coreference) into a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.545.1">
       unified framework
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.546.1">
      Handling documents with complex structures, such as scientific papers or
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.547.1">
       legal documents
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.548.1">
     Future research in this area may focus on combining GNNs with other deep learning architectures, such as transformers, to create hybrid models that leverage the strengths of both approaches for
    </span>
    <a id="_idIndexMarker548">
    </a>
    <span class="koboSpan" id="kobo.549.1">
     more accurate and comprehensive
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.550.1">
      information extraction.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-147">
    <a id="_idTextAnchor150">
    </a>
    <span class="koboSpan" id="kobo.551.1">
     Graph-based dialogue systems
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.552.1">
     Dialogue systems are
    </span>
    <a id="_idIndexMarker549">
    </a>
    <span class="koboSpan" id="kobo.553.1">
     sophisticated AI-powered applications designed to facilitate human-computer interaction through natural language.
    </span>
    <span class="koboSpan" id="kobo.553.2">
     These systems employ various NLP techniques such as natural language understanding, dialogue management, and natural language generation to interpret user inputs, maintain context, and produce appropriate responses.
    </span>
    <span class="koboSpan" id="kobo.553.3">
     Modern dialogue systems often integrate machine learning algorithms to improve their performance over time, adapting to user preferences and learning from past interactions.
    </span>
    <span class="koboSpan" id="kobo.553.4">
     They find applications in diverse fields, including customer service, virtual assistants, educational tools, and interactive storytelling, continuously evolving to provide more natural and effective communication between humans
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.554.1">
      and machines.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.555.1">
     Graph-based approaches have shown significant promise in enhancing the performance and capabilities of dialogue systems.
    </span>
    <span class="koboSpan" id="kobo.555.2">
     By leveraging graph structures to represent dialogue context, knowledge, and semantic relationships, these systems can better understand user intents, track conversation states, and generate more coherent and contextually
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.556.1">
      appropriate responses.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-148">
    <a id="_idTextAnchor151">
    </a>
    <span class="koboSpan" id="kobo.557.1">
     Dialogue state tracking with GNNs
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.558.1">
      Dialogue state tracking
     </span>
    </strong>
    <span class="koboSpan" id="kobo.559.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.560.1">
      DST
     </span>
    </strong>
    <span class="koboSpan" id="kobo.561.1">
     ) is a
    </span>
    <a id="_idIndexMarker550">
    </a>
    <span class="koboSpan" id="kobo.562.1">
     crucial
    </span>
    <a id="_idIndexMarker551">
    </a>
    <span class="koboSpan" id="kobo.563.1">
     component
    </span>
    <a id="_idIndexMarker552">
    </a>
    <span class="koboSpan" id="kobo.564.1">
     of task-oriented dialogue systems, responsible for maintaining an up-to-date representation of the user’s goals and preferences throughout the conversation.
    </span>
    <span class="koboSpan" id="kobo.564.2">
     GNNs have been successfully applied to improve the accuracy and robustness
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.565.1">
      of DST.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.566.1">
     In a typical GNN-based DST approach, the dialogue history is represented as a graph, where
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.567.1">
      nodes
     </span>
    </em>
    <span class="koboSpan" id="kobo.568.1">
     represent utterances, slots, and values, while
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.569.1">
      edges
     </span>
    </em>
    <span class="koboSpan" id="kobo.570.1">
     capture the relationships between these elements.
    </span>
    <span class="koboSpan" id="kobo.570.2">
     As the conversation progresses, the graph is dynamically updated, and GNN layers are applied to propagate information across the graph, enabling more accurate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.571.1">
      state predictions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.572.1">
     For example, the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.573.1">
      graph state tracker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.574.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.575.1">
      GST
     </span>
    </strong>
    <span class="koboSpan" id="kobo.576.1">
     ) proposed
    </span>
    <a id="_idIndexMarker553">
    </a>
    <span class="koboSpan" id="kobo.577.1">
     by Chen et al.
    </span>
    <span class="koboSpan" id="kobo.577.2">
     in 2020 (
    </span>
    <a href="https://doi.org/10.1609/aaai.v34i05.6250">
     <span class="koboSpan" id="kobo.578.1">
      https://doi.org/10.1609/aaai.v34i05.6250
     </span>
    </a>
    <span class="koboSpan" id="kobo.579.1">
     ) uses a GAT to model the dependencies between different dialogue elements.
    </span>
    <span class="koboSpan" id="kobo.579.2">
     This approach has shown superior performance on benchmark datasets
    </span>
    <a id="_idIndexMarker554">
    </a>
    <span class="koboSpan" id="kobo.580.1">
     such
    </span>
    <a id="_idIndexMarker555">
    </a>
    <span class="koboSpan" id="kobo.581.1">
     as MultiWOZ, particularly
    </span>
    <a id="_idIndexMarker556">
    </a>
    <span class="koboSpan" id="kobo.582.1">
     in handling complex
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.583.1">
      multi-domain conversations.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-149">
    <a id="_idTextAnchor152">
    </a>
    <span class="koboSpan" id="kobo.584.1">
     Graph-enhanced response generation
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.585.1">
     Graph structures
    </span>
    <a id="_idIndexMarker557">
    </a>
    <span class="koboSpan" id="kobo.586.1">
     can also
    </span>
    <a id="_idIndexMarker558">
    </a>
    <span class="koboSpan" id="kobo.587.1">
     significantly improve the quality and relevance of generated responses in both task-oriented and open-domain dialogue systems.
    </span>
    <span class="koboSpan" id="kobo.587.2">
     By incorporating knowledge graphs or conversation flow graphs, these systems can produce more informative, coherent, and contextually
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.588.1">
      appropriate responses.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.589.1">
     One approach is to use graph-to-sequence models, where the input dialogue context is first converted into a graph representation, and then a graph-aware decoder generates the response.
    </span>
    <span class="koboSpan" id="kobo.589.2">
     This allows the model to capture long-range dependencies and complex relationships within the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.590.1">
      conversation history.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.591.1">
     For instance, the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.592.1">
      GraphDialog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.593.1">
     model
    </span>
    <a id="_idIndexMarker559">
    </a>
    <span class="koboSpan" id="kobo.594.1">
     introduced by Yang et al.
    </span>
    <span class="koboSpan" id="kobo.594.2">
     in 2021 (
    </span>
    <a href="https://doi.org/10.18653/v1/2020.emnlp-main.147">
     <span class="koboSpan" id="kobo.595.1">
      https://doi.org/10.18653/v1/2020.emnlp-main.147
     </span>
    </a>
    <span class="koboSpan" id="kobo.596.1">
     ) constructs a dialogue graph that captures both the local context (recent utterances) and global context (overall conversation flow).
    </span>
    <span class="koboSpan" id="kobo.596.2">
     The model then uses graph attention mechanisms to generate responses that are more consistent with the entire conversation history.
    </span>
    <span class="koboSpan" id="kobo.596.3">
     This approach represents conversations as structured graphs where
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.597.1">
      nodes
     </span>
    </em>
    <span class="koboSpan" id="kobo.598.1">
     represent utterances and
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.599.1">
      edges
     </span>
    </em>
    <span class="koboSpan" id="kobo.600.1">
     capture various types of relationships between them, such as temporal sequence and semantic similarity.
    </span>
    <span class="koboSpan" id="kobo.600.2">
     The graph structure allows the model to better understand long-range dependencies and thematic connections across the dialogue, moving beyond the limitations of traditional sequential models.
    </span>
    <span class="koboSpan" id="kobo.600.3">
     Furthermore, the graph attention mechanism helps the model focus on relevant historical context when generating responses, even if it occurred many turns earlier in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.601.1">
      the conversation.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.602.1">
     This architecture has shown particular effectiveness in maintaining coherence during extended conversations and handling complex multi-topic dialogues where context from different parts of
    </span>
    <a id="_idIndexMarker560">
    </a>
    <span class="koboSpan" id="kobo.603.1">
     the conversation
    </span>
    <a id="_idIndexMarker561">
    </a>
    <span class="koboSpan" id="kobo.604.1">
     needs to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.605.1">
      be integrated.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-150">
    <a id="_idTextAnchor153">
    </a>
    <span class="koboSpan" id="kobo.606.1">
     Knowledge-grounded conversations using graphs
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.607.1">
     Incorporating
    </span>
    <a id="_idIndexMarker562">
    </a>
    <span class="koboSpan" id="kobo.608.1">
     external
    </span>
    <a id="_idIndexMarker563">
    </a>
    <span class="koboSpan" id="kobo.609.1">
     knowledge into
    </span>
    <a id="_idIndexMarker564">
    </a>
    <span class="koboSpan" id="kobo.610.1">
     dialogue systems is crucial for generating informative and engaging responses.
    </span>
    <span class="koboSpan" id="kobo.610.2">
     Graph-based approaches offer an effective way to represent and utilize large-scale knowledge bases in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.611.1">
      conversation models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.612.1">
     Knowledge graphs can be integrated into dialogue systems in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.613.1">
      several ways:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.614.1">
       As a source of factual information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.615.1">
      : The system can query the knowledge graph to retrieve relevant facts and incorporate them
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.616.1">
       into responses.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.617.1">
       For entity linking and disambiguation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.618.1">
      : Graph structures can help resolve ambiguities and link mentions in the conversation to specific entities in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.619.1">
       knowledge base.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.620.1">
       To guide response generation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.621.1">
      : The graph structure can inform the generation process, ensuring that the produced responses are consistent with the known facts
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.622.1">
       and relationships.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.623.1">
     An example of this approach is the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.624.1">
      Knowledge-Aware Graph-Enhanced GPT-2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.625.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.626.1">
      KG-GPT2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.627.1">
     ) model
    </span>
    <a id="_idIndexMarker565">
    </a>
    <span class="koboSpan" id="kobo.628.1">
     proposed by W Lin et al.
    </span>
    <span class="koboSpan" id="kobo.628.2">
     (
    </span>
    <a href="https://doi.org/10.48550/arXiv.2104.04466">
     <span class="koboSpan" id="kobo.629.1">
      https://doi.org/10.48550/arXiv.2104.04466
     </span>
    </a>
    <span class="koboSpan" id="kobo.630.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.630.2">
     This model incorporates a knowledge graph into a pre-trained language model, allowing it to generate more informative and factually correct responses in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.631.1">
      open-domain conversations.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.632.1">
     Imagine you’re using a virtual assistant to plan a trip to London.
    </span>
    <span class="koboSpan" id="kobo.632.2">
     You start by asking about hotels, then restaurants, and finally transportation.
    </span>
    <span class="koboSpan" id="kobo.632.3">
     A traditional GPT-2-based system might struggle to connect related information across these different domains.
    </span>
    <span class="koboSpan" id="kobo.632.4">
     For instance, if you mention wanting a “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.633.1">
      luxury hotel in central London
     </span>
    </em>
    <span class="koboSpan" id="kobo.634.1">
     ” and later ask about “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.635.1">
      restaurants near my hotel
     </span>
    </em>
    <span class="koboSpan" id="kobo.636.1">
     ,” the system needs to understand that you’re looking for high-end restaurants in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.637.1">
      central London.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.638.1">
     The proposed model in the aforementioned paper solves this by using graph networks to create connections between related information.
    </span>
    <span class="koboSpan" id="kobo.638.2">
     It works in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.639.1">
      three steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.640.1">
      First, it processes your conversation using GPT-2 to understand
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.641.1">
       the context.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.642.1">
      Then, it uses GATs to connect related information (such as location, price range, etc.) across
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.643.1">
       different services.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.644.1">
      Finally, it uses this enhanced understanding to make better predictions about what
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.645.1">
       you want.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.646.1">
     The researchers found this approach particularly effective when dealing with limited training data.
    </span>
    <span class="koboSpan" id="kobo.646.2">
     In real terms, this means the system could learn to make good recommendations even if it hasn’t seen many similar conversations before.
    </span>
    <span class="koboSpan" id="kobo.646.3">
     For example, if it learns that people booking luxury hotels typically also book high-end restaurants and premium taxis, it can apply this pattern to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.647.1">
      new conversations.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.648.1">
     Their approach showed significant improvements over existing systems, especially in understanding relationships between different services (such as hotels and restaurants) and maintaining consistency throughout the conversation.
    </span>
    <span class="koboSpan" id="kobo.648.2">
     This makes the system more natural and
    </span>
    <a id="_idIndexMarker566">
    </a>
    <span class="koboSpan" id="kobo.649.1">
     efficient
    </span>
    <a id="_idIndexMarker567">
    </a>
    <span class="koboSpan" id="kobo.650.1">
     for real-world applications
    </span>
    <a id="_idIndexMarker568">
    </a>
    <span class="koboSpan" id="kobo.651.1">
     such as travel booking or restaurant
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.652.1">
      reservation systems.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-151">
    <a id="_idTextAnchor154">
    </a>
    <span class="koboSpan" id="kobo.653.1">
     Graph-based dialogue policy learning
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.654.1">
     In task-oriented
    </span>
    <a id="_idIndexMarker569">
    </a>
    <span class="koboSpan" id="kobo.655.1">
     dialogue
    </span>
    <a id="_idIndexMarker570">
    </a>
    <span class="koboSpan" id="kobo.656.1">
     systems, graph structures can also be leveraged to improve dialogue policy learning.
    </span>
    <span class="koboSpan" id="kobo.656.2">
     By representing the dialogue state, action space, and task structure as a graph, reinforcement-learning algorithms can more effectively explore and exploit the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.657.1">
      action space.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.658.1">
     For example, the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.659.1">
      Graph-Based Dialogue Policy
     </span>
    </strong>
    <span class="koboSpan" id="kobo.660.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.661.1">
      GDP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.662.1">
     ) framework introduced by Chen et al.
    </span>
    <span class="koboSpan" id="kobo.662.2">
     in 2021 (
    </span>
    <a href="https://aclanthology.org/C18-1107">
     <span class="koboSpan" id="kobo.663.1">
      https://aclanthology.org/C18-1107
     </span>
    </a>
    <span class="koboSpan" id="kobo.664.1">
     ) uses a GNN to model the relationships between different dialogue states and actions.
    </span>
    <span class="koboSpan" id="kobo.664.2">
     This approach enables more efficient policy learning, especially in complex
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.665.1">
      multi-domain scenarios.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.666.1">
     There is an underlying scalability problem with using graphs for language understanding related to nonlinear memory complexity.
    </span>
    <span class="koboSpan" id="kobo.666.2">
     The quadratic memory complexity issue in graph-based NLP arises because when converting text into a fully connected graph, each token/word needs to be connected to every other token, resulting in
    </span>
    <span class="koboSpan" id="kobo.667.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/215.png" style="vertical-align:-0.195em;height:0.897em;width:2.422em"/>
    </span>
    <span class="koboSpan" id="kobo.668.1">
     connections where
    </span>
    <span class="koboSpan" id="kobo.669.1">
     <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/216.png" style="vertical-align:-0.012em;height:0.460em;width:0.531em"/>
    </span>
    <span class="koboSpan" id="kobo.670.1">
     is the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.671.1">
      sequence length.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.672.1">
     For example, in a 1,000-word document, 1 million edges must be stored in memory.
    </span>
    <span class="koboSpan" id="kobo.672.2">
     This becomes particularly problematic with transformer-like architectures where each connection also stores attention weights and edge features.
    </span>
    <span class="koboSpan" id="kobo.672.3">
     Modern NLP tasks often deal with much longer sequences or multiple documents simultaneously, making this quadratic scaling unsustainable for both memory usage and computational resources.
    </span>
    <span class="koboSpan" id="kobo.672.4">
     Common mitigation strategies include sparse attention mechanisms, hierarchical graph structures, and sliding window approaches, but these can potentially lose important long-range dependencies in the text.
    </span>
    <span class="koboSpan" id="kobo.672.5">
     Please refer to
    </span>
    <a href="B22118_05.xhtml#_idTextAnchor093">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.673.1">
        Chapter 5
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.674.1">
     for a more in-depth discussion of
    </span>
    <a id="_idIndexMarker571">
    </a>
    <span class="koboSpan" id="kobo.675.1">
     approaches
    </span>
    <a id="_idIndexMarker572">
    </a>
    <span class="koboSpan" id="kobo.676.1">
     to the issue
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.677.1">
      of scalability.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-152">
    <a id="_idTextAnchor155">
    </a>
    <span class="koboSpan" id="kobo.678.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.679.1">
     In this chapter, we covered a wide range of topics, starting with the fundamental concepts of graph representations in NLP and progressing through various applications.
    </span>
    <span class="koboSpan" id="kobo.679.2">
     These applications include graph-based text summarization, IE using GNNs, OpenIE, mapping natural language to logic, question answering over knowledge graphs, and graph-based
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.680.1">
      dialogue systems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.681.1">
     You learned that graph-based approaches offer powerful tools for enhancing various aspects of dialogue systems, from state tracking to response generation and policy learning.
    </span>
    <span class="koboSpan" id="kobo.681.2">
     As research in this area continues to advance, we can expect to see even more sophisticated and capable dialogue systems that leverage the rich structural information provided by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.682.1">
      graph representations.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.683.1">
     In the next chapter, we will go through some of the very common use cases of graph learning around
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.684.1">
      recommendation systems.
     </span>
    </span>
   </p>
  </div>
 </body></html>