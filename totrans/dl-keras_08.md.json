["```py\ninitialize Q-table Q\nobserve initial state s\nrepeat\n   select and carry out action a\n   observe reward r and move to new state s'\n   Q(s, a) = Q(s, a) + α(r + γ max_a' Q(s', a') - Q(s, a))\n   s = s'\nuntil game over\n\n```", "```py\nfrom __future__ import division, print_function\nimport collections\nimport numpy as np\nimport pygame\nimport random\nimport os\n\n```", "```py\nclass MyWrappedGame(object):\n\n    def __init__(self):\n        # run pygame in headless mode\n        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n\n        pygame.init()\n\n        # set constants\n        self.COLOR_WHITE = (255, 255, 255)\n        self.COLOR_BLACK = (0, 0, 0)\n        self.GAME_WIDTH = 400\n        self.GAME_HEIGHT = 400\n        self.BALL_WIDTH = 20\n        self.BALL_HEIGHT = 20\n        self.PADDLE_WIDTH = 50\n        self.PADDLE_HEIGHT = 10\n        self.GAME_FLOOR = 350\n        self.GAME_CEILING = 10\n        self.BALL_VELOCITY = 10\n        self.PADDLE_VELOCITY = 20\n        self.FONT_SIZE = 30\n        self.MAX_TRIES_PER_GAME = 1\n        self.CUSTOM_EVENT = pygame.USEREVENT + 1\n        self.font = pygame.font.SysFont(\"Comic Sans MS\", self.FONT_SIZE)\n\n```", "```py\n    def reset(self):\n        self.frames = collections.deque(maxlen=4)\n        self.game_over = False\n        # initialize positions\n        self.paddle_x = self.GAME_WIDTH // 2\n        self.game_score = 0\n        self.reward = 0\n        self.ball_x = random.randint(0, self.GAME_WIDTH)\n        self.ball_y = self.GAME_CEILING\n        self.num_tries = 0\n\n        # set up display, clock, etc\n        self.screen = pygame.display.set_mode((self.GAME_WIDTH, self.GAME_HEIGHT))\n        self.clock = pygame.time.Clock()\n\n```", "```py\n    def step(self, action):\n        pygame.event.pump()\n\n        if action == 0: # move paddle left\n            self.paddle_x -= self.PADDLE_VELOCITY\n            if self.paddle_x < 0:\n                # bounce off the wall, go right\n                self.paddle_x = self.PADDLE_VELOCITY\n        elif action == 2: # move paddle right\n            self.paddle_x += self.PADDLE_VELOCITY\n            if self.paddle_x > self.GAME_WIDTH - self.PADDLE_WIDTH:\n                # bounce off the wall, go left\n                self.paddle_x = self.GAME_WIDTH - self.PADDLE_WIDTH - self.PADDLE_VELOCITY\n        else: # don't move paddle\n            pass\n\n        self.screen.fill(self.COLOR_BLACK)\n        score_text = self.font.render(\"Score: {:d}/{:d}, Ball: {:d}\"\n            .format(self.game_score, self.MAX_TRIES_PER_GAME,\n                    self.num_tries), True, self.COLOR_WHITE)\n        self.screen.blit(score_text, \n            ((self.GAME_WIDTH - score_text.get_width()) // 2,\n            (self.GAME_FLOOR + self.FONT_SIZE // 2)))\n\n        # update ball position\n        self.ball_y += self.BALL_VELOCITY\n        ball = pygame.draw.rect(self.screen, self.COLOR_WHITE,\n            pygame.Rect(self.ball_x, self.ball_y, self.BALL_WIDTH, \n            self.BALL_HEIGHT))\n        # update paddle position\n        paddle = pygame.draw.rect(self.screen, self.COLOR_WHITE,\n            pygame.Rect(self.paddle_x, self.GAME_FLOOR, \n                        self.PADDLE_WIDTH, self.PADDLE_HEIGHT))\n\n        # check for collision and update reward\n        self.reward = 0\n        if self.ball_y >= self.GAME_FLOOR - self.BALL_WIDTH // 2:\n            if ball.colliderect(paddle):\n                self.reward = 1\n            else:\n                self.reward = -1\n\n        self.game_score += self.reward\n        self.ball_x = random.randint(0, self.GAME_WIDTH)\n        self.ball_y = self.GAME_CEILING\n        self.num_tries += 1\n\n        pygame.display.flip()\n\n        # save last 4 frames\n        self.frames.append(pygame.surfarray.array2d(self.screen))\n\n        if self.num_tries >= self.MAX_TRIES_PER_GAME:\n            self.game_over = True\n\n        self.clock.tick(30)\n        return np.array(list(self.frames)), self.reward, self.game_over\n\n```", "```py\nfrom __future__ import division, print_function\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dense, Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.optimizers import Adam\nfrom scipy.misc import imresize\nimport collections\nimport numpy as np\nimport os\n\nimport wrapped_game\n\n```", "```py\ndef preprocess_images(images):\n    if images.shape[0] < 4:\n        # single image\n        x_t = images[0]\n        x_t = imresize(x_t, (80, 80))\n        x_t = x_t.astype(\"float\")\n        x_t /= 255.0\n        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n    else:\n        # 4 images\n        xt_list = []\n        for i in range(images.shape[0]):\n            x_t = imresize(images[i], (80, 80))\n            x_t = x_t.astype(\"float\")\n            x_t /= 255.0\n            xt_list.append(x_t)\n        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), \n                       axis=2)\n    s_t = np.expand_dims(s_t, axis=0)\n    return s_t\n\ndef get_next_batch(experience, model, num_actions, gamma, batch_size):\n    batch_indices = np.random.randint(low=0, high=len(experience), \n        size=batch_size)\n    batch = [experience[i] for i in batch_indices]\n    X = np.zeros((batch_size, 80, 80, 4))\n    Y = np.zeros((batch_size, num_actions))\n    for i in range(len(batch)):\n        s_t, a_t, r_t, s_tp1, game_over = batch[i]\n        X[i] = s_t\n        Y[i] = model.predict(s_t)[0]\n        Q_sa = np.max(model.predict(s_tp1)[0])\n        if game_over:\n            Y[i, a_t] = r_t\n        else:\n            Y[i, a_t] = r_t + gamma * Q_sa\n    return X, Y\n\n```", "```py\n# build the model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=8, strides=4, \n                 kernel_initializer=\"normal\", \n                 padding=\"same\",\n                 input_shape=(80, 80, 4)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(64, kernel_size=4, strides=2, \n                 kernel_initializer=\"normal\", \n                 padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(64, kernel_size=3, strides=1,\n                 kernel_initializer=\"normal\",\n                 padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(Flatten())\nmodel.add(Dense(512, kernel_initializer=\"normal\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(3, kernel_initializer=\"normal\"))\n\n```", "```py\nmodel.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")\n\n```", "```py\n# initialize parameters\nDATA_DIR = \"../data\"\nNUM_ACTIONS = 3 # number of valid actions (left, stay, right)\nGAMMA = 0.99 # decay rate of past observations\nINITIAL_EPSILON = 0.1 # starting value of epsilon\nFINAL_EPSILON = 0.0001 # final value of epsilon\nMEMORY_SIZE = 50000 # number of previous transitions to remember\nNUM_EPOCHS_OBSERVE = 100\nNUM_EPOCHS_TRAIN = 2000\n\nBATCH_SIZE = 32\nNUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN\n\n```", "```py\ngame = wrapped_game.MyWrappedGame()\nexperience = collections.deque(maxlen=MEMORY_SIZE)\n\nfout = open(os.path.join(DATA_DIR, \"rl-network-results.tsv\"), \"wb\")\nnum_games, num_wins = 0, 0\nepsilon = INITIAL_EPSILON\n\n```", "```py\nfor e in range(NUM_EPOCHS):\n    game.reset() \n    loss = 0.0\n\n    # get first state\n    a_0 = 1 # (0 = left, 1 = stay, 2 = right)\n    x_t, r_0, game_over = game.step(a_0) \n    s_t = preprocess_images(x_t)\n\n```", "```py\n    while not game_over:\n        s_tm1 = s_t\n\n        # next action\n        if e <= NUM_EPOCHS_OBSERVE:\n            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n        else:\n            if np.random.rand() <= epsilon:\n                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n            else:\n                q = model.predict(s_t)[0]\n                a_t = np.argmax(q)\n\n```", "```py\n        # apply action, get reward\n        x_t, r_t, game_over = game.step(a_t)\n        s_t = preprocess_images(x_t)\n        # if reward, increment num_wins\n        if r_t == 1:\n            num_wins += 1\n        # store experience\n        experience.append((s_tm1, a_t, r_t, s_t, game_over))\n\n```", "```py\n        if e > NUM_EPOCHS_OBSERVE:\n            # finished observing, now start training\n            # get next batch\n            X, Y = get_next_batch(experience, model, NUM_ACTIONS, GAMMA, BATCH_SIZE)\n            loss += model.train_on_batch(X, Y)\n\n```", "```py\n    # reduce epsilon gradually\n    if epsilon > FINAL_EPSILON:\n        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS\n\n```", "```py\n    print(\"Epoch {:04d}/{:d} | Loss {:.5f} | Win Count {:d}\"\n        .format(e + 1, NUM_EPOCHS, loss, num_wins))\n    fout.write(\"{:04d}t{:.5f}t{:d}n\".format(e + 1, loss, num_wins))\n\n    if e % 100 == 0:\n        model.save(os.path.join(DATA_DIR, \"rl-network.h5\"), overwrite=True)\n\nfout.close()\nmodel.save(os.path.join(DATA_DIR, \"rl-network.h5\"), overwrite=True)\n\n```", "```py\nfrom __future__ import division, print_function\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom scipy.misc import imresize\nimport numpy as np\nimport os\nimport wrapped_game\n\n```", "```py\nDATA_DIR = \"../data\"\nmodel = load_model(os.path.join(DATA_DIR, \"rl-network.h5\"))\nmodel.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")\n\ngame = wrapped_game.MyWrappedGame()\n\n```", "```py\nnum_games, num_wins = 0, 0\nfor e in range(100):\n    game.reset()\n\n    # get first state\n    a_0 = 1 # (0 = left, 1 = stay, 2 = right)\n    x_t, r_0, game_over = game.step(a_0) \n    s_t = preprocess_images(x_t)\n\n    while not game_over:\n        s_tm1 = s_t\n        # next action\n        q = model.predict(s_t)[0]\n        a_t = np.argmax(q)\n        # apply action, get reward\n        x_t, r_t, game_over = game.step(a_t)\n        s_t = preprocess_images(x_t)\n        # if reward, increment num_wins\n        if r_t == 1:\n            num_wins += 1\n\n    num_games += 1\n    print(\"Game: {:03d}, Wins: {:03d}\".format(num_games, num_wins), end=\"r\")\nprint(\"\")\n\n```"]