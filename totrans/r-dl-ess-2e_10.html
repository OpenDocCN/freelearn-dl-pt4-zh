<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Running Deep Learning Models in the Cloud</h1>
                </header>
            
            <article>
                
<p class="mce-root">Up till now, we have only briefly discussed the hardware requirements for training deep learning models, as almost all of the examples in this book run on any modern computer. While you do not need a <strong>GPU</strong> <span>(</span><strong>Graphical Processing Unit</strong><span>) </span>based computer to run the examples in this book, there is no getting away from the fact that training complicated deep learning models requires a computer with a GPU. Even if you have a suitable GPU on your machine, installing the necessary software to train deep learning models using GPUs is not a trivial task. This section will briefly discuss how to install the necessary software to run deep learning models on GPUs and also discusses the advantages and disadvantages of using cloud computing for deep learning. We will use various cloud providers to create virtual instances or access services that will allow us to train <span>deep learning models in the cloud.</span></p>
<p>This chapter covers the following topics:</p>
<ul>
<li>Setting up a local computer for deep learning</li>
<li>Using Amazon Web Services (AWS) for deep learning</li>
<li>Using Azure for deep learning</li>
<li>Using Google Cloud for deep learning</li>
<li>Using Paperspace for deep learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a local computer for deep learning</h1>
                </header>
            
            <article>
                
<p>At the time of writing this book, it is possible to purchase <span>a computer with a GPU card suitable for deep learning for under $1,000. </span>The current on-demand cost of the cheapest GPU computer on AWS is $0.90 per hour, which is equivalent to using the machine constantly for 46 days. So, if you are just starting with deep learning, cloud resources are the cheapest way to begin. Once you have learned the basics, then you may decide to get a GPU-based computer, but even then you may continue using cloud resources for deep learning. You have much more flexibility in the cloud. For example, <span>in AWS, you can get a p3.16xlarge machine with 8 Tesla V100 GPU cards for an on-demand price of $24.48 per hour. An equivalent box is the DGX-1 from NVIDIA (<a href="https://www.nvidia.com/en-us/data-center/dgx-1/">https://www.nvidia.com/en-us/data-center/dgx-1/</a>), which has 8 Tesla V100 GPU cards and costs $149,000!</span></p>
<p><span>If you are considering using your own computer for deep learning, then one of the following applies to you:</span></p>
<ul>
<li>You already have a computer that you can use with a suitable GPU processor</li>
<li>You will buy a computer to build deep learning models</li>
<li>You will build a <span>computer to build deep learning models</span></li>
</ul>
<p><span>If you want to use your local computer for deep learning, you need a suitable GPU card, which must be from NVIDIA. </span>The best way to check this is to go to the NVIDIA site and check if your graphics card is compatible with CUDA. CUDA is an application programming interface (API) that allows programs to use GPU for computing. You need to install CUDA to be able to use the GPU for deep learning. The current link to check if your graphics card is compatible with CUDA is <a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a>.</p>
<p>While some companies sell machines that are designed specifically for deep learning, they are very expensive. I would not advise getting one of them if you are just beginning to, learn deep learning. Instead, I would recommend looking at buying a computer that is for high-end computer games<span>. This computer should have an appropriate GPU card for deep learning. Again, check that the card is compatible with CUDA (<a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a>) first.</span></p>
<div class="packt_infobox">A gaming computer for deep learning? It is not as strange as it seems. GPUs were developed to play high-end games on computers, not for deep learning. But a machine that is designed for games is likely to have a higher than usual specification, for example, an SSD drive, lots of (fast) RAM, and most importantly a GPU card. Early deep learning practitioners realized that the matrix operations involved in calculating 3D spaces were very similar to the matrix operations used in neural networks. NVIDIA<span> released CUDA as an API so that other applications could use the GPU as a co-processor. </span>Whether it was luck or foresight, NVIDIA became the de facto standard for GPU cards for deep learning and has seen its share price grow by 10 times in the past 3 years, largely because of the huge demand for GPU cards for artificial intelligence.</div>
<p>The third option is to build your own deep learning computer. If you are considering this option, then other than the GPU card, memory, and an SSD drive, you will also need to consider the power supply and the motherboard. You may need a bigger capacity power supply than what is in a standard computer because of the GPU card and fans. For the motherboard, you need to consider if the hardware interface between the motherboard and the GPU card may limit the data transfer <span>– </span>these are <span>PCIe lanes. A GPU can use 16 PCIe lanes at full capacity. For expansion purposes, you may want a motherboard that supports 40 PCIe lanes so that you can support two GPU cards and an SSD drive simultaneously.</span></p>
<p>Before we move on to the rest of this chapter which discusses using cloud computing for deep learning, we should briefly discuss the performance of GPU cards in the cloud against what was used for this book. For this book, I used a GTX 1050 Ti which has 768 cores and 4 GB RAM. In my experience, the performance of this card is about the same as a <strong>p2.xlarge</strong> instance on AWS. I checked this by running two models on a local CPU (i5 processor), local GPU (<span>GTX 1050 Ti), and AWS GPU (<strong>p2.xlarge</strong>). I ran the test on two models: the binary prediction task from <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>, <em>Training Deep Prediction Models</em>, and the LeNet convolutional neural network from <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>. Both of these models were built using MXNet, and ran for 50 epochs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99d7a3cf-186e-4501-86de-fc10925816ce.png" style="width:42.92em;height:34.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.1: Execution time in seconds for two deep learning networks on CPU, local GPU, and AWS GPU</div>
<p><span>On my local machine, running the deep learning model for the binary prediction task on the GPU is about </span>20% faster than running on the CPU, and the AWS GPU machine is approximately 13% faster than the local GPU. However, there is a much bigger difference when running convolutional <span>neural networks, training it on the local CPU is almost 16 times slower than training it on the local GPU. In turn, the AWS GPU is approximately 16% faster than the local GPU. These results are expected and mirror what I have seen in practice and other benchmarks on the web and show conclusively that for deep learning computer vision tasks, a GPU is a necessity. The GPU card on my local machine (GTX 1050 Ti) is probably the lowest specification GPU card you should use for deep learning. It currently costs under $200. As a comparison, a high-end GPU card (GTX 1080 Ti) has 3,584 cores and 11 GB of RAM, and currently costs approx $700. The GTX 1080 Ti is approximately 4-5 times faster than the GTX 1050 Ti.<br/></span></p>
<div class="packt_infobox">Why does the previous graph just look at AWS and not Azure, Google Cloud, and Paperspace? Why did I not benchmark all of them on performance and/or cost? I decided not to do so for a few reasons. Firstly, and most importantly, any recommendation would have been out of date after a few months—deep learning is very popular and the various cloud providers are changing their offerings and prices constantly. Another reason is that the examples in this book are relatively small and we are using the cheapest GPU instances. Therefore, any comparisons to production use cases would be misleading. Finally, when you are starting out, ease of use is probably more important than raw cost. All the examples in this book should run in under 1 hour in the cloud regardless of which provider you use, so arguing that one provider costs $0.55/hour and another costs $0.45<span>/hour </span>is not important.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How do I know if my model is training on a GPU?</h1>
                </header>
            
            <article>
                
<p>One question that many people starting in deep learning ask is, <em>how do I know if my model is training on a GPU?</em> Fortunately, whether you are using a cloud instance or your local machine, you can check if the deep learning model is being trained on the GPU or the CPU. There is a utility on the instance that shows the GPU's activity. In Linux, you can type in the following command:</p>
<pre><strong>watch -n0.5 nvidia-smi</strong></pre>
<p>In Windows, you can use the following command from a command prompt:</p>
<pre><strong>nvidia-smi -l 1</strong></pre>
<p>This will run a script that outputs diagnostic messages about the GPU on the computer. If your model is currently training on the GPU, the GPU utility will be high. In the following example, we can see that it is 75-78%. We can also see that the file called <kbd>rsession.exe</kbd> is using GPU memory. This confirms that the model is being trained on the GPU:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53b61b85-9ce9-40d7-bdbc-48764803c6df.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.2: nvidia-smi utility showing that the GPU card is at 75-85% utilization</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using AWS for deep learning</h1>
                </header>
            
            <article>
                
<p><strong>AWS</strong> is the biggest cloud provider, and so it deserves our attention. If you know how to use AWS and especially if you are familiar with spot requests, it can be a very cost-effective method to train complex deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to AWS</h1>
                </header>
            
            <article>
                
<p class="mce-root">This section gives you a brief introduction to how AWS works. It describes EC2, AMIs, and how to create a virtual machine in the cloud. This will not be an exhaustive introduction to AWS <span>–</span> there are plenty of tutorials online that will guide you.</p>
<p class="mce-root">AWS is a suite of cloud resources. Another term for it is <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>), as opposed to <strong>Software as a Service</strong> (<strong>SaaS</strong>) or <strong>Platform as a Service</strong> (<strong>PaaS</strong>). In IaaS, as opposed to SaaS or PaaS, you are supplied with i<span>nfrastructure (hardware), and it is up to you to use it as you wish. This includes installing software and managing security and networking, although AWS take care of some aspects of security and networking. AWS has many services, but for deep learning, the one you will use is EC2, which is a virtual computing environment so that you can launch instances (virtual computers). You can control these virtual computers either through web interfaces or by remote logging into them to run commands from the shell. When you launch an EC2 instance, you can select the operating system (Ubuntu, Linux, Windows, and so on) and the type of machine you want.</span></p>
<p class="mce-root"><span>You can also select to use an <strong>Amazon Machine Image</strong> (<strong>AMI</strong>), which has software applications and libraries pre-installed on it. This is a good choice for deep learning as it means that you can start an EC2 instance with the deep learning libraries already installed and jump straight into deep learning.</span></p>
<p class="mce-root"><span>One other service you should be familiar with is S3, which is a form of persistent storage. A very useful practice that I suggest you to adopt is to consider your virtual machines as temporary resources and to keep your data and interim results in S3. We will not discuss this in this chapter because it is an advanced topic.</span></p>
<p>In the previous section, we stated that the <span>current on-demand cost of the cheapest GPU computer on AWS is $0.90 per hour. <em>On-demand</em> is one way to use a virtual machine in AWS, but there are three different ways to rent a virtual machine in AWS:</span></p>
<ul>
<li><strong>On-demand instances</strong>: When you rent an instance as needed.</li>
<li><strong>Reserved instances</strong>: When you commit to renting the machine for a certain period of time (usually 1-3 years). This is about 50% cheaper than on-demand instances. However you are committed to paying for the resource for the <span>period of time.</span></li>
<li><strong>Spot instances</strong>: In order to deal with fluctuating demand, Amazon has <span>spare computing capacity most of the time</span>. You can bid for this unused capacity and, depending on the demand for that type of machine, you can usually get it cheaper than on-demand and reserved instances. However, once you have the machine, it is<span> not guaranteed that you will keep it as long as you need – if the demand for the computer goes up, you</span>r computer may be terminated.</li>
</ul>
<p>Reserve instances are not useful for deep learning. The cost of renting the cheapest GPU machine for 1 year would be over $5,000, and you can buy a deep learning machine with better performance for much less. On-demand instances guarantee that you will have the resource as long as needed, but are expensive. Spot instances are an interesting and cost-effective method to use if you know how to use them correctly and plan for the chance that your computer will be terminated.</p>
<p>Typically, the spot price is about 30% of the on-demand price, so the savings are significant. Your bid is the maximum amount that you are willing to pay for the spot instances, the actual price depends on the market price, which is based on the demand.</p>
<p>Therefore, you should set your bid price higher; I recommend to set it at either 51%, 76%, or 101% of the on-demand price. The extra 1% from 50%, 75%, and 100% is because, similar to any bidding market, humans anchor their bids to round numbers, so by avoiding this with an extra <span>1%, it </span>can make a difference.</p>
<div class="packt_infobox">The original use case for spot instances was for low-priority batch jobs. Companies used <span>spot instances to the avail of themselves cheaper computing resources for long-running jobs that could be restarted in the event that they did not finish. An example might be running a secondary data ingestion process on data that was not critical to operations. However, the demand pattern for GPU based instances is different, possibly because of online data mining competitions such as Kaggle. Because GPU instances are not very common, demand spikes much more for GPU instances. This has led to some strange behavior in spot pricing, where the price that people bid for a spot instance can be 10x that of the on-demand price. People do this because they believe that this makes it unlikely that they will be outbid. There are cases where a p2.16xlarge has had a spot price of $144 per hour while the on-demand price is $14.40. The people who set these bids do not want their machines to be terminated and believe that they will still pay less on average for spot instances than on-demand instances. This is not something I would encourage if you use spot instances as you can get a very nasty surprise if demand goes up! However, you should be aware of this pricing quirk – do not think that setting the bid price to just above the on-demand price guarantees that your machine will not be terminated.</span></div>
<p>AWS provides you help in setting up your spot request bid by providing pricing history charts that advise you on the on-demand price and the bid prices. In the following screenshot, we can see how the price has changed <span>over the past 3 months for </span>a particular region (us-east). There are 6 availability zones (us-east-1a to <span>us-east-1f) and the current spot price of this instance type (<strong>p2.16xlarge</strong>) varies from $4.32-$14.40, while the on-demand price is $14.40:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2acf192-d7df-49ce-9267-f6ba038f9a7d.png" style="width:75.08em;height:45.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.3: Pricing history for spot bids for p2.16xlarge instance type</span></div>
<p>Looking at the preceding graph for this resource, I would consider the following:</p>
<ul>
<li>I would use the availability zone <strong>us-east-1a</strong> if possible, as it has the lowest price volatility.</li>
<li>I would set the price to $7.21 per hour, which is just over 50% of the on-demand price. I would probably only pay $4.32 per hour as it has been 1 month since the bid price in <span><span>us-east-1a has gone over $4.32 per hour. Setting it at the higher price would make it less likely that my spot instance would be terminated.</span></span></li>
</ul>
<div class="packt_tip"><strong>Regions and <span>availability zones:</span></strong> AWS arranges its services in regions (<strong>us-east1</strong>, <strong>eu-west1</strong>, and so on). Currently, there are 18 different regions and in each region, there are multiple availability zones, which you can consider as physical data centers. For some use cases (for example, websites, disaster recovery, and so on) and regulatory requirements, <span>regions and </span><span>availability zones</span> are important<span>. For deep learning, they are not so important, as you can usually run your deep learning models at any location. The bid price for spot instances is different for regions/availability zones, and some resources are more expensive in some regions. You also need to be aware that there is a cost in transferring data between regions, so keep your data and instances in the same region.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a deep learning GPU instance in AWS</h1>
                </header>
            
            <article>
                
<p class="mce-root">This section will use AWS to <span>train a deep learning model</span> from <a href="e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml">Chapter 9</a>, <em>Anomaly Detection and Recommendation Systems</em>. This will include setting up the machine, accessing the machine, downloading the data, and running the model. We are going to use a pre-built AWS AMI from RStudio that has TensorFlow and Keras already installed. For details on this AMI, go to this link: <a href="https://aws.amazon.com/marketplace/pp/B0785SXYB2">https://aws.amazon.com/marketplace/pp/B0785SXYB2</a>. <span>You will need to sign up for an AWS account if you do not already have one at </span><a href="https://portal.aws.amazon.com/billing/signup">https://portal.aws.amazon.com/billing/signup</a><span>. Once you have signed up, follow these steps to create a virtual machine that has a GPU on AWS:</span></p>
<div class="packt_infobox">Note that when you set up an instance in AWS, you will be billed for as long as it is running! Always ensure that you shut down your instances, otherwise you will continue to be charged. Check the AWS console to ensure you have no running instances when you are finished using the virtual instance.</div>
<ol>
<li class="mce-root"><span>Log in to the AWS console and select EC2. You should see a screen similar to the following. This is the web interface for creating new virtual machines:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aba67547-2b69-4a67-b9c5-a36d4b11a68b.png" style="width:62.67em;height:38.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.4: AWS EC2 dashboard</span></div>
<ol start="2">
<li>Click on the launch instance button and the following page will load.</li>
<li>C<span>lick <strong>AWS Marketplace</strong> on the left and in the search box type <kbd>rstudio</kbd> (see the following screenshot).</span></li>
<li><span>Select <strong>RStudio Server with Tensorflow-GPU for AWS</strong>. Be aware that there is another option with the word <strong>Pro </strong>– this is a paid subscription with additional costs, so do not select this AMI:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aafcaf7a-2869-471c-acce-f673585f8536.png" style="width:62.50em;height:38.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.5: AWS launch instance wizard, Step 1</span></div>
<ol start="5">
<li>Once you click <strong>Select</strong>, the following screen may appear with some additional information on accessing the instance. R<span>ead the instructions carefully, as they may have changed from what's shown in the screenshot that follows:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d2ec8bac-a577-4bab-895c-35e7461b5a64.png" style="width:62.42em;height:38.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 10.6: RStudio AMI information</span></div>
<ol start="6">
<li>When <span>you click <strong>Continue</strong>, the following screen will appear for the machine type. It is vital to select a machine that has a GPU, so from the <strong>Filter by:</strong> option, select GPU compute and then select <strong>p2.xlarge</strong> from the list. Your options should look similar to the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/ed92abfe-bcc6-468b-98f1-e7c5c15ce603.png" style="width:62.42em;height:38.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.7: AWS launch instance wizard, Step 2</span></div>
<ol start="7">
<li><span>When you click <span class="packt_screen">Next</span>, you will get to the following screen with various configuration options. The default options are OK, so just press</span> <span class="packt_screen">Next</span> <span>again:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fd0097b8-c8c5-4039-ae0e-3eddc27cf31b.png" style="width:62.42em;height:38.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.8: AWS launch instance wizard, Step 3</span></div>
<ol start="8">
<li>This screen allows you to change the storage options. You may need to add additional storage depending on the size of the data. Storage is relatively cheap, so I recommend going with 3x-5x the size of the input data.</li>
<li>Click <strong>Next</strong> to go to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/286b5c73-eb73-4ac7-a270-878f3d9a958b.png" style="width:62.33em;height:38.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.9: AWS launch instance wizard, Step 4</span></div>
<ol start="10">
<li>The following screen is not important – tags are used to keep track of resources in AWS, but we do not need them. Click Next to go to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e25e132f-c0dd-4f66-b764-9cfa0554e48c.png" style="width:62.50em;height:40.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.10: AWS launch instance wizard, step 5</span></div>
<ol start="11">
<li><span>T</span>he following screenshot shows security options. AWS <span>restricts access to instances, so you must open any needed ports. The defaults provided here allow access to port <kbd>22</kbd> (SSH) to access the shell and also for port <kbd>8787</kbd>, which is the web port that RStudio uses. Click</span> <span class="packt_screen">Review and Launch </span><span>to continue:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cd54d2b2-35c6-45d5-9046-24345d306457.png" style="width:62.50em;height:38.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.11: AWS launch instance wizard, Step 6</span></div>
<p style="padding-left: 60px">The following screenshot will appear. Note the warning messages regarding security <span>–</span> in a production environment, you would probably want to address these.</p>
<ol start="12">
<li>Click the <span><strong>Launch</strong> button to continue:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/157480e8-9d0b-44f5-b32f-6f5422d462e3.png" style="width:62.33em;height:38.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.12: AWS launch instance wizard, Step 7</span></div>
<ol start="13">
<li>You will be asked for a key pair. If you have not already created a key pair, then select the option to do so. Give it a descriptive name and press the <span class="packt_screen">Download Key Pair </span>button. Then, click on <span class="packt_screen">Launch Instances</span>:</li>
</ol>
<div class="packt_tip"><span>A key pair is used to access the instance using SSH. You should guard this very carefully, as if someone manages to get your private key, then they will be able to log in to any of your instances. You should delete your key pair occasionally and create a new one.</span></div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ef21c0ea-5bf6-4899-9adc-fb34f76686bc.png" style="width:62.42em;height:38.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.13: AWS launch instance wizard, select key pair</span></div>
<ol start="14">
<li>Once you have completed this, you can go back to the EC2 dashboard and you will see that you have <span class="packt_screen">1 Running Instances</span>. Click on that link to move on to the details of the instance:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/34bf01f9-062b-498b-85da-c69914e6fcdd.png" style="width:62.58em;height:38.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.14: AWS EC2 dashboard</span></div>
<ol start="15">
<li>Here, you will see the details of the instance. In this case, the IP address is <kbd>34.227.109.123</kbd>. Also note down the instance ID that is highlighted, as this is the password that is used to connect to the RStudio instance:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/27feac10-e148-43ce-8972-9e7784921555.png" style="width:62.58em;height:38.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.15: AWS EC2 dashboard, instance details</span></div>
<ol start="16">
<li class="mce-root">Open another web page and browse to the IP address of your machine and add <kbd>:8787</kbd> to access the link. In my example, the link is <kbd>http://<span>34.227.109.123</span>:8787/</kbd>. Instructions for logging in are in <em>Figure 10.6</em>, that is, use rstudio-user as the username and the instance ID as the password. You should also consider changing the password as per the instructions.</li>
</ol>
<ol start="17">
<li>When you log in, you will see a familiar interface <span>– </span>it is similar to the RStudio desktop program. One difference you have is the <strong>Upload</strong> button on the bottom-right pane, which allows you to upload files. In the following example, I have uploaded the data and the script from <a href="e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml">Chapter 9</a>, <span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Anomaly Detection and Recommendation Systems</em>, </span></span>for the <span>Keras </span>recommender example and ran it successfully:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/dc6290f6-e4d3-47ae-b27d-9f9571921105.png" style="font-size: 1em;width:62.33em;height:38.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.16: Accessing deep learning instance in the cloud using RStudio Server</span></div>
<p><span>The web interface in RStudio is similar to using RStudio on your local computer. In <em>Figure 10.16</em>, </span>you can see data files that I have uploaded (<kbd>recomend.csv</kbd>, <span><kbd>recomend40.csv</kbd>) and the R script in the <span class="packt_screen">Files</span> in the bottom-left window. We can also see the code that was executed in the Console window in the bottom-left.</span></p>
<p class="mce-root">This finishes our example on how to set up a deep learning machine in AWS. Again, remember that you will be billed for as long as the computer is running. Ensure that <span>your instances are terminated, otherwise you will continue to be charged. To do so, go back to the EC2 dashboard, find the instance, and click on the <strong>Actions</strong> button. A pop-up menu will appear, where you can select <strong>Instance State</strong> and then select <strong>Terminate</strong>:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/3d35019f-f470-4a97-86eb-31bb1b1048d4.png" style="width:62.50em;height:40.58em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span><span>Figure 10.17: Terminating the AWS instance</span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a deep learning AMI in AWS</h1>
                </header>
            
            <article>
                
<p><span>In the previous example, we used an <strong>Amazon Machine Image</strong></span> (<span><strong>AMI</strong>) that was built by RStudio. In AWS, you can also create your own AMI's. When you create an AMI, you can install the software you want, load data onto it, and set it up as you wish. This section will show you how to use an AMI to use MXNet on AWS.</span></p>
<ol>
<li><span>The first step in creating an AMI is to select the base image that you are going to use. We could start with a base image that just has the operating system installed, but instead we are going to use the <strong>RStudio Server with Tensorflow-GPU for AWS</strong> that we used previously and add the MXNet package to it.</span></li>
<li><span>The instructions to install MXNet were adapted from <a href="https://mxnet.incubator.apache.org/install/index.html">https://mxnet.incubator.apache.org/install/index.html</a>. The first step is to create the instance from the <strong>RStudio Server with Tensorflow-GPU for AWS</strong> AMI as per the previous section.</span></li>
<li><span>Once you have done this, you need to SSH into the machine. How you do this depends on the operating system on your own computer. For Linux and macOS, you can execute a local command on the shell, and in Windows you can use Putty.</span></li>
<li><span>Once you have logged in to the machine, run the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>vi ~/.profile</strong></pre>
<ol start="5">
<li>Add the following line to the end of this file and save the file:</li>
</ol>
<pre style="padding-left: 60px"><strong>export CUDA_HOME=/usr/local/cuda</strong></pre>
<ol start="6">
<li>Once you are back at the shell, run the following lines one by one:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt-get update</strong><br/><strong>sudo dpkg --configure -a</strong><br/><strong>sudo apt-get install -y build-essential git</strong><br/><strong>export CUDA_HOME=/usr/local/cuda</strong><br/><strong>git clone --recursive https://github.com/apache/incubator-mxnet</strong><br/><strong>cd incubator-mxnet</strong><br/><strong>make -j $(nproc) USE_OPENCV=1 USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1</strong></pre>
<ol start="7">
<li>The last command could take up to 2 hours to complete. Once it is done, run the last few lines:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo ldconfig /usr/local/cuda/lib64</strong><br/><strong>sudo make rpkg</strong><br/><strong>sudo R CMD INSTALL mxnet_current_r.tar.gz</strong></pre>
<p style="padding-left: 60px">The second line may take up to 30 minutes. The final line may return a warning about a missing file, which can be ignored.</p>
<ol start="8">
<li>To test if everything installed correctly, go to the RStudio page for the instance and type in the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong><span class="kn">library</span><span class="p">(</span>mxnet<span class="p">)</span>
a <span class="o">&lt;-</span> mx.nd.ones<span class="p">(</span><span class="kt">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> ctx <span class="o">=</span> mx.gpu<span class="p">())</span>
b <span class="o">&lt;-</span> a <span class="o">*</span> <span class="m">2</span> <span class="o">+</span> <span class="m">1</span>
b</strong></pre>
<ol start="9">
<li>You should get the following output:</li>
</ol>
<pre style="padding-left: 60px"><strong>     [,1] [,2] [,3]</strong><br/><strong>[1,]    3    3    3</strong><br/><strong>[2,]    3    3    3</strong></pre>
<ol start="10">
<li>Now go back to the EC2 dashboard, click on <strong>Running Instances</strong>, and select the machine in the list. Click on the <strong>Action</strong> button, select <strong>Image</strong> from the drop-down menu, and select <strong>Create Image</strong>. This is shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-711 image-border" src="assets/6e79acde-0322-4ea8-bd54-af931a923b29.png" style="width:44.25em;height:28.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.18: Creating an AMI</span></div>
<ol start="11">
<li>The image may take 15-20 minutes to complete. When it is done, click on AMIs on the left menu selection to show the list of AMIs associated with your account. You should see the AMI you just created. This AMI can then be used to create a new on-demand instance or a new spot instance. The following screenshot shows the menu option to create a spot instance for the AMI:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-712 image-border" src="assets/1a09ca79-e660-464f-84bc-71adffd561a9.png" style="width:72.83em;height:46.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.19: Using an existing AMI for a spot request</span></div>
<p>This AMI is now available so that you can create new deep learning instances. You should be aware that there is an ongoing cost in storing the AMI, even if you do not use it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Azure for deep learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Azure is the brand name for Microsoft's cloud services. You can use Azure for deep learning and, similar to AWS, it provides deep learning virtual machines that is pre-configured with deep learning libraries installed. In this example, we are going to create a Windows instance that can be used for Keras or MXNet. This assumes that your local computer is also a Windows computer, as you will be using <strong>Remote Desktop Protocol</strong> (<strong>RDP</strong>) to access the cloud instance.</p>
<ol>
<li class="mce-root">The first step is to create an account in Azure and then log in to Azure at <a href="https://portal.azure.com">https://portal.azure.com</a>. You will see a screenshot similar to the following. <span>Click <strong>Create a resource</strong> and search for <strong>Deep Learning Virtual Machine</strong>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-713 image-border" src="assets/35fda232-a7e7-4c1c-99e6-a719b99d6798.png" style="width:96.92em;height:51.50em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.20: Azure portal website</span></div>
<ol start="2">
<li class="mce-root">When you select <strong>Deep Learning Virtual Machine</strong>, the following screen will appear. Click <strong>Create</strong>:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/865011d0-1c63-4727-913d-39a08bc61ffd.png"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.21: Provisioning a deep learning instance on Azure, step 0</span></div>
<ol start="3">
<li class="mce-root">You will now start a 4-step wizard to create the new instance.</li>
</ol>
<p style="padding-left: 90px">The first step (Basics), asks for some basic details. It is OK to enter the same values as I have done, but fill in the username and password carefully as you will need them later:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6b977cee-8553-449a-b482-efa7f5031dec.png"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.22: Provisioning a deep learning instance on Azure, step 1</span></div>
<p style="padding-left: 90px">For Step 2 (Settings), ensure that the virtual machine size is 1 x Standard NC6 (1 GPU), and click <strong>OK</strong> to continue:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/4a27d0e4-b492-4566-8383-1d37646b48c3.png"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.23: Provisioning a deep learning instance on Azure, step 2</span></div>
<p style="padding-left: 90px">For Step 3 (Summary), there is a brief validation check. You may be told that your account does not have sufficient Compute/VM (cores/vCPUs) resources available, which is because Microsoft may have restricted your account when it was first created. Create a support ticket to increase your resources and try again. If you have passed this step, click <strong>OK</strong> to continue. You are now on the final step, so just <span>click <strong>Create</strong>:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/55fb8721-62c1-4693-a074-b0b9c9a76f10.png"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.24: Provisioning a deep learning instance on Azure, Step 4</span></div>
<p class="mce-root" style="padding-left: 90px">You may have to wait 30-40 minutes until the resources are created. When this is complete, select <strong>All resources</strong> on the left and you will see that all of the objects have been created. The following screenshot shows an example of this. Click on the one where the type is <strong>Virtual Machine</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b96b0b9e-f5db-449d-846f-c1013f799bd9.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.25: List of currently provisioned resources on Azure</span></div>
<ol start="4">
<li>You will then see the following screenshot. Click on the <strong>Connect</strong> button on the top of the screen.</li>
<li>A pane will open up on the right and give you an option to <strong>Download RDP File</strong>. Click on that, and when the file is downloaded, double-click on it:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f60bb8b-8e00-4906-b815-9940b5a28833.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.26: Downloading the RDP file to connect to the cloud instance in Azure</span></div>
<ol start="6">
<li>This should bring up a login window to connect to the cloud instance. Enter the username and password that you created in step 1 to connect to the instance. When you connect, you will see a desktop similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a0697b47-5d20-4e0c-b5f5-9f775c2e5cb5.png" style="width:57.92em;height:32.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.27: The remote desktop of the deep learning instance (Azure) </span></div>
<p>Great! RStudio is already installed. Keras is already installed, so any Keras deep learning code you have will run. Let's try and run some MXNet code. Open RStudio and run the following commands to install MXNet:</p>
<pre>cran &lt;- getOption("repos")<br/>cran["dmlc"] &lt;- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/GPU/cu90"<br/>options(repos = cran)<br/>install.packages("mxnet")<br/><br/># validate install<br/>library(mxnet)<br/>a &lt;- mx.nd.ones(c(2,3), ctx = mx.gpu())<br/>b &lt;- a * 2 + 1<br/>b</pre>
<p>This will not work on the version of R installed. If you want to use MXNet, you must download the latest version of R (3.5.1 at the time of writing) and install it. <span>Unfortunately, this will disable Keras, so only do this if you want to use MXNet instead of Keras. Once you download R from https://cran.r-project.org/, then re-run the code above to install MXNet.</span></p>
<div class="packt_infobox">Note: The software installed on these AMI's change very frequently. Before installing any deep learning library, check the version of CUDA that is installed. You need to ensure that they deep learning library is compatible with the version of CUDA installed on the machine.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Google Cloud for deep learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Google Cloud also has GPU instances. At the time of writing this book, the price of an instance with an NVIDIA Tesla K80 GPU card (which is also the GPU card in an AWS p2.xlarge instance) is $0.45 per hour on-demand. This is significantly cheaper than the AWS on-demand price. Further details of Google Cloud's GPU instances are at <a href="https://cloud.google.com/gpu/">https://cloud.google.com/gpu/</a>. However, for Google Cloud, we are not going to use instances. Instead, we are going to use the Google Cloud Machine Learning Engine API to submit machine learning jobs to the cloud. One big advantage of this approach over provisioning virtual machines is that you <span>only </span>pay for the hardware resources that you use and do not have to worry about setting up and terminating instances. More details and pricing can be found at <a href="https://cloud.google.com/ml-engine/pricing">https://cloud.google.com/ml-engine/pricing</a>.</p>
<p>Go through the following steps to sign up for Google Cloud and enable the API:</p>
<ol>
<li>Sign up for an account with Google Cloud.</li>
<li>You need to login to the portal at <a href="https://console.cloud.google.com">https://console.cloud.google.com</a> and enable the <strong>Cloud Machine Learning Engine</strong> API.</li>
<li>Select <strong>APIs &amp; Services</strong> from the main menu and click on the <strong>Enable APIs and services</strong> button.</li>
<li>The APIs are contained in groups. Select <strong>View All</strong> for the <strong>Machine Learning</strong> group, then select<span> </span><span><strong>Cloud Machine Learning Engine</strong> and ensure that the API is enabled.</span></li>
</ol>
<p>Once the API is enabled, execute the following code from <span>RStudio</span>:</p>
<pre>devtools::install_github("rstudio/cloudml")<br/>library(cloudml)<br/>gcloud_init()</pre>
<p>This should install the Google Cloud SDK, and you will asked to connect your Google account to the SDK. Then, you will be taken through a menu of options in the Terminal window. The first option is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/09b57ce1-eb63-4e33-a609-82a1d2fe641b.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.28: Accessing Google Cloud SDK from RStudio</span></div>
<p>For now, do not create any new projects or configurations, just select the ones that already exist. Once you have linked your Google account to the Google SDK on your machine and enabled the services, you are ready to go. The <span>Cloud Machine Learning Engine allows you to submit a job to Google Cloud without having to create any instances. All the files in the working folder (R scripts and data) will be zipped up and sent to Google Cloud as a package.</span></p>
<p>For this example, I took a the recommendation file from the project in <a href="49a1fa27-1130-4f86-966e-cc73444b88a2.xhtml">Chapter 8</a>, <em>Deep Learning models using TensorFlow in R</em>. I copied this file and the <kbd>keras_recommend.R</kbd> script into a new directory and created a new RStudio project in that directory. I then opened the project in RStudio. You can see these two files and the RStudio project file in the previous screenshot. Then, I executed the following line in RStudio to submit the deep learning job:</p>
<pre>cloudml_train("keras_recommend.R", master_type = "standard_gpu")</pre>
<p>This will collect the files within the current working directory and send them to the <span>Cloud Machine Learning Engine</span>. A<span>s the job is executed, some </span>progress information will be sent back to RStudio. You can also monitor the activity on the <span>console page on </span><a href="https://console.cloud.google.com">https://console.cloud.google.com</a> by selecting <strong>ML Engine</strong> | <strong>Jobs</strong>. Here is a screenshot of this web page showing two finished jobs and one that was canceled:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8519af01-4cd4-4c31-b2c2-3a1d7d03a2f7.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.29: The ML Engine/Jobs page on the Google Cloud Platform web page</span></div>
<p>When the job is finished, the logs will be downloaded to your local machine. A nice summary web page is automatically created showing statistics for the job, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cfd08ae4-afaa-4f7f-b71a-d07a60c1f1d8.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.30: Web summary page from the machine learning job</span></div>
<p>We can see the graph showing the progress of the model during training, the model summary, some hyperparameters (<strong>epochs</strong>, <strong>batch_size</strong>, and so on), as well as the cost (<strong>ml_units</strong>). The web page also contains the output from the R script. Select <strong>Output</strong> from the menu to see it. In the following screenshot, we can see the R code and the output from that code:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ccfca405-fa9f-4dd7-b102-fe39f624f402.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.31: Web summary page from the machine learning job showing the R code and output</span></div>
<p>This is only a brief introduction to using the Google <span>Cloud Machine Learning Engine</span>. There is an excellent tutorial at <a href="https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html">https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html</a> that explains how you can use this service for hyperparameter training. Using <span>this service rather than cloud instances </span><span>for hyperparameter training is simpler and probably cheaper than trying to manage</span><span> it yourself using virtual instances. You do not have to monitor it and coordinate the different runs of the model training.</span> More information on using this service is available at <a href="https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html">https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Paperspace for deep learning</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Paperspace</strong> is another interesting way to perform deep learning in the cloud. It might be the easiest way to train deep learning models in the cloud. To <span>set up a cloud instance w</span>ith Paperspace, you can log in to their console, <span>provision </span>a new machine, and connect to it from your web browser:</p>
<ol>
<li class="mce-root">Start by signing up for a Paperspace account, log in to the console, and go into the Virtual Machine section by selecting Core or Compute. Paperspace has an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0) already installed, along with the GPU version of TensorFlow and Keras for R. You will see this machine type when you select <span><strong>Public Templates</strong>, as shown in the following screenshot:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e81d0c7e-1252-4cc6-9f55-da9d48ab433f.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.32: Paperspace portal</span></div>
<ol start="2">
<li>You will be given a choice of three GPU instances and the choice of pay by the hour or monthly. Select the cheapest option (currently P4000 at $0.40 per hour) and the hourly pricing. Scroll down <span>to the bottom of the page and press the <strong>Create</strong> button. After a few minutes, your machine will be provisioned and you will be able to access it through your browser. An example of an RStudio Paperspace instance is shown as follows:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5dac837c-ce96-4cea-ae66-14f848f46040.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 10.33: Accessing the virtual machine's desktop from a web page and running RStudio for a Paperspace instance</span></div>
<p>By default, Keras is already installed, so you can go ahead and train deep learning models using Keras. However, we are also going to install MXNet on our instance:</p>
<ol>
<li>The first step is to open RStudio and install a few packages. Execute the following commands from RStudio:</li>
</ol>
<pre style="padding-left: 60px">install.packages("devtools")<br/>install.packages(c("imager","DiagrammeR","influenceR","rgexf"))</pre>
<ol start="2">
<li>The next step is to access the Terminal (or shell) for the instance you just created. You can go back to the console page and do it from there. Alternatively, click on the circle target in the top right corner of the desktop (see the previous screenshot). This also gives you other options such as synchronizing copy-and-paste between your local computer and the VM.</li>
<li><span> </span>Once you have logged in to the Terminal for the instance, running the following commands will install MXNet:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo apt-get update</strong><br/><strong>sudo dpkg --configure -a</strong><br/><strong>sudo apt-get install -y build-essential git</strong><br/><strong>export CUDA_HOME=/usr/local/cuda</strong><br/><strong>git clone --recursive https://github.com/apache/incubator-mxnet</strong><br/><strong>cd incubator-mxnet</strong><br/><strong>make -j $(nproc) USE_OPENCV=1 USE_BLAS=blas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1</strong><br/><br/><strong>sudo ldconfig /usr/local/cuda/lib64</strong><br/><strong>sudo make rpkg</strong></pre>
<ol start="4">
<li>You also need to add the following line to the end of the <kbd>.profile</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>export CUDA_HOME=/usr/local/cuda</strong></pre>
<p><span>When you are done, restart the instance. You now have a machine that can train Keras and MXNet deep learning models in the cloud. For more details on using RStudio in Paperspace, see </span><span><a href="https://tensorflow.rstudio.com/tools/cloud_desktop_gpu.html">https://tensorflow.rstudio.com/tools/cloud_desktop_gpu.html</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have covered a lot of options for training deep learning models in this chapter! We discussed options for running it locally and showed the importance of having a GPU card. We used the three main cloud providers to train deep learning models in R on the cloud. Cloud computing is a fantastic resource <span>– </span>we gave an example of a super-computer costing $149,000. A few years ago, such a resource would have been out of reach for practically everyone, but now thanks to cloud computing, you can rent a machine like this on an hourly basis.</p>
<p>For AWS, Azure, and Paperspace, we <span>installed MXNet on the cloud </span>resources, giving us the option of which deep learning library to use. I encourage you to use the examples in the other chapters in this book and try all the different cloud providers here. It is amazing to think that you could do so and your total cost could be less than $10!</p>
<p>In the next and final chapter, we build an image classification solution from image files. We will demonstrate how to apply transfer learning, which allows you to adapt an existing model to a new dataset. We will show how to deploy a model to production using a REST API and briefly discuss Generative Adversarial Networks, reinforcement learning and provide some further resources if you wish to continue on your deep learning quest.</p>


            </article>

            
        </section>
    </body></html>