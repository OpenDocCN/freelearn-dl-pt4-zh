- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the Legal Landscape of LLM-Generated Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter examines and discusses the copyright law and regulations of LLM
    code, who owns what **intellectual property** ( **IP** ), who is liable, and how
    far the regulations should go. As with all chapters in this book, this chapter
    gives guidance on how to act. This chapter will help you to stay within the parameters
    of the law and not run into any legal issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’ll understand the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Unraveling copyright and intellectual property considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing liability and responsibility for LLM-generated code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining legal frameworks governing the use of LLMs in coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible future of the regulation of AI-generated code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After studying this chapter, you should be aware of possible issues arising
    from using code generated by an LLM, understand how to deal with legal considerations
    with the code, plan for the future, and create code that has futureproofing and
    innovative capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you may need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to an LLM/chatbot such as GPT-4 or Gemini; each requires logins. For
    GPT-4, you’d need an OpenAI account, and for Gemini, you’d need a Google account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we’ll get into the first subsection of the chapter, which talks about copyright
    and intellectual property laws.
  prefs: []
  type: TYPE_NORMAL
- en: Unraveling copyright and intellectual property considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s easy to use LLMs such as Devin, Gemini, and GPT-4o to quickly get some
    code to solve problems, but avoiding eliciting legal issues may be more difficult
    and needs careful examining.
  prefs: []
  type: TYPE_NORMAL
- en: This can be complex, not least because of the prevailing different jurisdictions
    around the world, and it partially rests on how similar one piece of work is to
    another. If you produce a piece of work that is similar enough to a preexisting
    piece of work, the creator of the preexisting work can claim that you infringed
    copyright law. Then the question becomes, “ *Is this second piece of work similar
    enough to the first piece of work to constitute a copy, derivative work, annotation,
    reproduction, translation, abridgement, condensation, dramatization, fictionalization
    or other recast, or adaptation or transformation of the preexisting work?* ” Currently,
    there has not been any universal legal framework to handle this kind of perplexity,
    as resolving this type of issue varies and differs across various jurisdictions,
    juries, and judges.
  prefs: []
  type: TYPE_NORMAL
- en: “ *There are only so many notes* ” is an argument that can be used for music,
    as there is literally a finite number of musical notes, but extending this to
    words in human language doesn’t really hold water at all.
  prefs: []
  type: TYPE_NORMAL
- en: However, it might be true for code, as code has to fit certain formats such
    as functions, classes, inputs, outputs, and statements such as **print** , **if**
    , **else** , **switch** , **while** , and so on [LawStackExchange].
  prefs: []
  type: TYPE_NORMAL
- en: That is not terribly clear or helpful and might make someone want to curl up
    into a ball and never write anything again! What is clearer is that copyright
    law usually protects works by humans, and LLMs are not human or legal entities
    and cannot hold copyrights.
  prefs: []
  type: TYPE_NORMAL
- en: The code is produced by the LLMs and comes from training data code, which was,
    presumably, mostly produced by humans (at least it was originally), so that original
    code can be subject to copyright.
  prefs: []
  type: TYPE_NORMAL
- en: Companies, including those owning LLMS, do have IP produced by employees and
    can own the rights to these words as “employee work.” Does this extend to LLM-generated
    works? The copyright protection needed or given to code from LLMs is debatable.
    The legal position on using copyrighted code is not clear and might be viewed
    differently in various jurisdictions (“fair use”) or require specific licensing
    [ *Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at individual jurisdictions in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: The EU – needs the human touch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the EU, things are not very clear-cut, but the law states that copyright
    will only subsist if there is originality flowing from the “ *author’s own* *intellectual
    creation.* ”
  prefs: []
  type: TYPE_NORMAL
- en: That is often interpreted as having significant human input into the work. The
    **Court of Justice of the European Union** ( **CJEU** ) has interpreted this to
    mean that the work must be original and reflect the author’s personality and expressive
    autonomy. This principle was decided through multiple court rulings by the CJEU
    [ *GPT-4o, Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: EU member states must still decide when and where the AI-generated work meets
    this requirement. In France, the law requires proof that “ *the personal touch
    or intellectual effort* ” is present to qualify for originality, and that “ *implementation
    of automatic and constraining logic* ” without “ *genuine personal effort* ” is
    not enough to mark the work as original. French copyright law (Code de la propriété
    intellectuelle) is decided through court decisions. French courts have consistently
    interpreted originality as requiring a “ *personal touch* ” or “ *intellectual
    effort* ” from the author. Articles L111-1 and following. [ *Cooley,* *GPT-4o,
    Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: In German copyright law ( **Urheberrechtsgesetz** ( **UrhG** ) or Copyright
    Act), it is required that a machine or computer program cannot be an author; only
    a natural human can be.
  prefs: []
  type: TYPE_NORMAL
- en: German courts have consistently interpreted the concept of “ *author* ” to require
    intellectual creation by a natural person. Machines or computer programs, on their
    own, haven’t been recognized as authors.
  prefs: []
  type: TYPE_NORMAL
- en: So, the EU seems to require a large proportion of all work to come from humans.
  prefs: []
  type: TYPE_NORMAL
- en: The UK – human creativity and arrangements necessary for the creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cooley says that UK law seems, on the surface, to be similar to EU law, in that
    there must be the touch and creativity of the human present in the work for it
    to be protected by copyright. That means that, if the creativity is all from the
    software, the work is not protected under UK law.
  prefs: []
  type: TYPE_NORMAL
- en: However, the UK law from the Copyright Designs and Patents Act 1988 states that
    the owner is the person who makes “ *arrangements necessary for the creation of
    the work,* ” (section 9(3)), even if it’s a computer program. It remains unclear
    if this means the person, group, or company that created the software or the user,
    such as a prompt engineer. The CDPA 1988 section 9(3) was upheld and endorsed
    by the UK government and UK Intellectual Property Office in 2022 specifically
    relating to generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: So, AI-generated work can be covered by copyright law in the UK, but it’s still
    not definitively resolved.
  prefs: []
  type: TYPE_NORMAL
- en: The USA – no ownership of AI-generated works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to [Cooley], in the USA, the law states that AI-generated works cannot
    be owned by anyone – not the AI, the creators of the AI, or the prompt writer/user.
    That makes these AI-generated works in the public domain, not under copyright.
    This extends to monkeys, as in the case of Naruto versus Slater – monkeys cannot
    be owners of photographs they took. As ruled by a US District Judge, copyright
    cannot be granted to (non-human) animals [ *Monkey_Business* ]. Most likely, the
    copyright owner was David Slater, the owner of the camera, but this is not fully
    resolved.
  prefs: []
  type: TYPE_NORMAL
- en: The People’s Republic of China – whoever made the greater contribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In China, there is also possible ownership and, therefore, copyright protection
    for AI-generated works. If a company works on an AI tool, including the settings,
    and the tool produces output, then the company can be found to be the owner of
    the works and copyright, such as the case of Shenzhen Tencent versus Shanghai
    Yingxun [ *Cooley* ], where Tencent was adjudged to be the owner.
  prefs: []
  type: TYPE_NORMAL
- en: However, where the user has input settings and originated the idea for the work,
    the copyright may go to the user, such as in the case of Stable Diffusion, even
    though Stable Diffusion developed the AI platform.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that ownership tends to go to the party that made the greatest contribution,
    be it the developer of the AI tool or platform or the user. Much like in other
    countries, this field of law is always evolving and not entirely resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Chinese law does work on a case-by-case basis [ *GPT-4o* ].
  prefs: []
  type: TYPE_NORMAL
- en: Taiwan – human creative expression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Taiwanese Intellectual Property Office, in the Shou-Zhi-11252800520 Circular
    of June 16, 2023, has stated that using copyrighted works to train AI models may
    infringe the copyright holder’s reproduction right; content lacking human creative
    expression is not protected by the Copyright Act. So, purely AI-generated works,
    with only commands from a human, are not protected by the Copyright Act.
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to European law but moving in the direction of the US law [
    *Lexology* *and GPT-4o* ].
  prefs: []
  type: TYPE_NORMAL
- en: India and Canada – human author’s skill and judgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In India, AI works can be covered by copyright law by meeting the originality
    requirement, but novelty is not relevant, as it’s too high a standard for copyright
    [ *Ind_Law&Tech* ]. The work must be the product of the author’s skill and judgement,
    and it must not be trivial enough that it is a solely mechanical exercise to qualify
    for copyright protection. The work cannot be produced solely by AI tools or software.
    This closely follows Canadian law where human skill and judgment is present. For
    Indian law, the amount of work required is where the work would be fundamentally
    different or non-existent without human intervention or action.
  prefs: []
  type: TYPE_NORMAL
- en: Australia – to the person making the necessary arrangements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Australian Copyright Act 1968 is similar to the UK act; authorship is attributed
    to the person making necessary arrangements for the creation of the works. There
    is still ambiguity here, and things are not clearly defined [ *GPT4-o* ].
  prefs: []
  type: TYPE_NORMAL
- en: Japan – copyright requires human authorship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, works produced by AI do not receive copyright protection and human
    input is required. The law is being adapted for the gen-AI age but doesn’t currently
    address Gen AI works [ *GPT4-o* *and Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: South Korea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Korean Copyright Act says that works purely from AI are not protected by
    copyright, but this might be different when there is significant human input.
    South Korea defines a “work” as expressing human thoughts and emotions, and an
    “author” as a person who creates the work [ *GPT4-o* *and Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Brazil – human authorship required
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is similar to the Republic of Korea, the USA and Japan. Brazilian copyright
    law grants protection to intellectual creations of a literary, artistic, or scientific
    nature. There’s some debate on whether AI-generated code qualifies due to the
    lack of a human author. [ *GPT4-o* *and Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Indonesia – human authorship needed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indonesia requires humans to be involved in the creation of works to be covered
    by copyright; it must be original and involve a substantial amount of creativity
    and intellectual investment. Purely AI-generated works might not be covered in
    Indonesia. The Intellectual Property Office hasn’t issued any specific rulings
    on AI-generated works and copyright [ *GPT-4o* *and Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Evolving legal landscape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing (2024), most countries are currently adapting and reacting
    to the new technology, updating their laws to deal with AI-generated code, art,
    writing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Precedent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve been here before, with photography. In 1884, in the USA, the Supreme Court
    ruled that photographers have the right to ownership of the photographs they take
    (with cameras) (Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53 ( 1884)).
    [ *Liu_Medium* ]
  prefs: []
  type: TYPE_NORMAL
- en: These are the IP considerations, but who is responsible and even liable for
    the code that comes from LLMs? This is what we’ll discuss in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing liability and responsibility for LLM-generated code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to know what we are liable for when presenting code from LLMs to the
    public – publishing and/or sharing it. What trouble could we get into, and how
    do we guard against this trouble?
  prefs: []
  type: TYPE_NORMAL
- en: According to Zerotolive.app, “ *Code is a liability because every line you write
    has an ongoing maintenance cost, indefinitely. As nearly any developer will attest,
    code is never “* *finished”.* ” [ *Zerotolive* ]
  prefs: []
  type: TYPE_NORMAL
- en: So, we’ve got a lot of work ahead of us.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by finding the main types of liability and responsibility from code
    and LLM-gen code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are various legal issues with AI-generated code:'
  prefs: []
  type: TYPE_NORMAL
- en: Licensing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribution and credit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality and reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product liability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following subsections, we will get into the details of these.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will look at what to do when things do go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Licensing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code from LLMs may need to be compatible with various open source licenses.
    Many software licenses have specific requirements that might not be automatically
    met by AI-generated code.
  prefs: []
  type: TYPE_NORMAL
- en: You need to understand and comply with the licensing agreements of the LLM used,
    as these often stipulate how the generated content can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Attribution and credit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the generated code is based on or influenced by existing work, proper attribution
    must be given to original authors to avoid plagiarism claims.
  prefs: []
  type: TYPE_NORMAL
- en: In cases involving theft, where AI could be generated to find, copy, and redistribute
    owned code with no permission given by the owners, things are less clear. If there
    is a lot of copying, then it may be more difficult to defend against claims of
    theft versus where there is a great deal of creativity added to the code claimed
    to be stolen.
  prefs: []
  type: TYPE_NORMAL
- en: If the AI was created with the *intention* of stealing copywritten code (or
    other materials), then the case may be clearer against the copiers.
  prefs: []
  type: TYPE_NORMAL
- en: Quality and reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The generated code might contain bugs or security vulnerabilities. The prompt
    engineer or developer getting the code from the LLM is responsible for reviewing
    and testing the code before deployment, ensuring that it meets quality and security
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: Potential misuse of inadequately vetted code can lead to legal liabilities,
    especially if it results in data breaches, financial losses, or other damages.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must always ensure that the generated code doesn’t perpetuate biases or lead
    to unfair practices. We must follow ethical programming practices to prevent harm.
  prefs: []
  type: TYPE_NORMAL
- en: Users and developers should be transparent about the fact that code was generated
    by an AI model, which can mitigate potential misunderstandings or misrepresentations.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Most or all of the code in this book is generated from LLMs such as Gemini/Bard
    and GPT-4. Do test all the code and prompts you get from this book or its GitHub
    repo. We do not accept liability for using the prompts and code or misusing them.
    The code and prompt examples in this book and the GitHub repo are meant as examples,
    allowing you to study how to use LLMs for coding and understanding code, and are
    not for production-ready code.
  prefs: []
  type: TYPE_NORMAL
- en: Product liability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If AI-generated code malfunctions, causing physical damage, data loss, or financial
    harm, there could be legal disputes regarding liability. Understanding the extent
    of legal protection or exposure is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Clarify any implied or explicit warranties regarding the functionality and safety
    of AI-generated code. Misrepresentation can lead to legal liabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Use case restrictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some LLM developers specifically prohibit certain uses of the generated content,
    such as military applications, illegal activities, and other sensitive environments.
    You, as the code user and publisher/sharer, must stick to these restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Certain industries (e.g., healthcare and finance) have specific regulations
    and standards. Make sure your AI-generated code complies with these sector-specific
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: These are liabilities, but what can be done when there are complaints and legal
    steps taken against you for your AI-gen code? Let’s discuss that.
  prefs: []
  type: TYPE_NORMAL
- en: Security concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI-generated code may accidentally introduce security vulnerabilities that could
    be exploited by malicious people. The user is responsible for conducting thorough
    security audits and implementing necessary safeguards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement best practices for secure coding, and consider using automated security
    analysis tools to detect potential issues in the AI-generated code. Examples include
    [https://sonarsource.com](https://sonarsource.com) and [https://www.synopsys.com](https://www.synopsys.com)
    . You can find out more here: [https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/](https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Transparency and explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI-generated code can sometimes be opaque, lack clear documentation, or even
    be “black box,” making it difficult to understand and maintain. Get your code
    fully reviewed, and document it well.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to understand and explain to others how your generated code
    functions, especially in critical applications where accountability is important.
  prefs: []
  type: TYPE_NORMAL
- en: This is where documenting everything is important; you can have this in place
    before it’s needed. Document intended use cases and users. Document the intended
    code ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: We covered how to create transparent and explainable code in [*Chapter 5*](B21009_05.xhtml#_idTextAnchor115)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Third-party dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When integrating AI-generated code with third-party software or services, ensure
    that all dependencies are secure and compliant with legal and licensing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Maintain an up-to-date inventory of all third-party components used by the AI-generated
    code, and monitor for security updates or legal changes.
  prefs: []
  type: TYPE_NORMAL
- en: Use good communication to avoid legal action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some measures you can undertake to ensure that you stay safe and also
    don’t cause trouble for your users:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User agreements and terms of service** : Clearly outline the terms of service
    and user agreements that govern the use of AI-generated code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Liability limitations** : Establish any limitations on the liability of the
    developers or platform providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User obligations** : Set out the responsibilities and obligations of users
    when utilizing generated code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indemnification** : Outline clauses related to indemnification, where users
    agree to hold developers or providers harmless in certain circumstances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback and improvement** : Establish channels through which users can provide
    feedback on generated code, report issues, and suggest improvement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code of ethics when using AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some more practices you can consider to make sure you’re as legally
    covered as you can be.
  prefs: []
  type: TYPE_NORMAL
- en: You can implement and adhere to a code of ethics that guides the development,
    deployment, and use of AI-generated code. This can help ensure that ethical considerations
    are consistently taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: You can also encourage transparency in how AI models work, including their training
    data and algorithms, to build trust with users and stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Social responsibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularly assess the societal impact of AI-generated code. This involves evaluating
    both the positive and negative effects on users and wider society.
  prefs: []
  type: TYPE_NORMAL
- en: Engage with stakeholders, including users, regulatory bodies, and advocacy groups,
    to understand and address ethical concerns related to AI-generated code.
  prefs: []
  type: TYPE_NORMAL
- en: Public awareness and education
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Educate users and the public about responsible use of AI-generated code. This
    can include best practices, potential risks, and the importance of scrutinizing
    AI output.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly communicate the capabilities, limitations, and potential impacts of
    AI-generated code to users and stakeholders. This transparency can foster trust
    and promote informed decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability and redress mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Redress is when you remedy, correct, or provide compensation for a wrong, injury,
    or injustice. It involves taking steps to make amends or address grievances, often
    through legal or formal mechanisms. In the context of legal and ethical responsibilities,
    redress mechanisms ensure that individuals or entities have the means to seek
    justice or compensation if they are harmed by an action, product, or service.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re developing LLMs that can generate code, you need to know how much
    you, as a developer or your organization, could be held accountable for the consequences
    of code generated by your models. Part of that involves establishing who is responsible
    if the code fails or causes harm.
  prefs: []
  type: TYPE_NORMAL
- en: For your organization, establish clear practices to handle errors or issues
    in the generated code. When something goes wrong, it’s important to have predefined
    processes to identify responsibility and address the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of redress mechanisms for developers who experience
    negative outcomes from using AI-generated code. This can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compensation** : Implementing policies to provide compensation to users affected
    by faulty or harmful code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal recourse** : Legal pathways are available to users to seek remediation
    or justice if AI-generated code causes damage. (Remediation is rectifying something.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support systems** : Developers or platform providers can offer support channels
    to help users resolve issues with generated code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dispute resolution** : Implementing mechanisms to resolve disputes that arise
    from the use of AI-generated code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mediation and arbitration** : Establishing neutral third-party mediation
    or arbitration processes to resolve conflicts between users and developers or
    platform providers [ *GPT-4o* ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regulatory compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, you should follow all relevant laws, regulations, and industry standards.
    This includes data protection laws, cybersecurity standards, and industry-specific
    regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Work proactively with regulatory bodies to help shape and comply with evolving
    laws and standards related to AI-generated code.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [*Chapter 5*](B21009_05.xhtml#_idTextAnchor115) for more on bias mitigation
    and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the liabilities and responsibilities, we must learn about how
    LLM coding is governed by legal frameworks, in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Examining legal frameworks governing the use of LLMs in coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to examine the regulations around AI-generated code. Again, this will
    differ, depending on the international union, country, province, or jurisdiction.
  prefs: []
  type: TYPE_NORMAL
- en: UN resolution on AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On March 21, 2024, the United Nations General Assembly adopted the first global
    resolution on AI. This non-binding resolution suggests that countries protect
    privacy and human rights and should watch out for risks from AI.
  prefs: []
  type: TYPE_NORMAL
- en: This resolution was needed because there are fears that AI could be “ *used
    to disrupt democratic processes, turbocharge fraud or lead to dramatic job losses,
    among other* *harms.* ” [ *Reuters* ].
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this UN resolution was not copyright law, but there may be some
    implications when countries start to implement these suggestions [ *Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Some countries already have AI laws; let’s explore that.
  prefs: []
  type: TYPE_NORMAL
- en: EU – the European Parliament adopts the “AI Act”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On March 13, 2024, the European Parliament adopted the AI Act, marking the world’s
    first comprehensive horizontal legal framework for AI.
  prefs: []
  type: TYPE_NORMAL
- en: It establishes EU-wide rules on data quality, transparency, human oversight,
    and accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Impact and penalties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI Act has significant extraterritorial effects and imposes challenging
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Companies conducting business in the EU could face fines of up to 35 million
    euros or 7% of global annual revenue (whichever is higher).
  prefs: []
  type: TYPE_NORMAL
- en: The AI Act came into force 20 days after publication in the Official Journal
    [written in June 2024].
  prefs: []
  type: TYPE_NORMAL
- en: Provisions become applicable two years after the Act comes into force, except
    for prohibited AI systems (after six months) and generative AI (after 12 months).
  prefs: []
  type: TYPE_NORMAL
- en: Definition of AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final definition focuses on AI systems operating with varying autonomy and
    generating outputs, such as predictions and decisions that influence environments.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems are distinct from simpler software. The ability to infer is a key
    characteristic, referring to obtaining output such as predictions, content, recommendations,
    or decisions that can influence physical or virtual environments.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems include machine learning approaches that learn from data, and logic-based
    approaches that infer from encoded knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Scope and applicability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI Act applies to providers of AI systems, including developers, importers,
    and distributors of AI systems in the EU.
  prefs: []
  type: TYPE_NORMAL
- en: It also applies to “deployers” – individuals or entities that use AI systems
    in their professional activities.
  prefs: []
  type: TYPE_NORMAL
- en: The Act has extraterritorial effects, applying to AI systems used in the EU
    regardless of where the provider is located.
  prefs: []
  type: TYPE_NORMAL
- en: Exemptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI developed solely for scientific research and development is exempt. The Act
    does not apply to AI research, testing, and development activities before market
    placement, but real-world testing is excluded.
  prefs: []
  type: TYPE_NORMAL
- en: Free and open source AI systems are exempt unless classified as high-risk, prohibited,
    or generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Risk-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI Act uses four risk levels in AI – unacceptable risk, high risk, limited
    risk, and minimal risk.
  prefs: []
  type: TYPE_NORMAL
- en: Unacceptable risk (prohibited)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AI practices posing clear threats to fundamental rights are prohibited. This
    includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems that manipulate human behavior or exploit vulnerabilities (e.g.,
    age and disability) with the aim of distorting behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biometric systems such as emotion recognition in the workplace or real-time
    categorization of individuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High risk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AI systems identified as high-risk must comply with stringent requirements,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Risk-mitigation measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-quality datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging of activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed documentation and clear user information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human oversight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High levels of robustness, accuracy, and cybersecurity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples include critical infrastructures (energy and transport), medical devices,
    and systems determining access to education or employment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited risk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For AI systems that interact directly with natural persons, such as chatbots,
    the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: Providers must ensure that individuals are informed that they are interacting
    with an AI system – a full disclosure requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployers of AI that generate or manipulate deepfakes must disclose that the
    content is artificially generated or manipulated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimal risk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: No specific restrictions are imposed on minimal-risk AI systems, such as AI-enabled
    video games or spam filters. However, companies may voluntarily commit to codes
    of conduct to ensure ethical AI use.
  prefs: []
  type: TYPE_NORMAL
- en: General-purpose AI models/generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A new chapter on general-purpose AI models has been added to the AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: It differentiates between general-purpose AI models, general-purpose AI models
    with systemic risk, and those with high-impact capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Specific regulatory requirements apply to these models, particularly concerning
    transparency, data quality, and human oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with the General Data Protection Regulation (GDPR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI Act does not affect the GDPR or the ePrivacy Directive (2002/58/EC).
  prefs: []
  type: TYPE_NORMAL
- en: Provisions of the AI Act related to the processing of personal data must align
    with existing data protection regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Developments and compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most provisions (i.e., specific rules and requirements that businesses need
    to follow) will become applicable two years after the Act’s official date of becoming
    law. However, provisions related to prohibited AI systems will apply six months
    after the Act comes into force, and provisions regarding generative AI will apply
    after 12 months.
  prefs: []
  type: TYPE_NORMAL
- en: Companies affected by the AI Act should begin preparing for compliance as soon
    as possible to meet these timelines. This preparation might involve significant
    redesigns of products and services, risk assessments, and adjustments to align
    with new requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The AI Act is a landmark regulation, introducing the world’s first comprehensive
    framework for AI.
  prefs: []
  type: TYPE_NORMAL
- en: It imposes strict requirements to ensure the safe and ethical deployment of
    AI systems across the EU.
  prefs: []
  type: TYPE_NORMAL
- en: The Act covers a wide range of entities involved in the development, distribution,
    and deployment of AI, including those outside the EU if their systems are used
    within the EU.
  prefs: []
  type: TYPE_NORMAL
- en: Provisions include risk-based categorization, with stringent requirements for
    high-risk systems and transparency mandates for lower-risk systems.
  prefs: []
  type: TYPE_NORMAL
- en: Companies need to prepare for compliance by conducting risk assessments, redesigning
    products and services as necessary, and aligning their operations with the new
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The AI Act also complements existing data protection laws such as the GDPR,
    ensuring that AI systems handle personal data responsibly and transparently.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation for compliance with the AI Act should begin immediately, despite
    the phased implementation timeline. [ *Zenitech,* *Gemini, GPT-4o* ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Find out more about the EU’s AI Act at [https://artificialintelligenceact.eu/
    #:~: text=What%20is%20the%20EU%20AI,AI%20to%20three%20risk%20 categories](https://artificialintelligenceact.eu/#:~:text=What%20is%20the%20EU%20AI,AI%20to%20three%20risk%20categories)
    or [https://www.europarl.europa.eu/topics/en/article/ 20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: California AI kill switch bill proposed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tech companies in California are concerned, as the state Senate has passed a
    bill that would introduce a new legislature for the development of AI, and some
    big names in tech feel that the bill seeks to set too many restrictions and would
    limit innovation. The bill is called the Safe and Secure Innovation for Frontier
    Artificial Intelligence Systems Act.
  prefs: []
  type: TYPE_NORMAL
- en: This could “ *end open source* ” AI such as Meta’s Llamas models, according
    to Arun Rao, lead product manager for generative AI at Meta.
  prefs: []
  type: TYPE_NORMAL
- en: Andrew Ng said, “ *If someone wanted to come up with regulations to stifle innovation,
    one could hardly* *do better.* ”
  prefs: []
  type: TYPE_NORMAL
- en: Tech companies are starting to think about whether they need to leave California.
  prefs: []
  type: TYPE_NORMAL
- en: However, the Democrat state senator who proposed this legislation, Scott Weiner,
    believes it is a “ *light touch bill,* ” only asking tech developers to take basic
    steps in the interests of safety. An amendment to the bill says it will only apply
    to large models costing at least $100 million to train [Financial_Times].
  prefs: []
  type: TYPE_NORMAL
- en: AI Acts of other countries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brazil is drafting legislation on AI, whereas the USA has some state-level regulations
    [Gemini].
  prefs: []
  type: TYPE_NORMAL
- en: Singapore got in there early with AI regulations and created the Model AI Governance
    Framework for Generative AI in 2019 [DataGuidance].
  prefs: []
  type: TYPE_NORMAL
- en: South Korea (the Republic of Korea) has an AI Act bill in its National Assembly,
    the Act on Promotion of AI Industry and Framework for Establishing Trustworthy
    AI, which aims to pull together different fragmented bills [ *LawAsia* ].
  prefs: []
  type: TYPE_NORMAL
- en: The Indian government, working with **Nasscom** ( **National Association of
    Software and** **Service Companies** ) and leading industry partners, has produced
    guidelines on AI development, the **Responsible AI** ( **RAI** ) Resource Kit.
    While not legally binding, it is a set of tools and guidance to enable companies
    to use AI in responsible and safe ways to help them grow [ *IndiaAI_RAI, IndiaAI_RAI_Kit*
    ].
  prefs: []
  type: TYPE_NORMAL
- en: Countries also developing AI regulations include the UK, China, and Japan.
  prefs: []
  type: TYPE_NORMAL
- en: Other nations and regions may create their own regulations on AI, so make sure
    to get good legal advice before deploying or selling in those jurisdictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This site gives an overview of the AI regulations of various nations: [https://iapp.org/resources/article/global-ai-governance-jurisdiction-overviews/](https://iapp.org/resources/article/global-ai-governance-jurisdiction-overviews/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Other regulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other than laws directly regulating AI and ML, there are national laws that
    relate to them. There are laws regulating training data and the technologies of
    the computing ecosystem – data privacy, exporting data, hardware, software, and
    ideas from your country if they could give military advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy laws
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over 137 out of 194 countries have data privacy and security regulations. Make
    sure that the use of AI-generated code complies with data privacy regulations
    such as the GDPR, the **California Consumer Privacy Act** ( **CCPA** ), the **Children’s
    Online Privacy Act** ( **COPPA** ) in the USA, the **Lei Geral de Proteção de
    Dados** ( **LGPD** ) in Brazil, the **Personal Information Protection Law** (
    **PIPL** ) in China, the **Personal Data Protection Act** ( **PDPA** ) in Thailand,
    the **Protection of Privacy Law** ( **PPL** ) in Israel, and the **Act on the
    Protection of Personal Information** ( **APPI** ) in Japan, especially if the
    code handles personal data.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you know about and are compliant with these regulations in the
    places you operate before you deploy your code in these locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sources for further study on how to be compliant:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CCPA: [https://www.varonis.com/blog/ccpa-compliance](https://www.varonis.com/blog/ccpa-compliance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'COPPA: [https://www.accessibilitychecker.org/blog/coppa-compliance-guidelines/](https://www.accessibilitychecker.org/blog/coppa-compliance-guidelines/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LGPD: [https://blog.didomi.io/what-is-the-lgpd-brazil-and-how-can-companies-be-compliant](https://blog.didomi.io/what-is-the-lgpd-brazil-and-how-can-companies-be-compliant)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PIPL: [https://www.cookieyes.com/blog/china-personal-information-protection-law-pipl/](https://www.cookieyes.com/blog/china-personal-information-protection-law-pipl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PDPA: [https://pdpa.guide/](https://pdpa.guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPL: [https://clym.io/regulations/israel-ppl](https://clym.io/regulations/israel-ppl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'APPI and others: [https://www.consentmo.com/compliance/appi#:~:text=To%20ensure%20employee%20training%20on,security%20measures%2C%20and%20reporting%20procedures](https://www.consentmo.com/compliance/appi#:~:text=To%20ensure%20employee%20training%20on,security%20measures%2C%20and%20reporting%20procedures)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting control laws
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware of any export control regulations that might apply, particularly if
    code or the underlying technology is subject to such laws. Export controls are
    when a country limits or prohibits you from taking certain technologies out of
    a country or talking about them outside of the country. They need to be in the
    public domain first, so if they are spoken about at a conference in the home country
    or published in a journal first, then you can mention them outside of your country.
  prefs: []
  type: TYPE_NORMAL
- en: The technologies that governments worry about are the ones that can have military
    or terrorist applications, such as AI and weapons (nuclear, chemical, or biological).
    This includes software, data, or knowledge [ *OxfordUni* ].
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about the legal frameworks relating to LLM code, we will move
    on to how this could change in the future.
  prefs: []
  type: TYPE_NORMAL
- en: What good and effective regulation could be put in place in the future for AI-generated
    code? That is the subject of the final subsection of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Possible future of the regulation of AI-generated code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are some ideas about where regulations might move in the future,
    with regard to AI-gen code. This is just speculation, but it can help us think
    about how to prepare and also what movements to try to create ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Key points moving forward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must consider the courts, international cooperation, and standardization,
    as well as differences between countries, national competition and sovereignty,
    audits, and how the future of AI regulation will be shaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Courts and copyright** : As AI advances, courts will likely interpret existing
    laws on authorship and copyright for AI-generated content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legislative reforms** : Copyright laws may need updates to address the complexities
    of AI-generated works. Countries might introduce clearer guidelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**International standards** : Disparate treatment of AI works across countries
    could lead to calls for international standards or treaties on AI and intellectual
    property. Some countries might follow pioneers such as the EU and the USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Challenges to standardization** : However, cultural differences, economic
    priorities, and the rapid pace of AI development could hinder global standardization.
    Laws may struggle to keep up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Competition for AI developers** : Competition between countries to attract
    AI developers might influence regulations. For example, some see China as having
    more relaxed regulations to boost AI innovation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Public concerns** : Conversely, public worries about privacy, bias, and job
    losses due to AI could lead to stricter regulations. Ethical considerations such
    as transparency and accountability of AI might be addressed. AI systems might
    need to explain their decision-making process and data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sovereignty** : National policymakers might consider data sovereignty
    rules, keeping data within borders to train AI and protect national interests
    (this goes beyond current export controls).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry certification** : Industries such as aviation, pharmaceuticals,
    construction, engineering, and manufacturing require certifications to meet predefined
    standards. AI systems may also eventually require such certifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability and liability** : Laws might establish clearer accountability
    and liability for AI developers, users, and publishers in cases of harm or misuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI audits** : AI systems could require independent audits to ensure compliance
    with safety laws and ethical standards, similar to certifications in industries
    such as aviation and manufacturing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shaping the future landscape** : Ongoing research, legal scholarship, and
    industry input will be crucial for shaping future AI regulations. Collaboration
    between technologists, legal experts, and policymakers is essential for balanced
    regulations that support innovation while addressing ethical and societal concerns.
    This cross-disciplinary approach can lead to well-informed, practical, and forward-thinking
    policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are all theories on how l aws may change; they are not definite plans,
    just speculation [ GPT-4o, Gemini].
  prefs: []
  type: TYPE_NORMAL
- en: Questions that should still be answered
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'here are still questions that we need to ask and answer about AI, and we need
    to know how we’re going to manage in the future as we transition to a world with
    increasing levels of AI, with humans increasingly depending on it:'
  prefs: []
  type: TYPE_NORMAL
- en: The EU regulations on AI suggest that deepfakes are allowed, but developers
    must be open about the fakes. Should deepfakes be allowed at all?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What should legislators do about the risks of minors becoming addicted to AI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who or what should be taxed when these AIs do work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If somebody lives in Singapore and uses AI based in California, developed by
    a Californian company, while the human is working for a company in Japan, should
    the person or the employer be taxed? Should the owners or the creators or providers
    of the AI be taxed?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How are we going to tax and fund government services for the people in the AI
    age, when most tasks and jobs are done by AI?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What jobs or hobbies are we going to do when AI does all the work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we really ensure that living things (other than diseases and pests)
    are kept safe from AI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we actually care if AIs are conscious?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Experts say it doesn’t make a difference to our outcomes.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What work should we never allow AI to do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these questions need to be answered to get regulations done right.
  prefs: []
  type: TYPE_NORMAL
- en: Keep up to date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These future ideas for regulations are not certain. What is clear is that AI
    legislation, like the AI tech itself, moves quickly, and practitioners need to
    keep updated with developments to make sure that they’re not left behind!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some useful links to help you stay up to date with the evolution of
    AI legality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global AI Law and Policy Tracker by the International Association of Privacy
    Professionals (IAPP)** ( [https://iapp.org/](https://iapp.org/) ): This offers
    a comprehensive and regularly updated overview of AI legislation and policy developments
    worldwide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future of Life Institute (FLI)** ( [https://futureoflife.org/](https://futureoflife.org/)
    ): This non-profit organization focuses on existential risks from advanced technologies,
    including AI. They track and analyze legal developments related to AI, particularly
    concerning safety and ethics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Law.com – Artificial Intelligence Topics** ( [https://www.law.com/topics/artificial-intelligence/](https://www.law.com/topics/artificial-intelligence/)
    ): This legal news website provides articles, analysis, and expert insights on
    the legal issues surrounding AI technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Lawyer** ( [https://www.artificiallawyer.com/](https://www.artificiallawyer.com/)
    ): This website focuses on legal tech and AI news, including developments in legal
    regulations for AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a guide to staying up to date with AI regulation; it also has links
    to help with that: [https://www.linkedin.com/pulse/beginners-guide-keeping-up-date-ai-regulations-bsfitaly-guqsf/](https://www.linkedin.com/pulse/beginners-guide-keeping-up-date-ai-regulations-bsfitaly-guqsf/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that we need to be aware of legal liabilities, follow
    regulations, and even stay up to date with changing regulations when using LLMs
    to get code. Different countries allow or disallow the copyright of AI-generated
    materials such as code. There are many jurisdictions, and they have different
    interpretations of how to manage AI and AI code, although there are some similarities,
    such as how they look at the risks of AI and require more transparency and clarity.
    Regulations are starting to appear, in the EU and other places. There is always
    the need to keep watching the news.
  prefs: []
  type: TYPE_NORMAL
- en: Keep thinking about what AI should be allowed to do, and talk to others about
    it, especially your lawmakers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll uncover the security concerns associated with using
    LLMs, how to be ready for threats, and how to guard against them.
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Cooley* : “AI Outputs Varies Around the World,” Cooley: [https://www.cooley.com/news/insight/2024/2024-01-29-copyright-ownership-of-generative-ai-outputs-varies-around-the-world](https://www.cooley.com/news/insight/2024/2024-01-29-copyright-ownership-of-generative-ai-outputs-varies-around-the-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DataGuidance* : “Singapore: IMDA publishes Model AI Governance Framework for
    Generative AI,” DataGuidance: [https://www.dataguidance.com/news/singapore-imda-publishes-model-ai-governance-framework](https://www.dataguidance.com/news/singapore-imda-publishes-model-ai-governance-framework)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Financial_Times* : “Silicon Valley in uproar over Californian AI safety bill,”
    Hannah Murphy and Tabby Kinder: [https://www.ft.com/content/eee08381-962f-4bdf-b000-eeff42234ee0](https://www.ft.com/content/eee08381-962f-4bdf-b000-eeff42234ee0)
    ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gemini* : Gemini, Alphabet: [https://gemini.google.com/](https://gemini.google.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GPT-4o* : GPT-4o, OpenAI: [https://platform.openai.com/playground/chat?models=gpt-4o](https://platform.openai.com/playground/chat?models=gpt-4o)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*IndiaAI_RAI* : “The State of Responsible AI in India 2023,” Anjali Raja: [https://indiaai.gov.in/research-reports/the-state-of-responsible-ai-in-india-2023](https://indiaai.gov.in/research-reports/the-state-of-responsible-ai-in-india-2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*IndiaAI_RAI_Kit* : “NASSCOM RESPONSIBLE AI RESOURCE KIT,” IndiaAI: [https://indiaai.gov.in/responsible-ai/homepage](https://indiaai.gov.in/responsible-ai/homepage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ind_Law&Tech* : “Balancing Indian Copyright Law with AI-Generated Content:
    The ‘Significant Human Input’ Approach,” Harshal Chhabra, Kanishk Gaurav Pandey”:
    [https://www.ijlt.in/post/balancing-indian-copyright-law-with-ai-generated-content-the-significant-human-input-approach#:~:text=Novelty%20is%20considered%20too%20high,pre%2Dexisting%20bodies%20of%20knowledge](https://www.ijlt.in/post/balancing-indian-copyright-law-with-ai-generated-content-the-significant-human-input-approach#:~:text=Novelty%20is%20considered%20too%20high,pre%2Dexisting%20bodies%20of%20knowledge)
    . ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LawAsia* : “Analysis of AI regulatory frameworks in South Korea,” Hwan Kyoung
    Ko: [https://law.asia/ai-regulatory-frameworks-south-korea/](https://law.asia/ai-regulatory-frameworks-south-korea/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LawStackExchange* : “Copyright risks for code contributed by generative AI,”
    Dr Xorile, User6726, Dale M: [https://law.stackexchange.com/questions/97621/copyright-risks-for-code-contributed-by-generative-ai](https://law.stackexchange.com/questions/97621/copyright-risks-for-code-contributed-by-generative-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lexology* : “Interpretation Released by Taiwan’s IPO to Clarify Copyright
    Disputes Regarding Generative AI,” Lee Tsai & Partners: [https://www.lexology.com/library/detail.aspx?g=831aa3a8-5db4-4c6e-8988-a6e297614ba7](https://www.lexology.com/library/detail.aspx?g=831aa3a8-5db4-4c6e-8988-a6e297614ba7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Liu_Medium* : “Generative AI and Copyright Law: Who owns what and your intellectual
    property rights,” Ginger Liu M.F.A.: [https://medium.com/technology-hits/generative-ai-and-copyright-law-47aceb4ebb17](https://medium.com/technology-hits/generative-ai-and-copyright-law-47aceb4ebb17)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monkey_Business* : “Monkey business finally settled: the ‘monkey selfie’ disputes,”
    Paulina Julia Perkal ( IViR): [https://copyrightblog.kluweriplaw.com/2018/02/05/monkey-business-finally-settled-monkey-selfie-disputes/#:~:text=During%20the%20hearing%20in%20January,cannot%20be%20granted%20to%20animals](https://copyrightblog.kluweriplaw.com/2018/02/05/monkey-business-finally-settled-monkey-selfie-disputes/#:~:text=During%20the%20hearing%20in%20January,cannot%20be%20granted%20to%20animals)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OxfordUni* : “Export controls and research collaborations,” Research Support:
    [https://researchsupport.admin.ox.ac.uk/policy/export#:~:text=What%20items%20are%20controlled%3F,or%20their%20means%20of%20delivery](https://researchsupport.admin.ox.ac.uk/policy/export#:~:text=What%20items%20are%20controlled%3F,or%20their%20means%20of%20delivery)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reuters* : “UN adopts first global artificial intelligence resolution,” Alexandra
    Alper: [https://www.reuters.com/technology/cybersecurity/un-adopts-first-global-artificial-intelligence-resolution-2024-03-21/](https://www.reuters.com/technology/cybersecurity/un-adopts-first-global-artificial-intelligence-resolution-2024-03-21/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wilmerhale* : “The European Parliament Adopts the AI Act,” Krik J. Nahra,
    Arianna Evers, Ali A. Jessani, Martin Braun, Anne Vallery, Isiq Benizri: [https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240314-the-european-parliament-adopts-the-ai-act](https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240314-the-european-parliament-adopts-the-ai-act)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zenitech* : “Security analysis of AI-generated code,” Tautvydas Bakšys: [https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/](https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zerotolive* : “Code is a liability,” zerotolive: [https://zerotolive.app/code-is-a-liability](https://zerotolive.app/code-is-a-liability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
