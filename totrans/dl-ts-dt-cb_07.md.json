["```py\npip install -U scikit-learn\n```", "```py\nimport pandas as pd\nmvtseries = pd.read_csv\n    ('assets/data/daily_multivariate_timeseries.csv',\n    parse_dates=['datetime'],\n    index_col='datetime')\n```", "```py\nmvtseries['target'] = \n    \\(mvtseries['Incoming Solar'].diff() < -2000).astype(int)\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pytorch_forecasting import TimeSeriesDataSet\nfrom pytorch_lightning import LightningDataModule\nclass ExceedanceDataModule(LightningDataModule):\n    def __init__(self,\n                 data: pd.DataFrame,\n                 test_size: float = 0.2,\n                 batch_size: int = 1):\n        super().__init__()\n        self.data = data\n        self.var_names = self.data.columns.tolist()\n        self.batch_size = batch_size\n        self.test_size = test_size\n        self.training = None\n        self.validation = None\n        self.test = None\n        self.predict_set = None\n```", "```py\n    def setup(self, stage=None):\n        self.data['target'] = (\n            self.data['Incoming Solar'].diff() < -2000).astype(int)\n        self.data['time_index'] = np.arange(self.data.shape[0])\n        self.data['group_id'] = 0\n        unique_times = self.data['time_index'].sort_values().unique()\n        tr_ind, ts_ind = \\\n            train_test_split(unique_times,\n                test_size=self.test_size,\n                shuffle=False)\n        tr_ind, vl_ind = train_test_split(tr_ind,\n                test_size=0.1, shuffle=False)\n        training_df = self.data.loc[\n            self.data['time_index'].isin(tr_ind), :]\n        validation_df = self.data.loc[\n            self.data['time_index'].isin(vl_ind), :]\n        test_df = self.data.loc[\n            self.data['time_index'].isin(ts_ind), :]\n        self.training = TimeSeriesDataSet(\n            data=training_df,\n            time_idx=\"time_index\",\n            target=\"target\",\n            group_ids=['group_id'],\n            max_encoder_length=14,\n            max_prediction_length=7,\n            time_varying_unknown_reals=self.var_names,\n            scalers={k: MinMaxScaler()\n                     for k in self.var_names\n                     if k != 'target'}\n        )\n        self.validation = TimeSeriesDataSet.from_dataset(\n            self.training, validation_df)\n        self.test = TimeSeriesDataSet.from_dataset(\n            self.training, test_df)\n        self.predict_set = TimeSeriesDataSet.from_dataset(\n            self.training, self.data, predict=True)\n```", "```py\n    def train_dataloader(self):\n        return self.training.to_dataloader(\n            batch_size=self.batch_size, shuffle=False)\n    def val_dataloader(self):\n        return self.validation.to_dataloader(\n            batch_size=self.batch_size, shuffle=False)\n    def test_dataloader(self):\n        return self.test.to_dataloader(\n            batch_size=self.batch_size, shuffle=False)\n    def predict_dataloader(self):\n        return self.predict_set.to_dataloader(\n            batch_size=1, shuffle=False)\n```", "```py\ndatamodule = ExceedanceDataModule(data=mvtseries)\ndatamodule.setup()\nx, y = next(iter(datamodule.train_dataloader()))\n```", "```py\nN_LAGS = 14\nHORIZON = 7\nmvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',\n        parse_dates=['datetime'],\n        index_col='datetime')\ndatamodule = ExceedanceDataModule(data=mvtseries,\n        batch_size=64, test_size=0.3)\n```", "```py\nimport torch.nn as nn\nimport lightning.pytorch as pl\nclass ExceedanceLSTM(pl.LightningModule):\n    def __init__(self, input_dim, hidden_dim, num_layers):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, self.hidden_dim,\n                    num_layers, batch_first=True)\n        self.fc = nn.Linear(self.hidden_dim, 1)\n    def forward(self, x):\n        h0 = torch.zeros(self.lstm.num_layers, x.size(0),\n            self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.lstm.num_layers, x.size(0),\n            self.hidden_dim).to(self.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        out = torch.sigmoid(out)\n        return out\n```", "```py\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_bin = (y[0] > 0).any(axis=1).long().type(torch.FloatTensor)\n        y_pred = self(x['encoder_cont'])\n        loss = F.binary_cross_entropy(y_pred.squeeze(-1), y_bin)\n        self.log('train_loss', loss)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_bin = (y[0] > 0).any(axis=1).long().type(torch.FloatTensor)\n        y_pred = self(x['encoder_cont'])\n        loss = F.binary_cross_entropy(y_pred.squeeze(-1), y_bin)\n        self.log('val_loss', loss)\n        return loss\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n```", "```py\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_bin = (y[0] > 0).any(axis=1).long().type(torch.FloatTensor)\n        y_pred = self(x['encoder_cont'])\n        loss = F.binary_cross_entropy(y_pred.squeeze(-1), y_bin)\n        auroc = AUROC(task='binary')\n        auc_score = auroc(y_pred, y_bin)\n        self.log('test_bce', loss)\n        self.log('test_auc', auc_score)\n```", "```py\nmodel = ExceedanceLSTM(input_dim=10, hidden_dim=32, num_layers=1)\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\",\n                                    min_delta=1e-4,\n                                    patience=10,\n                                    verbose=False,\n                                    mode=\"min\")\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator=\"cpu\",\n    callbacks=[early_stop_callback]\n)\ntrainer.fit(model, datamodule)\ntrainer.test(model, datamodule)\n```", "```py\nimport pandas as pd\ndataset = pd.read_csv(\n    \"assets/daily_multivariate_timeseries.csv\",\n    parse_dates=[\"datetime\"],\n)\n```", "```py\nseries = dataset[['Incoming Solar']].reset_index()\nseries['id'] = 'Solar'\nseries = series.rename(columns={'index': 'ds', 'Incoming Solar': 'y', \n    'id': 'unique_id'})\n```", "```py\nfrom sklearn.model_selection import train_test_split\nHORIZON = 7\ntrain, test = train_test_split(series, test_size=HORIZON)\n```", "```py\nfrom statsforecast.utils import ConformalIntervals\nintervals = ConformalIntervals(h=HORIZON)\nmodels = [\n    ARIMA(order=(2, 0, 2),\n          season_length=365,\n          prediction_intervals=intervals),\n]\n```", "```py\nsf = StatsForecast(\n    df=train,\n    models=models,\n    freq='D',\n)\nforecasts = sf.forecast(h=HORIZON, level=[95])\n```", "```py\n    class LossTrackingCallback(Callback):\n        def __init__(self):\n            self.train_losses = []\n            self.val_losses = []\n        def on_train_epoch_end(self, trainer, pl_module):\n            if trainer.logged_metrics.get(\"train_loss_epoch\"):\n                self.train_losses.append(\n                    trainer.logged_metrics[\"train_loss_epoch\"].item())\n        def on_validation_epoch_end(self, trainer, pl_module):\n            if trainer.logged_metrics.get(\"val_loss_epoch\"):\n                self.val_losses.append(\n                    trainer.logged_metrics[\"val_loss_epoch\"].item())\n    ```", "```py\n    class ProbabilisticLSTM(LightningModule):\n        def __init__(self, input_size,\n                     hidden_size, seq_len,\n                     num_layers=2):\n            super().__init__()\n            self.save_hyperparameters()\n            self.lstm = nn.LSTM(input_size, hidden_size,\n                num_layers, batch_first=True)\n            self.fc_mu = nn.Linear(hidden_size, 1)\n            self.fc_sigma = nn.Linear(hidden_size, 1)\n            self.hidden_size = hidden_size\n            self.softplus = nn.Softplus()\n        def forward(self, x):\n            lstm_out, _ = self.lstm(x)\n            lstm_out = lstm_out[:, -1, :]\n            mu = self.fc_mu(lstm_out)\n            sigma = self.softplus(self.fc_sigma(lstm_out))\n            return mu, sigma\n    ```", "```py\n        def training_step(self, batch, batch_idx):\n            x, y = batch[0][\"encoder_cont\"], batch[1][0]\n            mu, sigma = self.forward(x)\n            dist = torch.distributions.Normal(mu, sigma)\n            loss = -dist.log_prob(y).mean()\n            self.log(\n                \"train_loss\", loss, on_step=True,\n                on_epoch=True, prog_bar=True, logger=True\n            )\n            return {\"loss\": loss, \"log\": {\"train_loss\": loss}}\n        def validation_step(self, batch, batch_idx):\n            x, y = batch[0][\"encoder_cont\"], batch[1][0]\n            mu, sigma = self.forward(x)\n            dist = torch.distributions.Normal(mu, sigma)\n            loss = -dist.log_prob(y).mean()\n            self.log(\n                \"val_loss\", loss, on_step=True,\n                on_epoch=True, prog_bar=True, logger=True\n            )\n            return {\"val_loss\": loss}\n        def configure_optimizers(self):\n            optimizer = optim.Adam(self.parameters(), lr=0.0001)\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, \"min\")\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            }\n    ```", "```py\n    datamodule = ContinuousDataModule(data=mvtseries)\n    datamodule.setup()\n    model = ProbabilisticLSTM(\n        input_size = input_size, hidden_size=hidden_size,\n        seq_len=seq_len\n    )\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", \n        patience=5)\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=\"./model_checkpoint/\", save_top_k=1, \n        monitor=\"val_loss\"\n    )\n    loss_tracking_callback = LossTrackingCallback()\n    trainer = Trainer(\n        max_epochs=100,\n        callbacks=[early_stop_callback, checkpoint_callback,\n        loss_tracking_callback],\n    )\n    trainer.fit(model, datamodule)\n    ```", "```py\ndef load_and_prepare_data(file_path, time_column, series_column, \n    aggregation_freq):\n    \"\"\"Load the time series data and prepare it for modeling.\"\"\"\n    dataset = pd.read_csv(file_path, parse_dates=[time_column])\n    dataset.set_index(time_column, inplace=True)\n    target_series = (\n        dataset[series_column].resample(aggregation_freq).mean()\n    )\n    return target_series\ndef add_time_features(dataframe, date_column):\n    \"\"\"Add time-related features to the DataFrame.\"\"\"\n    dataframe[\"week_of_year\"] = (\n        dataframe[date_column].dt.isocalendar().week.astype(float)\n    )\n    dataframe[\"month\"] = dataframe[date_column].dt.month.astype(float)\n    dataframe[\"sin_week\"] = np.sin(\n        2 * np.pi * dataframe[\"week_of_year\"] / 52\n    )\n    dataframe[\"cos_week\"] = np.cos(\n        2 * np.pi * dataframe[\"week_of_year\"] / 52\n    )\n    dataframe[\"sin_2week\"] = np.sin(\n        4 * np.pi * dataframe[\"week_of_year\"] / 52\n    )\n    dataframe[\"cos_2week\"] = np.cos(\n        4 * np.pi * dataframe[\"week_of_year\"] / 52\n    )\n    dataframe[\"sin_month\"] = np.sin(\n        2 * np.pi * dataframe[\"month\"] / 12\n    )\n    dataframe[\"cos_month\"] = np.cos(\n        2 * np.pi * dataframe[\"month\"] / 12\n    )\n    return dataframe\ndef scale_features(dataframe, feature_columns):\n    \"\"\"Scale features.\"\"\"\n    scaler = MinMaxScaler()\n    dataframe[feature_columns] = (\n        scaler.fit_transform(dataframe[feature_columns])\n    )\n    return dataframe, scaler\nFILE_PATH = \"assets/daily_multivariate_timeseries.csv\"\nTIME_COLUMN = \"datetime\"\nTARGET_COLUMN = \"Incoming Solar\"\nAGGREGATION_FREQ = \"W\"\nweekly_data = load_and_prepare_data(\n    FILE_PATH, TIME_COLUMN, TARGET_COLUMN, AGGREGATION_FREQ\n)\nweekly_data = (\n    weekly_data.reset_index().rename(columns={TARGET_COLUMN: \"y\"})\n)\nweekly_data = add_time_features(weekly_data, TIME_COLUMN)\nnumerical_features = [\n    \"y\",\n    \"week_of_year\",\n    \"sin_week\",\n    \"cos_week\",\n    \"sin_2week\",\n    \"cos_2week\",\n    \"sin_month\",\n    \"cos_month\",\n]\nfeatures_to_scale = [\"y\", \"week_of_year\"]\nweekly_data, scaler = scale_features(weekly_data, features_to_scale)\n```", "```py\ndef split_data(dataframe, date_column, split_time):\n    \"\"\"Split the data into training and test sets.\"\"\"\n    train = dataframe[dataframe[date_column] <= split_time]\n    test = dataframe[dataframe[date_column] > split_time]\n    return train, test\nSPLIT_TIME = weekly_data[\"ds\"].max() - pd.Timedelta(weeks=52)\ntrain, test = split_data(weekly_data, \"ds\", SPLIT_TIME)\n```", "```py\nnf = NeuralForecast(\n    models=[\n        DeepAR(\n            h=52,\n            input_size=52,\n            lstm_n_layers=3,\n            lstm_hidden_size=128,\n            trajectory_samples=100,\n            loss=DistributionLoss(\n                distribution=\"Normal\", level=[80, 90], \n                    return_params=False\n            ),\n            futr_exog_list=[\n                \"week_of_year\",\n                \"sin_week\",\n                \"cos_week\",\n                \"sin_2week\",\n                \"cos_2week\",\n                \"sin_month\",\n                \"cos_month\",\n            ],\n            learning_rate=0.001,\n            max_steps=1000,\n            val_check_steps=10,\n            start_padding_enabled=True,\n            early_stop_patience_steps=30,\n            scaler_type=\"identity\",\n            enable_progress_bar=True,\n        ),\n    ],\n    freq=\"W\",\n)\nnf.fit(df=train, val_size=52)\nY_hat_df = nf.predict(\n    futr_df=test[\n        [\n            \"ds\",\n            \"unique_id\",\n            \"week_of_year\",\n            \"sin_week\",\n            \"cos_week\",\n            \"sin_2week\",\n            \"cos_2week\",\n            \"sin_month\",\n            \"cos_month\",\n        ]\n    ]\n)\n```", "```py\nimport torch\nimport gpytorch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom pytorch_lightning import LightningDataModule\n```", "```py\n    class GPModel(gpytorch.models.ExactGP):\n        def init(self, train_x, train_y, likelihood):\n            super(GPModel, self).init(train_x, train_y, likelihood)\n            self.mean_module = gpytorch.means.ConstantMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) + gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(\n                mean_x, covar_x)\n    ```", "```py\n    likelihood = GaussianLikelihood()\n    model = GPModel(datamodule.train_x[:, 0], datamodule.train_y, \n        likelihood)\n    model.train()\n    likelihood.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    mll = ExactMarginalLogLikelihood(likelihood, model)\n    training_iter = 100\n    for i in range(training_iter):\n        optimizer.zero_grad()\n        output = model(datamodule.train_x[:, 0])\n        loss = -mll(output, datamodule.train_y)\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        observed_pred = likelihood\n            (model(datamodule.original_x[:, 0]))\n    ```", "```py\nimport pandas as pd\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nmvtseries = pd.read_csv(\n\"assets/daily_multivariate_timeseries.csv\",\nparse_dates=[\"datetime\"],\n)\nmvtseries['ds'] = mvtseries['datetime']\nmvtseries['y'] = mvtseries['Incoming Solar']\n```", "```py\n    train_data, test_data = train_test_split(mvtseries, \n        test_size=0.2, shuffle=False)\n    ```", "```py\n    model = Prophet()\n    model.fit(train_data[['ds', 'y']])\n    ```", "```py\n    future = model.make_future_dataframe(periods=len(test_data))\n    forecast = model.predict(future)\n    ```", "```py\n    fig = model.plot(forecast)\n    plt.show()\n    ```", "```py\nfig = model.plot_components(forecast)\nplt.show()\n```"]