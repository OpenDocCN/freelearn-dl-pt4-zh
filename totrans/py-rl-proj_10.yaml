- en: Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past few hundred pages, we have faced numerous challenges, to which
    we applied reinforcement and deep learning algorithms. To conclude our **reinforcement
    learning** (**RL**) journey, this chapter will look at several aspects of the
    field that we have not covered yet. We will start by looking at several of the
    drawbacks of reinforcement learning, which any practitioner or researcher should
    be aware of. To end on a positive note, we will follow up by describing numerous
    exciting academic developments and achievements the field has seen in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: The shortcomings of reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have only covered what reinforcement learning algorithms can do.
    To the reader, reinforcement learning may seem like the panacea for all kinds
    of problems. But why do we not see a ubiquitous application of reinforcement learning
    algorithms in real-life situations? The reality is that the field has a myriad
    of shortcomings that hinder commercial adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it necessary to talk about the field's flaws? We think this will help
    you build a more holistic, less biased view of reinforcement learning. Moreover,
    understanding the weaknesses of reinforcement learning and machine learning is
    an important quality of a good machine learning researcher or practitioner. In
    the following subsections, we will discuss a few of the most important limitations
    that reinforcement learning is currently facing.
  prefs: []
  type: TYPE_NORMAL
- en: Resource efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Current deep reinforcement learning algorithms require vast amounts of time,
    training data, and computational resources in order to reach a desirable level
    of proficiency. For algorithms such as AlphaGo Zero, where our reinforcement learning
    algorithm learns to play Go with zero prior knowledge and experience, resource
    efficiency becomes a major bottleneck for taking such algorithms to commercial
    scales. Recall that when DeepMind implemented AlphaGo Zero, they needed to train
    the agent on tens of millions of games using hundreds of GPUs and thousands of
    CPUs. For AlphaGo Zero to reach a reasonable proficiency, it needs to play a number
    of games, equivalent to what hundreds of thousands of humans would play in their
    lifetimes.
  prefs: []
  type: TYPE_NORMAL
- en: Unless, in the future, the average consumer can readily leverage vast amounts
    of computational power that only the likes of Google and Nvidia can offer today,
    the ability to develop superhuman reinforcement learning algorithms will continue
    to be way beyond the public's reach. This means that powerful, resource-hungry
    reinforcement learning algorithms will be monopolized by a small consortium of
    institutions, which is probably not a great thing.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, making reinforcement learning algorithms trainable under limited resources
    will continue to be an important issue that the community must address.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In numerous fields of scientific research, a prevalent problem has been the
    inability to reproduce the experimental results claimed in academic papers and
    journals. In a 2016 survey conducted by Nature, the world's most renowned scientific
    journal, 70% of respondents claimed that they have failed to reproduce their own
    or another researcher's experimental results. Moreover, the attitude toward the
    inability to reproduce experimental results was a stark one, with 90% of researchers
    thinking that there is indeed a reproducibility crisis.
  prefs: []
  type: TYPE_NORMAL
- en: The original work reported by nature can be found here: [https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970).
  prefs: []
  type: TYPE_NORMAL
- en: While this survey targeted researchers across a number of disciplines, including
    biology and chemistry, reinforcement learning is also facing a similar problem.
    In the paper *Deep Reinforcement Learning Matters* (reference at the end of this
    chapter; you can view it at [https://arxiv.org/pdf/1709.06560.pdf](https://arxiv.org/pdf/1709.06560.pdf) for
    the online version), Peter Henderson et al. study the effects of different configurations
    of a deep reinforcement learning algorithm on experimental outcomes. These configurations
    include hyperparameters, seeds for the random number generator, and network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In extreme cases, they found that, when training the same model on two sets
    of five different random seed configurations, the resulting average return for
    the two sets of models diverged  significantly. Moreover, changing other settings,
    such as the architecture of the CNN, activation functions, and learning rates,
    have profound effects on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: What are the implications of inconsistent, unreproducible results? As the adoption
    and popularity of reinforcement learning and machine learning continues to grow
    at near exponential rates, the number of implementations of reinforcement learning
    algorithms freely available on the internet also increases. If those implementations
    cannot reproduce the results they claim to be able to achieve, this would cause
    major issues and potential danger in real-life applications. Certainly, no one
    would want their self-driving car to be implemented so that it cannot produce
    consistent decisions!
  prefs: []
  type: TYPE_NORMAL
- en: Explainability/accountability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how an agent's policy can return either a single action or a probability
    distribution over a set of possible actions and how its value function can return
    how desirable a certain state is. But how can a model explain how it arrived at
    such predictions? As reinforcement learning becomes more popular and potentially
    more prevalent in real-life applications, there will be an ever-increasing need
    to be able to explain the output of reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Today, most advanced reinforcement learning algorithms incorporate deep neural
    networks, which, as of now, can only be represented as a set of weights and a
    sequence of non-linear functions. Moreover, due to its high dimensional nature,
    neural networks are not able to provide any meaningful, intuitive relationships
    between input and their corresponding output that can be understood easily by
    humans. Hence, deep learning algorithms are often referred to as black boxes,
    for it is difficult for us to understand what is really going on inside a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it important for a reinforcement learning algorithm to be explainable?
    Suppose an autonomous car is involved in a car accident (let's assume it was just
    an innocuous bump between two cars and the drivers are not hurt). Human drivers
    would be able to explain what led to the crash; they can give reasons for why
    they performed a particular maneuver and what exactly happened when the accident
    occurred. This would help law enforcement ascertain the cause of the accident
    and potentially determine who or what was accountable. However, even if we create
    an agent that can drive cars sufficiently well using algorithms available today,
    this is simply not possible.
  prefs: []
  type: TYPE_NORMAL
- en: Without the ability to explain predictions, it will be difficult for users and
    the general public to trust software that uses any kind of machine learning, especially
    in use cases where the algorithms are accountable for making important decisions.
    This is a serious impediment to the adoption of reinforcement learning algorithms
    in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Susceptibility to attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning algorithms have shown incredible results across numerous tasks,
    including computer vision, natural language processing, and speech recognition.
    In several tasks, deep learning has already surpassed human capabilities. However,
    recent work has shown that these algorithms are incredibly vulnerable to attacks.
    By attacks, we mean attempts to make imperceptible modifications to the input
    which causes the model to behave differently. Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6104b4d-4113-4c78-9833-54ef67989b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of adversarial attacks. By adding imperceptible perturbations
    to an image, an attacker can easily fool deep learning image classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: The rightmost image is the result of adding the left image, which is the original
    image, and the middle image, which represents the perturbations added to the original
    image. Even the most accurate, well-performing deep neural network image classifier
    fails to identify the right image as a goat and instead predicts it to be a toaster.
  prefs: []
  type: TYPE_NORMAL
- en: These examples have shocked many in the research community, for people did not
    expect that deep learning algorithms can be incredibly brittle and susceptible
    to such attacks. This field is now called **adversarial machine learning** and
    has been rapidly increasing in prominence and importance as more researchers around
    the world are investigating the robustness and vulnerabilities of deep learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms are also no stranger to these results and
    attacks. According to the paper titled *Robust Deep Reinforcement Learning with
    Adversarial Attacks* ([https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632))
    by Anay Pattanaik et. al., adversarial attacks to reinforcement learning algorithms
    can be defined as any possible perturbation that leads the agent into an increased
    probability of taking the worst possible action in that state. For example, we
    can add noise to the screen of an Atari game with the intention of tricking the
    RL agent playing the game to make a poor decision, which leads to a lower score.
  prefs: []
  type: TYPE_NORMAL
- en: More serious applications include adding noise to street signs to trick a self-driving
    car into thinking that a STOP sign is a speed sign, making an ATM recognize a
    $100 check as a $1,000,000 one, or even fooling a facial-recognition system to
    identify an attacker's face as that of another user.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, these vulnerabilities further add to the risks of adopting
    deep learning algorithms in practical, safety-critical use cases. While there
    are numerous ongoing efforts to countervail adversarial attacks, there is still
    a long way to go for deep learning algorithms to become robust enough for such
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Upcoming developments in reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The past few sections may have painted a stark outlook for deep learning and
    reinforcement learning. However, there is no need to feel entirely discouraged;
    this is, in fact, an exciting time for DL and RL, where many significant advances
    in research are continuing to shape the field and cause it to evolve at a rapid
    pace. With increasing availability of computational resources and data, the possibilities
    of expanding and improving deep learning and reinforcement learning algorithms
    continue to expand.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For one, the issues raised in the preceding section are recognized and acknowledged
    by the research community. There are several efforts being made to address them.
    In the work by Pattanaik et. al., not only do the authors demonstrate that current
    deep reinforcement learning algorithms are susceptible to adversarial attacks,
    they also propose techniques that can make the same algorithms more robust toward
    such attacks. In particular, by training deep RL algorithms on examples that were
    adversarially perturbed, the model can improve its robustness against similar
    attacks. This technique is commonly referred to as adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the research community is actively taking actions to solve the reproducibility
    problem. ICLR and ICML, two of the biggest conferences in machine learning, have
    hosted challenges where participants are invited to reimplement and re-run experiments
    conducted by submitted papers to reproduce the reported results. Participants
    are then required to critique the original work by writing a reproducibility report
    that describes the problem statement, experimental methodology, implementation
    details, analyses, and the reproducibility of the original paper. Organized by
    Joelle Pineau and McGill University, this challenge aims to promote transparency
    in experiments and academic work as well as to ensure the reproducibility and
    integrity of results.
  prefs: []
  type: TYPE_NORMAL
- en: More information on the ICLR 2018 reproducibility challenge can be found here: [https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html](https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html).
    Similarly, the original ICML workshop on reproducibility can be found here: [https://sites.google.com/view/icml-reproducibility-workshop/home](https://sites.google.com/view/icml-reproducibility-workshop/home).
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important topic that is increasing in importance and attention is transfer
    learning. Transfer learning is a paradigm in machine learning, where a model trained
    on one task is fine-tuned to accomplish another.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can train a model to recognize images of cars and use the weights
    of that model to initialize an identical model that learns to recognize trucks.
    The main intuition is that certain abstract concepts and features learned by training
    on one task are transferable to other similar tasks. This idea is applicable to
    many reinforcement learning problems as well. An agent that learns to play a particular
    Atari game should be able to play other Atari games proficiently without training
    entirely from scratch, much like how a human can.
  prefs: []
  type: TYPE_NORMAL
- en: Demis Hassabis, the founder of DeepMind and a pioneer in deep reinforcement
    learning, said in a recent talk that transfer learning is the key to general intelligence. And
    I think the key to doing transfer learning will be the acquisition of conceptual
    knowledge that is abstracted away from perceptual details of where you learned
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: The Demis Hassabis quote and the talk in which this was mentioned can be found
    here: [https://www.youtube.com/watch?v=YofMOh6_WKo](https://www.youtube.com/watch?v=YofMOh6_WKo)
  prefs: []
  type: TYPE_NORMAL
- en: There have already been several advances in computer vision and natural language
    processing, where models initialized with knowledge and priors from one domain
    are used to learn about data from another domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is especially useful when the second domain lacks data. Called **few-shot**
    or **one-shot** learning, these techniques allow models to learn to perform tasks
    well, even when the dataset is small, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13a45856-a815-4b92-9a54-68b401f7db6a.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of a few-shot learning classifier learning good decision boundaries
    for classes with small volumes of data
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning for reinforcement learning would involve having the agent
    learn to achieve high proficiency on a given task without a high dependence on
    time, data, and computational resources. Imagine a generalized game-playing agent
    that can easily be fine-tuned to perform well on any other video game using readily-available
    computational resources; this would make training RL algorithms a lot more efficient
    and thus more accessible to a wider audience.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-agent reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another promising area making significant strides is multi-agent reinforcement
    learning. Contrary to the problems we''ve seen where only one agent makes decisions,
    this topic involves having multiple agents make decisions simultaneously and cooperatively
    in order to achieve a common objective. One of the most significant works related
    to this has been OpenAI''s Dota2-playing system, called **OpenAI Five**. Dota2
    is one of the world''s most popular **Massively Multiplayer Online Role Playing
    Game** (**MMORPGs**). Compared to traditional RL games such as Go and Atari, Dota2
    is more complex for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiple agents**: Dota2 games involve two teams of five players, each fighting
    to destroy the other team''s base. Hence there are multiple agents, not just one,
    making decisions simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: The screen only shows the proximity of the agent''s character
    instead of the whole map. This means that the whole game state, including the
    locations of opponents and what they are doing, is not observable. In reinforcement
    learning, we call this a *partially-observable* state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High dimensionality**: A Dota2 agent''s observations can include 20,000 points,
    each depicting what a human player may observe on the screen, including health,
    the location of the controlling character, the location of enemies, and any attacks.
    Go, on the other hand, requires fewer data points to construct an observation
    (19 x 19 board, past moves). Hence, observations have high dimensionality and
    complexity. This also goes for decisions, where a Dota2 AI''s action space consists
    of 170,000 possibilities, which includes decisions on movement, casting spells,
    and using items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on OpenAI's Dota2 AI, check out their blogs on the project
    at [https://blog.openai.com/openai-five/](https://blog.openai.com/openai-five/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, by using novel upgrades on traditional reinforcement learning algorithms,
    each agent in OpenAI Five was able to learn to cooperate with one another in order
    to reach the common objective of destroying the enemy''s base. They were even
    able to learn several team strategies that experienced human players employ. The
    following is a screenshot from a game being played between a team of Dota players
    and OpenAI Five:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c596eab2-e204-41ac-930a-2206475bec90.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenAI versus human players (source: [https://www.youtube.com/watch?v=eaBYhLttETw](https://www.youtube.com/watch?v=eaBYhLttETw))
  prefs: []
  type: TYPE_NORMAL
- en: Despite the extreme levels of resource requirements (240 GPUs, 120,000 CPU cores,
    ~200 human years of gameplay in a single day), this project demonstrates that
    current AI algorithms are indeed able to cooperate with one another to reach a
    common objective in a vastly complex environment. This work symbolizes another
    significant advancement in AI and RL research and demonstrates what the current
    technology is capable of.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes our introductory journey into reinforcement learning. Over the
    course of this book, we learned how to implement agents that can play Atari games,
    navigate Minecraft, predict stock market prices, play the complex board game of
    Go, and even generate other neural networks to train on `CIFAR-10` data. In doing
    so, you acquired and became accustomed to some of the fundamental and state-of-the-art
    deep learning and reinforcement learning algorithms. In short, you have achieved
    a lot!
  prefs: []
  type: TYPE_NORMAL
- en: But the journey does not and should not end here. We hope that, with your newfound
    skills and knowledge, you will continue to utilize deep learning and reinforcement
    learning algorithms to tackle problems that you face outside of this book. More
    importantly, we hope that this guide motivates you to explore other fields of
    machine learning and further develop your knowledge and experience.
  prefs: []
  type: TYPE_NORMAL
- en: There are many obstacles for the reinforcement learning community to overcome.
    However, there is much to look forward to. With the increasing popularity and
    development of the field, we can't wait to see what new developments and milestones
    the field will achieve. We hope the reader, upon completing this guide, will feel
    more equipped and ready to build reinforcement learning algorithms and make significant
    contributions to the field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open Science Collaboration. (2015). *Estimating the reproducibility of psychological
    science*. Science, 349(6251), aac4716.
  prefs: []
  type: TYPE_NORMAL
- en: Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
    (2017). *Deep reinforcement learning that matters*. arXiv preprint arXiv:1709.06560.
  prefs: []
  type: TYPE_NORMAL
- en: Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., and Chowdhary, G. (2018, July).
    *Robust deep reinforcement learning with adversarial attacks*. In Proceedings
    of the 17th International Conference on Autonomous Agents and MultiAgent Systems (pp.
    2040-2042). International Foundation for Autonomous Agents and Multiagent Systems.
  prefs: []
  type: TYPE_NORMAL
