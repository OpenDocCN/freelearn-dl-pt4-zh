["```py\npip install pomegranate\n```", "```py\n# Imports \nimport random\nfrom itertools import chain\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n\nSentence = namedtuple(\"Sentence\", \"words tags\")\n\ndef read_data(filename):\n    \"\"\"\n    Function to read tagged sentence data.\n\n    Parameters\n    ----------\n    filename: str\n        The path to the file from where to read the data.\n\n    \"\"\"\n    with open(filename, 'r') as f:\n        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\") for l in s[1:]]))) \n                               for s in sentence_lines if s[0])\n\ndef read_tags(filename):\n    \"\"\"\n    Function to read a list of word tag classes.\n\n    Parameters\n    ----------\n    filename: str\n        The path to the file from where to read the tags.\n    \"\"\"\n    with open(filename, 'r') as f:\n        tags = f.read().split(\"\\n\")\n    return frozenset(tags)\n```", "```py\nclass Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n    \"\"\"\n    Class to handle a subset of the whole data. This is required when we split the\n    data into training and test sets.\n    \"\"\"\n    def __new__(cls, sentences, keys):\n        word_sequences = tuple([sentences[k].words for k in keys])\n        tag_sequences = tuple([sentences[k].tags for k in keys])\n        wordset = frozenset(chain(*word_sequences))\n        tagset = frozenset(chain(*tag_sequences))\n        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset,             \n                               word_sequences, tagset, tag_sequences, N, stream.__iter__)\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __iter__(self):\n        return iter(self.sentences.items())\n\nclass Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y\" + \n                                     \"training_set testing_set N stream\")):\n    \"\"\"\n    Class to represent the data in structured form for easy processing.\n    \"\"\"\n    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n        tagset = read_tags(tagfile)\n        sentences = read_data(datafile)\n        keys = tuple(sentences.keys())\n        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n        word_sequences = tuple([sentences[k].words for k in keys])\n        tag_sequences = tuple([sentences[k].tags for k in keys])\n        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n\n        # split data into train/test sets\n        _keys = list(keys)\n\n        if seed is not None:\n            random.seed(seed)\n\n        random.shuffle(_keys)\n        split = int(train_test_split * len(_keys))\n        training_data = Subset(sentences, _keys[:split])\n        testing_data = Subset(sentences, _keys[split:])\n        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __iter__(self):\n        return iter(self.sentences.items())\n```", "```py\n>>> data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n\n>>> print(\"There are {} sentences in the corpus.\".format(len(data)))\nThere are 57340 sentences in the corpus.\n>>> print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\nThere are 45872 sentences in the training set.\n>>> print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\nThere are 11468 sentences in the testing set.\n```", "```py\n>>> key = 'b100-38532'\n>>> print(\"Sentence: {}\".format(key))\nSentence: b100–38532\n>>> print(\"words: {!s}\".format(data.sentences[key].words))\\\nwords: ('Perhaps', 'it', 'was', 'right', ';', ';')\n>>> print(\"tags: {!s}\".format(data.sentences[key].tags))\ntags: ('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n```", "```py\n>>> print(\"There are a total of {} samples of {} unique words in the corpus.\".format(\n              data.N, len(data.vocab)))\nThere are a total of 1161192 samples of 56057 unique words in the corpus.\n\n>>> print(\"There are {} samples of {} unique words in the training set.\".format(\n              data.training_set.N, len(data.training_set.vocab)))\nThere are 928458 samples of 50536 unique words in the training set.\n\n>>> print(\"There are {} samples of {} unique words in the testing set.\".format(\n              data.testing_set.N, len(data.testing_set.vocab)))\nThere are 232734 samples of 25112 unique words in the testing set.\n>>> print(\"There are {} words in the test set that are missing in the training set.\".format(\n              len(data.testing_set.vocab - data.training_set.vocab)))\nThere are 5521 words in the test set that are missing in the training set.\n```", "```py\n>>> for i in range(2):\n...     print(\"Sentence {}:\".format(i + 1), data.X[i])\n...     print(\"Labels {}:\".format(i + 1), data.Y[i], \"\\n\")\nSentence 1: ('Mr.', 'Podger', ‘had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\nLabels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n\nSentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\nLabels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB','VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n\n```", "```py\n>>> for i, pair in enumerate(data.stream()):\n...     print(pair)\n...     if i > 3:\n...         break\n('Podger', 'NOUN')\n('had', 'VERB')\n('thanked', 'VERB')\n('him', 'PRON')\n```", "```py\ndef pair_counts(tags, words):\n    d = defaultdict(lambda: defaultdict(int))\n    for tag, word in zip(tags, words):\n        d[tag][word] += 1\n\n    return d\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\nwords = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n```", "```py\nFakeState = namedtuple('FakeState', 'name')\n\nclass MFCTagger:\n    missing = FakeState(name = '<MISSING>')\n\n    def __init__(self, table):\n        self.table = defaultdict(lambda: MFCTagger.missing)\n        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n\n    def viterbi(self, seq):\n        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\nwords = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n\nword_counts = pair_counts(words, tags)\nmfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())\n\nmfc_model = MFCTagger(mfc_table)\n```", "```py\ndef replace_unknown(sequence):\n    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n\ndef simplify_decoding(X, model):\n    _, state_path = model.viterbi(replace_unknown(X))\n    return [state[1].name for state in state_path[1:-1]]\n```", "```py\n>>> for key in data.testing_set.keys[:2]:\n...     print(\"Sentence Key: {}\\n\".format(key))\n...     print(\"Predicted labels:\\n-----------------\")\n...     print(simplify_decoding(data.sentences[key].words, mfc_model))\n...     print()\n...     print(\"Actual labels:\\n--------------\")\n...     print(data.sentences[key].tags)\n...     print(\"\\n\")\n```", "```py\ndef accuracy(X, Y, model):    \n    correct = total_predictions = 0\n    for observations, actual_tags in zip(X, Y):\n\n        # The model.viterbi call in simplify_decoding will return None if the HMM\n        # raises an error (for example, if a test sentence contains a word that\n        # is out of vocabulary for the training set). Any exception counts the\n        # full sentence as an error (which makes this a conservative estimate).\n        try:\n            most_likely_tags = simplify_decoding(observations, model)\n            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n        except:\n            pass\n        total_predictions += len(observations)\n    return correct / total_predictions\n```", "```py\n>>> mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n>>> print(\"Training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\nTraining accuracy mfc_model: 95.72%\n```", "```py\n>>> mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n>>> print(\"Testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\nTesting accuracy mfc_model: 93.01%\n```", "```py\ndef unigram_counts(sequences):\n    return Counter(sequences)\n\ntags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\ntag_unigrams = unigram_counts(tags)\n```", "```py\ndef bigram_counts(sequences):\n    d = Counter(sequences)\n    return d\n\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\no = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]\ntag_bigrams = bigram_counts(o)\n```", "```py\ndef starting_counts(sequences):\n    d = Counter(sequences)\n    return d\n\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\nstarts_tag = [i[0] for i in data.Y]\ntag_starts = starting_counts(starts_tag)\n```", "```py\ndef ending_counts(sequences):\n    d = Counter(sequences)\n    return d\n\nend_tag = [i[len(i)-1] for i in data.Y]\ntag_ends = ending_counts(end_tag)\n```", "```py\nbasic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n\ntags = [tag for i, (word, tag) in enumerate(data.stream())]\nwords = [word for i, (word, tag) in enumerate(data.stream())]\n\ntags_count=unigram_counts(tags)\ntag_words_count=pair_counts(tags,words)\n\nstarting_tag_list=[i[0] for i in data.Y]\nending_tag_list=[i[-1] for i in data.Y]\n\nstarting_tag_count=starting_counts(starting_tag_list)#the number of times a tag occurred at the start\nending_tag_count=ending_counts(ending_tag_list)      #the number of times a tag occurred at the end\n\nto_pass_states = []\nfor tag, words_dict in tag_words_count.items():\n    total = float(sum(words_dict.values()))\n    distribution = {word: count/total for word, count in words_dict.items()}\n    tag_emissions = DiscreteDistribution(distribution)\n    tag_state = State(tag_emissions, name=tag)\n    to_pass_states.append(tag_state)\n\nbasic_model.add_states()\n\nstart_prob={} \n\nfor tag in tags:\n    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]\n\nfor tag_state in to_pass_states :\n    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])\n\nend_prob={}\n\nfor tag in tags:\n    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\nfor tag_state in to_pass_states :\n    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])\n\ntransition_prob_pair={}\n\nfor key in tag_bigrams.keys():\n    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]\nfor tag_state in to_pass_states :\n    for next_tag_state in to_pass_states :\n        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n\nbasic_model.bake()\n```", "```py\n>>> for key in data.testing_set.keys[:2]:\n...     print(\"Sentence Key: {}\\n\".format(key))\n...     print(\"Predicted labels:\\n-----------------\")\n...     print(simplify_decoding(data.sentences[key].words, basic_model))\n...     print()\n...     print(\"Actual labels:\\n--------------\")\n...     print(data.sentences[key].tags)\n...     print(\"\\n\")\n```", "```py\n>>> hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n>>> print(\"Training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\nTraining accuracy basic hmm model: 97.49%\n\n>>> hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n>>> print(\"Testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\nTesting accuracy basic hmm model: 96.09%\n```", "```py\npip install SpeechRecognition\n```", "```py\n>>> import speech_recognition as sr\n>>> r = sr.Recognizer()\n```", "```py\n>>> harvard = sr.AudioFile('harvard.wav')\n>>> with harvard as source:\n...    audio = r.record(source)\n```", "```py\n>>> type(audio)\n<class 'speech_recognition.AudioData'>\n```", "```py\n>>> r.recognize_google(audio)\n    'the stale smell of old beer lingers it takes heat\n    to bring out the odor a cold dip restores health and\n    zest a salt pickle taste fine with ham tacos al\n    Pastore are my favorite a zestful food is the hot\n    cross bun'\n```", "```py\n>>> with harvard as source:\n...     audio_part = r.record(source, duration=4)\n\n>>> r.recognize_google(audio_part)\n'the stale smell of old beer lingers'\n```", "```py\n>>> with harvard as source:\n...     audio_offset = r.record(source, offset=4, duration=3)\n\n>>> recognizer.recognize_google(audio_offset)\n'it takes heat to bring out the odor'\n```", "```py\n>>> jackhammer = sr.AudioFile('jackhammer.wav')\n>>> with jackhammer as source:\n...     audio_jack = r.record(source)\n\n>>> r.recognize_google(audio_jack)\n'the snail smell of old gear vendors'\n```", "```py\n>>> with jackhammer as source:\n...     r.adjust_for_ambient_noise(source, duration=1)\n...     audio = r.record(source)\n\n>>> r.recognize_google(audio)\n'still smell of old beer vendors'\n```", "```py\n>>> r.recognize_google(audio, show_all=True)\n{'alternative': [\n  {'transcript': 'the snail smell like old Beer Mongers'}, \n  {'transcript': 'the still smell of old beer vendors'}, \n  {'transcript': 'the snail smell like old beer vendors'},\n  {'transcript': 'the stale smell of old beer vendors'}, \n  {'transcript': 'the snail smell like old beermongers'}, \n  {'transcript': 'destihl smell of old beer vendors'}, \n  {'transcript': 'the still smell like old beer vendors'}, \n  {'transcript': 'bastille smell of old beer vendors'}, \n  {'transcript': 'the still smell like old beermongers'}, \n  {'transcript': 'the still smell of old beer venders'}, \n  {'transcript': 'the still smelling old beer vendors'}, \n  {'transcript': 'musty smell of old beer vendors'}, \n  {'transcript': 'the still smell of old beer vendor'}\n], 'final': True}\n```", "```py\n>>> import speech_recognition as sr\n>>> r = sr.Recognizer()\n\n>>> mic = sr.Microphone()\n\n>>> with mic as source:\n...     r.adjust_for_ambient_noise(source)\n...     audio = r.listen(source)\n```", "```py\n>>> r.recognize_google(audio)\n'hello world' # This would depend on what you said in the microphone\n```"]