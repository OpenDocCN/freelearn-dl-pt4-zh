["```py\n>>> text1 = \"some text\" # this will not work for fastText\n>>> type(text1)\n<type 'str'>\n>>> text2 = unicode(\"some text\") # in fastText you will need to use this.\n>>> type(text2)\n<type 'unicode'>\n>>>\n```", "```py\n$ ./fasttext\nusage: fasttext <command> <args>\n\nThe commands supported by fasttext are:\n\n supervised train a supervised classifier\n quantize quantize a model to reduce the memory usage\n test evaluate a supervised classifier\n predict predict most likely labels\n predict-prob predict most likely labels with probabilities\n skipgram train a skipgram model\n cbow train a cbow model\n print-word-vectors print word vectors given a trained model\n print-sentence-vectors print sentence vectors given a trained model\n print-ngrams print ngrams given a trained model and word\n nn query for nearest neighbors\n analogies query for analogies\n dump dump arguments,dictionary,input/output vectors\n```", "```py\n$ ./fasttext supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n -input training file path\n -output output file path\n\nThe following arguments are optional:\n -verbose verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n -minCount minimal number of word occurences [1]\n -minCountLabel minimal number of label occurences [0]\n -wordNgrams max length of word ngram [1]\n -bucket number of buckets [2000000]\n -minn min length of char ngram [0]\n -maxn max length of char ngram [0]\n -t sampling threshold [0.0001]\n -label labels prefix [__label__]\n\nThe following arguments for training are optional:\n -lr learning rate [0.1]\n -lrUpdateRate change the rate of updates for the learning rate [100]\n -dim size of word vectors [100]\n -ws size of the context window [5]\n -epoch number of epochs [5]\n -neg number of negatives sampled [5]\n -loss loss function {ns, hs, softmax} [softmax]\n -thread number of threads [12]\n -pretrainedVectors pretrained word vectors for supervised learning []\n -saveOutput whether output params should be saved [false]\n\nThe following arguments for quantization are optional:\n -cutoff number of words and ngrams to retain [0]\n -retrain whether embeddings are finetuned if a cutoff is applied [false]\n -qnorm whether the norm is quantized separately [false]\n -qout whether the classifier is quantized [false]\n -dsub size of each sub-vector [2]\n```", "```py\n$ ./fasttext skipgram\nEmpty input or output path.\n\nThe following arguments are mandatory:\n -input training file path\n -output output file path\n\nThe following arguments are optional:\n -verbose verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n -minCount minimal number of word occurences [5]\n -minCountLabel minimal number of label occurences [0]\n -wordNgrams max length of word ngram [1]\n -bucket number of buckets [2000000]\n -minn min length of char ngram [3]\n -maxn max length of char ngram [6]\n -t sampling threshold [0.0001]\n -label labels prefix [__label__]\n\nThe following arguments for training are optional:\n -lr learning rate [0.05]\n -lrUpdateRate change the rate of updates for the learning rate [100]\n -dim size of word vectors [100]\n -ws size of the context window [5]\n -epoch number of epochs [5]\n -neg number of negatives sampled [5]\n -loss loss function {ns, hs, softmax} [ns]\n -thread number of threads [12]\n -pretrainedVectors pretrained word vectors for supervised learning []\n -saveOutput whether output params should be saved [false]\n\nThe following arguments for quantization are optional:\n -cutoff number of words and ngrams to retain [0]\n -retrain whether embeddings are finetuned if a cutoff is applied [false]\n -qnorm whether the norm is quantized separately [false]\n -qout whether the classifier is quantized [false]\n -dsub size of each sub-vector [2]\n```", "```py\n$ ./fasttext cbow\nEmpty input or output path.\n\nThe following arguments are mandatory:\n -input training file path\n -output output file path\n\nThe following arguments are optional:\n -verbose verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n -minCount minimal number of word occurences [5]\n -minCountLabel minimal number of label occurences [0]\n -wordNgrams max length of word ngram [1]\n -bucket number of buckets [2000000]\n -minn min length of char ngram [3]\n -maxn max length of char ngram [6]\n -t sampling threshold [0.0001]\n -label labels prefix [__label__]\n\nThe following arguments for training are optional:\n -lr learning rate [0.05]\n -lrUpdateRate change the rate of updates for the learning rate [100]\n -dim size of word vectors [100]\n -ws size of the context window [5]\n -epoch number of epochs [5]\n -neg number of negatives sampled [5]\n -loss loss function {ns, hs, softmax} [ns]\n -thread number of threads [12]\n -pretrainedVectors pretrained word vectors for supervised learning []\n -saveOutput whether output params should be saved [false]\n\nThe following arguments for quantization are optional:\n -cutoff number of words and ngrams to retain [0]\n -retrain whether embeddings are finetuned if a cutoff is applied [false]\n -qnorm whether the norm is quantized separately [false]\n -qout whether the classifier is quantized [false]\n -dsub size of each sub-vector [2]\n```"]