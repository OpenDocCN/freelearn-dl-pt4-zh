<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Recurrent Neural Network — RNN</h1>
            </header>

            <article>
                
<p>In <a href="4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml" target="_blank">Chapter 3</a>, <em>Deep Learning with ConvNets</em>, we learned about <strong>convolutional neural networks</strong> (CNN) and saw how they exploit the spatial geometry of their input. For example, CNNs apply convolution and pooling operations in one dimension for audio and text data along the time dimension, in two dimensions for images along the (height x width) dimensions and in three dimensions, for videos along the (height x width x time) dimensions.</p>
<p>In this chapter, we will learn about <strong>recurrent neural networks</strong> (<strong>RNN</strong>), a class of neural networks that exploit the sequential nature of their input. Such inputs could be text, speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it. For example, the next word in the sentence <em>the dog...</em> is more likely to be <em>barks</em> than <em>car</em>, therefore, given such a sequence, an RNN is more likely to predict <em>barks</em> than <em>car</em>.</p>
<p>An RNN can be thought of as a graph of RNN cells, where each cell performs the same operation on every element in the sequence. RNNs are very flexible and have been used to solve problems such as speech recognition, language modeling, machine translation, sentiment analysis, and image captioning, to name a few. RNNs can be adapted to different types of problems by rearranging the way the cells are arranged in the graph. We will see some examples of these configurations and how they are used to solve specific problems.</p>
<p>We will also learn about a major limitation of the SimpleRNN cell, and how two variants of the SimpleRNN cell—<strong>long short term memory</strong> (<strong>LSTM</strong>) and <strong>gated recurrent unit</strong> (<strong>GRU</strong>)—overcome this limitation. Both LSTM and GRU are drop-in replacements for the SimpleRNN cell, so just replacing the RNN cell with one of these variants can often result in a major performance improvement in your network. While LSTM and GRU are not the only variants, it has been shown empirically (for more information refer to the articles: <em>An Empirical Exploration of Recurrent Network Architectures</em>, by R. Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015 and <em>LSTM: A Search Space Odyssey</em>, by K. Greff, arXiv:1503.04069, 2015) that they are the best choices for most sequence problems.</p>
<p>Finally, we will also learn about some tips to improve the performance of our RNNs and when and how to apply them.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>SimpleRNN cell</li>
<li>Basic RNN implementation in Keras in generating text</li>
<li>RNN topologies</li>
<li>LSTM, GRU, and other RNN variants</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">SimpleRNN cells</h1>
            </header>

            <article>
                
<p>Traditional multilayer perceptron neural networks make the assumption that all inputs are independent of each other. This assumption breaks down in the case of sequence data. You have already seen the example in the previous section where the first two words in the sentence affect the third. The same idea is true of speech—if we are having a conversation in a noisy room, I can make reasonable guesses about a word I may not have understood based on the words I have heard so far. Time series data, such as stock prices or weather, also exhibit a dependence on past data, called the secular trend.</p>
<p>RNN cells incorporate this dependence by having a hidden state, or memory, that holds the essence of what has been seen so far. The value of the hidden state at any point in time is a function of the value of the hidden state at the previous time step and the value of the input at the current time step, that is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="15" src="assets/rnn-eq1.png" width="106"/></div>
<p><em>h<sub>t</sub></em> and <em>h<sub>t-1</sub></em> are the values of the hidden states at the time steps <em>t</em> and <em>t-1</em> respectively, and <em>x<sub>t</sub></em> is the value of the input at time <em>t</em>. Notice that the equation is recursive, that is, <em>h<sub>t-1</sub></em> can be represented in terms of <em>h<sub>t-2</sub></em> and <em>x<sub>t-1</sub></em>, and so on, until the beginning of the sequence. This is how RNNs encode and incorporate information from arbitrarily long sequences.</p>
<p>We can also represent the RNN cell graphically as shown in the following diagram on the left. At time <em>t</em>, the cell has an input <em>x<sub>t</sub></em> and an output <em>y<sub>t</sub></em>. Part of the output <em>y<sub>t</sub></em> (the hidden state <em>h<sub>t</sub></em>) is fed back into the cell for use at a later time step <em>t+1</em>. Just as a traditional neural network's parameters are contained in its weight matrix, the RNN's parameters are defined by three weight matrices <em>U</em>, <em>V</em>, and <em>W</em>, corresponding to the input, output, and hidden state respectively:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="185" src="assets/basic-rnn-1.png" width="343"/></div>
<p>Another way to look at an RNN to <em>unroll</em> it, as shown in the preceding diagram on the right. Unrolling means that we draw the network out for the complete sequence. The network shown here is a three-layer RNN, suitable for processing three element sequences. Notice that the weight matrices <em>U</em>, <em>V</em>, and <em>W</em> are shared across the steps. This is because we are applying the same operation on different inputs at each time step. Being able to share these weight vectors across all the time steps greatly reduces the number of parameters that the RNN needs to learn.</p>
<p>We can also describe the computations within an RNN in terms of equations. The internal state of the RNN at a time <em>t</em> is given by the value of the hidden vector <em>h<sub>t</sub></em>, which is the sum of the product of the weight matrix <em>W</em> and the hidden state <em>h<sub>t-1</sub></em> at time <em>t-1</em> and the product of the weight matrix <em>U</em> and the input <em>x<sub>t</sub></em> at time <em>t</em>, passed through the <em>tanh</em> nonlinearity. The choice of <em>tanh</em> over other nonlinearities has to do with its second derivative decaying very slowly to zero. This keeps the gradients in the linear region of the activation function and helps combat the vanishing gradient problem. We will learn more about the vanishing gradient problem later in this chapter.</p>
<p>The output vector <em>y<sub>t</sub></em> at time <em>t</em> is the product of the weight matrix <em>V</em> and the hidden state <em>h<sub>t</sub></em>, with <em>softmax</em> applied to the product so the resulting vector is a set of output probabilities:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" src="assets/rnn-eq2.png" width="166"/></div>
<p>Keras provides the SimpleRNN (for more information refer to: <a href="https://keras.io/layers/recurrent/" target="_blank">https://keras.io/layers/recurrent/</a>) recurrent layer that incorporates all the logic we have seen so far, as well as the more advanced variants such as LSTM and GRU that we will see later in this chapter, so it is not strictly necessary to understand how they work in order to start building with them. However, an understanding of the structure and equations is helpful when you need to compose your own RNN to solve a given problem.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">SimpleRNN with Keras — generating text</h1>
            </header>

            <article>
                
<p>RNNs have been used extensively by the <strong>natural language processing</strong> (<strong>NLP</strong>) community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.</p>
<p>A side effect of the ability to predict the next word given previous words is a generative model that allows us to generate text by sampling from the output probabilities. In language modeling, our input is typically a sequence of words and the output is a sequence of predicted words. The training data used is existing unlabeled text, where we set the label <em>y<sub>t</sub></em> at time <em>t</em> to be the input <em>x<sub>t+1</sub></em> at time <em>t+1</em>.</p>
<p>For our first example of using Keras for building RNNs, we will train a character based language model on the text of <em>Alice in Wonderland</em> to predict the next character given 10 previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. The idea is the same as using a word-based language model, except we use characters instead of words. We will then use the trained model to generate some text in the same style.</p>
<p>First we import the necessary modules:</p>
<pre>
from __future__ import print_function<br/>from keras.layers import Dense, Activation<br/>from keras.layers.recurrent import SimpleRNN<br/>from keras.models import Sequential<br/>from keras.utils.visualize_util import plot<br/>import numpy as np
</pre>
<p>We read our input text from the text of <em>Alice in Wonderland</em> on the Project Gutenberg website (<a href="http://www.gutenberg.org/files/11/11-0.txt" target="_blank">http://www.gutenberg.org/files/11/11-0.txt</a>). The file contains line breaks and non-ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called <kbd>text</kbd>:</p>
<pre>
fin = open("../data/alice_in_wonderland.txt", 'rb')<br/>lines = []<br/>for line in fin:<br/>    line = line.strip().lower()<br/>    line = line.decode("ascii", "ignore")<br/>    if len(line) == 0:<br/>        continue<br/>    lines.append(line)<br/>fin.close()<br/>text = " ".join(lines)
</pre>
<p>Since we are building a character-level RNN, our vocabulary is the set of characters that occur in the text. There are 42 of them in our case. Since we will be dealing with the indexes to these characters rather than the characters themselves, the following code snippet creates the necessary lookup tables:</p>
<pre>
chars = set([c for c in text])<br/>nb_chars = len(chars)<br/>char2index = dict((c, i) for i, c in enumerate(chars))<br/>index2char = dict((i, c) for i, c in enumerate(chars))
</pre>
<p>The next step is to create the input and label texts. We do this by stepping through the text by a number of characters given by the <kbd>STEP</kbd> variable (<kbd>1</kbd> in our case) and then extracting a span of text whose size is determined by the <kbd>SEQLEN</kbd> variable (<kbd>10</kbd> in our case). The next character after the span is our label character:</p>
<pre>
SEQLEN = 10<br/>STEP = 1<br/><br/>input_chars = []<br/>label_chars = []<br/>for i in range(0, len(text) - SEQLEN, STEP):<br/>    input_chars.append(text[i:i + SEQLEN])<br/>    label_chars.append(text[i + SEQLEN])
</pre>
<p>Using the preceding code, the input and label texts for the text <kbd>it turned into a pig</kbd> would look like this:</p>
<pre>
<strong>it turned -&gt; i</strong><br/><strong> t turned i -&gt; n</strong><br/><strong> turned in -&gt; t</strong><br/><strong>turned int -&gt; o</strong><br/><strong>urned into -&gt;</strong><br/><strong>rned into -&gt; a</strong><br/><strong>ned into a -&gt;</strong><br/><strong>ed into a -&gt; p</strong><br/><strong>d into a p -&gt; i</strong><br/><strong> into a pi -&gt; g</strong>
</pre>
<p>The next step is to vectorize these input and label texts. Each row of the input to the RNN corresponds to one of the input texts shown previously. There are <kbd>SEQLEN</kbd> characters in this input, and since our vocabulary size is given by <kbd>nb_chars</kbd>, we represent each input character as a one-hot encoded vector of size (<kbd>nb_chars</kbd>). Thus each input row is a tensor of size (<kbd>SEQLEN</kbd> and <kbd>nb_chars</kbd>). Our output label is a single character, so similar to the way we represent each character of our input, it is represented as a one-hot vector of size (<kbd>nb_chars</kbd>). Thus, the shape of each label is <kbd>nb_chars</kbd>:</p>
<pre>
X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)<br/>y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)<br/>for i, input_char in enumerate(input_chars):<br/>    for j, ch in enumerate(input_char):<br/>        X[i, j, char2index[ch]] = 1<br/>    y[i, char2index[label_chars[i]]] = 1
</pre>
<p>Finally, we are ready to build our model. We define the RNN's output dimension to have a size of 128. This is a hyper-parameter that needs to be determined by experimentation. In general, if we choose too small a size, then the model does not have sufficient capacity for generating good text, and you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if the value chosen is too large, the model has too many parameters and needs a lot more data to train effectively. We want to return a single character as output, not a sequence of characters, so <kbd>return_sequences=False</kbd>. We have already seen that the input to the RNN is of shape (<kbd>SEQLEN</kbd> and <kbd>nb_chars</kbd>). In addition, we set <kbd>unroll=True</kbd> because it improves performance on the TensorFlow backend.</p>
<p>The RNN is connected to a dense (fully connected) layer. The dense layer has (<kbd>nb_char</kbd>) units, which emits scores for each of the characters in the vocabulary. The activation on the dense layer is a softmax, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction. We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer:</p>
<pre>
HIDDEN_SIZE = 128<br/>BATCH_SIZE = 128<br/>NUM_ITERATIONS = 25<br/>NUM_EPOCHS_PER_ITERATION = 1<br/>NUM_PREDS_PER_EPOCH = 100<br/><br/>model = Sequential()<br/>model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,<br/>    input_shape=(SEQLEN, nb_chars),<br/>    unroll=True))<br/>model.add(Dense(nb_chars))<br/>model.add(Activation("softmax"))<br/><br/>model.compile(loss="categorical_crossentropy", optimizer="rmsprop")
</pre>
<p>Our training approach is a little different from what we have seen so far. So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data. Since we don't have any labeled data here, we train the model for an epoch (<kbd>NUM_EPOCHS_PER_ITERATION=1</kbd>) then test it. We continue training like this for 25 (<kbd>NUM_ITERATIONS=25</kbd>) iterations, stopping once we see intelligible output. So effectively, we are training for <kbd>NUM_ITERATIONS</kbd> epochs and testing the model after each epoch. </p>
<p>Our test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. We continue this 100 times (<kbd>NUM_PREDS_PER_EPOCH=100</kbd>) and generate and print the resulting string. The string gives us an indication of the quality of the model:</p>
<pre>
for iteration in range(NUM_ITERATIONS):<br/>    print("=" * 50)<br/>    print("Iteration #: %d" % (iteration))<br/>    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)<br/><br/>    test_idx = np.random.randint(len(input_chars))<br/>    test_chars = input_chars[test_idx]<br/>    print("Generating from seed: %s" % (test_chars))<br/>    print(test_chars, end="")<br/>    for i in range(NUM_PREDS_PER_EPOCH):<br/>        Xtest = np.zeros((1, SEQLEN, nb_chars))<br/>        for i, ch in enumerate(test_chars):<br/>            Xtest[0, i, char2index[ch]] = 1<br/>        pred = model.predict(Xtest, verbose=0)[0]<br/>        ypred = index2char[np.argmax(pred)]<br/>        print(ypred, end="")<br/>        # move forward with test_chars + ypred<br/>        test_chars = test_chars[1:] + ypred<br/>print()
</pre>
<p>The output of this run is shown as follows. As you can see, the model starts out predicting gibberish, but by the end of the 25th epoch, it has learned to spell reasonably well, although it has trouble expressing coherent thoughts. The amazing thing about this model is that it is character-based and has no knowledge of words, yet it learns to spell words that look like they might have come from the original text:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="354" src="assets/ss-6-1.png" width="692"/></div>
<p>Generating the next character or next word of text is not the only thing you can do with this sort of model. This kind of model has been successfully used to make stock predictions (for more information refer to the article: <span><em>Financial Market Time Series Prediction with Recurrent Neural Networks</em>, by A. Bernal, S. Fok, and R. Pidaparthi, 2012</span>) and generate classical music (<span>for more information refer to the article: <em>DeepBach: A Steerable Model for Bach Chorales Generation</em>, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016</span>), to name a few interesting applications. Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: <em>The Unreasonable Effectiveness of Recurrent Neural Networks</em> at <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.</p>
<p>The source code for this example is available in <kbd>alice_chargen_rnn.py</kbd> in the code download for the chapter. The data is available from Project Gutenberg.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">RNN topologies</h1>
            </header>

            <article>
                
<p>The APIs for MLP and CNN architectures are limited. Both architectures accept a fixed-size tensor as input and produce a fixed-size tensor as output; and they perform the transformation from input to output in a fixed number of steps given by the number of layers in the model. RNNs don't have this limitation—you can have sequences in the input, the output, or both. This means that RNNs can be arranged in many ways to solve specific problems.</p>
<p>As we have learned, RNNs combine the input vector with the previous state vector to produce a new state vector. This can be thought of as similar to running a program with some inputs and some internal variables. Thus RNNs can be thought of as essentially describing computer programs. In fact, it has been shown that RNNs are turing complete (<span>for more information refer to the article: <em>On the Computational Power of Neural Nets</em>, by H. T. Siegelmann and E. D. Sontag, proceedings of the fifth annual workshop on computational learning theory, ACM, 1992.</span>) in the sense that given the proper weights, they can simulate arbitrary programs.</p>
<p>This property of being able to work with sequences gives rise to a number of common topologies, some of which we'll discuss, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="289" src="assets/rnn-toplogies.png" width="457"/></div>
<p>All these different topologies derive from the same basic structure shown in the preceding diagram. In this basic topology, all input sequences are of the same length and an output is produced at each time step. We have already seen an example of this with our character level RNN for generating words in <em>Alice in Wonderland</em>.</p>
<p>Another example of a many to many RNN could be a machine translation network shown as <strong>(b)</strong>, part of a general family of networks called sequence-to-sequence (for more information refer to: <span><em>Grammar as a Foreign Language</em>, by O. Vinyals, Advances in Neural Information Processing Systems, 2015</span>). These take in a sequence and produces another sequence. In the case of machine translation, the input could be a sequence of English words in a sentence and the output could be the words in a translated Spanish sentence. In the case of a model that uses sequence-to-sequence to do <strong>part-of-speech</strong> (<strong>POS</strong>) tagging, the input could be the words in a sentence and the output could be the corresponding POS tags. It differs from the previous topology in that at certain time steps there is no input and at others there is no output. We will see an example of such a network later in this chapter.</p>
<p>Other variants are the one-to-many network shown as <strong>(c)</strong>, an example of which could be an image captioning network (for more information refer to the article: <span><em>Deep Visual-Semantic Alignments for Generating Image Descriptions</em>, by A. Karpathy, and F. Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.</span>), where the input is an image and the output a sequence of words.</p>
<p>Similarly, an example of a many-to-one network as shown in <strong>(d)</strong> could be a network that does sentiment analysis of sentences, where the input is a sequence of words and the output is a positive or negative sentiment (<span>for more information refer to the article: <em>Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank</em>, by R. Socher, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Vol. 1631, 2013</span>). We will see an (much simplified compared to the cited model) example of this topology as well later in the chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Vanishing and exploding gradients</h1>
            </header>

            <article>
                
<p>Just like traditional neural networks, training the RNN also involves backpropagation. The difference in this case is that since the parameters are shared by all time steps, the gradient at each output depends not only on the current time step, but also on the previous ones. This process is called <strong>backpropagation through time</strong> (<strong>BPTT</strong>) (for more information refer to the article: <span><em>Learning Internal Representations by Backpropagating errors</em>, by G. E. Hinton, D. E. Rumelhart, and R. J. Williams, Parallel Distributed Processing: Explorations in the Microstructure of Cognition 1, 1985</span>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/rnn-bptt.png"/></div>
<p>Consider the small three layer RNN shown in the preceding diagram. During the forward propagation (shown by the solid lines), the network produces predictions that are compared to the labels to compute a loss <em>L<sub>t</sub></em> at each time step. During backpropagation (shown by dotted lines), the gradients of the loss with respect to the parameters <em>U</em>, <em>V</em>, and <em>W</em> are computed at each time step and the parameters are updated with the sum of the gradients.</p>
<p>The following equation shows the gradient of the loss with respect to <em>W</em>, the matrix that encodes weights for the long term dependencies. We focus on this part of the update because it is the cause of the vanishing and exploding gradient problem. The other two gradients of the loss with respect to the matrices <em>U</em> and <em>V</em> are also summed up across all time steps in a similar way:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" src="assets/bptt-eq1.png" width="105"/></div>
<p>Let us now look at what happens to the gradient of the loss at the last time step (<em>t=3</em>). As you can see, this gradient can be decomposed to a product of three sub gradients using the chain rule. The gradient of the hidden state <em>h2</em> with respect to <em>W</em> can be further decomposed as the sum of the gradient of each hidden state with respect to the previous one. Finally, each gradient of the hidden state with respect to the previous one can be further decomposed as the product of gradients of the current hidden state against the previous one:</p>
<div class="CDPAlignCenter CDPAlign"><img height="142" src="assets/bptt-eq2.png" width="276"/></div>
<p>Similar calculations are done to compute the gradient of losses <em>L<sub>1</sub></em> and <em>L<sub>2</sub></em> (at time steps 1 and 2) with respect to <em>W</em> and to sum them into the gradient update for <em>W</em>. We will not explore the math further in this book. If you want to do so on your own, this WILDML blog post (<a href="https://goo.gl/l06lbX" target="_blank">https://goo.gl/l06lbX</a>) has a very good explanation of BPTT, including more detailed derivations of the mathematics behind the process.</p>
<p>For our purposes, the final form of the gradient in the equation above tells us why RNNs have the problem of vanishing and exploding gradients. Consider the case where the individual gradients of a hidden state with respect to the previous one is less than one. As we backpropagate across multiple time steps, the product of gradients get smaller and smaller, leading to the problem of vanishing gradients. Similarly, if the gradients are larger than one, the products get larger and larger, leading to the problem of exploding gradients.</p>
<p>The effect of vanishing gradients is that the gradients from steps that are far away do not contribute anything to the learning process, so the RNN ends up not learning long range dependencies. Vanishing gradients can happen for traditional neural networks as well, it is just more visible in case of RNNs, since RNNs tend to have many more layers (time steps) over which back propagation must occur.</p>
<p>Exploding gradients are more easily detectable, the gradients will become very large and then turn into <strong>not a number</strong> (<strong>NaN</strong>) and the training process will crash. Exploding gradients can be controlled by clipping them at a predefined threshold as discussed in the paper: <span><em>On the Difficulty of Training Recurrent Neural Networks</em>, by R. Pascanu, T. Mikolov, and Y. Bengio, ICML, Pp 1310-1318, 2013.</span></p>
<p>While there are a few approaches to minimize the problem of vanishing gradients, such as proper initialization of the <em>W</em> matrix, using a ReLU instead of <em>tanh</em> layers, and pre-training the layers using unsupervised methods, the most popular solution is to use the LSTM or GRU architectures. These architectures have been designed to deal with the vanishing gradient problem and learn long term dependencies more effectively. We will learn more about LSTM and GRU architectures later in this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Long short term memory — LSTM</h1>
            </header>

            <article>
                
<p>The LSTM is a variant of RNN that is capable of learning long term dependencies. LSTMs were first proposed by Hochreiter and Schmidhuber and refined by many other researchers. They work well on a large variety of problems and are the most widely used type of RNN.</p>
<p>We have seen how the SimpleRNN uses the hidden state from the previous time step and the current input in a <em>tanh</em> layer to implement recurrence. LSTMs also implement recurrence in a similar way, but instead of a single <em>tanh</em> layer, there are four layers interacting in a very specific way. The following diagram illustrates the transformations that are applied to the hidden state at time step <em>t</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="295" src="assets/lstm-cell.png" width="462"/></div>
<p>The diagram looks complicated, but let us look at it component by component. The line across the top of the diagram is the cell state <em>c</em>, and represents the internal memory of the unit. The line across the bottom is the hidden state, and the <em>i</em>, <em>f</em>, <em>o</em>, and <em>g</em> gates are the mechanism by which the LSTM works around the vanishing gradient problem. During training, the LSTM learns the parameters for these gates.</p>
<p>In order to gain a deeper understanding of how these gates modulate the LSTM's hidden state, let us consider the equations that show how it calculates the hidden state <em>h<sub>t</sub></em> at time <em>t</em> from the hidden state <em>h<sub>t-1</sub></em> at the previous time step:</p>
<div class="CDPAlignCenter CDPAlign"><img height="138" src="assets/lstm-eq1.png" width="185"/></div>
<p>Here <em>i</em>, <em>f</em>, and <em>o</em> are the input, forget, and output gates. They are computed using the same equations but with different parameter matrices. The sigmoid function modulates the output of these gates between zero and one, so the output vector produced can be multiplied element-wise with another vector to define how much of the second vector can pass through the first one.</p>
<p>The forget gate defines how much of the previous state <em>h<sub>t-1</sub></em> you want to allow to pass through. The input gate defines how much of the newly computed state for the current input <em>x<sub>t</sub></em> you want to let through, and the output gate defines how much of the internal state you want to expose to the next layer. The internal hidden state <em>g</em> is computed based on the current input <em>x<sub>t</sub></em> and the previous hidden state <em>h<sub>t-1</sub></em>. Notice that the equation for <em>g</em> is identical to that for the SimpleRNN cell, but in this case we will modulate the output by the output of the input gate <em>i</em>.</p>
<p>Given <em>i</em>, <em>f</em>, <em>o</em>, and <em>g</em>, we can now calculate the cell state <em>c<sub>t</sub></em> at time <em>t</em> in terms of <em>c<sub>t-1</sub></em> at time (<em>t-1</em>) multiplied by the forget gate and the state <em>g</em> multiplied by the input gate <em>i</em>. So this is basically a way to combine the previous memory and the new input—setting the forget gate to <em>0</em> ignores the old memory and setting the input gate to <em>0</em> ignores the newly computed state.</p>
<p>Finally, the hidden state <em>h<sub>t</sub></em> at time <em>t</em> is computed by multiplying the memory <em>c<sub>t</sub></em> with the output gate.</p>
<p>One thing to realize is that an LSTM is a drop-in replacement for a SimpleRNN cell, the only difference is that LSTMs are resistant to the vanishing gradient problem. You can replace an RNN cell in a network with an LSTM without worrying about any side effects. You should generally see better results along with longer training times.</p>
<p>If you would like to know more, WILDML blog post has a very detailed explanation of these LSTM gates and how they work. For a more visual explanation, take a look at Christopher Olah's blog post: <em>Understanding LSTMs</em> (<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>) where he walks you step by step through these computations, with illustrations at each step.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">LSTM with Keras — sentiment analysis</h1>
            </header>

            <article>
                
<p>Keras provides an LSTM layer that we will use here to construct and train a many-to-one RNN. Our network takes in a sentence (a sequence of words) and outputs a sentiment value (positive or negative). Our training set is a dataset of about 7,000 short sentences from UMICH SI650 sentiment classification competition on Kaggle (<a href="https://inclass.kaggle.com/c/si650winter11" target="_blank">https://inclass.kaggle.com/c/si650winter11</a>). Each sentence is labeled <em>1</em> or <em>0</em> for positive or negative sentiment respectively, which our network will learn to predict.</p>
<p>We start with the imports, as usual:</p>
<pre>
from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers.recurrent import LSTM<br/>from keras.models import Sequential<br/>from keras.preprocessing import sequence<br/>from sklearn.model_selection import train_test_split<br/>import collections<br/>import matplotlib.pyplot as plt<br/>import nltk<br/>import numpy as np<br/>import os
</pre>
<p>Before we start, we want to do a bit of exploratory analysis on the data. Specifically we need to know how many unique words there are in the corpus and how many words are there in each sentence:</p>
<pre>
maxlen = 0<br/>word_freqs = collections.Counter()<br/>num_recs = 0<br/>ftrain = open(os.path.join(DATA_DIR, "umich-sentiment-train.txt"), 'rb')<br/>for line in ftrain:<br/>    label, sentence = line.strip().split("t")<br/>    words = nltk.word_tokenize(sentence.decode("ascii", "ignore").lower())<br/>    if len(words) &gt; maxlen:<br/>        maxlen = len(words)<br/>    for word in words:<br/>        word_freqs[word] += 1<br/>    num_recs += 1<br/>ftrain.close()
</pre>
<p>Using this, we get the following estimates for our corpus:</p>
<pre>
<strong>maxlen : 42</strong><br/><strong>len(word_freqs) : 2313</strong>
</pre>
<p>Using the number of unique words <kbd>len(word_freqs)</kbd>, we set our vocabulary size to a fixed number and treat all the other words as <strong>out of vocabulary</strong> (<strong>OOV</strong>) words and replace them with the pseudo-word UNK (for unknown). At prediction time, this will allow us to handle previously unseen words as OOV words as well.</p>
<p>The number of words in the sentence (<kbd>maxlen</kbd>) allows us to set a fixed sequence length and zero pad shorter sentences and truncate longer sentences to that length as appropriate. Even though RNNs handle variable sequence length, this is usually achieved either by padding and truncating as above, or by grouping the inputs in different batches by sequence length. We will use the former approach here. For the latter approach, Keras recommends using batches of size one (for more information refer to: <a href="https://github.com/fchollet/keras/issues/40" target="_blank">https://github.com/fchollet/keras/issues/40</a>).</p>
<p>Based on the preceding estimates, we set our <kbd>VOCABULARY_SIZE</kbd> to <kbd>2002</kbd>. This is 2,000 words from our vocabulary plus the UNK pseudo-word and the PAD pseudo word (used for padding sentences to a fixed number of words), in our case 40 given by <kbd>MAX_SENTENCE_LENGTH</kbd>:</p>
<pre>
DATA_DIR = "../data"<br/><br/>MAX_FEATURES = 2000<br/>MAX_SENTENCE_LENGTH = 40
</pre>
<p>Next we need a pair of lookup tables. Each row of input to the RNN is a sequence of word indices, where the indices are ordered by most frequent to least frequent word in the training set. The two lookup tables allow us to lookup an index given the word and the word given the index. This includes the <kbd>PAD</kbd> and <kbd>UNK</kbd> pseudo-words as well:</p>
<pre>
vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2<br/>word2index = {x[0]: i+2 for i, x in<br/>enumerate(word_freqs.most_common(MAX_FEATURES))}<br/>word2index["PAD"] = 0<br/>word2index["UNK"] = 1<br/>index2word = {v:k for k, v in word2index.items()}
</pre>
<p>Next, we convert our input sentences to word index sequences, pad them to the <kbd>MAX_SENTENCE_LENGTH</kbd> words. Since our output label in this case is binary (positive or negative sentiment), we don't need to process the labels:</p>
<pre>
X = np.empty((num_recs, ), dtype=list)<br/>y = np.zeros((num_recs, ))<br/>i = 0<br/>ftrain = open(os.path.join(DATA_DIR, "umich-sentiment-train.txt"), 'rb')<br/>for line in ftrain:<br/>    label, sentence = line.strip().split("t")<br/>    words = nltk.word_tokenize(sentence.decode("ascii", "ignore").lower())<br/>    seqs = []<br/>    for word in words:<br/>        if word2index.has_key(word):<br/>            seqs.append(word2index[word])<br/>        else:<br/>            seqs.append(word2index["UNK"])<br/>    X[i] = seqs<br/>    y[i] = int(label)<br/>    i += 1<br/>ftrain.close()<br/>X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)
</pre>
<p>Finally, we split the training set into a 80-20 training test split:</p>
<pre>
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)
</pre>
<p>The following diagram shows the structure of our RNN:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="291" src="assets/sentiment-lstm.png" width="319"/></div>
<p>The input for each row is a sequence of word indices. The sequence length is given by <kbd>MAX_SENTENCE_LENGTH</kbd>. The first dimension of the tensor is set to <kbd>None</kbd> to indicate that the batch size (the number of records fed to the network each time) is currently unknown at definition time; it is specified during run time using the <kbd>batch_size</kbd> parameter. So assuming an as-yet undetermined batch size, the shape of the input tensor is <kbd>(None, MAX_SENTENCE_LENGTH, 1)</kbd>. These tensors are fed into an embedding layer of size <kbd>EMBEDDING_SIZE</kbd> whose weights are initialized with small random values and learned during training. This layer will transform the tensor to a shape <kbd>(None,MAX_SENTENCE_LENGTH, EMBEDDING_SIZE)</kbd>. The output of the embedding layer is fed into an LSTM with sequence length <kbd>MAX_SENTENCE_LENGTH</kbd> and output layer size <kbd>HIDDEN_LAYER_SIZE</kbd>, so the output of the LSTM is a tensor of shape <kbd>(None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH)</kbd>. By default, the LSTM will output a single tensor of shape <kbd>(None, HIDDEN_LAYER_SIZE)</kbd> at its last sequence (<kbd>return_sequences=False</kbd>). This is fed to a dense layer with output size of <kbd>1</kbd> with a sigmoid activation function, so it will output either <kbd>0</kbd> (negative review) or <kbd>1</kbd> (positive review).</p>
<p>We compile the model using the binary cross-entropy loss function since it predicts a binary value, and the Adam optimizer, a good general purpose optimizer. Note that the hyperparameters <kbd>EMBEDDING_SIZE</kbd>, <kbd>HIDDEN_LAYER_SIZE</kbd>, <kbd>BATCH_SIZE</kbd> and <kbd>NUM_EPOCHS</kbd> (set as constants as follows) were tuned experimentally over several runs:</p>
<pre>
EMBEDDING_SIZE = 128<br/>HIDDEN_LAYER_SIZE = 64<br/>BATCH_SIZE = 32<br/>NUM_EPOCHS = 10<br/><br/>model = Sequential()<br/>model.add(Embedding(vocab_size, EMBEDDING_SIZE,<br/>input_length=MAX_SENTENCE_LENGTH))<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))<br/>model.add(Dense(1))<br/>model.add(Activation("sigmoid"))<br/><br/>model.compile(loss="binary_crossentropy", optimizer="adam",<br/>    metrics=["accuracy"])
</pre>
<p>We then train the network for <kbd>10</kbd> epochs (<kbd>NUM_EPOCHS</kbd>) and batch size of <kbd>32</kbd> (<kbd>BATCH_SIZE</kbd>). At each epoch we validate the model using the test data:</p>
<pre>
history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,<br/>    validation_data=(Xtest, ytest))
</pre>
<p>The output of this step shows how the loss decreases and accuracy increases over multiple epochs:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="311" src="assets/ss-6-2.png" width="617"/></div>
<p>We can also plot the loss and accuracy values over time using the following code:</p>
<pre>
plt.subplot(211)<br/>plt.title("Accuracy")<br/>plt.plot(history.history["acc"], color="g", label="Train")<br/>plt.plot(history.history["val_acc"], color="b", label="Validation")<br/>plt.legend(loc="best")<br/><br/>plt.subplot(212)<br/>plt.title("Loss")<br/>plt.plot(history.history["loss"], color="g", label="Train")<br/>plt.plot(history.history["val_loss"], color="b", label="Validation")<br/>plt.legend(loc="best")<br/><br/>plt.tight_layout()<br/>plt.show()
</pre>
<p>The output of the preceding example is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/umich-lossplot.png"/></div>
<p>Finally, we evaluate our model against the full test set and print the score and accuracy. We also pick a few random sentences from our test set and print the RNN's prediction, the label and the actual sentence:</p>
<pre>
score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)<br/>print("Test score: %.3f, accuracy: %.3f" % (score, acc))<br/><br/>for i in range(5):<br/>    idx = np.random.randint(len(Xtest))<br/>    xtest = Xtest[idx].reshape(1,40)<br/>    ylabel = ytest[idx]<br/>    ypred = model.predict(xtest)[0][0]<br/>    sent = " ".join([index2word[x] for x in xtest[0].tolist() if x != 0])<br/>    print("%.0ft%dt%s" % (ypred, ylabel, sent))
</pre>
<p>As you can see from the results, we get back close to 99% accuracy. The predictions the model makes for this particular set match exactly with the labels, although this is not the case for all predictions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="167" src="assets/ss-6-3.png" width="710"/></div>
<p>If you would like to run this code locally, you need to get the data from the Kaggle website.</p>
<p>The source code for this example is available in the file <kbd>umich_sentiment_lstm.py</kbd> in the code download for this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Gated recurrent unit — GRU</h1>
            </header>

            <article>
                
<p>The GRU is a variant of the LSTM and was introduced by K. Cho (for more information refer to: <span><em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em>, by K. Cho, arXiv:1406.1078, 2014</span>). It retains the LSTM's resistance to the vanishing gradient problem, but its internal structure is simpler, and therefore is faster to train, since fewer computations are needed to make updates to its hidden state. The gates for a GRU cell are illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="256" src="assets/gru-cell.png" width="392"/></div>
<p><span>Instead of the input, forget, and output gates in the LSTM cell, the GRU cell has two gates,<br/>
an update gate</span> <em>z</em>, <span>and a reset gate r.</span> The update gate defines how much previous memory to keep around and the reset gate defines how to combine the new input with the previous memory. There is no persistent cell state distinct from the hidden state as in LSTM. The following equations define the gating mechanism in a GRU:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/gru-eq1.png"/></div>
<p>According to several empirical evaluations (for more information refer to the articles: <em>An Empirical Exploration of Recurrent Network Architectures</em>, by R. Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015 and <em>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</em>, by J. Chung, arXiv:1412.3555. 2014), GRU and LSTM have comparable performance and there is no simple way to recommend one or the other for a specific task. While GRUs are faster to train and need less data to generalize, in situations where there is enough data, an LSTM's greater expressive power may lead to better results. Like LSTMs, GRUs are drop-in replacements for the SimpleRNN cell.</p>
<p>Keras provides built in implementations of both <kbd>LSTM</kbd> and <kbd>GRU</kbd>, as well as the <kbd>SimpleRNN</kbd> class we saw earlier.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">GRU with Keras — POS tagging</h1>
            </header>

            <article>
                
<p>Keras provides a GRU implementation, that we will use here to build a network that does POS tagging. A POS is a grammatical category of words that are used in the same way across multiple sentences. Examples of POS are nouns, verbs, adjectives, and so on. For example, nouns are typically used to identify things, verbs are typically used to identify what they do, and adjectives to describe some attribute of these things. POS tagging used to be done manually, but nowadays this is done automatically using statistical models. In recent years, deep learning has been applied to this problem as well (<span>for more information refer to the article: <em>Natural Language Processing (almost) from Scratch</em>, by R. Collobert, Journal of Machine Learning Research, Pp. 2493-2537, 2011</span>).</p>
<p>For our training data, we will need sentences tagged with part of speech tags. The Penn Treebank (<a href="https://catalog.ldc.upenn.edu/ldc99t42" target="_blank">https://catalog.ldc.upenn.edu/ldc99t42</a>) is one such dataset, it is a human annotated corpus of about 4.5 million words of American English. However, it is a non-free resource. A 10% sample of the Penn Treebank is freely available as part of the NLTK (<a href="http://www.nltk.org/" target="_blank">http://www.nltk.org/</a>), which we will use to train our network.</p>
<p>Our model will take in a sequence of words in a sentence and output the corresponding POS tags for each word. Thus for an input sequence consisting of the words [<em>The</em>, <em>cat</em>, <em>sat</em>, <em>on</em>, <em>the</em>, <em>mat</em>, <em>.</em>], the output sequence emitted would be the POS symbols [<em>DT</em>, <em>NN</em>, <em>VB</em>, <em>IN</em>, <em>DT</em>, <em>NN</em>].</p>
<p>We start with the imports:</p>
<pre>
from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers.recurrent import GRU<br/>from keras.layers.wrappers import TimeDistributed<br/>from keras.models import Sequential<br/>from keras.preprocessing import sequence<br/>from keras.utils import np_utils<br/>from sklearn.model_selection import train_test_split<br/>import collections<br/>import nltk<br/>import numpy as np<br/>import os
</pre>
<p>We then download the data from NLTK in a format suitable for our downstream code. Specifically, the data is available in parsed form as part of the NLTK Treebank corpus. We use the following Python code to download this data into two parallel files, one for the words in the sentences and one for the POS tags:</p>
<pre>
DATA_DIR = "../data"<br/><br/>fedata = open(os.path.join(DATA_DIR, "treebank_sents.txt"), "wb")<br/>ffdata = open(os.path.join(DATA_DIR, "treebank_poss.txt"), "wb")<br/><br/>sents = nltk.corpus.treebank.tagged_sents()<br/>for sent in sents:<br/>    words, poss = [], []<br/>    for word, pos in sent:<br/>        if pos == "-NONE-":<br/>            continue<br/>        words.append(word)<br/>        poss.append(pos)<br/>    fedata.write("{:s}n".format(" ".join(words)))<br/>    ffdata.write("{:s}n".format(" ".join(poss)))<br/><br/>fedata.close()<br/>ffdata.close()
</pre>
<p>Once again, we want to explore the data a little to find out what vocabulary size to set. This time, we have to consider two different vocabularies, the source vocabulary for the words and the target vocabulary for the POS tags. We need to find the number of unique words in each vocabulary. We also need to find the maximum number of words in a sentence in our training corpus and the number of records. Because of the one-to-one nature of POS tagging, the last two values are identical for both vocabularies:</p>
<pre>
def parse_sentences(filename):<br/>    word_freqs = collections.Counter()<br/>    num_recs, maxlen = 0, 0<br/>    fin = open(filename, "rb")<br/>    for line in fin:<br/>        words = line.strip().lower().split()<br/>        for word in words:<br/>            word_freqs[word] += 1<br/>        if len(words) &gt; maxlen:<br/>            maxlen = len(words)<br/>        num_recs += 1<br/>    fin.close()<br/>    return word_freqs, maxlen, num_recs<br/><br/>    s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(<br/>    os.path.join(DATA_DIR, "treebank_sents.txt"))<br/>    t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(<br/>    os.path.join(DATA_DIR, "treebank_poss.txt"))<br/>print(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs)
</pre>
<p>Running this code tells us that there are 10,947 unique words and 45 unique POS tags. The maximum sentence size is 249, and the number of sentences in the 10% set is 3,914. Using this information, we decide to consider only the top 5,000 words for our source vocabulary. Our target vocabulary has 45 unique POS tags, we want to be able to predict all of them, so we will consider all of them in our vocabulary. Finally, we set 250 to be our maximum sequence length:</p>
<pre>
MAX_SEQLEN = 250<br/>S_MAX_FEATURES = 5000<br/>T_MAX_FEATURES = 45
</pre>
<p>Just like our sentiment analysis example, each row of the input will be represented as a sequence of word indices. The corresponding output will be a sequence of POS tag indices. So we need to build lookup tables to translate between the words/POS tags and their corresponding indices. Here is the code to do that. On the source side, we build a vocabulary index with two extra slots to hold the <kbd>PAD</kbd> and <kbd>UNK</kbd> pseudo-words. On the target side, we don't drop any words so there is no need for the <kbd>UNK</kbd> pseudo-word:</p>
<pre>
s_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2<br/>s_word2index = {x[0]:i+2 for i, x in<br/>enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}<br/>s_word2index["PAD"] = 0<br/>s_word2index["UNK"] = 1<br/>s_index2word = {v:k for k, v in s_word2index.items()}<br/><br/>t_vocabsize = len(t_wordfreqs) + 1<br/>t_word2index = {x[0]:i for i, x in<br/>enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}<br/>t_word2index["PAD"] = 0<br/>t_index2word = {v:k for k, v in t_word2index.items()}
</pre>
<p>The next step is to build our datasets to feed into our network. We will use these lookup tables to convert our input sentences into a word ID sequence of length <kbd>MAX_SEQLEN</kbd> (<kbd>250</kbd>). The labels need to be structured as a sequence of one-hot vectors of size <kbd>T_MAX_FEATURES</kbd> + 1 (<kbd>46</kbd>), also of length <kbd>MAX_SEQLEN</kbd> (<kbd>250</kbd>). The <kbd>build_tensor</kbd> function reads the data from the two files and converts them to the input and output tensors. Additional default parameters are passed in to build the output tensor. This triggers the call to <kbd>np_utils.to_categorical()</kbd> to convert the output sequence of POS tag IDs to one-hot vector representation:</p>
<pre>
def build_tensor(filename, numrecs, word2index, maxlen,<br/>        make_categorical=False, num_classes=0):<br/>    data = np.empty((numrecs, ), dtype=list)<br/>    fin = open(filename, "rb")<br/>    i = 0<br/>    for line in fin:<br/>        wids = []<br/>        for word in line.strip().lower().split():<br/>            if word2index.has_key(word):<br/>                wids.append(word2index[word])<br/>            else:<br/>                wids.append(word2index["UNK"])<br/>        if make_categorical:<br/>            data[i] = np_utils.to_categorical(wids, <br/>                num_classes=num_classes)<br/>        else:<br/>            data[i] = wids<br/>        i += 1<br/>    fin.close()<br/>    pdata = sequence.pad_sequences(data, maxlen=maxlen)<br/>    return pdata<br/><br/>X = build_tensor(os.path.join(DATA_DIR, "treebank_sents.txt"),<br/>    s_numrecs, s_word2index, MAX_SEQLEN)<br/>Y = build_tensor(os.path.join(DATA_DIR, "treebank_poss.txt"),<br/>    t_numrecs, t_word2index, MAX_SEQLEN, True, t_vocabsize)
</pre>
<p>We can then split the dataset into a 80-20 train-test split:</p>
<pre>
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)
</pre>
<p>The following figure shows the schematic of our network. It looks complicated, so let us deconstruct it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="474" src="assets/postag-gru.png" width="265"/></div>
<p>As previously, assuming that the batch size is as yet undetermined, the input to the network is a tensor of word IDs of shape <kbd>(None, MAX_SEQLEN, 1)</kbd>. This is sent through an embedding layer, which converts each word into a dense vector of shape (<kbd>EMBED_SIZE</kbd>), so the output tensor from this layer has the shape <kbd>(None, MAX_SEQLEN, EMBED_SIZE)</kbd>. This tensor is fed to the encoder GRU with an output size of <kbd>HIDDEN_SIZE</kbd>. The GRU is set to return a single context vector (<kbd>return_sequences=False</kbd>) after seeing a sequence of size <kbd>MAX_SEQLEN</kbd>, so the output tensor from the GRU layer has shape <kbd>(None, HIDDEN_SIZE)</kbd>.</p>
<p>This context vector is then replicated using the RepeatVector layer into a tensor of shape <kbd>(None, MAX_SEQLEN, HIDDEN_SIZE)</kbd> and fed into the decoder GRU layer. This is then fed into a dense layer which produces an output tensor of shape <kbd>(None, MAX_SEQLEN, t_vocab_size)</kbd>. The activation function on the dense layer is a softmax. The argmax of each column of this tensor is the index of the predicted POS tag for the word at that position.</p>
<p>The model definition is shown as follows: <kbd>EMBED_SIZE</kbd>, <kbd>HIDDEN_SIZE</kbd>, <kbd>BATCH_SIZE</kbd>, and <kbd>NUM_EPOCHS</kbd> are hyperparameters which have been assigned these values after experimenting with multiple different values. The model is compiled with the <kbd>categorical_crossentropy</kbd> loss function since we have multiple categories of labels, and the optimizer used is the popular <kbd>adam</kbd> optimizer:</p>
<pre>
EMBED_SIZE = 128<br/>HIDDEN_SIZE = 64<br/>BATCH_SIZE = 32<br/>NUM_EPOCHS = 1<br/><br/>model = Sequential()<br/>model.add(Embedding(s_vocabsize, EMBED_SIZE,<br/>input_length=MAX_SEQLEN))<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))<br/>model.add(RepeatVector(MAX_SEQLEN))<br/>model.add(GRU(HIDDEN_SIZE, return_sequences=True))<br/>model.add(TimeDistributed(Dense(t_vocabsize)))<br/>model.add(Activation("softmax"))<br/><br/>model.compile(loss="categorical_crossentropy", optimizer="adam",<br/>    metrics=["accuracy"])
</pre>
<p>We train this model for a single epoch. The model is very rich, with many parameters, and begins to overfit after the first epoch of training. When fed the same data multiple times in the next epochs, the model begins to overfit to the training data and does worse on the validation data:</p>
<pre>
model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,<br/>    validation_data=[Xtest, Ytest])<br/><br/>score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)<br/>print("Test score: %.3f, accuracy: %.3f" % (score, acc))
</pre>
<p>The output of the training and the evaluation is shown as follows. As you can see, the model does quite well after the first epoch of training:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/ss-6-4.png"/></div>
<p>Similar to actual RNNs, the three recurrent classes in Keras (<kbd>SimpleRNN</kbd>, <kbd>LSTM</kbd>, and <kbd>GRU</kbd>) are interchangeable. To demonstrate, we simply replace all occurrences of <kbd>GRU</kbd> in the previous program with <kbd>LSTM</kbd> and rerun the program. The model definition and the import statements are the only things that change:</p>
<pre>
from keras.layers.recurrent import GRU<br/><br/>model = Sequential()<br/>model.add(Embedding(s_vocabsize, EMBED_SIZE,<br/>input_length=MAX_SEQLEN))<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))<br/>model.add(RepeatVector(MAX_SEQLEN))<br/>model.add(GRU(HIDDEN_SIZE, return_sequences=True))<br/>model.add(TimeDistributed(Dense(t_vocabsize)))<br/>model.add(Activation("softmax"))
</pre>
<p>As you can see from the output, the results of the GRU-based network are quite comparable to our previous LSTM-based network.</p>
<p>Sequence-to-sequence models are a very powerful class of model. Its most canonical application is machine translation, but there are many others such as the previous example. Indeed, a lot of NLP tasks further up in the hierarchy, such as named entity recognition (for more information refer to the article: <span><em>Named Entity Recognition with Long Short Term Memory</em>, by J. Hammerton, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL, Association for Computational Linguistics, 2003</span>) and sentence parsing (<span>for more information refer to the article: <em>Grammar as a Foreign Language</em>, by O. Vinyals, Advances in Neural Information Processing Systems, 2015</span>), as well as more complex networks such as those for image captioning (<span>for more information refer to the article: <em>Deep Visual-Semantic Alignments for Generating Image Descriptions</em>, by A. Karpathy, and F. Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.</span>), are examples of the sequence-to-sequence compositional model.</p>
<p>The full code for this example can be found in the file <kbd>pos_tagging_gru.py</kbd> in the the code download for this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Bidirectional RNNs</h1>
            </header>

            <article>
                
<p>At a given time step <em>t</em>, the output of the RNN is dependent on the outputs at all previous time steps. However, it is entirely possible that the output is also dependent on the future outputs as well. This is especially true for applications such as NLP, where the attributes of the word or phrase we are trying to predict may be dependent on the context given by the entire enclosing sentence, not just the words that came before it. Bidirectional RNNs also help a network architecture place equal emphasis on the beginning and end of the sequence, and increase the data available for training.</p>
<p>Bidirectional RNNs are two RNNs stacked on top of each other, reading the input in opposite directions. So in our example, one RNN will read the words left to right and the other RNN will read the words right to left. The output at each time step will be based on the hidden state of both RNNs.</p>
<p>Keras provides support for bidirectional RNNs through a bidirectional wrapper layer. For example, for our POS tagging example, we could make our LSTMs bidirectional simply by wrapping them with this Bidirectional wrapper, as shown in the model definition code as follows:</p>
<pre>
from keras.layers.wrappers import Bidirectional<br/><br/>model = Sequential()<br/>model.add(Embedding(s_vocabsize, EMBED_SIZE,<br/>input_length=MAX_SEQLEN))<br/>model.add(SpatialDropout1D(Dropout(0.2)))<br/>model.add(Bidirectional(LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2)))<br/>model.add(RepeatVector(MAX_SEQLEN))<br/>model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))<br/>model.add(TimeDistributed(Dense(t_vocabsize)))<br/>model.add(Activation("softmax"))
</pre>
<p>This gives us performance comparable to the unidirectional LSTM example shown as follows:</p>
<div class="CDPAlignCenter"><img class="image-border" height="106" src="assets/ss-6-6.png" width="611"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Stateful RNNs</h1>
            </header>

            <article>
                
<p>RNNs can be stateful, which means that they can maintain state across batches during training. That is, the hidden state computed for a batch of training data will be used as the initial hidden state for the next batch of training data. However, this needs to be explicitly set, since Keras RNNs are stateless by default and resets the state after each batch. Setting an RNN to be stateful means that it can build a state across its training sequence and even maintain that state when doing predictions.</p>
<p>The benefits of using stateful RNNs are smaller network sizes and/or lower training times. The disadvantage is that we are now responsible for training the network with a batch size that reflects the periodicity of the data, and resetting the state after each epoch. In addition, data should not be shuffled while training the network, since the order in which the data is presented is relevant for stateful networks.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Stateful LSTM with Keras — predicting electricity consumption</h1>
            </header>

            <article>
                
<p>In this example, we predict electricity consumption for a consumer using a stateful and stateless LSTM network and compare their behaviors. As you will recall, RNNs in Keras are stateless by default. In case of stateful models, the internal states computed after processing a batch of input is reused as initial states for the next batch. In other words, the state computed from element <em>i</em> in a batch will be used as initial state for for the element <em>i</em> in the next batch.</p>
<p>The dataset we will use is the electricity load diagram dataset from the UCI Machine Learning Repository (<a href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014" target="_blank">https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</a>), and contains consumption information about 370 customers, taken at 15 minute intervals over a four year period from 2011 to 2014. We randomly choose customer number 250 for our example.</p>
<p>One thing to remember is that most problems can be solved with stateless RNNs, so if you do use a stateful RNN, make sure you need it. Typically, you would need it when the data has a periodic component. If you think a bit, you will realize that electricity consumption is periodic. Consumption tends to be higher during the day than at night. Let us extract the consumption data for customer number 250 and plot the first 10 days of data. Finally we also save it to a binary NumPy file for our next step:</p>
<pre>
import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os<br/>import re<br/><br/>DATA_DIR = "../data"<br/><br/>fld = open(os.path.join(DATA_DIR, "LD2011_2014.txt"), "rb")<br/>data = []<br/>cid = 250<br/>for line in fld:<br/>    if line.startswith(""";"):<br/>        continue<br/>    cols = [float(re.sub(",", ".", x)) for x in <br/>                line.strip().split(";")[1:]]<br/>    data.append(cols[cid])<br/>fld.close()<br/><br/>NUM_ENTRIES = 1000<br/>plt.plot(range(NUM_ENTRIES), data[0:NUM_ENTRIES])<br/>plt.ylabel("electricity consumption")<br/>plt.xlabel("time (1pt = 15 mins)")<br/>plt.show()<br/><br/>np.save(os.path.join(DATA_DIR, "LD_250.npy"), np.array(data))
</pre>
<p>The output of the preceding example is as follow:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/econs_plot.png"/></div>
<p>As you can see, there is clearly a daily periodic trend. So the problem is a good candidate for a stateful model. Also, based on our observation, a <kbd>BATCH_SIZE</kbd> of <kbd>96</kbd> (number of 15 minute readings over 24 hours) seems appropriate.</p>
<p>We will show the code for the stateless version of the model simultaneously with the one for the stateful version. Most of the code is identical for both versions, so we will look at both versions simultaneously. I will point out the differences in the code as they arise.</p>
<p>First, as usual, we import the necessary libraries and classes:</p>
<pre>
from keras.layers.core import Dense<br/>from keras.layers.recurrent import LSTM<br/>from keras.models import Sequential<br/>from sklearn.preprocessing import MinMaxScaler<br/>import numpy as np<br/>import math<br/>import os
</pre>
<p>Next we load the data for customer 250 into a long array of size (<kbd>140256</kbd>) from the saved NumPy binary file and rescale it to the range <em>(0, 1)</em>. Finally, we reshape the input to three dimensions as needed by our network:</p>
<pre>
DATA_DIR = "../data"<br/><br/>data = np.load(os.path.join(DATA_DIR, "LD_250.npy"))<br/>data = data.reshape(-1, 1)<br/>scaler = MinMaxScaler(feature_range=(0, 1), copy=False)<br/>data = scaler.fit_transform(data)
</pre>
<p>Within each batch, the model will take a sequence of 15 minute readings and predict the next one. The length of the input sequence is given by the <kbd>NUM_TIMESTEPS</kbd> variable in the code. Based on some experimentation, we get a value of <kbd>NUM_TIMESTEPS</kbd> as <kbd>20</kbd>, that is, each input row will be a sequence of length <kbd>20</kbd>, and the output will have length <kbd>1</kbd>. The next step rearranges the input array into <kbd>X</kbd> and <kbd>Y</kbd> tensors of shapes <kbd>(None, 4)</kbd> and <kbd>(None, 1)</kbd>. Finally, we reshape the input tensor <kbd>X</kbd> to three dimensions as required by the network:</p>
<pre>
X = np.zeros((data.shape[0], NUM_TIMESTEPS))<br/>Y = np.zeros((data.shape[0], 1))<br/>for i in range(len(data) - NUM_TIMESTEPS - 1):<br/>    X[i] = data[i:i + NUM_TIMESTEPS].T<br/>    Y[i] = data[i + NUM_TIMESTEPS + 1]<br/><br/># reshape X to three dimensions (samples, timesteps, features)<br/>X = np.expand_dims(X, axis=2)
</pre>
<p>We then split our <kbd>X</kbd> and <kbd>Y</kbd> tensors into a 70-30 training test split. Since we are working with time series, we just choose a split point and cut the data into two parts, rather than using the <kbd>train_test_split</kbd> function, which also shuffles the data:</p>
<pre>
sp = int(0.7 * len(data))<br/>Xtrain, Xtest, Ytrain, Ytest = X[0:sp], X[sp:], Y[0:sp], Y[sp:]<br/>print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)
</pre>
<p>First we define our stateless model. We also set the values of <kbd>BATCH_SIZE</kbd> and <kbd>NUM_TIMESTEPS</kbd>, as we discussed previously. Our LSTM output size is given by <kbd>HIDDEN_SIZE</kbd>, another hyperparameter that is usually arrived at through experimentation. Here, we just set it to <kbd>10</kbd> since our objective is to compare two networks:</p>
<pre>
NUM_TIMESTEPS = 20<br/>HIDDEN_SIZE = 10<br/>BATCH_SIZE = 96 # 24 hours (15 min intervals)<br/><br/># stateless<br/>model = Sequential()<br/>model.add(LSTM(HIDDEN_SIZE, input_shape=(NUM_TIMESTEPS, 1), <br/>    return_sequences=False))<br/>model.add(Dense(1))
</pre>
<p>The corresponding definition for the stateful model is very similar, as you can see as follows. In the LSTM constructor, you need to set <kbd>stateful=True</kbd>, and instead of <kbd>input_shape</kbd> where the batch size is determined at runtime, you need to set <kbd>batch_input_shape</kbd> explicitly with the batch size. You also need to ensure that your training and test data sizes are perfect multiples of your batch size. We will see how to do that later when we look at the training code:</p>
<pre>
# stateful<br/>model = Sequential()<br/>model.add(LSTM(HIDDEN_SIZE, stateful=True,<br/>    batch_input_shape=(BATCH_SIZE, NUM_TIMESTEPS, 1), <br/>    return_sequences=False))<br/>model.add(Dense(1))
</pre>
<p>Next we compile the model, which is the same for both stateless and stateful RNNs. Notice that our metric here is mean squared error instead of our usual accuracy. This is because this is really a regression problem; we are interested in knowing how far off our predictions are with respect to the labels rather than knowing whether our prediction matched the label. You can find a full list of Keras built-in metrics on the Keras metrics page:</p>
<pre>
model.compile(loss="mean_squared_error", optimizer="adam",<br/>    metrics=["mean_squared_error"])
</pre>
<p>To train the stateless model, we can use the one liner that we have probably become very familiar with by now:</p>
<pre>
BATCH_SIZE = 96 # 24 hours (15 min intervals)<br/><br/># stateless<br/>model.fit(Xtrain, Ytrain, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,<br/>    validation_data=(Xtest, Ytest),<br/>    shuffle=False)
</pre>
<p>The corresponding code for the stateful model is shown as follows. There are three things to be aware of here.</p>
<p>First, you should select a batch size that reflects the periodicity of your data. This is because stateful RNNs align the states from each batch to the next, so selecting the right batch size allows the network to learn faster.</p>
<p>Once you set the batch size, the size of your training and test sets needs to be exact multiples of your batch size. We have ensured this below by truncating the last few records from both our training and test sets.</p>
<p>The second thing is that you need to fit the model manually, training the model in a loop for the required number of epochs. Each iteration trains the model for one epoch, and the state is retained across multiple batches. After each epoch, the state of the model needs to be reset manually.</p>
<p>The third thing is that the data should be fed in sequence. By default, Keras will shuffle the rows within each batch, which will destroy the alignment we need for the stateful RNN to learn effectively. This is done by setting <kbd>shuffle=False</kbd> in the call to <kbd>model.fit()</kbd>:</p>
<pre>
BATCH_SIZE = 96 # 24 hours (15 min intervals)<br/><br/># stateful<br/># need to make training and test data to multiple of BATCH_SIZE<br/>train_size = (Xtrain.shape[0] // BATCH_SIZE) * BATCH_SIZE<br/>test_size = (Xtest.shape[0] // BATCH_SIZE) * BATCH_SIZE<br/>Xtrain, Ytrain = Xtrain[0:train_size], Ytrain[0:train_size]<br/>Xtest, Ytest = Xtest[0:test_size], Ytest[0:test_size]<br/>print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)<br/>for i in range(NUM_EPOCHS):<br/>    print("Epoch {:d}/{:d}".format(i+1, NUM_EPOCHS))<br/>    model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=1,<br/>        validation_data=(Xtest, Ytest),<br/>        shuffle=False)<br/>    model.reset_states()
</pre>
<p>Finally, we evaluate the model against the test data and print out the scores:</p>
<pre>
score, _ = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)<br/>rmse = math.sqrt(score)<br/>print("MSE: {:.3f}, RMSE: {:.3f}".format(score, rmse))
</pre>
<p>The output for the stateless model, run over five epochs, is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="221" src="assets/ss-6-7.png" width="491"/></div>
<p>The corresponding output for the stateful model, also run in a loop five times for one epoch each time, is as follows. Notice the result of the truncating operation in the second line:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="265" src="assets/ss-6-8.png" width="488"/></div>
<p>As you can see, the stateful model produces results that are slightly better than the stateless model. In absolute terms, since we have scaled our data to the <em>(0, 1)</em> range, this means that the stateless model has about 6.2% error rate and the stateful model has a 5.9% error rate, or conversely, they are about 93.8% and 94.1% accurate respectively. In relative terms, therefore, our stateful model outperforms the stateless model by a slight margin.</p>
<p>The source code for this example is provided in the files <kbd>econs_data.py</kbd> that parses the dataset, and <kbd>econs_stateful.py</kbd> that defines and trains the stateless and stateful models, available from the code download for this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Other RNN variants</h1>
            </header>

            <article>
                
<p>We will round up this chapter by looking at some more variants of the RNN cell. RNN is an area of active research and many researchers have suggested variants for specific purposes.</p>
<p>One popular LSTM variant is adding <em>peephole connections</em>, which means that the gate layers are allowed to peek at the cell state. This was introduced by Gers and Schmidhuber (for more information refer to the article: <span><em>Learning Precise Timing with LSTM Recurrent Networks</em>, by F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, Journal of Machine Learning Research, pp. 115-43</span>) in 2002.</p>
<p>Another LSTM variant, that ultimately led to the GRU, is to use coupled forget and output gates. Decisions about what information to forget and what to acquire are made together, and the new information replaces the forgotten information.</p>
<p>Keras provides only the three basic variants, namely the SimpleRNN, LSTM, and GRU layers. However, that isn't necessarily a problem. Gref conducted an experimental survey (for more information refer to the article: <span><em>LSTM: A Search Space Odyssey</em>, by K. Greff, arXiv:1503.04069, 2015</span>) of many LSTM variants, and concluded that none of the variants improved significantly over the standard LSTM architecture. So the components provided in Keras are usually sufficient to solve most problems.</p>
<p>In case you do need the capability to construct your own layer, you can build custom Keras layers. We will look at how to build a custom layer in the next chapter. There is also an open source framework called recurrent shop (<a href="https://github.com/datalogai/recurrentshop" target="_blank">https://github.com/datalogai/recurrentshop</a>) that allows you to build complex recurrent neural networks with Keras.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at the basic architecture of recurrent neural networks and how they work better than traditional neural networks over sequence data. We saw how RNNs can be used to learn an author's writing style and generate text using the learned model. We also saw how this example can be extended to predicting stock prices or other time series, speech from noisy audio, and so on, as well as generate music that was composed by a learned model.</p>
<p>We looked at different ways to compose our RNN units and these topologies can be used to model and solve specific problems such as sentiment analysis, machine translation, image captioning, and classification, and so on.</p>
<p>We then looked at one of the biggest drawbacks of the SimpleRNN architecture, that of vanishing and exploding gradients. We saw how the vanishing gradient problem is handled using the LSTM (and GRU) architectures. We also looked at the LSTM and GRU architectures in some detail. We also saw two examples of predicting sentiment using an LSTM-based model, and predicting POS tags using a GRU-based sequence-to-sequence architecture.</p>
<p>We then learned about stateful RNNs and how they can be used in Keras. We also saw an example of learning a stateful RNN to predict CO levels in the atmosphere.</p>
<p>Finally, we learned about some RNN variants that are not available in Keras, and briefly explored how to build them.</p>
<p>In the next chapter, we will look at models that don't quite fit into the basic molds we have looked at so far. We will also look at composing these basic models larger and more complex ones using the Keras functional API, as well as look at some examples of customizing Keras to our needs.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>