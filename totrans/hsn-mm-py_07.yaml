- en: Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic speech recognition has a lot of potential applications, such as audio
    transcription, dictation, audio search, and virtual assistants. I am sure that
    everyone has interacted with at least one of the virtual assistants by now, be
    it Apple's Siri, Amazon's Alexa, or Google's Assistant. At the core of all these
    speech recognition systems are a set of statistical models over the different
    words or sounds in a language. And since speech has a temporal structure, HMMs
    are the most natural framework to model it.
  prefs: []
  type: TYPE_NORMAL
- en: HMMs are virtually at the core of all speech recognition systems and the core
    concepts in modeling haven't changed much in a long time. But over time, a lot
    of sophisticated techniques have been developed to build better systems. In the
    following sections, we will try to cover the main concepts leading to the development
    of these systems.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first problem that we will look into is known as **part-of-speech tagging **(**POS
    tagging**). According to Wikipedia, POS tagging, also known as **grammatical tagging**
    or **word-category disambiguation**, is the process of marking up a word in a
    text as corresponding to a particular part of speech based on both its definition
    and its context, that is, its relationship with adjacent and related words in
    a phrase, sentence, or paragraph. A simpler version of this, which is usually
    taught in schools, is classifying words as noun, verbs, adjectives, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'POS tagging is not as easy as it sounds because the same word can take different
    parts of speech in different contexts. A simple example of this is the word *dogs*.
    The word *dogs* is usually considered a noun, but in the following sentence, it
    acts like a verb:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The sailor dogs the hatch*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct grammatical tagging will reflect that *dogs* is used as a verb here,
    not as the more common plural noun. Grammatical context is one way to determine
    this; semantic analysis can also be used to infer that *sailor* and *hatch* implicate
    *dogs* as:'
  prefs: []
  type: TYPE_NORMAL
- en: In the nautical context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An action applied to the object *hatch*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In teaching English, generally only nine parts of speech are taught: noun,
    verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection.
    But we can divide words into more categories and subcategories for finer-grained
    tagging. For example, nouns can be sub-categorized into plural, possessive, and
    singular. Similarly, verbs can be sub-categorized on the basis of tense, aspect,
    and so on. In general, computer-based POS tagging systems are able to distinguish
    50 to 150 separate parts of speech for English. Work on stochastic methods for
    tagging Koine Greek has used over 1,000 parts of speech and found that about as
    many words were ambiguous there as in English.'
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the code example, we will use the `pomegranate` library to build an HMM
    for POS tagging. Pomegranate can be installed by running the following on the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we will not go into the details of the statistical POS tagger.
    The data we are using is a copy of the Brown corpus. The Brown corpus contains
    500 samples of English-language text, totaling roughly 1,000,000 words, compiled
    from works published in the United States in 1961.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by defining some functions to read the data from the `corpus`
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now define a couple of classes, `Subset` and `Dataset`, to make it easier
    to handle the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try to initialize the `Dataset` class and see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now explore the data to better understand how our classes store the
    information. We have randomly selected the `b100-38532` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check the unique elements in the `corpus`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the `X` and `Y` attributes of the `Dataset` class to access
    the words and the corresponding tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the `stream` method to iterate over pairs of a word and its
    tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finding the most frequent tag
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, just to compare the performance of our HMM model, let''s build a **most
    frequent class tagger** (**MFC** **Tagger**). We start by defining a function
    to count the pairs of tags and words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the `MFCTagger` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some helper functions to make predictions from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0768458e-588b-4b73-834e-d7c7a8e3c4a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating model accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To check how well our model performs, let''s evaluate the accuracy of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: An HMM-based tagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will try to build a POS tagger using HMM and hopefully it will improve
    our prediction performance. We will first define some helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build the model now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9a73582f-4681-4e84-9d86-3ebf5b0f6d93.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the HMM-based model has been able to improve the accuracy of
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the 1950s, Bell Labs was the pioneer in speech recognition. The early designed
    systems were limited to a single speaker and had a very limited vocabulary. After
    around 70 years of work, the current speech-recognition systems are able to work
    with speech from multiple speakers and can recognize thousands of words in multiple
    languages. A detailed discussion of all the techniques used is beyond the scope
    of this book as enough work has been done on each technique to have a book on
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: But the general workflow for a speech-recognition system is to first capture
    the audio by converting the physical sound into an electrical signal using a microphone.
    The electrical signal generated by the microphone is analog and needs to be converted
    to a digital form for storage and processing, for which analog-to-digital converters
    are used. Once we have the speech in digital form, we can apply algorithms on
    it to understand the speech.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, most of the state-of-the-art speech-recognition systems
    still use the concept of **Hidden Markov Models** (**HMM**) as their core. This
    is based on the assumption that a speech signal is a stationary process in a short
    time period of a few milliseconds. Hence, the first step for the speech-recognition
    system is to split the signal into small fragments of around 10 milliseconds.
    Then the power spectrum of each fragment is mapped to a vector of real numbers,
    known as **cepstral coefficients**. The dimension of this vector is usually small,
    although more accurate systems usually work with more than 32 dimensions. The
    final output of the HMM model is a sequence of these vectors. Once we have these
    vectors, these groups of vectors are matched to one or more phonemes, which are
    a fundamental unit of speech. But for effectively matching these groups of vectors
    to phonemes, we need to train our system since there is a huge variation in the
    sound of phonemes between different speakers as well as different utterances from
    the same speaker. Once we have the sequence of phonemes, our system tries to guess
    the most likely word that could have possibly produced that sequence of phonemes.
  prefs: []
  type: TYPE_NORMAL
- en: As we can imagine, this whole detection process can be computationally quite
    expensive. For dealing with this complexity issue, modern speech-recognition systems
    use neural networks for feature-transformation and dimensionality-reduction before
    using the HMM for recognition. Another commonly used technique to reduce computation
    is to use voice activity detectors, which can detect the regions in the signal
    that contain speech. Using this information, we can design the recognizer to only
    spend computation on the parts of the signal that contain speech.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Python has a very developed ecosystem to work with speech recognition.
    In the next section, we will look into the different Python packages available
    for working with speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Python packages for speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python package hosting service, PyPI, has a lot of speech-recognition systems
    listed. Some of the most commonly used ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: SpeechRecognition ([https://pypi.org/project/SpeechRecognition/](https://pypi.org/project/SpeechRecognition/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apiai ([https://pypi.org/project/apiai/](https://pypi.org/project/apiai/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assemblyai ([https://pypi.org/project/assemblyai/](https://pypi.org/project/assemblyai/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pocketsphinx ([https://pypi.org/project/pocketsphinx/](https://pypi.org/project/pocketsphinx/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: google-cloud-speech ([https://pypi.org/project/google-cloud-speech/](https://pypi.org/project/google-cloud-speech/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: watson-developer-cloud ([https://pypi.org/project/watson-developer-cloud/](https://pypi.org/project/watson-developer-cloud/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these Python packages (such as `apiai`) offer more than just speech
    recognition and have implementations of natural language processing algorithms,
    using which the user can identify the speaker's intent from speech. The other
    packages focus only on speech recognition, which can be used to convert audio
    to text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the `SpeechRecognition` package. We have chosen `SpeechRecognition`
    for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a very simple-to-use API to directly access and process audio signals.
    For other packages, we usually need to write small scripts for them to be able
    to access files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a wrapper over several popular speech APIs and therefore is extremely
    flexible, and multiple services can be used without making much change to our
    code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, to start using `SpeechRecognition`, we need to install the package. Since
    it''s hosted on PyPI, it can be installed directly using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Basics of SpeechRecognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important class in the package is the `Recognizer` class as it handles
    most of the recognition tasks. We can specify different settings and functionality
    for recognizing speech from an audio source while initializing the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Recognizer` class can be initialized very easily without passing any argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Each instance of the `Recognizer` class has seven different possible methods
    that can be used to convert speech to text. Each of these methods uses a specific
    speech-recognition service. The seven methods are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`recognize_bing`: Uses Microsoft''s Bing Speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_google`: Uses Google''s Web Speech API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_google_cloud`: Uses Google''s Cloud Speech. Using this method would
    need `google-cloud-speech` to be installed, which can be easily installed through `pip`by
    running `pip install google-cloud-speech`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_houndify`: Uses SoundHound''s Houndify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_ibm`: Uses IBM''s speech to text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_sphinx`: Uses CMU''s Sphinx. This method has a dependency on `PocketSphinx`,
    which can be installed by running `pip install pocketsphinx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_wit`: Uses Wit.ai.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important thing to keep in mind while using these methods is that since
    most of these recognition services are offered by companies through web APIs,
    we need an internet connection to access these services. Also, some of these services
    only allow usage after registering with them online. Out of these seven, only `recognize_sphinx`
    works offline.
  prefs: []
  type: TYPE_NORMAL
- en: Out of all these web APIs, only Google's Web Speech API works without any registration
    or API key. Therefore, to keep things simple, we will use that in the rest of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The recognize methods throw `RequestError` if the server is unavailable, there
    is no internet connection, or the API quota limits are met.
  prefs: []
  type: TYPE_NORMAL
- en: The next thing that we would need in order to do any recognition is some audio
    data. `SpeechRecognition` provides direct functionality to either work with an
    audio file or use audio from an attached microphone. In the following sections,
    we will look into both of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition from audio files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start working with audio files, we first need to download one. For the following
    example, we will use the `harvard.wav` file, which can be downloaded from [https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to save the audio files in the same directory from where the Python
    interpreter is running. Otherwise, for the following code, the path to the files
    will need to be modified.
  prefs: []
  type: TYPE_NORMAL
- en: 'For working with audio files, `SpeechRecognition` has the `AudioFile` class,
    which can be used for reading and working with audio files. We can use the `record`
    method of `AudioFile` to process the contents of the audio file before it can
    be used with the `Recognizer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The context manager opens the audio and records its content into `audio`, which
    is an instance of `AudioFile`. We can check it by calling the `type` method on `audio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, once we have an `AudioFile` instance, we can call any of the recognize
    methods with it as an argument. The recognize method would call the specific web
    API to translate the speech from the audio file and return the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we transcribed the whole audio file, but what if we want to only
    translate a specific part of the audio file? This can be done by passing additional
    arguments to the `record` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `record` method keeps a pointer in the audio file to point at the position
    until which recording has happened. So, if we do another record of four seconds
    on the same file, it will record from the four-second mark to the eight-second
    mark of the original audio file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we transcribed a part of the audio file but the starting
    point was the start of the file. What if we want to start at a different time
    point? It can be done by passing another argument, `offset`, to the `record` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you listen to the `harvard.wav` file, you will realize that the recording
    is done in perfect conditions without any external noise, but that is usually
    not the case in real-life audio. Let''s try to transcribe another audio signal, `jackhammer.wav`,
    which can be downloaded from [https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav).
    If you listen to the audio file, you can notice that it has a lot of background
    noise. Let''s try to transcribe this file and see how the recognizer performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the transcription is way off. In such cases, we can use the `adjust_for_ambient_noise`
    method provided in the `Recognizer` class to calibrate our audio signal with the
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: '`adjust_for_ambient_noise` by default uses the first one second of data to
    do the calibration, but we can change that by passing a `duration` argument to
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If we don't want to lose much information, we can reduce the value of the `duration`
    argument, but that can result in a poorer calibration. As we can see, the transcription
    is still not perfect, but it is much better than when we didn't use `adjust_for_ambient_noise`.
    We can actually get better results by trying to clean the noise from the audio
    using signal processing techniques, which are outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing that we can do in such cases is to look at all the most likely
    transcriptions by the recognizer. It can be done by using the `show_all` argument
    while calling the `recognize` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using this, we can then choose the best transcription for our specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition using the microphone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used the recognizer methods to transcribe speech
    from audio files. In this section, we will do a transcription using speech recorded
    from our microphone.
  prefs: []
  type: TYPE_NORMAL
- en: But before we get into that, we will need to install an additional package,
    called `PyAudio`. It is also available on PyPI and can be installed directly using
    `pip: pip install PyAudio`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, for working with audio files, we were using the `AudioFile`
    class, but for working with a microphone, we will need to use the `Microphone`
    class. Most of the recognition API still remains the same. Let''s take a simple
    example to understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly to how we had initialized the `AudioFile` class in the previous section,
    we need to initialize the `Microphone` class this time. Also, instead of `record`,
    we need to call the `listen` method to record the audio. The Python interpreter
    would wait for a while to record audio when executing the previous code block.
    Try saying something into the microphone. Once the interpreter prompt returns,
    we can call the `recognize` method to transcribe the recorded audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked into two of the major applications of HMMs: POS
    tagging and speech recognition. We coded the POS tagger using a most-frequent
    tag algorithm and used the `pomegranate` package to build one based on HMM. We
    compared the performance using both these methods and saw that an HMM-based approach
    outperforms the most-frequent tag method. Then, we used the `SpeechRecognition`
    package to transcribe audio to text using Google''s Web Speech API. We looked
    into using the package with both audio files and live audio from a microphone.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more applications of HMMs, specifically
    in the field of image recognition.
  prefs: []
  type: TYPE_NORMAL
