- en: Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic speech recognition has a lot of potential applications, such as audio
    transcription, dictation, audio search, and virtual assistants. I am sure that
    everyone has interacted with at least one of the virtual assistants by now, be
    it Apple's Siri, Amazon's Alexa, or Google's Assistant. At the core of all these
    speech recognition systems are a set of statistical models over the different
    words or sounds in a language. And since speech has a temporal structure, HMMs
    are the most natural framework to model it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: HMMs are virtually at the core of all speech recognition systems and the core
    concepts in modeling haven't changed much in a long time. But over time, a lot
    of sophisticated techniques have been developed to build better systems. In the
    following sections, we will try to cover the main concepts leading to the development
    of these systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first problem that we will look into is known as **part-of-speech tagging **(**POS
    tagging**). According to Wikipedia, POS tagging, also known as **grammatical tagging**
    or **word-category disambiguation**, is the process of marking up a word in a
    text as corresponding to a particular part of speech based on both its definition
    and its context, that is, its relationship with adjacent and related words in
    a phrase, sentence, or paragraph. A simpler version of this, which is usually
    taught in schools, is classifying words as noun, verbs, adjectives, and so on.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'POS tagging is not as easy as it sounds because the same word can take different
    parts of speech in different contexts. A simple example of this is the word *dogs*.
    The word *dogs* is usually considered a noun, but in the following sentence, it
    acts like a verb:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*The sailor dogs the hatch*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct grammatical tagging will reflect that *dogs* is used as a verb here,
    not as the more common plural noun. Grammatical context is one way to determine
    this; semantic analysis can also be used to infer that *sailor* and *hatch* implicate
    *dogs* as:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In the nautical context
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An action applied to the object *hatch*
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In teaching English, generally only nine parts of speech are taught: noun,
    verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection.
    But we can divide words into more categories and subcategories for finer-grained
    tagging. For example, nouns can be sub-categorized into plural, possessive, and
    singular. Similarly, verbs can be sub-categorized on the basis of tense, aspect,
    and so on. In general, computer-based POS tagging systems are able to distinguish
    50 to 150 separate parts of speech for English. Work on stochastic methods for
    tagging Koine Greek has used over 1,000 parts of speech and found that about as
    many words were ambiguous there as in English.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Code
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the code example, we will use the `pomegranate` library to build an HMM
    for POS tagging. Pomegranate can be installed by running the following on the
    command line:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we will not go into the details of the statistical POS tagger.
    The data we are using is a copy of the Brown corpus. The Brown corpus contains
    500 samples of English-language text, totaling roughly 1,000,000 words, compiled
    from works published in the United States in 1961.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Getting data
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by defining some functions to read the data from the `corpus`
    files:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s now define a couple of classes, `Subset` and `Dataset`, to make it easier
    to handle the data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s try to initialize the `Dataset` class and see how it works:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exploring the data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now explore the data to better understand how our classes store the
    information. We have randomly selected the `b100-38532` key:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also check the unique elements in the `corpus`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also use the `X` and `Y` attributes of the `Dataset` class to access
    the words and the corresponding tags:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also use the `stream` method to iterate over pairs of a word and its
    tag:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finding the most frequent tag
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, just to compare the performance of our HMM model, let''s build a **most
    frequent class tagger** (**MFC** **Tagger**). We start by defining a function
    to count the pairs of tags and words:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s define the `MFCTagger` class:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here are some helper functions to make predictions from the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/0768458e-588b-4b73-834e-d7c7a8e3c4a8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Evaluating model accuracy
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To check how well our model performs, let''s evaluate the accuracy of our model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: An HMM-based tagger
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will try to build a POS tagger using HMM and hopefully it will improve
    our prediction performance. We will first define some helper functions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s build the model now:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/9a73582f-4681-4e84-9d86-3ebf5b0f6d93.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: '[PRE21]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can see that the HMM-based model has been able to improve the accuracy of
    our model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the 1950s, Bell Labs was the pioneer in speech recognition. The early designed
    systems were limited to a single speaker and had a very limited vocabulary. After
    around 70 years of work, the current speech-recognition systems are able to work
    with speech from multiple speakers and can recognize thousands of words in multiple
    languages. A detailed discussion of all the techniques used is beyond the scope
    of this book as enough work has been done on each technique to have a book on
    itself.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: But the general workflow for a speech-recognition system is to first capture
    the audio by converting the physical sound into an electrical signal using a microphone.
    The electrical signal generated by the microphone is analog and needs to be converted
    to a digital form for storage and processing, for which analog-to-digital converters
    are used. Once we have the speech in digital form, we can apply algorithms on
    it to understand the speech.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, most of the state-of-the-art speech-recognition systems
    still use the concept of **Hidden Markov Models** (**HMM**) as their core. This
    is based on the assumption that a speech signal is a stationary process in a short
    time period of a few milliseconds. Hence, the first step for the speech-recognition
    system is to split the signal into small fragments of around 10 milliseconds.
    Then the power spectrum of each fragment is mapped to a vector of real numbers,
    known as **cepstral coefficients**. The dimension of this vector is usually small,
    although more accurate systems usually work with more than 32 dimensions. The
    final output of the HMM model is a sequence of these vectors. Once we have these
    vectors, these groups of vectors are matched to one or more phonemes, which are
    a fundamental unit of speech. But for effectively matching these groups of vectors
    to phonemes, we need to train our system since there is a huge variation in the
    sound of phonemes between different speakers as well as different utterances from
    the same speaker. Once we have the sequence of phonemes, our system tries to guess
    the most likely word that could have possibly produced that sequence of phonemes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: As we can imagine, this whole detection process can be computationally quite
    expensive. For dealing with this complexity issue, modern speech-recognition systems
    use neural networks for feature-transformation and dimensionality-reduction before
    using the HMM for recognition. Another commonly used technique to reduce computation
    is to use voice activity detectors, which can detect the regions in the signal
    that contain speech. Using this information, we can design the recognizer to only
    spend computation on the parts of the signal that contain speech.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Python has a very developed ecosystem to work with speech recognition.
    In the next section, we will look into the different Python packages available
    for working with speech recognition.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Python packages for speech recognition
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python package hosting service, PyPI, has a lot of speech-recognition systems
    listed. Some of the most commonly used ones are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: SpeechRecognition ([https://pypi.org/project/SpeechRecognition/](https://pypi.org/project/SpeechRecognition/))
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apiai ([https://pypi.org/project/apiai/](https://pypi.org/project/apiai/))
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assemblyai ([https://pypi.org/project/assemblyai/](https://pypi.org/project/assemblyai/))
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pocketsphinx ([https://pypi.org/project/pocketsphinx/](https://pypi.org/project/pocketsphinx/))
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: google-cloud-speech ([https://pypi.org/project/google-cloud-speech/](https://pypi.org/project/google-cloud-speech/))
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: watson-developer-cloud ([https://pypi.org/project/watson-developer-cloud/](https://pypi.org/project/watson-developer-cloud/))
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these Python packages (such as `apiai`) offer more than just speech
    recognition and have implementations of natural language processing algorithms,
    using which the user can identify the speaker's intent from speech. The other
    packages focus only on speech recognition, which can be used to convert audio
    to text.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the `SpeechRecognition` package. We have chosen `SpeechRecognition`
    for two reasons:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: It has a very simple-to-use API to directly access and process audio signals.
    For other packages, we usually need to write small scripts for them to be able
    to access files.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a wrapper over several popular speech APIs and therefore is extremely
    flexible, and multiple services can be used without making much change to our
    code.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, to start using `SpeechRecognition`, we need to install the package. Since
    it''s hosted on PyPI, it can be installed directly using `pip`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Basics of SpeechRecognition
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important class in the package is the `Recognizer` class as it handles
    most of the recognition tasks. We can specify different settings and functionality
    for recognizing speech from an audio source while initializing the class.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Recognizer` class can be initialized very easily without passing any argument:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Each instance of the `Recognizer` class has seven different possible methods
    that can be used to convert speech to text. Each of these methods uses a specific
    speech-recognition service. The seven methods are the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '`recognize_bing`: Uses Microsoft''s Bing Speech.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_google`: Uses Google''s Web Speech API.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_google_cloud`: Uses Google''s Cloud Speech. Using this method would
    need `google-cloud-speech` to be installed, which can be easily installed through `pip`by
    running `pip install google-cloud-speech`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_houndify`: Uses SoundHound''s Houndify.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_ibm`: Uses IBM''s speech to text.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_sphinx`: Uses CMU''s Sphinx. This method has a dependency on `PocketSphinx`,
    which can be installed by running `pip install pocketsphinx`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recognize_wit`: Uses Wit.ai.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important thing to keep in mind while using these methods is that since
    most of these recognition services are offered by companies through web APIs,
    we need an internet connection to access these services. Also, some of these services
    only allow usage after registering with them online. Out of these seven, only `recognize_sphinx`
    works offline.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Out of all these web APIs, only Google's Web Speech API works without any registration
    or API key. Therefore, to keep things simple, we will use that in the rest of
    this chapter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The recognize methods throw `RequestError` if the server is unavailable, there
    is no internet connection, or the API quota limits are met.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The next thing that we would need in order to do any recognition is some audio
    data. `SpeechRecognition` provides direct functionality to either work with an
    audio file or use audio from an attached microphone. In the following sections,
    we will look into both of these methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做任何识别的，是一些音频数据。`SpeechRecognition`提供了直接的功能，可以使用音频文件或附加的麦克风音频。在接下来的部分中，我们将探讨这两种方法。
- en: Speech recognition from audio files
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从音频文件中进行语音识别
- en: To start working with audio files, we first need to download one. For the following
    example, we will use the `harvard.wav` file, which can be downloaded from [https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用音频文件，首先需要下载一个。对于下面的示例，我们将使用`harvard.wav`文件，它可以从[https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav)下载。
- en: Make sure to save the audio files in the same directory from where the Python
    interpreter is running. Otherwise, for the following code, the path to the files
    will need to be modified.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将音频文件保存在Python解释器运行的同一目录中。否则，在接下来的代码中，文件的路径需要进行修改。
- en: 'For working with audio files, `SpeechRecognition` has the `AudioFile` class,
    which can be used for reading and working with audio files. We can use the `record`
    method of `AudioFile` to process the contents of the audio file before it can
    be used with the `Recognizer` class:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频文件的处理，`SpeechRecognition`提供了`AudioFile`类，用于读取和操作音频文件。我们可以使用`AudioFile`的`record`方法来处理音频文件的内容，然后再与`Recognizer`类一起使用：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The context manager opens the audio and records its content into `audio`, which
    is an instance of `AudioFile`. We can check it by calling the `type` method on `audio`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器打开音频并将其内容记录到`audio`中，`audio`是`AudioFile`的一个实例。我们可以通过调用`type`方法检查它：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, once we have an `AudioFile` instance, we can call any of the recognize
    methods with it as an argument. The recognize method would call the specific web
    API to translate the speech from the audio file and return the following text:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们拥有了一个`AudioFile`实例，就可以调用任何识别方法，并将其作为参数传递。识别方法会调用特定的网络API，将音频文件中的语音转换成文本，并返回如下文本：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this case, we transcribed the whole audio file, but what if we want to only
    translate a specific part of the audio file? This can be done by passing additional
    arguments to the `record` method:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们转录了整个音频文件，但如果我们只想翻译音频文件的特定部分，该怎么办？可以通过将额外的参数传递给`record`方法来实现：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `record` method keeps a pointer in the audio file to point at the position
    until which recording has happened. So, if we do another record of four seconds
    on the same file, it will record from the four-second mark to the eight-second
    mark of the original audio file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`record`方法会在音频文件中保留一个指针，指向记录已经完成的位置。因此，如果我们在同一个文件上再录制四秒钟，它将从原始音频文件的四秒标记到八秒标记进行录音。'
- en: 'In the preceding example, we transcribed a part of the audio file but the starting
    point was the start of the file. What if we want to start at a different time
    point? It can be done by passing another argument, `offset`, to the `record` method:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们转录了音频文件的一部分，但起始点是文件的开始。如果我们希望从不同的时间点开始该怎么办？可以通过将另一个参数`offset`传递给`record`方法来实现：
- en: '[PRE28]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you listen to the `harvard.wav` file, you will realize that the recording
    is done in perfect conditions without any external noise, but that is usually
    not the case in real-life audio. Let''s try to transcribe another audio signal, `jackhammer.wav`,
    which can be downloaded from [https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav).
    If you listen to the audio file, you can notice that it has a lot of background
    noise. Let''s try to transcribe this file and see how the recognizer performs:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你听一下`harvard.wav`文件，你会发现录音是在没有外部噪音的完美环境下完成的，但在现实生活中的音频通常不是这样。我们来试着转录另一个音频信号`jackhammer.wav`，它可以从[https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav](https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav)下载。如果你听这个音频文件，你会注意到它有很多背景噪音。让我们试着转录这个文件，看看识别器的表现如何：
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As we can see, the transcription is way off. In such cases, we can use the `adjust_for_ambient_noise`
    method provided in the `Recognizer` class to calibrate our audio signal with the
    noise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '`adjust_for_ambient_noise` by default uses the first one second of data to
    do the calibration, but we can change that by passing a `duration` argument to
    it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If we don't want to lose much information, we can reduce the value of the `duration`
    argument, but that can result in a poorer calibration. As we can see, the transcription
    is still not perfect, but it is much better than when we didn't use `adjust_for_ambient_noise`.
    We can actually get better results by trying to clean the noise from the audio
    using signal processing techniques, which are outside the scope of this book.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing that we can do in such cases is to look at all the most likely
    transcriptions by the recognizer. It can be done by using the `show_all` argument
    while calling the `recognize` method:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using this, we can then choose the best transcription for our specific problem.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition using the microphone
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used the recognizer methods to transcribe speech
    from audio files. In this section, we will do a transcription using speech recorded
    from our microphone.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: But before we get into that, we will need to install an additional package,
    called `PyAudio`. It is also available on PyPI and can be installed directly using
    `pip: pip install PyAudio`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, for working with audio files, we were using the `AudioFile`
    class, but for working with a microphone, we will need to use the `Microphone`
    class. Most of the recognition API still remains the same. Let''s take a simple
    example to understand how it works:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Similarly to how we had initialized the `AudioFile` class in the previous section,
    we need to initialize the `Microphone` class this time. Also, instead of `record`,
    we need to call the `listen` method to record the audio. The Python interpreter
    would wait for a while to record audio when executing the previous code block.
    Try saying something into the microphone. Once the interpreter prompt returns,
    we can call the `recognize` method to transcribe the recorded audio:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked into two of the major applications of HMMs: POS
    tagging and speech recognition. We coded the POS tagger using a most-frequent
    tag algorithm and used the `pomegranate` package to build one based on HMM. We
    compared the performance using both these methods and saw that an HMM-based approach
    outperforms the most-frequent tag method. Then, we used the `SpeechRecognition`
    package to transcribe audio to text using Google''s Web Speech API. We looked
    into using the package with both audio files and live audio from a microphone.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more applications of HMMs, specifically
    in the field of image recognition.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
