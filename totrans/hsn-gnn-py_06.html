<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer262">
<h1 class="chapter-number" id="_idParaDest-71"><a id="_idTextAnchor074"/>6</h1>
<h1 id="_idParaDest-72"><a id="_idTextAnchor075"/>Introducing Graph Convolutional Networks</h1>
<p>The <strong class="bold">Graph Convolutional Network</strong> (<strong class="bold">GCN</strong>) architecture<a id="_idIndexMarker309"/> is the blueprint of what a GNN looks like. Introduced by Kipf and Welling in 2017 [1], it is based on the idea of creating an efficient variant of <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) applied <a id="_idIndexMarker310"/>to graphs. More accurately, it is an approximation of a graph convolution operation in graph signal processing. Thanks to its versatility and ease of use, the GCN has become the most popular GNN in scientific literature. More generally, it is the architecture of choice to create a solid baseline when dealing with <span class="No-Break">graph data.</span></p>
<p>In this chapter, we’ll talk about the limitations of our previous vanilla GNN layer. This will help us to understand the motivation behind GCNs. We’ll detail how the GCN layer works and why it performs better than our solution. We’ll test this statement by implementing a GCN on the <strong class="source-inline">Cora</strong> and <strong class="source-inline">Facebook Page-Page</strong> datasets using PyTorch Geometric. This should improve our results <span class="No-Break">even further.</span></p>
<p>The last section is dedicated to a <a id="_idIndexMarker311"/>new task: <strong class="bold">node regression</strong>. This is not a very common task when it comes to GNNs, but it is particularly useful when you’re working with tabular data. If you have the opportunity to transform your tabular dataset into a graph, this will enable you to perform regression in addition <span class="No-Break">to classification.</span></p>
<p>By the end of this chapter, you will be able to implement a GCN in PyTorch Geometric for classification or regression tasks. Thanks to linear algebra, you’ll understand why this model performs better than our vanilla GNN. Finally, you’ll know how to plot node degrees and the density distribution of a <span class="No-Break">target variable.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Designing the graph <span class="No-Break">convolutional layer</span></li>
<li>Comparing graph convolutional and graph <span class="No-Break">linear layers</span></li>
<li>Predicting web traffic with <span class="No-Break">node regression</span></li>
</ul>
<h1 id="_idParaDest-73"><a id="_idTextAnchor076"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06</span></a><span class="No-Break">.</span></p>
<p>Installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> section of <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor077"/>Designing the graph convolutional layer</h1>
<p>First, let’s talk about<a id="_idIndexMarker312"/> a problem we did not anticipate in the previous chapter. Unlike tabular or image data, nodes do not always have the same number of neighbors. For instance, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>, node <img alt="" height="32" src="image/Formula_B19153_06_001.png" width="20"/> has 3 neighbors while node <img alt="" height="33" src="image/Formula_B19153_06_002.png" width="21"/> only <span class="No-Break">has 1:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<img alt="Figure 6.1 – Simple graph where nodes have different numbers of neighbors" height="382" src="image/B19153_06_001.jpg" width="473"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Simple graph where nodes have different numbers of neighbors</p>
<p>However, if <a id="_idIndexMarker313"/>we look at our GNN layer, we don’t take into account this difference in the number of neighbors. Our layer consists of a simple sum without any normalization coefficient. Here is how we calculated the embedding of a <span class="No-Break">node, <img alt="" height="32" src="image/Formula_B19153_06_003.png" width="10"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer220">
<img alt="" height="157" src="image/Formula_B19153_06_004.jpg" width="364"/>
</div>
</div>
<p>Imagine that node <img alt="" height="33" src="image/Formula_B19153_06_005.png" width="20"/> has 1,000 neighbors and node <img alt="" height="30" src="image/Formula_B19153_06_006.png" width="19"/> only has 1: the embedding <img alt="" height="40" src="image/Formula_B19153_06_007.png" width="40"/> will have much larger values than <img alt="" height="38" src="image/Formula_B19153_06_008.png" width="41"/>. This is an issue because we want to compare these embeddings. How are we supposed to make meaningful comparisons when their values are so <span class="No-Break">vastly different?</span></p>
<p>Fortunately, there <a id="_idIndexMarker314"/>is a simple solution: dividing the embedding by the number of neighbors. Let’s write <img alt="" height="40" src="image/Formula_B19153_06_009.png" width="115"/>, the degree of node <img alt="" height="31" src="image/Formula_B19153_06_010.png" width="27"/>. Here is the new formula for the <span class="No-Break">GNN layer:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer227">
<img alt="" height="134" src="image/Formula_B19153_06_011.jpg" width="426"/>
</div>
</div>
<p>But how do we translate it into a matrix multiplication? As a reminder, this was what we obtained for our vanilla <span class="No-Break">GNN layer:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<img alt="" height="44" src="image/Formula_B19153_06_012.jpg" width="240"/>
</div>
</div>
<p><span class="No-Break">Here, <img alt="" height="41" src="image/Formula_B19153_06_013.png" width="192"/>.</span></p>
<p>The only thing that is missing from this formula is a matrix to give us the normalization coefficient, <img alt="" height="69" src="image/Formula_B19153_06_020.png" width="80"/>. This is something that can be obtained thanks to the degree matrix <img alt="" height="29" src="image/Formula_B19153_06_015.png" width="27"/>, which counts the number of neighbors for each node. Here is the degree matrix for the graph shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer232">
<img alt="" height="206" src="image/Formula_B19153_06_016.jpg" width="433"/>
</div>
</div>
<p>Here is the same matrix <span class="No-Break">in NumPy:</span></p>
<pre class="source-code">
import numpy as np
D = np.array([
    [3, 0, 0, 0],
    [0, 1, 0, 0],
    [0, 0, 2, 0],
    [0, 0, 0, 2]
])</pre>
<p>By <a id="_idIndexMarker315"/>definition, <img alt="" height="30" src="image/Formula_B19153_06_017.png" width="27"/> gives us the degree of each node, <img alt="" height="47" src="image/Formula_B19153_06_018.png" width="109"/>. Therefore, the inverse of this matrix <img alt="" height="37" src="image/Formula_B19153_06_019.png" width="69"/> directly gives us the normalization <span class="No-Break">coefficients, <img alt="" height="91" src="image/Formula_B19153_06_0201.png" width="105"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="" height="372" src="image/Formula_B19153_06_021.jpg" width="488"/>
</div>
</div>
<p>The<a id="_idIndexMarker316"/> inverse of a matrix can directly be calculated using <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">numpy.linalg.inv()</strong></span><span class="No-Break">function:</span></p>
<pre class="source-code">
np.linalg.inv(D)
<strong class="bold">array([[0.33333333, 0.        , 0.        , 0.        ],</strong>
<strong class="bold">       [0.        , 1.        , 0.        , 0.        ],</strong>
<strong class="bold">       [0.        , 0.        , 0.5       , 0.        ],</strong>
<strong class="bold">       [0.        , 0.        , 0.        , 0.5       ]])</strong></pre>
<p>This is exactly what we were looking for. To be even more accurate, we added self-loops to the graph, represented by <img alt="" height="41" src="image/Formula_B19153_06_0131.png" width="162"/>. Likewise, we should add self-loops to the degree matrix, <img alt="" height="44" src="image/Formula_B19153_06_023.png" width="177"/>. The final matrix we are actually interested in <span class="No-Break">is <img alt="" height="56" src="image/Formula_B19153_06_024.png" width="305"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="" height="337" src="image/Formula_B19153_06_025.jpg" width="378"/>
</div>
</div>
<p>NumPy has <a id="_idIndexMarker317"/>a specific function, <strong class="source-inline">numpy.identity(n)</strong>, to quickly create an identity matrix <img alt="" height="33" src="image/Formula_B19153_06_026.png" width="19"/> of <img alt="" height="24" src="image/Formula_B19153_06_027.png" width="27"/> dimensions. In this example, we have <span class="No-Break">four dimensions:</span></p>
<pre class="source-code">
np.linalg.inv(D + np.identity(4))
<strong class="bold">array([[0.25      , 0.        , 0.        , 0.        ],</strong>
<strong class="bold">       [0.        , 0.5       , 0.        , 0.        ],</strong>
<strong class="bold">       [0.        , 0.        , 0.33333333, 0.        ],</strong>
<strong class="bold">       [0.        , 0.        , 0.        , 0.33333333]])</strong></pre>
<p>Now that we have our matrix of normalization coefficients, where should we put it in the formula? There are <span class="No-Break">two options:</span></p>
<ul>
<li><img alt="" height="42" src="image/Formula_B19153_06_028.png" width="196"/> will normalize every row <span class="No-Break">of features.</span></li>
<li><img alt="" height="43" src="image/Formula_B19153_06_029.png" width="193"/>will normalize every column <span class="No-Break">of features.</span></li>
</ul>
<p>We can verify this experimentally by calculating <img alt="" height="38" src="image/Formula_B19153_06_030.png" width="103"/> <span class="No-Break">and <img alt="" height="38" src="image/Formula_B19153_06_031.png" width="95"/>.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="" height="365" src="image/Formula_B19153_06_032.jpg" width="1012"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="" height="377" src="image/Formula_B19153_06_033.jpg" width="1044"/>
</div>
</div>
<p>Indeed, in <a id="_idIndexMarker318"/>the first case, the sum of every row is equal to 1. In the second case, the sum of every column is equal <span class="No-Break">to 1.</span></p>
<p>Matrix multiplications can be performed using the <strong class="source-inline">numpy.matmul()</strong> function. Even more conveniently, Python has had its own matrix multiplication operator, <strong class="source-inline">@</strong>, since version 3.5. Let’s define the adjacency matrix <img alt="" height="34" src="image/Formula_B19153_06_034.png" width="29"/> and use this operator to compute our <span class="No-Break">matrix multiplications:</span></p>
<pre class="source-code">
A = np.array([
    [1, 1, 1, 1],
    [1, 1, 0, 0],
    [1, 0, 1, 1],
    [1, 0, 1, 1]
])
print(np.linalg.inv(D + np.identity(4)) @ A)
<strong class="bold">[[0.25       0.25       0.25       0.25      ]</strong>
<strong class="bold"> [0.5        0.5        0.         0.        ]</strong>
<strong class="bold"> [0.33333333 0.         0.33333333 0.33333333]</strong>
<strong class="bold"> [0.33333333 0.         0.33333333 0.33333333]]</strong>
print(A @ np.linalg.inv(D + np.identity(4)))
<strong class="bold">[[0.25       0.5        0.33333333 0.33333333]</strong>
<strong class="bold"> [0.25       0.5        0.         0.        ]</strong>
<strong class="bold"> [0.25       0.         0.33333333 0.33333333]</strong>
<strong class="bold"> [0.25       0.         0.33333333 0.33333333]]</strong></pre>
<p>We obtain<a id="_idIndexMarker319"/> the same results as with manual <span class="No-Break">matrix multiplication.</span></p>
<p>So, which option should we use? Naturally, the first option looks more appealing because it nicely normalizes neighboring <span class="No-Break">node features.</span></p>
<p>However, Kipf and Welling [1] noticed that features from nodes with a lot of neighbors spread very easily, unlike features from more isolated nodes. In the original GCN paper, the authors proposed a hybrid normalization to counterbalance this effect. In practice, they assign higher weights to nodes with few neighbors using the <span class="No-Break">following formula:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="" height="79" src="image/Formula_B19153_06_035.jpg" width="448"/>
</div>
</div>
<p>In terms of individual embeddings, this operation can be written <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="" height="132" src="image/Formula_B19153_06_036.jpg" width="578"/>
</div>
</div>
<p>Those are<a id="_idIndexMarker320"/> the original formulas to implement a graph convolutional layer. As with our vanilla GNN layer, we can stack these layers to create a GCN. Let’s implement a GCN and verify that it performs better than our <span class="No-Break">previous approaches.</span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor078"/>Comparing graph convolutional and graph linear layers</h1>
<p>In the<a id="_idIndexMarker321"/> previous chapter, our vanilla GNN outperformed the Node2Vec model, but how does it compare to a GCN? In this section, we will compare their performance on the Cora and Facebook <span class="No-Break">Page-Page datasets.</span></p>
<p>Compared to the vanilla GNN, the main feature of the GCN is that it considers node degrees to weigh its features. Before the real implementation, let’s analyze the node degrees in both datasets. This information is relevant since it is directly linked to the performance of <span class="No-Break">the GCN.</span></p>
<p>From what we know about this architecture, we expect it to perform better when node degrees vary greatly. If every node has the same number of neighbors, these architectures are equivalent: (<img alt="" height="61" src="image/Formula_B19153_06_037.png" width="450"/>):</p>
<ol>
<li>We import the <strong class="source-inline">Planetoid</strong> class from PyTorch Geometric. To visualize the node degrees, we also import <strong class="source-inline">matplotlib</strong> and two additional classes: <strong class="source-inline">degree</strong> to get the number of neighbors of each node and <strong class="source-inline">Counter</strong> to count the number of nodes for <span class="No-Break">each degree:</span><pre class="source-code">
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import degree
from collections import Counter
import matplotlib.pyplot as plt</pre></li>
<li>The Cora dataset is imported and its graph is stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">data</strong></span><span class="No-Break">:</span><pre class="source-code">
dataset = Planetoid(root=".", name="Cora")
data = dataset[0]</pre></li>
<li>We<a id="_idIndexMarker322"/> compute the number of neighbors of each node in <span class="No-Break">the graph:</span><pre class="source-code">
degrees = degree(data.edge_index[0]).numpy()</pre></li>
<li>To produce a more natural visualization, we count the number of nodes for <span class="No-Break">each degree:</span><pre class="source-code">
numbers = Counter(degrees)</pre></li>
<li>Let’s plot this result using a <span class="No-Break">bar plot:</span><pre class="source-code">
fig, ax = plt.subplots()
ax.set_xlabel('Node degree')
ax.set_ylabel('Number of nodes')
plt.bar(numbers.keys(), numbers.values())</pre></li>
</ol>
<p>That gives us the <span class="No-Break">following plot.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer254">
<img alt="Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset" height="803" src="image/B19153_06_002.jpg" width="1209"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset</p>
<p>This<a id="_idIndexMarker323"/> distribution looks exponential with a heavy tail: it ranges from 1 neighbor (485 nodes) to 168 neighbors (1 node)! This is exactly the kind of dataset where we want a normalization process to consider <span class="No-Break">this disbalance.</span></p>
<p>The same process is repeated with the Facebook Page-Page dataset with the <span class="No-Break">following result:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page dataset" height="786" src="image/B19153_06_003.jpg" width="1199"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page dataset</p>
<p>This <a id="_idIndexMarker324"/>distribution of node degrees looks even more skewed, with a number of neighbors that ranges from 1 to 709. For the same reason, the Facebook Page-Page dataset is also a good case in which to apply <span class="No-Break">a GCN.</span></p>
<p>We could build our own graph layer but, conveniently enough, PyTorch Geometric already has a predefined GCN layer. Let’s implement it on the Cora <span class="No-Break">dataset first:</span></p>
<ol>
<li>We import PyTorch and the GCN layer from <span class="No-Break">PyTorch Geometric:</span><pre class="source-code">
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv</pre></li>
<li>We create a function to calculate the <span class="No-Break">accuracy score:</span><pre class="source-code">
def accuracy(pred_y, y):
    return ((pred_y == y).sum() / len(y)).item()</pre></li>
<li>We create a GCN class with a <strong class="source-inline">__init_()</strong> function that takes three parameters as input: the number of input dimensions, <strong class="source-inline">dim_in</strong>, the number of hidden dimensions, <strong class="source-inline">dim_h</strong>, and the number of output <span class="No-Break">dimensions, </span><span class="No-Break"><strong class="source-inline">dim_out</strong></span><span class="No-Break">:</span><pre class="source-code">
class GCN(torch.nn.Module):
    """Graph Convolutional Network"""
    def __init__(self, dim_in, dim_h, dim_out):
        super().__init__()
        self.gcn1 = GCNConv(dim_in, dim_h)
        self.gcn2 = GCNConv(dim_h, dim_out)</pre></li>
<li>The <strong class="source-inline">forward</strong> method is identical, and has two GCN layers. A log <strong class="source-inline">softmax</strong> function<a id="_idIndexMarker325"/> is applied to the result <span class="No-Break">for classification:</span><pre class="source-code">
    def forward(self, x, edge_index):
        h = self.gcn1(x, edge_index)
        h = torch.relu(h)
        h = self.gcn2(h, edge_index)
        return F.log_softmax(h, dim=1)</pre></li>
<li>The <strong class="source-inline">fit()</strong> method is the same, with the exact same parameters for the <strong class="source-inline">Adam</strong> optimizer (a learning rate of 0.1 and L2 regularization <span class="No-Break">of 0.0005):</span><pre class="source-code">
    def fit(self, data, epochs):
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(),
                                    lr=0.01,
                                    weight_decay=5e-4)
        self.train()
        for epoch in range(epochs+1):
            optimizer.zero_grad()
            out = self(data.x, data.edge_index)
            loss = criterion(out[data.train_mask], data.y[data.train_mask])
            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])
            loss.backward()
            optimizer.step()
            if(epoch % 20 == 0):
                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])
                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])
                print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:&gt;5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')</pre></li>
<li>We<a id="_idIndexMarker326"/> implement the same <span class="No-Break"><strong class="source-inline">test()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
    @torch.no_grad()
    def test(self, data):
        self.eval()
        out = self(data.x, data.edge_index)
        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])
        return acc</pre></li>
<li>Let’s instantiate and train our model for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
gcn = GCN(dataset.num_features, 16, dataset.num_classes)
print(gcn)
gcn.fit(data, epochs=100)</pre></li>
<li>Here is<a id="_idIndexMarker327"/> the output of <span class="No-Break">the training:</span><pre class="source-code">
<strong class="bold">GCN(</strong>
<strong class="bold">  (gcn1): GCNConv(1433, 16)</strong>
<strong class="bold">  (gcn2): GCNConv(16, 7)</strong>
<strong class="bold">)</strong>
<strong class="bold">Epoch   0 | Train Loss: 1.963 | Train Acc:  8.57% | Val Loss: 1.96 | Val Acc: 9.80%</strong>
<strong class="bold">Epoch  20 | Train Loss: 0.142 | Train Acc: 100.00% | Val Loss: 0.82 | Val Acc: 78.40%</strong>
<strong class="bold">Epoch  40 | Train Loss: 0.016 | Train Acc: 100.00% | Val Loss: 0.77 | Val Acc: 77.40%</strong>
<strong class="bold">Epoch  60 | Train Loss: 0.015 | Train Acc: 100.00% | Val Loss: 0.76 | Val Acc: 76.40%</strong>
<strong class="bold">Epoch  80 | Train Loss: 0.018 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 76.60%</strong>
<strong class="bold">Epoch 100 | Train Loss: 0.017 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 77.20%</strong></pre></li>
<li>Finally, let’s evaluate it on the <span class="No-Break">test set:</span><pre class="source-code">
acc = gcn.test(data)
print(f'GCN test accuracy: {acc*100:.2f}%')
<strong class="bold">GCN test accuracy: 79.70%</strong></pre></li>
</ol>
<p>If we <a id="_idIndexMarker328"/>repeat this experiment 100 times, we obtain an average accuracy score of 80.17% (± 0.61%), which is significantly higher than the 74.98% (± 1.50%) obtained by our <span class="No-Break">vanilla GNN.</span></p>
<p>The exact same model is then applied to the Facebook Page-Page dataset, where it obtains an average accuracy of 91.54% (± 0.28%). Once again, it is significantly higher than the result obtained by the vanilla GNN, with only 84.85% (± 1.68%). The following table summarizes the accuracy scores with <span class="No-Break">standard deviation:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MLP</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">GNN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">GCN</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Cora</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">53.47%</span></p>
<p>(±<span class="No-Break">1.81%)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">74.98%</span></p>
<p>(±<span class="No-Break">1.50%)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80.17%</span></p>
<p>(±<span class="No-Break">0.61%)</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Facebook</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">75.21%</span></p>
<p>(±<span class="No-Break">0.40%)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">84.85%</span></p>
<p>(±<span class="No-Break">1.68%)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">91.54%</span></p>
<p>(±<span class="No-Break">0.28%)</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Summary of accuracy scores with standard deviation</p>
<p>We can attribute these high scores to the wide range of node degrees in these two datasets. By normalizing features and considering the number of neighbors of the central node and its own neighbors, the GCN gains a lot of flexibility and can work well with various types <span class="No-Break">of graphs.</span></p>
<p>Nonetheless, node classification is not the only task that GNNs can perform. In the next section, we’ll see a new type of application that is rarely covered in <span class="No-Break">the literature.</span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor079"/>Predicting web traffic with node regression</h1>
<p>In <a id="_idIndexMarker329"/>machine learning, <strong class="bold">regression</strong> refers to the<a id="_idIndexMarker330"/> prediction of continuous values. It is often contrasted with <strong class="bold">classification</strong>, where<a id="_idIndexMarker331"/> the goal <a id="_idIndexMarker332"/>is to find the correct categories (which are not continuous). In graph data, their counterparts are node classification and node regression. In this section, we will try to predict a continuous value instead of a categorical variable for <span class="No-Break">each node.</span></p>
<p>The dataset <a id="_idIndexMarker333"/>we will use is the Wikipedia Network (GNU General Public License v3.0), introduced by Rozemberckzi et al. in 2019 [2]. It is composed of three page-page networks: chameleons (2,277 nodes and 31,421 edges), crocodiles (11,631 nodes and 170,918 edges), and squirrels (5,201 nodes and 198,493 edges). In these datasets, nodes represent articles and edges are mutual links between them. Node features reflect the presence of particular words in the articles. Finally, the goal is to predict the log average monthly traffic of <span class="No-Break">December 2018.</span></p>
<p>In this section, we <a id="_idIndexMarker334"/>will apply a GCN to predict this traffic on the <span class="No-Break">chameleon dataset:</span></p>
<ol>
<li>We import the Wikipedia Network and download the chameleon dataset. We apply the <strong class="source-inline">transform</strong> function, <strong class="source-inline">RandomNodeSplit()</strong>, to randomly create an evaluation mask and a <span class="No-Break">test mask:</span><pre class="source-code">
from torch_geometric.datasets import WikipediaNetwork
import torch_geometric.transforms as T
dataset = WikipediaNetwork(root=".", name="chameleon", transform = T.RandomNodeSplit(num_val=200, num_test=500))
data = dataset[0]</pre></li>
<li>We print information about <span class="No-Break">this dataset:</span><pre class="source-code">
print(f'Dataset: {dataset}')
print('-------------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of nodes: {data.x.shape[0]}')
print(f'Number of unique features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')</pre></li>
</ol>
<p>This is the output <span class="No-Break">we obtain:</span></p>
<pre class="source-code">
<strong class="bold">Dataset: WikipediaNetwork()</strong>
<strong class="bold">-------------------</strong>
<strong class="bold">Number of graphs: 1</strong>
<strong class="bold">Number of nodes: 2277</strong>
<strong class="bold">Number of unique features: 2325</strong>
<strong class="bold">Number of classes: 5</strong></pre>
<ol>
<li value="3">There is a <a id="_idIndexMarker335"/>problem with our dataset: the output says that we have five classes. However, we want to perform<a id="_idIndexMarker336"/> node regression, not classification. So <span class="No-Break">what happened?</span></li>
</ol>
<p>In fact, these five classes are bins of the continuous values we want to predict. Unfortunately, these labels are not the ones we want: we have to change them manually. First, let’s download the <strong class="source-inline">wikipedia.zip</strong> file from the following page: <a href="https://snap.stanford.edu/data/wikipedia-article-networks.xhtml">https://snap.stanford.edu/data/wikipedia-article-networks.xhtml</a>. After unzipping the file, we import <strong class="source-inline">pandas</strong> and use it to load <span class="No-Break">the targets:</span></p>
<pre class="source-code">
import pandas as pd
df = pd.read_csv('wikipedia/chameleon/musae_chameleon_target.csv')</pre>
<ol>
<li value="4">We apply a log function to the target values using <strong class="source-inline">np.log10()</strong> because the goal is to predict the log average <span class="No-Break">monthly traffic:</span><pre class="source-code">
values = np.log10(df['target'])</pre></li>
<li>We redefine <strong class="source-inline">data.y</strong> as a tensor of the continuous values from the previous step. Note that these values are not normalized in this example, which is a good practice that is usually implemented. We will not perform it here for ease <span class="No-Break">of exposition:</span><pre class="source-code">
data.y = torch.tensor(values)
<strong class="bold">tensor([2.2330, 3.9079, 3.9329,  ..., 1.9956, 4.3598, 2.4409], dtype=torch.float64)</strong></pre></li>
</ol>
<p>Once <a id="_idIndexMarker337"/>again, it is a good idea to visualize the node degrees as we did for the two previous datasets. We use the exact same code to produce the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia Network" height="803" src="image/B19153_6_0005.jpg" width="1189"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia Network</p>
<p>This<a id="_idIndexMarker338"/> distribution has a shorter tail than the previous ones but keeps a similar shape: most nodes have one or a few neighbors, but some of them act as “hubs” and can connect more than <span class="No-Break">80 nodes.</span></p>
<p>In the case of node regression, the distribution of node degrees is not the only type of distribution we should check: the distribution of our target values is also essential. Indeed, non-normal distribution (such as node degrees) tends to be harder to predict. We can use the Seaborn library to plot the target values and compare them to a normal distribution provided <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">scipy.stats.norm</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import seaborn as sns
from scipy.stats import norm
df['target'] = values
sns.distplot(df['target'], fit=norm)</pre>
<p>This <a id="_idIndexMarker339"/>gives us the <span class="No-Break">following plot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="Figure 6.6 – Density plot of target values from the Wikipedia Network" height="788" src="image/B19153_06_006.jpg" width="1162"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Density plot of target values from the Wikipedia Network</p>
<p>This <a id="_idIndexMarker340"/>distribution is not exactly normal, but it is not exponential like the node degrees either. We can expect our model to perform well to predict <span class="No-Break">these values.</span></p>
<p>Let’s implement it step by step with <span class="No-Break">PyTorch Geometric:</span></p>
<ol>
<li>We define the GCN class and the <strong class="source-inline">__init__()</strong> function. This time, we have three <strong class="source-inline">GCNConv</strong> layers with a decreasing number of neurons. The idea behind this encoder architecture is to force the model to select the most relevant features to predict the target values. We also added a linear layer to output a prediction that is not limited to a number between 0 or -1 <span class="No-Break">and 1:</span><pre class="source-code">
class GCN(torch.nn.Module):
    def __init__(self, dim_in, dim_h, dim_out):
        super().__init__()
        self.gcn1 = GCNConv(dim_in, dim_h*4)
        self.gcn2 = GCNConv(dim_h*4, dim_h*2)
        self.gcn3 = GCNConv(dim_h*2, dim_h)
        self.linear = torch.nn.Linear(dim_h, dim_out)</pre></li>
<li>The <strong class="source-inline">forward()</strong> method <a id="_idIndexMarker341"/>includes the new <strong class="source-inline">GCNConv</strong> and <strong class="source-inline">nn.Linear</strong> layers. There is <a id="_idIndexMarker342"/>no need for a log <strong class="source-inline">softmax</strong> function here since we’re not predicting <span class="No-Break">a class:</span><pre class="source-code">
def forward(self, x, edge_index):
    h = self.gcn1(x, edge_index)
    h = torch.relu(h)
    h = F.dropout(h, p=0.5, training=self.training)
    h = self.gcn2(h, edge_index)
    h = torch.relu(h)
    h = F.dropout(h, p=0.5, training=self.training)
    h = self.gcn3(h, edge_index)
    h = torch.relu(h)
    h = self.linear(h)
    return h</pre></li>
<li>The main change in the <strong class="source-inline">fit()</strong> method is the <strong class="source-inline">F.mse_loss()</strong> function, which replaces the cross-entropy loss used in classification tasks. The <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) will be <a id="_idIndexMarker343"/>our main metric. It corresponds to the average of the squares of the errors and can be defined <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="" height="172" src="image/Formula_B19153_06_038.jpg" width="469"/>
</div>
</div>
<ol>
<li>In <a id="_idIndexMarker344"/>code, this is <a id="_idIndexMarker345"/>how it <span class="No-Break">is implemented:</span><pre class="source-code">
    def fit(self, data, epochs):
        optimizer = torch.optim.Adam(self.parameters(),
                                      lr=0.02,
                                      weight_decay=5e-4)
        self.train()
        for epoch in range(epochs+1):
            optimizer.zero_grad()
            out = self(data.x, data.edge_index)
            loss = F.mse_loss(out.squeeze()[data.train_mask], data.y[data.train_mask].float())
            loss.backward()
            optimizer.step()
            if epoch % 20 == 0:
                val_loss = F.mse_loss(out.squeeze()[data.val_mask], data.y[data.val_mask])
                print(f"Epoch {epoch:&gt;3} | Train Loss: {loss:.5f} | Val Loss: {val_loss:.5f}")</pre></li>
<li>The MSE is also included in the <span class="No-Break"><strong class="source-inline">test()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
    @torch.no_grad()
    def test(self, data):
        self.eval()
        out = self(data.x, data.edge_index)
        return F.mse_loss(out.squeeze()[data.test_mask], data.y[data.test_mask].float())</pre></li>
<li>We <a id="_idIndexMarker346"/>instantiate the model with <strong class="source-inline">128</strong> hidden<a id="_idIndexMarker347"/> dimensions and only <strong class="source-inline">1</strong> output dimension (the target value). It is trained on <span class="No-Break"><strong class="source-inline">200</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
gcn = GCN(dataset.num_features, 128, 1)
print(gcn)
gcn.fit(data, epochs=200)
<strong class="bold">GCN(</strong>
<strong class="bold">  (gcn1): GCNConv(2325, 512)</strong>
<strong class="bold">  (gcn2): GCNConv(512, 256)</strong>
<strong class="bold">  (gcn3): GCNConv(256, 128)</strong>
<strong class="bold">  (linear): Linear(in_features=128, out_features=1, bias=True)</strong>
<strong class="bold">)</strong>
<strong class="bold">Epoch   0 | Train Loss: 12.05177 | Val Loss: 12.12162</strong>
<strong class="bold">Epoch  20 | Train Loss: 11.23000 | Val Loss: 11.08892</strong>
<strong class="bold">Epoch  40 | Train Loss: 4.59072 | Val Loss: 4.08908</strong>
<strong class="bold">Epoch  60 | Train Loss: 0.82827 | Val Loss: 0.84340</strong>
<strong class="bold">Epoch  80 | Train Loss: 0.63031 | Val Loss: 0.71436</strong>
<strong class="bold">Epoch 100 | Train Loss: 0.54679 | Val Loss: 0.75364</strong>
<strong class="bold">Epoch 120 | Train Loss: 0.45863 | Val Loss: 0.73487</strong>
<strong class="bold">Epoch 140 | Train Loss: 0.40186 | Val Loss: 0.67582</strong>
<strong class="bold">Epoch 160 | Train Loss: 0.38461 | Val Loss: 0.54889</strong>
<strong class="bold">Epoch 180 | Train Loss: 0.33744 | Val Loss: 0.56676</strong>
<strong class="bold">Epoch 200 | Train Loss: 0.29155 | Val Loss: 0.59314</strong></pre></li>
<li>We<a id="_idIndexMarker348"/> test it to obtain the MSE on the <span class="No-Break">test set:</span><pre class="source-code">
loss = gcn.test(data)
print(f'GCN test loss: {loss:.5f}')
<strong class="bold">GCN test loss: 0.43005</strong></pre></li>
</ol>
<p>This MSE loss is <a id="_idIndexMarker349"/>not the most interpretable metric by itself. We can get more meaningful results using the two <span class="No-Break">following metrics:</span></p>
<ul>
<li>The RMSE, which measures the average magnitude of <span class="No-Break">the error:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="" height="226" src="image/Formula_B19153_06_039.jpg" width="806"/>
</div>
</div>
<ul>
<li>The <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>), which gives the mean absolute difference between the <a id="_idIndexMarker350"/>predicted and <span class="No-Break">real values:</span><div class="IMG---Figure" id="_idContainer260"><img alt="" height="207" src="image/Formula_B19153_06_040.jpg" width="550"/></div></li>
</ul>
<p>Let’s implement them <span class="No-Break">in Python:</span></p>
<ol>
<li>We can<a id="_idIndexMarker351"/> directly import the MSE and the MAE from the <span class="No-Break">scikit-learn library:</span><pre class="source-code">
from sklearn.metrics import mean_squared_error, mean_absolute_error</pre></li>
<li>We convert the PyTorch tensors for the predictions into the NumPy arrays given by the model <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">.detach().numpy()</strong></span><span class="No-Break">:</span><pre class="source-code">
out = gcn(data.x, data.edge_index)
y_pred = out.squeeze()[data.test_mask].detach().numpy()
mse = mean_squared_error(data.y[data.test_mask], y_pred)
mae = mean_absolute_error(data.y[data.test_mask], y_pred)</pre></li>
<li>We compute<a id="_idIndexMarker352"/> the MSE and the MAE with their dedicated function. The RMSE is calculated as the square root of MSE <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">np.sqrt()</strong></span><span class="No-Break">:</span><pre class="source-code">
print('=' * 43)
print(f'MSE = {mse:.4f} | RMSE = {np.sqrt(mse):.4f} | MAE = {mae:.4f}')
print('=' * 43)
<strong class="bold">===========================================</strong>
<strong class="bold">MSE = 0.4300 | RMSE = 0.6558 | MAE = 0.5073</strong>
<strong class="bold">===========================================</strong></pre></li>
</ol>
<p>These metrics are useful for comparing different models, but it can be difficult to interpret the MSE and <span class="No-Break">the RMSE.</span></p>
<p>The best tool to<a id="_idIndexMarker353"/> visualize the results of our model is a scatter plot, where the horizontal axis represents our predictions and the vertical axis represents the real values. Seaborn has a dedicated function (<strong class="source-inline">regplot()</strong>) for this type <span class="No-Break">of visualization:</span></p>
<pre class="source-code">
fig = sns.regplot(x=data.y[data.test_mask].numpy(), y=y_pred)</pre>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values (y-axis)" height="1013" src="image/B19153_06_007.jpg" width="1488"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values (y-axis)</p>
<p>We don’t <a id="_idIndexMarker354"/>have a baseline to work with in this example, but this is a decent prediction with few outliers. It would work in a lot of applications, despite a minimalist dataset. If we wanted to improve these results, we could tune the hyperparameters and do more error analysis to understand where the outliers <span class="No-Break">come from.</span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor080"/>Summary</h1>
<p>In this chapter, we improved our vanilla GNN layer to correctly normalize features. This enhancement introduced the GCN layer and smart normalization. We compared this new architecture to Node2Vec and our vanilla GNN on the Cora and Facebook Page-Page datasets. Thanks to this normalization process, the GCN obtained the highest accuracy scores by a large margin in both cases. Finally, we applied it to node regression with the Wikipedia Network and learned how to handle this <span class="No-Break">new task.</span></p>
<p>In <a href="B19153_07.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Graph Attention Networks</em>, we will go a step further by discriminating neighboring nodes based on their importance. We will see how to automatically weigh node features through a process called self-attention. This will improve our performance, as we will see by comparing it to the <span class="No-Break">GCN architecture.</span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor081"/>Further reading</h1>
<ul>
<li>[1] T. N. Kipf and M. Welling, <em class="italic">Semi-Supervised Classification with Graph Convolutional Networks</em>. arXiv, 2016. DOI: 10.48550/ARXIV.1609.02907. <span class="No-Break">Available: </span><a href="https://arxiv.org/abs/1609.02907"><span class="No-Break">https://arxiv.org/abs/1609.02907</span></a><span class="No-Break">.</span></li>
<li>[2] B. Rozemberczki, C. Allen, and R. Sarkar, <em class="italic">Multi-scale Attributed Node Embedding</em>. arXiv, 2019. DOI: 10.48550/ARXIV.1909.13021. <span class="No-Break">Available: </span><a href="https://arxiv.org/abs/1909.13021"><span class="No-Break">https://arxiv.org/abs/1909.13021</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div></body></html>