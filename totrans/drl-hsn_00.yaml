- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is on reinforcement learning (RL), which is a subfield of machine
    learning (ML); it focuses on the general and challenging problem of learning optimal
    behavior in complex environments. The learning process is driven only by the reward
    value and observations obtained from the environment. This model is very general
    and can be applied to many practical situations, from playing games to optimizing
    complex manufacturing processes. We largely focus on deep RL in this book, which
    is RL that leverages deep learning (DL) methods.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its flexibility and generality, the field of RL is developing very quickly
    and attracting lots of attention, both from researchers who are trying to improve
    existing methods or create new methods and from practitioners interested in solving
    their problems in the most efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Why I wrote this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a lot of ongoing research activity in the RL field all around the world.
    New research papers are being published almost every day, and a large number of
    DL conferences, such as Neural Information Processing Systems (NeurIPS) or the
    International Conference on Learning Representations (ICLR), are dedicated to
    RL methods. There are also several large research groups focusing on the application
    of RL methods to robotics, medicine, multi-agent systems, and others.
  prefs: []
  type: TYPE_NORMAL
- en: However, although information about the recent research is widely available,
    it is too specialized and abstract to be easily understandable. Even worse is
    the situation surrounding the practical aspect of RL, as it is not always obvious
    how to make the step from an abstract method described in its mathematics-heavy
    form in a research paper to a working implementation solving an actual problem.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it hard for somebody interested in the field to get a clear understanding
    of the methods and ideas behind papers and conference talks. There are some very
    good blog posts about various aspects of RL that are illustrated with working
    examples, but the limited format of a blog post allows authors to describe only
    one or two methods, without building a complete structured picture and showing
    how different methods are related to each other in a systematic way. This book
    was written as an attempt to fill this obvious gap in practical and structured
    information about RL methods and approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key aspect of the book is its orientation to practice. Every method is implemented
    for various environments, from the very trivial to the quite complex. I’ve tried
    to make the examples clean and easy to understand, which was made possible by
    the expressiveness and power of PyTorch. On the other hand, the complexity and
    requirements of the examples are oriented to RL hobbyists without access to very
    large computational resources, such as clusters of graphics processing units (GPUs)
    or very powerful workstations. This, I believe, will make the fun-filled and exciting
    RL domain accessible to a much wider audience than just research groups or large
    artificial intelligence companies. On the other hand, this is still deep RL, so
    access to a GPU is highly recommended, as computation speed up will make experimentations
    much more convenient (waiting for several weeks for a single optimization to complete
    is not very fun). Approximately half of the examples in the book will benefit
    from being run on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to traditional medium-sized examples of environments used in RL,
    such as Atari games or continuous control problems, this book contains several
    chapters (10, 13, 14, 19, 20, and 21) that contain larger projects, illustrating
    how RL methods can be applied to more complicated environments and tasks. These
    examples are still not full-sized, real-life projects (they would occupy a separate
    book on their own), but just larger problems illustrating how the RL paradigm
    can be applied to domains beyond the well-established benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note about the examples in Parts 1, 2, and 3 of the book is
    that I’ve tried to make them self-contained, with the source code shown in full.
    Sometimes this has led to the repetition of code pieces (for example, the training
    loop is very similar in most of the methods), but I believe that giving you the
    freedom to jump directly into the method you want to learn is more important than
    avoiding a few repetitions. All examples in the book are available on GitHub at
    [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/),
    and you’re welcome to fork them, experiment, and contribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the source code, several chapters (15, 16, 19, and 22) are accompanied
    by video recordings of the trained model. All these recordings are available in
    the following YouTube playlist: [https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7](https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7).'
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is ideal for machine learning engineers, software engineers, and data
    scientists looking to learn and apply deep RL in practice. It assumes familiarity
    with Python, calculus, and ML concepts. With practical examples and high-level
    overviews, it’s also suitable for experienced professionals looking to deepen
    their understanding of advanced deep RL methods and apply them across industries,
    such as gaming and finance.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1, What Is Reinforcement Learning?, contains an introduction to RL ideas
    and the main formal models.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2, OpenAI Gym API and Gymansium, introduces the practical aspects of
    RL, using the open source library Gym and its descendant, Gymnasium.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3, Deep Learning with PyTorch, gives you a quick overview of the PyTorch
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4, The Cross-Entropy Method, introduces one of the simplest methods
    in RL to give you an impression of RL methods and problems.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5, Tabular Learning and the Bellman Equation, this chapter opens Part
    2 of the book, devoted to value-based family of methods.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6, Deep Q-Networks, describes deep Q-networks (DQNs), an extension of
    the basic value-based methods, allowing us to solve complicated environments.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7, Higher-Level RL Libraries, describes the library PTAN, which we will
    use in the book to simplify the implementations of RL methods.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8, DQN Extensions, gives a detailed overview of a modern extension to
    the DQN method, to improve its stability and convergence in complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9, Ways to Speed up RL Methods, provides an overview of ways to make
    the execution of RL code faster.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10, Stocks Trading Using RL, is the first practical project and focuses
    on applying the DQN method to stock trading.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 11, Policy Gradients, opens Part 3 of the book and introduces another
    family of RL methods that is based on direct policy optimisation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 12, The Actor-Critic Method: A2C and A3C, describes one of the most
    widely used policy-based method in RL.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 13, The TextWorld Environment, covers the application of RL methods
    to interactive fiction games.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 14, Web Navigation, is another long project that applies RL to web page
    navigation using the MiniWoB++ environment.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 15, Continuous Action Space, opens the advanced RL part of the book
    and describes the specifics of environments using continuous action spaces and
    various methods (widely used in robotics).
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 16, Trust Regions, is yet another chapter about continuous action spaces
    describing the trust region set of methods: PPO, TRPO, ACKTR and SAC.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 17, Black-Box Optimization in RL, shows another set of methods that
    don’t use gradients in their explicit form.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 18, Advanced Exploration, covers different approaches that can be used
    for better exploration of the environment — a very important aspect of RL.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 19, Reinforcement Learning with Human Feedback, introduces and implements
    recent approach to guide the process of learning by giving human feedback. This
    methed is widely used in training large language models (LLMs). In this chapter,
    we’ll implement RLHF pipeline from scratch and check its efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 20, AlphaGo Zero and MuZero, describes the AlphaGo Zero method and its
    evolution into MuZero, and applies both these methods to the game Connect 4.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 21, RL in Discrete Optimization, describes the application of RL methods
    to the domain of discrete optimization, using the Rubik’s cube as an environment.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 22, Multi-Agent RL, introduces a relatively new direction of RL methods
    for situations with multiple agents.
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is suitable for you if you’re using a machine with at least 32 GB
    of RAM. A GPU is not strictly required, but an Nvidia GPU is highly recommended.
    The code has been tested on Linux and macOS. For more details on the hardware
    and software requirements, refer to Chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the chapters in this book that describe RL methods have the same structure:
    in the beginning, we discuss the motivation of the method, its theoretical foundation,
    and the idea behind it. Then, we follow several examples of the method applied
    to different environments with the full source code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the book in different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: To quickly become familiar with a particular method, you can read only the introductory
    part of the relevant chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get a deeper understanding of the way the method is implemented, you can
    read the code and the explanations accompanying it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To gain a deeper familiarity with the method (which I beleive is the best way
    to learn) you can try to reimplement the method and make it work, using the provided
    source code as a reference point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whichever approach you choose, I hope the book will be useful for you!
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the third edition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In comparison to the second edition of this book (published in 2020), there
    are several major changes made to the book’s contents in this new edition:'
  prefs: []
  type: TYPE_NORMAL
- en: All the dependencies of code examples have been updated to the recent versions
    or replaced with better alternatives. For example, OpenAI Gym is not supported
    anymore, but we have the Farama Foundation Gymnasium fork. Another example is
    the MiniWoB++ library, which has replaced the MiniWoB and Universe environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new chapter on RLHF has been included, and the MuZero method has been added
    to the chapter on AlphaGo Zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are lots of small fixes and improvements — most of the figures have been
    redrawn to make them clearer and more easily understandable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better meet book volume limitations, several chapters were rearranged, which
    I hope made the book more consistent and easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code bundle for the book is hosted on GitHub at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://packt.link/gbp/9781835882702](https://packt.link/gbp/9781835882702).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of text conventions used throughout this book. CodeInText:
    Indicates code words in text, database table names, folder names, filenames, file
    extensions, pathnames, dummy URLs, user input, and Twitter handles. For example:
    ”For the reward table, it is represented as a tuple with [State, Action, State]
    and for the transition table, it is written as [State, Action].”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Bold: Indicates a new term, an important word, or words that you see on the
    screen. For instance, words in menus or dialog boxes appear in the text like this.
    For example: ”The second term is called cross-entropy, which is a very common
    optimization objective in deep learning.” Citations are represented using a condensed
    author–year format within square brackets, similar to [Sut88] or [Kro+11]. You
    can find the details of the corresponding paper in the Bibliography section at
    the end of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'General feedback: Email feedback@packtpub.com and mention the book’s title
    in the subject of your message. If you have questions about any aspect of this
    book, please email us at questions@packtpub.com.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Errata: Although we have taken every care to ensure the accuracy of our content,
    mistakes do happen. If you have found a mistake in this book, we would be grateful
    if you reported this to us. Please visit [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    click Submit Errata, and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Piracy: If you come across any illegal copies of our works in any form on the
    internet, we would be grateful if you would provide us with the location address
    or website name. Please contact us at copyright@packtpub.com with a link to the
    material.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in becoming an author: If there is a topic that you have
    expertise in and you are interested in either writing or contributing to a book,
    please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Leave a Review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an [Amazon review](https://packt.link/r/1835882714);
    it will only take a minute, but it makes a big difference for readers like you.
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code below to receive a free ebook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*https://packt.link/NzOWQ*'
  prefs: []
  type: TYPE_NORMAL
- en: Download a free PDF copy of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks for purchasing this book!
  prefs: []
  type: TYPE_NORMAL
- en: Do you like to read on the go but are unable to carry your print books everywhere?
    Is your eBook purchase not compatible with the device of your choice?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry; with every Packt book, you now get a DRM-free PDF version of that
    book at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: Read anywhere, on any device. Search, copy, and paste code from your favorite
    technical books directly into your application.
  prefs: []
  type: TYPE_NORMAL
- en: The perks don’t stop there! You can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these simple steps to get the benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scan the QR code or visit the link below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![PIC](img/file4.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*https://packt.link/free-ebook/9781835882702*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Submit your proof of purchase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! We’ll send your free PDF and other benefits to your email address
    directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
