- en: Up and Running with Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What will **artificial intelligence** (**AI**) look like in the future? As 
    applications of AI algorithms and software become more prominent, it is a question
    that should interest many. Researchers and practitioners of AI face further relevant
    questions; how will we realize what we envision and solve known problems? What
    kinds of innovations and algorithms are yet to be developed? Several subfields
    in machine learning display great promise toward answering many of our questions.
    In this book, we shine the spotlight on reinforcement learning, one such, area
    and perhaps one of the most exciting topics in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is motivated by the objective to learn from the environment
    by interacting with it. Imagine an infant and how it goes about in its environment.
    By moving around and acting upon its surroundings, the infant learns about physical
    phenomena, causal relationships, and various attributes and properties of the
    objects he or she interacts with. The infant's learning is often motivated by
    a desire to accomplish some objective, such as playing with surrounding objects
    or satiating some spark of curiosity. In reinforcement learning, we pursue a similar
    endeavor; we take a computational approach toward learning about the environment.
    In other words, our goal is to design algorithms that learn through their interactions
    with the environment in order to accomplish a task.
  prefs: []
  type: TYPE_NORMAL
- en: What use do such algorithms provide? By having a generalized learning algorithm,
    we can offer effective solutions to several real-world problems. A prominent example
    is the use of reinforcement learning algorithms to drive cars autonomously. While
    not fully realized, such use cases would provide great benefits to society, for
    reinforcement learning algorithms have empirically proven their ability to surpass
    human-level performance in several tasks. One watershed moment occurred in 2016
    when DeepMind's AlphaGo program defeated 18-time Go world champion Lee Sedol four
    games to one. AlphaGo was essentially able to learn and surpass three millennia
    of Go wisdom cultivated by humans in a matter of months. Recently, reinforcement
    learning algorithms have been shown to be effective in playing more complex, real-time
    multi-agent games such as Dota. The same algorithms that power these game-playing
    algorithms have also succeeded in controlling robotic arms to pick up objects
    and navigating drones through mazes. These examples suggest not only what these
    algorithms are capable of, but also what they can potentially accomplish down
    the road.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book offers a practical guide for those eager to learn about reinforcement
    learning. We will take a hands-on approach toward learning about reinforcement
    learning by going through numerous examples of algorithms and their applications.
    Each chapter focuses on a particular use case and introduces reinforcement learning
    algorithms that are used to solve the given problem. Some of these use cases rely
    on state-of-the-art algorithms; hence through this book, we will learn about and
    implement some of the best-performing algorithms and techniques in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The projects increase in difficulty/complexity as you go through the book.
    The following table describes what you will learn from each chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Chapter name** | **The use case/problem** | **Concepts/algorithms/technologies
    discussed and used** |'
  prefs: []
  type: TYPE_TB
- en: '| *Balancing Cart Pole* | Control horizontal movement of a cart to balance
    a vertical bar | OpenAI Gym framework, Q-Learning |'
  prefs: []
  type: TYPE_TB
- en: '| *Playing Atari Games* | Play various Atari games at human-level proficiency
    | Deep Q-Networks |'
  prefs: []
  type: TYPE_TB
- en: '| *Simulating Control Tasks* | Control agents in a continuous action space
    as opposed to a discrete one | **Deterministic policy gradients** (**DPG**), **Trust
    Region Policy Optimization** (**TRPO**), multi-tasking |'
  prefs: []
  type: TYPE_TB
- en: '| *Building Virtual Worlds in Minecraft* | Navigate a character in the virtual
    world of Minecraft | Asynchronous Advantage Actor-Critic (**A3C**) |'
  prefs: []
  type: TYPE_TB
- en: '| *Learning to Play Go* | Go, one of the oldest and most complex board games
    in the world | Monte Carlo tree search, policy and value networks |'
  prefs: []
  type: TYPE_TB
- en: '| *Creating a Chatbot* | Generating natural language in a conversational setting
    | Policy gradient methods, **Long Short-Term Memory** (**LSTM**) |'
  prefs: []
  type: TYPE_TB
- en: '| *Auto Generating a Deep Learning Image Classifier* | Create an agent that
    generates neural networks to solve a given task | Recurrent neural networks, policy
    gradient methods (REINFORCE) |'
  prefs: []
  type: TYPE_TB
- en: '| *Predicting Future Stock Prices* | Predict stock prices and make buy and
    sell decisions | Actor-Critic methods, time-series analysis, experience replay
    |'
  prefs: []
  type: TYPE_TB
- en: Expectations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book is best suited for the reader who:'
  prefs: []
  type: TYPE_NORMAL
- en: Has intermediate proficiency in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Possesses a basic understanding of machine learning and deep learning, especially
    for the following topics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for better generalization and reduced overfitting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enjoys a hands-on, practical approach toward learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since this book serves as a practical introduction to the field, we try to
    keep theoretical content to a minimum. However, it is advisable for the reader
    to have basic knowledge of some of the fundamental mathematical and statistical
    concepts on which the field of machine learning depends. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculus (single and multivariate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear algebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having some experience with these subjects would greatly assist the reader in
    understanding the concepts and algorithms we will cover throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware and software requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ensuing chapters will require you to implement various reinforcement learning
    algorithms. Hence a proper development environment is necessary for a smooth learning
    journey. In particular, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A computer running either macOS or the Linux operating system (for those on
    Windows, try setting up a Virtual Machine with a Linux image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A stable internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GPU (preferably)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will exclusively use the Python programming language to implement our reinforcement
    learning and deep learning algorithms. Moreover, we will be using Python 3.6\.
    A list of libraries we will be using can be found on the official GitHub repository,
    located at ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    You will also find the implementations of every algorithm we will cover in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming you have a working Python installation, you can install all the required
    packages using the `requirements.txt` file found in our repository. We also recommend
    you create a `virtualenv` to isolate your development environment from your main
    OS system. The following steps will help you construct an environment and install
    the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And now you are all set and ready to start! The next few sections of this chapter
    will introduce the field of reinforcement learning and will also provide a refresher
    on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: What is reinforcement learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our journey begins with understanding what reinforcement learning is about.
    Those who are familiar with machine learning may be aware of several learning
    paradigms, namely **supervised learning** and **unsupervised learning**. In supervised
    learning, a machine learning model has a supervisor that gives the ground truth for
    every data point. The model learns by minimizing the distance between its own
    prediction and the ground truth. The dataset is thus required to have an annotation
    for each data point, for example, each image of a dog and a cat would have its
    respective label. In unsupervised learning, the model does not have access to
    the ground truths of the data and thus has to learn about the distribution and
    patterns of the data without them.
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, the agent refers to the model/algorithm that learns
    to complete a particular task. The agent learns primarily by receiving **reward
    signals**, which is a scalar indication of how well the agent is performing a
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an agent that is tasked with controlling a robot's walking movement;
    the agent would receive positive rewards for successfully walking toward a destination
    and negative rewards for falling/failing to make progress.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, unlike in supervised learning, these reward signals are not given
    to the model immediately; rather, they are returned as a consequence of a sequence
    of **actions** that the agent makes. Actions are simply the things an agent can
    do within its **environment**. The environment refers to the world in which the
    agent resides and is primarily responsible for returning reward signals to the
    agent. An agent's actions are usually conditioned on what the agent perceives
    from the environment. What the agent perceives is referred to as the **observation**
    or the **state** of the environment. What further distinguishes reinforcement
    learning from other paradigms is that the actions of the agent can alter the environment
    and its subsequent responses.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose an agent is tasked with playing Space Invaders, the popular
    Atari 2600 arcade game. The environment is the game itself, along with the logic
    upon which it runs. During the game, the agent queries the environment to make
    an observation. The observation is simply an array of the (210, 160, 3) shape,
    which is the screen of the game that displays the agent's ship, the enemies, the
    score, and any projectiles. Based on this observation, the agent makes some actions,
    which can include moving left or right, shooting a laser, or doing nothing. The
    environment receives the agent's action as input and makes any necessary updates
    to the state.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if a laser touches an enemy ship, it is removed from the game.
    If the agent decides to simply move to the left, the game updates the agent's
    coordinates accordingly. This process repeats until a **terminal state***,* a
    state that represents the end of the sequence, is reached. In Space Invaders,
    the terminal state corresponds to when the agent's ship is destroyed, and the
    game subsequently returns the score that it keeps track of, a value that is calculated
    based on the number of enemy ships the agent successfully destroys.
  prefs: []
  type: TYPE_NORMAL
- en: Note that some environments do not have terminal states, such as the stock market.
    These environments keep running for as long as they exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recap the terms we have learned about so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Term** | **Description** | **Examples** |'
  prefs: []
  type: TYPE_TB
- en: '| Agent | A model/algorithm that is tasked with learning to accomplish a task.
    | Self-driving cars, walking robots, video game players |'
  prefs: []
  type: TYPE_TB
- en: '| Environment | The world in which the agent acts. It is responsible for controlling
    what the agent perceives and providing feedback on how well the agent is performing
    a particular task. | The road on which a car drives, a video game, the stock market
    |'
  prefs: []
  type: TYPE_TB
- en: '| Action | A decision the agent makes in an environment, usually dependent
    on what the agent perceives. | Steering a car, buying or selling a particular
    stock, shooting a laser from the spaceship the agent is controlling |'
  prefs: []
  type: TYPE_TB
- en: '| Reward signal | A scalar indication of how well the agent is performing a
    particular task. | Space Invaders score, return on investment for some stock,
    distance covered by a robot learning to walk |'
  prefs: []
  type: TYPE_TB
- en: '| Observation/state | A description of the environment as can be perceived
    by the agent. | Video from a dashboard camera, the screen of the game, stock market
    statistics |'
  prefs: []
  type: TYPE_TB
- en: '| Terminal state | A state at which no further actions can be made by the agent.
    | Reaching the end of a maze, the ship in Space Invaders getting destroyed |'
  prefs: []
  type: TYPE_TB
- en: 'Put formally, at a given timestep, `t`, the following happens for an agent, `P`, and
    environment, `E`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How does the environment compute![](img/c1cbd545-2753-478c-866f-ed057127bf3d.png)and ![](img/8f52597c-a3e6-43ed-baaa-1e849131b1fe.png)?
    The environment usually has its own algorithm that computes these values based
    on numerous input/factors, including what action the agent takes.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the environment is composed of multiple agents that try to maximize
    their own rewards. The way gravity acts upon a ball that we drop from a height
    is a good representation of how the environment works; just like how our surroundings
    obey the laws of physics, the environment has some internal mechanism for computing
    rewards and the next state. This internal mechanism is usually hidden to the agent,
    and thus our job is to build agents that can learn to do a good job at their respective
    tasks, despite this uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss in more detail the main protagonist
    of every reinforcement learning problem—the agent.
  prefs: []
  type: TYPE_NORMAL
- en: The agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of a reinforcement learning agent is to learn to perform a task well
    in an environment. Mathematically, this means to maximize the cumulative reward,
    *R*, which can be expressed in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34073884-68e7-4113-9acc-2d51f9f4216f.png)'
  prefs: []
  type: TYPE_IMG
- en: We are simply calculating a weighted sum of the reward received at each timestep.![](img/349f3223-b4de-429a-a18a-8ec1a2f14483.png)is
    called the **discount factor**, which is a scalar value between 0 and 1\. The
    idea is that the later a reward comes, the less valuable it becomes. This reflects
    our perspectives on rewards as well; that we'd rather receive $100 now rather
    than a year later shows how the same reward signal can be valued differently based
    on its proximity to the present.
  prefs: []
  type: TYPE_NORMAL
- en: Because the mechanics of the environment are not fully observable or known to
    the agent, it must gain information by performing an action and observing how
    the environment reacts to it. This is much like how humans learn to perform certain
    tasks as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we are learning to play chess. While we don''t have all the possible
    moves committed to memory or know exactly how an opponent will play, we are able
    to improve our proficiency over time. In particular, we are able to become proficient
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to react to a move made by the opponent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing how good of a position we are in to win the game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting what the opponent will do next and using that prediction to decide
    on a move
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how others would play in a similar situation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In fact, reinforcement learning agents can learn to do similar things. In particular,
    an agent can be composed of multiple functions and models to assist its decision-making.
    There are three main components that an agent can have: the policy, the value
    function, and the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A policy is an algorithm or a set of rules that describe how an agent makes
    its decisions. An example policy can be the strategy an investor uses to trade
    stocks, where the investor buys a stock when its price goes down and sells the
    stock when the price goes up.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, a policy is a function, usually denoted as ![](img/82dc222a-f23e-4460-ac00-c4ea31ac3cd5.png),
    that maps a state, ![](img/170b9e93-b340-45e9-988b-09c9e07e7fce.png), to an action, ![](img/db64ce78-6f6b-4bd8-913f-f1e4ec162373.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67683436-16d2-412c-ade2-3dcd805dbb1f.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that an agent decides its action given its current state. This function
    can represent anything, as long as it can receive a state as input and output
    an action, be it a table, graph, or machine learning classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we have an agent that is supposed to navigate a maze.
    We shall further assume that the agent knows what the maze looks like; the following
    is how the agent''s policy can be represented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b35ccef7-dfd6-4d7e-8f9d-38584fd4bf87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A maze where each arrow indicates where an agent would go next'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each white square in this maze represents a state the agent can be in. Each
    blue arrow refers to the action an agent would take in the corresponding square.
    This essentially represents the agent''s policy for this maze. Moreover, this
    can also be regarded as a deterministic policy, for the mapping from the state
    to the action is deterministic. This is in contrast to a stochastic policy, where
    a policy would output a probability distribution over the possible actions given
    some state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55a6880b-56ca-4c80-b1c8-22b60a84e942.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here,![](img/fbac4100-413c-4698-b8dc-bee15f1dc709.png)is a normalized probability
    vector over all the possible actions, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A policy mapping the game state (the screen) to actions (probabilities)'
  prefs: []
  type: TYPE_NORMAL
- en: The agent playing the game of Breakout has a policy that takes the screen of
    the game as input and returns a probability for each possible action.
  prefs: []
  type: TYPE_NORMAL
- en: Value function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second component an agent can have is called the **value function**. As
    mentioned previously, it is useful to assess your position, good or bad, in a
    given state. In a game of chess, a player would like to know the likelihood that
    they are going to win in a board state. An agent navigating a maze would like
    to know how close it is to the destination. The value function serves this purpose;
    it predicts the expected future reward an agent would receive in a given state.
    In other words, it measures whether a given state is desirable for the agent.
    More formally, the value function takes a state and a policy as input and returns
    a scalar value representing the expected cumulative reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37ff7369-c631-4dcd-ac2a-3a8e563ae28c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take our maze example, and suppose the agent receives a reward of -1 for every
    step it takes. The agent''s goal is to finish the maze in the smallest number
    of steps possible. The value of each state can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0d2e053-a325-409f-b5b6-43627cd96e45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A maze where each square indicates the value of being in the state'
  prefs: []
  type: TYPE_NORMAL
- en: Each square basically represents the number of steps it takes to get to the
    end of the maze. As you can see, the smallest number of steps required to reach
    the goal is 15.
  prefs: []
  type: TYPE_NORMAL
- en: How can the value function help an agent perform a task well, other than informing
    us of how desirable a given state is? As we will see in the following sections,
    value functions play an integral role in predicting how well a sequence of actions
    will do even before the agent performs them. This is similar to chess players
    imagining how well a sequence of future actions will do in improving his or her 
    chances of winning. To do this, the agent also needs to have an understanding
    of how the environment works. This is where the third component of an agent, the **model**,
    becomes relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we discussed how the environment is not fully known
    to the agent. In other words, the agent usually does not have an idea of how the
    internal algorithm of the environment looks. The agent thus needs to interact
    with it to gain information and learn how to maximize its expected cumulative
    reward. However, it is possible for the agent to have an internal replica, or
    a model, of the environment. The agent can use the model to predict how the environment
    would react to some action in a given state. A model of the stock market, for
    example, is tasked with predicting what the prices will look like in the future.
    If the model is accurate, the agent can then use its value function to assess
    how desirable future states look. More formally, a model can be denoted as a function, ![](img/9cbc6ea0-aae1-46ea-b5f8-e0d8ac439aa0.png), that
    predicts the probability of the next state given the current state and an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5605d9cb-dc64-4b04-8515-c60b34e88486.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other scenarios, the model of the environment can be used to enumerate possible
    future states. This is commonly used in turn-based games, such as chess and tic-tac-toe,
    where the rules and scope of possible actions are clearly defined. Trees are often
    used to illustrate the possible sequence of actions and states in turn-based games:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A model using its value function to assess possible moves'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example of the tic-tac-toe game,![](img/357a67fa-d45f-4755-90ce-03d347133091.png)denotes
    the possible states that taking the![](img/e8560c16-362b-4411-b69f-50eddf987dba.png)action
    (represented as the shaded circle) could yield in a given state, ![](img/02d85b2c-a516-4199-8255-d805511504ea.png).
    Moreover, we can calculate the value of each state using the agent's value function.
    The middle and bottom states would yield a high value since the agent would be
    one step away from victory, whereas  the top state would yield a medium value
    since the agent needs to prevent the opponent from winning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review the terms we have covered so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Term** | **Description** | **What does it output?** |'
  prefs: []
  type: TYPE_TB
- en: '| Policy | The algorithm or function that outputs decisions the agent makes
    | A scalar/single decision (deterministic policy) or a vector of probabilities
    over possible actions (stochastic policy) |'
  prefs: []
  type: TYPE_TB
- en: '| Value Function | The function that describes how good or bad a given state
    is | A scalar value representing the expected cumulative reward |'
  prefs: []
  type: TYPE_TB
- en: '| Model | An agent''s representation of the environment, which predicts how
    the environment will react to the agent''s actions | The probability of the next
    state given an action and current state, or an enumeration of possible states
    given the rules of the environment |'
  prefs: []
  type: TYPE_TB
- en: 'In the following sections, we will use these concepts to learn about one of
    the most fundamental frameworks in reinforcement learning: the Markov decision
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process (MDP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Markov decision process is a framework used to represent the environment
    of a reinforcement learning problem. It is a graphical model with directed edges
    (meaning that one node of the graph points to another node). Each node represents
    a possible state in the environment, and each edge pointing out of a state represents
    an action that can be taken in the given state. For example, consider the following
    MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3689b1c6-d71e-44ff-b5bf-e7ddf2ac33aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A sample Markov Decision Process'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding MDP represents what a typical day of a programmer could look like.
    Each circle represents a particular state the programmer can be in, where the
    blue state (Wake Up) is the initial state (or the state the agent is in at *t*=0),
    and the orange state (Publish Code) denotes the terminal state. Each arrow represents
    the transitions that the programmer can make between states. Each state has a
    reward that is associated with it, and the higher the reward, the more desirable
    the state is.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can tabulate the rewards as an adjacency matrix as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **State\action** | **Wake Up** | **Netflix** | **Code and debug** | **Nap**
    | **Deploy** | **Sleep** |'
  prefs: []
  type: TYPE_TB
- en: '| Wake Up | N/A | -2 | -3 | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Netflix | N/A | -2 | N/A | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Code and debug | N/A | N/A | N/A | 1 | 10 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Nap | 0 | N/A | N/A | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Deploy | N/A | N/A | N/A | N/A | N/A | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Sleep | N/A | N/A | N/A | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: The left column represents the possible states and the top row represents the
    possible actions. N/A means that the action is not performable from the given
    state. This system basically represents the decisions that a programmer can make
    throughout their day.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the programmer wakes up, they can either decide to work (code and debug
    the code) or watch Netflix. Notice that the reward for watching Netflix is higher
    than that of coding and debugging. For the programmer in question, watching Netflix
    seems like a more rewarding activity, while coding and debugging is perhaps a
    chore (which, I hope, is not the case for the reader!). However, both actions
    yield negative rewards, even though our objective is to maximize our cumulative
    reward. If the programmer chooses to watch Netflix, they will be stuck in an endless
    loop of binge-watching, which continuously lowers the reward. Rather, more rewarding
    states will become available to the programmer if they decide to code diligently.
    Let''s look at the possible trajectories, which are the sequence of actions, the
    programmer can take:'
  prefs: []
  type: TYPE_NORMAL
- en: Wake Up | Netflix | Netflix | ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wake Up | Code and debug | Nap | Wake Up | Code and debug | Nap | ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wake Up | Code and debug | Sleep
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wake Up | Code and debug | Deploy | Sleep
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both the first and second trajectories represent infinite loops. Let''s calculate
    the cumulative reward for each, where we set ![](img/2ce1a228-1215-4a7d-8ede-d76546086518.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e4c2b34-4bcc-483b-af84-4487c6bcbada.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/f13ee029-465e-44d3-a489-b3f0f3efe3b2.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/df4ac549-14a7-409e-9a1a-903e43cc4000.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/dd65787b-7681-4863-af35-b177d76c4ee2.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: It is easy to see that both the first and second trajectories, despite not reaching
    a terminal state, will never return positive rewards. The fourth trajectory yields
    the highest reward (successfully deploying code is a highly rewarding accomplishment!).
  prefs: []
  type: TYPE_NORMAL
- en: What we have calculated are the value functions for four policies that a programmer
    can take to go through their day. Recall that the value function is the expected
    cumulative reward starting from a given state and following a policy. We have
    observed four possible policies and have evaluated how each leads to a different
    cumulative reward; this exercise is also called **policy evaluation**. Moreover,
    the equations we have applied to calculate the expected rewards are also known
    as **Bellman expectation equations**. The Bellman equations are a set of equations
    used to evaluate and improve policies and value functions to help a reinforcement
    learning agent learn better. Though a thorough introduction to Bellman equations
    is outside the scope of this book, they are foundational to building a theoretical
    understanding of reinforcement learning. We encourage the reader to look into
    this further.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we will not cover Bellman equations in depth, we highly recommend the
    reader to do so in order to build a solid understanding of reinforcement learning.
    For more information, refer to *Reinforcement Learning: An Introduction*, by Richard
    S. Sutton and Andrew Barto (reference at the end of this chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned about some the key terms and concepts of reinforcement
    learning, you may be wondering how we teach a reinforcement learning agent to
    maximize its reward, or in other words, find that the fourth trajectory is the
    best. In this book, you will be working on solving this question for numerous
    tasks and problems, all using deep learning. While we encourage you to be familiar
    with the basics of deep learning, the following sections will serve as a light
    refresher to the field.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become one of the most popular and recognizable fields of
    machine learning and computer science. Thanks to an increase in both available
    data and computational resources, deep learning algorithms have successfully surpassed
    previous state-of-the-art results in countless tasks. For several domains, including
    image recognition and playing Go, deep learning has even exceeded the capabilities
    of mankind.
  prefs: []
  type: TYPE_NORMAL
- en: It is thus not surprising that many reinforcement learning algorithms have started
    to utilize deep learning to bolster performance. Many of the reinforcement learning
    algorithms from the beginning of this chapter rely on deep learning. This book,
    too, will revolve around deep learning algorithms used to tackle reinforcement
    learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will serve as a refresher on some of the most fundamental
    concepts of deep learning, including neural networks, backpropagation, and convolution.
    However, if are unfamiliar with these topics, we highly encourage you to seek
    other sources for a more in-depth introduction.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A neural network is a type of computational architecture that is composed of
    layers of perceptrons. A perceptron, first conceived  in the 1950s by Frank Rosenblatt,
    models the biological neuron and computes a linear combination of a vector of
    input. It also outputs a transformation of the linear combination using a non-linear
    activation, such as the sigmoid function. Suppose a perceptron receives an input
    vector of ![](img/b39f4391-4708-416b-b9ec-d84b48dd6199.png). The output, *a*,
    of the perceptron, would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0b5df84-5d65-4f22-8c83-ffac52e8f37b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/83824b91-f9f3-465e-9729-60b612a40ac7.png)'
  prefs: []
  type: TYPE_IMG
- en: Where![](img/f5edf276-9cdb-4ffb-9a46-b75cf44e960a.png)are the weights of the
    perceptron, *b* is a constant, called the **bias**, and![](img/4a6b153a-6786-4012-8b2a-af618dba2365.png)is
    the sigmoid activation function that outputs a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons have been widely used as a computational model to make decisions.
    Suppose the task was to predict the likelihood of sunny weather the next day.
    Each![](img/0d974f85-0cd9-4f6a-9af3-185498a44dd6.png)would represent a variable,
    such as the temperature of the current day, humidity, or the weather of the previous
    day. Then,![](img/5a2faf8a-f99b-4355-8570-70d24f7dfb7b.png)would compute a value
    that reflects how likely it is that there will be sunny weather tomorrow. If the
    model has a good set of values for ![](img/d46a16f7-4915-42c1-8f20-1d63cdc4612b.png),
    it is able to make accurate decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a typical neural network, there are multiple layers of neurons, where each
    neuron in a given layer is connected to all neurons in the prior and subsequent
    layers. Hence these layers are also referred to as **fully-connected layers**.
    The weights of a given layer, *l*, can be represented as a matrix, *W^l*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b87c57b-a48f-45a5-be1e-1e8975ee902e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/888affc6-ff04-4339-b490-62aff2826f91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where each *w[ij]* denotes the weight between the *i* neuron of the previous
    layer and the *j* neuron of this layer. *B^l* denotes a vector of biases, one
    for each neuron in the *l* layer. Hence, the activation, *a^l*, of a given layer, *l*,
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16fbd6ed-bd48-47ec-8f89-fcf3e21390d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/89cfdef4-6377-40c7-a9c2-c7b3689f9ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *a⁰(x)* is just the input. Such neural networks with multiple layers
    of neurons are called **multilayer perceptrons** (**MLP**). There are three components
    in an MLP: the input layer, the hidden layers, and the output layer. The data
    flows from the input layer, transformed through a series of linear and non-linear
    functions in the hidden layers, and is outputted from the output layer as a decision
    or a prediction. Hence this architecture is also referred to as a feed-forward
    network. The following diagram shows what a fully-connected network would look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84aad163-22ce-4216-b5bb-afd517c12ce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A sketch of a multilayer perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, a neural network's performance depends on how good
    the values of *W* are (for simplicity, we will refer to both the weights and biases
    as *W*). When the whole network grows in size, it becomes untenable to manually
    determine the optimal weights for each neuron in every layer. Therefore, we rely
    on backpropagation, an algorithm that iteratively and automatically updates the
    weights of every neuron.
  prefs: []
  type: TYPE_NORMAL
- en: To update the weights, we first need the ground truth, or the target value that
    the neural network tries to output. To understand what this ground truth could
    look like, we formulate a sample problem. The `MNIST` dataset is a large repository
    of 28x28 images of handwritten digits. It contains 70,000 images in total and
    serves as a popular benchmark for machine learning models. Given ten different
    classes of digits (from zero to nine), we would like to identify which digit class
    a given images belongs to. We can represent the ground truth of each image as
    a vector of length 10, where the index of the class (starting from 0) is marked
    as 1 and the rest are 0s. For example, an image, *x*, with a class label of five
    would have the ground truth of ![](img/82513266-77d8-4584-9f41-3c233a88f953.png),
    where *y* is the target function we approximate.
  prefs: []
  type: TYPE_NORMAL
- en: What should the neural network look like? If we take each pixel in the image
    to be an input, we would have 28x28 neurons in the input layer (every image would
    be flattened to become a 784-dimensional vector). Moreover, because there are
    10 digit classes, we have 10 neurons in the output layer, each neuron producing
    a sigmoid activation for a given class. There can be an arbitrary number of neurons
    in the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let *f* represent the sequence of transformations that the neural network computes,
    parameterized by the weights, *W*. *f* is essentially an approximation of the
    target function, *y*, and maps the 784-dimensional input vector to a 10 dimensional
    output prediction. We classify the image according to the index of the largest
    sigmoid output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have formulated the ground truth, we can measure the distance between
    it and the network''s prediction. This error is what allows the network to update
    its weights. We define the error function *E(W)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11792f9a-716f-4a06-b64a-82ac338d94d3.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of backpropagation is to minimize *E* by finding the right set of *W*.
    This minimization is an optimization problem whereby we use gradient descent to
    iteratively compute the gradients of *E* with respect to *W* and propagate them
    through the network starting from the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, an in-depth explanation of backpropagation is outside the scope
    of this introductory chapter. If you are unfamiliar with this concept, we highly
    encourage you to study it first.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using backpropagation, we are now able to train large networks automatically.
    This has led to the development of increasingly complex neural network architectures.
    One example is the **convolutional neural network** (**CNN**). There are mainly
    three types of layers in a CNN: the convolutional layer, the pooling layer, and
    the fully-connected layer. The fully-connected layer is identical to the standard
    neural network discussed previously. In the convolutional layer, weights are part
    of convolutional kernels. Convolution on a two-dimensional array of image pixels
    is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c81ae657-337c-4b21-be08-f4a2a7dc81f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *f(u, v)* is the pixel intensity of the input at coordinate *(u, v)*,
    and *g(x-u, y-v)* is the weight of the convolutional kernel at that location.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer comprises a stack of convolutional kernels; hence the
    weights of a convolutional layer can be visualized as a three-dimensional box
    as opposed to the two-dimensional array that we defined for fully-connected layers.
    The output of a single convolutional kernel applied to an input is also a two-dimensional
    mapping, which we call a filter. Because there are multiple kernels, the output
    of a convolutional layer is again a three-dimensional box, which can be referred
    to as a volume.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the pooling layer reduces the size of the input by taking *m*m* local
    patches of pixels and outputting a scalar. The max-pooling layer takes *m*m* patches
    and outputs the greatest value among the patch of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Given an input volume of the (32, 32, 3) shape—corresponding to height, width,
    and depth (channels)—a max-pooling layer with a pooling size of 2x2 will output
    a volume of the (16, 16, 3) shape. The input to the CNN are usually images, which
    can also be viewed as volumes where the depth corresponds to RGB channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a depiction of a typical convolutional neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc56335f-cb3f-48b6-bfac-238e51e32725.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example convolutional neural network'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main advantage of a CNN over a standard neural network is that the former
    is able to learn visual and spatial features of the input, while for the latter
    such information is lost due to flattening input data into a vector. CNNs have
    made significant strides in the field of computer vision, starting with increased
    classification accuracies of `MNIST` data and object recognition, semantic segmentation,
    and other domains. CNNs have many applications in real life, from facial detection
    in social media to autonomous vehicles. Recent approaches have also applied CNNs
    to natural language processing and text classification tasks to produce state-of-the-art
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the basics of machine learning, we will go through
    our first implementation exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a convolutional neural network in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement a simple convolutional neural network in
    TensorFlow to solve an image classification task. As the rest of this book will
    be heavily reliant on TensorFlow and CNNs, we highly recommend that  you become
    sufficiently familiar with implementing deep learning algorithms using this framework.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow, developed by Google in 2015, is one of the most popular deep learning
    frameworks in the world. It is used widely for research and commercial projects
    and boasts a rich set of APIs and functionalities to help researchers and practitioners
    develop deep learning models. TensorFlow programs can run on GPUs as well as CPUs,
    and thus abstract the GPU programming to make development more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will be using TensorFlow exclusively, so make sure
    you are familiar with the basics as you progress through the chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://www.tensorflow.org/](https://www.tensorflow.org/) for a complete
    set of documentation and other tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: The Fashion-MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Those who have experience with deep learning have most likely heard about the
    `MNIST` dataset. It is one of the most widely-used image datasets, serving as
    a benchmark for tasks such as image classification and image generation, and is
    used by many computer vision models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8248ffb8-5363-4ec7-84fc-21d807ecd434.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The MNIST dataset (reference at end of chapter)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several problems with `MNIST`, however. First of all, the dataset
    is too easy, since a simple convolutional neural network is able to achieve 99%
    test accuracy. In spite of this, the dataset is used far too often in research
    and benchmarks. The `F-MNIST` dataset, produced by the online fashion retailer
    Zalando, is a more complex, much-needed upgrade to `MNIST`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1323649e-eba8-48da-8242-490ff261a28b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Fashion-MNIST dataset (taken from [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist),
    reference at the end of this chapter)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of digits, the `F-MNIST` dataset includes photos of ten different clothing
    types (ranging from t-shirts to shoes) compressed in to 28x28 monochrome thumbnails.
    Hence, `F-MNIST` serves as a convenient drop-in replacement to `MNIST` and is
    increasingly gaining popularity in the community. Hence we will train our CNN
    on `F-MNIST` as well. The preceding table maps each label index to its class:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Index** | **Class** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | T-shirt/top |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Trousers |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Pullover |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Dress |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Coat |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Sandal |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Shirt |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Sneaker |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bag |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Ankle boot |'
  prefs: []
  type: TYPE_TB
- en: In the following subsections, we will design a convolutional neural network
    that will learn to classify data from this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple deep learning frameworks have already implemented APIs for loading
    the `F-MNIST` dataset, including TensorFlow. For our implementation, we will be
    using Keras, another popular deep learning framework that is integrated with TensorFlow.
    The Keras datasets module provides a highly convenient interface for loading the
    datasets as `numpy` arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can start coding! For this exercise, we only need one Python module,
    which we will call `cnn.py`. Open up your favorite text editor or IDE, and let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to declare the modules that we are going to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following describes what each module is for and how we will use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Module(s)** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `logging` | For printing statistics as we run the code |'
  prefs: []
  type: TYPE_TB
- en: '| `os`, `sys` | For interacting with the operating system, including writing
    files |'
  prefs: []
  type: TYPE_TB
- en: '| `tensorflow` | The main TensorFlow library |'
  prefs: []
  type: TYPE_TB
- en: '| `numpy` | An optimized library for vector calculations and simple data processing
    |'
  prefs: []
  type: TYPE_TB
- en: '| `keras` | For downloading the F-MNIST dataset |'
  prefs: []
  type: TYPE_TB
- en: 'We will implement our CNN as a class called `SimpleCNN`. The `__init__` constructor
    takes a number of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters our `SimpleCNN` is initialized with are described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `learning_rate` | The learning rate for the optimization algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| `num_epochs` | The number of epochs it takes to train the network |'
  prefs: []
  type: TYPE_TB
- en: '| `beta` | A float value (between 0 and 1) that controls the strength of the
    L2-penalty |'
  prefs: []
  type: TYPE_TB
- en: '| `batch_size` | The number of images to train on in a single step |'
  prefs: []
  type: TYPE_TB
- en: Moreover, `save_dir` and `save_path` refer to the locations where we will store
    our network's parameters. `logs_dir` and `logs_path` refer to the locations where
    the statistics of the training run will be stored (we will show how we can retrieve
    these logs later).
  prefs: []
  type: TYPE_NORMAL
- en: Methods for building the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, in this section, we will see two methods that can be used to build the
    function, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: build method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fit method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first method we will define for our `SimpleCNN` class is the `build` method,
    which is responsible for building the architecture of our CNN. Our `build` method
    takes two pieces of input: the input tensor and the number of classes it should
    expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will first initialize `tf.placeholder`, called `is_training`. TensorFlow
    placeholders are like variables that don''t have values. We only pass them values
    when we actually train the network and call the relevant operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `tf.name_scope(...)` block allows us to name our operations and tensors
    properly. While this is not absolutely necessary, it helps us organize our code
    better and will help us to visualize the network. Here, we define a `tf.placeholder_with_default`
    called `is_training`, which has a default value of `True`. This placeholder will
    be used for our dropout operations (since dropout has different modes during training
    and inference).
  prefs: []
  type: TYPE_NORMAL
- en: Naming your operations and tensors is considered a good practice. It helps you
    organize your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step is to define the convolutional layers of our CNN. We make use
    of three different kinds of layers to create multiple layers of convolutions: `tf.layers.conv2d`, `tf.max_pooling2d`,
    and `tf.layers.dropout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some explanations of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `filters` | `int` | Number of filters output by the convolution. |'
  prefs: []
  type: TYPE_TB
- en: '| `kernel_size` | Tuple of `int` | The shape of the kernel. |'
  prefs: []
  type: TYPE_TB
- en: '| `pool_size` | Tuple of `int` | The shape of the max-pooling window. |'
  prefs: []
  type: TYPE_TB
- en: '| `strides` | `int` | The number of pixels to slide across per convolution/max-pooling
    operation. |'
  prefs: []
  type: TYPE_TB
- en: '| `padding` | `str` | Whether to add padding (SAME) or not (VALID). If padding
    is added, the output shape of the convolution remains the same as the input shape.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `activation` | `func` | A TensorFlow activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `kernel_regularizer` | `op` | Which regularization to use for the convolutional
    kernel. The default value is `None`. |'
  prefs: []
  type: TYPE_TB
- en: '| `training` | `op` | A tensor/placeholder that tells the dropout operation
    whether the forward pass is for training or for inference. |'
  prefs: []
  type: TYPE_TB
- en: 'In the preceding table, we have specified the convolutional architecture to
    have the following sequence of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: CONV | CONV | POOL | DROPOUT | CONV | CONV | POOL | DROPOUT
  prefs: []
  type: TYPE_NORMAL
- en: However, you are encouraged to explore different configurations and architectures.
    For example, you could add batch-normalization layers to improve the stability
    of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add the fully-connected layers that lead to the output of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.layers.flatten` turns the output of the convolutional layers (which is
    3-D) into a single vector (1-D) so that we can pass them through the `tf.layers.dense`
    layers. After going through two fully-connected layers, we return the final output,
    which we define as `logits`.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in the final `tf.layers.dense` layer, we do not specify an `activation`.
    We will see why when we move on to specifying the training operations of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we implement several helper functions. `_create_tf_dataset` takes two
    instances of `numpy.ndarray` and turns them into TensorFlow tensors, which can
    be directly fed into a network. `_log_loss_and_acc` simply logs training statistics,
    such as loss and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: fit method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last method we will implement for our `SimpleCNN` is the `fit` method.
    This function triggers training for our CNN. Our `fit` method takes four input:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Argument** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `X_train` | Training data |'
  prefs: []
  type: TYPE_TB
- en: '| `y_train` | Training labels |'
  prefs: []
  type: TYPE_TB
- en: '| `X_test` | Test data |'
  prefs: []
  type: TYPE_TB
- en: '| `y_test` | Test labels |'
  prefs: []
  type: TYPE_TB
- en: 'The first step of `fit` is to initialize `tf.Graph` and `tf.Session`. Both
    of these objects are essential to any TensorFlow program. `tf.Graph` represents
    the graph in which all the operations for our CNN are defined. You can think of
    it as a sandbox where we define all the layers and functions. `tf.Session` is
    the class that actually executes the operations defined in `tf.Graph`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create datasets using TensorFlow''s Dataset API and the `_create_tf_dataset`
    method we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.data.Iterator` builds an iterator object that outputs a batch of images
    every time we call `iterator.get_next()`. We initialize a dataset each for the
    training and testing data. The result of `iterator.get_next()` is a tuple of input
    images and corresponding labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The former is `input_tensor`, which we feed into the `build` method. The latter
    is used for calculating the loss function and backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`logits` (the non-activated outputs of the network) are fed into two other
    operations: `prediction`, which is just the softmax over `logits` to obtain normalized
    probabilities over the classes, and `loss_ops`, which calculates the mean categorical
    cross-entropy between the predictions and the labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the backpropagation algorithm used to train the network and
    the operations used for calculating accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now done building the network along with its optimization algorithms.
    We use `tf.global_variables_initializer()` to initialize the weights and operations
    of our network. We also initialize the `tf.train.Saver` and `tf.summary.FileWriter`
    objects. The `tf.train.Saver` object saves the weights and architecture of the
    network, whereas the latter keeps track of various training statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, once we have set up everything we need, we can implement the actual
    training loop. For every epoch, we keep track of the training cross-entropy loss
    and accuracy of the network. At the end of every epoch, we save the updated weights
    to disk. We also calculate the validation loss and accuracy every 10 epochs. This
    is done by calling `sess.run(...)`, where the arguments to this function are the
    operations that the `sess` object should execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: And that completes our `fit` function. Our final step is to create the script
    for instantiating the datasets, the neural network, and then running training,
    which we will write at the bottom of `cnn.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first configure our logger and load the dataset using the Keras `fashion_mnist`
    module, which loads the training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We then apply some simple preprocessing to the data. The Keras API returns `numpy`
    arrays of the `(Number of images, 28, 28)` shape.
  prefs: []
  type: TYPE_NORMAL
- en: However, what we actually want is `(Number of images, 28, 28, 1)`, where the
    third axis is the channel axis. This is required because our convolutional layers
    expect input that have three axes. Moreover, the pixel values themselves are in
    the range of `[0, 255]`. We will divide them by 255 to get a range of `[0, 1]`.
    This is a common technique that helps stabilize training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we turn the labels, which are simply an array of label indices,
    into one-hot encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the input to the constructor of our `SimpleCNN`. Feel free to
    tweak the numbers to see how they affect the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we instantiate `SimpleCNN` and call its `fit` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the entire script, all you need to do is run the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it! You have successfully implemented a convolutional neural network
    in TensorFlow to train on the `F-MNIST` dataset. To track the progress of the
    training, you can simply look at the output in your terminal/editor. You should
    see an output that resembles the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Another thing to check out is TensorBoard, a visualization tool developed by
    the developers of TensorFlow, to graph the model''s accuracy and loss. The `tf.summary.FileWriter`
    object we have used serves this purpose. You can run TensorBoard with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`logs` is where our `SimpleCNN` model writes the statistics to. TensorBoard
    is a great tool for visualizing the structure of our `tf.Graph`, as well as seeing
    how statistics such as accuracy and loss change over time. By default, the TensorBoard
    logs can be accessed by pointing your browser to `localhost:6006`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92d80158-fa80-4b15-ae61-602e9bd0af1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: TensorBoard and its visualization of our CNN'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We have successfully implemented a convolutional neural network
    using TensorFlow. However, the CNN we implemented is rather rudimentary, and only
    achieves mediocre accuracy—the challenge to the reader is to tweak the architecture
    to improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took our first step in the world of reinforcement learning.
    We covered some of the fundamental concepts and terminology of the field, including
    the agent, the policy, the value function, and the reward. We  also covered basic
    topics in deep learning and implemented a simple convolutional neural network
    using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The field of reinforcement learning is vast and ever-expanding; it would be
    impossible to cover all of it in a single book. We do, however, hope to equip
    you with the practical skills and the necessary experience to navigate this field.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapters will consist of individual projects—we will use a combination
    of reinforcement learning and deep learning algorithms to tackle several tasks
    and problems. We will build agents that will learn to play Go, explore the world
    of Minecraft, and play Atari video games. We hope you are ready to embark on this
    exciting learning journey!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT press, 1998.'
  prefs: []
  type: TYPE_NORMAL
- en: Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner*. Gradient-based learning applied
    to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998. *
  prefs: []
  type: TYPE_NORMAL
- en: 'Xiao, Han, Kashif Rasul, and Roland Vollgraf. *Fashion-mnist: a novel image
    dataset for benchmarking machine learning algorithms*. arXiv preprint arXiv:1708.07747 (2017).'
  prefs: []
  type: TYPE_NORMAL
