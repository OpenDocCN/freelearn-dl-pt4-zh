- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Advanced Exploration
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜çº§æ¢ç´¢
- en: In this chapter, we will talk about the topic of exploration in reinforcement
    learning (RL). It has been mentioned several times in the book that the exploration/exploitation
    dilemma is a fundamental thing in RL and very important for efficient learning.
    However, in the previous examples, we used quite a trivial approach to exploring
    the environment, which was, in most cases, ğœ–-greedy action selection. Now itâ€™s
    time to go deeper into the exploration subfield of RL, as more complicated environments
    might require much better exploration strategies than ğœ–-greedy approach.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„æ¢ç´¢ä¸»é¢˜ã€‚ä¹¦ä¸­å¤šæ¬¡æåˆ°ï¼Œæ¢ç´¢/åˆ©ç”¨å›°å¢ƒæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œå¯¹äºé«˜æ•ˆå­¦ä¹ éå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§ç›¸å½“ç®€å•çš„æ¢ç´¢ç¯å¢ƒçš„æ–¹æ³•ï¼Œå³å¤§å¤šæ•°æƒ…å†µä¸‹çš„
    ğœ–-greedy è¡ŒåŠ¨é€‰æ‹©ã€‚ç°åœ¨æ˜¯æ—¶å€™æ·±å…¥æ¢è®¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢å­é¢†åŸŸï¼Œå› ä¸ºæ›´å¤æ‚çš„ç¯å¢ƒå¯èƒ½éœ€è¦æ¯” ğœ–-greedy æ–¹æ³•æ›´å¥½çš„æ¢ç´¢ç­–ç•¥ã€‚
- en: 'More specifically, we will cover the following key topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹å…³é”®ä¸»é¢˜ï¼š
- en: Why exploration is such a fundamental topic in RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ¢ç´¢æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­å¦‚æ­¤åŸºæœ¬çš„è¯é¢˜
- en: The effectiveness of the epsilon-greedy (ğœ–-greedy) approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğœ–-greedy æ–¹æ³•çš„æœ‰æ•ˆæ€§
- en: Alternatives and how they work in different environments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›¿ä»£æ–¹æ³•åŠå…¶åœ¨ä¸åŒç¯å¢ƒä¸­çš„å·¥ä½œåŸç†
- en: We will implement the methods described to solve a toy, but still challenging,
    problem called MountainCar. This will allow us to better understand the methods,
    the way they could be implemented, and their behavior. After that, we will try
    to tackle a harder problem from the Atari suite.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®ç°æ‰€æè¿°çš„æ–¹æ³•ï¼Œè§£å†³ä¸€ä¸ªåä¸º MountainCar çš„ç©å…·é—®é¢˜ï¼Œå°½ç®¡å®ƒä¾ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¿™äº›æ–¹æ³•ã€å®ƒä»¬å¦‚ä½•å®ç°ä»¥åŠå®ƒä»¬çš„è¡Œä¸ºã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†å°è¯•è§£å†³ä¸€ä¸ªæ¥è‡ª
    Atari å¥—ä»¶çš„æ›´éš¾é—®é¢˜ã€‚
- en: Why exploration is important
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ¢ç´¢å¾ˆé‡è¦
- en: In this book, lots of environments and methods have been discussed, and in almost
    every chapter, exploration was mentioned. Very likely, youâ€™ve already got ideas
    about why itâ€™s important to explore the environment effectively, so Iâ€™m just going
    to discuss the main reasons.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦è®¨è®ºäº†è®¸å¤šç¯å¢ƒå’Œæ–¹æ³•ï¼Œå‡ ä¹æ¯ä¸€ç« éƒ½æåˆ°äº†æ¢ç´¢ã€‚å¾ˆå¯èƒ½ä½ å·²ç»å¯¹ä¸ºä»€ä¹ˆæœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒå¾ˆé‡è¦æœ‰äº†ä¸€äº›æƒ³æ³•ï¼Œæ‰€ä»¥æˆ‘å°†åªè®¨è®ºä¸»è¦çš„åŸå› ã€‚
- en: Before that, it might be useful to agree on the term â€œeffective exploration.â€
    In theoretical RL, a strict definition of this exists, but the high-level idea
    is simple and intuitive. Exploration is effective when we donâ€™t waste time in
    states of the environment that have already been seen by and are familiar to the
    agent. Rather than taking the same actions again and again, the agent needs to
    look for a new experience. As weâ€™ve already discussed, exploration has to be balanced
    by exploitation, which is the opposite and means using our knowledge to get the
    best reward in the most efficient way. Letâ€™s now quickly discuss why we might
    be interested in effective exploration in the first place.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ä¹‹å‰ï¼Œå®šä¹‰â€œæœ‰æ•ˆæ¢ç´¢â€å¯èƒ½ä¼šæœ‰å¸®åŠ©ã€‚åœ¨ç†è®ºå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå·²æœ‰ä¸¥æ ¼çš„å®šä¹‰ï¼Œä½†é«˜å±‚æ¬¡çš„æ¦‚å¿µæ—¢ç®€å•åˆç›´è§‚ã€‚å½“æˆ‘ä»¬ä¸å†æµªè´¹æ—¶é—´åœ¨å·²ç»è¢«æ™ºèƒ½ä½“è§è¿‡å¹¶ä¸”ç†Ÿæ‚‰çš„ç¯å¢ƒçŠ¶æ€ä¸­æ—¶ï¼Œæ¢ç´¢å°±æ˜¯æœ‰æ•ˆçš„ã€‚æ™ºèƒ½ä½“ä¸åº”ä¸€ééåšç›¸åŒçš„åŠ¨ä½œï¼Œè€Œæ˜¯éœ€è¦å¯»æ‰¾æ–°çš„ç»éªŒã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ï¼Œæ¢ç´¢å¿…é¡»ä¸åˆ©ç”¨ç›¸å¹³è¡¡ï¼Œåè€…æ˜¯ç›¸åçš„æ¦‚å¿µï¼ŒæŒ‡çš„æ˜¯åˆ©ç”¨æˆ‘ä»¬çš„çŸ¥è¯†ä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼è·å¾—æœ€å¥½çš„å¥–åŠ±ã€‚ç°åœ¨è®©æˆ‘ä»¬å¿«é€Ÿè®¨è®ºä¸€ä¸‹ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ€åˆä¼šå¯¹æœ‰æ•ˆæ¢ç´¢æ„Ÿå…´è¶£ã€‚
- en: First, good exploration of the environment might have a fundamental influence
    on our ability to learn a good policy. If the reward is sparse and the agent obtains
    a good reward on some rare conditions, it might experience a positive reward only
    once in many episodes, so the ability of the learning process to explore the environment
    effectively and fully might bring more samples with a good reward that the method
    could learn from.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè‰¯å¥½çš„ç¯å¢ƒæ¢ç´¢å¯èƒ½å¯¹æˆ‘ä»¬å­¦ä¹ è‰¯å¥½ç­–ç•¥çš„èƒ½åŠ›äº§ç”Ÿæ ¹æœ¬æ€§å½±å“ã€‚å¦‚æœå¥–åŠ±ç¨€ç–ï¼Œä¸”æ™ºèƒ½ä½“åªæœ‰åœ¨æŸäº›ç½•è§æ¡ä»¶ä¸‹æ‰èƒ½è·å¾—è‰¯å¥½çš„å¥–åŠ±ï¼Œé‚£ä¹ˆå®ƒå¯èƒ½åœ¨è®¸å¤šå›åˆä¸­åªä¼šç»å†ä¸€æ¬¡æ­£å¥–åŠ±ï¼Œå› æ­¤å­¦ä¹ è¿‡ç¨‹æœ‰æ•ˆä¸”å……åˆ†åœ°æ¢ç´¢ç¯å¢ƒçš„èƒ½åŠ›ï¼Œå¯èƒ½ä¼šå¸¦æ¥æ›´å¤šèƒ½å¤Ÿä»ä¸­å­¦ä¹ åˆ°çš„è‰¯å¥½å¥–åŠ±æ ·æœ¬ã€‚
- en: In some cases, which are very frequent in practical applications of RL, a lack
    of good exploration might mean that the agent will never experience a positive
    reward at all, which makes everything else useless. If you have no good samples
    to learn from, you can have the most efficient RL method, but the only thing it
    will learn is that there is no way to get a good reward. This is the case for
    lots of practically interesting problems around us. For instance, we will take
    a closer look at the MountainCar environment later in the chapter, which has trivial
    dynamics, but due to a sparsity of rewards, is quite tricky to solve.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œè¿™ç§æƒ…å†µåœ¨å¼ºåŒ–å­¦ä¹ çš„å®é™…åº”ç”¨ä¸­éå¸¸å¸¸è§ï¼Œç¼ºä¹è‰¯å¥½çš„æ¢ç´¢å¯èƒ½æ„å‘³ç€ä»£ç†æ ¹æœ¬æ— æ³•ä½“éªŒåˆ°æ­£å‘å¥–åŠ±ï¼Œè¿™æ ·å…¶ä»–ä¸€åˆ‡å°±å˜å¾—æ— ç”¨ã€‚å¦‚æœä½ æ²¡æœ‰å¥½çš„æ ·æœ¬æ¥å­¦ä¹ ï¼Œä½ å¯ä»¥æ‹¥æœ‰æœ€æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½†å®ƒå”¯ä¸€èƒ½å­¦åˆ°çš„å°±æ˜¯æ²¡æœ‰åŠæ³•è·å¾—å¥½çš„å¥–åŠ±ã€‚è¿™æ­£æ˜¯è®¸å¤šå®é™…ä¸­æœ‰è¶£çš„é—®é¢˜çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« ç¨åè¯¦ç»†äº†è§£MountainCarç¯å¢ƒï¼Œå®ƒçš„åŠ¨åŠ›å­¦éå¸¸ç®€å•ï¼Œä½†ç”±äºå¥–åŠ±ç¨€ç–ï¼Œè§£å†³èµ·æ¥ç›¸å½“æ£˜æ‰‹ã€‚
- en: On the other hand, even if the reward is not sparse, effective exploration increases
    the training speed due to better convergence and training stability. This happens
    because our sample from the environment becomes more diverse and requires less
    communication with the environment. As a result, our RL method has the chance
    to learn a better policy in a shorter time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œå³ä½¿å¥–åŠ±ä¸æ˜¯ç¨€ç–çš„ï¼Œæœ‰æ•ˆçš„æ¢ç´¢ä¹Ÿèƒ½æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºæ›´å¥½çš„æ”¶æ•›æ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ä»ç¯å¢ƒä¸­é‡‡æ ·å˜å¾—æ›´åŠ å¤šæ ·åŒ–ï¼Œä¸”ä¸ç¯å¢ƒçš„é€šä¿¡éœ€æ±‚å‡å°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ‰æœºä¼šåœ¨æ›´çŸ­çš„æ—¶é—´å†…å­¦ä¹ åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚
- en: Whatâ€™s wrong with ğœ–-greedy?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğœ–-greedyæœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
- en: Throughout the book, we have used the ğœ–-greedy exploration strategy as a simple,
    but still acceptable, approach to exploring the environment. The underlying idea
    behind ğœ–-greedy is to take a random action with the probability of ğœ–; otherwise,
    (with 1 âˆ’ğœ– probability) we act according to the policy (greedily). By varying
    the hyperparameter 0 â‰¤ğœ– â‰¤ 1, we can change the exploration ratio. This approach
    was used in most of the value-based methods described in the book. Quite a similar
    idea was used in policy-based methods, when our network returns the probability
    distribution over actions to take. To prevent the network from becoming too certain
    about actions (by returning a probability of 1 for a specific action and 0 for
    others), we added the entropy loss, which is just the entropy of the probability
    distribution multiplied by some hyperparameter. In the early stages of the training,
    this entropy loss pushes our network toward taking random actions (by regularizing
    the probability distribution). But in later stages, when we have explored the
    environment enough and our reward is relatively high, the policy gradient dominates
    over this entropy regularization. But this hyperparameter requires tuning to work
    properly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¨ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ğœ–-greedyæ¢ç´¢ç­–ç•¥ä½œä¸ºä¸€ç§ç®€å•ä½†ä»ç„¶å¯æ¥å—çš„ç¯å¢ƒæ¢ç´¢æ–¹æ³•ã€‚ğœ–-greedyèƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯ä»¥ğœ–çš„æ¦‚ç‡é‡‡å–éšæœºåŠ¨ä½œï¼›å¦åˆ™ï¼Œï¼ˆä»¥1
    âˆ’ğœ–çš„æ¦‚ç‡ï¼‰æˆ‘ä»¬æŒ‰ç…§ç­–ç•¥ï¼ˆè´ªå©ªåœ°ï¼‰æ‰§è¡ŒåŠ¨ä½œã€‚é€šè¿‡è°ƒæ•´è¶…å‚æ•°0 â‰¤ğœ– â‰¤ 1ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜æ¢ç´¢çš„æ¯”ä¾‹ã€‚è¿™ç§æ–¹æ³•åœ¨æœ¬ä¹¦ä¸­æè¿°çš„å¤§å¤šæ•°åŸºäºå€¼çš„æ–¹æ³•ä¸­éƒ½æœ‰ä½¿ç”¨ã€‚ç±»ä¼¼çš„æ€æƒ³ä¹Ÿè¢«åº”ç”¨äºåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œå½“æˆ‘ä»¬çš„ç½‘ç»œè¿”å›ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒæ—¶ã€‚ä¸ºäº†é˜²æ­¢ç½‘ç»œå¯¹åŠ¨ä½œå˜å¾—è¿‡äºç¡®å®šï¼ˆé€šè¿‡ä¸ºæŸä¸ªç‰¹å®šåŠ¨ä½œè¿”å›1çš„æ¦‚ç‡ï¼Œä¸ºå…¶ä»–åŠ¨ä½œè¿”å›0çš„æ¦‚ç‡ï¼‰ï¼Œæˆ‘ä»¬æ·»åŠ äº†ç†µæŸå¤±ï¼Œå®ƒå®é™…ä¸Šæ˜¯æ¦‚ç‡åˆ†å¸ƒçš„ç†µä¹˜ä»¥æŸä¸ªè¶…å‚æ•°ã€‚åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œè¿™ä¸ªç†µæŸå¤±æ¨åŠ¨æˆ‘ä»¬çš„ç½‘ç»œé‡‡å–éšæœºåŠ¨ä½œï¼ˆé€šè¿‡æ­£åˆ™åŒ–æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚ä½†åœ¨åæœŸï¼Œå½“æˆ‘ä»¬è¶³å¤Ÿæ¢ç´¢äº†ç¯å¢ƒä¸”å¥–åŠ±ç›¸å¯¹è¾ƒé«˜æ—¶ï¼Œç­–ç•¥æ¢¯åº¦å°±ä¸»å¯¼äº†è¿™ç§ç†µæ­£åˆ™åŒ–ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªè¶…å‚æ•°éœ€è¦è°ƒæ•´æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚
- en: 'At a high level, both approaches are doing the same thing: to explore the environment,
    we introduce randomness into our actions. However, recent research shows that
    this approach is very far from being ideal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œä¸¤ç§æ–¹æ³•åšçš„äº‹æƒ…æ˜¯ç›¸åŒçš„ï¼šä¸ºäº†æ¢ç´¢ç¯å¢ƒï¼Œæˆ‘ä»¬å°†éšæœºæ€§å¼•å…¥åˆ°æˆ‘ä»¬çš„åŠ¨ä½œä¸­ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•è·ç¦»ç†æƒ³çŠ¶æ€è¿˜æœ‰å¾ˆå¤§å·®è·ï¼š
- en: In the case of value iteration methods, random actions taken in some pieces
    of our trajectory introduce bias into our Q-value estimation. The Bellman equation
    assumes that the Q-value for the next state is obtained from the action with the
    largest Q. In other words, the rest of the trajectory is supposed to be from our
    optimal behavior. But with ğœ–-greedy, we might take not the optimal action, but
    just a random action, and this piece of the trajectory will be stored in the replay
    buffer for a long time, until our ğœ– is decayed and old samples are pushed from
    the buffer. Before that happens, we will learn wrong Q-values.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With random actions injected into our trajectory, our policy changes with every
    step. With the frequency defined by the value of ğœ– or the entropy loss coefficient,
    our trajectory constantly switches from a random policy to our current policy.
    This might lead to poor state space coverage in situations when multiple steps
    are needed to reach some isolated areas in the environmentâ€™s state space.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate the last issue, letâ€™s consider a simple example taken from the
    paper by Strehl and Littman called An analysis of model-based interval estimation
    for Markov decision processes, which was published in 2008 [SL08]. The example
    is called â€œRiver Swimâ€ and it models a river that the agent needs to cross. The
    environment contains six states and two actions: left and right. States 1 and
    6 are on the riverâ€™s opposite sides and states 2 to 5 are in the water.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'FigureÂ [18.1](#x1-330004r1) shows the transition diagram for the first two
    states, 1 and 2:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![123pppppp ====== 001000.6.4.0.6.355 ](img/B22150_18_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.1: Transitions for the first two states of the River Swim environment'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In the first state (the circle with the label â€œ1â€), the agent stands on the
    ground of the riverbank. The only action is right (shown in solid lines), which
    means entering the river and swimming against the current to state 2\. But the
    current is strong, and our right action from state 1 succeeds only with a probability
    of 60% (the solid line from state 1 to state 2). With a probability of 40%, the
    current keeps us in state 1 (the solid line connecting state 1 to itself).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second state (the circle with the label â€œ2â€), we have two actions: left,
    which is shown by the dotted line connecting states 2 and 1 (this action always
    succeeds, as the current flushes us back to the riverbank), and right (dashed
    lines), which means swimming against the current to state 3\. As before, swimming
    against the current is hard, so the probability of getting from state 2 to state
    3 is just 35% (the dashed line connecting states 2 and 3). With a probability
    of 60%, our left action ends up in the same state (the curved dashed line connecting
    state 2 to itself). But sometimes, despite our efforts, our left action ends up
    in state 1, which happens with a 5% probability (the curved dashed line connecting
    states 2 and 1).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'As Iâ€™ve said, there are six states in River Swim, but the transitions for states
    3, 4, and 5 are identical to those for state 2\. The last state, 6, is similar
    to state 1, so there is only one action available there: left, meaning to swim
    back. In FigureÂ [18.2](#x1-330006r2), you can see the full transition diagram
    (which is just clones of the diagram weâ€™ve already seen, where right action transitions
    are shown as solid lines, and left action transitions are dotted lines):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘æ‰€è¯´ï¼ŒRiver Swim æœ‰å…­ä¸ªçŠ¶æ€ï¼Œä½†çŠ¶æ€ 3ã€4 å’Œ 5 çš„è½¬æ¢ä¸çŠ¶æ€ 2 ç›¸åŒã€‚æœ€åä¸€ä¸ªçŠ¶æ€ 6 ä¸çŠ¶æ€ 1 ç›¸ä¼¼ï¼Œå› æ­¤åœ¨è¯¥çŠ¶æ€ä¸‹åªæœ‰ä¸€ä¸ªåŠ¨ä½œå¯ç”¨ï¼šå·¦ï¼Œå³æ¸¸å›å»ã€‚åœ¨å›¾
    [18.2](#x1-330006r2) ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°å®Œæ•´çš„è½¬æ¢å›¾ï¼ˆè¿™åªæ˜¯æˆ‘ä»¬ä¹‹å‰è§è¿‡çš„å›¾çš„å…‹éš†ï¼Œå³è½¬åŠ¨ä½œçš„è½¬æ¢ç”¨å®çº¿è¡¨ç¤ºï¼Œå·¦è½¬åŠ¨ä½œçš„è½¬æ¢ç”¨è™šçº¿è¡¨ç¤ºï¼‰ï¼š
- en: '![123456ppppppppppppppppppp = = = = = = = = = = = = = = = = = = = 0010001000100010001.6.4.0.6.3.0.6.3.0.6.3.0.6.355555555
    ](img/B22150_18_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![123456ppppppppppppppppppp = = = = = = = = = = = = = = = = = = = 0010001000100010001.6.4.0.6.3.0.6.3.0.6.3.0.6.355555555
    ](img/B22150_18_02.png)'
- en: 'FigureÂ 18.2: The full transition diagram for the River Swim environment'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 18.2ï¼šRiver Swim ç¯å¢ƒçš„å®Œæ•´è½¬æ¢å›¾
- en: In terms of the reward, the agent gets a small reward of 1 for the transition
    between states 1 to 5, but it gets a very high reward of 1,000 for getting into
    state 6, which acts as compensation for all the efforts of swimming against the
    current.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°±å¥–åŠ±è€Œè¨€ï¼Œä»£ç†åœ¨çŠ¶æ€ 1 åˆ°çŠ¶æ€ 5 ä¹‹é—´çš„è½¬æ¢è·å¾— 1 çš„å°å¥–åŠ±ï¼Œä½†è¿›å…¥çŠ¶æ€ 6 æ—¶ä¼šè·å¾— 1,000 çš„é«˜å¥–åŠ±ï¼Œä½œä¸ºå¯¹é€†æµæ¸¸æ³³æ‰€æœ‰åŠªåŠ›çš„è¡¥å¿ã€‚
- en: 'Despite the simplicity of the environment, its structure creates a problem
    for the ğœ–-greedy strategy being able to fully explore the state space. To check
    this, I implemented a very simple simulation of this environment, which you will
    find in Chapter18/riverswim.py. The simulated agent always acts randomly (ğœ– =
    1) and the result of the simulation is the frequency of various state visits.
    The number of steps the agent can take in one episode is limited to 10, but this
    can be changed using the command line. We wonâ€™t go over the entire code here;
    you can refer to it in the GitHub repository. Here, letâ€™s look at the results
    of the experiments:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç¯å¢ƒæœ¬èº«å¾ˆç®€å•ï¼Œä½†å…¶ç»“æ„ä¸º ğœ–-è´ªå©ªç­–ç•¥èƒ½å¤Ÿå®Œå…¨æ¢ç´¢çŠ¶æ€ç©ºé—´å¸¦æ¥äº†é—®é¢˜ã€‚ä¸ºäº†æ£€æŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘å®ç°äº†è¿™ä¸ªç¯å¢ƒçš„ä¸€ä¸ªéå¸¸ç®€å•çš„æ¨¡æ‹Ÿï¼Œä½ å¯ä»¥åœ¨ Chapter18/riverswim.py
    ä¸­æ‰¾åˆ°å®ƒã€‚æ¨¡æ‹Ÿä¸­çš„ä»£ç†æ€»æ˜¯éšæœºè¡ŒåŠ¨ï¼ˆğœ– = 1ï¼‰ï¼Œæ¨¡æ‹Ÿç»“æœæ˜¯å„ç§çŠ¶æ€è®¿é—®çš„é¢‘ç‡ã€‚ä»£ç†åœ¨ä¸€ä¸ªå›åˆä¸­å¯ä»¥é‡‡å–çš„æ­¥æ•°é™åˆ¶ä¸º 10ï¼Œä½†å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œæ›´æ”¹ã€‚æˆ‘ä»¬ä¸ä¼šåœ¨è¿™é‡Œè¯¦ç»†è®²è§£æ•´ä¸ªä»£ç ï¼›ä½ å¯ä»¥åœ¨
    GitHub ä»“åº“ä¸­æŸ¥çœ‹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å®éªŒç»“æœï¼š
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding output, each line shows the state number and the number of
    times it was visited during the simulation. With default command-line options,
    the simulation of 100 steps (10 episodes) was performed. As you can see, the agent
    never reached state 6 and was only in state 5 once. By increasing the number of
    episodes, the situation became a bit better, but not much:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„è¾“å‡ºä¸­ï¼Œæ¯ä¸€è¡Œæ˜¾ç¤ºäº†çŠ¶æ€ç¼–å·ä»¥åŠåœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­è®¿é—®è¯¥çŠ¶æ€çš„æ¬¡æ•°ã€‚ä½¿ç”¨é»˜è®¤çš„å‘½ä»¤è¡Œé€‰é¡¹ï¼Œè¿›è¡Œäº† 100 æ­¥ï¼ˆ10 ä¸ªå›åˆï¼‰çš„æ¨¡æ‹Ÿã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œä»£ç†ä»æœªåˆ°è¾¾çŠ¶æ€
    6ï¼Œå¹¶ä¸”ä»…åœ¨çŠ¶æ€ 5 ä¸­å‡ºç°è¿‡ä¸€æ¬¡ã€‚é€šè¿‡å¢åŠ å›åˆæ•°ï¼Œæƒ…å†µç¨æœ‰æ”¹å–„ï¼Œä½†å¹¶æ²¡æœ‰å¤ªå¤§å˜åŒ–ï¼š
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With 10 times more episodes simulated, we still didnâ€™t visit state 6, so the
    agent had no idea about the large reward there.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿäº† 10 å€å›åˆåï¼Œæˆ‘ä»¬ä»ç„¶æ²¡æœ‰è®¿é—®çŠ¶æ€ 6ï¼Œå› æ­¤ä»£ç†å®Œå…¨ä¸çŸ¥é“é‚£é‡Œæœ‰å¦‚æ­¤é«˜çš„å¥–åŠ±ã€‚
- en: 'Only with 10,000 episodes simulated were we able to get to state 6, but only
    five times, which is 0.05% of all the steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨æ¨¡æ‹Ÿäº† 10,000 ä¸ªå›åˆåï¼Œæˆ‘ä»¬æ‰æˆåŠŸåˆ°è¾¾çŠ¶æ€ 6ï¼Œä½†ä»…ä»… 5 æ¬¡ï¼Œå æ‰€æœ‰æ­¥éª¤çš„ 0.05%ï¼š
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Therefore, itâ€™s not very likely that the training will be efficient, even with
    the best RL method. Also, we had only six states in this example. Imagine how
    inefficient it will be with 20 or 50 states, which is not that unlikely; for example,
    in Atari games, there might be hundreds of decisions to be made before something
    interesting happens. If you want to, you can experiment with the riverswim.py
    tool, which allows you to change the random seed, the number of steps in the episode,
    the total number of steps, and even the number of states in the environment.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå³ä½¿é‡‡ç”¨æœ€å¥½çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒçš„æ•ˆç‡ä¹Ÿä¸å¤ªå¯èƒ½å¾ˆé«˜ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæœ‰å…­ä¸ªçŠ¶æ€ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæœ‰ 20 æˆ– 50 ä¸ªçŠ¶æ€ï¼Œæ•ˆç‡ä¼šä½åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Œè€Œè¿™å¹¶éä¸å¯èƒ½ï¼›ä¾‹å¦‚ï¼Œåœ¨
    Atari æ¸¸æˆä¸­ï¼Œå¯èƒ½éœ€è¦åšå‡ºæ•°ç™¾ä¸ªå†³ç­–æ‰èƒ½å‘ç”Ÿä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥ä½¿ç”¨ riverswim.py å·¥å…·è¿›è¡Œå®éªŒï¼Œå·¥å…·å…è®¸ä½ æ›´æ”¹éšæœºç§å­ã€å›åˆä¸­çš„æ­¥æ•°ã€æ€»æ­¥æ•°ï¼Œç”šè‡³ç¯å¢ƒä¸­çš„çŠ¶æ€æ•°ã€‚
- en: This simple example illustrates the issue with random actions in exploration.
    By acting randomly, our agent does not try to actively explore the environment;
    it just hopes that random actions will bring something new to its experience,
    which is not always the best thing to do.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s now discuss more efficient approaches to the exploration problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Alternative ways of exploration
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide you with an overview of a set of alternative
    approaches to the exploration problem. This wonâ€™t be an exhaustive list of approaches
    that exist, but rather will provide an outline of the landscape.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Weâ€™re going to explore the following three approaches to exploration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Randomness in the policy, when stochasticity is added to the policy that we
    use to get samples. The method in this family is noisy networks, which we have
    already covered in ChapterÂ [8](ch012.xhtml#x1-1240008).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Count-based methods, which keep track of the number of times the agent has
    seen the particular state. We will check two methods: the direct counting of states
    and the pseudo-count method.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction-based methods, which try to predict something from the state and
    from the quality of the prediction. We can make judgements about the familiarity
    of the agent with this state. To illustrate this approach, we will take a look
    at the policy distillation method, which has shown state-of-the-art results on
    hard-exploration Atari games like Montezumaâ€™s Revenge.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before implementing these methods, letâ€™s try and understand them in greater
    detail.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Noisy networks
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s start with an approach that is already familiar to us. We covered the
    method called noisy networks in ChapterÂ [8](ch012.xhtml#x1-1240008), when we referred
    to Hessel et al. [[Hes+18](#)] and discussed deep Q-network (DQN) extensions.
    The idea is to add Gaussian noise to the networkâ€™s weights and learn the noise
    parameters (mean and variance) using backpropagation, in the same way that we
    learn the modelâ€™s weights. In that chapter, this simple approach gave a significant
    boost in Pong training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, this might look very similar to the ğœ–-greedy approach, but
    Fortunato et al. [[For+17](#)] claimed a difference. The difference lies in the
    way we apply stochasticity to the network. In ğœ–-greedy, randomness is added to
    the actions. In noisy networks, randomness is injected into part of the network
    itself (several fully connected layers close to the output), which means adding
    stochasticity to our current policy. In addition, parameters of the noise might
    be learned during the training, so the training process might increase or decrease
    this policy randomness if needed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: According to the paper, the noise in noisy layers needs to be sampled from time
    to time, which means that our training samples are not produced by our current
    policy, but by the ensemble of policies. With this, our exploration becomes directed,
    as random values added to the weights produce a different policy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Count-based methods
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This family of methods is based on the intuition to visit states that have not
    been explored before. In simple cases, when the state space is not very large
    and different states are easily distinguishable from each other, we just count
    the number of times we have seen the state or state + action and prefer to get
    to the states for which this count is low.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç±»æ–¹æ³•åŸºäºä¸€ä¸ªç›´è§‰ï¼šè®¿é—®é‚£äº›ä¹‹å‰æ²¡æœ‰è¢«æ¢ç´¢è¿‡çš„çŠ¶æ€ã€‚åœ¨ç®€å•çš„æƒ…å†µä¸‹ï¼Œå½“çŠ¶æ€ç©ºé—´ä¸å¤ªå¤§å¹¶ä¸”ä¸åŒçš„çŠ¶æ€å¾ˆå®¹æ˜“åŒºåˆ†æ—¶ï¼Œæˆ‘ä»¬åªéœ€è®¡ç®—çœ‹åˆ°çŠ¶æ€æˆ–çŠ¶æ€+åŠ¨ä½œçš„æ¬¡æ•°ï¼Œå¹¶å€¾å‘äºå‰å¾€é‚£äº›è®¡æ•°è¾ƒä½çš„çŠ¶æ€ã€‚
- en: 'This could be implemented as an extra reward, not obtained from the environment
    but from the visit count of the state. In the literature, such a reward is called
    an intrinsic reward. In this context, the reward from the environment is called
    an extrinsic reward. One of the options to formulate such a reward is to use the
    bandits exploration approach: ![âˆš-1--- NËœ(s)](img/eq70.png). Here, Ã‘(s) is a count
    or pseudo-count of times we have seen the state, s, and value c defines the weight
    of the intrinsic reward.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ä½œä¸ºé¢å¤–çš„å¥–åŠ±æ¥å®ç°ï¼Œè¿™ç§å¥–åŠ±ä¸æ˜¯æ¥è‡ªç¯å¢ƒï¼Œè€Œæ˜¯æ¥è‡ªçŠ¶æ€çš„è®¿é—®æ¬¡æ•°ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œè¿™ç§å¥–åŠ±è¢«ç§°ä¸ºå†…åœ¨å¥–åŠ±ã€‚åœ¨è¿™ä¸ªè¯­å¢ƒä¸­ï¼Œç¯å¢ƒä¸­çš„å¥–åŠ±è¢«ç§°ä¸ºå¤–åœ¨å¥–åŠ±ã€‚åˆ¶å®šè¿™ç§å¥–åŠ±çš„ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨å¼ºç›—æ¢ç´¢æ–¹æ³•ï¼š![âˆš-1---
    NËœ(s)](img/eq70.png)ã€‚è¿™é‡Œï¼ŒÃ‘(s)æ˜¯æˆ‘ä»¬çœ‹åˆ°çŠ¶æ€sçš„æ¬¡æ•°æˆ–ä¼ªè®¡æ•°ï¼Œå€¼cå®šä¹‰äº†å†…åœ¨å¥–åŠ±çš„æƒé‡ã€‚
- en: If the number of states is small, like in the tabular learning case (we discussed
    it in ChapterÂ [5](ch009.xhtml#x1-820005)), we can just count them. In more difficult
    cases, when there are too many states, some transformation of the states needs
    to be introduced, like the hashing function or some embeddings of the states (weâ€™ll
    discuss this later in the chapter in more detail).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœçŠ¶æ€çš„æ•°é‡å¾ˆå°‘ï¼Œæ¯”å¦‚åœ¨è¡¨æ ¼å­¦ä¹ çš„æƒ…å†µä¸‹ï¼ˆæˆ‘ä»¬åœ¨ç¬¬[5](ch009.xhtml#x1-820005)ç« è®¨è®ºè¿‡ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¯¹å…¶è¿›è¡Œè®¡æ•°ã€‚åœ¨æ›´å›°éš¾çš„æƒ…å†µä¸‹ï¼Œå½“çŠ¶æ€å¤ªå¤šæ—¶ï¼Œéœ€è¦å¼•å…¥ä¸€äº›å¯¹çŠ¶æ€çš„è½¬æ¢ï¼Œä¾‹å¦‚å“ˆå¸Œå‡½æ•°æˆ–æŸäº›çŠ¶æ€çš„åµŒå…¥ï¼ˆæˆ‘ä»¬ç¨åä¼šåœ¨æœ¬ç« æ›´è¯¦ç»†åœ°è®¨è®ºï¼‰ã€‚
- en: For pseudo-count methods, Ã‘(s) is factorized into the density function and the
    total number of states visited, given by Ã‘(s) = Ï(x)n(x), where Ï(x) is a â€œdensity
    function,â€ representing the likelihood of the state x and approximated by a neural
    network. There are several different methods for how to do this, but they might
    be tricky to implement, so we wonâ€™t deal with complex cases in this chapter. If
    youâ€™re curious, you can refer to the paper by Georg Ostrovski et al. called Count-based
    exploration with neural density models [[Ost+17](#)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¼ªè®¡æ•°æ–¹æ³•ï¼ŒÃ‘(s)è¢«åˆ†è§£ä¸ºå¯†åº¦å‡½æ•°å’Œè®¿é—®çš„çŠ¶æ€æ€»æ•°ï¼Œç»™å®šÃ‘(s) = Ï(x)n(x)ï¼Œå…¶ä¸­Ï(x)æ˜¯â€œå¯†åº¦å‡½æ•°â€ï¼Œè¡¨ç¤ºçŠ¶æ€xçš„å¯èƒ½æ€§ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œè¿›è¡Œè¿‘ä¼¼ã€‚æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†å®ƒä»¬å¯èƒ½å¾ˆéš¾å®ç°ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨æœ¬ç« ä¸ä¼šè®¨è®ºå¤æ‚çš„æƒ…å†µã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥å‚è€ƒGeorg
    Ostrovskiç­‰äººå‘è¡¨çš„ã€ŠåŸºäºè®¡æ•°çš„æ¢ç´¢ä¸ç¥ç»å¯†åº¦æ¨¡å‹ã€‹[[Ost+17](#)]ã€‚
- en: A special case of introducing the intrinsic reward is called curiosity-driven
    exploration, when we donâ€™t take the reward from the environment into account at
    all. In that case, the training and exploration is driven 100% by the novelty
    of the agentâ€™s experience. Surprisingly, this approach might be very efficient
    not only in discovering new states in the environment but also in learning quite
    good policies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å…¥å†…åœ¨å¥–åŠ±çš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µå«åšå¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢ï¼Œå½“æˆ‘ä»¬å®Œå…¨ä¸è€ƒè™‘æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒå’Œæ¢ç´¢å®Œå…¨ç”±æ™ºèƒ½ä½“ç»éªŒçš„æ–°é¢–æ€§é©±åŠ¨ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½éå¸¸æœ‰æ•ˆï¼Œä¸ä»…èƒ½å‘ç°ç¯å¢ƒä¸­çš„æ–°çŠ¶æ€ï¼Œè¿˜èƒ½å­¦ä¹ å‡ºç›¸å½“ä¸é”™çš„ç­–ç•¥ã€‚
- en: Prediction-based methods
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºäºé¢„æµ‹çš„æ–¹æ³•
- en: The third family of exploration methods is based on another idea of predicting
    something from the environment data. If the agent can make accurate predictions,
    it means the agent has been in this situation enough and it isnâ€™t worth exploring
    it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç±»æ¢ç´¢æ–¹æ³•åŸºäºä»ç¯å¢ƒæ•°æ®ä¸­é¢„æµ‹æŸäº›ä¸œè¥¿çš„å¦ä¸€ä¸ªæƒ³æ³•ã€‚å¦‚æœæ™ºèƒ½ä½“èƒ½å¤Ÿåšå‡ºå‡†ç¡®çš„é¢„æµ‹ï¼Œæ„å‘³ç€æ™ºèƒ½ä½“å·²ç»åœ¨è¿™ç§æƒ…å†µä¸‹ç»å†äº†è¶³å¤Ÿå¤šçš„æ—¶é—´ï¼Œå› æ­¤ä¸å€¼å¾—å†å»æ¢ç´¢å®ƒã€‚
- en: But if something unusual happens and our prediction is significantly off, it
    might mean that we need to pay attention to the state that weâ€™re currently in.
    There are many different approaches to doing this, but in this chapter, we will
    discuss how to implement this approach, as proposed by Burda et al. in 2018 in
    the paper called Exploration by random network distillation [[Bur+18](#)]. The
    authors were able to reach state-of-the-art results in so-called hard-exploration
    games in Atari.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœå‘ç”Ÿäº†ä¸€äº›ä¸å¯»å¸¸çš„æƒ…å†µï¼Œä¸”æˆ‘ä»¬çš„é¢„æµ‹åå·®å¾ˆå¤§ï¼Œè¿™å¯èƒ½æ„å‘³ç€æˆ‘ä»¬éœ€è¦å…³æ³¨å½“å‰æ‰€å¤„çš„çŠ¶æ€ã€‚åšè¿™ä»¶äº‹æœ‰å¾ˆå¤šä¸åŒçš„æ–¹å¼ï¼Œä½†åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å®ç°è¿™ä¸€æ–¹æ³•ï¼Œæ­£å¦‚Burdaç­‰äººåœ¨2018å¹´æå‡ºçš„ã€Šé€šè¿‡éšæœºç½‘ç»œè’¸é¦è¿›è¡Œæ¢ç´¢ã€‹ä¸€æ–‡ä¸­æ‰€æå‡ºçš„é‚£æ ·[[Bur+18](#)]ã€‚ä½œè€…ä»¬åœ¨æ‰€è°“çš„ç¡¬æ¢ç´¢æ¸¸æˆä¸­ï¼ˆå¦‚Atariï¼‰è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
- en: 'The approach used in the paper is quite simple: we add the intrinsic reward,
    which is calculated from the ability of one neural network (NN) (which is being
    trained) to predict the output from another randomly initialized (untrained) NN.
    The input to both NNs is the current observation, and the intrinsic reward is
    proportional to the mean squared error (MSE) of the prediction.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: MountainCar experiments
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to implement and compare the effectiveness of different
    exploration approaches on a simple, but still challenging, environment, which
    could be classified as a â€œclassical RLâ€ problem that is very similar to the familiar
    CartPole problem. But in contrast to CartPole, the MountainCar problem is quite
    challenging from an exploration point of view.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The problemâ€™s illustration is shown in FigureÂ [18.3](#x1-335002r3) and it consists
    of a small car starting from the bottom of the valley. The car can move left and
    right, and the goal is to reach the top of the mountain on the right.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file265.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.3: The MountainCar environment'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The trick here is in the environmentâ€™s dynamics and the action space. To reach
    the top, the actions need to be applied in a particular way to swing the car back
    and forth to speed it up. In other words, the agent needs to apply the actions
    for several time steps to make the car go faster and eventually reach the top.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this coordination of actions is not something that is easy to achieve
    with just random actions, so the problem is hard from the exploration point of
    view and very similar to our River Swim example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'In Gym, this environment has the name MountainCar-v0 and it has a very simple
    observation and action space. The observations are just two numbers: the first
    one gives the horizontal position of the car and the second value is the carâ€™s
    velocity. The action could be 0, 1, or 2, where 0 means pushing the car to the
    left, 1 applies no force, and 2 pushes the car to the right. The following is
    a very simple illustration of this in Python REPL:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, in every step, we get the reward of -1, so the agent needs to
    learn how to get to the goal as soon as possible to get as little total negative
    reward as possible. By default, the number of steps is limited to 200, so if we
    havenâ€™t reached the goal (which happens most of the time), our total reward is
    âˆ’200.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: DQN + ğœ–-greedy
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first method that we will use is our traditional ğœ–-greedy approach to exploration.
    It is implemented in the source file Chapter18/mcar_dqn.py. I wonâ€™t include the
    source code here, as it is already familiar to you. This program implements various
    exploration strategies on top of the DQN method, allowing us to select between
    them using the -p command-line option. To launch the normal ğœ–-greedy method, the
    option -p egreedy needs to be passed. During the training, we are decreasing ğœ–
    from 1.0 to 0.02 for the first 10âµ training steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The training is quite fast; it takes just two to three minutes to do 10âµ training
    steps. But from the charts shown in FigureÂ [18.4](#x1-336002r4) and FigureÂ [18.5](#x1-336003r5),
    it is obvious that during those 10âµ steps, which was 500 episodes, we didnâ€™t reach
    the goal state even once. Thatâ€™s really bad news, as our ğœ– has decayed, so we
    will do no more exploration in the future.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.4: The reward (left) and steps (right) during the DQN training with
    the ğœ–-greedy strategy'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.5: Epsilon (left) and loss (right) during the training'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2% of random actions that we still perform are just not enough because
    it requires dozens of coordinated steps to reach the top of the mountain (the
    best policy on MountainCar has a total reward of around âˆ’80). We can now continue
    our training for millions of steps, but the only data we will get from the environment
    will be episodes, which will take 200 steps with âˆ’200 total reward. This illustrates
    once more how important exploration is. Regardless of the training method we have,
    without proper exploration, we might just fail to train. So, what should we do?
    If we want to stay with ğœ–-greedy, the only option for us is to explore for longer
    (by changing the speed of ğœ– decrease). You can experiment with the hyperparameters
    of the -p egreedy mode, but I went to the extreme and implemented the -p egreedy-long
    hyperparameter set. In this regime, we keep ğœ– = 1.0 until we reach at least one
    episode with a total reward better than âˆ’200\. Once this has happened, we start
    training the normal way, decreasing ğœ– from 1.0 to 0.02 for subsequent 10â¶ frames.
    As we donâ€™t do training, during the initial exploration phase, it normally runs
    5 to 10 times faster. To start the training in this mode, we use the following
    command line: ./mcar_dqn.py -n t1 -p egreedy-long.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, even with this improvement of ğœ–-greedy, it still failed to solve
    the environment due to its complexity. I left this version to run for five hours,
    but after 500k episodes, it still hadnâ€™t faced even a single example of the goal,
    so I gave up. Of course, you could try it for a longer period.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: DQN + noisy networks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To apply the noisy networks approach to our MountainCar problem, we just need
    to replace one of two layers in our network with the NoisyLinear class, so our
    architecture will become as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The only difference between the NoisyLinear class and the version from ChapterÂ [8](ch012.xhtml#x1-1240008)
    is that this version has an explicit method, sample_noise(), to update the noise
    tensors, so we need to call this method on every training iteration; otherwise,
    the noise will be constant during the training. This modification is needed for
    future experiments with policy-based methods, which require the noise to be constant
    during the relatively long period of trajectories. In any case, the modification
    is simple, and we just need to call this method from time to time. In the case
    of the DQN method, it is called on every training iteration. As in ChapterÂ [8](ch012.xhtml#x1-1240008),
    the implementation of NoisyLinear is taken from the TorchRL library. The code
    is the same as before, so to activate the noisy networks, you need to run the
    training with the -p noisynet command line.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In FigureÂ [18.6](#x1-337010r6), you can see the plots for the three hours of
    training:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.6: The training reward (left) and test steps (right) on DQN with
    noisy networks exploration'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the training process wasnâ€™t able to reach the mean test reward
    of âˆ’130 (as required in the code), but after just 7k training steps (20 minutes
    of training), we discovered the goal state, which is great progress in comparison
    to ğœ–-greedy, which didnâ€™t find a single instance of the goal state after 5 hours
    of trial and error.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: From the test steps chart (on the right of FigureÂ [18.6](#x1-337010r6)) we can
    see that there are some tests with less than 100 steps, which is very close to
    the optimal policy. But they were not often enough to push mean test reward below
    the âˆ’130 level.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: DQN + state counts
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last exploration technique that we will apply to the DQN method is count-based.
    As our state space is just two floating-point values, we will discretize the observation
    by rounding values to three digits after the decimal point, which should provide
    enough precision to distinguish different states from each other but still group
    similar states together. For every individual state, we will keep the count of
    times we have seen this state before and use that to give an extra reward to the
    agent. For an off-policy method, it might not be the best idea to modify rewards
    during the training, but we will examine the effect.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: As before, Iâ€™m not going to provide the full source code; I will just emphasize
    the differences from the base version. First, we apply the wrapper to the environment
    to keep track of the counters and calculate the intrinsic reward value. You will
    find the code for the wrapper is in the lib/common.py module and it is shown here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the constructor first:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the constructor, we take the environment we want to wrap, the optional hash
    function to be applied to the observations, and the scale of the intrinsic reward.
    We also create the container for our counters, which will map the hashed state
    into the number of times we have seen it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the helper function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function will calculate the intrinsic reward value of the state. It applies
    the hash to the observation, updates the counter, and calculates the reward using
    the formula we have already seen.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The last method of the wrapper is responsible for the environment step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here we call the helper function to get the reward and return the sum of the
    extrinsic and intrinsic reward components.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the wrapper, we need to pass to it the hashing function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Three digits are probably too many, so you can experiment with a different way
    of hashing states.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: To start the training, pass -p counts to the training program. In FigureÂ [18.7](#x1-338021r7),
    you can see the charts with training and testing rewards. As the training environment
    is wrapped in our PseudoCountReward wrapper, values during training are higher
    than testing.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.7: The training reward (left) and test rewards (right) on DQN with
    pseudo-count reward bonus'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we werenâ€™t able to get -130 average test reward using this method,
    but were very close to it. It took it just 10 minutes to discover the goal state,
    which is also quite impressive.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: PPO method
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another set of experiments that we will conduct with our MountainCar problem
    is related to the on-policy method Proximal Policy Optimization (PPO), which we
    covered in ChapterÂ [16](ch020.xhtml#x1-29000016). There are several motivations
    for this choice:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: First, as you saw in the DQN method + noisy networks case, when good examples
    are rare, DQNs have trouble adapting to them quickly. This might be solved by
    increasing the replay buffer size and switching to the prioritized buffer, or
    we could try on-policy methods, which adjust the policy immediately according
    to the obtained experience.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another reason for choosing this method is the modification of the reward during
    the training. Count-based exploration and policy distillation introduce the intrinsic
    reward component, which might change over time. The value-based methods might
    be sensitive to the modification of the underlying reward as, basically, they
    will need to relearn values during the training. On-policy methods shouldnâ€™t have
    any problems with that, as an increase of the reward just puts more emphasis on
    a sample with higher reward in terms of the policy gradient.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, itâ€™s just interesting to check our exploration strategies on both families
    of RL methods.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement this approach, in the file Chapter18/mcar_ppo.py, we have a PPO
    implementation combined with various exploration strategies applied to MountainCar.
    The code is not very different from the PPO from ChapterÂ [16](ch020.xhtml#x1-29000016),
    so Iâ€™m not going to repeat it here. To start the normal PPO without extra exploration
    tweaks, you should run the command ./mcar_ppo.py -n t1 -p ppo. In this version,
    nothing specifically is done to perform exploration â€“ we purely rely on random
    weights initialization in the beginning of the training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, PPO is in the policy gradient methods family, which limits Kullback-Leibler
    divergence between the old and new policy during the training, avoiding dramatic
    policy updates. Our network has two heads: the actor and the critic. The actor
    network returns the probability distribution over our actions (our policy) and
    the critic estimates the value of the state. The critic is trained using MSE loss,
    while the actor is driven by the PPO surrogate objective we discussed in ChapterÂ [16](ch020.xhtml#x1-29000016).
    In addition to those two losses, we regularize the policy by applying entropy
    loss scaled by the hyperparameter Î². There is nothing new here so far. The following
    is the PPO network structure:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I stopped the training after three hours, as it showed no improvements. The
    goal state was found after an hour and 30k episodes. The charts in FigureÂ [18.8](#x1-339015r8)
    show the reward dynamics during the training:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_08.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.8: The training reward (left) and test rewards (right) on plain PPO'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: As the PPO result wasnâ€™t very impressive, letâ€™s try to extend it with extra
    exploration tricks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: PPO + Noisy Networks
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with the DQN method, we can apply the noisy networks exploration approach
    to our PPO method. To do that, we need to replace the output layer of the actor
    with the NoisyLinear layer. Only the actor network needs to be affected because
    we would like to inject the noisiness only into the policy and not into the value
    estimation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one subtle nuance related to the application of noisy networks: where
    the random noise needs to be sampled. In ChapterÂ [8](ch012.xhtml#x1-1240008),
    when you first met noisy networks, the noise was sampled on every forward() pass
    of the NoisyLinear layer. According to the original research paper, this is fine
    for off-policy methods, but for on-policy methods, it needs to be done differently.
    Indeed, when we train on-policy, we obtain the training samples produced by our
    current policy and calculate the policy gradient, which should push the policy
    toward improvement. The goal of noisy networks is to inject randomness, but as
    we have discussed, we prefer directed exploration over just a random change of
    policy after every step. With that in mind, our random component in the NoisyLinear
    layer needs to be updated not after every forward() pass, but much less frequently.
    In my code, I resampled the noise on every PPO batch, which was 2,048 transitions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, I trained PPO+NoisyNets for 3 hours. But in this case, the goal
    state was found after 30 minutes and 18k episodes, which is a better result. In
    addition, according to the train steps count, the training process was able to
    drive the car in the optimal way a couple of times (with a step count less than
    100). But these successes did not lead to the optimal policy at the end. The charts
    in FigureÂ [18.9](#x1-340003r9) show the reward dynamics during the training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_09.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.9: The training reward (left) and test rewards (right) on PPO with
    Noisy Networks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: PPO + state counts
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, exactly the same count-based approach with three-digit hashing
    is implemented for the PPO method and can be triggered by passing -p counts to
    the training process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'In my experiments, the method was able to solve the environment (get an average
    reward higher than -130) in 1.5 hours, and it required 61k episodes. The following
    is the final part of the console output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see from the plots in FigureÂ [18.10](#x1-341011r10), the training
    discovered the goal state after 23k episodes. It took another 40k episodes to
    polish the policy to the optimal count of steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.10: The training reward (left) and test rewards (right) on PPO with
    pseudo-count reward bonus'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: PPO + network distillation
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the final exploration method in our MountainCar experiment, I implemented
    the network distillation method proposed by Burda et al. [[Bur+18](#)]. In this
    method, two extra NNs are introduced. Both need to map the observation into one
    number, in the same way that our value head does. The difference is in the way
    they are used. The first NN is randomly initialized and kept untrained. This will
    be our reference NN. The second one is trained to minimize the MSE loss between
    the second and the first NN. In addition, the absolute difference between the
    outputs of the NNs is used as the intrinsic reward component.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this is that the better the agent has explored some state, the
    better our second (trained) NN will predict the output of the first (untrained)
    one. This will lead to a smaller intrinsic reward being added to the total reward,
    which will decrease the policy gradient assigned to the sample.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors suggested training separate value heads to predict
    separate intrinsic and extrinsic reward components, but for this example, I decided
    to keep it simple and just added both rewards in the wrapper, the same way that
    we did in the counter-based exploration method. This minimizes the number of modifications
    in the code.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of those extra NN architectures, I did a small experiment and tried
    several architectures for both NNs. The best results were obtained with the reference
    NN having three layers and the trained NN having just one layer. This helps to
    prevent the overfitting of the trained NN, as our observation space is not very
    large. Both NNs are implemented in the MountainCarNetDistillery class in the lib/ppo.py
    module:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Besides the forward() method, which returns the output from both NNs, the class
    includes two helper methods for intrinsic reward calculation and for getting the
    loss between two NNs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the training, the argument -p distill needs to be passed to the mcar_ppo.py
    program. In my experiment, 33k episodes were required to solve the problem, which
    is almost two times less than Noisy Networks. As discussed in earlier chapters,
    there might be some bugs and inefficiencies in my implementation, so youâ€™re welcome
    to modify it to make it faster and more efficient:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The plots with the training and testing rewards are shown in FigureÂ [18.11](#x1-342040r11).
    In FigureÂ [18.12](#x1-342041r12), the total loss and distillation loss are shown.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_11.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.11: The training reward (left) and test rewards (right) on PPO with
    network distillation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_12.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.12: The total loss (left) and distillation loss (right)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: As before, due to the intrinsic reward component, the training episodes have
    a higher reward on the plots. From the distillation loss plot, it is clear that
    before the agent discovered the goal state, everything was boring and predictable,
    but once it had figured out how to end the episode earlier than 200 steps, the
    loss grew significantly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of methods
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To simplify the comparison of the experiments weâ€™ve done on MountainCar, I
    put all the numbers into the following table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Goal state found | Solved |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '|  | Episodes | Time | Episodes | Time |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| DQN + ğœ–-greedy | x | x | x | x |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| DQN + noisy nets | 8k | 15 min | x | x |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| PPO | 40k | 60 min | x | x |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| PPO + noisy nets | 20k | 30 min | x | x |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| PPO + counts | 25k | 36 min | 61k | 90 min |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| PPO + distillation | 16k | 36 min | 33k | 84 min |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: 'TableÂ 18.1: Summary of experiments'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, both DQN and PPO with exploration extensions are able to solve
    the MountainCar environment. Concrete method selection is up to you and your concrete
    situation, but it is important to be aware of the different approaches to the
    exploration you might use.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Atari experiments
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MountainCar environment is a nice and fast way to experiment with exploration
    methods, but to conclude the chapter, Iâ€™ve included Atari versions of the DQN
    and PPO methods with the exploration tweaks we described to check a more complicated
    environment.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: As the primary environment, Iâ€™ve used Seaquest, which is a game where the submarine
    needs to shoot fish and enemy submarines, and save aquanauts. This game is not
    as famous as Montezumaâ€™s Revenge, but it still might be considered as medium-hard
    exploration because, to continue the game, you need to control the level of oxygen.
    When it becomes low, the submarine needs to rise to the surface for some time.
    Without this, the episode will end after 560 steps and with a maximum reward of
    20\. But once the agent learns how to replenish the oxygen, the game might continue
    almost infinitely and bring to the agent a 10k-100k score. Surprisingly, traditional
    exploration methods struggle with discovering this; normally, training gets stuck
    at 560 steps, after which the oxygen runs out and the submarine dies.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The negative aspect of Atari is that every experiment requires at least half
    a day of training to check the effect, so my code and hyperparameters are very
    far from being the best, but they might be useful as a starting point for your
    own experiments. Of course, if you discover a way to improve the code, please
    share your findings on GitHub.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, there are two program files: atari_dqn.py, which implements the
    DQN method with ğœ–-greedy and noisy networks exploration, and atari_ppo.py, which
    is the PPO method with optional noisy networks and the network distillation method.
    To switch between hyperparameters, the command-line option -p needs to be used.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, let us look at the results that I got from a few
    runs of the code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: DQN + ğœ–-greedy
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In comparison to other methods tried on Atari, ğœ–-greedy was the best, which
    might be surprising, as it gave s the worst results in the MountainCar experiment
    earlier in this chapter. But this happens quite often in reality and can lead
    to new directions of research and even breakthroughs. After 13 hours of training,
    it was able to reach an average reward of 18 with a maximum reward of 25\. According
    to the chart showing the number of steps, just a few episodes were able to discover
    how to get the oxygen so, maybe with more training, this method can break the
    560-step boundary. In FigureÂ [18.13](#x1-345002r13), the plots with the average
    reward and number of steps are shown:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_13.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.13: The average training reward (left) and count of steps (right)
    on DQN with ğœ–-greedy'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: DQN + noisy networks
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Noisy networks combined with DQN showed worse results â€” after 6 hours of training,
    it was able to reach a reward of 6\. In FigureÂ [18.14](#x1-346002r14), the plots
    are shown:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_18_14.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'FigureÂ 18.14: The average training reward (left) and count of steps (right)
    on DQN with noisy networks'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: PPO
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PPO experiments were much worse â€” all the combinations (vanilla PPO, noisy networks,
    and network distillation) showed no reward progress and were able to reach an
    average reward of 4\. This is a bit surprising, as experiments with the same code
    in the previous edition of the book were able to get better results. This might
    be an indication of some subtle bugs in the code or in the training environment
    I used. Feel free to experiment with these methods by yourself!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed why ğœ–-greedy exploration is not the best in some
    cases and checked alternative modern approaches for exploration. The topic of
    exploration is much wider and lots of interesting methods are left uncovered,
    but I hope you were able to get an overall impression of the new methods and the
    way they should be implemented and used in your own problems.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, weâ€™ll take a look at another approach to the exploration
    in complex enviroments: RL with human feedback (RLHF).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
