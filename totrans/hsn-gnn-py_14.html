<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer666">
<h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor165"/><a id="_idTextAnchor166"/>14</h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor167"/>Explaining Graph Neural Networks</h1>
<p>One of the most common criticisms of NNs is that their outputs are difficult to understand. Unfortunately, GNNs are not immune to this limitation: in addition to explaining which features are important, it is necessary to consider neighboring nodes and connections. In response to this issue, the area of <strong class="bold">explainability</strong> (in the form of <strong class="bold">explainable AI</strong> or <strong class="bold">XAI</strong>) has developed many techniques to better understand the reasons behind a prediction or the <a id="_idIndexMarker803"/>general behavior of a model. Some of these techniques have been translated to GNNs, while others take advantage of the graph structure to offer more <span class="No-Break">precise explanations.</span></p>
<p>In this chapter, we will explore some explanation techniques to understand why a given prediction has been made. We will see different families of techniques and focus on two of the most popular: <strong class="bold">GNNExplainer</strong> and <strong class="bold">integrated gradients</strong>. We will apply the former on a graph classification task using the <strong class="source-inline">MUTAG</strong> dataset. Then, we will introduce <strong class="source-inline">Captum</strong>, a Python library <a id="_idIndexMarker804"/>that offers many explanation techniques. Finally, using the Twitch social network, we will implement integrated gradients to explain the model’s outputs on a node <span class="No-Break">classification task.</span></p>
<p>By the end of this chapter, you will be able to understand and implement several XAI techniques on GNNs. More specifically, you will learn how to use GNNExplainer and the <strong class="source-inline">Captum</strong> library (with integrated gradients) for graph and node <span class="No-Break">classification tasks.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Introducing <span class="No-Break">explanation techniques</span></li>
<li>Explaining GNNs <span class="No-Break">with GNNExplainer</span></li>
<li>Explaining GNNs <span class="No-Break">with Captum</span></li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor168"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter14"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter14</span></a><span class="No-Break">.</span></p>
<p>Installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> of <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor169"/>Introducing explanation techniques</h1>
<p>GNN explanation is a recent field that is heavily inspired by other XAI techniques [1]. We divide it into local explanations on a <a id="_idIndexMarker805"/>per-prediction basis and global explanations for entire models. While understanding the behavior of a GNN model is desirable, we <a id="_idIndexMarker806"/>will focus on local explanations that are more popular and essential to get insight into <span class="No-Break">a prediction.</span></p>
<p>In this chapter, we distinguish between “interpretable” and “explainable” models. A model is called “interpretable” if it is <a id="_idIndexMarker807"/>human-understandable by design, such as a decision tree. On the other hand, it is “explainable” when it acts as a black <a id="_idIndexMarker808"/>box whose predictions can only be retroactively understood using explanation techniques. This is typically the case with NNs: their weights and biases do not provide clear rules like a decision tree, but their results can be <span class="No-Break">explained indirectly.</span></p>
<p>There are four main categories of local <span class="No-Break">explanation techniques:</span></p>
<ul>
<li><strong class="bold">Gradient-based methods</strong> analyze <a id="_idIndexMarker809"/>gradients of the output to <a id="_idIndexMarker810"/>estimate attribution <a id="_idIndexMarker811"/>scores (for example, <span class="No-Break"><strong class="bold">integrated gradients</strong></span><span class="No-Break">)</span></li>
<li><strong class="bold">Perturbation-based methods</strong> mask <a id="_idIndexMarker812"/>or modify input features to measure changes <a id="_idIndexMarker813"/>in the output (for <span class="No-Break">example, </span><span class="No-Break"><strong class="bold">GNNExplainer</strong></span><span class="No-Break">)</span></li>
<li><strong class="bold">Decomposition methods</strong> decompose <a id="_idIndexMarker814"/>the model’s predictions into several terms to <a id="_idIndexMarker815"/>gauge their importance (for example, graph <a id="_idIndexMarker816"/>neural network <strong class="bold">layer-wise relevance </strong><span class="No-Break"><strong class="bold">propagation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GNN-LRP</strong></span><span class="No-Break">))</span></li>
<li><strong class="bold">Surrogate methods</strong> use a <a id="_idIndexMarker817"/>simple <a id="_idIndexMarker818"/>and interpretable model to approximate the original model’s <a id="_idIndexMarker819"/>prediction around an area (for <span class="No-Break">example, </span><span class="No-Break"><strong class="bold">GraphLIME</strong></span><span class="No-Break">)</span></li>
</ul>
<p>These techniques are complementary: they sometimes disagree on the contribution of edges and features, which can be used to refine the explanation of a prediction further. Explanation techniques are traditionally evaluated using metrics such as <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Fidelity</strong>, which compares <a id="_idIndexMarker820"/>the prediction probabilities of <img alt="" height="31" src="image/Formula_B19153_14_001.png" width="33"/> between the original graph <img alt="" height="41" src="image/Formula_B19153_14_002.png" width="35"/> and a modified graph <img alt="" height="42" src="image/Formula_B19153_14_003.png" width="36"/>. The modified graph only keeps the most important features (nodes, edges, node features) of <img alt="" height="42" src="image/Formula_B19153_14_003.png" width="36"/>, based on an explanation of <img alt="" height="41" src="image/Formula_B19153_14_005.png" width="39"/>. In other words, fidelity measures the extent to which the features identified as important are sufficient to obtain the correct prediction. It is formally defined <span class="No-Break">as follows:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer639">
<img alt="" height="155" src="image/Formula_B19153_14_006.jpg" width="759"/>
</div>
</div>
<ul>
<li><strong class="bold">Sparsity</strong>, which <a id="_idIndexMarker821"/>measures the fraction of features (nodes, edges, node features) that are considered important. Explanations that are too lengthy are more challenging to understand, which is why sparsity is encouraged. It is computed <span class="No-Break">as follows:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer640">
<img alt="" height="153" src="image/Formula_B19153_14_007.jpg" width="575"/>
</div>
</div>
<p>Here, <img alt="" height="43" src="image/Formula_B19153_14_008.png" width="67"/> is the number of important input features and <img alt="" height="42" src="image/Formula_B19153_14_009.png" width="64"/> is the total number <span class="No-Break">of features.</span></p>
<p>In addition to the traditional graphs we saw in previous chapters, explanation techniques are often evaluated <a id="_idIndexMarker822"/>on synthetic datasets, such as <strong class="source-inline">BA-Shapes</strong>, <strong class="source-inline">BA-Community</strong>, <strong class="source-inline">Tree-Cycles</strong>, and <strong class="source-inline">Tree-Grid</strong> [2]. These datasets were generated using graph generation algorithms to create specific patterns. We will not use them in this chapter, but they are an interesting alternative that is easy to implement <span class="No-Break">and understand.</span></p>
<p>In the following sections, we will describe a gradient-based method (integrated gradients) and a perturbation-based <span class="No-Break">technique (GNNExplainer).</span></p>
<h1 id="_idParaDest-165"><a id="_idTextAnchor170"/>Explaining GNNs with GNNExplainer</h1>
<p>In this section, we will introduce our first XAI technique with GNNExplainer. We will use it to understand the predictions produced by a GIN model on the <span class="No-Break"><strong class="source-inline">MUTAG</strong></span><span class="No-Break"> dataset.</span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor171"/>Introducing GNNExplainer</h2>
<p>Introduced in 2019 <a id="_idIndexMarker823"/>by Ying et al. [2], GNNExplainer is a GNN architecture designed to explain predictions from another GNN model. With tabular data, we want to know which features are the most important to a prediction. However, this is not enough with graph data: we also need to know which nodes are the most influential. GNNExplainer generates explanations with these two components by providing a subgraph <img alt="" height="44" src="image/Formula_B19153_14_010.png" width="43"/> and a subset of node features <img alt="" height="45" src="image/Formula_B19153_14_011.png" width="48"/>. The following figure illustrates an <a id="_idIndexMarker824"/>explanation provided by GNNExplainer for a <span class="No-Break">given node:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer645">
<img alt="Figure 14.1 – Explanation for node ￼’s label with ￼ in green and non-excluded node features ￼" height="680" src="image/B19153_14_001.jpg" width="771"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Explanation for node <img alt="" height="22" src="image/Formula_B19153_14_012.png" width="25"/>’s label with <img alt="" height="36" src="image/Formula_B19153_14_013.png" width="39"/> in green and non-excluded node features <img alt="" height="36" src="image/Formula_B19153_14_014.png" width="43"/></p>
<p>To predict <img alt="" height="45" src="image/Formula_B19153_14_015.png" width="43"/> and <img alt="" height="45" src="image/Formula_B19153_14_016.png" width="48"/>, GNNExplainer implements an edge mask (to hide connections) and a feature mask (to hide node features). If a connection or a feature is important, removing it should drastically change the prediction. On the other hand, if the prediction does not change, it means that this information was redundant or simply irrelevant. This principle is at the core of perturbation-based techniques such <span class="No-Break">as GNNExplainer.</span></p>
<p>In practice, we must <a id="_idIndexMarker825"/>carefully craft a loss function to find the best masks possible. GNNExplainer measures the mutual dependence between the predicted label distribution <img alt="" height="34" src="image/Formula_B19153_14_017.png" width="29"/> and <img alt="" height="46" src="image/Formula_B19153_14_018.png" width="136"/>, also called <strong class="bold">mutual information</strong> (<strong class="bold">MI</strong>). Our goal is to maximize the MI, which is <a id="_idIndexMarker826"/>equivalent to minimizing the conditional cross-entropy. GNNExplainer is trained to find the variables <img alt="" height="42" src="image/Formula_B19153_14_019.png" width="41"/> and <img alt="" height="39" src="image/Formula_B19153_14_020.png" width="41"/> that maximize the probability of a <span class="No-Break">prediction <img alt="" height="44" src="image/Formula_B19153_14_021.png" width="27"/>.</span></p>
<p>In addition to this optimization framework, GNNExplainer learns a binary feature mask and implements several regularization techniques. The most important technique is a term used to minimize the size of the explanation (sparsity). It is computed as the sum of all elements of the mask parameters and added to the loss function. It creates more user-friendly and concise explanations that are easier to understand <span class="No-Break">and interpret.</span></p>
<p>GNNExplainer can be applied to most GNN architectures and different tasks such as node classification, link prediction, or graph classification. It can also generate explanations of a class label or an entire graph. When classifying a graph, the model considers the union of <a id="_idIndexMarker827"/>adjacency matrices for all nodes in the graph instead of a single one. In the next section, we will apply it to explain <span class="No-Break">graph classifications.</span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor172"/>Implementing GNNExplainer</h2>
<p>In this example, we will explore the <strong class="source-inline">MUTAG</strong> dataset [3]. Each of the 188 graphs in this dataset represents <a id="_idIndexMarker828"/>a chemical compound, where nodes are atoms (seven possible atoms), and edges are chemical bonds (four possible bonds). Node and edge features represent one-hot encodings of the atom and edge types, respectively. The goal is to classify each compound into two classes according to their mutagenic effect on the bacteria <span class="No-Break"><em class="italic">Salmonella typhimurium</em></span><span class="No-Break">.</span></p>
<p>We will reuse the GIN model introduced in <a href="B19153_09.xhtml#_idTextAnchor106"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> for protein classification. In <a href="B19153_09.xhtml#_idTextAnchor106"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we visualized correct and incorrect classifications made by the model. However, we could not explain the predictions made by the GNN. This time, we’ll use GNNExplainer to understand the most important subgraph and node features to explain a classification. In this example, we will ignore the edge features for ease of use. Here are <span class="No-Break">the steps:</span></p>
<ol>
<li>We import the required classes from PyTorch and <span class="No-Break">PyTorch Geometric:</span><pre class="source-code">
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout
from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GINConv, global_add_pool, GNNExplainer</pre></li>
<li>We load the <strong class="source-inline">MUTAG</strong> dataset and <span class="No-Break">shuffle it:</span><pre class="source-code">
dataset = TUDataset(root='.', name='MUTAG').shuffle()</pre></li>
<li>We create training, validation, and <span class="No-Break">test sets:</span><pre class="source-code">
train_dataset = dataset[:int(len(dataset)*0.8)]
val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]
test_dataset  = dataset[int(len(dataset)*0.9):]</pre></li>
<li>We create data loaders to <span class="No-Break">implement mini-batching:</span><pre class="source-code">
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=True)</pre></li>
<li>We create <a id="_idIndexMarker829"/>a GIN model with 32 hidden dimensions using code from <a href="B19153_09.xhtml#_idTextAnchor106"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">:</span><pre class="source-code">
class GIN(torch.nn.Module):
...
model = GIN(dim_h=32)</pre></li>
<li>We train this model for 100 epochs and test it using code from <a href="B19153_09.xhtml#_idTextAnchor106"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">:</span><pre class="source-code">
def train(model, loader):
 ...
model = train(model, train_loader)
test_loss, test_acc = test(model, test_loader)
print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')
<strong class="bold">Test Loss: 0.48 | Test Acc: 84.21%</strong></pre></li>
<li>Our GIN model is trained and achieved a high accuracy score (<strong class="source-inline">84.21%</strong>). Now, let’s create a GNNExplainer model using the <strong class="source-inline">GNNExplainer</strong> class from PyTorch Geometric. We will train it for <span class="No-Break">100 epochs:</span><pre class="source-code">
explainer = GNNExplainer(model, epochs=100, num_hops=1)</pre></li>
<li>GNNExplainer can be used to explain the prediction made for a node (<strong class="source-inline">.explain_node()</strong>) or an entire graph (<strong class="source-inline">.explain_graph()</strong>). In this case, we will use it on the last graph of the <span class="No-Break">test set:</span><pre class="source-code">
data = dataset[-1]
feature_mask, edge_mask = explainer.explain_graph(data.x, data.edge_index)</pre></li>
<li>The last step returned the feature and edge masks. Let’s print the feature mask to see the most <span class="No-Break">important values:</span><pre class="source-code">
feature_mask
<strong class="bold">tensor([0.7401, 0.7375, 0.7203, 0.2692, 0.2587, 0.7516, 0.2872])</strong></pre></li>
</ol>
<p>The values are normalized between 0 (less important) and 1 (more important). These seven <a id="_idIndexMarker830"/>values correspond to the seven atoms we find in the dataset in the following order: carbon (C), nitrogen (N), oxygen (O), fluorine (F), iodine (I), chlorine (Cl), and bromine (Br). Features have similar importance: the most useful is the last one, representing bromine (Br), and the least important is the fifth one, representing <span class="No-Break">iodine (I).</span></p>
<ol>
<li value="10">Instead of printing the edge mask, we can plot it on the graph using the <strong class="source-inline">.visualize_graph()</strong> method. The arrows’ opacity represents the importance of <span class="No-Break">each connection:</span><pre class="source-code">
ax, G = explainer.visualize_subgraph(-1, data.edge_index, edge_mask, y=data.y)
plt.show()</pre></li>
</ol>
<p>This gives us <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer656">
<img alt="Figure 14.2 – Graph representation of a chemical compound: the edge opacity represents the importance of each connection" height="1093" src="image/B19153_14_002.jpg" width="1117"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – Graph representation of a chemical compound: the edge opacity represents the importance of each connection</p>
<p>The last plot shows the connections that contributed the most to the prediction. In this case, the GIN model correctly classified the graph. We can see that the links between nodes <strong class="bold">6</strong>, <strong class="bold">7</strong>, and <strong class="bold">8</strong> are the <a id="_idIndexMarker831"/>most relevant here. The highlighted connections were crucial in the classification of this chemical compound. We can learn more about them by printing <strong class="source-inline">data.edge_attr</strong> to obtain the label associated with their chemical bonds (aromatic, single, double, or triple). In this example, it corresponds to edges <strong class="bold">16</strong> to <strong class="bold">19</strong>, which all are either single or <span class="No-Break">double bonds.</span></p>
<p>By printing <strong class="source-inline">data.x</strong>, we can also look at nodes <strong class="bold">6</strong>, <strong class="bold">7</strong>, and <strong class="bold">8</strong> to gain more information. Node <strong class="bold">6</strong> represents an atom of nitrogen, while nodes <strong class="bold">7</strong> and <strong class="bold">8</strong> are two atoms of oxygen. These results should be reported to people with the right domain knowledge to get feedback on <span class="No-Break">our model.</span></p>
<p>GNNExplainer does not provide precise rules about the decision-making process but gives insights into what the GNN model focused on to make its prediction. Human expertise is still needed to <a id="_idIndexMarker832"/>ensure that these ideas are coherent and correspond to traditional <span class="No-Break">domain knowledge.</span></p>
<p>In the next section, we will use Captum to explain node classifications on a new <span class="No-Break">social network.</span></p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor173"/>Explaining GNNs with Captum</h1>
<p>In this section, we will first introduce Captum and the integrated gradients technique applied to graph data. Then, we will implement it using a PyTorch Geometric model on the Twitch <span class="No-Break">social network.</span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor174"/>Introducing Captum and integrated gradients</h2>
<p><strong class="source-inline">Captum</strong> (<a href="http://captum.ai">captum.ai</a>) is a Python <a id="_idIndexMarker833"/>library that implements many state-of-the-art <a id="_idIndexMarker834"/>explanation algorithms for PyTorch models. This library is not dedicated to GNNs: it can also be applied to text, images, tabular data, and so on. It is particularly useful because it allows users to quickly test various techniques and compare different explanations for the same prediction. In addition, <strong class="source-inline">Captum</strong> implements popular algorithms such as LIME and Gradient SHAP for primary, layer, and <span class="No-Break">neuron attributions.</span></p>
<p>In this section, we will use it to apply a graph version of integrated gradients [4]. This technique aims to <a id="_idIndexMarker835"/>assign an attribution score to every input feature. To this end, it uses gradients with respect to the model’s inputs. Specifically, it uses an input <img alt="" height="23" src="image/Formula_B19153_14_022.png" width="25"/> and a baseline input <img alt="" height="36" src="image/Formula_B19153_14_023.png" width="35"/> (all edges have zero weight in our case). It computes the gradients at all points along the path between <img alt="" height="24" src="image/Formula_B19153_14_024.png" width="25"/> and <img alt="" height="39" src="image/Formula_B19153_14_025.png" width="35"/> and <span class="No-Break">accumulates them.</span></p>
<p>Formally, the integrated gradient along the <img alt="" height="46" src="image/Formula_B19153_14_026.png" width="52"/> dimension for an input <img alt="" height="23" src="image/Formula_B19153_14_027.png" width="25"/> is defined <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer663">
<img alt="" height="121" src="image/Formula_B19153_14_028.jpg" width="1224"/>
</div>
</div>
<p>In practice, instead of directly <a id="_idIndexMarker836"/>calculating this integral, we approximate it with a <a id="_idIndexMarker837"/><span class="No-Break">discrete sum.</span></p>
<p>Integrated gradients is model-agnostic and based on <span class="No-Break">two axioms:</span></p>
<ul>
<li><strong class="bold">Sensitivity</strong>: Every input <a id="_idIndexMarker838"/>contributing to the prediction must receive a <span class="No-Break">nonzero attribution</span></li>
<li><strong class="bold">Implementation invariance</strong>: Two NNs <a id="_idIndexMarker839"/>whose outputs are equal for all inputs (these networks are called functionally equivalent) must have <span class="No-Break">identical attributions</span></li>
</ul>
<p>The graph version we will employ is slightly different: it considers nodes and edges <em class="italic">instead of</em> features. As a result, you can see that the output differs from GNNExplainer, which considers node features <em class="italic">and</em> edges. This is why these two approaches can <span class="No-Break">be complementary.</span></p>
<p>Let’s now implement this technique and visualize <span class="No-Break">the results.</span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor175"/>Implementing integrated gradients</h2>
<p>We will implement integrated <a id="_idIndexMarker840"/>gradients on a new dataset: the Twitch social networks dataset (English version) [5]. It represents a user-user graph, where nodes correspond to Twitch streamers and connections to mutual friendships. The 128 node features represent information such as streaming habits, location, games liked, and so on. The goal is to determine whether a streamer uses explicit language (<span class="No-Break">binary classification).</span></p>
<p>We will implement a simple two-layer GCN with PyTorch Geometric for this task. We will then convert <a id="_idIndexMarker841"/>our model to Captum to use the integrated gradients algorithm and explain our results. Here are <span class="No-Break">the steps:</span></p>
<ol>
<li>We install the <span class="No-Break"><strong class="source-inline">captum</strong></span><span class="No-Break"> library:</span><pre class="source-code">
!pip install captum</pre></li>
<li>We import the <span class="No-Break">required libraries:</span><pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
from captum.attr import IntegratedGradients
import torch_geometric.transforms as T
from torch_geometric.datasets import Twitch
from torch_geometric.nn import Explainer, GCNConv, to_captum</pre></li>
<li>Let’s fix the random seeds to make <span class="No-Break">computations deterministic:</span><pre class="source-code">
torch.manual_seed(0)
np.random.seed(0)</pre></li>
<li>We load the Twitch gamer network dataset (<span class="No-Break">English version):</span><pre class="source-code">
dataset = Twitch('.', name="EN")
data = dataset[0]</pre></li>
<li>This time, we will use a simple two-layer GCN <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">dropout</strong></span><span class="No-Break">:</span><pre class="source-code">
class GCN(torch.nn.Module):
    def __init__(self, dim_h):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_features, dim_h)
        self.conv2 = GCNConv(dim_h, dataset.num_classes)
    def forward(self, x, edge_index):
        h = self.conv1(x, edge_index).relu()
        h = F.dropout(h, p=0.5, training=self.training)
        h = self.conv2(h, edge_index)
        return F.log_softmax(h, dim=1)</pre></li>
<li>We try to train the model on a GPU—if one is available—using the <span class="No-Break"><strong class="source-inline">Adam</strong></span><span class="No-Break"> optimizer:</span><pre class="source-code">
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(64).to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)</pre></li>
<li>We train the <a id="_idIndexMarker842"/>model for 200 epochs using the negative log-likelihood <span class="No-Break">loss function:</span><pre class="source-code">
for epoch in range(200):
    model.train()
    optimizer.zero_grad()
    log_logits = model(data.x, data.edge_index)
    loss = F.nll_loss(log_logits, data.y)
    loss.backward()
    optimizer.step()</pre></li>
<li>We test the trained model. Note that we did not specify any test, so we will evaluate the GCN’s accuracy on the training set in <span class="No-Break">this case:</span><pre class="source-code">
def accuracy(pred_y, y):
    return ((pred_y == y).sum() / len(y)).item()
@torch.no_grad()
def test(model, data):
    model.eval()
    out = model(data.x, data.edge_index)
    acc = accuracy(out.argmax(dim=1), data.y)
    return acc
acc = test(model, data)
print(f'Accuracy: {acc*100:.2f}%')
<strong class="bold">Accuracy: 79.75%</strong></pre></li>
</ol>
<p>The model achieved an accuracy score of <strong class="source-inline">79.75%</strong>, which is relatively low considering that it was evaluated on the <span class="No-Break">training set.</span></p>
<ol>
<li value="9">We can now start implementing the explanation method we chose: the integrated gradients. First, we <a id="_idIndexMarker843"/>must specify the node we want to explain (node <strong class="source-inline">0</strong> in this example) and convert the PyTorch Geometric model to <strong class="source-inline">Captum</strong>. Here, we also specify we want to use a feature and an edge mask <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">mask_type=node_and_feature</strong></span><span class="No-Break">:</span><pre class="source-code">
node_idx = 0
captum_model = to_captum(model, mask_type='node_and_edge', output_idx=node_idx)</pre></li>
<li>Let’s create the integrated gradients object using <strong class="source-inline">Captum</strong>. We give it the result of the previous step <span class="No-Break">as input:</span><pre class="source-code">
ig = IntegratedGradients(captum_model)</pre></li>
<li>We already have the node mask we need to pass to <strong class="source-inline">Captum</strong> (<strong class="source-inline">data.x</strong>), but we need to create a tensor for the edge mask. In this example, we want to consider every edge in the graph, so initialize a tensor of ones with <span class="No-Break">size </span><span class="No-Break"><strong class="source-inline">data.num_edges</strong></span><span class="No-Break">:</span><pre class="source-code">
edge_mask = torch.ones(data.num_edges, requires_grad=True, device=device)</pre></li>
<li>The <strong class="source-inline">.attribute()</strong> method <a id="_idIndexMarker844"/>takes a specific format of inputs for the node and edge masks (hence the use of <strong class="source-inline">.unsqueeze(0)</strong> to reformat these tensors). The target corresponds to the class of our target node. Finally, we pass the adjacency matrix (<strong class="source-inline">data.edge_index</strong>) as an additional <span class="No-Break">forward argument:</span><pre class="source-code">
attr_node, attr_edge = ig.attribute(
    (data.x.unsqueeze(0), edge_mask.unsqueeze(0)),
    target=int(data.y[node_idx]),
    additional_forward_args=(data.edge_index),
    internal_batch_size=1)</pre></li>
<li>We scale the attribution scores between <strong class="source-inline">0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span><pre class="source-code">
attr_node = attr_node.squeeze(0).abs().sum(dim=1)
attr_node /= attr_node.max()
attr_edge = attr_edge.squeeze(0).abs()
attr_edge /= attr_edge.max()</pre></li>
<li>Using PyTorch Geometric’s <strong class="source-inline">Explainer</strong> class, we visualize a graph representation of <span class="No-Break">these attributions:</span><pre class="source-code">
explainer = Explainer(model)
ax, G = explainer.visualize_subgraph(node_idx, data.edge_index, attr_edge, node_alpha=attr_node, y=data.y)
plt.show()</pre></li>
</ol>
<p>This gives us the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer664">
<img alt="Figure 14.3 – Explanation for node 0’s classification with edge and node attribution scores represented with different opacity values" height="1047" src="image/B19153_14_003.jpg" width="1075"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Explanation for node 0’s classification with edge and node attribution scores represented with different opacity values</p>
<p>Node <strong class="bold">0</strong>’s subgraph <a id="_idIndexMarker845"/>comprises blue nodes, which share the same class. We can see that node <strong class="bold">82</strong> is the most important node (other than 0) and the connection between these two nodes is the most critical edge. This is a straightforward explanation: we have a group of four streamers using the same language. The mutual friendship between nodes <strong class="bold">0</strong> and <strong class="bold">82</strong> is a good argument for <span class="No-Break">this prediction.</span></p>
<p>Let’s now look at another graph illustrated in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.4</em>, the explanation for node <span class="No-Break"><strong class="bold">101</strong></span><span class="No-Break">’s classification:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer665">
<img alt="Figure 14.4 – Explanation for node 101’s classification with edge and node attribution scores represented with different opacity values" height="923" src="image/B19153_14_004.jpg" width="967"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – Explanation for node 101’s classification with edge and node attribution scores represented with different opacity values</p>
<p>In this case, our target node is connected to neighbors with different classes (nodes <strong class="bold">5398</strong> and <strong class="bold">2849</strong>). Integrated gradients give greater importance to the node that shares the same class as node <strong class="bold">101</strong>. We also see that their connection is the one that contributed the <a id="_idIndexMarker846"/>most to this classification. This subgraph is richer; you can see that even two-hop neighbors contribute <span class="No-Break">a little.</span></p>
<p>However, these explanations should not be considered a silver bullet. Explainability in AI is a rich topic that often involves people with different backgrounds. Thus, communicating the results and getting regular feedback is particularly important. Knowing the importance of edges, nodes, and features is essential, but it should only be the start of a discussion. Experts from other fields can exploit or refine these explanations, and even find issues that can lead to <span class="No-Break">architectural changes.</span></p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor176"/>Summary</h1>
<p>In this chapter, we explored the field of XAI applied to GNNs. Explainability is a key component in many areas and can help us to build better models. We saw different techniques to provide local explanations and focused on GNNExplainer (a perturbation-based method) and integrated gradients (a gradient-based method). We implemented them on two different datasets using PyTorch Geometric and Captum to obtain explanations for graph and node classification. Finally, we visualized and discussed the results of <span class="No-Break">these techniques.</span></p>
<p>In <a href="B19153_15.xhtml#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 15</em></span></a>, <em class="italic">Forecasting Traffic Using A3T-GCN</em>, we will revisit temporal GNNs to predict future traffic on a road network. In this practical application, we will see how to translate roads into graphs and apply a recent GNN architecture to forecast short-term <span class="No-Break">traffic accurately.</span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor177"/>Further reading</h1>
<ul>
<li>[1] H. Yuan, H. Yu, S. Gui, and S. Ji. <em class="italic">Explainability in Graph Neural Networks: A Taxonomic Survey</em>. arXiv, 2020. DOI: 10.48550/ARXIV.2012.15445. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/2012.15445"><span class="No-Break">https://arxiv.org/abs/2012.15445</span></a><span class="No-Break">.</span></li>
<li>[2] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec. <em class="italic">GNNExplainer: Generating Explanations for Graph Neural Networks</em>. arXiv, 2019. DOI: 10.48550/ARXIV.1903.03894. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1903.03894"><span class="No-Break">https://arxiv.org/abs/1903.03894</span></a><span class="No-Break">.</span></li>
<li>[3] Debnath, A. K., Lopez de Compadre, R. L., Debnath, G., Shusterman, A. J., and Hansch, C. (1991). <em class="italic">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity</em>. DOI: 10.1021/jm00106a046. <em class="italic">Journal of Medicinal Chemistry</em>, 34(2), 786–797. Available <span class="No-Break">at </span><a href="https://doi.org/10.1021/jm00106a046"><span class="No-Break">https://doi.org/10.1021/jm00106a046</span></a><span class="No-Break">.</span></li>
<li>[4] M. Sundararajan, A. Taly, and Q. Yan. <em class="italic">Axiomatic Attribution for Deep Networks</em>. arXiv, 2017. DOI: 10.48550/ARXIV.1703.01365. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1703.01365"><span class="No-Break">https://arxiv.org/abs/1703.01365</span></a><span class="No-Break">.</span></li>
<li>[5] B. Rozemberczki, C. Allen, and R. Sarkar. <em class="italic">Multi-Scale Attributed Node Embedding</em>. <em class="italic">arXiv</em>, <em class="italic">2019</em>. DOI: 10.48550/ARXIV.1909.13021. Available <span class="No-Break">at </span><a href="https://arxiv.org/pdf/1909.13021.pdf"><span class="No-Break">https://arxiv.org/pdf/1909.13021.pdf</span></a><span class="No-Break">.</span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer667">
</div>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer668">
<h1 id="_idParaDest-173"><a id="_idTextAnchor178"/>Part 4: Applications</h1>
<p>In this fourth and final part of the book, we delve into the development of comprehensive applications that utilize real-world data. Our focus will be on encompassing aspects previously omitted in previous chapters, such as exploratory data analysis and data processing. We aim to provide an exhaustive overview of the machine learning pipeline, from raw data to model output analysis. We will also highlight the strengths and limitations of the <span class="No-Break">techniques discussed.</span></p>
<p>The projects in this section have been designed to be adaptable and customizable, enabling readers to apply them to other datasets and tasks with ease. This makes it an ideal resource for readers who wish to build a portfolio of applications and showcase their work <span class="No-Break">on GitHub.</span></p>
<p>By the end of this part, you will know how to implement GNNs for traffic forecasting, anomaly detection, and recommender systems. These projects have been selected to demonstrate the versatility and potential of GNNs in solving real-world problems. The knowledge and skills gained from these projects will prepare readers for developing their own applications and contributing to the field of <span class="No-Break">graph learning.</span></p>
<p>This part comprises the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B19153_15.xhtml#_idTextAnchor179"><em class="italic">Chapter 15</em></a><em class="italic">, Forecasting Traffic Using A3T-GCN</em></li>
<li><a href="B19153_16.xhtml#_idTextAnchor187"><em class="italic">Chapter 16</em></a><em class="italic">, Detecting Anomalies Using Heterogeneous Graph Neural Networks</em></li>
<li><a href="B19153_17.xhtml#_idTextAnchor195"><em class="italic">Chapter 17</em></a><em class="italic">, Recommending Books Using LightGCN</em></li>
<li><a href="B19153_18.xhtml#_idTextAnchor203"><em class="italic">Chapter 18</em></a><em class="italic">, Unlocking the Potential of Graph Neural Networks for Real-Word Applications</em></li>
</ul>
</div>
<div>
<div id="_idContainer669">
</div>
</div>
</div></body></html>