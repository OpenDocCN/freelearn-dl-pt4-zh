["```py\nimport logging\n\nimport tensorflow as tf\n\nlogger = logging.getLogger(__name__)\n\nclass ChildCNN(object):\n\n    def __init__(self, cnn_dna, child_id, beta=1e-4, drop_rate=0.2, **kwargs):\n        self.cnn_dna = self.process_raw_controller_output(cnn_dna)\n        self.child_id = child_id\n        self.beta = beta\n        self.drop_rate = drop_rate\n        self.is_training = tf.placeholder_with_default(True, shape=None, name=\"is_training_{}\".format(self.child_id))\n        self.num_classes = 10\n```", "```py\ndef process_raw_controller_output(self, output):\n    \"\"\"\n    A helper function for preprocessing the output of the NASCell\n    Args:\n        output (numpy.ndarray) The output of the NASCell\n\n    Returns:\n        (list) The child network's architecture\n    \"\"\"\n    output = output.ravel()\n    cnn_dna = [list(output[x:x+4]) for x in range(0, len(output), 4)]\n    return cnn_dna\n```", "```py\ndef build(self, input_tensor):\n    \"\"\"\n    Method for creating the child neural network\n    Args:\n        input_tensor: The tensor which represents the input\n\n    Returns:\n        The tensor which represents the output logit (pre-softmax activation)\n\n    \"\"\"\n    logger.info(\"DNA is: {}\".format(self.cnn_dna))\n    output = input_tensor\n    for idx in range(len(self.cnn_dna)):\n        # Get the configuration for the layer\n        kernel_size, stride, num_filters, max_pool_size = self.cnn_dna[idx]\n        with tf.name_scope(\"child_{}_conv_layer_{}\".format(self.child_id, idx)):\n            output = tf.layers.conv2d(output,\n                    # Specify the number of filters the convolutional layer will output\n                    filters=num_filters,\n                    # This specifies the size (height, width) of the convolutional kernel\n                    kernel_size=(kernel_size, kernel_size),\n                    # The size of the stride of the kernel\n                    strides=(stride, stride),\n                    # We add padding to the image\n                    padding=\"SAME\",\n                    # It is good practice to name your layers\n                    name=\"conv_layer_{}\".format(idx),\n                    activation=tf.nn.relu,\n                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                    bias_initializer=tf.zeros_initializer(),\n                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta))\n```", "```py\n            # We apply 2D max pooling on the output of the conv layer\n            output = tf.layers.max_pooling2d(\n                output, pool_size=(max_pool_size, max_pool_size), strides=1,\n                padding=\"SAME\", name=\"pool_out_{}\".format(idx)\n            )\n            # Dropout to regularize the network further\n            output = tf.layers.dropout(output, rate=self.drop_rate, training=self.is_training)\n```", "```py\n    # Lastly, we flatten the outputs and add a fully-connected layer\n    with tf.name_scope(\"child_{}_fully_connected\".format(self.child_id)):\n        output = tf.layers.flatten(output, name=\"flatten\")\n        logits = tf.layers.dense(output, self.num_classes)\n\n    return logits\n```", "```py\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\n\nlogger = logging.getLogger(__name__)\n\ndef _create_tf_dataset(x, y, batch_size):\n    return tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x),\n                                tf.data.Dataset.from_tensor_slices(y))).shuffle(500).repeat().batch(batch_size)\n```", "```py\ndef get_tf_datasets_from_numpy(batch_size, validation_split=0.1):\n    \"\"\"\n    Main function getting tf.Data.datasets for training, validation, and testing\n\n    Args:\n        batch_size (int): Batch size\n        validation_split (float): Split for partitioning training and validation sets. Between 0.0 and 1.0.\n    \"\"\"\n    # Load data from keras datasets api\n    (X, y), (X_test, y_test) = cifar10.load_data()\n\n    logger.info(\"Dividing pixels by 255\")\n    X = X / 255.\n    X_test = X_test / 255.\n\n    X = X.astype(np.float32)\n    X_test = X_test.astype(np.float32)\n    y = y.astype(np.float32)\n    y_test = y_test.astype(np.float32)\n\n    # Turn labels into onehot encodings\n    if y.shape[1] != 10:\n        y = np_utils.to_categorical(y, num_classes=10)\n        y_test = np_utils.to_categorical(y_test, num_classes=10)\n\n    logger.info(\"Loaded data from keras\")\n\n    split_idx = int((1.0 - validation_split) * len(X))\n    X_train, y_train = X[:split_idx], y[:split_idx]\n    X_valid, y_valid = X[split_idx:], y[split_idx:]\n```", "```py\ntrain_dataset = _create_tf_dataset(X_train, y_train, batch_size)\nvalid_dataset = _create_tf_dataset(X_valid, y_valid, batch_size)\ntest_dataset = _create_tf_dataset(X_test, y_test, batch_size)\n\n# Get the batch sizes for the train, valid, and test datasets\nnum_train_batches = int(X_train.shape[0] // batch_size)\nnum_valid_batches = int(X_valid.shape[0] // batch_size)\nnum_test_batches = int(X_test.shape[0] // batch_size)\n\nreturn train_dataset, valid_dataset, test_dataset, num_train_batches, num_valid_batches, num_test_batches\n```", "```py\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom child_network import ChildCNN\nfrom cifar10_processor import get_tf_datasets_from_numpy\nfrom config import child_network_params, controller_params\n\nlogger = logging.getLogger(__name__)\n\ndef ema(values):\n    \"\"\"\n    Helper function for keeping track of an exponential moving average of a list of values.\n    For this module, we use it to maintain an exponential moving average of rewards\n\n    Args:\n        values (list): A list of rewards \n\n    Returns:\n        (float) The last value of the exponential moving average\n    \"\"\"\n    weights = np.exp(np.linspace(-1., 0., len(values)))\n    weights /= weights.sum()\n    a = np.convolve(values, weights, mode=\"full\")[:len(values)]\n    return a[-1]\n```", "```py\nclass Controller(object):\n\n    def __init__(self):\n        self.graph = tf.Graph()\n        self.sess = tf.Session(graph=self.graph)\n        self.num_cell_outputs = controller_params['components_per_layer'] * controller_params['max_layers']\n        self.reward_history = []\n        self.architecture_history = []\n        self.divison_rate = 100\n        with self.graph.as_default():\n            self.build_controller()\n```", "```py\ndef build_controller(self):\n    logger.info('Building controller network')\n    # Build inputs and placeholders\n    with tf.name_scope('controller_inputs'):\n        # Input to the NASCell\n        self.child_network_architectures = tf.placeholder(tf.float32, [None, self.num_cell_outputs], \n                                                          name='controller_input')\n        # Discounted rewards\n        self.discounted_rewards = tf.placeholder(tf.float32, (None, ), name='discounted_rewards')\n```", "```py\n# Build controller\nwith tf.name_scope('network_generation'):\n    with tf.variable_scope('controller'):\n        self.controller_output = tf.identity(self.network_generator(self.child_network_architectures), \n                                             name='policy_scores')\n        self.cnn_dna_output = tf.cast(tf.scalar_mul(self.divison_rate, self.controller_output), tf.int32,\n                                      name='controller_prediction')\n```", "```py\n# Set up optimizer\nself.global_step = tf.Variable(0, trainable=False)\nself.learning_rate = tf.train.exponential_decay(0.99, self.global_step, 500, 0.96, staircase=True)\nself.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n\n# Gradient and loss computation\nwith tf.name_scope('gradient_and_loss'):\n    # Define policy gradient loss for the controller\n    self.policy_gradient_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n        logits=self.controller_output[:, -1, :],\n        labels=self.child_network_architectures))\n    # L2 weight decay for Controller weights\n    self.l2_loss = tf.reduce_sum(tf.add_n([tf.nn.l2_loss(v) for v in\n                                           tf.trainable_variables(scope=\"controller\")]))\n    # Add the above two losses to define total loss\n    self.total_loss = self.policy_gradient_loss + self.l2_loss * controller_params[\"beta\"]\n    # Compute the gradients\n    self.gradients = self.optimizer.compute_gradients(self.total_loss)\n\n    # Gradients calculated using REINFORCE\n    for i, (grad, var) in enumerate(self.gradients):\n        if grad is not None:\n            self.gradients[i] = (grad * self.discounted_rewards, var)\n```", "```py\nwith tf.name_scope('train_controller'):\n    # The main training operation. This applies REINFORCE on the weights of the Controller\n    self.train_op = self.optimizer.apply_gradients(self.gradients, global_step=self.global_step)\n\nlogger.info('Successfully built controller')\n```", "```py\ndef network_generator(self, nas_cell_hidden_state):\n    # number of output units we expect from a NAS cell\n    with tf.name_scope('network_generator'):\n        nas = tf.contrib.rnn.NASCell(self.num_cell_outputs)\n        network_architecture, nas_cell_hidden_state = tf.nn.dynamic_rnn(nas, tf.expand_dims(\n            nas_cell_hidden_state, -1), dtype=tf.float32)\n        bias_variable = tf.Variable([0.01] * self.num_cell_outputs)\n        network_architecture = tf.nn.bias_add(network_architecture, bias_variable)\n        return network_architecture[:, -1:, :]\n```", "```py\ndef generate_child_network(self, child_network_architecture):\n    with self.graph.as_default():\n        return self.sess.run(self.cnn_dna_output, {self.child_network_architectures: child_network_architecture})\n```", "```py\ndef train_child_network(self, cnn_dna, child_id):\n    \"\"\"\n    Trains a child network and returns reward, or the validation accuracy\n    Args:\n        cnn_dna (list): List of tuples representing the child network's DNA\n        child_id (str): Name of child network\n\n    Returns:\n        (float) validation accuracy\n    \"\"\"\n    logger.info(\"Training with dna: {}\".format(cnn_dna))\n    child_graph = tf.Graph()\n    with child_graph.as_default():\n        sess = tf.Session()\n\n        child_network = ChildCNN(cnn_dna=cnn_dna, child_id=child_id, **child_network_params)\n```", "```py\n# Create input pipeline\ntrain_dataset, valid_dataset, test_dataset, num_train_batches, num_valid_batches, num_test_batches = \\\n    get_tf_datasets_from_numpy(batch_size=child_network_params[\"batch_size\"])\n\n# Generic iterator\niterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\nnext_tensor_batch = iterator.get_next()\n\n# Separate train and validation set init ops\ntrain_init_ops = iterator.make_initializer(train_dataset)\nvalid_init_ops = iterator.make_initializer(valid_dataset)\n\n# Build the graph\ninput_tensor, labels = next_tensor_batch\n```", "```py\n# Build the child network, which returns the pre-softmax logits of the child network\nlogits = child_network.build(input_tensor)\n\n# Define the loss function for the child network\nloss_ops = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits, name=\"loss\")\n\n# Define the training operation for the child network\ntrain_ops = tf.train.AdamOptimizer(learning_rate=child_network_params[\"learning_rate\"]).minimize(loss_ops)\n\n# The following operations are for calculating the accuracy of the child network\npred_ops = tf.nn.softmax(logits, name=\"preds\")\ncorrect = tf.equal(tf.argmax(pred_ops, 1), tf.argmax(labels, 1), name=\"correct\")\naccuracy_ops = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n\ninitializer = tf.global_variables_initializer()\n```", "```py\n# Training\nsess.run(initializer)\nsess.run(train_init_ops)\n\nlogger.info(\"Training child CNN {} for {} epochs\".format(child_id, child_network_params[\"max_epochs\"]))\nfor epoch_idx in range(child_network_params[\"max_epochs\"]):\n    avg_loss, avg_acc = [], []\n\n    for batch_idx in range(num_train_batches):\n        loss, _, accuracy = sess.run([loss_ops, train_ops, accuracy_ops])\n        avg_loss.append(loss)\n        avg_acc.append(accuracy)\n\n    logger.info(\"\\tEpoch {}:\\tloss - {:.6f}\\taccuracy - {:.3f}\".format(epoch_idx,\n                                                                       np.mean(avg_loss), np.mean(avg_acc)))\n```", "```py\n    # Validate and return reward\n    logger.info(\"Finished training, now calculating validation accuracy\")\n    sess.run(valid_init_ops)\n    avg_val_loss, avg_val_acc = [], []\n    for batch_idx in range(num_valid_batches):\n        valid_loss, valid_accuracy = sess.run([loss_ops, accuracy_ops])\n        avg_val_loss.append(valid_loss)\n        avg_val_acc.append(valid_accuracy)\n    logger.info(\"Valid loss - {:.6f}\\tValid accuracy - {:.3f}\".format(np.mean(avg_val_loss),\n                                                                      np.mean(avg_val_acc)))\n\nreturn np.mean(avg_val_acc)\n```", "```py\ndef train_controller(self):\n    with self.graph.as_default():\n        self.sess.run(tf.global_variables_initializer())\n\n    step = 0\n    total_rewards = 0\n    child_network_architecture = np.array([[10.0, 128.0, 1.0, 1.0] *\n                                           controller_params['max_layers']], dtype=np.float32)\n```", "```py\nfor episode in range(controller_params['max_episodes']):\n    logger.info('=============> Episode {} for Controller'.format(episode))\n    step += 1\n    episode_reward_buffer = []\n\n    for sub_child in range(controller_params[\"num_children_per_episode\"]):\n        # Generate a child network architecture\n        child_network_architecture = self.generate_child_network(child_network_architecture)[0]\n\n        if np.any(np.less_equal(child_network_architecture, 0.0)):\n            reward = -1.0\n        else:\n            reward = self.train_child_network(cnn_dna=child_network_architecture,\n                                              child_id='child/{}'.format(\"{}_{}\".format(episode, sub_child)))\n        episode_reward_buffer.append(reward)\n```", "```py\nmean_reward = np.mean(episode_reward_buffer)\n\nself.reward_history.append(mean_reward)\nself.architecture_history.append(child_network_architecture)\ntotal_rewards += mean_reward\n\nchild_network_architecture = np.array(self.architecture_history[-step:]).ravel() / self.divison_rate\nchild_network_architecture = child_network_architecture.reshape((-1, self.num_cell_outputs))\nbaseline = ema(self.reward_history)\nlast_reward = self.reward_history[-1]\nrewards = [last_reward - baseline]\nlogger.info(\"Buffers before loss calculation\")\nlogger.info(\"States: {}\".format(child_network_architecture))\nlogger.info(\"Rewards: {}\".format(rewards))\n\nwith self.graph.as_default():\n    _, loss = self.sess.run([self.train_op, self.total_loss],\n                            {self.child_network_architectures: child_network_architecture,\n                             self.discounted_rewards: rewards})\n\nlogger.info('Episode: {} | Loss: {} | DNA: {} | Reward : {}'.format(\n    episode, loss, child_network_architecture.ravel(), mean_reward))\n```", "```py\n$ ipython\nPython 3.6.4 (default, Jan 6 2018, 11:49:38)\nType 'copyright', 'credits' or 'license' for more information\nIPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.\n```", "```py\nIn [1]: import sys\n\nIn [2]: import logging\n\nIn [3]: logging.basicConfig(stream=sys.stdout,\n   ...: level=logging.DEBUG,\n   ...: format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n   ...:\n\nIn [4]:\n```", "```py\nIn [4]: import numpy as np\n\nIn [5]: from controller import Controller\n\nIn [6]:\n```", "```py\nIn [7]: dna = np.array([[3, 1, 30, 2], [3, 1, 30, 2], [3, 1, 40, 2]])\n```", "```py\nIn [8]: controller = Controller()\n\n...\n\n2018-09-16 01:58:54,978 controller INFO Successfully built controller\n\nIn [9]: controller.train_child_network(dna, \"test\")\n\n2018-09-16 01:58:59,208 controller INFO Training with dna: [[ 3 1 30 2]\n [ 3 1 30 2]\n [ 3 1 40 2]]\n2018-09-16 01:58:59,605 cifar10_processor INFO Dividing pixels by 255\n2018-09-16 01:59:01,289 cifar10_processor INFO Loaded data from keras\n2018-09-16 01:59:03,150 child_network INFO DNA is: [[3, 1, 30, 2], [3, 1, 30, 2], [3, 1, 40, 2]]\n2018-09-16 01:59:14,270 controller INFO Training child CNN first for 1000 epochs\n```", "```py\n2018-09-16 06:25:01,927 controller INFO Epoch 436: loss - 1.119608 accuracy - 0.663\n2018-09-16 06:25:19,310 controller INFO Epoch 437: loss - 0.634937 accuracy - 0.724\n2018-09-16 06:25:36,438 controller INFO Epoch 438: loss - 0.769766 accuracy - 0.702\n2018-09-16 06:25:53,413 controller INFO Epoch 439: loss - 0.760520 accuracy - 0.711\n2018-09-16 06:26:10,530 controller INFO Epoch 440: loss - 0.606741 accuracy - 0.812\n```", "```py\nchild_network_params = {\n    \"learning_rate\": 3e-5,\n    \"max_epochs\": 100,\n    \"beta\": 1e-3,\n    \"batch_size\": 20\n}\n\ncontroller_params = {\n    \"max_layers\": 3,\n    \"components_per_layer\": 4,\n    'beta': 1e-4,\n    'max_episodes': 2000,\n    \"num_children_per_episode\": 10\n}\n```", "```py\nimport logging\nimport sys\n\nfrom .controller import Controller\n\nif __name__ == '__main__':\n    # Configure the logger\n    logging.basicConfig(stream=sys.stdout,\n                        level=logging.DEBUG,\n                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n    controller = Controller()\n    controller.train_controller()\n```", "```py\nsrc\n|-- __init__.py\n|-- child_network.py\n|-- cifar10_processor.py\n|-- config.py\n|-- constants.py\n|-- controller.py\n`-- train.py\n```", "```py\n$ python train.py\n```", "```py\n2018-09-16 04:13:45,484 src.controller INFO Successfully built controller \n2018-09-16 04:13:45,542 src.controller INFO =============> Episode 0 for Controller \n2018-09-16 04:13:45,952 src.controller INFO Training with dna: [[ 2 10 2 4 1 1 12 14 7 1 1 1]] 2018-09-16 04:13:45.953482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0 \n2018-09-16 04:13:45.953530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix: \n2018-09-16 04:13:45.953543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0 \n2018-09-16 04:13:45.953558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0: N \n2018-09-16 04:13:45.953840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wi th 21618 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2) \n2018-09-16 04:13:47,143 src.cifar10_processor INFO Dividing pixels by 255 \n2018-09-16 04:13:55,119 src.cifar10_processor INFO Loaded data from keras \n2018-09-16 04:14:09,050 src.child_network INFO DNA is: [[2, 10, 2, 4], [1, 1, 12, 14], [7, 1, 1, 1]] \n2018-09-16 04:14:21,326 src.controller INFO Training child CNN child/0_0 for 100 epochs \n2018-09-16 04:14:32,830 src.controller INFO Epoch 0: loss - 2.351300 accuracy - 0.100\n2018-09-16 04:14:43,976 src.controller INFO Epoch 1: loss - 2.202928 accuracy - 0.180 \n2018-09-16 04:14:53,412 src.controller INFO Epoch 2: loss - 2.102713 accuracy - 0.220 \n2018-09-16 04:15:03,704 src.controller INFO Epoch 3: loss - 2.092676 accuracy - 0.232 \n2018-09-16 04:15:14,349 src.controller INFO Epoch 4: loss - 2.092633 accuracy - 0.240\n\n```"]