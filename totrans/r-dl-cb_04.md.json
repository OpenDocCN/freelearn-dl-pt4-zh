["```py\n# Function to load Occupancy data\nload_occupancy_data<-function(train){\nxFeatures = c(\"Temperature\", \"Humidity\", \"Light\", \"CO2\",\n  \"HumidityRatio\")\nyFeatures = \"Occupancy\"\n  if(train){\n    occupancy_ds <-  as.matrix(read.csv(\"datatraining.txt\",stringsAsFactors = T))\n  } else\n  {\n    occupancy_ds <- as.matrix(read.csv(\"datatest.txt\",stringsAsFactors = T))\n  }\n  occupancy_ds<-apply(occupancy_ds[, c(xFeatures, yFeatures)], 2, FUN=as.numeric) \n  return(occupancy_ds)\n}\n\n```", "```py\noccupancy_train <-load_occupancy_data(train=T)\noccupancy_test <- load_occupancy_data(train = F)\n\n```", "```py\n> ggpairs(occupancy_train$data[, occupancy_train$xFeatures])\n\n```", "```py\nminmax.normalize<-function(ds, scaler=NULL){\n  if(is.null(scaler)){\n    for(f in ds$xFeatures){\n      scaler[[f]]$minval<-min(ds$data[,f])\n      scaler[[f]]$maxval<-max(ds$data[,f])\n      ds$data[,f]<-(ds$data[,f]-scaler[[f]]$minval)/(scaler[[f]]$maxval-scaler[[f]]$minval)\n    }\n    ds$scaler<-scaler\n  } else\n  {\n    for(f in ds$xFeatures){\n      ds$data[,f]<-(ds$data[,f]-scaler[[f]]$minval)/(scaler[[f]]$maxval-scaler[[f]]$minval)\n    }\n  }\n  return(ds)\n}\n\n```", "```py\n# Reset the graph and set-up a interactive session\ntf$reset_default_graph()\nsess<-tf$InteractiveSession()\n\n```", "```py\n# Network Parameters\nn_hidden_1 = 5 # 1st layer num features\nn_input = length(xFeatures) # Number of input features\nnRow<-nrow(occupancy_train)\n\n```", "```py\n# Define input feature\nx <- tf$constant(unlist(occupancy_train[, xFeatures]), shape=c(nRow, n_input), dtype=np$float32) \n\n# Define hidden and bias layer for encoder and decoders\nhiddenLayerEncoder<-tf$Variable(tf$random_normal(shape(n_input, n_hidden_1)), dtype=np$float32)\nbiasEncoder <- tf$Variable(tf$zeros(shape(n_hidden_1)), dtype=np$float32)\nhiddenLayerDecoder<-tf$Variable(tf$random_normal(shape(n_hidden_1, n_input)))\nbiasDecoder <- tf$Variable(tf$zeros(shape(n_input)))\n\n```", "```py\nauto_encoder<-function(x, hiddenLayerEncoder, biasEncoder){\n  x_transform <- tf$nn$sigmoid(tf$add(tf$matmul(x, hiddenLayerEncoder), biasEncoder))\n  x_transform\n}\n\n```", "```py\nencoder_obj = auto_encoder(x,hiddenLayerEncoder, biasEncoder)\ny_pred = auto_encoder(encoder_obj, hiddenLayerDecoder, biasDecoder)\n\n```", "```py\nDefine loss function and optimizer module. \nlearning_rate = 0.01\ncost = tf$reduce_mean(tf$pow(x - y_pred, 2))\noptimizer = tf$train$RMSPropOptimizer(learning_rate)$minimize(cost)\n\n```", "```py\n# Initializing the variables\ninit = tf$global_variables_initializer()\nsess$run(init)\n\n```", "```py\ncostconvergence<-NULL\nfor (step in 1:1000) {\n  sess$run(optimizer)\n  if (step %% 20==0){\n    costconvergence<-rbind(costconvergence, c(step, sess$run(cost), sess$run(costt)))\n    cat(step, \"-\", \"Traing Cost ==>\", sess$run(cost), \"\\n\")\n  }\n}\n\n```", "```py\ncostconvergence<-data.frame(costconvergence)\ncolnames(costconvergence)<-c(\"iter\", \"train\", \"test\")\nplot(costconvergence[, \"iter\"], costconvergence[, \"train\"], type = \"l\", col=\"blue\", xlab = \"Iteration\", ylab = \"MSE\")\nlines(costconvergence[, \"iter\"], costconvergence[, \"test\"], col=\"red\")\nlegend(500,0.25, c(\"Train\",\"Test\"), lty=c(1,1), lwd=c(2.5,2.5),col=c(\"blue\",\"red\"))\n\n```", "```py\nLambda=0.01\ncost = tf$reduce_mean(tf$pow(x - y_pred, 2))\nRegularize_weights = tf$nn$l2_loss(weights)\ncost = tf$reduce_mean(cost + lambda * Regularize_weights)\n\n```", "```py\ninstall.packages(\"SAENET\")\n\n```", "```py\nrequire(SAENET)\n\n```", "```py\noccupancy_train <-load_occupancy_data(train=T)\noccupancy_test <- load_occupancy_data(train = F)\n\n```", "```py\n# Normalize dataset\noccupancy_train<-minmax.normalize(occupancy_train, scaler = NULL)\noccupancy_test<-minmax.normalize(occupancy_test, scaler = occupancy_train$scaler)\n\n```", "```py\n# Building Stacked Autoencoder\nSAE_obj<-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 2), unit.type =\"tanh\", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000)\n\n```", "```py\ntrain_data <- flat_data(x_listdata = images.rgb.train)\ntest_data <- flat_data(x_listdata = images.rgb.test)\nvalid_data <- flat_data(x_listdata = images.rgb.valid)\n\n```", "```py\n> dim(train_data$images)\n[1] 40000  3072\n\n```", "```py\n# Add noise using masking or salt & pepper noise method\nadd_noise<-function(data, frac=0.10, corr_type=c(\"masking\", \"saltPepper\", \"none\")){\n  if(length(corr_type)>1) corr_type<-corr_type[1] \n\n  # Assign a copy of data\n  data_noise = data\n\n  # Evaluate chaining parameters for autoencoder\n  nROW<-nrow(data)\n  nCOL<-ncol(data)\n  nMask<-floor(frac*nCOL)\n\n  if(corr_type==\"masking\"){\n    for( i in 1:nROW){\n      maskCol<-sample(nCOL, nMask)\n      data_noise[i,maskCol,,]<-0\n    }\n  } else if(corr_type==\"saltPepper\"){\n    minval<-min(data[,,1,])\n    maxval<-max(data[,,1,])\n    for( i in 1:nROW){\n      maskCol<-sample(nCOL, nMask)\n      randval<-runif(length(maskCol))\n      ixmin<-randval<0.5\n      ixmax<-randval>=0.5\n      if(sum(ixmin)>0) data_noise[i,maskCol[ixmin],,]<-minval\n      if(sum(ixmax)>0) data_noise[i,maskCol[ixmax],,]<-maxval\n    }\n  } else\n  {\n    data_noise<-data\n  }\n  return(data_noise)\n}\n\n```", "```py\n# Corrupting input signal\nxcorr<-add_noise(train_data$images, frac=0.10, corr_type=\"masking\")\n\n```", "```py\n# Reset the graph and set-up an interactive session\ntf$reset_default_graph()\nsess<-tf$InteractiveSession()\n\n```", "```py\n# Define Input as Placeholder variables\nx = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x')\nx_corrput<-tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x_corrput')\n\n```", "```py\n# Setting-up denoising autoencoder\ndenoisingAutoencoder<-function(x, x_corrput, img_size_flat=3072, hidden_layer=c(1024, 512), out_img_size=256){\n\n  # Building Encoder\n  encoder = NULL\n  n_input<-img_size_flat\n  curentInput<-x_corrput\n  layer<-c(hidden_layer, out_img_size)\n  for(i in 1:length(layer)){\n    n_output<-layer[i]\n    W = tf$Variable(tf$random_uniform(shape(n_input, n_output), -1.0 / tf$sqrt(n_input), 1.0 / tf$sqrt(n_input)))\n    b = tf$Variable(tf$zeros(shape(n_output)))\n    encoder<-c(encoder, W)\n    output = tf$nn$tanh(tf$matmul(curentInput, W) + b)\n    curentInput = output\n    n_input<-n_output\n  }\n\n  # latent representation\n  z = curentInput\n  encoder<-rev(encoder)\n  layer_rev<-c(rev(hidden_layer), img_size_flat)\n\n  # Build the decoder using the same weights\n  decoder<-NULL\n  for(i in 1:length(layer_rev)){\n    n_output<-layer_rev[i]\n    W = tf$transpose(encoder[[i]])\n    b = tf$Variable(tf$zeros(shape(n_output)))\n    output = tf$nn$tanh(tf$matmul(curentInput, W) + b)\n    curentInput = output\n  }\n\n  # now have the reconstruction through the network\n  y = curentInput\n\n  # cost function measures pixel-wise difference\n  cost = tf$sqrt(tf$reduce_mean(tf$square(y - x)))\n  return(list(\"x\"=x, \"z\"=z, \"y\"=y, \"x_corrput\"=x_corrput, \"cost\"=cost))\n}\n\n```", "```py\n# Create denoising AE object\ndae_obj<-denoisingAutoencoder(x, x_corrput, img_size_flat=3072, hidden_layer=c(1024, 512), out_img_size=256)\n\n```", "```py\n# Learning set-up\nlearning_rate = 0.001\noptimizer = tf$train$AdamOptimizer(learning_rate)$minimize(dae_obj$cost)\n\n```", "```py\n# We create a session to use the graph\nsess$run(tf$global_variables_initializer())\nfor(i in 1:500){\n  spls <- sample(1:dim(xcorr)[1],1000L)\n  if (i %% 1 == 0) {\n    x_corrput_ds<-add_noise(train_data$images[spls, ], frac = 0.3, corr_type = \"masking\")\n    optimizer$run(feed_dict = dict(x=train_data$images[spls, ], x_corrput=x_corrput_ds))\n    trainingCost<-dae_obj$cost$eval((feed_dict = dict(x=train_data$images[spls, ], x_corrput=x_corrput_ds)))\n    cat(\"Training Cost - \", trainingCost, \"\\n\")\n  }\n}\n\n```", "```py\nrequire(tensorflow)\n\n```", "```py\nrequire(imager)\nrequire(caret)\n\n```", "```py\n# Normalize Dataset\nnormalizeObj<-preProcess(trainData, method=\"range\")\ntrainData<-predict(normalizeObj, trainData)\nvalidData<-predict(normalizeObj, validData)\n\n```", "```py\nlibrary(tensorflow)\ndatasets <- tf$contrib$learn$datasets\nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE) \n\n```", "```py\n# Function to reduce image size\nreduceImage<-function(actds, n.pixel.x=16, n.pixel.y=16){\n  actImage<-matrix(actds, ncol=28, byrow=FALSE)\n  img.col.mat <- imappend(list(as.cimg(actImage)),\"c\")\n  thmb <- resize(img.col.mat, n.pixel.x, n.pixel.y)\n  outputImage<-matrix(thmb[,,1,1], nrow = 1, byrow = F)\n  return(outputImage)\n} \n\n```", "```py\n# Covert train data to 16 x 16  image\ntrainData<-t(apply(mnist$train$images, 1, FUN=reduceImage))\nvalidData<-t(apply(mnist$test$images, 1, FUN=reduceImage))\n\n```", "```py\n# Function to plot MNIST dataset\nplot_mnist<-function(imageD, pixel.y=16){\n  actImage<-matrix(imageD, ncol=pixel.y, byrow=FALSE)\n  img.col.mat <- imappend(list(as.cimg(actImage)), \"c\")\n  plot(img.col.mat)\n}\n\n```", "```py\ntf$reset_default_graph()\nsess<-tf$InteractiveSession()\n\n```", "```py\nn_input=256\nn.hidden.enc.1<-64\n\n```", "```py\ntf$reset_default_graph()\nsess<-tf$InteractiveSession()\n\n```", "```py\nn_input=256\nn.hidden.enc.1<-64\n\n```", "```py\nmodel_init<-function(n.hidden.enc.1, n.hidden.enc.2, \n                               n.hidden.dec.1,  n.hidden.dec.2, \n                               n_input, n_h)\n{ weights<-NULL\n ############################\n # Set-up Encoder\n ############################\n # Initialize Layer 1 of encoder\n weights[[\"encoder_w\"]][[\"h1\"]]=tf$Variable(xavier_init(n_input,\n n.hidden.enc.1))\n weights[[\"encoder_w\"]]\n[[\"h2\"]]=tf$Variable(xavier_init(n.hidden.enc.1, n.hidden.enc.2))\n  weights[[\"encoder_w\"]][[\"out_mean\"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))\n  weights[[\"encoder_w\"]][[\"out_log_sigma\"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))\n  weights[[\"encoder_b\"]][[\"b1\"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.1), dtype=tf$float32))\n  weights[[\"encoder_b\"]][[\"b2\"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.2), dtype=tf$float32))\n  weights[[\"encoder_b\"]][[\"out_mean\"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))\n  weights[[\"encoder_b\"]][[\"out_log_sigma\"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))\n\n ############################\n # Set-up Decoder\n ############################\n weights[['decoder_w']][[\"h1\"]]=tf$Variable(xavier_init(n_h, n.hidden.dec.1))\n weights[['decoder_w']][[\"h2\"]]=tf$Variable(xavier_init(n.hidden.dec.1, n.hidden.dec.2))\n weights[['decoder_w']][[\"out_mean\"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))\n weights[['decoder_w']][[\"out_log_sigma\"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))\n weights[['decoder_b']][[\"b1\"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.1), dtype=tf$float32))\n weights[['decoder_b']][[\"b2\"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.2), dtype=tf$float32))\n weights[['decoder_b']][[\"out_mean\"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))\n weights[['decoder_b']][[\"out_log_sigma\"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))\n return(weights)\n} \n\n```", "```py\n# Xavier Initialization using Uniform distribution \nxavier_init<-function(n_inputs, n_outputs, constant=1){\n  low = -constant*sqrt(6.0/(n_inputs + n_outputs)) \n  high = constant*sqrt(6.0/(n_inputs + n_outputs))\n  return(tf$random_uniform(shape(n_inputs, n_outputs), minval=low, maxval=high, dtype=tf$float32))\n}\n\n```", "```py\n# Encoder update function\nvae_encoder<-function(x, weights, biases){\n  layer_1 = tf$nn$softplus(tf$add(tf$matmul(x, weights[['h1']]), biases[['b1']])) \n  layer_2 = tf$nn$softplus(tf$add(tf$matmul(layer_1, weights[['h2']]), biases[['b2']])) \n  z_mean = tf$add(tf$matmul(layer_2, weights[['out_mean']]), biases[['out_mean']])\n  z_log_sigma_sq = tf$add(tf$matmul(layer_2, weights[['out_log_sigma']]), biases[['out_log_sigma']])\n  return (list(\"z_mean\"=z_mean, \"z_log_sigma_sq\"=z_log_sigma_sq))\n} \n\n```", "```py\n# Decoder update function\nvae_decoder<-function(z, weights, biases){\n  layer1<-tf$nn$softplus(tf$add(tf$matmul(z, weights[[\"h1\"]]), biases[[\"b1\"]]))\n  layer2<-tf$nn$softplus(tf$add(tf$matmul(layer1, weights[[\"h2\"]]), biases[[\"b2\"]]))\n  x_reconstr_mean<-tf$nn$sigmoid(tf$add(tf$matmul(layer2, weights[['out_mean']]), biases[['out_mean']]))\n  return(x_reconstr_mean)\n}\n\n```", "```py\n# Parameter evaluation\nnetwork_ParEval<-function(x, network_weights, n_h){\n\n  distParameter<-vae_encoder(x, network_weights[[\"encoder_w\"]], network_weights[[\"encoder_b\"]])\n  z_mean<-distParameter$z_mean\n  z_log_sigma_sq <-distParameter$z_log_sigma_sq\n\n  # Draw one sample z from Gaussian distribution\n  eps = tf$random_normal(shape(BATCH, n_h), 0, 1, dtype=tf$float32)\n\n  # z = mu + sigma*epsilon\n  z = tf$add(z_mean, tf$multiply(tf$sqrt(tf$exp(z_log_sigma_sq)), eps))\n\n  # Use generator to determine mean of\n  # Bernoulli distribution of reconstructed input\n  x_reconstr_mean <- vae_decoder(z, network_weights[[\"decoder_w\"]], network_weights[[\"decoder_b\"]])\n  return(list(\"x_reconstr_mean\"=x_reconstr_mean, \"z_log_sigma_sq\"=z_log_sigma_sq, \"z_mean\"=z_mean))\n}\n\n```", "```py\n# VAE cost function\nvae_optimizer<-function(x, networkOutput){\n  x_reconstr_mean<-networkOutput$x_reconstr_mean\n  z_log_sigma_sq<-networkOutput$z_log_sigma_sq\n  z_mean<-networkOutput$z_mean\n  loss_reconstruction<--1*tf$reduce_sum(x*tf$log(1e-10 + x_reconstr_mean)+\n                                       (1-x)*tf$log(1e-10 + 1 - x_reconstr_mean), reduction_indices=shape(1))\n  loss_latent<--0.5*tf$reduce_sum(1+z_log_sigma_sq-tf$square(z_mean)-\n                                    tf$exp(z_log_sigma_sq), reduction_indices=shape(1))\n  cost = tf$reduce_mean(loss_reconstruction + loss_latent)\n  return(cost)\n}\n\n```", "```py\n# VAE Initialization\nx = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x')\nnetwork_weights<-model_init(n.hidden.enc.1, n.hidden.enc.2, \n                            n.hidden.dec.1,  n.hidden.dec.2, \n                            n_input, n_h)\nnetworkOutput<-network_ParEval(x, network_weights, n_h)\ncost=vae_optimizer(x, networkOutput)\noptimizer = tf$train$AdamOptimizer(lr)$minimize(cost) \n\n```", "```py\nsess$run(tf$global_variables_initializer())\nfor(i in 1:ITERATION){\n  spls <- sample(1:dim(trainData)[1],BATCH)\n  out<-optimizer$run(feed_dict = dict(x=trainData[spls,]))\n  if (i %% 100 == 0){\n  cat(\"Iteration - \", i, \"Training Loss - \",  cost$eval(feed_dict = dict(x=trainData[spls,])), \"\\n\")\n  }\n}\n\n```", "```py\nspls <- sample(1:dim(trainData)[1],BATCH)\nnetworkOutput_run<-sess$run(networkOutput, feed_dict = dict(x=trainData[spls,]))\n\n# Plot reconstructured Image\nx_sample<-trainData[spls,]\nNROW<-nrow(networkOutput_run$x_reconstr_mean)\nn.plot<-5\npar(mfrow = c(n.plot, 2), mar = c(0.2, 0.2, 0.2, 0.2), oma = c(3, 3, 3, 3))\npltImages<-sample(1:NROW,n.plot)\nfor(i in pltImages){\n  plot_mnist(x_sample[i,])\n  plot_mnist(networkOutput_run$x_reconstr_mean[i,])\n}\n\n```", "```py\n# Setting-up principal component analysis \npca_obj <- prcomp(occupancy_train$data,\n                 center = TRUE,\n                 scale. = TRUE)\n                 scale. = TRUE)\n\n```", "```py\nplot(pca_obj, type = \"l\")\n\n```", "```py\nSAE_obj<-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 1), unit.type =\"tanh\", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000) \n\n```", "```py\nSAE_obj<-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 2), unit.type =\"tanh\", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000)  \n\n# plotting encoder values\nplot(SAE_obj[[3]]$X.output[,1], SAE_obj[[3]]$X.output[,2], col=\"blue\", xlab = \"Node 1 of layer 3\", ylab = \"Node 2 of layer 3\")\nix<-occupancy_train$data[,6]==1  \npoints(SAE_obj[[3]]$X.output[ix,1], SAE_obj[[3]]$X.output[ix,2], col=\"red\")\n\n```", "```py\ninstall.packages(\"autoencoder\")\nrequire(autoencoder)\n\n```", "```py\n### Setting-up parameter\nnl<-3 \nN.hidden<-100 \nunit.type<-\"logistic\" \nlambda<-0.001 \nrho<-0.01 \nbeta<-6 \nmax.iterations<-2000 \nepsilon<-0.001 \n\n### Running sparse autoencoder\nspe_ae_obj <- autoencode(X.train=trainData,  X.test = validData, nl=nl, N.hidden=N.hidden, unit.type=unit.type,lambda=lambda,beta=beta,              epsilon=epsilon,rho=rho,max.iterations=max.iterations, rescale.flag = T)\n\n```"]