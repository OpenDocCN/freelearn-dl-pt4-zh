["```py\nimport TensorFlow  as tf\nhelloWorld = tf.constant(\"Hello World!\")\nsess = tf.Session()\nprint(sess.run(helloWorld))\n```", "```py\nimport TensorFlow  as tf\nimport numpy as np\n\ntf.InteractiveSession()\n\n# TensorFlow  operations\na = tf.zeros((3,3))\nb = tf.ones((3,3))\n\nprint(tf.reduce_sum(b, reduction_indices=1).eval())\nprint(a.get_shape())\n\n# numpy operations\na = np.zeros((3, 3))\nb = np.ones((3, 3))\nprint(np.sum(b, axis=1))\nprint(a.shape)\n```", "```py\n[ 3\\.  3\\.  3.]\n(3, 3)\n[ 3\\.  3\\.  3.]\n(3, 3)\n```", "```py\nnewGraph = tf.Graph()\nwith newGraph.as_default():\n    newGraphConst = tf.constant([2., 3.])\n\n```", "```py\n# session objects\na = tf.constant(6.0)\nb = tf.constant(7.0)\n\nc = a * b\nwith tf.Session() as sess:\n   print(sess.run(c))\n   print(c.eval())\n```", "```py\n42.0, 42.0\n```", "```py\nsession = tf.InteractiveSession()\ncons1 = tf.constant(1)\ncons2 = tf.constant(2)\ncons3 = cons1 + cons2\n# instead of sess.run(cons3)\ncons3.eval()\n```", "```py\n# tensor variablesW1 = tf.ones((3,3))\nW2 = tf.Variable(tf.zeros((3,3)), name=\"weights\")\n\n with tf.Session() as sess:\n   print(sess.run(W1))\n   sess.run(tf.global_variables_initializer())\n   print(sess.run(W2))\n```", "```py\n[[ 1\\.  1\\.  1.] [ 1\\.  1\\.  1.] [ 1\\.  1\\.  1.]]\n[[ 0\\.  0\\.  0.] [ 0\\.  0\\.  0.] [ 0\\.  0\\.  0.]]\n```", "```py\n# Variable objects can be initialized from constants or random values\nW = tf.Variable(tf.zeros((2,2)), name=\"weights\")\nR = tf.Variable(tf.random_normal((2,2)), name=\"random_weights\")\n\nwith tf.Session() as sess:\n   # Initializes all variables with specified values.\n   sess.run(tf.initialize_all_variables())\n   print(sess.run(W))\n   print(sess.run(R))\n```", "```py\n[[ 0\\.  0.] [ 0\\.  0.]]\n[[ 0.65469146 -0.97390586] [-2.39198709  0.76642162]]\n```", "```py\nstate = tf.Variable(0, name=\"counter\")\nnew_value = tf.add(state, tf.constant(1))\nupdate = tf.assign(state, new_value)\n\nwith tf.Session() as sess:\n   sess.run(tf.initialize_all_variables())\n   print(sess.run(state))\n   for _ in range(3):\n      sess.run(update)\n      print(sess.run(state))\n```", "```py\n0 1 2 3\n```", "```py\ninput1 = tf.constant(5.0)\ninput2 = tf.constant(6.0)\ninput3 = tf.constant(7.0)\nintermed = tf.add(input2, input3)\nmul = tf.multiply(input1, intermed)\n\n# Calling sess.run(var) on a tf.Session() object retrieves its value. Can retrieve multiple variables simultaneously with sess.run([var1, var2])\nwith tf.Session() as sess:\n   result = sess.run([mul, intermed])\n   print(result)\n```", "```py\n[65.0, 13.0]\n```", "```py\nwith tf.variable_scope(\"foo\"):\n     with tf.variable_scope(\"bar\"):\n         v = tf.get_variable(\"v\", [1])\n assert v.name == \"foo/bar/v:0\" with tf.variable_scope(\"foo\"):\n     v = tf.get_variable(\"v\", [1])\n     tf.get_variable_scope().reuse_variables()\n     v1 = tf.get_variable(\"v\", [1])\n assert v1 == v\n```", "```py\n#reuse is falsewith tf.variable_scope(\"foo\"):\n     n = tf.get_variable(\"n\", [1])\n assert v.name == \"foo/n:0\" *#Reuse is true* with tf.variable_scope(\"foo\"):\n     n = tf.get_variable(\"n\", [1])\n with tf.variable_scope(\"foo\", reuse=True):\n     v1 = tf.get_variable(\"n\", [1])\n assert v1 == n\n```", "```py\na = np.zeros((3,3))\nta = tf.convert_to_tensor(a)\nwith tf.Session() as sess:\n   print(sess.run(ta))\n```", "```py\n[[ 0\\. 0\\. 0.] [ 0\\. 0\\. 0.] [ 0\\. 0\\. 0.]]\n```", "```py\ninput1 = tf.placeholder(tf.float32)\n input2 = tf.placeholder(tf.float32)\n output = tf.multiply(input1, input2)\n\n with tf.Session() as sess:\n    print(sess.run([output], feed_dict={input1:[5.], input2:[6.]}))\n```", "```py\n[array([ 30.], dtype=float32)]\n```", "```py\nimport TensorFlow  as tf\n\n# get mnist dataset\nfrom TensorFlow .examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# x represents image with 784 values as columns (28*28), y represents output digit\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# initialize weights and biases [w1,b1][w2,b2]\nnumNeuronsInDeepLayer = 30\nw1 = tf.Variable(tf.truncated_normal([784, numNeuronsInDeepLayer]))\nb1 = tf.Variable(tf.truncated_normal([1, numNeuronsInDeepLayer]))\nw2 = tf.Variable(tf.truncated_normal([numNeuronsInDeepLayer, 10]))\nb2 = tf.Variable(tf.truncated_normal([1, 10]))\n\n# non-linear sigmoid function at each neuron\ndef sigmoid(x):\n    sigma = tf.div(tf.constant(1.0), tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n    return sigma\n\n# starting from first layer with wx+b, then apply sigmoid to add non-linearity\nz1 = tf.add(tf.matmul(x, w1), b1)\na1 = sigmoid(z1)\nz2 = tf.add(tf.matmul(a1, w2), b2)\na2 = sigmoid(z2)\n\n# calculate the loss (delta)\nloss = tf.subtract(a2, y)\n\n# derivative of the sigmoid function der(sigmoid)=sigmoid*(1-sigmoid)\ndef sigmaprime(x):\n    return tf.multiply(sigmoid(x), tf.subtract(tf.constant(1.0), sigmoid(x)))\n\n# automatic differentiation\ncost = tf.multiply(loss, loss)\nstep = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n\nacct_mat = tf.equal(tf.argmax(a2, 1), tf.argmax(y, 1))\nacct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(10000):\n    batch_xs, batch_ys = data.train.next_batch(10)\n    sess.run(step, feed_dict={x: batch_xs,\n                              y: batch_ys})\n    if i % 1000 == 0:\n        res = sess.run(acct_res, feed_dict=\n        {x: data.test.images[:1000],\n         y: data.test.labels[:1000]})\n        print(res)\n```"]