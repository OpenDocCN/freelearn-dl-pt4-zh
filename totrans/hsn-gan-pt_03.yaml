- en: Getting Started with PyTorch 1.3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch 1.3 has finally arrived! Are you ready to exploit its new features and
    functionalities to make your research and production easier?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will walk you through the breaking changes introduced in
    PyTorch, including switching from eager mode to graph mode. We will look at how
    to migrate older code to 1.x and walk you through the PyTorch ecosystem along
    with Cloud support.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we will introduce how to install CUDA so that you can take advantage of
    GPU acceleration for faster training and evaluation with your PyTorch code. We
    will show you the step-by-step installation process of PyTorch on Windows 10 and
    Ubuntu 18.04 (with pure Python or an Anaconda environment) and how to build PyTorch
    from source.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as bonus content, we will present how to configure Microsoft VS Code
    for PyTorch development and some of the best extensions to make your work more
    enjoyable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What's new in PyTorch 1.3?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA - GPU acceleration for fast training and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Pytorch on Windows and Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's new in PyTorch 1.3?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PyTorch** ([https://pytorch.org](https://pytorch.org)) is an open source machine
    learning platform for Python. It is specifically designed for deep learning applications,
    such as **Convolutional Neural Networks** (**CNNs**), **Recurrent Neural Networks**
    (**RNNs**), and **Generative Adversarial Networks** (**GANs**), and it includes extensive
    layer definitions for these applications. It has built-in tensor operations that
    are designed to be used in the same way as NumPy arrays, and they are also optimized
    to run on GPUs for fast computation. It provides an automatic computational graph
    scheme so that you won''t need to calculate derivatives by hand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After around 3 years of development and improvements, PyTorch has finally reached
    its newest milestone, version 1.3! What comes with it is a big package of new
    features and new functionalities. Don''t worry about whether you''ll have to re-learn
    the tool; even when it''s a totally new version, PyTorch has always been good
    at keeping its core functionality consistent. In fact, its core modules haven''t
    changed much since its alpha release (version 0.1.1): `torch.nn`, `torch.autograd`,
    and `torch.optim`, unlike some other platforms. (Yes! We''re talking about you,
    TensorFlow!) Now let''s take a look at some of the new features in PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Easy switching from eager mode to graph mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When PyTorch first caught people's attention around 2 years ago, one of its
    biggest advantages over other deep learning tools was its dynamic graph support.
    It might be the main reason people ditch their old tools and embrace PyTorch.
    As you might have noticed, recently, more authors of the latest deep learning
    papers are using PyTorch to implement their experiments.
  prefs: []
  type: TYPE_NORMAL
- en: However, it doesn't mean that PyTorch is not fit for production environments.
    In version 1.0, PyTorch provides a **hybrid frontend** that easily transfers your
    code from eager mode (dynamic graph) to graph mode (static graph). You can write
    your code in as flexible a way as before. When you are satisfied with your code,
    just by changing a few lines of code in your model, it will be ready to be optimized
    for efficiency in graph mode. This process is accomplished by the torch.jit compiler.
    **JIT** (**Just**-**In**-**Time**) compiler is designed to serialize and optimize
    PyTorch code into **TorchScript**, which can run without a Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that, now, you can easily export your model to an environment where
    Python is not available or efficiency is extremely important, and call your model
    with C++ code. Two modalities are provided to convert traditional PyTorch code
    to TorchScript: tracing and scripting. **Tracing** is perfect for directly transforming
    your fixed model scheme with fixed inputs to graph mode.'
  prefs: []
  type: TYPE_NORMAL
- en: However, if there is any data-dependent control flow in your model (for example,
    RNN), **scripting** is designed for this type of scenario, where all possible
    control flow routes are converted into TorchScript. Bear in mind that, for now
    (at the time of writing this book), scripting still has its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic graph** means that the computational graph is established each time
    you run your model and can be changed between different runs. It''s like everyone
    driving their own cars around the streets, when anyone can go anywhere each time
    they leave their home. It''s flexible for research purposes. However, the additional
    resource overheads that building the graphs before each run requires cannot be
    overlooked. Therefore, it might be a little inefficient for production purposes.
    **Static graph** means that the computational graph has to be established before
    the first run and it cannot be changed once established. It''s like everyone going
    to work on the bus. It''s efficient, but if the passengers want to travel to different
    destinations, they have to talk to the bus driver, who will then talk to the public
    transportation authorities. Then, the bus route can be changed the next day.'
  prefs: []
  type: TYPE_NORMAL
- en: Here's an example of how to change your models to graph mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we already have `model` on a given `device`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We only need to add these lines to `trace` the `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can `save` the traced model to file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that you should avoid using `torch.save(traced_model.state_dict(), "model_jit.pth")`
    to save the traced model, because, at the time of writing this book, the checkpoint
    file created in this way cannot be properly processed by the C++ API.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the traced model can be used in the same way as a normal `torch.nn.Module`
    in Python, and can also be used by other C++ code, which we will cover later.
    The full code for this example, where we train and export a CNN for classification
    on MNIST can be found in the `jit/mnist_jit.py` file located in the code repository
    for this chapter. You can refer to the official tutorial for more information
    about the hybrid frontend: [https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html](https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: The C++ frontend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though the backend of PyTorch is mostly implemented by C++, its frontend
    API has always been focused on Python. It's partly because Python has already
    been very popular among data scientists and it has tons of open source packages
    that help you focus on solving the problems, rather than re-creating the wheel.
    Also, it's extremely easy to read and write. However, Python is not known for
    computation and memory resource efficiency. Big companies often develop their
    own tools in C++ for better performance. But smaller companies or individual developers
    find it difficult to divert their main focus to developing their own C++ tools.
    Luckily, PyTorch has now shipped the C++ API with version 1.0\. Now, anyone can
    build efficient projects with it.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, right now, the C++ API of PyTorch is still under development and
    may undergo some changes in the future. In fact, the changes between v1.0.1 and
    v1.0.0 are so huge that the official documents and tutorials for v1.0.0 would
    not fit v1.0.1.
  prefs: []
  type: TYPE_NORMAL
- en: Here's an example of how to use the C++ API provided by PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the traced model we exported previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s feed a dummy input image to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The full code for the C++ example can be found under the `jit` directory located
    in the code repository for this chapter, including a `CMakeLists.txt` file for
    compiling the `.cpp` file. You can refer to the official documentation for more
    information about C++ APIs: [https://pytorch.org/cppdocs](https://pytorch.org/cppdocs).
  prefs: []
  type: TYPE_NORMAL
- en: The redesigned distributed library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Debugging multithreading programs on a CPU is painful. Designing efficient GPU
    programs on distributed systems can be even more so. Fortunately, PyTorch keeps
    delivering ease-of-use distributed solutions for this very purpose. In version
    1.0, the `torch.distributed` module is performance-driven and runs asynchronously
    for all backends, including Gloo, NCCL, and MPI. The new distributed library is
    designed to deliver near-optimal performance on both single-node and multi-node
    systems. It's also specially optimized for less advanced network communication
    scenarios by reducing bandwidth exchanges and thus improves the performance of
    these systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NCCL backend is used for distributed GPU training, and the Gloo backend
    is used for distributed CPU training. The new distributed package also provides
    a helper utility, `torch.distributed.launch`, which is designed to launch multiple processes
    on both single-node and multi-node systems. An example of how to use it for distributed
    training is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Single-node `distributed` training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Multi-node `distributed` training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding, `MASTER_IP` is a string containing the IP address of the master
    node, for example, `192.168.1.1`.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check out the official tutorial on distributed training with PyTorch
    1.3: [https://pytorch.org/docs/master/distributed.html](https://pytorch.org/docs/master/distributed.html), [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html), [https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html) and [https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: Better research reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have heard complaints about how hard it is to reproduce the experimental
    results in deep learning papers. Apparently, we need to trust the reviewers, even
    though they have to review thousands of papers for each top conference every year.
    However, does it mean we cannot trust our abilities to follow the exact steps
    written on paper? Now PyTorch has announced torch.hub to help with the research
    reproducible problem. Authors can now publish their trained models with Torch
    Hub and users can directly download and use them in their code.
  prefs: []
  type: TYPE_NORMAL
- en: Here's an example of how to publish and use pre-trained models with Torch Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To publish your model, you need to create a `hubconf.py` file in a GitHub repository
    and define the entrypoint (for example, named `cnn`) like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, `dependencies` is a list of the dependencies required
    to run your model, and `Net()` is the class that defines your model. Note that
    the published models have to live under certain a branch/tag, for example, the master
    branch. You can also upload your `pretrained` model files to other sites and download
    them in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Say we have published our model to [https://github.com/johnhany/torchhub](https://github.com/johnhany/torchhub).
    To use a published model, you only need to call `torch.hub`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The full code for the Torch Hub example can be found under the `torchhub` directory
    located in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other than what we've mentioned previously, there are other things we can benefit
    from in the new version of PyTorch. By the end of this section, we will also talk
    about how to migrate your old PyTorch code to version 1.x.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many wonderful tools and projects built on the PyTorch platform. They
    explore the full potential of PyTorch in many domains. For example, **AllenNLP**
    ([https://allennlp.org](https://allennlp.org)) is an open source natural language
    processing library. Check out their demo site and see what state-of-the-art NLP
    algorithms are capable of: [https://demo.allennlp.org](https://demo.allennlp.org). **Fastai**
    ([https://docs.fast.ai](https://docs.fast.ai)) provides a simplified procedure
    of model training with PyTorch, and also offers practical deep learning courses
    at [https://course.fast.ai](https://course.fast.ai). **Translate** ([https://github.com/pytorch/translate](https://github.com/pytorch/translate))
    is a PyTorch library that's dedicated to natural language translation.
  prefs: []
  type: TYPE_NORMAL
- en: Check out this site to find out more about the PyTorch ecosystem: [https://pytorch.org/ecosystem](https://pytorch.org/ecosystem).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is fully supported by popular cloud platforms such as Amazon AWS, Google
    Cloud Platform, and Microsoft Azure. If you don't currently own a CUDA-enabled
    GPU (which we will discuss in the next section), feel free to rent a GPU server
    provided by the platforms previously mentioned. Here's an official tutorial on
    distributed training with PyTorch on Amazon AWS: [https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: Migrating your previous code to 1.x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite all the breaking changes in PyTorch 1.x, most of the APIs or coding
    conventions have not changed too much. Therefore, if you are already comfortable
    with PyTorch 0.4, your code *should* mostly work as is. API changes from v0.4
    to v1.3 are listed in Breaking Changes at [https://github.com/pytorch/pytorch/releases](https://github.com/pytorch/pytorch/releases).
  prefs: []
  type: TYPE_NORMAL
- en: The most common issue you might run into when migrating older code to PyTorch
    1.x would stem from indexing a 0-dimension tensor. You need to use `loss.item()`
    when, for example, printing the loss value, instead of `loss[0]`. The full code
    for this example is contained in the `ind-0-dim.py` file under the `pytorch_test` directory
    located in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If your code is targeted at older versions than 0.4, you should perhaps check
    out the migration guide for PyTorch 0.4; first: [https://pytorch.org/blog/pytorch-0_4_0-migration-guide](https://pytorch.org/blog/pytorch-0_4_0-migration-guide).
    There is no official migration guide for versions later than 0.4, however, you'll
    certainly find plenty of information on the internet with a simple web search.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA – GPU acceleration for fast training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NVIDIA CUDA Toolkit ([https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit))
    is a fully optimized parallel computing platform for general-purpose computing
    on graphics processing units (GPGPU). It allows us to perform scientific computing
    on NVIDIA graphic cards, including linear algebra, image and video processing,
    deep learning, and graph analytics. It is used by a lot of commercial and open
    source software to enable GPU-accelerated computation across different domains.
    If we look back at the development of deep learning, we should realize that the
    latest breakthroughs in GANs would have been almost impossible without the help
    of CUDA and powerful GPUs. Therefore, we highly recommend you try out the experiments
    in this book on a CUDA-compatible GPU; otherwise, the training time of neural
    networks could be painfully long on CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will walk you through the installation of CUDA on Windows
    10 and Ubuntu 18.04\. Before we start installing CUDA, you should make sure that
    your video card supports CUDA and you have installed the latest driver for your
    video card. To check whether your GPU is compatible with CUDA (or the exact CUDA
    version you want to install), you should first make sure you have an NVIDIA video
    card on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows, you can use third-party tools such as GPU-Z ([https://www.techpowerup.com/gpuz](https://www.techpowerup.com/gpuz))
    or GPU Caps Viewer ([http://www.ozone3d.net/gpu_caps_viewer](http://www.ozone3d.net/gpu_caps_viewer))
    to examine the specifications of your video card. You can always check this web
    pageto see if your video card is on the list: [https://www.geforce.com/hardware/technology/cuda/supported-gpus](https://www.geforce.com/hardware/technology/cuda/supported-gpus).
    The most straightforward and practical way, however, to check whether the latest
    CUDA perfectly runs on your system, is to finish the installation and evaluation
    steps in the following subsections without any issues.'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, the latest version of CUDA is 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: Installing NVIDIA driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Windows 10, visit [https://www.nvidia.com/Download/index.aspx](https://www.nvidia.com/Download/index.aspx)
    to download the driver by choosing the product and operating system based on your
    video card and system. Installation on Windows should be very straightforward
    since it has a **graphical user interface** (**GUI**). You can keep the default
    settings during installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, you can always download CUDA from the *How to install CUDA
    10.1 on Ubuntu 18.04* ([https://gist.github.com/eddex/707f9cbadfaec9d419a5dfbcc2042611](https://gist.github.com/eddex/707f9cbadfaec9d419a5dfbcc2042611)).
    However, we recommend you install the NVIDIA driver in the following way so that
    your graphics driver can be updated in the same way as other software. First,
    open up a Terminal and add the proper repository to your package management source
    list by typing in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can check your video card model and the recommended driver version
    by implementing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, install the recommended driver with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you already have CUDA installed and plan on installing a different version
    of CUDA, we recommend you uninstall both the NVIDIA driver and CUDA toolkit, reboot
    your system, and install the latest driver before re-installing CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: When the installation is finished, reboot your system.
  prefs: []
  type: TYPE_NORMAL
- en: Installing CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's the full list of CUDA toolkits: [https://developer.nvidia.com/cuda-toolkit-archive](https://developer.nvidia.com/cuda-toolkit-archive).
    Click CUDA Toolkit 10.1 to navigate to the download page for CUDA 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: On **Windows 10**, select Windows | x86_64 | 10 | exe(local), and download the
    base installer. The installer file is about 2.1 GB. Again, we won't go into details
    regarding the installation process since it's GUI-based. Just keep the default
    settings during installation.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you also install the official CUDA samples during installation. They
    are essential for us to evaluate the successful installation of CUDA later on
    and very useful for learning CUDA programming (if you are interested). Also, if
    you plan on installing Microsoft Visual Studio on Windows as well, make sure you
    install it before CUDA, because CUDA will then automatically detect Visual Studio
    and install the corresponding integration tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, select Linux | x86_64 | Ubuntu | 18.04 | runfile(local), and
    download the Base Installer. The installer file is about 2.0 GB. When the download
    is finished, which could take a little while (say it''s downloaded under the `~/Downloads`
    directory), open up a Terminal and type in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: During the installation, accept all default settings, except that we don't need
    to install the NVIDIA driver when prompted, since we have already installed a
    newer version previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of the installation of CUDA, there might be several warning messages,
    such as Missing recommended library: `libGLU.so`. Simply run `apt-get install
    libglu1-mesa libxi-dev libxmu-dev libglu1-mesa-dev` to install those optional
    libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, add CUDA directories to your `~/.bashrc` file so that other software
    can find your CUDA library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can open the file with `gedit ~/.bashrc` and manually add
    these two lines at the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Run `sudo ldconfig` to refresh the changes we make to the `.bashrc` file. Make
    sure you close and re-open the Terminal before running any other bash command.
  prefs: []
  type: TYPE_NORMAL
- en: For other platforms, please visit [https://docs.nvidia.com/cuda/archive/10.0](https://docs.nvidia.com/cuda/archive/10.0)
    and follow the instructions there to install CUDA 10.0.
  prefs: []
  type: TYPE_NORMAL
- en: Installing cuDNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to enable the fast computation capability provided by CUDA for neural
    networks, we need to install cuDNN. The **NVIDIA CUDA Deep Neural Network library**
    (**cuDNN**) is a GPU-accelerated library for deep neural networks. It's basically
    a low-level driver that runs on GPUs that provides multiple fully optimized forward
    and backward computation for common neural network operations. It has been used
    by many deep learning platforms, including PyTorch, so that the platform developers
    don't have to worry about implementing the basic neural network components and
    can focus on delivering better APIs for us to use.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to download cuDNN from this site: [https://developer.nvidia.com/rdp/cudnn-download](https://developer.nvidia.com/rdp/cudnn-download).
    Previous versions are available at [https://developer.nvidia.com/rdp/cudnn-archive](https://developer.nvidia.com/rdp/cudnn-archive).
    Look for the cuDNN release that fits your CUDA version and your OS. Normally,
    any version of cuDNN that's bigger than **7.0** would be acceptable for PyTorch.
    You can always grab the latest version, of course. Here, we will download **cuDNN
    v7.5.0 for CUDA 10.1** from the first of the preceding links. Please note that
    you will need to register an NVIDIA Developer account with a valid email address to
    become a member of the NVIDIA Developer Program; then all the cuDNN release files
    are free to download.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows 10, click Download cuDNN v7.5.0 (Feb 21, 2019); for CUDA 10.0, click
    **cuDNN Library for Windows 10**. This will download a `cudnn-10.0-windows10-x64-v7.5.0.56.zip`
    file that is about 224 MB. Unzip the downloaded file and copy the unzipped files
    to CUDA directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[UNZIPPED_DIR]\cuda\bin\cudnn64_7.dll -> C:\Program Files\NVIDIA GPU Computing
    Toolkit\CUDA\v10.0\bin\cudnn64_7.dll`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[UNZIPPED_DIR]\cuda\include\cudnn.h -> C:\Program Files\NVIDIA GPU Computing
    Toolkit\CUDA\v10.0\include\cudnn.h`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[UNZIPPED_DIR]\cuda\lib\x64\cudnn.lib -> C:\Program Files\NVIDIA GPU Computing
    Toolkit\CUDA\v10.0\lib\x64\cudnn.lib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, click**Download cuDNN v7.5.0 (Feb 21, 2019)**; for CUDA 10.0,
    click **cuDNN Library for Linux**. A `cudnn-10.0-linux-x64-v7.5.0.56.tgz` file
    will be downloaded. The file size is about 433 MB. When the download is finished,
    let''s open up a Terminal and run the following scripts (we assume that your file
    has been downloaded to the `~/Downloads` directory):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unzip the downloaded file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the files to the system directory and grant the read permissions for all
    users (you may need to `cd` to the extracted folder first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: On other platforms, please follow the instructions at [https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html)
    to install cuDNN.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating your CUDA installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see if CUDA works properly on your machine. Here, we assume that you have
    also installed the official CUDA samples.
  prefs: []
  type: TYPE_NORMAL
- en: Here, Microsoft Visual Studio is needed to build and test the CUDA sample on
    Windows. We are using Visual Studio Community 2017 in this example.
  prefs: []
  type: TYPE_NORMAL
- en: On Windows 10, navigate to the CUDA samples directory (for example, `C:\ProgramData\NVIDIA
    Corporation\CUDA Samples\v10.0`). Open the `1_Utilities\deviceQuery\deviceQuery_vs2017.sln`
    solution file with Visual Studio 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Visual Studio, switch the **Solution Configurations** to **Release**. Then,
    click Build | Build deviceQuery to build the sample code. When the build is finished,
    navigate to `C:\ProgramData\NVIDIA Corporation\CUDA Samples\v10.0\bin\win64\Release`
    and open PowerShell under this directory. Type in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that CUDA 10.0 has been successfully installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, navigate to the CUDA samples directory (for example, `~/NVIDIA_CUDA-10.0_Samples`).
    Open the Terminal and type in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This should compile the `deviceQuery` program without any issue. Then, navigate
    to the build directory and run the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output should look similar to that from Windows 10.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can move on to installing PyTorch 1.0!
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch on Windows and Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To install and use PyTorch, we need to properly set up the Python development
    environment first. So, in this section, we will first talk about how to set up
    the Python environment, then how to install PyTorch either with official release
    binaries or by building from source. At the end of this section, we will introduce
    you to a lightweight, yet extremely powerful code editor tool, Microsoft VS Code,
    and show you how to configure it for PyTorch programming.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Python environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will walk you through how to set up the Python
    environment and how to install or build PyTorch on Windows 10 and Ubuntu 18.04\.
    We assume that, of course, you have successfully installed CUDA on your system
    (for example, CUDA 10.1).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Windows 10, visit [https://www.python.org/downloads/windows](https://www.python.org/downloads/windows)
    to download the Windows x86-64 executable installer. You may install any version
    you want. We'll install the latest version (at the time of writing), 3.7.5, as
    an example. Actually, 3.8.0 is the very latest version, but it's better to stay
    on the 3.7.x track. The downloaded `python-3.7.5-amd64.exe` file is about 25 MB.
    Keep the default settings during installation, except that we could change the
    installation path to an easier-to-find location, that is, `C:\Python37`.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you check the box for Add Python 3.7 to PATH during installation,
    otherwise, you'll have to add the environment variables manually: `C:\Python37\`
    and `C:\Python37\Scripts\`. The detailed process of adding environment variables
    on Windows 10 is described later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: On Ubuntu 18.04, Python 2.7.15 and 3.7.1 have already been shipped with the
    system. So, you don't have to do anything for now.
  prefs: []
  type: TYPE_NORMAL
- en: On Ubuntu, if you plan on using the default version of Python provided by the
    system, think twice before you modify it (including upgrading, downgrading, or
    uninstalling) because it will affect many other things in your system. And always
    make sure you are using the right version of Python (that is, Python 2 vs 3).
    Sometimes, installing and using packages across Python 2 and Python 3 can be a
    little bit messy.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Anaconda Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On Windows 10, download the installer from [https://www.anaconda.com/distribution/#windows](https://www.anaconda.com/distribution/#windows). We'll
    download and install Python 3.7 version as an example. This will download an `Anaconda3-2018.12-Windows-x86_64.exe`
    file that is about 614 MB in size. Open this file to install Anaconda and keep
    the default settings unchanged. Note that we don't have to check the box for **Register
    Anaconda as the system Python 3.7** because we will create a new Python environment
    and add the corresponding environment variables manually later on.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the installation, you will be asked whether you want to install
    the Microsoft VS Code. We recommend you install one for Python development.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, download the installer from [https://www.anaconda.com/distribution/#linux](https://www.anaconda.com/distribution/#linux).
    Here, we download and install Python version 3.7, for example. An `Anaconda3-2018.12-Linux-x86_64.sh`
    file will be downloaded. The file size is around 684 MB. Run this file to install
    it (assume that it''s located at `~/Downloads`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: During the installation, accept all default settings. By the end of the installation,
    you will be prompted as to whether to install Microsoft VS Code on your system.
    You can accept it if you haven't installed it yet.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites before we move on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some important, or even necessary, Python tools and libraries we
    need to install before moving on to the next section, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pip **<q>(</q>required<q>):</q> It is required to manage your Python packages.
    On Ubuntu, run `sudo apt-get install python-pip` for Python 2 or `sudo apt-get
    install python3-pip` for Python 3\. On Windows, it''s usually installed along
    with Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NumPy** <q>(</q>required<q>):</q> A scientific computing library for tensor
    representation, manipulation, and calculation, along with linear algebra, the
    Fourier transform, and random number capabilities. It is required to install PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SciPy** <q>(</q>optional<q>):</q> A collection of numerical algorithms including
    signal processing, optimization, and statistics. We will use it mainly for its
    statistics capability, for example, initializing parameters based on a certain
    random distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenCV** <q>(</q>optional<q>):</q> A cross-platform open source computer
    vision library for efficient and real-time image processing and pattern recognition.
    We will use it to preprocess or visualize the data, parameters, and feature maps
    in neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** <q>(</q>optional<q>):</q> A publication-quality plotting library.
    We will use it to illustrate loss curves or other plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On Windows 10, you can visit [https://www.lfd.uci.edu/~gohlke/pythonlibs](https://www.lfd.uci.edu/~gohlke/pythonlibs)
    to download the `.whl` files for these libraries and install them with `pip install
    [FILENAME]` (for Python 2) or `pip3 install [FILENAME]` (for Python 3).
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, you can install these packages with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The installation may fail due to user permission issues. If you are an administrator
    user on Windows, make sure you open Command Prompt as administrator. If you have
    root access on Ubuntu, simply add `sudo` before the installation command. If you
    don't have the administrator or root access at all, install the packages with `pip3
    install --user`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can either install PyTorch using the official release binaries or by building
    it from source. You can install PyTorch directly on your system, or use a package
    manager (such as Anaconda) to avoid potential conflicts with other tools. At the
    time of writing this book, the latest version of PyTorch is v1.3.1\. Since we
    want to take advantage of the cutting-edge functionalities provided by PyTorch,
    we will install and use PyTorch 1.3 in all the remaining chapters of this book.
    You can, of course, choose any other version you wish, or install an even newer
    version than the one we use in this book. Simply change the version number to
    your own version when you follow the following instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We highly recommend that you install PyTorch with Anaconda if you are using
    Ubuntu because it won't affect the default Python environment that's shipped with
    the system. If you are on Windows, you can basically delete the Python installation
    and re-install any other version you want, if anything goes seriously wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Installing official binaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not too long ago, installing PyTorch was a major endeavor. However, the good
    folks at PyTorch.org have made it very easy for you to install PyTorch on your
    system. Go to [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    to get started. There, you will find a very simple point and click method to get
    the proper installation information.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should start with the build that you want to install, and then select the
    operating system. Next, you should determine the way you want to install PyTorch,
    if it''s via Conda, pip, and so on. Next, select the version of Python you are
    going to target, and, finally, pick which version of CUDA you are using or whether
    you are going to go without a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecc98148-db55-40dd-803f-d1d83c8ffc76.png)'
  prefs: []
  type: TYPE_IMG
- en: The last step is to select and copy the command from the box at the bottom of
    the grid. Paste this into your terminal or Command Prompt and run it. In a minute
    or two, you'll be all set.
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to consider making the created Python environment the default
    Python for your system. To do that, all you need to do is add these environment
    variables: `C:\Users\John\Anaconda3\envs\torch` and `C:\Users\John\Anaconda3\envs\torch\Scripts`.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to add **environment variables** on Windows 10: (1) Right-click on the
    <q>Start</q> button and click <q>System</q>. (2) Click <q>System information</q>
    on the right in the <q>Settings</q> window, which will open up the <q>System Control
    Panel</q> (You may not need this step if you are on a rather old version of Windows
    10). (3) Click <q>Advanced system settings</q> on the left, which will open the
    <q>System Properties</q> window. (4) Click <q>the Environment Variables</q> button,
    which will open the <q>Environment Variables</q> window. (5) Double-click the
    line for <q>Path</q> variable in <q>User variables</q>. Now you can add or edit
    the paths pointing to Anaconda or Python directories. Each time you edit environment
    variables, make sure that you close the <q>Environment Variables</q> window and
    run this script in PowerShell: `$env:Path = [System.Environment]::GetEnvironmentVariable("Path","Machine")
    + ";" + [System.Environment]::GetEnvironmentVariable("Path","User")`.'
  prefs: []
  type: TYPE_NORMAL
- en: That's it! PyTorch has now been installed on your machine and you can follow
    the instructions in the *Evaluating your PyTorch* installation section to see
    if it works properly.
  prefs: []
  type: TYPE_NORMAL
- en: Building Pytorch from source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will only talk about building PyTorch from source using Anaconda Python
    on Ubuntu 18.04, because the build process has a very high chance of failing on
    Windows. First, let's create a new Python environment called `torch-nt` for building
    and installing the nightly version with `conda create -n torch-nt python=3.7` and
    activate it with `conda activate torch-nt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, install the dependencies needed for building PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, download the source code of PyTorch with Git:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, `CMAKE_PREFIX_PATH` points to the root directory of your Python environment.
    All your environments created by Anaconda are located under the `~/anaconda3/envs`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Wait a moment for it to finish. When it's done, run `python` in the Terminal,
    type in `import torch`, and press Enter. If no error pops up, it means that PyTorch
    has been successfully built and installed.
  prefs: []
  type: TYPE_NORMAL
- en: Do you remember, do not run `import torch` under the same directory you build
    PyTorch from, because Python will try to pick up the Torch library from the source
    files, instead of the installed package.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating your PyTorch installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From now on, we will use the Anaconda Python environment called **torch** we
    previously created as the default Python environment in this book. We will also
    omit the (torch) indicator in front of the scripts. Also, by default, all of the
    code in this book is written for Python 3 (specifically, Python 3.7). If you are
    looking for Python 2 implementations, you might want to look at 3to2 ([https://pypi.org/project/3to2](https://pypi.org/project/3to2)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a short snippet for matrix multiplication using PyTorch. Create
    a Python source code file named `pytorch_test.py` and copy the following lines
    into this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the Terminal and run this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The last line is totally random, so don't worry if you get a different result.
    The code is also available under the `pytorch_test` directory located in the code
    repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You can always use the jit or torchhub examples in previous sections to evaluate
    the installation of PyTorch. Also, feel free to check out the official examples
    at [https://github.com/pytorch/examples](https://github.com/pytorch/examples).
  prefs: []
  type: TYPE_NORMAL
- en: Remember the simple GAN we implemented with NumPy in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*? Now that you have your PyTorch up and ready,
    you can think about how you would implement it with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: setting up VS Code for Python coding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**VS Code** is a lightweight, open source code editor developed by Microsoft.
    It has built-in syntax highlighting, autocompleting, debugging, Git management,
    and it has more than 10,000 extensions developed by the community. It supports Windows,
    macOS, and Linux, and it is the most popular development tool among software developers,
    according to a StackOverflow survey: [https://insights.stackoverflow.com/survey/2018/#technology-most-popular-development-environments](https://insights.stackoverflow.com/survey/2018/#technology-most-popular-development-environments).
    If you mainly work on your own machine for learning GANs with this book, we highly
    recommend you use VS Code for PyTorch development.'
  prefs: []
  type: TYPE_NORMAL
- en: If you often work remotely, which means that you have to write Python code locally
    and run that code on a remote server, you may consider using PyCharm Professional
    Edition ([https://www.jetbrains.com/pycharm](https://www.jetbrains.com/pycharm))
    for this purpose. It has a more mature remote development functionality than free
    VS Code extensions have to offer.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring VS Code for Python development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, you only need the Python extension *(*`ms-python.python`*)* for
    Python programming in VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows 10, click File | Preferences | Settings, click the {} button (Open
    Settings (JSON)) on the upper right, and type in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu 18.04, click File |Preferences | Settings, click the {} button (Open
    Settings (JSON)) on the upper right, and type in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, VS Code will automatically recognize it as an Anaconda Python environment
    and you are ready to write Python code with it!
  prefs: []
  type: TYPE_NORMAL
- en: Recommended VS Code extensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are some VS Code extensions that I personally find useful in Python development.
    I'm sure they will make your work a lot easier as well. Many thanks to their creators!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bracket Pair Colorizer** ([coenraads.bracket-pair-colorizer](https://marketplace.visualstudio.com/items?itemName=CoenraadS.bracket-pair-colorizer)):
    This matches each pair of brackets with different colors, which allows you to
    easily recognize them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code Runner** ([formulahendry.code-runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner)):
    This allows you to run Python (and many other languages'') code with a click of
    the button. However, we don''t recommend you use it to run the training snippets
    of neural networks because the logging messages can be rather long and some messages
    might go missing in VS Code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitLens - Git supercharged** ([eamodio.gitlens](https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens)):
    This is a powerful tool if you rely on Git to manage your source code. For example,
    it shows Git history on each line you''re currently looking at in the editor,
    shows all the local and remote changes in a tree structure, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**indent-switcher** ([ephoton.indent-switcher](https://marketplace.visualstudio.com/items?itemName=ephoton.indent-switcher)):
    Everyone''s programming habits are different. Some like two spaces as indentation,
    and some like four spaces. You can switch between two-space and four-space indentation
    with this extension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial Diff** ([ryu1kn.partial-diff](https://marketplace.visualstudio.com/items?itemName=ryu1kn.partial-diff)):
    This allows you to compare two code snippets across different files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Path Intellisense** ([christian-kohler.path-intellisense](https://marketplace.visualstudio.com/items?itemName=christian-kohler.path-intellisense)):
    This extension autocompletes filenames in your code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search - Open All Results** ([fabiospampinato.vscode-search-open-all-results](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-search-open-all-results)):
    This supports searching keywords across multiple source files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Settings Sync** ([shan.code-settings-sync](https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync)):
    This saves the installed extensions and user settings to a Gist file and recovers
    from that file. It can be very useful if you work on multiple machines and systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Udacity India. (2018, Mar 8). *Why Python is the most popular language used
    for Machine Learning*. Retrieved from [https://medium.com/@UdacityINDIA/why-use-python-for-machine-learning-e4b0b4457a77](https://medium.com/@UdacityINDIA/why-use-python-for-machine-learning-e4b0b4457a77).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'S Bhutani. (2018, Oct 7). *PyTorch 1.0 - A brief summary of the PTDC ’18: PyTorch
    1.0 Preview and Promise*. Retrieved from [https://hackernoon.com/pytorch-1-0-468332ba5163](https://hackernoon.com/pytorch-1-0-468332ba5163).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C Perone. (2018, Oct 2). *PyTorch 1.0 tracing JIT and LibTorch C++ API to integrate
    PyTorch into NodeJS*. Retrieved from [http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs](http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'T Wolf. (2018, Oct 15). *Training Neural Nets on Larger Batches: Practical
    Tips for 1-GPU, Multi-GPU and Distributed setups*. Retrieved from [https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wow! That was a lot of work and information. Take a minute, grab a cup of coffee
    or tea, and come back. I'll wait.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at all the things we've done.
  prefs: []
  type: TYPE_NORMAL
- en: We have made sure that we are up to date with our Python installation, installed
    CUDA (assuming we have an NVIDIA GPU graphics card) and installed PyTorch. If
    you are anything like me, you are *chomping at the bit* to get going and do some
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need to get some more basics defined before we can really be productive,
    which is our goal. In the next chapter, we will go through some of the basics.
  prefs: []
  type: TYPE_NORMAL
