["```py\npip3 install tensorflow\n```", "```py\nimport tensorflow as tf\nimport random\n```", "```py\n[ [ [1], [0], [1], [1], …, [0], [1] ]\n[ [0], [1], [0], [1], …, [0], [1] ]\n[ [1], [1], [1], [0], …, [0], [0] ]\n[ [1], [0], [0], [0], …, [1], [1] ] ]\n\n```", "```py\nnum_examples = 100000\nnum_classes = 50\n\ndef input_values():\n    multiple_values = [map(int, '{0:050b}'.format(i)) for i in range(2**20)]\n    random.shuffle(multiple_values)\n    final_values = []\n    for value in multiple_values[:num_examples]:\n        temp = []\n        for number in value:\n            temp.append([number])\n        final_values.append(temp)\n    return final_values\n```", "```py\ndef output_values(inputs):\n    final_values = []\n    for value in inputs:\n        output_values = [0 for _ in range(num_classes)]\n        count = 0\n        for i in value:\n            count += i[0]\n        if count < num_classes:\n            output_values[count] = 1\n        final_values.append(output_values)\n    return final_values\n```", "```py\ndef generate_data():\n    inputs = input_values()\n    return inputs, output_values(inputs)\n```", "```py\nX = tf.placeholder(tf.float32, shape=[None, num_classes, 1])\nY = tf.placeholder(tf.float32, shape=[None, num_classes])\nnum_hidden_units = 24\nweights = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes]))\nbiases = tf.Variable(tf.truncated_normal([num_classes]))\n```", "```py\n     prediction = tf.matmul(last_output, weights) + biases\n```", "```py\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,    \n    logits=prediction)\n    total_loss = tf.reduce_mean(loss)\n```", "```py\nlearning_rate = 0.001\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=total_loss)\n```", "```py\nX = tf.placeholder(tf.float32, shape=[None, num_classes, 1])\nY = tf.placeholder(tf.float32, shape=[None, num_classes])\n\nnum_hidden_units = 24\n\nweights = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes]))\nbiases = tf.Variable(tf.truncated_normal([num_classes]))\n\nrnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=num_hidden_units, activation=tf.nn.relu)\noutputs1, state = tf.nn.dynamic_rnn(rnn_cell, inputs=X, dtype=tf.float32)\noutputs = tf.transpose(outputs1, [1, 0, 2])\n\nlast_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\nprediction = tf.matmul(last_output, weights) + biases\n\nloss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=prediction)\ntotal_loss = tf.reduce_mean(loss)\n\nlearning_rate = 0.001\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=total_loss)\n```", "```py\nbatch_size = 1000\nnumber_of_batches = int(num_examples/batch_size)\nepoch = 100\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    X_train, y_train = generate_data()\n    for epoch in range(epoch):\n        iter = 0\n        for _ in range(number_of_batches):\n            training_x = X_train[iter:iter+batch_size]\n            training_y = y_train[iter:iter+batch_size]\n            iter += batch_size\n            _, current_total_loss = sess.run([optimizer, total_loss], \n            feed_dict={X: training_x, Y: training_y})\n            print(\"Epoch:\", epoch, \"Iteration:\", iter, \"Loss\", current_total_loss)\n            print(\"__________________\")\n```", "```py\nprediction_result = sess.run(prediction, {X: test_example})\nlargest_number_index = prediction_result[0].argsort()[-1:][::-1]\n\nprint(\"Predicted sum: \", largest_number_index, \"Actual sum:\", 30)\nprint(\"The predicted sequence parity is \", largest_number_index % 2, \" and it should be: \", 0)\n\n```", "```py\n[[[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],\n[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],\n[1],[0],[0],[1],[1],[0],[1],[1],[1],[0]]]\n```"]