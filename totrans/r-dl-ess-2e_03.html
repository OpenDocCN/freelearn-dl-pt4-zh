<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning Fundamentals</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we created some machine learning models using neural network packages in R. This chapter will look at some of the fundamentals of neural networks and deep learning by creating a neural network using basic mathematical and matrix operations. This application sample will be useful for explaining some key parameters in deep learning algorithms and some of the optimizations that allow them to train on large datasets. We will also demonstrate how to evaluate different hyper-parameters for models to find the best set. In the previous chapter, we briefly looked at the problem of overfitting; this chapter goes into that topic in more depth and looks at how you can overcome this problem. It includes an example use case using dropout, the most common regularization technique in deep learning.</p>
<p class="mce-root">This chapter covers the following topics:</p>
<ul>
<li class="mce-root">Building neural networks from scratch in R</li>
<li>Common parameters in deep learning</li>
<li>Some key components in deep learning algorithms</li>
<li class="mce-root">Using regularization to overcome overfitting</li>
<li class="mce-root">Use case—improving out-of-sample model performance using dropout</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building neural networks from scratch in R</h1>
                </header>
            
            <article>
                
<p class="mce-root">Although we have already used some neural network algorithms, it's time to dig a bit deeper into how they work. This section demonstrates how to code a neural network from scratch. It might surprise you to see that the core code for a neural network can be written in fewer than 80 lines! The code for this chapter does just that using an interactive web application written in R. It should give you more of an intuitive understanding of neural networks. First we will look at the web application, then we will delve more deeply into the code for the neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network web application</h1>
                </header>
            
            <article>
                
<p class="mce-root">First, we will look at an R Shiny web application. I encourage you to run the application and follow the examples as it will really help you to get a better understanding of how neural networks work. In order to run it, you will have to open the <kbd>Chapter3</kbd> project in RStudio.</p>
<div class="packt_infobox"><strong>What is R Shiny</strong>?<br/>
<span>R Shiny is an R package from the RStudio company that allows you to create interactive web apps using only R code. You can build dashboards and visualizations, and use the full functionality of R. You can extend R Shiny apps with CSS, widgets, and JavaScript. It is also possible to host your applications online. It is a great tool with which to showcase data science applications and I encourage you to look into it if you are not already familiar with it. For more information, see </span><a href="https://shiny.rstudio.com/">https://shiny.rstudio.com/</a><span>, and, for examples of what is possible with R Shiny, see </span><a href="https://shiny.rstudio.com/gallery/">https://shiny.rstudio.com/gallery/</a><span>.</span></div>
<ol>
<li class="mce-root">Open the <kbd>server.R</kbd> file in RStudio and click on the <span class="packt_screen">Run App</span> button:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-577 image-border" src="assets/fb1268b3-83d8-4a8a-816f-88ead0e115a4.png" style="width:46.33em;height:33.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 3.1: How to run an R Shiny application</span></div>
<ol start="2">
<li class="mce-root">When you click on the <span><span class="packt_screen">Run App</span> button</span>, you should get a pop-up screen for your web application. The following is a screenshot of the web application after it starts up:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-578 image-border" src="assets/bd66fb12-3ed7-40ef-8407-b5b2a8d444d9.png" style="width:79.67em;height:59.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 3.2: <span>R Shiny application on startup</span></div>
<p class="mce-root">This web application can be used in the pop-up window or opened in a browser. On the left, there is a set of input choices; these are parameters for the neural network. These are known as hyper-parameters, in order to distinguish between the <em>parameters</em> that the model is trying to optimize. From top to bottom, these <span>hyper-parameters are</span>:</p>
<ul>
<li class="mce-root"><strong>Select data</strong>: There are four different datasets that you can use as training data.</li>
<li class="mce-root"><strong>Nodes in hidden layer</strong>: The number of nodes in the hidden layer. The neural network has only one hidden layer.</li>
<li class="mce-root"><strong># Epochs</strong>: The number of times that the algorithm iterates over the data during model-building.</li>
<li class="mce-root"><strong>Learning rate</strong>: The learning rate applied during backpropagation. The learning rate affects how much the algorithm changes the weights during every epoch.</li>
<li class="mce-root"><strong>Activation function</strong>: The activation function applied to the output of each node.</li>
<li>The <span class="packt_screen">Run NN Model</span> button trains a model with the selection of input. The <span class="packt_screen">Reset</span> button restores input choices to the default values.</li>
</ul>
<p class="mce-root">There are four different datasets to choose from, each with a different data distribution; you can select them from the drop-down box. They have descriptive names; for example, the data that is plotted in <em>Figure 3.2</em> is called <kbd>bulls_eye</kbd>. These datasets are from another R package that is used to test clustering algorithms. The data has two classes of equal size and is composed of various geometric shapes. <span>You can explore these datasets using t</span>he web application. The only change we make to the data is to randomly switches labels for 5% of the data. When you run the application, you will notice that there are some red points in the inner circle and some blue points in the outer circle. This is done so that our models should only achieve a maximum accuracy of 0.95 (95%). This gives us confidence that the model is working correctly. If the accuracy is higher than this, the model could be overfitting because the function it has learned is too complex. We will discuss overfitting again in the next section.</p>
<p class="mce-root">One of the first steps in machine learning should be to establish a benchmark score, this is useful for gauging your progress. A benchmark score could be a rule of thumb, or a simple machine learning algorithm; it should not be something that you spend a lot of time working on. In this application, we use a basic <span>logistic regression model </span>as a benchmark. We can see that in the previous screenshot, the accuracy for the logistic regression model is only 0.6075, or 60.75% accuracy. This is not much over 50%, but recall that logistic regression can only fit a straight line and this data cannot be separated using a straight line. A neural network should improve on the <span>logistic regression benchmark, so if we get an accuracy of less than 0.6075 on this dataset, something is wrong with our model and we should review it.</span></p>
<p class="mce-root">So let's begin! Click on the <span class="packt_screen">Run NN Model</span> button, which runs a neural network model on the data using the input choices. After a few seconds, the application should change to resemble the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-752 image-border" src="assets/a81d0b71-cb02-4770-8b2c-f98b0a58ef3e.png" style="width:44.50em;height:42.83em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 3.3: Neural network model execution with default settings</div>
<p>The application takes a few seconds and then it creates a graph of the cost function over the <strong># Epochs</strong> and outputs cost function values as the algorithm iterates over the data. The text output also includes the final accuracy for the model in the text at the bottom right of the screen. In the diagnostic messages in the bottom right, we can see <span>that the cost decreases during training and </span>we achieved a final accuracy rate of 0.825. The cost is what the model is trying to minimize <span>–</span> a lower cost means better accuracy. It took some time for the cost to start decreasing as the model struggled initially to get the right weights.</p>
<div class="mce-root packt_infobox">In deep learning models, <span>weights and biases should be not initialized with random values. If </span>random values are used, this can lead to problems with training, such as vanishing or exploding gradients. This is where the weights get too small or too large and the model fails to train successfully. Also, if the weights are not correctly initialized, the model will take longer to train, as we saw earlier. Two of the most popular techniques to initialize weights to avoid these problems are the<span> </span>Xavier initialization and the He initialization<span> (named after their inventors).</span></div>
<p class="mce-root">We can see in <em>Figure 3.3</em> that the cost has not plateaued, the last few values show it is still decreasing. This indicates that the model can be improved if we train it for longer. Change <strong># Epochs</strong> to <strong>7000</strong> and click the <strong>Run NN Model</strong> button again; the screen will change to resemble the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-753 image-border" src="assets/3d8c44aa-f4e8-4abc-b034-ad19f339540c.png" style="width:36.58em;height:35.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.4: Neural network model execution with more epochs</span></div>
<p class="mce-root">Now we get an accuracy of 0.95, which is the maximum possible accuracy rate. We notice that the cost values have plateaued <span>(that is, are not decreasing further) to </span>around 0.21. This indicates that training the model for longer (<span>that is,</span> more epochs) will probably not improve the results, regardless of the current accuracy number. If the model is under training and the <span>cost values have plateaued,</span> we would need to consider changing the architecture of the model or getting more data to improve our accuracy. Let's look at changing the number of nodes in our model. Click the <span class="packt_screen">Reset</span> button to change the input values to their defaults, then change the number of nodes to 7, and click the <strong>Run NN Model</strong> button. Now the screen will change to the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8b24308-5c5f-4579-84a9-d4464c3bfaf3.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.5: Neural network model execution with more nodes</span></div>
<p class="mce-root">Our accuracy here is 0.68, but compare this to the earlier examples, when we used the same input and only three nodes. We actually get worse performance with more nodes! This is because our data has a relatively simple pattern, and a model with seven nodes might be too complex and will take longer to train. Adding more nodes to a layer will increase training time but does not always improve performance.</p>
<p>Let's look at the <strong>Learning rate</strong>. Click the <strong>Reset</strong> button to change the input values to their defaults, then change the <strong>Learning rate</strong> to around <strong>5</strong>, and click the <strong>Run NN Model</strong> button again to replicate the following screen:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-754 image-border" src="assets/763dd827-fdfa-48c6-9dcc-42e3465a985a.png" style="width:106.67em;height:85.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.6: Neural network model execution with larger learning rate</span></div>
<p class="mce-root">We get 0.95 accuracy again, which is the best possible accuracy. If we compare it to the previous examples, we can see that the model <em>converged</em> (that is, the length of time it took for the cost function to plateau) much quicker, after just <strong>500</strong> epochs. We needed fewer epochs, so we can see an inverse relationship between <span>learning rates and </span>training epochs. A higher learning rate may mean you need fewer epochs. But are bigger learning rates always better? Well, no.</p>
<p>Click the <strong>Reset</strong> button to change the input values to their defaults, then change the <strong>Learning rate</strong> to the maximum value (<strong>20</strong>), and click the <strong>Run NN Model</strong> button again. When you do, you will get similar output to the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-755 image-border" src="assets/68feee2c-d3dc-44ca-9f99-7cf61160a1c2.png" style="width:106.67em;height:85.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.7: Neural network model execution with too great a learning rate</span></div>
<p class="mce-root">We get an accuracy rate of 0.83. What just happened? By selecting a huge learning rate, our model failed to converge at all. <span>We can see that the cost function actually increases at the start of training, which indicates that the Learning rate is too high. </span>Our cost function graph seems to have repeating values, which indicates that the gradient-descent algorithm is overshooting the minima at times. </p>
<p>Finally, we can look at how the choice of activation function affects model training. By changing the activation function, you may <span>also </span>need to change the <strong>Learning rate</strong>. <span>Click the <strong>Reset</strong> button to change the input values to their defaults and select <kbd>tanh</kbd> for the activation function. W</span>hen we select <kbd>tanh</kbd> as the activation function and 1.5 as the <strong>Learning rate</strong>, the cost gets <kbd>stuck</kbd> at 0.4 from epochs 500-3,500 before suddenly decreasing to 0.2. This can occur in neural networks when they get stuck in local optima. This phenomena can be seen in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/205bbcd5-1d22-4e25-a98b-39cfc4ef1baa.png" style="width:44.58em;height:33.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.8<span>: Neural network model execution with the tanh activation function</span></div>
<p>In contrast, using relu activation results in the model training faster. The following is an example where we only run 1,500 epochs with the relu activation to get the maximum possible accuracy of 0.95:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/06a0a37f-e52a-4037-a549-b455d75dd459.png" style="width:44.58em;height:33.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.9: Neural network model execution with the relu activation function</span></div>
<p class="mce-root">I encourage you to experiment with the other datasets. For reference purposes, here is the max accuracy I got for each of those datasets. An interesting experiment is to see how different activation functions and learning rates work with these datasets:</p>
<ul>
<li class="mce-root"><strong>worms (accuracy=0.95)</strong>: 3 nodes, 3,000 epochs, Learning rate = 0.5, activation = tanh</li>
<li class="mce-root"><strong>moon (accuracy=0.95)</strong>: 5 nodes, 5,000 epochs, Learning rate = 5, <span><span>activation = sigmoid</span></span></li>
<li class="mce-root"><strong>blocks (accuracy=0.9025)</strong>: 5 nodes, 5,000 epochs, Learning rate = 10<span>, activation = sigmoid</span></li>
</ul>
<p class="mce-root">In general, you will see the following:</p>
<ul>
<li class="mce-root">Using more epochs means a longer training time, which may not always be needed.</li>
<li class="mce-root">If the model has not achieved the best accuracy and the cost function has plateaued (that is, it is not decreasing by much) toward the end of the training, then running it longer (<span>that is,</span> more epochs) or increasing the learning rate is unlikely to improve performance. Instead, look at changing the <span>model's </span>architecture, <span>such as by</span> changing the # layers (not an option in this demo), adding more nodes, or changing the activation functions.</li>
<li class="mce-root">The learning rate must be selected carefully. If the value selected is too low, it will take a long time for the model to train. If the value selected is too high, the model will fail to train.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network code</h1>
                </header>
            
            <article>
                
<p>While the web application is useful to see the output of the neural network, we can also run the code for the neural network <span>to really see how it works</span>. The code in <kbd>Chapter3/nnet.R</kbd> allows us to <span>do just that</span>. This code has the same hyper-parameters as in the web application; this file allows you to run the neural network from the RStudio IDE. <span>The following </span><span>is the code that loads the data and sets the initial hyper-parameters for the neural network:</span></p>
<pre>source("nnet_functions.R")<br/>data_sel &lt;- "bulls_eye"<br/><br/>........<br/><br/>####################### neural network ######################<br/>hidden &lt;- 3<br/>epochs &lt;- 3000<br/>lr &lt;- 0.5<br/>activation_ftn &lt;- "sigmoid"<br/><br/>df &lt;- getData(data_sel) # from nnet_functions<br/>X &lt;- as.matrix(df[,1:2])<br/>Y &lt;- as.matrix(df$Y)<br/>n_x=ncol(X)<br/>n_h=hidden<br/>n_y=1<br/>m &lt;- nrow(X)</pre>
<p><span>This code should not be too difficult to understand, it loads a dataset and sets some variables. The data is created in the <kbd>getData</kbd> function from the <kbd>Chapter3/nnet_functions.R</kbd> file. The data is created from functions in the <kbd>clustersim</kbd> package. The <kbd>Chapter3/nnet_functions.R</kbd> file contains the core functionality of our neural network that we will look at here</span><span>. Once we load our data, the next step is to initialize our weights and biases. The <kbd>hidden</kbd> variable controls the number of </span><span>nodes in the hidden layer; we set it to 3. We need two sets of weights and biases, one for the hidden layer and one for the output layer:</span></p>
<pre># initialise weights<br/>set.seed(42)<br/>weights1 &lt;- matrix(0.01*runif(n_h*n_x)-0.005, ncol=n_x, nrow=n_h)<br/>weights2 &lt;- matrix(0.01*runif(n_y*n_h)-0.005, ncol=n_h, nrow=n_y)<br/>bias1 &lt;- matrix(rep(0,n_h),nrow=n_h,ncol=1)<br/>bias2 &lt;- matrix(rep(0,n_y),nrow=n_y,ncol=1)</pre>
<p>This creates matrices for the <kbd>(weights1, bias1)</kbd> hidden layer and <span>the</span> <kbd>(weights2, bias2)</kbd><span> output layer. </span>We need to ensure our matrices have the correct dimensions. For example, the <kbd>weights1</kbd> <span>matrix should have the same number of columns as the input layer and the same number of rows as the hidden layer. Now we move on to the actual processing loop of the neural network:</span></p>
<pre>for (i in 0:epochs)<br/>{<br/>  activation2 &lt;- forward_prop(t(X),activation_ftn,weights1,bias1, weights2,bias2)<br/>  cost &lt;- cost_f(activation2,t(Y))<br/>  backward_prop(t(X),t(Y),activation_ftn,weights1,weights2, activation1,activation2)<br/>  weights1 &lt;- weights1 - (lr * dweights1)<br/>  bias1 &lt;- bias1 - (lr * dbias1)<br/>  weights2 &lt;- weights2 - (lr * dweights2)<br/>  bias2 &lt;- bias2 - (lr * dbias2)<br/> <br/>  if ((i %% 500) == 0)<br/>    print (paste(" Cost after",i,"epochs =",cost))<br/>}<br/>[1] " Cost after 0 epochs = 0.693147158995952"<br/>[1] " Cost after 500 epochs = 0.69314587328381"<br/>[1] " Cost after 1000 epochs = 0.693116915341439"<br/>[1] " Cost after 1500 epochs = 0.692486724429629"<br/>[1] " Cost after 2000 epochs = 0.687107068792801"<br/>[1] " Cost after 2500 epochs = 0.660418522655335"<br/>[1] " Cost after 3000 epochs = 0.579832913091798"</pre>
<p>We first run the forward-propagation function, then calculate a cost. We then call a backward-propagation step that calculates our derivatives, <kbd>(dweights1, dbias1, dweights2, dbias2)</kbd><span>. Then we update the weights and biases, <kbd>(weights1, bias1, weights2, bias2)</kbd>, using our Learning rate, <kbd>(lr)</kbd>. We run this loop for the number of </span><kbd>epochs (3000)</kbd> <span>and print out a diagnostic message every 500</span> <kbd>epochs</kbd><span>. This describes how every neural network and deep learning model works: first call forward-propagation, then calculate costs and derivative values, use those to update the weights through back-propagation and repeat.</span></p>
<p><span>Now let's look at some of the functions in the <kbd>nnet_functions.R</kbd> file. The following is the <kbd>forward</kbd> propagation function:</span></p>
<pre>forward_prop &lt;- function(X,activation_ftn,weights1,bias1,weights2,bias2)<br/>{<br/>  # broadcast hack<br/>  bias1a&lt;-bias1<br/>  for (i in 2:ncol(X))<br/>    bias1a&lt;-cbind(bias1a,bias1)<br/>  bias2a&lt;-bias2<br/>  for (i in 2:ncol(activation1))<br/>    bias2a&lt;-cbind(bias2a,bias2)<br/>  <br/>  Z1 &lt;&lt;- weights1 %*% X + bias1a<br/>  activation1 &lt;&lt;- activation_function(activation_ftn,Z1)<br/>  bias2a&lt;-bias2<br/>  for (i in 2:ncol(activation1))<br/>    bias2a&lt;-cbind(bias2a,bias2)<br/>  Z2 &lt;&lt;- weights2 %*% activation1 + bias2a<br/>  activation2 &lt;&lt;- sigmoid(Z2)<br/>  return (activation2)<br/>}</pre>
<div class="packt_infobox">If you looked at the code carefully, you may have noticed that the assignment to the <kbd>activation1</kbd>, <kbd>activation2</kbd>, <kbd>Z1</kbd>, and <kbd>Z2</kbd> variables uses <kbd>&lt;&lt;-</kbd> rather than <kbd>&lt;-</kbd>. This makes those variables global in scope; we also want to use these values during back propagation. Using global variables is generally frowned upon and I could have returned a list, but <span>it is acceptable here to use them </span>because this application is for learning purposes.</div>
<p>The two for loops expand the bias vectors into<span> matrices</span>, then repeat the vector n times. The interesting code starts with the <kbd>Z1</kbd> assignment. <kbd>Z1</kbd> is a matrix multiplication, followed by an addition. We call the <kbd>activation_function</kbd> function on that value. We then use that output value and perform a similar operation for <kbd>Z2</kbd>. Finally, we apply a sigmoid activation to our output layer because our problem is binary classification.</p>
<p><span>The following</span> is the code for the activation function; the first parameter decides which function to use (<span><kbd>sigmoid</kbd>, <kbd>tanh</kbd>, or <kbd>relu</kbd></span>). The second parameter is the value to be used as input:</p>
<pre>activation_function &lt;- function(activation_ftn,v)<br/>{<br/>  if (activation_ftn == "sigmoid")<br/>    res &lt;- sigmoid(v)<br/>  else if (activation_ftn == "tanh")<br/>    res &lt;- tanh(v)<br/>  else if (activation_ftn == "relu")<br/>  {<br/>    v[v&lt;0] &lt;- 0<br/>    res &lt;- v<br/>  }<br/>  else<br/>    res &lt;- sigmoid(v)<br/>  return (res)<br/>}</pre>
<p><span>The following</span> is the <kbd>cost</kbd> function:</p>
<pre>cost_f &lt;- function(activation2,Y)<br/>{<br/>  cost = -mean((log(activation2) * Y)+ (log(1-activation2) * (1-Y)))<br/>  return(cost)<br/>}</pre>
<p>As a reminder, the output of the <kbd>cost</kbd> function is what we are trying to minimize. There are many types of <kbd>cost</kbd> functions; in this application we are using binary cross-entropy. The formula for <span>binary cross-entropy </span>is <em>-1/m <span>∑ log(ȳ<sub>i</sub>) * y<sub>i</sub> + (log(1 -ȳ<sub>i</sub>) * (1-y<sub>i</sub></span>)</em>. Our target values (<em><span>y</span><sub>i</sub></em>) are always either <em>1</em> or <em>0</em>, so for instances where <em><span>y</span><sub>i</sub> = 1</em>, this reduces to <em><span>∑</span></em><em><span>log(ȳ</span><sub>i</sub></em><span><em>)</em>. If we have two rows where <em>y<sub>i</sub> = 1</em> and suppose that our model predicts <em>1.0</em> for the first row and the <em>0.0001</em> for the second row, then the costs for the rows are <em>log(1)=0</em> and <em>log(0.0001)=-9.1</em>, respectively. We can see that the closer to <em>1</em> the prediction is for these rows, the lower the <kbd>cost</kbd> value. Similarly, for rows where y<sub>i</sub> = 0, this reduces to log(1-ȳ<sub>i</sub>), so the closer to 0 the prediction is for these rows, the lower the <kbd>cost</kbd> value.</span></p>
<div class="packt_infobox"><span>If we are trying to maximize accuracy, why don't we just use what during model training? Binary cross-entropy is a better <kbd>cost</kbd> function because our model does not just output 0 or 1, but instead outputs continuous values from 0.0 to 1.0. For example, if two </span><span>input </span><span>rows had a target value=1 (that is, y=1), and our model gave probabilities of 0.51 and 0.99, then binary cross-entropy would give them a cost of 0.67 and 0.01, respectively. It assigns a higher cost to the first row because the model is unsure about it (the probability is close to 0.5). If instead we just looked at accuracy, we might decide that both rows have the same cost value because they are classified correctly (assuming we assign class=0 where predicted values &lt; 0.5, and class=1 where predicted values &gt;= 0.5).</span></div>
<p><span>The following</span> is the code for the backward-propagation function:</p>
<pre>backward_prop &lt;- function(X,Y,activation_ftn,weights1,weights2,activation1,activation2)<br/>{<br/>  m &lt;- ncol(Y)<br/>  derivative2 &lt;- activation2-Y<br/>  dweights2 &lt;&lt;- (derivative2 %*% t(activation1)) / m<br/>  dbias2 &lt;&lt;- rowSums(derivative2) / m<br/>  upd &lt;- derivative_function(activation_ftn,activation1)<br/>  derivative1 &lt;- t(weights2) %*% derivative2 * upd<br/>  dweights1 &lt;&lt;- (derivative1 %*% t(X)) / m<br/>  dbias1 &lt;&lt;- rowSums(derivative1) / m<br/>}</pre>
<p>Backward propagation processes the network in reverse, starting at the last hidden layer and finishing at the first hidden layer, that is, in the direction of the output layer to the input layer. In our case, we only have one hidden layer, so it first calculates the loss from the output layer and calculates <kbd>dweight2</kbd> and <kbd>dbias2</kbd>. It then calculates the <kbd>derivative</kbd> of the <kbd>activation1</kbd> value, which was calculated during the forward-propagation step. The <kbd>derivative</kbd> function is similar to the activation function, but instead of calling an activation function, it calculates the <kbd>derivative</kbd> of that function. For example, the <kbd>derivative</kbd> of <kbd>sigmoid(x)</kbd> is <em><span>sigmoid(x)</span> * (1 - <span>sigmoid(x)</span>)</em>. The <kbd>derivative</kbd> values of simple functions can be found in any calculus reference or online:</p>
<pre>derivative_function &lt;- function(activation_ftn,v)<br/>{<br/>  if (activation_ftn == "sigmoid")<br/>   upd &lt;- (v * (1 - v))<br/>  else if (activation_ftn == "tanh")<br/>   upd &lt;- (1 - (v^2))<br/>  else if (activation_ftn == "relu")<br/>   upd &lt;- ifelse(v &gt; 0.0,1,0)<br/>  else<br/>   upd &lt;- (v * (1 - v))<br/>  return (upd)<br/>}</pre>
<p>That's it! A working neural network using basic R code. It can fit complex functions and performs better than logistic regression. You might not get all the parts at once, that's OK. <span>The following</span> is a quick recap of the steps:</p>
<ol>
<li>Run a forward-propagation step, which involves multiplying the weights by the input for each layer and passing the output to the next layer.</li>
<li>Evaluate the output from the final layer using the <kbd>cost</kbd> function.</li>
<li>Based on the error rate, use backpropagation to make small adjustments to the weights in the nodes in each layer. The learning rate controls how much of an adjustment we make each time.</li>
<li>Repeat steps 1-3, maybe thousands of times, until the <kbd>cost</kbd> <span>function begins to plateau, which indicates our model is trained.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back to deep learning</h1>
                </header>
            
            <article>
                
<p>Many of the concepts in the previous section apply to deep learning because deep learning is simply neural networks with two or more hidden layers. To demonstrate this, let's look at the following code in R that loads the <kbd>mxnet</kbd> deep learning library and calls the help command on the function in that library that trains a deep learning model. Even though we have not trained any models using this library yet, we have already seen many of the parameters in this function:</p>
<pre>library(mxnet)<br/>?mx.model.FeedForward.create</pre>
<div class="packt_tip">If you get errors saying the <kbd>mxnet</kbd> package is unavailable, see <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml" target="_blank"/><a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, for installation instructions. However, <span>we are not running any <kbd>mxnet</kbd> code in this chapter, we only want to display the help page for a function. So </span>feel free to just <span>continue reading and you can install the package later when we use it in the next chapter.</span></div>
<p>This brings up the help page for the <span><kbd>FeedForward</kbd> function in the <kbd>mxnet</kbd> library, which is the forward-propagation/model train function. <kbd>mxnet</kbd> and most deep learning libraries do not have a specific <em>backward-</em>propagation function, they handle this implicitly:</span></p>
<pre>mx.model.FeedForward.create(symbol, X, y = NULL, ctx = NULL,<br/> begin.round = 1, num.round = 10, optimizer = "sgd",<br/> initializer = mx.init.uniform(0.01), eval.data = NULL,<br/> eval.metric = NULL, epoch.end.callback = NULL,<br/> batch.end.callback = NULL, array.batch.size = 128<br/> ...)</pre>
<p class="mce-root"><span>We will see more of this function in subsequent chapters; for now we will just look at the parameters.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The symbol, X, y, and ctx parameters</h1>
                </header>
            
            <article>
                
<p>The symbol parameter defines the deep learning architecture; X and y are the input and output data structures. The ctx <span>parameter</span> <span>controls which device (for example, CPU/GPU) the model is trained on.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The num.round and begin.round parameters</h1>
                </header>
            
            <article>
                
<p><kbd>num.round</kbd> is equivalent to epochs in our code; that is, however many times we iterate over the data. <kbd>begin.round</kbd> is where we resume training the model if we paused training previously. If we pause training, we can save the partially-trained model, reload it later, and resume training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The optimizer parameter</h1>
                </header>
            
            <article>
                
<p class="mce-root">Our implementation of neural networks used gradient descent. When researchers started creating more complicated multilayer neural network models, they found that they took an extraordinarily long time to train. This is because the basic gradient-descent algorithm with no optimization is not very efficient; it makes small steps towards its goal in each epoch regardless of what occurred in previous epochs. We can compare it with a guessing game: one person has to guess a number in a range and for each guess, they are told to go higher or lower (assuming they do not guess the correct number!). The higher/lower instruction is similar to the derivative value, it indicates the direction we must travel. Now let's say that the range of possible numbers is 1 to 1,000,000 and the first guess is 1,000. The person is told to go higher, which should they do:</p>
<ul>
<li>Try 1001.</li>
<li>Take the difference between the guess and the max value and divide by 2. Add this value to the previous guess.</li>
</ul>
<p class="mce-root">The second option is much better and should mean the person gets to the right answer in 20 guesses or fewer. If you have a background in computer science, you may recognize this as the binary-search algorithm. The first option, guessing 1,001, 1,002, ...., 1,000,000, is a terrible choice and will probably fail as one party will give up! But this is similar to how gradient descent works. It moves incrementally towards the target. If you try increasing the learning rate to overcome this problem, you can overshoot the target and the model fails to converge.</p>
<p class="mce-root">Researchers came up with some clever optimizations to speed up training. One of the first optimizers was called momentum, and it does exactly what its name states. It looks at the extent of the derivative and takes bigger <em>steps</em> for each epoch if the previous steps were all in the same direction. It should mean that the model trains much quicker. There are other algorithms that are enhancements of these, such as RMS-Prop and Adam. You don't usually need to know how they work, just that, when you change the optimizer, you may also have to adjust other hyper-parameters, such as the learning rate. In general, look for previous examples done by others and copy those <span>hyper-parameters.</span></p>
<p>We actually used one of these optimizers in an example in the previous chapter. In that chapter, we had 2 models with a similar architecture (<span>40 hidden nodes)</span>. The first model (<kbd>digits.m3</kbd>) used the <kbd>nnet</kbd> library and took 40 minutes to train. The second model (<kbd>digits.m3</kbd>) <span>used resilient backpropagation and took 3 minutes to train. This shows the benefit of using an optimizer in neural networks and deep learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The initializer parameter</h1>
                </header>
            
            <article>
                
<p><span>When we created the initial values for our weights and biases (that is, model parameters), we used random numbers, but limited them to the values of -0.005 to +0.005. If you go back and review some of the graphs of the <kbd>cost</kbd> functions, you see that it took 2,000 epochs before the <kbd>cost</kbd> function began to decline. This is because the initial values were not in the right range and it took 2,000 epochs to get to the correct magnitude. Fortunately, we do not have to worry about how to set these parameters in the <kbd>mxnet</kbd> library because this parameter </span>controls how the weights and biases are initialized before training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The eval.metric and eval.data parameters</h1>
                </header>
            
            <article>
                
<p>These two parameters control what data <span>and which metric are </span>used to evaluate <span>the model. <kbd>eval.metric</kbd> is equivalent to the <kbd>cost</kbd> function we used in our code. <kbd>eval.data</kbd> is used if you want to evaluate the model on a holdout dataset that is not used in training.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The epoch.end.callback parameter</h1>
                </header>
            
            <article>
                
<p>This is a <kbd>callback</kbd> function that allows you to register another function that is called after <em>n</em> epochs. Deep learning models take a long time to train, so you need some feedback to know they are working correctly! You can write a custom <kbd>callback</kbd> function to do whatever you need, but usually it outputs to the screen or log after <em>n</em> epochs. This is equivalent to the code in our neural network that printed a diagnostic message every <em>500</em> epochs. The <kbd>callback</kbd> function can also be used to save the model to disk, for example, if you wanted to save the model before it begins to overfit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The array.batch.size parameter</h1>
                </header>
            
            <article>
                
<p class="mce-root">We only had 400 instances (rows) in our data, which can easily fit into memory. However, if your input data has millions of instances, the data needs to be split into batches during training in order to fit in the memory of the CPU/GPU. The number of instances you train at a time is the batch size. Note, you still iterate over all the data for the number of epochs, you just split the data into batches during each iteration and run the forward-propagation, backpropagation step over each batch for each epoch. For example, if you had 100 instances and selected a batch size of <em>32</em> with <em>6</em> epochs, you would need <em>4</em> batches for each epoch (<em>100/32 = 3.125</em>, so we need <em>4</em> batches to process all the data), for a total of <em>24</em> loops.</p>
<p class="mce-root">There is a trade-off in choosing the batch size. If you choose too low a value, the model will take a longer time to train because it's running more operations and batches will have more variability because of the small size. You cannot choose an enormous batch size either, this might cause your model to crash because it loads too much data into either the CPU or GPU. In most cases, you either take a sensible default that works from another deep learning model, or you set it at some value (for example, 1,024) and if your model crashes, then try again with a value of half the previous value (512).</p>
<p class="mce-root">There is a relationship between <strong>Batch size</strong>, <strong>Learning rates</strong>, and <strong># Epochs</strong> for training. But there are no hard and fast rules in selecting values. However, in general, consider changing these values together and do not use an extreme value for one of these hyper-parameters. For example, picking a large Learning rate should mean fewer epochs, but if your batch size is too small, the model may fail to train. The best advice is to look at similar architectures and pick a similar set and range of values.</p>
<p>Now that we can see that deep learning still uses many of the concepts from neural networks, we will move on to talk about an important issue that you will probably encounter with every deep learning model: overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using regularization to overcome overfitting</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we saw the diminishing returns from further training iterations on neural networks in terms of their predictive ability on holdout or test data (that is, data not used to train the model). This is because complex models may memorize some of the noise in the data rather than learning the general patterns. These models then perform much worse when predicting new data. There are some methods we can apply to make our model generalize, that is, fit the overall patterns. These are called <strong>regularization</strong> and aim to reduce testing errors so that the model performs well on new data.</p>
<p class="mce-root">The most common regularization technique used in deep learning is dropout. However, we will also discuss two other regularization techniques that have a basis in regression and deep learning. These two regularization techniques are <strong>L1 penalty</strong>, which is also known as <strong>Lasso</strong>, and <strong>L2 penalty</strong>, which is also known as <strong>Ridge</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L1 penalty</h1>
                </header>
            
            <article>
                
<p class="mce-root">The basic concept of the <strong>L1 penalty</strong>, also known as the <strong>least-absolute shrinkage and selection operator</strong> (<strong>Lasso</strong><span>–</span>Hastie, T., Tibshirani, R., and Friedman, J. (2009)), is that a penalty is used to shrink weights toward zero. The penalty term uses the sum of the absolute weights, so some weights may get shrunken to zero. This means that Lasso can also be used as a type of variable selection. The strength of the penalty is controlled by a hyper-parameter, alpha (<span>λ</span>), which multiplies the sum of the absolute weights, and it can be a fixed value or, as with other hyper-parameters, optimized using cross-validation or some similar approach.</p>
<p class="mce-root">It is easier to describe Lasso if we use an <strong>ordinary least squares</strong> (<strong>OLS</strong>) regression model. In regression, a set of coefficients or model weights is estimated using the least-squared error criterion, where the weight/coefficient vector, <em><span>Θ</span></em>, is estimated such that it minimizes <em>∑(y<sub>i</sub> - ȳ<sub>i</sub>)</em> where <em><span>ȳ</span><sub>i</sub><span>=b+Θx</span>, y<sub>i</sub></em> is the target value we want to predict and <em><span>ȳ</span><sub>i</sub></em> is the predicted value. Lasso regression adds an additional penalty term that now tries to minimize <span>∑(y</span><sub>i</sub><span> - ȳ</span><sub>i</sub><span>) </span>+ <span>λ⌊Θ⌋</span><span>, where ⌊Θ⌋ is the absolute value of <em>Θ</em>. </span><span>Typically, the intercept or offset term is excluded from this constraint.</span></p>
<p class="mce-root">There are a number of practical implications for <span>Lasso regression</span>. First, the effect of the penalty depends on the size of the weights, and the size of the weights depends on the scale of the data. Therefore, data is typically standardized to have unit variance first (or at least to make the variance of each variable equal). The L1 penalty has a tendency to shrink small weights to zero (for explanations as to why this happens, see Hastie, T., Tibshirani, R., and Friedman, J. (2009)). If you only consider variables for which the L1 penalty leaves non-zero weights, it can essentially function as feature-selection. The tendency for the L1 penalty to shrink small coefficients to zero can also be convenient for simplifying the interpretation of the model results.</p>
<p class="mce-root">Applying the L1 penalty to neural networks works exactly the same for neural networks as it does for regression. If <em>X</em> represents the input, <em>Y</em> is the outcome or dependent variable, <em>B</em> the parameters, and <em>F</em> the objective function that will be optimized to obtain <em>B</em>, that is, we want to minimize <em>F(B; X, Y)</em>. The L1 penalty modifies the objective function to be F(B; X, Y) + <span>λ⌊Θ⌋</span>, where <em><span>Θ</span></em> represents the weights (typically offsets are ignored). The L1 penalty tends to result in a sparse solution (that is, more zero weights) as small and larger weights result in equal penalties, so that at each update of the gradient, the weights are moved toward zero.</p>
<p class="mce-root">We have only considered the case where <span><em>λ</em></span> is a constant, controlling the degree of penalty or regularization. However, it is possible to set different values with deep neural networks, where varying degrees of regularization can be applied to different layers. One reason for considering such differential regularization is that it is sometimes desirable to allow a greater number of parameters (say by including more neurons in a particular layer) but then counteract this somewhat through stronger regularization. However, this approach <span>can be quite computationally demanding if we are allowing the L1 penalty to vary for every layer of a deep neural network and using</span> cross-validation to optimize all possible combinations of the <span>L1 penalty. </span>Therefore, usually a single value is used across the entire model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L1 penalty in action</h1>
                </header>
            
            <article>
                
<p class="mce-root">To see how the L1 penalty works, we can use a simulated linear regression problem. The code for the rest of this chapter is in <kbd>Chapter3/overfitting.R</kbd>. We simulate the data, using a correlated set of predictors:</p>
<pre>set.seed(1234)<br/>X &lt;- mvrnorm(n = 200, mu = c(0, 0, 0, 0, 0),<br/>  Sigma = matrix(c(<br/>    1, .9999, .99, .99, .10,<br/>    .9999, 1, .99, .99, .10,<br/>    .99, .99, 1, .99, .10,<br/>    .99, .99, .99, 1, .10,<br/>    .10, .10, .10, .10, 1<br/>  ), ncol = 5))<br/>y &lt;- rnorm(200, 3 + X %*% matrix(c(1, 1, 1, 1, 0)), .5)</pre>
<p class="mce-root">Next, we can fit an OLS regression model to the first 100 cases, and then use <kbd>lasso</kbd>. To use <kbd>lasso</kbd>, we use the <kbd>glmnet()</kbd> function from the <kbd>glmnet</kbd> package. This function can actually fit the L1 or the L2 (discussed in the next section) penalties, and which one occurs is determined by the argument, alpha. When <kbd>alpha = 1</kbd>, it is the L1 penalty (that is, <kbd>lasso</kbd>), and when <kbd>alpha = 0</kbd>, it is the L2 penalty (that is, ridge regression). Further, because we don't know which value of <kbd>lambda</kbd> we should pick, we can evaluate a range of options and tune this hyper-parameter automatically using cross-validation, which is the <kbd>cv.glmnet()</kbd> function. <span>We can then plot the <kbd>lasso</kbd> object to see the mean squared error for a variety of <kbd>lambda</kbd> values to allow us to select the correct level of regularization:</span></p>
<pre>m.ols &lt;- lm(y[1:100] ~ X[1:100, ])<br/>m.lasso.cv &lt;- cv.glmnet(X[1:100, ], y[1:100], alpha = 1)<br/>plot(m.lasso.cv)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d0b8aa13-f250-4111-9d27-c7b13b088466.png" style="width:40.58em;height:25.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 3.10: Lasso regularization</div>
<p class="mce-root">One thing that we can see from the graph is that, when the penalty gets too high, the cross-validated model increases. Indeed, <kbd>lasso</kbd> seems to do well with very low lambda values, perhaps indicating <kbd>lasso</kbd> does not help improve out-of-sample performance/generalizability much for this dataset. For the sake of this example, we will continue but in actual use, this might give us pause to consider whether <kbd>lasso</kbd> was really helping. Finally, we can compare the coefficients with those from <kbd>lasso</kbd>:</p>
<pre>cbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1])<br/>               OLS Lasso<br/>(Intercept)  2.958  2.99<br/>X[1:100, ]1 -0.082  1.41<br/>X[1:100, ]2  2.239  0.71<br/>X[1:100, ]3  0.602  0.51<br/>X[1:100, ]4  1.235  1.17<br/>X[1:100, ]5 -0.041  0.00</pre>
<p class="mce-root">Notice that the OLS coefficients are noisier and also that, in <kbd>lasso</kbd>, predictor 5 is penalized to 0. Recall from the simulated data that the true coefficients are 3, 1, 1, 1, 1, and 0. The OLS estimates have much too low a value for the first predictor and much too high a value for the second, whereas <kbd>lasso</kbd> has more accurate values for each. This demonstrates that <kbd>lasso</kbd> regression generalizes better than OLS regression for this dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L2 penalty</h1>
                </header>
            
            <article>
                
<p>The <strong>L2 penalty</strong>, also known as <strong>ridge regression</strong>, is similar in many ways to the L1 penalty, but instead of adding a penalty based on the sum of the absolute weights, the penalty is based on the squared weights. This means that larger absolute weights are penalized more. In the context of neural networks, this is sometimes referred to as weight decay. If you examine the gradient of the regularized objective function, there is a penalty such that, at every update, there is a multiplicative penalty to the weights. As for the L1 penalty, although they could be included, biases or offsets are usually excluded from this.</p>
<p class="mce-root">From the perspective of a linear regression problem, the L2 penalty is a modification to the objective function minimized, from <em><span>∑(y</span><sub>i</sub><span> - ȳ</span><sub>i</sub><span>)</span> </em>to<em> <span>∑(y</span><sub>i</sub><span> - ȳ</span><sub>i</sub><span>) </span><span>+ </span><span>λΘ<sup>2</sup></span></em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L2 penalty in action</h1>
                </header>
            
            <article>
                
<p>To see how the L2 penalty works, we can use the same simulated linear regression problem we used for the Ll penalty. To fit a ridge regression model, we use the <kbd>glmnet()</kbd> function from the <kbd>glmnet</kbd> package. As mentioned previously, this function can actually fit the L1 or the L2 penalties, and which one occurs is determined by the argument, alpha. When <kbd>alpha = 1</kbd>, it fits <kbd>lasso</kbd>, and when <kbd>alpha = 0</kbd>, it fits ridge regression. This time, we choose <kbd>alpha = 0</kbd>. Again, we evaluate a range of lambda options and tune this hyper-parameter automatically using cross-validation. This is accomplished by using the <kbd>cv.glmnet()</kbd> function. <span>We plot the ridge regression object to see the error for a variety of lambda values:</span></p>
<pre>m.ridge.cv &lt;- cv.glmnet(X[1:100, ], y[1:100], alpha = 0)<br/>plot(m.ridge.cv)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e89c9b9a-ea90-4458-9471-b6a6ea584165.png" style="width:41.33em;height:25.58em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 3.11: Ridge regularization</div>
<p>Although the shape is different from <kbd>lasso</kbd> in that the error appears to asymptote for higher lambda values, it is still clear that, when the penalty gets too high, the cross-validated model error increases. As with <kbd>lasso</kbd>, the ridge regression model seems to do well with very low lambda values, perhaps indicating the L2 penalty does not improve out-of-sample performance/generalizability by much.</p>
<p class="mce-root">Finally, we can compare the OLS coefficients with those from <kbd>lasso</kbd> and the ridge regression model:</p>
<pre>&gt; cbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1],Ridge = coef(m.ridge.cv)[,1])<br/>               OLS Lasso   Ridge<br/>(Intercept)  2.958  2.99  2.9919<br/>X[1:100, ]1 -0.082  1.41  0.9488<br/>X[1:100, ]2  2.239  0.71  0.9524<br/>X[1:100, ]3  0.602  0.51  0.9323<br/>X[1:100, ]4  1.235  1.17  0.9548<br/>X[1:100, ]5 -0.041  0.00 -0.0023</pre>
<p class="mce-root">Although ridge regression does not shrink the coefficient for the fifth predictor to exactly 0, it is smaller than in the OLS, and the remaining parameters are all slightly shrunken, but quite close to their true values of 3, 1, 1, 1, 1, and 0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight decay (L2 penalty in neural networks)</h1>
                </header>
            
            <article>
                
<p>We have already unknowingly used regularization in the previous chapter. The neural network we trained using the <kbd>caret</kbd> and <kbd>nnet</kbd> package used a weight decay of <kbd>0.10</kbd>. We can investigate the use of weight decay by varying it, and tuning it using cross-validation:</p>
<ol>
<li>Load the data as before. Then we create a local cluster to run the cross-validation in parallel:</li>
</ol>
<pre style="padding-left: 60px">set.seed(1234)<br/>## same data as from previous chapter<br/>if (!file.exists('../data/train.csv'))<br/>{<br/>  link &lt;- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'<br/>  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))<br/>    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))<br/>  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)<br/>  if (file.exists(paste(dataDirectory,'/test.csv',sep="")))<br/>    file.remove(paste(dataDirectory,'/test.csv',sep=""))<br/>}<br/><br/>digits.train &lt;- read.csv("../data/train.csv")<br/><br/>## convert to factor<br/>digits.train$label &lt;- factor(digits.train$label, levels = 0:9)<br/><br/>sample &lt;- sample(nrow(digits.train), 6000)<br/>train &lt;- sample[1:5000]<br/>test &lt;- sample[5001:6000]<br/><br/>digits.X &lt;- digits.train[train, -1]<br/>digits.y &lt;- digits.train[train, 1]<br/>test.X &lt;- digits.train[test, -1]<br/>test.y &lt;- digits.train[test, 1]<br/><br/>## try various weight decays and number of iterations<br/>## register backend so that different decays can be<br/>## estimated in parallel<br/>cl &lt;- makeCluster(5)<br/>clusterEvalQ(cl, {source("cluster_inc.R")})<br/>registerDoSNOW(cl)</pre>
<ol start="2">
<li class="mce-root">Train a neural network on the digit classification, and vary the weight-decay penalty at <kbd>0</kbd> (no penalty) and <kbd>0.10</kbd>. We also loop through two sets of the number of iterations allowed: <kbd>100</kbd> or <kbd>150</kbd>. Note that this code is computationally intensive and takes some time to run:</li>
</ol>
<pre style="padding-left: 60px">set.seed(1234)<br/>digits.decay.m1 &lt;- lapply(c(100, 150), function(its) {<br/>  caret::train(digits.X, digits.y,<br/>           method = "nnet",<br/>           tuneGrid = expand.grid(<br/>             .size = c(10),<br/>             .decay = c(0, .1)),<br/>           trControl = caret::trainControl(method="cv", number=5, repeats=1),<br/>           MaxNWts = 10000,<br/>           maxit = its)<br/>})</pre>
<ol start="3">
<li class="mce-root">Examining the results, we see that, when we limit to only <kbd>100</kbd> iterations, both the non-­regularized model and regularized model have the same accuracy at <kbd>0.56</kbd>, based on cross-validated results, which is not very good on this data:</li>
</ol>
<pre style="padding-left: 60px">digits.decay.m1[[1]]<br/>Neural Network <br/><br/>5000 samples<br/> 784 predictor<br/>  10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' <br/><br/>No pre-processing<br/>Resampling: Cross-Validated (5 fold) <br/>Summary of sample sizes: 4000, 4001, 4000, 3998, 4001 <br/>Resampling results across tuning parameters:<br/><br/>  decay  Accuracy   Kappa<br/>    0.0     0.56    0.51 <br/>    0.1     0.56    0.51 <br/><br/>Tuning parameter 'size' was held constant at a value of 10<br/>Accuracy was used to select the optimal model using the<br/> largest value.<br/>The final values used for the model were size = 10 and decay = 0.1.</pre>
<ol start="4">
<li class="mce-root">Examine the model with <kbd>150</kbd> iterations to see whether the regularized or non-regularized model performs better:</li>
</ol>
<pre style="padding-left: 60px">digits.decay.m1[[2]]<br/>Neural Network <br/><br/>5000 samples<br/> 784 predictor<br/>  10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' <br/><br/>No pre-processing<br/>Resampling: Cross-Validated (5 fold) <br/>Summary of sample sizes: 4000, 4002, 3998, 4000, 4000 <br/>Resampling results across tuning parameters:<br/><br/>  decay  Accuracy   Kappa<br/>    0.0      0.64    0.60 <br/>    0.1      0.63    0.59 <br/><br/>Tuning parameter 'size' was held constant at a value of 10<br/>Accuracy was used to select the optimal model using the<br/> largest value.<br/>The final values used for the model were size = 10 and decay = 0.</pre>
<p class="mce-root">Overall, the model with more iterations outperforms the model with fewer iterations, regardless of the regularization. However, comparing both models with 150 iterations, the regularized model is superior (<kbd>accuracy= 0.66</kbd>) to the non-regularized model (<kbd>accuracy= 0.65</kbd>), although here the difference is relatively small.</p>
<p class="mce-root">These results highlight that regularization is often most useful for more complex models that have greater flexibility to fit (and overfit) the data. In models that are appropriate or overly simplistic for the data, regularization will probably decrease performance. When developing a new model architecture, you should avoid adding regularization until the model is performing well on the <span>training data.</span> If you add <span>regularization beforehand and the model performs poorly on the training data, you will not know whether the problem is with the model's architecture or because of the regularization. </span>In the next section, we'll discuss ensemble and model averaging techniques, the last forms of regularization that are highlighted in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensembles and model-averaging</h1>
                </header>
            
            <article>
                
<p>Another approach to regularization involves creating multiple models (ensembles) and combining them, such as by model-averaging or some other algorithm for combining individual model results. There is a rich history of using ensemble techniques in machine learning, such as bagging, boosting, and random forest, that use this technique. The general idea is that, if you build different models using the training data, each model has different errors in the predicted values. Where one model predicts too high a value, another may predict too low a value, and when averaged, some of the errors cancel out, resulting in a more accurate prediction than would have been otherwise obtained.</p>
<p class="mce-root">The key to ensemble methods is that the different models must have some variability in their predictions. If the predictions from the different models are highly correlated, then using ensemble techniques will not be beneficial. If <span>the predictions from the different models </span>have very low correlations, then the average will be far more accurate as it gains the strengths of each model. The following code gives an example using simulated data. This small example <span>illustrates the point with just</span> three models:</p>
<pre>## simulated data<br/>set.seed(1234)<br/>d &lt;- data.frame(<br/> x = rnorm(400))<br/>d$y &lt;- with(d, rnorm(400, 2 + ifelse(x &lt; 0, x + x^2, x + x^2.5), 1))<br/>d.train &lt;- d[1:200, ]<br/>d.test &lt;- d[201:400, ]<br/> <br/>## three different models<br/>m1 &lt;- lm(y ~ x, data = d.train)<br/>m2 &lt;- lm(y ~ I(x^2), data = d.train)<br/>m3 &lt;- lm(y ~ pmax(x, 0) + pmin(x, 0), data = d.train)<br/> <br/>## In sample R2<br/>cbind(M1=summary(m1)$r.squared,<br/> M2=summary(m2)$r.squared,M3=summary(m3)$r.squared)<br/>       M1  M2   M3<br/>[1,] 0.33 0.6 0.76</pre>
<p class="mce-root">We can see that the predictive value of each model, at least in the training data, varies quite a bit. Evaluating the correlations among fitted values in the training data can also help to indicate how much overlap there is among the model predictions:</p>
<pre>cor(cbind(M1=fitted(m1),<br/> M2=fitted(m2),M3=fitted(m3)))<br/>     M1   M2   M3<br/>M1 1.00 0.11 0.65<br/>M2 0.11 1.00 0.78<br/>M3 0.65 0.78 1.00</pre>
<p class="mce-root">Next, we generate predicted values for the testing data, the average of the predicted values, and again correlate the predictions along with reality in the testing data:</p>
<pre>## generate predictions and the average prediction<br/>d.test$yhat1 &lt;- predict(m1, newdata = d.test)<br/>d.test$yhat2 &lt;- predict(m2, newdata = d.test)<br/>d.test$yhat3 &lt;- predict(m3, newdata = d.test)<br/>d.test$yhatavg &lt;- rowMeans(d.test[, paste0("yhat", 1:3)])<br/><br/>## correlation in the testing data<br/>cor(d.test)<br/>             x     y  yhat1  yhat2 yhat3 yhatavg<br/>x        1.000  0.44  1.000 -0.098  0.60    0.55<br/><strong>y        0.442  1.00  0.442  0.753  0.87    0.91</strong><br/>yhat1    1.000  0.44  1.000 -0.098  0.60    0.55<br/>yhat2   -0.098  0.75 -0.098  1.000  0.69    0.76<br/>yhat3    0.596  0.87  0.596  0.687  1.00    0.98<br/>yhatavg  0.552  0.91  0.552  0.765  0.98    1.00</pre>
<p class="mce-root">From the results, we can see that the average of the three models' predictions performs better than any of the models individually. However, this is not always the case; one good model may have better <span>predictions </span>than the average <span>predictions. </span>In general, it is good to check that the models being averaged perform similarly, at least in the training data. The second lesson is that, given models with similar performance, it is desirable to have lower correlations between model predictions, as this will result in the best performing average.</p>
<p class="mce-root">There are other forms of ensemble methods that are included in other machine learning algorithms, for example, bagging and boosting. Bagging is used in random forests, where many models are generated, each having different samples of the data. The models are deliberately designed to be small, incomplete models. By averaging the predictions of lots of <span>undertrained </span>models that use only a portion of the data, we should get a more powerful model. An example of boosting includes gradient-boosted models (GBMs), which also use multiple models, but this time each model focuses on the instances that were incorrectly predicted in the previous model. Both random forests and GBMs have proven to be very successful with structured data because they <span>reduce variance, that is, avoid overfitting the data.</span></p>
<p class="mce-root">Bagging and model-averaging are not used as frequently in deep neural networks because the computational cost of training each model can be quite high, and thus repeating the process many times becomes prohibitively expensive in terms of time and compute resources. Nevertheless, it is still possible to use model averaging in the context of deep neural networks, even if perhaps it is on only a handful of models rather than hundreds, as is common in random forests and some other approaches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case – improving out-of-sample model performance using dropout</h1>
                </header>
            
            <article>
                
<p class="mce-root">Dropout is a novel approach to regularization that is particularly valuable for large and complex deep neural networks. For a much more detailed exploration of dropout in deep neural networks, see Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinav, R. (2014). The concept behind dropout is actually quite straightforward. During the training of the model, units (for example, input and hidden neurons) are probabilistically dropped along with all connections to and from them.</p>
<p class="mce-root">For example, the following diagram is an example of what might happen at each step of training for a model where hidden neurons and their connections are dropped with a probability of 1/3 for each epoch. Once a node is dropped, its connections to the next layer are also dropped. In the the following diagram, the grayed-out nodes and dashed connections are the ones that were dropped. It is important to note that the choice of nodes that are dropped changes for each epoch:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-579 image-border" src="assets/d34f15bc-117f-450b-a667-8d2809c2ae95.jpg" style="width:24.58em;height:54.83em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 3.12: Dropout applied to a layer for different epochs</div>
<p class="mce-root">One way to think about dropout is that it forces models to be more robust to perturbations. Although many neurons are included in the full model, during training they are not all simultaneously present, and so neurons must operate somewhat more independently than they would otherwise. Another way of viewing dropout is that, if you have a large model with N weights between hidden neurons, but 50% are dropped during training, although all N weights will be used during some stages of training, you have effectively halved the total model complexity as the average number of weights will be halved. This reduces model complexity, and hence helps to prevent the overfitting of the data. Because of this feature, if the proportion of dropout is p, Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014) recommend scaling up the target model complexity by 1/p in order to end up with a roughly equally complex model.</p>
<p class="mce-root">During model testing/scoring, neurons are not usually dropped because it is computationally inconvenient. Instead, we can use an approximate average based on scaling the weights from a single neural network based on each weight's probability of being included (that is, 1/p). This is usually taken care of by the deep learning library.</p>
<p class="mce-root">In addition to working well, this approximate weight re-scaling is a fairly trivial calculation. Thus, the primary computational cost of dropout comes from the fact that a model with more neurons and weights must be used because so many (a commonly recommended value is around 50% for hidden neurons) are dropped during each training update.</p>
<p class="mce-root">Although dropout is easy to implement, <span>a larger model may be required </span>to compensate. To speed up training, a higher learning rate can be used so that fewer epochs are required. One potential downside of combining these approaches is that, with fewer neurons and a faster learning rate, some weights may become quite large. Fortunately, it is possible to use dropout along with other forms of regularization, such as the L1 or L2 penalty. Taken together, the result is a larger model that that can quickly (a faster Learning rate) explore a broader parameter space, but is regularized through dropout and a penalty to keep the weights in check.</p>
<p class="mce-root">To show the use of dropout in a neural network, we will return to the <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) dataset (which we downloaded in <a href="cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml">Chapter 2</a>, <em>Training a Prediction Model</em>) we worked with previously. We will use the <kbd>nn.train()</kbd> function from the <kbd>deepnet</kbd> package, as it allows for dropout. As in the previous chapter, we will run the four models in parallel to reduce the time it takes. Specifically, we compare four models, two with and two without dropout regularization and with either 40 or 80 hidden neurons. For dropout, we specify the proportion to dropout separately for the hidden and visible units. Based on the rule of thumb that about 50% of hidden units (and 80% of observed units) should be kept, we specify the dropout proportions at <kbd>0.5</kbd> and <kbd>0.2</kbd>, respectively:</p>
<pre>## Fit Models<br/>nn.models &lt;- foreach(i = 1:4, .combine = 'c') %dopar% {<br/>set.seed(1234)<br/> list(nn.train(<br/>    x = as.matrix(digits.X),<br/>    y = model.matrix(~ 0 + digits.y),<br/>    hidden = c(40, 80, 40, 80)[i],<br/>    activationfun = "tanh",<br/>    learningrate = 0.8,<br/>    momentum = 0.5,<br/>    numepochs = 150,<br/>    output = "softmax",<br/>    hidden_dropout = c(0, 0, .5, .5)[i],<br/>    visible_dropout = c(0, 0, .2, .2)[i]))<br/>}</pre>
<p> </p>
<p> </p>
<p class="mce-root">Next, we can loop through the models to obtain predicted values and get the overall model performance:</p>
<pre>nn.yhat &lt;- lapply(nn.models, function(obj) {<br/> encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))<br/> })<br/>perf.train &lt;- do.call(cbind, lapply(nn.yhat, function(yhat) {<br/> caret::confusionMatrix(xtabs(~ I(yhat - 1) + digits.y))$overall<br/> }))<br/>colnames(perf.train) &lt;- c("N40", "N80", "N40_Reg", "N80_Reg")<br/>options(digits = 4)<br/>perf.train<br/>                   N40     N80 N40_Reg N80_Reg<br/><strong>Accuracy        0.9478  0.9622  0.9278  0.9400</strong><br/>Kappa           0.9420  0.9580  0.9197  0.9333<br/>AccuracyLower   0.9413  0.9565  0.9203  0.9331<br/>AccuracyUpper   0.9538  0.9673  0.9348  0.9464<br/>AccuracyNull    0.1126  0.1126  0.1126  0.1126<br/>AccuracyPValue  0.0000  0.0000  0.0000  0.0000<br/>McnemarPValue      NaN     NaN     NaN     NaN</pre>
<p class="mce-root">When evaluating the models in the in-sample training data, it seems those <span>without regularization </span>perform better those<span> </span>with regularization. Of course, the real test comes with the testing or holdout data:</p>
<pre>nn.yhat.test &lt;- lapply(nn.models, function(obj) {<br/> encodeClassLabels(nn.predict(obj, as.matrix(test.X)))<br/> })<br/><br/>perf.test &lt;- do.call(cbind, lapply(nn.yhat.test, function(yhat) {<br/> caret::confusionMatrix(xtabs(~ I(yhat - 1) + test.y))$overall<br/> }))<br/>colnames(perf.test) &lt;- c("N40", "N80", "N40_Reg", "N80_Reg")<br/><br/>perf.test<br/>                   N40     N80 N40_Reg N80_Reg<br/><strong>Accuracy        0.8890  0.8520  0.8980  0.9030</strong><br/>Kappa           0.8765  0.8352  0.8864  0.8920<br/>AccuracyLower   0.8679  0.8285  0.8776  0.8830<br/>AccuracyUpper   0.9078  0.8734  0.9161  0.9206<br/>AccuracyNull    0.1180  0.1180  0.1180  0.1180<br/>AccuracyPValue  0.0000  0.0000  0.0000  0.0000<br/>McnemarPValue      NaN     NaN     NaN     NaN</pre>
<p class="mce-root">The testing data highlights that the in-sample performance was overly optimistic (accuracy = 0.9622 versus accuracy = 0.8520 for the 80-neuron, non-regularized model in the training and testing data, respectively). We can see the advantage of the regularized models for both the 40- and the 80-neuron models. Although both still perform worse in the testing data than they did in the training data, they perform on a par with, or better than, the equivalent non-regularized models in the testing data. This difference is particularly important for the 80-neuron model as the best performing model on the test data is the regularized model.</p>
<p class="mce-root">Although these numbers are by no means record-setting, they do show the value of using dropout, or regularization more generally, and how one might go about trying to tune the model and dropout parameters to improve the ultimate testing performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter began by showing you how to program a neural network from scratch. We demonstrated the neural network in a web application created by just using R code. We delved into how the neural network actually worked, showing how to code forward-propagation, <kbd>cost</kbd> functions, and backpropagation. Then we looked at how the parameters for our neural network apply to modern deep learning libraries by looking at the <kbd>mx.model.FeedForward.create</kbd> function from the <kbd><span>mxnet</span></kbd> deep learning library.</p>
<p class="mce-root">Then we covered overfitting, demonstrating several approaches to preventing overfitting, including common penalties, the Ll penalty and L2 penalty, ensembles of simpler models, and dropout, where variables and/or cases are dropped to make the model noisy. We examined the role of penalties in regression problems and neural networks. In the next chapter, we will move into deep learning and deep neural networks, and see how to push the accuracy and performance of our predictive models even further.</p>


            </article>

            
        </section>
    </body></html>