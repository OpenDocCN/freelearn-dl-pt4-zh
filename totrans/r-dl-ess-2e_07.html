<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Natural Language Processing Using Deep Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter will demonstrate how to use deep learning for <strong>natural language processing</strong> (<strong>NLP</strong>). NLP is the processing of human language text. <span>NLP is a broad term for a</span> number of different tasks involving text data, which include (but are not limited to) the following:</p>
<ul>
<li class="mce-root"><strong>Document classification</strong>: Classifying documents into different categories based on their subject</li>
<li class="mce-root"><strong>Named entity recognition</strong>: Extracting key information from documents, for example, people, organizations, and locations</li>
<li class="mce-root"><strong>Sentiment analysis</strong>: Classifying comments, tweets, or reviews as positive or negative sentiment</li>
<li class="mce-root"><strong>Language translation</strong>: Translating text data from one language to another</li>
<li class="mce-root"><strong>Part of speech tagging</strong>: Assigning the type to each word in a document, which is usually used in conjunction with another task</li>
</ul>
<p class="mce-root">In this chapter, we will look at document classification, which is probably the most common NLP technique. This chapter follows a different structure to previous chapters, as we will be looking at a single use case (text classification) but applying multiple approaches to it. This chapter will cover:</p>
<ul>
<li>How to perform text classification using traditional machine learning techniques</li>
<li>Word vectors</li>
<li>Comparing traditional text classification and deep learning</li>
<li>Advanced deep learning text classification including 1D convolutionals, RNNs, LSTMs and GRUs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document classification</h1>
                </header>
            
            <article>
                
<p>This chapter will be looking at text classification using Keras. The dataset we will use is included in the Keras library. As we have done in previous chapters, we will use traditional machine learning techniques to create a benchmark before applying a deep learning algorithm. The reason for this is to show how deep learning models perform against other techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Reuters dataset</h1>
                </header>
            
            <article>
                
<p>We will use the Reuters dataset, which can be accessed through a function in the Keras library. This dataset has 11,228 records with 46 categories. To see more information about this dataset, run the following code:</p>
<pre>library(keras)<br/>?dataset_reuters</pre>
<p><span>Although the Reuters dataset can be accessed from Keras, it is not in a format that can be used by other machine learning algorithms. Instead of the actual words, the text data is a list of word indices. We will write a short script (</span><kbd>Chapter7/create_reuters_data.R</kbd><span>) that downloads the data and the lookup index file and creates a data frame of the <kbd>y</kbd> variable and the text string. We will then save the train and test data into two separate files. Here is the first part of the code that creates the file with the train data:</span></p>
<pre>library(keras)<br/><br/># the reuters dataset is in Keras<br/>c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% dataset_reuters()<br/>word_index &lt;- dataset_reuters_word_index()<br/><br/># convert the word index into a dataframe<br/>idx&lt;-unlist(word_index)<br/>dfWords&lt;-as.data.frame(idx)<br/>dfWords$word &lt;- row.names(dfWords)<br/>row.names(dfWords)&lt;-NULL<br/>dfWords &lt;- dfWords[order(dfWords$idx),]<br/><br/># create a dataframe for the train data<br/># for each row in the train data, we have a list of index values<br/># for words in the dfWords dataframe<br/>dfTrain &lt;- data.frame(y_train)<br/>dfTrain$sentence &lt;- ""<br/>colnames(dfTrain)[1] &lt;- "y"<br/>for (r in 1:length(x_train))<br/>{<br/>  row &lt;- x_train[r]<br/>  line &lt;- ""<br/>  for (i in 1:length(row[[1]]))<br/>  {<br/>     index &lt;- row[[1]][i]<br/>     if (index &gt;= 3)<br/>       line &lt;- paste(line,dfWords[index-3,]$word)<br/>  }<br/>  dfTrain[r,]$sentence &lt;- line<br/>  if ((r %% 100) == 0)<br/>    print (r)<br/>}<br/>write.table(dfTrain,"../data/reuters.train.tab",sep="\t",row.names = FALSE)<br/><br/></pre>
<p>The second part of the code is similar, it <span>creates the file with the test data:</span></p>
<pre># create a dataframe for the test data<br/># for each row in the train data, we have a list of index values<br/># for words in the dfWords dataframe<br/>dfTest &lt;- data.frame(y_test)<br/>dfTest$sentence &lt;- ""<br/>colnames(dfTest)[1] &lt;- "y"<br/>for (r in 1:length(x_test))<br/>{<br/>  row &lt;- x_test[r]<br/>  line &lt;- ""<br/>  for (i in 1:length(row[[1]]))<br/>  {<br/>    index &lt;- row[[1]][i]<br/>    if (index &gt;= 3)<br/>      line &lt;- paste(line,dfWords[index-3,]$word)<br/>  }<br/>  dfTest[r,]$sentence &lt;- line<br/>  if ((r %% 100) == 0)<br/>    print (r)<br/>}<br/>write.table(dfTest,"../data/reuters.test.tab",sep="\t",row.names = FALSE)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This creates two files called <kbd>../data/reuters.train.tab</kbd> and <kbd>../data/reuters.test.tab</kbd>. If we open the first file, this is the first data row. This sentence is a normal English sentence:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 10%">
<p><strong>y</strong></p>
</td>
<td style="width: 88%">
<p><strong>sentence</strong></p>
</td>
</tr>
<tr>
<td style="width: 10%">
<p>3</p>
</td>
<td style="width: 88%">
<p>mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now that we have the data in tabular format, we can use <em>traditional</em> NLP machine learning methods to create a classification model. When we merge the train and test sets and look at the distribution of the <em>y</em> variable, we can see that there are 46 classes, but that the number of instances in each class are not the same:</p>
<pre>&gt; table(y_train)<br/><strong>   0   1   2    3    4   5   6   7   8   9  10  11  12  13  14  15  16  17 </strong><br/>  67 537  94 3972 2423  22  62  19 177 126 154 473  62 209  28  29 543  51 <br/><br/><strong>  18   19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35 </strong><br/>  86  682 339 127  22  53  81 123  32  19  58  23  57  52  42  16  57  16 <br/><br/><strong>  36  37  38  39  40  41  42  43  44  45 </strong><br/>  60  21  22  29  46  38  16  27  17  19 </pre>
<p>For our test set, we will create a binary classification problem. Our task will be to identify the news snippets where the classification is 3 from all other records. When we change the labels, our <em>y</em> distribution changes to the following:</p>
<pre>y_train[y_train!=3] &lt;- 0<br/>y_train[y_train==3] &lt;- 1<br/>table(y_train)<br/><strong>   0    1</strong> <br/>7256 3972 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Traditional text classification</h1>
                </header>
            
            <article>
                
<p>Our first NLP model will <span>use traditional NLP techniques, that is, not deep learning. For the rest of this chapter, when we use the term traditional NLP, we mean approaches that do not use deep learning. </span>The most used method for NLP in <span>traditional NLP classification uses a </span><em>bag-of-words</em> approach.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will also use a set of hyperparameters and machine learning algorithms to maximize accuracy:</p>
<ul>
<li><strong>Feature generation</strong>: The features can be term frequency, tf-idf, or binary flags</li>
<li><strong>Preprocessing</strong>: We preprocess text data by <span>stemming the words</span></li>
<li><strong><span>Remove </span>stop-words</strong>: We treat the feature creation, stop-words, and stemming options as hyperparameters</li>
<li><strong>Machine learning algorithm</strong>: The script applies three machine learning algorithms to the data (Naive Bayes, SVM, neural network, and random forest)</li>
</ul>
<p>We train 48 machine learning algorithms on the data in total, and evaluate which model is best. The script for this code is in the <kbd><span>Chapter7/</span>classify_text.R</kbd> folder. The code does not contain any deep learning models, so feel free to skip it if you want. First we load in the necessary libraries and create a function that creates a set of text classification models for a combination of hyperparameters on multiple machine learning algorithms:</p>
<pre>library(tm)<br/>require(nnet)<br/>require(kernlab)<br/>library(randomForest)<br/>library(e1071)<br/>options(digits=4)<br/><br/>TextClassification &lt;-function (w,stem=0,stop=0,verbose=1)<br/>{<br/>  df &lt;- read.csv("../data/reuters.train.tab", sep="\t", stringsAsFactors = FALSE)<br/>  df2 &lt;- read.csv("../data/reuters.test.tab", sep="\t", stringsAsFactors = FALSE)<br/>  df &lt;- rbind(df,df2)<br/><br/>  # df &lt;- df[df$y %in% c(3,4),]<br/>  # df$y &lt;- df$y-3<br/>  df[df$y!=3,]$y&lt;-0<br/>  df[df$y==3,]$y&lt;-1<br/>  rm(df2)<br/><br/>  corpus &lt;- Corpus(DataframeSource(data.frame(df[, 2])))<br/>  corpus &lt;- tm_map(corpus, content_transformer(tolower))<br/>  <br/>  # hyperparameters<br/>  if (stop==1)<br/>    corpus &lt;- tm_map(corpus, function(x) removeWords(x, stopwords("english")))<br/>  if (stem==1)<br/>    corpus &lt;- tm_map(corpus, stemDocument)<br/>  if (w=="tfidf")<br/>    dtm &lt;- DocumentTermMatrix(corpus,control=list(weighting=weightTfIdf))<br/>  else if (w=="tf")<br/>    dtm &lt;- DocumentTermMatrix(corpus,control=list(weighting=weightTf))<br/>  else if (w=="binary")<br/>    dtm &lt;- DocumentTermMatrix(corpus,control=list(weighting=weightBin))<br/>  <br/>  # keep terms that cover 95% of the data<br/>  dtm2&lt;-removeSparseTerms(dtm, 0.95)<br/>  m &lt;- as.matrix(dtm2)<br/>  remove(dtm,dtm2,corpus)<br/>  <br/>  data&lt;-data.frame(m)<br/>  data&lt;-cbind(df[, 1],data)<br/>  colnames(data)[1]="y"<br/>  <br/>  # create train, test sets for machine learning<br/>  seed &lt;- 42 <br/>  set.seed(seed) <br/>  nobs &lt;- nrow(data)<br/>  sample &lt;- train &lt;- sample(nrow(data), 0.8*nobs)<br/>  validate &lt;- NULL<br/>  test &lt;- setdiff(setdiff(seq_len(nrow(data)), train), validate)</pre>
<p>Now that we have created a sparse data-frame, we will use 4 different machine learning algorithms on the data: Naive Bayes, SVM, a neural network model and a random forest model. We use 4 <span>machine learning algorithms because, as you see below, the code to call a machine learning algorithm is small compared to the code needed to create the data in the previous section and the code needed to the the NLP. It is almost always a good idea to run multiple machine learning algorithms when possible because no machine learning algorithm is consistently the best.</span></p>
<pre>  <br/>  # create Naive Bayes model<br/>  nb &lt;- naiveBayes(as.factor(y) ~., data=data[sample,])<br/>  pr &lt;- predict(nb, newdata=data[test, ])<br/>  # Generate the confusion matrix showing counts.<br/>  tab&lt;-table(na.omit(data[test, ])$y, pr,<br/>             dnn=c("Actual", "Predicted"))<br/>  if (verbose) print (tab)<br/>  nb_acc &lt;- 100*sum(diag(tab))/length(test)<br/>  if (verbose) print(sprintf("Naive Bayes accuracy = %1.2f%%",nb_acc))<br/>  <br/>  # create SVM model<br/>  if (verbose) print ("SVM")<br/>  if (verbose) print (Sys.time())<br/>  ksvm &lt;- ksvm(as.factor(y) ~ .,<br/>               data=data[sample,],<br/>               kernel="rbfdot",<br/>               prob.model=TRUE)<br/>  if (verbose) print (Sys.time())<br/>  pr &lt;- predict(ksvm, newdata=na.omit(data[test, ]))<br/>  # Generate the confusion matrix showing counts.<br/>  tab&lt;-table(na.omit(data[test, ])$y, pr,<br/>             dnn=c("Actual", "Predicted"))<br/>  if (verbose) print (tab)<br/>  svm_acc &lt;- 100*sum(diag(tab))/length(test)<br/>  if (verbose) print(sprintf("SVM accuracy = %1.2f%%",svm_acc))<br/>  <br/>  # create Neural Network model<br/>  rm(pr,tab)<br/>  set.seed(199)<br/>  if (verbose) print ("Neural Network")<br/>  if (verbose) print (Sys.time())<br/>  nnet &lt;- nnet(as.factor(y) ~ .,<br/>               data=data[sample,],<br/>               size=10, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)<br/>  if (verbose) print (Sys.time())<br/>  pr &lt;- predict(nnet, newdata=data[test, ], type="class")<br/>  # Generate the confusion matrix showing counts.<br/>  tab&lt;-table(data[test, ]$y, pr,<br/>             dnn=c("Actual", "Predicted"))<br/>  if (verbose) print (tab)<br/>  nn_acc &lt;- 100*sum(diag(tab))/length(test)<br/>  if (verbose) print(sprintf("Neural Network accuracy = %1.2f%%",nn_acc))<br/>  <br/>  # create Random Forest model<br/>  rm(pr,tab)<br/>  if (verbose) print ("Random Forest")<br/>  if (verbose) print (Sys.time())<br/>  rf_model&lt;-randomForest(as.factor(y) ~., data=data[sample,])<br/>  if (verbose) print (Sys.time())<br/>  pr &lt;- predict(rf_model, newdata=data[test, ], type="class")<br/>  # Generate the confusion matrix showing counts.<br/>  tab&lt;-table(data[test, ]$y, pr,<br/>             dnn=c("Actual", "Predicted"))<br/>  if (verbose) print (tab)<br/>  rf_acc &lt;- 100*sum(diag(tab))/length(test)<br/>  if (verbose) print(sprintf("Random Forest accuracy = %1.2f%%",rf_acc))<br/>  <br/>  dfParams &lt;- data.frame(w,stem,stop)<br/>  dfParams$nb_acc &lt;- nb_acc<br/>  dfParams$svm_acc &lt;- svm_acc<br/>  dfParams$nn_acc &lt;- nn_acc<br/>  dfParams$rf_acc &lt;- rf_acc<br/>  <br/>  return(dfParams)<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We now call the function with different hyperparameters in the following code:</p>
<pre>dfResults &lt;- TextClassification("tfidf",verbose=1) # tf-idf, no stemming<br/>dfResults&lt;-rbind(dfResults,TextClassification("tf",verbose=1)) # tf, no stemming<br/>dfResults&lt;-rbind(dfResults,TextClassification("binary",verbose=1)) # binary, no stemming<br/><br/>dfResults&lt;-rbind(dfResults,TextClassification("tfidf",1,verbose=1)) # tf-idf, stemming<br/>dfResults&lt;-rbind(dfResults,TextClassification("tf",1,verbose=1)) # tf, stemming<br/>dfResults&lt;-rbind(dfResults,TextClassification("binary",1,verbose=1)) # binary, stemming<br/><br/>dfResults&lt;-rbind(dfResults,TextClassification("tfidf",0,1,verbose=1)) # tf-idf, no stemming, remove stopwords<br/>dfResults&lt;-rbind(dfResults,TextClassification("tf",0,1,verbose=1)) # tf, no stemming, remove stopwords<br/>dfResults&lt;-rbind(dfResults,TextClassification("binary",0,1,verbose=1)) # binary, no stemming, remove stopwords<br/><br/>dfResults&lt;-rbind(dfResults,TextClassification("tfidf",1,1,verbose=1)) # tf-idf, stemming, remove stopwords<br/>dfResults&lt;-rbind(dfResults,TextClassification("tf",1,1,verbose=1)) # tf, stemming, remove stopwords<br/>dfResults&lt;-rbind(dfResults,TextClassification("binary",1,1,verbose=1)) # binary, stemming, remove stopwords<br/><br/>dfResults[, "best_acc"] &lt;- apply(dfResults[, c("nb_acc","svm_acc","nn_acc","rf_acc")], 1, max)<br/>dfResults &lt;- dfResults[order(-dfResults$best_acc),]<br/>dfResults<br/><br/>strResult &lt;- sprintf("Best accuracy score was %1.2f%%. Hyper-parameters: ",dfResults[1,"best_acc"])<br/>strResult &lt;- paste(strResult,dfResults[1,"w"],",",sep="")<br/>strResult &lt;- paste(strResult,<br/>                   ifelse(dfResults[1,"stem"] == 0,"no stemming,","stemming,"))<br/>strResult &lt;- paste(strResult,<br/>                   ifelse(dfResults[1,"stop"] == 0,"no stop word processing,","removed stop words,"))<br/>if (dfResults[1,"best_acc"] == dfResults[1,"nb_acc"]){<br/>  strResult &lt;- paste(strResult,"Naive Bayes model")<br/>} else if (dfResults[1,"best_acc"] == dfResults[1,"svm_acc"]){<br/>  strResult &lt;- paste(strResult,"SVM model")<br/>} else if (dfResults[1,"best_acc"] == dfResults[1,"nn_acc"]){<br/>  strResult &lt;- paste(strResult,"Neural Network model")<br/>}else if (dfResults[1,"best_acc"] == dfResults[1,"rf_acc"]){<br/>  strResult &lt;- paste(strResult,"Random Forest model")<br/>}<br/><br/>print (strResult)</pre>
<p>For each combination of hyperparameters, the script saves the best score from the four machine learning algorithms in the <kbd>best_acc</kbd> field. Once the training is complete, we can look at the results:</p>
<pre>&gt; dfResults<br/> w stem stop nb_acc svm_acc nn_acc rf_acc best_acc<br/><strong>12 binary    1    1   86.06   95.24   90.52   94.26     95.24</strong><br/>9  binary    0    1   87.71   95.15   90.52   93.72     95.15<br/>10 tfidf     1    1   91.99   95.15   91.05   94.17     95.15<br/>3  binary    0    0   85.98   95.01   90.29   93.99     95.01<br/>6  binary    1    0   84.59   95.01   90.34   93.63     95.01<br/>7  tfidf     0    1   91.27   94.43   94.79   93.54     94.79<br/>11 tf        1    1   77.47   94.61   92.30   94.08     94.61<br/>4  tfidf     1    0   92.25   94.57   90.96   93.99     94.57<br/>5  tf        1    0   75.11   94.52   93.46   93.90     94.52<br/>1  tfidf     0    0   91.54   94.26   91.59   93.23     94.26<br/>2  tf        0    0   75.82   94.03   91.54   93.59     94.03<br/>8  tf        0    1   78.14   94.03   91.63   93.68     94.03<br/><br/>&gt; print (strResult)<br/>[1] "Best accuracy score was 95.24%. Hyper-parameters: binary, stemming, removed stop words, SVM model"</pre>
<p><span>The results are ordered by best results, so here we can can see that our best accuracy overall </span>was<span> </span><kbd>95.24%</kbd><span>. </span>The reason for training so many models is that there is no right formula for traditional NLP tasks that's work for most cases, so you should try multiple combinations of preprocessing and different algorithms, as we have done here. For example, if you searched for an example online on text classification, you could find an example that would suggest to use tf-idf and naive bayes. Here, we can see that it is one of the worst performers.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning text classification</h1>
                </header>
            
            <article>
                
<p>The previous code ran 48 traditional machine learning algorithms over the data across a number of different hyperparameters. Now, it is time to see if we can find a deep learning model that outperforms them. The first deep learning model is in <kbd>Chapter7/classify_keras1.R</kbd>. The first part of the code loads the data. The tokens in the <span>reuters dataset</span> are ranked by how often they occur (in the training set) and the <kbd><span>max_features</span></kbd> parameter controls how many distinct tokens will be used in the model. We will use all the tokens by setting this to the number of entries in the word index. The maxlen parameter controls the length of the input sequences to the model, they must all be the same length. If the sequences are longer than the maxlen variable, they are truncated, if they are shorter, then padding is added to make the length=<span>maxlen. We set this to 250, which means our deep learning model expects 250 tokens as input per instance:</span></p>
<pre>library(keras)<br/><br/>set.seed(42)<br/>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 250<br/>skip_top = 0<br/><br/>reuters &lt;- dataset_reuters(num_words = max_features,skip_top = skip_top)<br/>c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% reuters<br/>x_train &lt;- pad_sequences(x_train, maxlen = maxlen)<br/>x_test &lt;- pad_sequences(x_test, maxlen = maxlen)<br/>x_train &lt;- rbind(x_train,x_test)<br/>y_train &lt;- c(y_train,y_test)<br/>table(y_train)<br/><br/>y_train[y_train!=3] &lt;- 0<br/>y_train[y_train==3] &lt;- 1<br/>table(y_train)</pre>
<p>The next section of code builds the model:</p>
<pre>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %&gt;%<br/>  layer_flatten() %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;% <br/>  layer_dense(units = 16, activation = 'relu') %&gt;%<br/>  layer_dropout(rate = 0.5) %&gt;% <br/>  layer_dense(units = 16, activation = 'relu') %&gt;%<br/>  layer_dropout(rate = 0.5) %&gt;% <br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>model %&gt;% compile(<br/>  optimizer = "rmsprop",<br/>  loss = "binary_crossentropy",<br/>  metrics = c("acc")<br/>)<br/>summary(model)<br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 5,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p>The only thing in this code that we have not seen before is <kbd>layer_embedding</kbd>.  This takes the input and creates an embedding layer, which is a vector of numbers for each input token. We will describe word vectors in more detail in the next section. Another thing to note is that we don't preprocess the text or create any features <span>– </span>we just feed in the word indices and let the deep learning algorithm make sense of it. Here is the output of the script as the model is trained:</p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/5<br/>8982/8982 [==============================] - 3s 325us/step - loss: 0.4953 - acc: 0.7674 - val_loss: 0.2332 - val_acc: 0.9274<br/>Epoch 2/5<br/>8982/8982 [==============================] - 3s 294us/step - loss: 0.2771 - acc: 0.9235 - val_loss: 0.1990 - val_acc: 0.9394<br/>Epoch 3/5<br/>8982/8982 [==============================] - 3s 297us/step - loss: 0.2150 - acc: 0.9414 - val_loss: 0.1975 - val_acc: 0.9497<br/>Epoch 4/5<br/>8982/8982 [==============================] - 3s 282us/step - loss: 0.1912 - acc: 0.9515 - val_loss: 0.2118 - val_acc: 0.9461<br/>Epoch 5/5<br/>8982/8982 [==============================] - 3s 280us/step - loss: 0.1703 - acc: 0.9584 - val_loss: 0.2490 - val_acc: 0.9466</pre>
<p>Despite the simplicity of the code, we get 94.97% accuracy on the validation set after just three epochs, which is only 0.27% less than the best traditional NLP approach. Now, it is time to discuss word vectors in more detail.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word vectors</h1>
                </header>
            
            <article>
                
<p class="mce-root">Instead of representing our text data as a bag of words, deep learning represents them as word vectors or<span> </span>embeddings. A vector/embedding is nothing more than a series of numbers that represent a word. You may have already<span> </span>heard of popular word vectors such as <span>Word2Vec and GloVe</span>. The Word2vec model was invented by Google (<em>Mikolov, Tomas, et al. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013)</em>). In their paper, they provide examples of how these word vectors have somewhat mysterious and magical properties. If you take the vector of the word "<em>King</em>", subtract the vector of the word "<em>Man</em>", add the vector of the word "<em>Man</em>", then you get a value close to the vector of the word "<em>Queen"</em>. Other similarities also exist, for example:</p>
<ul>
<li class="mce-root"><em>vector('King') - vector('Man') + vector('Woman') = vector('Queen')</em></li>
<li class="mce-root"><em>vector('Paris') - vector('France') + vector('Italy') = vector('Rome')</em></li>
</ul>
<p class="mce-root">If this is the first time you have seen Word2Vec, then you are probably somewhat amazed by this. I know I was! These examples imply that word vectors <em>understand</em> language, so have we solved natural language processing? The answer is no <span>–</span> we are very far away from this. Vectors are learned from collections of text documents. In fact, the very first layer in our deep learning model is an embedding layer which creates a vector space for the words. Let's look at some of the code from <kbd>Chapter7/classify_keras.R</kbd> again:</p>
<pre>library(keras)<br/><br/>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>max_features<br/>[1] 30979<br/>.......<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/> layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %&gt;%<br/>.......<br/><br/>summary(model)<br/>_______________________________________________________________________________________<br/>Layer (type)                Output Shape         Param # <br/>=======================================================================================<br/>embedding_1 (Embedding)     (None, 150, 16)      495664<br/>.......</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The value for <kbd>max_features</kbd> is <kbd>30979</kbd>, that is, we have <kbd>30979</kbd> unique features. These features are<span> </span><strong>tokens</strong>, or words. In the traditional text classification, we had almost the same number of <span>unique tokens (<kbd>30538</kbd>). The difference between these two numbers is not important; it is due to the different tokenization processes used between the two approaches, that is, how the documents were split into tokens. The embedding layer has <kbd>495664</kbd> parameters, which is <em>30,979 x 16</em>, that is, each unique feature/token is represented by a vector of <kbd>16</kbd> numbers. The word vectors or embeddings learned by deep learning algorithms will have some of the characteristics described earlier, for example:</span></p>
<ul>
<li><span>Synonyms (two words that have the same meaning) will have very similar word vectors</span></li>
<li>Words from the same semantic collection will be clustered (<span>for example,</span> colors, days of the week, makes of cars, and so on)</li>
<li>The vector space between related words can signify the relationship between those words (<span>for example,</span> gender for w(King) <span>–</span> w(Queen))</li>
</ul>
<p>The embedding layer creates word vectors/embeddings based on words and their surrounding words. The word vectors end up having these characteristics because of a simple fact, which can be summarized by a quote from John Firth, an English linguist in 1957:</p>
<div class="mce-root packt_quote">"You shall know a word by the company it keeps."</div>
<p>The deep learning algorithm learns the vectors for each word by looking at surrounding words and therefore learns some of the context. When it sees the word <em>King</em>, some words near this word may indicate gender, for example, The <em>King</em><span> </span>picked up <em>his</em><span> </span>sword. Another sentence could be The<span> </span><em>Queen</em><span> </span>looked in <em>her</em><span> </span>mirror. The word vectors for <em>King</em> and <em>Queen</em> have a latent gender component that is learned from the words<span> </span><span>surrounding <em>King</em> and <em>Queen</em> </span>in the data. But it is important to realize that the deep learning algorithm has no concept of what gender is, or what type of entities it applies to. Even so, word vectors are a huge improvement over bag-of-word approaches which have no way of identifying relationships between different tokens. Using word vectors also means that we do not have to discard sparse terms. Finally, as the number of unique tokens increases,<span> </span><span>they are much more efficient to process than bag-of-words approaches</span>.</p>
<p>We will look at embeddings again in <a href="e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml">Chapter 9</a>, <em>Anomaly Detection and Recommendation Systems</em>, when we use them in auto-encoders. Now that we have seen some traditional machine learning and deep learning approaches for solving this problem, it is time to compare them in more detail.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing traditional text classification and deep learning</h1>
                </header>
            
            <article>
                
<p>The <span>traditional text classification performed a number of preprocessing steps, including word stemming, stop-word processing, and feature generation (tf-idf, tf or binary). The deep learning text classification did not need this preprocessing. You may have heard various reasons for this previously:</span></p>
<ul>
<li><span>Deep learning can learn features automatically, so feature creation is not needed</span></li>
<li>Deep learning algorithms for NLP tasks requires far less preprocessing than <span>traditional </span><span>text classification</span></li>
</ul>
<p>There is some truth to this, but this does not answer why we need complex feature generation in <span>traditional </span><span>text classification. A big </span>reason that preprocessing is needed in <span>traditional </span><span>text classification is to overcome a fundamental problem.</span></p>
<p>For some traditional NLP approaches (for example, classification), text preprocessing is not just about creating better features. It is also necessary because the bag-of-words representation creates a sparse high-dimensional dataset. Most machine learning algorithms have problems with such datasets, meaning that we have to reduce the dimensionality before applying machine learning algorithms. Proper preprocessing of the text is an essential part of this to ensure that relevant data is not thrown away.</p>
<p>For <span>traditional </span>text classification, we used an approach called <strong>bag-of-words</strong>. This is essentially one-hot encoding each <em><strong>token</strong></em> (word). Each column represents a single token, and the value of each cell is one of the following:</p>
<ul>
<li>A <strong>tf-idf</strong> (<strong>term frequency, inverse document frequency</strong>) for that token</li>
<li>The term frequency, that is, the count of how many times that token occurs for that document/instance</li>
<li>A binary flag, that is, one if the <span>token is in that document/instance; otherwise, it is zero</span></li>
</ul>
<p><span>You may not have heard of <em>tf-idf</em> before. It measures the importance of a token by calculating the term frequency (<em>tf</em>) of the token in the document (such as how many times it occurs in the document) divided by the log of how many times it appears in the entire corpus (<em>idf</em>). The <strong>corpus</strong> is the entire collection of documents. The <em>tf</em> part measures how important the token is within a single document, and the <em>idf</em> measures how unique the token is among all the documents. If the token appears many times in the document, but also many times in other documents, then it is unlikely to be useful for categorizing documents. If the token appears in only a few documents, then it is a potentially valuable feature for the classification task.</span></p>
<p class="mce-root"/>
<p>Our traditional <span>text classification approach also used <em>stemming</em> and processed <em>stop-words</em>. Indeed, our best result in traditional text classification used both approaches. Stemming tries to reduce words to their word stem or root form, which reduces the vocabulary size. It also means that words with the same meaning but with different verb tenses or noun forms are standardized to the same token. Here is an example of stemming. Note that 6/7 of the input words have the same output value:</span></p>
<pre>library(corpus)<br/>text &lt;- "love loving lovingly loved lover lovely love"<br/>text_tokens(text, stemmer = "en") # english stemmer<br/>[[1]]<br/>[1] "love" "love" "love" "love" "lover" "love" "love" </pre>
<p>Stop-words are common words that appear in most documents for a language. They occur so frequently in most documents that they are almost never useful for machine learning. The following example shows the list of stop-words for the English language:</p>
<pre>library(tm)<br/>&gt; stopwords()<br/> [1] "i" "me" "my" "myself" "we" "our" <br/> [7] "ours" "ourselves" "you" "your" "yours" "yourself" <br/> [13] "yourselves" "he" "him" "his" "himself" "she"<br/> [19] "her" "hers" "herself" "it" "its" "itself" <br/> [25] "they" "them" "their" "theirs" "themselves" "what" <br/>.........</pre>
<p>The final piece we want to cover in <span>traditional NLP is how</span> <span>it deals with sparse terms. Recall from earlier that traditional NLP uses a bag-of-words approach, where each unique token gets an individual column. For a large collection of documents, there will be thousands of unique tokens, and since most tokens will not appear in an individual document, this a very sparse representation, that is, most cells are empty. We can check this by looking at taking some of the code from <kbd>classify_text.R</kbd>, modifying it slightly, and looking at the <kbd>dtm</kbd> and <kbd>dtm2</kbd> variables:</span></p>
<pre>library(tm)<br/>df &lt;- read.csv("../data/reuters.train.tab", sep="\t", stringsAsFactors = FALSE)<br/>df2 &lt;- read.csv("../data/reuters.test.tab", sep="\t", stringsAsFactors = FALSE)<br/>df &lt;- rbind(df,df2)<br/><br/>df[df$y!=3,]$y&lt;-0<br/>df[df$y==3,]$y&lt;-1<br/>rm(df2)<br/><br/>corpus &lt;- Corpus(DataframeSource(data.frame(df[, 2])))<br/>corpus &lt;- tm_map(corpus, content_transformer(tolower))<br/><br/>dtm &lt;- DocumentTermMatrix(corpus,control=list(weighting=weightBin))<br/><br/># keep terms that cover 95% of the data<br/>dtm2&lt;-removeSparseTerms(dtm, 0.95)<br/><br/>dtm<br/><strong>&lt;&lt;DocumentTermMatrix (documents: 11228, terms: 30538)&gt;&gt;</strong><br/><strong>Non-/sparse entries: 768265/342112399</strong><br/><strong>Sparsity : 100%</strong><br/><strong>Maximal term length: 24</strong><br/><strong>Weighting : binary (bin)</strong><br/><br/>dtm2<br/><strong>&lt;&lt;DocumentTermMatrix (documents: 11228, terms: 230)&gt;&gt;</strong><br/><strong>Non-/sparse entries: 310275/2272165</strong><br/><strong>Sparsity : 88%</strong><br/><strong>Maximal term length: 13</strong><br/><strong>Weighting : binary (bin)</strong></pre>
<p><span>We can see that our first document-term matrix (dtm) has 11,228 documents and 30,538 unique tokens. In this document-term matrix, only 768,265 (0.22%) cells have values. Most machine learning algorithms would struggle with such a high-dimensionality sparse data frame. If you tried using these machine learning algorithms (for example, SVM, random forest, naive bayes) on a data frame with 30,538 dimensions, they would fail to run in R (I tried!). This is a known problem in traditional NLP, so there is a function (<kbd>removeSparseTerms</kbd>) in the NLP libraries to remove sparse terms from the document-term matrix. This removes columns that have the most empty cells. We can see the effect of this, as the second document-term matrix has only 230 unique tokens and 310,275 (12%) cells have values. This dataset is still relatively sparse, but it is in a usable format for machine learning.</span></p>
<p>This highlights the problem with traditional NLP approaches: the <em>bag-of-words</em> approach creates a very sparse high-dimensional dataset which is not usable by machine learning algorithms. Therefore, you need to remove some of the dimensions, and this results in a number of cells with values going from <span>768,265 to 310,275 in our example. We </span>threw away almost 60% of the data before applying any machine learning! This also explains why text preprocessing, such as stemming and stop-word removal, is used in <span>traditional </span><span>NLP. Stemming helps to reduce the vocabulary and standardize terms by combining variations of many words into one form.</span></p>
<p class="mce-root"/>
<p><span>By combining variations, it means they are more likely to survive the culling of data. We process stop-words for the opposite reason: if we don't remove stop-words, these terms will probably be kept after removing sparse terms. There are 174 terms in the <kbd>stopwords()</kbd> function in the <kbd>tm</kbd> package. If the reduced dataset had many of these terms, then they would probably not be useful as predictor variables due to their commonality throughout the documents.</span></p>
<p>It is also worth noting that this is a very small dataset in NLP terms. We only have 11,228 documents and <span>30,538 unique tokens. A larger <em><strong>corpus</strong></em> (collection of text documents) could have half a million unique tokens. In order to reduce the number of tokens to something that could be processed in R, we would have to throw away a lot more data.</span></p>
<p>When we use a deep learning approach for NLP, we represent the data as word vectors/embeddings rather than using the bag-of-words approach in traditional NLP. This is much more efficient, so do not have to preprocess data to remove common words, reduce words to a simpler form, or <span>reduce the number of terms before applying the deep learning algorithm. The only thing we do have to do is pick an embedding size and a max length size for the number of tokens we process for each instance. This is needed because deep learning algorithms cannot use variable length sequences as inputs to a layer. When instances have more tokens than the max length, they are truncated and when instances have less tokens than the max length, they are padded.</span></p>
<p>After all of this, you may be wondering <span>why the deep learning algorithm did not outperform </span>the <span>traditional </span>NLP approach significantly, if the <span>traditional </span><span>NLP approach throws away 60% of the data. There are a few reasons for this:</span></p>
<ul>
<li>The dataset is small. If we had more data, the deep learning approach would improve at a faster rate than the <span>traditional </span>NLP approach.</li>
<li>Certain NLP tasks such as document classification and sentiment analysis depend on a very small set of terms. For example, to differentiate between sports news and financial news, maybe 50 selected terms would be sufficient to get over 90% accuracy. Recall that the function to remove sparse terms in the traditional text classification approach <span>–</span> this works because it assumes (correctly) that non-sparse terms will be useful features for the machine learning algorithms.</li>
<li>We ran 48 machine learning algorithms and only one deep learning approach, which was relatively simple! We will soon come across approaches that beat the <span>traditional </span><span>NLP approach.</span></li>
</ul>
<p>This book has really only touched the surface of <span>traditional </span>NLP approaches. Entire books have been written on the subject. The purpose of looking at these approaches is to show how brittle these approaches can be. The deep learning approach is much simpler to understand and has far fewer settings. It does not involve preprocessing the text or creating features based on weightings such as tf-idf. Even so, our first <span>deep learning approach </span>is not very far away from the best model out of 48 models in traditional text classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced deep learning text classification</h1>
                </header>
            
            <article>
                
<p class="mce-root">Our basic deep learning model is much less complex than the traditional machine learning approach, but its performance is not quite as good. This section looks at some advanced techniques for <span>text classification </span>in deep learning. The following sections explain a number of different approaches and focus on code examples rather than heavy deep explanations. If you are interested in more detail, then look at the book <span><em>Deep Learning</em> by Goodfellow, Bengio, and Courville (</span><em>Goodfellow, Ian, et al. Deep learning. Vol. 1. Cambridge: MIT Press, 2016.</em>). Another good reference that covers NLP in deep learning is a book by Yoav Goldberg (<em>Goldberg, Yoav. Neural network methods for natural language processing</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">1D convolutional neural network model</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have seen that the bag-of-words approach in traditional NLP approaches ignores sentence structure. Consider applying a sentiment analysis task on the four movie reviews in the <span>following table</span>:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 12%">
<p><strong>Id</strong></p>
</td>
<td style="width: 44.4327%">
<p><strong>sentence</strong></p>
</td>
<td style="width: 88.5673%">
<p><strong>Rating (1=recommended, 0=not recommended)</strong></p>
</td>
</tr>
<tr>
<td style="width: 12%">
<p><span>1</span></p>
</td>
<td style="width: 44.4327%">
<p><span>this movie is very good</span></p>
</td>
<td style="width: 88.5673%">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 12%">
<p><span>2</span></p>
</td>
<td style="width: 44.4327%">
<p><span>this movie is not good</span></p>
</td>
<td style="width: 88.5673%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 12%">
<p><span>3</span></p>
</td>
<td style="width: 44.4327%">
<p><span>this movie is not very good</span></p>
</td>
<td style="width: 88.5673%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 12%">
<p><span>4</span></p>
</td>
<td style="width: 44.4327%">
<p>this movie is not bad</p>
</td>
<td style="width: 88.5673%">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>If we represent this as a bag of words with term frequency, we will get the following output:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 10%">
<p><strong>Id</strong></p>
</td>
<td style="width: 10%">
<p><strong>bad</strong></p>
</td>
<td style="width: 10%">
<p><strong>good</strong></p>
</td>
<td style="width: 10%">
<p><strong>is</strong></p>
</td>
<td style="width: 10%">
<p><strong>movie</strong></p>
</td>
<td style="width: 10%">
<p><strong>not</strong></p>
</td>
<td style="width: 10%">
<p><strong>this</strong></p>
</td>
<td style="width: 10%">
<p><strong>very</strong></p>
</td>
</tr>
<tr>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 10%">
<p>2</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 10%">
<p>3</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 10%">
<p>4</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In this simple example, we can see some of the problems with a bag-of-words approach, we have lost the relationship between the negation (<strong>not</strong>) and the adjectives (<strong>good</strong>, <strong>bad</strong>). <span>To work around this problem, traditional NLP could use bigrams, so instead of using single words as tokens, use two words as tokens. Now, for the second example, <strong>not good</strong> is a single token, which makes it more likely that the machine learning algorithm will pick it up. However, we still have a problem with the third example (<strong>not very good</strong>), where we will have tokens for <strong>not very</strong> and <strong>very good</strong>. These are still ambiguous, as <strong>not very</strong> implies negative sentiment, while <strong>very good</strong> implies positive sentiment. We could try higher order n-grams, but this further exacerbates the sparsity problem we saw in the previous section.</span></p>
<p>Word vectors or embeddings have the same problem. We need some method to handle word sequences. Fortunately, there are types of layers in deep learning algorithms that can handle sequential data. One that we have already seen is convolutional neural networks in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>,<em> Image Classification Using Convolutional Neural Networks</em>. Recall that these are 2D patches that are moved across the image to identify patterns such as a diagonal line or an edge. In a similar manner, we can apply a 1D convolutional <span>neural network across the word vectors. Here is an example of using a 1D convolutional neural network layer for the same text classification problem. The code is in <kbd>Chapter7/classify_keras2.R</kbd>. We are only showing the code for the model architecture, because that is the only change from the code in <kbd>Chapter7/classify_keras1.R</kbd>:</span></p>
<pre>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 16,input_length = maxlen) %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  layer_conv_1d(64,5, activation = "relu") %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  layer_max_pooling_1d() %&gt;%<br/>  layer_flatten() %&gt;%<br/>  layer_dense(units = 50, activation = 'relu') %&gt;%<br/>  layer_dropout(rate = 0.6) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")</pre>
<p>We can see that this follows the same pattern that we saw in the image data; we have a convolutional layer followed by a max pooling layer. There are 64 convolutional layers with a <kbd>length=5</kbd>, and so these <em>learn</em> local patterns in the data. Here is the output from the model's training:</p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/5<br/>8982/8982 [==============================] - 13s 1ms/step - loss: 0.3020 - acc: 0.8965 - val_loss: 0.1909 - val_acc: 0.9470<br/>Epoch 2/5<br/>8982/8982 [==============================] - 13s 1ms/step - loss: 0.1980 - acc: 0.9498 - val_loss: 0.1816 - val_acc: 0.9537<br/>Epoch 3/5<br/>8982/8982 [==============================] - 12s 1ms/step - loss: 0.1674 - acc: 0.9575 - val_loss: 0.2233 - val_acc: 0.9368<br/>Epoch 4/5<br/>8982/8982 [==============================] - 12s 1ms/step - loss: 0.1587 - acc: 0.9606 - val_loss: 0.1787 - val_acc: 0.9573<br/>Epoch 5/5<br/>8982/8982 [==============================] - 12s 1ms/step - loss: 0.1513 - acc: 0.9628 - val_loss: 0.2186 - val_acc: 0.9408</pre>
<p>This model is an improvement on our previous deep learning model; it gets 95.73% accuracy on the fourth epoch. This beats the<span> </span>traditional NLP approach by 0.49%, which is a significant improvement. Let's move on to other methods that also look to matching sequences. We will start with <strong>recurrent neural networks</strong> (<strong>RNNs</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural network model</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>The deep learning networks we have seen so far have no concept of memory. Every new piece of information is treated as atomic and has no relation to what has already occurred. But, sequences are important in time series and text classification, especially sentiment analysis. In the previous section, we saw how word structure and order matters, and we used CNNs to resolve this. While this approach worked, it does not resolve the problem completely as we still must pick a filter size, which limits the range of the layer. Recurrent neural networks are deep learning layers which are used to solve this problem. They are networks with feedback loops that allow information to flow and therefore are able to <em>remember</em> important features:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-603 image-border" src="assets/50985a6f-072d-41b1-ad78-6a456fa9d8f1.png" style="width:26.92em;height:16.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7.1: A recurrent neural network</div>
<p>In the preceding diagram, we can see an example of a<span> recurrent neural network. Each piece of information (X<sub>o</sub>, X<sub>1</sub>, X<sub>2</sub>) is fed into a node which predicts <em>y</em> variables. The predicted value is also passed to the next node as input, thus preserving some sequence information.</span></p>
<p class="mce-root"><span>Our first RNN model is in <kbd>Chapter7/classify_keras3.R</kbd>. We have to change some of the parameters for the model: we must decrease the number of features used to 4,000, our max length to 100, and drop the most common 100 tokens. We must also increase the size of the embedding layer to 32 and run it for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>max_features &lt;- 4000<br/>maxlen &lt;- 100<br/>skip_top = 100<br/><br/>........<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_spatial_dropout_1d(rate = 0.25) %&gt;%<br/>  layer_simple_rnn(64,activation = "relu", dropout=0.2) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>........<br/><br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)<br/><br/></pre>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 4s 409us/step - loss: 0.5289 - acc: 0.7848 - val_loss: 0.3162 - val_acc: 0.9078<br/>Epoch 2/10<br/>8982/8982 [==============================] - 4s 391us/step - loss: 0.2875 - acc: 0.9098 - val_loss: 0.2962 - val_acc: 0.9305<br/>Epoch 3/10<br/>8982/8982 [==============================] - 3s 386us/step - loss: 0.2496 - acc: 0.9267 - val_loss: 0.2487 - val_acc: 0.9234<br/>Epoch 4/10<br/>8982/8982 [==============================] - 3s 386us/step - loss: 0.2395 - acc: 0.9312 - val_loss: 0.2709 - val_acc: 0.9332<br/>Epoch 5/10<br/>8982/8982 [==============================] - 3s 381us/step - loss: 0.2259 - acc: 0.9336 - val_loss: 0.2360 - val_acc: 0.9270<br/>Epoch 6/10<br/>8982/8982 [==============================] - 3s 381us/step - loss: 0.2182 - acc: 0.9348 - val_loss: 0.2298 - val_acc: 0.9341<br/>Epoch 7/10<br/>8982/8982 [==============================] - 3s 383us/step - loss: 0.2129 - acc: 0.9380 - val_loss: 0.2114 - val_acc: 0.9390<br/>Epoch 8/10<br/>8982/8982 [==============================] - 3s 382us/step - loss: 0.2128 - acc: 0.9341 - val_loss: 0.2306 - val_acc: 0.9359<br/>Epoch 9/10<br/>8982/8982 [==============================] - 3s 378us/step - loss: 0.2053 - acc: 0.9382 - val_loss: 0.2267 - val_acc: 0.9368<br/>Epoch 10/10<br/>8982/8982 [==============================] - 3s 385us/step - loss: 0.2031 - acc: 0.9389 - val_loss: 0.2204 - val_acc: 0.9368</pre>
<p>The best validation accuracy was after epoch 7, where we got 93.90% accuracy, which is not as good as the CNN model. One of the problems with simple RNN models is that it is difficult to maintain context as the gap grows between the different pieces of information. Let's move onto a more complex model, that is, the LSTM model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long short term memory model</h1>
                </header>
            
            <article>
                
<p><span>LSTMs are designed to learn long-term dependencies. Similar to RNNs, they are chained and have four internal neural network layers. They split the state into two parts, where one part manages short-term state and the other adds long-term state. LSTMs have <em>gates</em> which control how <em>memories</em> are stored. The input gate controls which part of the input should be added to the long-term memory. The forget gate controls the part of long-term memory that should be forgotten. The final gate, the output gate, controls which part of the long-term memory should be in the output. This is a brief description of LSTMs – a good reference for more details is <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</span></p>
<p><span>The code for our LSTM model is in <kbd>Chapter7/classify_keras4.R</kbd>. The parameters for the model are max length=150, the size of the embedding layer=32, and the model was trained for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 150<br/>skip_top = 0<br/><br/>.........<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  layer_lstm(128,dropout=0.2) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>.........<br/><br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 25s 3ms/step - loss: 0.3238 - acc: 0.8917 - val_loss: 0.2135 - val_acc: 0.9394<br/>Epoch 2/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.2465 - acc: 0.9206 - val_loss: 0.1875 - val_acc: 0.9470<br/>Epoch 3/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1815 - acc: 0.9493 - val_loss: 0.2577 - val_acc: 0.9408<br/>Epoch 4/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1691 - acc: 0.9521 - val_loss: 0.1956 - val_acc: 0.9501<br/>Epoch 5/10<br/>8982/8982 [==============================] - 25s 3ms/step - loss: 0.1658 - acc: 0.9507 - val_loss: 0.1850 - val_acc: 0.9537<br/>Epoch 6/10<br/>8982/8982 [==============================] - 25s 3ms/step - loss: 0.1658 - acc: 0.9508 - val_loss: 0.1764 - val_acc: 0.9510<br/>Epoch 7/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1659 - acc: 0.9522 - val_loss: 0.1884 - val_acc: 0.9466<br/>Epoch 8/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1548 - acc: 0.9556 - val_loss: 0.1900 - val_acc: 0.9479<br/>Epoch 9/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1562 - acc: 0.9548 - val_loss: 0.2035 - val_acc: 0.9461<br/>Epoch 10/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1508 - acc: 0.9567 - val_loss: 0.2052 - val_acc: 0.9470</pre>
<p><span>The best validation accuracy was after epoch 5, when we got 95.37% accuracy, which is a big improvement on the simple RNN model, although still not as good as the CNN model. We will cover GRU cells next, which are a similar concept to </span>LSTM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gated Recurrent Units model</h1>
                </header>
            
            <article>
                
<p><strong>Gated recurrent units (GRUs)</strong> are similar to LSTM cells but simpler. They have one gate that combines the forget and input gates in LSTM, and there is no output gate. While GRUs are simpler than LSTMs and therefore quicker to train, it is a matter of debate on whether they are better than LSTMs, as the research is inconclusive. Therefore, it is recommended to try both, as the results of your task may vary. <span>The code for our GRU model is in <kbd>Chapter7/classify_keras5.R</kbd>. The parameters for the model are max length=150, the size of the embedding layer=32, and the model was trained for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 250<br/>skip_top = 0<br/><br/>...........<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  layer_gru(128,dropout=0.2) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>...........<br/>  <br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.3231 - acc: 0.8867 - val_loss: 0.2068 - val_acc: 0.9372<br/>Epoch 2/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.2084 - acc: 0.9381 - val_loss: 0.2065 - val_acc: 0.9421<br/>Epoch 3/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1824 - acc: 0.9454 - val_loss: 0.1711 - val_acc: 0.9501<br/>Epoch 4/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1656 - acc: 0.9515 - val_loss: 0.1719 - val_acc: 0.9550<br/>Epoch 5/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1569 - acc: 0.9551 - val_loss: 0.1668 - val_acc: 0.9541<br/>Epoch 6/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1477 - acc: 0.9570 - val_loss: 0.1667 - val_acc: 0.9555<br/>Epoch 7/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1441 - acc: 0.9605 - val_loss: 0.1612 - val_acc: 0.9581<br/>Epoch 8/10<br/>8982/8982 [==============================] - 36s 4ms/step - loss: 0.1361 - acc: 0.9611 - val_loss: 0.1593 - val_acc: 0.9590<br/>Epoch 9/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1361 - acc: 0.9620 - val_loss: 0.1646 - val_acc: 0.9568<br/>Epoch 10/10<br/>8982/8982 [==============================] - 35s 4ms/step - loss: 0.1306 - acc: 0.9634 - val_loss: 0.1660 - val_acc: 0.9559</pre>
<p><span>The best validation accuracy was after epoch 5, when we got 95.90% accuracy, which is an improvement on the 95.37% we got with LSTM. In fact, this is the best result we have seen so far. In the next section, we will look at bidirectional architectures.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bidirectional LSTM model</h1>
                </header>
            
            <article>
                
<p><span>We saw in <em>Figure 7.1</em> that RNNs (as well as LSTMs and GRUs) are useful because they can pass information forwards. But in NLP tasks, it is also useful to look backwards. For example, the following two strings have the same meaning:</span></p>
<ul>
<li>I went to Berlin in spring</li>
<li>In spring I went to Berlin</li>
</ul>
<p><span>Bidirectional LSTMs can pass information backwards as well as forwards. The code for our bidirectional LSTM model is in <kbd>Chapter7/classify_keras6.R</kbd>. The parameters for the model are max length=150, the size of the embedding layer=32, and the model was trained for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 250<br/>skip_top = 0<br/><br/>..................<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  bidirectional(layer_lstm(units=128,dropout=0.2)) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>..................<br/>  <br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 82s 9ms/step - loss: 0.3312 - acc: 0.8834 - val_loss: 0.2166 - val_acc: 0.9377<br/>Epoch 2/10<br/>8982/8982 [==============================] - 87s 10ms/step - loss: 0.2487 - acc: 0.9243 - val_loss: 0.1889 - val_acc: 0.9457<br/>Epoch 3/10<br/>8982/8982 [==============================] - 86s 10ms/step - loss: 0.1873 - acc: 0.9464 - val_loss: 0.1708 - val_acc: 0.9519<br/>Epoch 4/10<br/>8982/8982 [==============================] - 82s 9ms/step - loss: 0.1685 - acc: 0.9537 - val_loss: 0.1786 - val_acc: 0.9577<br/>Epoch 5/10<br/>8982/8982 [==============================] - 83s 9ms/step - loss: 0.1634 - acc: 0.9531 - val_loss: 0.2094 - val_acc: 0.9310<br/>Epoch 6/10<br/>8982/8982 [==============================] - 82s 9ms/step - loss: 0.1567 - acc: 0.9571 - val_loss: 0.1809 - val_acc: 0.9475<br/>Epoch 7/10<br/>8982/8982 [==============================] - 83s 9ms/step - loss: 0.1499 - acc: 0.9575 - val_loss: 0.1652 - val_acc: 0.9555<br/>Epoch 8/10<br/>8982/8982 [==============================] - 83s 9ms/step - loss: 0.1488 - acc: 0.9586 - val_loss: 0.1795 - val_acc: 0.9510<br/>Epoch 9/10<br/>8982/8982 [==============================] - 83s 9ms/step - loss: 0.1513 - acc: 0.9567 - val_loss: 0.1758 - val_acc: 0.9555<br/>Epoch 10/10<br/>8982/8982 [==============================] - 83s 9ms/step - loss: 0.1463 - acc: 0.9571 - val_loss: 0.1731 - val_acc: 0.9550</pre>
<p><span>The best validation accuracy was after epoch 4, when we got 95.77% accuracy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacked bidirectional model</h1>
                </header>
            
            <article>
                
<p><span>Bidirectional models are good at picking up information from future states that can affect the current state. Stacked bidirectional models allow us to stack multiple LSTM/GRU layers in a similar manner to how we stack multiple convolutional layers in computer vision tasks. The code for our bidirectional LSTM model is in <kbd>Chapter7/classify_keras7.R</kbd>. The parameters for the model are max length=150, the size of the embedding layer=32, and the model was trained for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 250<br/>skip_top = 0<br/><br/>..................<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_dropout(rate = 0.25) %&gt;%<br/>  bidirectional(layer_lstm(units=32,dropout=0.2,return_sequences = TRUE)) %&gt;%<br/>  bidirectional(layer_lstm(units=32,dropout=0.2)) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>..................<br/>  <br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.2854 - acc: 0.9006 - val_loss: 0.1945 - val_acc: 0.9372<br/>Epoch 2/10<br/>8982/8982 [==============================] - 66s 7ms/step - loss: 0.1795 - acc: 0.9511 - val_loss: 0.1791 - val_acc: 0.9484<br/>Epoch 3/10<br/>8982/8982 [==============================] - 69s 8ms/step - loss: 0.1586 - acc: 0.9557 - val_loss: 0.1756 - val_acc: 0.9492<br/>Epoch 4/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1467 - acc: 0.9607 - val_loss: 0.1664 - val_acc: 0.9559<br/>Epoch 5/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1394 - acc: 0.9614 - val_loss: 0.1775 - val_acc: 0.9533<br/>Epoch 6/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1347 - acc: 0.9636 - val_loss: 0.1667 - val_acc: 0.9519<br/>Epoch 7/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1344 - acc: 0.9618 - val_loss: 0.2101 - val_acc: 0.9332<br/>Epoch 8/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1306 - acc: 0.9647 - val_loss: 0.1893 - val_acc: 0.9479<br/>Epoch 9/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1286 - acc: 0.9646 - val_loss: 0.1663 - val_acc: 0.9550<br/>Epoch 10/10<br/>8982/8982 [==============================] - 70s 8ms/step - loss: 0.1254 - acc: 0.9669 - val_loss: 0.1687 - val_acc: 0.9492</pre>
<p><span>The best validation accuracy was after epoch 4, when we got 95.59% accuracy, which is worse than our bidirectional model, which got 95.77% accuracy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bidirectional with 1D convolutional neural network model</h1>
                </header>
            
            <article>
                
<p><span>So far, the best approaches we have seen are from the 1D convolutional neural network model which got 95.73%, and the gated recurrent units model which got 95.90% accuracy. The following code combines them! The code for our bidirectional with 1D convolutional neural network model is in <kbd>Chapter7/classify_keras8.R</kbd>.</span></p>
<p><span>The parameters for the model are max length=150, the size of the embedding layer=32, and the model was trained for 10 epochs:</span></p>
<pre>word_index &lt;- dataset_reuters_word_index()<br/>max_features &lt;- length(word_index)<br/>maxlen &lt;- 250<br/>skip_top = 0<br/><br/>..................<br/><br/>model &lt;- keras_model_sequential() %&gt;%<br/>  layer_embedding(input_dim = max_features, output_dim = 32,input_length = maxlen) %&gt;%<br/>  layer_spatial_dropout_1d(rate = 0.25) %&gt;%<br/>  layer_conv_1d(64,3, activation = "relu") %&gt;%<br/>  layer_max_pooling_1d() %&gt;%<br/>  bidirectional(layer_gru(units=64,dropout=0.2)) %&gt;%<br/>  layer_dense(units = 1, activation = "sigmoid")<br/><br/>..................<br/>  <br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs = 10,<br/>  batch_size = 32,<br/>  validation_split = 0.2<br/>)</pre>
<p><span>Here is the output from the model's training:</span></p>
<pre>Train on 8982 samples, validate on 2246 samples<br/>Epoch 1/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.2891 - acc: 0.8952 - val_loss: 0.2226 - val_acc: 0.9319<br/>Epoch 2/10<br/>8982/8982 [==============================] - 25s 3ms/step - loss: 0.1712 - acc: 0.9505 - val_loss: 0.1601 - val_acc: 0.9586<br/>Epoch 3/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1651 - acc: 0.9548 - val_loss: 0.1639 - val_acc: 0.9541<br/>Epoch 4/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1466 - acc: 0.9582 - val_loss: 0.1699 - val_acc: 0.9550<br/>Epoch 5/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1391 - acc: 0.9606 - val_loss: 0.1520 - val_acc: 0.9586<br/>Epoch 6/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1347 - acc: 0.9626 - val_loss: 0.1626 - val_acc: 0.9550<br/>Epoch 7/10<br/>8982/8982 [==============================] - 27s 3ms/step - loss: 0.1332 - acc: 0.9638 - val_loss: 0.1572 - val_acc: 0.9604<br/>Epoch 8/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1317 - acc: 0.9629 - val_loss: 0.1693 - val_acc: 0.9470<br/>Epoch 9/10<br/>8982/8982 [==============================] - 26s 3ms/step - loss: 0.1259 - acc: 0.9654 - val_loss: 0.1531 - val_acc: 0.9599<br/>Epoch 10/10<br/>8982/8982 [==============================] - 28s 3ms/step - loss: 0.1233 - acc: 0.9665 - val_loss: 0.1653 - val_acc: 0.9573</pre>
<p><span>The best validation accuracy was after epoch 6, when we got 96.04% accuracy, which beats all of the previous models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing the deep learning NLP architectures</h1>
                </header>
            
            <article>
                
<p>Here is a summary of all of the models in this chapter, ordered by their sequence in this chapter. We can see that the best traditional machine learning approach got 95.24%, which was beaten by many of the deep learning approaches. While the incremental changes from <span>the best </span><span>traditional machine learning to the best deep learning model </span>may seem small at 0.80%, it reduces our misclassified examples by 17%, which is a significant relative change:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 56.4032%">
<p><strong>Model</strong></p>
</td>
<td style="width: 10.5968%">
<p><strong>Accuracy</strong></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Best traditional machine learning approach</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.24%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Simple deep learning approach</span></p>
</td>
<td style="width: 10.5968%">
<p><span>94.97%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>1D convolutional neural network model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.73%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Recurrent neural network model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>93.90%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Long short term memory model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.37%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Gated recurrent units model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.90%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Bidirectional LSTM model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.77%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Stacked bidirectional model</span></p>
</td>
<td style="width: 10.5968%">
<p><span>95.59%</span></p>
</td>
</tr>
<tr>
<td style="width: 56.4032%">
<p><span>Bidirectional with 1D convolutional neural network</span></p>
</td>
<td style="width: 10.5968%">
<p><span>96.04%</span></p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">We really covered a lot in this chapter! We built a fairly complex traditional NLP example that had many hyperparameters, as well as training it on several machine learning algorithms. It achieved a reputable result of getting 95.24% accuracy. However, when we looked into traditional NLP in more detail, we found that it had some major problems: it requires non-trivial feature engineering, it creates sparse high-dimensional data frames, and it may require discarding a substantial amount of data before machine learning.</p>
<p class="mce-root">In comparison, the deep learning approach uses word vectors or embeddings, which are much more efficient and do not require preprocessing. We ran through a number of deep learning approaches, including <span>1D convolutional layers, Recurrent Neural Networks, </span><span>GRUs, and LSTM. We finally </span>combined the two best previous approaches into one approach in our final model to get 96.08% accuracy, compared to 95.24% by using traditional NLP.</p>
<p>In the next chapter, we will develop models using TensorFlow. We will look at TensorBoard, which allows us to visualize and debug complex deep learning models. We will also look at using <span>TensorFlow estimators, an alternative option for using TensorFlow. Then, we will also look at TensorFlow Runs, which automates a lot of the steps for hyperparameter tuning. Finally, we will look at options for deploying deep learning models.</span></p>


            </article>

            
        </section>
    </body></html>