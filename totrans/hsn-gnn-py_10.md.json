["```py\n    import numpy as np\n    np.random.seed(0)\n    import torch\n    torch.manual_seed(0)\n    import matplotlib.pyplot as plt\n    import torch_geometric.transforms as T\n    from torch_geometric.datasets import Planetoid\n    ```", "```py\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    transform = T.Compose([\n        T.NormalizeFeatures(),\n        T.ToDevice(device),\n        T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n    ])\n    ```", "```py\n    dataset = Planetoid('.', name='Cora', transform=transform)\n    ```", "```py\n    train_data, val_data, test_data = dataset[0]\n    ```", "```py\n    from torch_geometric.nn import GCNConv, VGAE\n    ```", "```py\n    class Encoder(torch.nn.Module):\n        def __init__(self, dim_in, dim_out):\n            super().__init__()\n            self.conv1 = GCNConv(dim_in, 2 * dim_out)\n            self.conv_mu = GCNConv(2 * dim_out, dim_out)\n            self.conv_logstd = GCNConv(2 * dim_out, dim_out)\n        def forward(self, x, edge_index):\n            x = self.conv1(x, edge_index).relu()\n            return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n    ```", "```py\n    model = VGAE(Encoder(dataset.num_features, 16)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    ```", "```py\n    def train():\n        model.train()\n        optimizer.zero_grad()\n        z = model.encode(train_data.x, train_data.edge_index)\n        loss = model.recon_loss(z, train_data.pos_edge_label_index) + (1 / train_data.num_nodes) * model.kl_loss()\n        loss.backward()\n        optimizer.step()\n        return float(loss)\n    ```", "```py\n    @torch.no_grad()\n    def test(data):\n        model.eval()\n        z = model.encode(data.x, data.edge_index)\n        return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n    ```", "```py\n    for epoch in range(301):\n        loss = train()\n        val_auc, val_ap = test(val_data)\n        if epoch % 50 == 0:\n            print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')\n    ```", "```py\n    Epoch 0 | Loss: 3.4210 | Val AUC: 0.6772 | Val AP: 0.7110\n    Epoch 50 | Loss: 1.3324 | Val AUC: 0.6593 | Val AP: 0.6922\n    Epoch 100 | Loss: 1.1675 | Val AUC: 0.7366 | Val AP: 0.7298\n    Epoch 150 | Loss: 1.1166 | Val AUC: 0.7480 | Val AP: 0.7514\n    Epoch 200 | Loss: 1.0074 | Val AUC: 0.8390 | Val AP: 0.8395\n    Epoch 250 | Loss: 0.9541 | Val AUC: 0.8794 | Val AP: 0.8797\n    Epoch 300 | Loss: 0.9509 | Val AUC: 0.8833 | Val AP: 0.8845\n    ```", "```py\n    test_auc, test_ap = test(test_data)\n    print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')\n    Test AUC: 0.8833 | Test AP 0.8845\n    ```", "```py\n    z = model.encode(test_data.x, test_data.edge_index)\n    Ahat = torch.sigmoid(z @ z.T)\n    tensor([[0.8846, 0.5068, ..., 0.5160, 0.8309, 0.8378],\n            [0.5068, 0.8741, ..., 0.3900, 0.5367, 0.5495],\n            [0.7074, 0.7878, ..., 0.4318, 0.7806, 0.7602],\n            ...,\n            [0.5160, 0.3900, ..., 0.5855, 0.5350, 0.5176],\n            [0.8309, 0.5367, ..., 0.5350, 0.8443, 0.8275],\n            [0.8378, 0.5495, ..., 0.5176, 0.8275, 0.8200]\n      ], device='cuda:0', grad_fn=<SigmoidBackward0>)\n    ```", "```py\n    import numpy as np\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    from scipy.sparse.csgraph import shortest_path\n    import torch\n    import torch.nn.functional as F\n    from torch.nn import Conv1d, MaxPool1d, Linear, Dropout, BCEWithLogitsLoss\n    from torch_geometric.datasets import Planetoid\n    from torch_geometric.transforms import RandomLinkSplit\n    from torch_geometric.data import Data\n    from torch_geometric.loader import DataLoader\n    from torch_geometric.nn import GCNConv, aggr\n    from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n    ```", "```py\n    transform = RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True)\n    dataset = Planetoid('.', name='Cora', transform=transform)\n    train_data, val_data, test_data = dataset[0]\n    ```", "```py\n    train_data\n    Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[4488], pos_edge_label_index=[2, 4488], neg_edge_label=[4488], neg_edge_label_index=[2, 4488])\n    ```", "```py\n    def seal_processing(dataset, edge_label_index, y):\n        data_list = []\n    ```", "```py\n        for src, dst in edge_label_index.t().tolist():\n            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph([src, dst], 2, dataset.edge_index, relabel_nodes=True)\n            src, dst = mapping.tolist()\n    ```", "```py\n            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n    ```", "```py\n            src, dst = (dst, src) if src > dst else (src, dst)\n            adj = to_scipy_sparse_matrix(sub_edge_index, num_nodes=sub_nodes.size(0)).tocsr()\n            idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n            adj_wo_src = adj[idx, :][:, idx]\n            idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n            adj_wo_dst = adj[idx, :][:, idx]\n    ```", "```py\n            d_src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)\n            d_src = np.insert(d_src, dst, 0, axis=0)\n            d_src = torch.from_numpy(d_src)\n            d_dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst-1)\n            d_dst = np.insert(d_dst, src, 0, axis=0)\n            d_dst = torch.from_numpy(d_dst)\n    ```", "```py\n            dist = d_src + d_dst\n            z = 1 + torch.min(d_src, d_dst) + dist // 2 * (dist // 2 + dist % 2 - 1)\n            z[src], z[dst], z[torch.isnan(z)] = 1., 1., 0.\n            z = z.to(torch.long)\n    ```", "```py\n            node_labels = F.one_hot(z, num_classes=200).to(torch.float)\n            node_emb = dataset.x[sub_nodes]\n            node_x = torch.cat([node_emb, node_labels], dim=1)\n    ```", "```py\n            data = Data(x=node_x, z=z, edge_index=sub_edge_index, y=y)\n            data_list.append(data)\n        return data_list\n    ```", "```py\n    train_pos_data_list = seal_processing(train_data, train_data.pos_edge_label_index, 1)\n    train_neg_data_list = seal_processing(train_data, train_data.neg_edge_label_index, 0)\n    val_pos_data_list = seal_processing(val_data, val_data.pos_edge_label_index, 1)\n    val_neg_data_list = seal_processing(val_data, val_data.neg_edge_label_index, 0)\n    test_pos_data_list = seal_processing(test_data, test_data.pos_edge_label_index, 1)\n    test_neg_data_list = seal_processing(test_data, test_data.neg_edge_label_index, 0)\n    ```", "```py\n    train_dataset = train_pos_data_list + train_neg_data_list\n    val_dataset = val_pos_data_list + val_neg_data_list\n    test_dataset = test_pos_data_list + test_neg_data_list\n    ```", "```py\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32)\n    test_loader = DataLoader(test_dataset, batch_size=32)\n    ```", "```py\n    class DGCNN(torch.nn.Module):\n        def __init__(self, dim_in, k=30):\n            super().__init__()\n    ```", "```py\n            self.gcn1 = GCNConv(dim_in, 32)\n            self.gcn2 = GCNConv(32, 32)\n            self.gcn3 = GCNConv(32, 32)\n            self.gcn4 = GCNConv(32, 1)\n    ```", "```py\n            self.global_pool = aggr.SortAggregation(k=k)\n    ```", "```py\n            self.conv1 = Conv1d(1, 16, 97, 97)\n            self.conv2 = Conv1d(16, 32, 5, 1)\n            self.maxpool = MaxPool1d(2, 2)\n    ```", "```py\n            self.linear1 = Linear(352, 128)\n            self.dropout = Dropout(0.5)\n            self.linear2 = Linear(128, 1)\n    ```", "```py\n        def forward(self, x, edge_index, batch):\n            h1 = self.gcn1(x, edge_index).tanh()\n            h2 = self.gcn2(h1, edge_index).tanh()\n            h3 = self.gcn3(h2, edge_index).tanh()\n            h4 = self.gcn4(h3, edge_index).tanh()\n            h = torch.cat([h1, h2, h3, h4], dim=-1)\n    ```", "```py\n            h = self.global_pool(h, batch)\n            h = h.view(h.size(0), 1, h.size(-1))\n            h = self.conv1(h).relu()\n            h = self.maxpool(h)\n            h = self.conv2(h).relu()\n            h = h.view(h.size(0), -1)\n            h = self.linear1(h).relu()\n            h = self.dropout(h)\n            h = self.linear2(h).sigmoid()\n            return h\n    ```", "```py\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = DGCNN(train_dataset[0].num_features).to(device)\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n    criterion = BCEWithLogitsLoss()\n    ```", "```py\n    def train():\n        model.train()\n        total_loss = 0\n        for data in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            out = model(data.x, data.edge_index, data.batch)\n            loss = criterion(out.view(-1), data.y.to(torch.float))\n            loss.backward()\n            optimizer.step()\n            total_loss += float(loss) * data.num_graphs\n        return total_loss / len(train_dataset)\n    ```", "```py\n    @torch.no_grad()\n    def test(loader):\n        model.eval()\n        y_pred, y_true = [], []\n        for data in loader:\n            data = data.to(device)\n            out = model(data.x, data.edge_index, data.batch)\n            y_pred.append(out.view(-1).cpu())\n            y_true.append(data.y.view(-1).cpu().to(torch.float))\n        auc = roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n        ap = average_precision_score(torch.cat(y_true), torch.cat(y_pred))\n        return auc, ap\n    ```", "```py\n    for epoch in range(31):\n        loss = train()\n        val_auc, val_ap = test(val_loader)\n        print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')\n    Epoch 0 | Loss: 0.6925 | Val AUC: 0.8215 | Val AP: 0.8357\n    Epoch 1 | Loss: 0.6203 | Val AUC: 0.8543 | Val AP: 0.8712\n    Epoch 2 | Loss: 0.5888 | Val AUC: 0.8783 | Val AP: 0.8877...\n    Epoch 29 | Loss: 0.5461 | Val AUC: 0.8991 | Val AP: 0.8973\n    Epoch 30 | Loss: 0.5460 | Val AUC: 0.9005 | Val AP: 0.8992\n    ```", "```py\n    test_auc, test_ap = test(test_loader)\n    print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')\n    Test AUC: 0.8808 | Test AP 0.8863\n    ```"]