["```py\n    vocab[word] += 1 \n```", "```py\n    vocab[word] += 1 \n```", "```py\n    ubuntu@ubuntu-PC:~/github/mlwithtf/chapter_05$: python translate.py\n    Attempting to download http://www.statmt.org/wmt10/training-giga-  \n    fren.tar\n    File output path:   \n    /home/ubuntu/github/mlwithtf/datasets/WMT/training-giga-fren.tar\n    Expected size: 2595102720\n    File already downloaded completely!\n    Attempting to download http://www.statmt.org/wmt15/dev-v2.tgz\n    File output path: /home/ubuntu/github/mlwithtf/datasets/WMT/dev- \n    v2.tgz\n    Expected size: 21393583\n    File already downloaded completely!\n    /home/ubuntu/github/mlwithtf/datasets/WMT/training-giga-fren.tar \n    already extracted to  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train\n    Started extracting /home/ubuntu/github/mlwithtf/datasets/WMT/dev- \n    v2.tgz to /home/ubuntu/github/mlwithtf/datasets/WMT\n    Finished extracting /home/ubuntu/github/mlwithtf/datasets/WMT/dev-  \n    v2.tgz to /home/ubuntu/github/mlwithtf/datasets/WMT\n    Started extracting  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/giga- \n    fren.release2.fixed.fr.gz to  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga- \n    fren.release2.fixed.fr\n    Finished extracting  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/giga-\n    fren.release2.fixed.fr.gz to \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga-  \n    fren.release2.fixed.fr\n    Started extracting  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/giga-\n    fren.release2.fixed.en.gz to \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga- \n    fren.release2.fixed.en\n    Finished extracting  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/giga-\n    fren.release2.fixed.en.gz to \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga- \n    fren.release2.fixed.en\n    Creating vocabulary  \n    /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/vocab40000.fr \n    from \n    data /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga- \n    fren.release2.fixed.fr\n      processing line 100000\n      processing line 200000\n      processing line 300000\n     ...\n      processing line 22300000\n      processing line 22400000\n      processing line 22500000\n     Tokenizing data in \n     /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga-\n     fren.release2.fr\n      tokenizing line 100000\n      tokenizing line 200000\n      tokenizing line 300000\n     ...\n      tokenizing line 22400000\n      tokenizing line 22500000\n     Creating vocabulary \n     /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/vocab\n     40000.en from data \n     /home/ubuntu/github/mlwithtf/datasets/WMT/train/data/giga-\n     fren.release2.en\n      processing line 100000\n      processing line 200000\n      ...\n```", "```py\n    encoder_inputs, decoder_inputs, target_weights = \n     model.get_batch(train_set, bucket_id)\n```", "```py\n    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, \n     target_weights, bucket_id, False)\n```", "```py\n    put_count=2530 evicted_count=2000 eviction_rate=0.790514 and \n     unsatisfied allocation rate=0\n    global step 200 learning rate 0.5000 step-time 0.94 perplexity \n     1625.06\n      eval: bucket 0 perplexity 700.69\n      eval: bucket 1 perplexity 433.03\n      eval: bucket 2 perplexity 401.39\n      eval: bucket 3 perplexity 312.34\n    global step 400 learning rate 0.5000 step-time 0.91 perplexity   \n     384.01\n      eval: bucket 0 perplexity 124.89\n      eval: bucket 1 perplexity 176.36\n      eval: bucket 2 perplexity 207.67\n      eval: bucket 3 perplexity 239.19\n    global step 600 learning rate 0.5000 step-time 0.87 perplexity   \n     266.71\n      eval: bucket 0 perplexity 75.80\n      eval: bucket 1 perplexity 135.31\n      eval: bucket 2 perplexity 167.71\n      eval: bucket 3 perplexity 188.42\n    global step 800 learning rate 0.5000 step-time 0.92 perplexity  \n     235.76\n      eval: bucket 0 perplexity 107.33\n      eval: bucket 1 perplexity 159.91\n      eval: bucket 2 perplexity 177.93\n      eval: bucket 3 perplexity 263.84  \n```", "```py\n tf.app.flags.DEFINE_float(\"learning_rate\"\", 0.5, \"\"Learning  \n                             rate.\"\") \n tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\"\", 0.99, \n                          \"Learning rate decays by this much.\"\") \n tf.app.flags.DEFINE_float(\"max_gradient_norm\"\", 5.0, \n                          \"Clip gradients to this norm.\"\") \n tf.app.flags.DEFINE_integer(\"batch_size\"\", 64, \n                            \"Batch size to use during training.\"\") \n tf.app.flags.DEFINE_integer(\"en_vocab_size\"\", 40000, \"\"Size ....\"\") \n tf.app.flags.DEFINE_integer(\"fr_vocab_size\"\", 40000, \"\"Size  \n                              of....\"\") \n tf.app.flags.DEFINE_integer(\"size\"\", 1024, \"\"Size of each  \n                              model...\"\") \n tf.app.flags.DEFINE_integer(\"num_layers\"\", 3, \"\"#layers in the \n                model.\"\")tf.app.flags.DEFINE_string(\"train_dir\"\", \n  os.path.realpath(''../../datasets/WMT''), \"\"Training directory.\"\") \n tf.app.flags.DEFINE_integer(\"max_train_data_size\"\", 0, \n                            \"Limit size of training data \"\") \n tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\"\", 200, \n                            \"Training steps to do per \n                             checkpoint.\"\")\n```", "```py\n    model = seq2seq_model.Seq2SeqModel( \n      FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets, \n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, \n       FLAGS.batch_size, \n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor, \n      forward_only=forward_only) \n```", "```py\nglobal step 200 learning rate 0.5000 step-time 0.94 perplexity \n  1625.06\n   eval: bucket 0 perplexity 700.69\n   eval: bucket 1 perplexity 433.03\n   eval: bucket 2 perplexity 401.39\n   eval: bucket 3 perplexity 312.34\n   ...\n```"]