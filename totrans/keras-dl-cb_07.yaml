- en: Deep Belief Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **deep belief network** (**DBN**) is a class of deep neural network, composed
    of multiple layers of hidden units, with connections between the layers; where
    a DBN differs is these hidden units don't interact with other units within each
    layer. A DBN can learn to probabilistically reconstruct its input without supervision,
    when trained, using a set of training datasets. It is a joint (multivariate) distributions
    over large numbers of random variables that interact with each other. These representations
    sit at the intersection of statistics and computer science, relying on concepts
    from probability theory, graph algorithms, machine learning, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The layers act as feature detectors. After the training step, a DBN can be trained
    with supervision to perform classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following chapters in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deep belief networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the accuracy of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBN implementation for the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of the number of neurons in an RBM Layer in a DBN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBNs with two RBM layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the NotMNIST dataset with a DBN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding deep belief networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DBNs can be considered a composition of simple, unsupervised networks such as
    **Restricted Boltzmann machines** (**RBMs**) or autoencoders; in these, each subnetwork's
    hidden layer serves as the visible layer for the next. An RBM is an undirected,
    generative model with an input layer (which is visible) and a hidden layer, with connections
    between the layers but not within layers. This topology leads to a fast, layer-by-layer,
    unsupervised training procedure. Contrastive divergence is applied to each subnetwork,
    starting from the lowest pair of layers (the lowest visible layer is a training
    set).
  prefs: []
  type: TYPE_NORMAL
- en: DBNs are trained (greedily), one layer at a time, which makes it one of the
    first effective deep learning algorithms. There are many implementations and uses
    of DBNs in real-life applications and scenarios; we will be looking at using a
    DBN to classify MNIST and NotMNIST datasets.
  prefs: []
  type: TYPE_NORMAL
- en: DBN implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This class instantiates the Restricted Boltzmann machines (RBN) layers and
    the cost functions. The **DeepBeliefNetwork** class is itself a subclass of the **Model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70002aa2-1334-419c-818a-23a3a0021660.png)'
  prefs: []
  type: TYPE_IMG
- en: Class initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In DBN initialization, the `Model` class''s initialization method `__init__(self,
    name)` is called. The `Model` class references the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input data**: `self.input_data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input labels**: `self.input_labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: `` `self.cost` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of nodes in final layer**: `self.layer_nodes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow session**: `self.tf_session`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow graph**: `self.tf_graph= tf.graph`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Other variables that are defined are the loss functions, which should be one
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Code Listing for **DeepBeliefNetwork** class is given below. The `__init__()`
    function is shown in the following code. Here all the variables, such as array
    of parameters for each RBM layer are specified. We are also making a call to the `__init__()`
    function of `SupervisedModel`, which is the super class for the `DeepBeliefNetwork`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two important parameters to initialize are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.rbms = []`: Array of `RBM` class instances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.rbm_graphs = []`: An array `tf.Graph` for each of those RBMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how RBM layers are constructed from the `rbm_layers` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: RBM class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For each RBM layer, an **RBM** class is initialized. This class extends the
    **UnsupervisedModel** and **Model** classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c37de519-b528-4c0c-a841-a2b242ad02ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Details of the **RBM** class `__init__(..)` function are specified in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the `rbm` graphs are initialized they are appended to the the TensorFlow
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pretraining the DBN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we look at how a DBN is pretrained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This in turn calls `SupervisedModel.pretrain_procedure(..)`, which takes the
    following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`layer_objs`: A list of model objects (autoencoders or RBMs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_graphs`: A list of model `tf.Graph` objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_params_func`: The function used to set the parameters after pretraining'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_set`: The training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_set`: The validation set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function returns data encoded by the last layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This in turn calls `self._pretrain_layer_and_gen_feed(...)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Inside the preceding function, each `layer_obj` is called **iteratively**.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model training is implemented in the `fit(..)` method. It takes the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_X`: `array_like, shape (n_samples, n_features)`, Training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_Y`: `array_like, shape (n_samples, n_classes)`, Training labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val_X`: `array_like, shape (N, n_features) optional, (default = None)`, Validation
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`val_Y`: `array_like, shape (N, n_classes) optional, (default = None)`, Validation
    labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`graph`: `tf.Graph, optional (default = None)`, TensorFlow Graph object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we look at the implementation of `fit(...)` function where the model is
    trained and saved in the model path specified by `model_path`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Predicting the label
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prediction of the label can be made by calling the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Finding the accuracy of the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accuracy of the model is found by computing mean accuracy over the test set.
    It is implemented in the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`test_X`: `array_like, shape (n_samples, n_features)`, Test data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_Y`: `array_like, shape (n_samples, n_features)`, Test labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return float`: mean accuracy over the test set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will look at how DBN implementation can be used on the
    MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: DBN implementation for the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at how the DBN class implemented earlier is used for the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we load the dataset from `idx3` and `idx1` formats into test, train,
    and validation sets. We need to import TensorFlow common utilities that are defined
    in the common module explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find details about `load_mnist_dataset()` in the following code listing.
    As `mode=''supervised''` is set, the train, test, and validation labels are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Input parameters for a DBN with 256-Neuron RBM layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will initialize various parameters that are needed by the DBN class defined
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the parameters are defined, let''s run the DBN network on the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Output for a DBN with 256-neuron RBN layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output of the preceding listing shows the test set''s accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Overall accuracy and Test set accuracy is quite low. With the increase in number
    of iterations it improves. Let us run same sample with 20 epochs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen the reconstruction loss has come down and the Test set accuracy
    has improved by 20% to  0.10339999944
  prefs: []
  type: TYPE_NORMAL
- en: Let us increase the number of Epochs to 40\. Output is shown below
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Effect of the number of neurons in an RBM layer in a DBN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how changing the number of neurons in an RBM layer affects the
    test set''s accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: An RBM layer with 512 neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the output of a DBN with 512 neurons in an RBM layer. The
    reconstruction loss has come down and the test set''s accuracy has come down as
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the accuracy and test set accuracy both have come down. This means
    increasing the number of neurons doesn't necessarily improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: An RBM layer with 128 neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A 128-neuron RBM layer leads to higher test set accuracy but a lower overall
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the accuracy metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have trained the neural network with multiple neuron numbers in RBM layers,
    let''s compare metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23687400-5515-410c-9181-711d9e3fd7da.png)'
  prefs: []
  type: TYPE_IMG
- en: Reconstruction loss reduces as a function of the number of neurons, as shown
    in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7959b46-15cd-4b5f-8821-6404fa591c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: The test set accuracy peaks for 256 neurons and then comes down.
  prefs: []
  type: TYPE_NORMAL
- en: DBNs with two RBM layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create a DBN with two RBM layers and run it on the
    MNIST dataset. We will modify the input parameters for the `DeepBeliefNetwork(..)`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that some of the parameters have two elements for array so we need to
    specify these parameters for two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rbm_layers = [256, 256]`: Number of neurons in each RBM layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rbm_learning_rate = [0.001, 0001]`: Learning rate for each RBM layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rbm_num_epochs = [5, 5]`: Number of epochs in each layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rbm_batch_size= [32, 32]`: Batch size for each RBM layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the DBN initialization and the training of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code listing can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch08/implementation/boltzmann/run_dbn_mnist_two_layers.py](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch08/implementation/boltzmann/run_dbn_mnist_two_layers.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen from the preceding listing, the test set accuracy is better than
    the single RBM layer DBN.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying the NotMNIST dataset with a DBN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at the NotMNIST dataset, which we explored in [Chapter 2](c373e64c-90d3-4676-96fa-cf67e652c25b.xhtml),
    *Deep Feedforward Networks*, in the *Implementing feedforward networks* section
    with images, and see how our DBN works for that dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will leverage the same pickle file, `notMNIST.pickle`, created in [Chapter
    2](c373e64c-90d3-4676-96fa-cf67e652c25b.xhtml), *Deep Feedforward Networks*. The
    initialization parameters and imports are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Implementation remains more or less similar to the MNIST dataset. The main
    implementation listing is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code listing can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch08/implementation/boltzmann/run_dbn_nomnist.py](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch08/implementation/boltzmann/run_dbn_nomnist.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding listing will point the performance of our model
    for the NotMNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, this network performed much better than the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored DBNs and looked at how these could be used to build
    classification pipelines using one or more RBM layers. We looked at various parameters
    within the RBM layer and their effects on accuracy, reconstruction loss, and test
    set accuracy. We also looked at single layer and multilayer DBNs using one or
    more RBMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we look at Generative models and how they differ from discriminative
    models.
  prefs: []
  type: TYPE_NORMAL
