- en: Jupyter Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter Notebooks are one of the most important tools for data scientists using
    Python. This is because they're an ideal environment for developing reproducible
    data analysis pipelines. Data can be loaded, transformed, and modeled all inside
    a single Notebook, where it's quick and easy to test out code and explore ideas
    along the way. Furthermore, all of this can be documented "**inline**" using formatted
    text, so you can make notes for yourself or even produce a structured report.
    Other comparable platforms - for example, RStudio or Spyder - present the user
    with multiple windows, which promote arduous tasks such as copy and pasting code
    around and rerunning code that has already been executed. These tools also tend
    to involve **Read Eval Prompt Loops (REPLs)** where code is run in a terminal
    session that has saved memory. This type of development environment is bad for
    reproducibility and not ideal for development either. Jupyter Notebooks solve
    all these issues by giving the user a single window where code snippets are executed
    and outputs are displayed inline. This lets users develop code efficiently and
    allows them to look back at previous work for reference, or even to make alterations.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start the chapter by explaining exactly what Jupyter Notebooks are and
    continue to discuss why they are so popular among data scientists. Then, we'll
    open a Notebook together and go through some exercises to learn how the platform
    is used. Finally, we'll dive into our first analysis and perform an exploratory
    analysis in the section *Basic Functionality and Features.*
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn what a Jupyter Notebook is and why it's useful for data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Jupyter Notebook features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Study Python data science libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform simple exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code from this book are available as chapter-specific IPython notebooks in
    the code bundle. All color plots from this book are also available in the code
    bundle.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Functionality and Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we first demonstrate the usefulness of Jupyter Notebooks with
    examples and through discussion. Then, in order to cover the fundamentals of Jupyter
    Notebooks for beginners, we'll see the basic usage of them in terms of launching
    and interacting with the platform. For those who have used Jupyter Notebooks before,
    this will be mostly a review; however, you will certainly see new things in this
    topic as well.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Jupyter Notebook and Why is it Useful?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jupyter Notebooks are locally run web applications which contain live code,
    equations, figures, interactive apps, and Markdown text. The standard language
    is Python, and that''s what we''ll be using for this book; however, note that
    a variety of alternatives are supported. This includes the other dominant data
    science language, R:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd2128ae-e666-46c5-8c5a-5f7b892cac26.png)'
  prefs: []
  type: TYPE_IMG
- en: Those familiar with R will know about R Markdown. Markdown documents allow for
    Markdown-formatted text to be combined with executable code. Markdown is a simple
    language used for styling text on the web. For example, most GitHub repositories
    have a `README.md` `Markdown` file. This format is useful for basic text formatting.
    It's comparable to HTML but allows for much less customization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly used symbols in Markdown include hashes (#) to make text into a heading,
    square and round brackets to insert hyperlinks, and stars to create italicized
    or bold text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb9fd5a0-5267-4233-8eae-2b8f3e37f867.png)'
  prefs: []
  type: TYPE_IMG
- en: Having seen the basics of Markdown, let's come back to R Markdown, where Markdown
    text can be written alongside executable code. Jupyter Notebooks offer the equivalent
    functionality for Python, although, as we'll see, they function quite differently
    than R Markdown documents. For example, R Markdown assumes you are writing Markdown
    unless otherwise specified, whereas Jupyter Notebooks assume you are inputting
    code. This makes it more appealing to use Jupyter Notebooks for rapid development
    and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a data science perspective, there are two primary types for a Jupyter
    Notebook depending on how they are used: lab-style and deliverable.'
  prefs: []
  type: TYPE_NORMAL
- en: Lab-style Notebooks are meant to serve as the programming analog of research
    journals. These should contain all the work you've done to load, process, analyze,
    and model the data. The idea here is to document everything you've done for future
    reference, so it's usually not advisable to delete or alter previous lab-style
    Notebooks. It's also a good idea to accumulate multiple date-stamped versions
    of the Notebook as you progress through the analysis, in case you want to look
    back at previous states.
  prefs: []
  type: TYPE_NORMAL
- en: Deliverable Notebooks are intended to be presentable and should contain only
    select parts of the lab-style Notebooks. For example, this could be an interesting
    discovery to share with your colleagues, an in-depth report of your analysis for
    a manager, or a summary of the key findings for stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, an important concept is reproducibility. If you've been diligent
    in documenting your software versions, anyone receiving the reports will be able
    to rerun the Notebook and compute the same results as you did. In the scientific
    community, where reproducibility is becoming increasingly difficult, this is a
    breath of fresh air.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are going to open up a Jupyter Notebook and start to learn the interface.
    Here, we will assume you have no prior knowledge of the platform and go over the
    basic usage.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Jupyter Notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigate to the companion material directory in the terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On Unix machines such as Mac or Linux, command-line navigation can be done using
    ls to display directory contents and `cd` to change directories. On Windows machines,
    use `dir` to display directory contents and use `cd` to change directories instead.
    If, for example, you want to change the drive from `C:` to `D:` , you should execute
    `d:` to change drives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new local Notebook server here by typing the following into the terminal:
    `jupyter notebook.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new window or tab of your default browser will open the Notebook Dashboard to
    the working directory. Here, you will see a list of folders and files contained therein.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on a folder to navigate to that particular path and open a file by clicking
    on it. Although its main use is editing IPYNB Notebook files, Jupyter functions
    as a standard text editor as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reopen the terminal window used to launch the app. We can see the `NotebookApp` being
    run on a local server. In particular, you should see a line like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`[I 20:03:01.045 NotebookApp] The Jupyter Notebook is running at: http:// localhost:8888/
    ? oken=e915bb06866f19ce462d959a9193a94c7c088e81765f9d8a`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Going to that HTTP address will load the app in your browser window, as was done
    automatically when starting the app. Closing the window does not stop the app;
    this should be done from the terminal by typing *Ctrl + C*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Close the app by typing *Ctrl +* *C* in the terminal. You may also have to confirm
    by entering `y`. Close the web browser window as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When loading the NotebookApp, there are various options available to you. In
    the terminal, see the list of available options by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`jupyter notebook –-help.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'One such option is to specify a specific port. Open a NotebookApp at `local
    port 9000` by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`jupyter notebook --port 9000`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The primary way to create a new Jupyter Notebook is from the Jupyter Dashboard. Click
    **New** in the upper-right corner and select a kernel from the drop-down menu (that
    is, select something in the Notebooks section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d1a81c3a-8d52-4977-91e0-103c9b8c8af3.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernels provide programming language support for the Notebook. If you have installed
    Python with Anaconda, that version should be the default kernel. Conda virtual
    environments will also be available here.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual environments are a great tool for managing multiple projects on the
    same machine. Each virtual environment may contain a different version of Python
    and external libraries. Python has built-in virtual environments; however, the
    Conda virtual environment integrates better with Jupyter Notebooks and boasts
    other nice features. The documentation is available at [https://conda.io/docs/user-guide/tasks/manage-environments.html](https://conda.io/docs/user-guide/tasks/manage-environments.html).
  prefs: []
  type: TYPE_NORMAL
- en: With the newly created blank Notebook, click in the top cell and type `print('hello
    world')` , or any other code snippet that writes to the screen. Execute it by
    clicking in the cell and pressing *Shift + Enter*, or by selecting **Run Cell**
    in the **Cell menu**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any `stdout` or `stderr` output from the code will be displayed beneath as the
    cell runs. Furthermore, the string representation of the object written in the
    final line will be displayed as well. This is very handy, especially for displaying
    tables, but sometimes we don't want the final object to be displayed. In such
    cases, a semicolon (; ) can be added to the end of the line to suppress the display.
  prefs: []
  type: TYPE_NORMAL
- en: New cells expect and run code input by default; however, they can be changed
    to render Markdown instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click into an empty cell and change it to accept Markdown-formatted text. This
    can be done from the drop-down menu icon in the toolbar or by selecting **Markdown**
    from the **Cell** menu. Write some text in here (any text will do), making sure
    to utilize Markdown formatting symbols such as #.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Focus on the toolbar at the top of the Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8f15ecf5-0db6-4dd2-9177-a2f88eb960c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a Play icon in the toolbar, which can be used to run cells. As we''ll
    see later,however, it''s handier to use the keyboard shortcut *Shift +* *Enter*
    to run cells. Right next to this is a Stop icon, which can be used to stop cells
    from running. This is useful, for example, if a cell is taking too long to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed67bc3e-b989-4f56-99cf-2f4def9cfd55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'New cells can be manually added from the Insert menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69a5c020-9d23-479c-be44-c375dfd0c9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cells can be copied, pasted, and deleted using icons or by selecting options
    from the Edit menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b346eda-8c8d-466f-9bd4-6135b2b0a7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/36336f67-aacc-4e60-9d6d-d2a213851bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cells can also be moved up and down this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f74be09-9e20-48a8-a404-9e40fe551ced.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are useful options under the Cell menu to run a group of cells or the
    entire Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb8230f9-c773-4415-b903-0afa60e6f1ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Experiment with the toolbar options to move cells up and down, insert new cells,and
    delete cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An important thing to understand about these Notebooks is the shared memory between
    cells. It''s quite simple: every cell existing on the sheet has access to the global
    set of variables. So, for example, a function defined in one cell could be called from
    any other, and the same applies to variables. As one would expect, anything within
    the scope of a function will not be a global variable and can only be accessed from
    within that specific function.'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Kernel menu to see the selections. The Kernel menu is useful for stopping script
    executions and restarting the Notebook if the kernel dies. Kernels can also be swapped
    here at any time, but it is unadvisable to use multiple kernels for a single Notebook
    due to reproducibility concerns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the **File** menu to see the selections. The **File** menu contains options
    for downloading the Notebook in various formats. In particular, it's recommended
    to save an HTML version of your Notebook, where the content is rendered statically
    and can be opened and viewed "as you would expect" in web browsers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Notebook name will be displayed in the upper-left corner. New Notebooks
    will automatically be named **Untitled**.
  prefs: []
  type: TYPE_NORMAL
- en: Change the name of your IPYNB `Notebook` file by clicking on the current name
    in the upper-left corner and typing the new name. Then, save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Close the current tab in your web browser (exiting the Notebook) and go to the Jupyter
    Dashboard tab, which should still be open. (If it's not open, then reload it by
    copy and pasting the HTTP link from the terminal.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we didn't shut down the Notebook, we just saved and exited, it will have
    a green book symbol next to its name in the Files section of the Jupyter Dashboard and
    will be listed as Running on the right side next to the last modified date. Notebooks
    can be shut down from here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quit the Notebook you have been working on by selecting it (checkbox to the
    left of the name) and clicking the orange Shutdown button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fa60a534-064a-4c87-9ab3-ad6ae266a58b.png)'
  prefs: []
  type: TYPE_IMG
- en: If you plan to spend a lot of time working with Jupyter Notebooks, it's worthwhile
    to learn the keyboard shortcuts. This will speed up your workflow considerably.
    Particularly useful commands to learn are the shortcuts for manually adding new
    cells and converting cells from code to Markdown formatting. Click on **Keyboard
    Shortcuts** from the **Help menu** to see how.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter has many appealing features that make for efficient Python programming.
    These include an assortment of things, from methods for viewing docstrings to
    executing Bash commands. Let's explore some of these features together in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The official IPython documentation can be found here: [http://ipython.readthedocs.io/en/stable/](https://ipython.readthedocs.io/en/stable/).
    It has details on the features we will discuss here and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring some of Jupyter's most useful features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the Jupyter Dashboard, navigate to the `chapter-1` directory and open the `chapter-1-workbook.ipynb`
    file by selecting it. The standard file extension for Jupyter Notebooks is `.ipynb`,
    which was introduced back when they were called IPython Notebooks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down to Subtopic `Jupyter Features` in the Jupyter Notebook. We start by
    reviewing the basic keyboard shortcuts. These are especially helpful to avoid having
    to use the mouse so often, which will greatly speed up the workflow. Here are
    the most useful keyboard shortcuts. Learning to use these will greatly improve your
    experience with Jupyter Notebooks as well as your own efficiency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Shift + Enter* is used to run a cell'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Esc* *key* is used to leave a cell
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The *M* key is used to change a cell to Markdown (after pressing Esc)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Y* key is used to change a cell to code (after pressing Esc)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Arrow keys* move cells (after pressing Esc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Enter* *key* is used to enter a cell
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving on from shortcuts, the help option is useful for beginners and experienced
    coders alike. It can help provide guidance at each uncertain step.
  prefs: []
  type: TYPE_NORMAL
- en: Users can get help by adding a question mark to the end of any object and running
    the cell. Jupyter finds the docstring for that object and returns it in a pop-out
    window at the bottom of the app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the **Getting Help** section cells and check out how Jupyter displays the docstrings
    at the bottom of the Notebook. Add a cell in this section and get help on the
    object of your choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0c4022a8-0948-487c-a807-8cf5cc5dc79f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tab completion can be used to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: List available modules when importing external libraries
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: List available modules of imported external libraries
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Function and variable completion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be especially useful when you need to know the available input arguments for
    a module, when exploring a new library, to discover new modules, or simply to
    speed up workflow. They will save time writing out variable names or functions and
    reduce bugs from typos. The tab completion works so well that you may have difficulty
    coding Python in other editors after today!
  prefs: []
  type: TYPE_NORMAL
- en: 'Click into an empty code cell in the Tab Completion section and try using tab completion
    in the ways suggested immediately above. For example, the fist suggestion can
    be done by typing import (including the space after) and then pressing the Tab
    key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/095220a0-0d48-43b9-9b83-edada33dea2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Last but not least of the basic Jupyter Notebook features are **magic** commands.
    These consist of one or two percent signs followed by the command. Magics starting
    with `%%` will apply to the entire cell, and magics starting with `%` will only
    apply to that line. This will make sense when seen in an example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll to the **Jupyter Magic Functions** section and run the cells containing
    `%lsmagic and %matplotlib inline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a74b3cc-a816-4269-8a14-1c5eed6855c6.png)'
  prefs: []
  type: TYPE_IMG
- en: '`%lsmagic` lists the available options. We will discuss and show examples of
    some of the most useful ones. The most common magic command you will probably
    see is `%matplotlib inline`, which allows matplotlib figures to be displayed in
    the Notebook without having to explicitly use `plt.show()` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The timing functions are very handy and come in two varieties: a standard timer
    `(%time or %%time)` and a timer that measures the average runtime of many iterations
    `(%timeit and %%timeit)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the Timers section. Note the difference between using one and
    two percent signs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even by using a Python kernel (as you are currently doing), other languages
    can be invoked using magic commands. The built-in options include JavaScript,
    R, Pearl, Ruby, and Bash. Bash is particularly useful, as you can use Unix commands
    to find out where you are currently (`pwd`), what's in the directory (`ls`), make
    new folders `(mkdir)`, and write file contents `(cat / head / tail)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the fist cell in the **Using bash in the notebook section**. This cell
    writes some text to a file in the working directory, prints the directory contents,
    prints an empty line, and then writes back the contents of the newly created file
    before removing it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1db82a4c-7baf-43f0-9bfe-eed4a3571ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: Run the following cells containing only `ls` and `pwd`. Note how we did not
    have to explicitly use the Bash magic command for these to work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are plenty of external magic commands that can be installed. A popular
    one is `ipython-sql`, which allows for SQL code to be executed in cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''ve not already done so, install `ipython-sql` now. Open a new terminal
    window and execute the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9f8aff1e-7c8b-44f9-a965-11a5fad0c8b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Run the `%load_ext sql` cell to load the external command into the Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/409e7029-b8f6-41f7-9f1b-52c8b005cba1.png)'
  prefs: []
  type: TYPE_IMG
- en: This allows for connections to remote databases so that queries can be executed (and
    thereby documented) right inside the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell containing the SQL sample query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4144970d-a09c-4d76-a1b9-78e6b28fb506.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we first connect to the local sqlite source; however, this line could
    instead point to a specific database on a local or remote server. Then, we execute
    a simple `SELECT` to show how the cell has been converted to run SQL code instead
    of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to other useful magic functions, we'll briefly discuss one that helps
    with documentation. The command is `%version_information`, but it does not come
    as standard with Jupyter. Like the SQL one we just saw, it can be installed from
    the command line with pip.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If not already done, install the version documentation tool now from the terminal
    using `pip`.Open up a new window and run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once installed, it can then be imported into any Notebook using `%load_ext version_information`.
    Finally, once loaded, it can be used to display the versions of each piece of
    software in the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell that loads and calls the version_information command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1078ccb9-2f8c-4ad7-9416-98dc5052755e.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a Jupyter Notebook to a Python Script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can convert a Jupyter Notebook to a Python script. This is equivalent to
    copying and pasting the contents of each code cell into a single `.py` file. The
    Markdown sections are also included as comments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conversion can be done from the `NotebookApp` or in the command line as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`jupyter nbconvert --to=python chapter-1-notebook.ipynb`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d010e40c-459e-4aca-a2c1-f39f66d1ee7f.png)'
  prefs: []
  type: TYPE_IMG
- en: This is useful, for example, when you want to determine the library requirements
    for a Notebook using a tool such as `pipreqs`. This tool determines the libraries
    used in a project and exports them into a `requirements.txt` file (and it can
    be installed by running `pip install pipreqs`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The command is called from outside the folder containing your `.py` files.
    For example, if the `.py` files are inside a folder called `chapter-1`, you could
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f02a2880-0b50-4c03-87df-a78f2596315c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The resulting `requirements.txt` file for `chapter-1-workbook.ipynb` looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Python Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having now seen all the basics of Jupyter Notebooks, and even some more advanced
    features, we'll shift our attention to the Python libraries we'll be using in
    this book. Libraries, in general, extend the default set of Python functions.
    Examples of commonly used standard libraries are `datetime`, `time`, and `os`.
    These are called standard libraries because they come standard with every installation
    of Python.
  prefs: []
  type: TYPE_NORMAL
- en: For data science with Python, the most important libraries are external, which
    means they do not come standard with Python.
  prefs: []
  type: TYPE_NORMAL
- en: The external data science libraries we'll be using in this book are `NumPy`,
    `Pandas`, `Seaborn`, `matplotlib`, `scikit-learn`, `Requests`, and `Bokeh`. Let's
    briefly introduce each.
  prefs: []
  type: TYPE_NORMAL
- en: It's a good idea to import libraries using industry standards, for example,
    `import numpy` as `np`; this way, your code is more readable. Try to avoid doing
    things such as from `numpy import *`, as you may unwittingly overwrite functions.
    Furthermore, it's often nice to have modules linked to the library via a dot (.
    ) for code readability.
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy** offers multi-dimensional data structures (arrays) on which operations
    can be performed far quicker than standard Python data structures (for example,
    lists). This is done in part by performing operations in the background using
    C. NumPy also offers various mathematical and data manipulation functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pandas** is Python''s answer to the R DataFrame. It stores data in 2-D tabular
    structures where columns represent different variables and rows correspond to
    samples. Pandas provides many handy tools for data wrangling such as filling in
    `NaN` entries and computing statistical descriptions of the data. Working with
    Pandas DataFrames will be a big focus of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** is a plotting tool inspired by the MATLAB platform. Those familiar
    with R can think of it as Python''s version of ggplot. It''s the most popular
    Python library for plotting figures and allows for a high level of customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seaborn** works as an extension to matplotlib, where various plotting tools
    useful for data science are included. Generally speaking, this allows for analysis
    to be done much faster than if you were to create the same things manually with
    libraries such as matplotlib and scikit-learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn** is the most commonly used machine learning library. It offers
    top-of the-line algorithms and a very elegant API where models are instantiated
    and then fit with data. It also provides data processing modules and other tools
    useful for predictive analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Requests** is the go-to library for making HTTP requests. It makes it straightforward
    to get HTML from web pages and interface with APIs. For parsing the HTML, many
    choose `BeautifulSoup4`, which we will also cover in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh** is an interactive visualization library. It functions similar to
    matplotlib, but allows us to add hover, zoom, click, and use other interactive
    tools to our plots. It also allows us to render and play with the plots inside
    our Jupyter Notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having introduced these libraries, let's go back to our Notebook and load them,
    by running the import statements. This will lead us into our fist analysis, where
    we finally start working with a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Import the external libraries and set up the plotting environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open up the `chapter 1` Jupyter Notebook and scroll to the `Python Libraries
    section`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just like for regular Python scripts, libraries can be imported into the Notebook
    at any time. It's best practice to put the majority of the packages you use at
    the top of the file. Sometimes it makes sense to load things midway through the
    Notebook and that is completely OK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cells to import the external libraries and set the plotting options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4df0e5af-ac74-49d4-aa62-88e8cec74570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a nice Notebook setup, it''s often useful to set various options along
    with the imports at the top. For example, the following can be run to change the
    figure appearance to something more aesthetically pleasing than the `matplotlib`
    and Seaborn defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So far in this book, we've gone over the basics of using Jupyter Notebooks for
    data science. We started by exploring the platform and finding our way around
    the interface. Then, we discussed the most useful features, which include tab
    completion and magic functions. Finally, we introduced the Python libraries we'll
    be using in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will be very interactive as we perform our fist analysis together
    using the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Analysis - The Boston Housing Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, this chapter has focused on the features and basic usage of Jupyter.
    Now, we'll put this into practice and do some data exploration and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we'll look at in this section is the so-called *Boston housing dataset*.
    It contains US census data concerning houses in various areas around the city
    of Boston. Each sample corresponds to a unique area and has about a dozen measures.
    We should think of samples as rows and measures as columns. The data was fist
    published in 1978 and is quite small, containing only about 500 samples.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know something about the context of the dataset, let's decide on
    a rough plan for the exploration and analysis. If applicable, this plan would
    accommodate the relevant question(s) under study. In this case, the goal is not
    to answer a question but to instead show Jupyter in action and illustrate some
    basic data analysis methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our general approach to this analysis will be to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data into Jupyter using a Pandas DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantitatively understand the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for patterns and generate questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer the questions to the problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the Data into Jupyter Using a Pandas DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oftentimes, data is stored in tables, which means it can be saved as a `comma-separated
    variable (CSV)` file. This format, and many others, can be read into Python as
    a DataFrame object, using the Pandas library. Other common formats include `tab-separated
    variable (TSV)`, SQL tables, and JSON data structures. Indeed, Pandas has support
    for all of these. In this example, however, we are not going to load the data
    this way because the dataset is available directly through scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: An important part after loading data for analysis is ensuring that it's clean.
    For example, we would generally need to deal with missing data and ensure that
    all columns have the correct datatypes. The dataset we use in this section has
    already been cleaned, so we will not need to worry about this. However, we'll
    see messier data in the second chapter and explore techniques for dealing with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Boston housing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the chapter 1 Jupyter Notebook, scroll to subtopic `Loading the Data into
    Jupyter Using a Pandas DataFrame`of `Our First Analysis`: `The Boston Housing
    Dataset`. The Boston housing dataset can be accessed from the `sklearn.datasets`
    module using the `load_boston` method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the first two cells in this section to load the Boston dataset and see
    the data structures type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f8abc0d1-4ae2-486b-852e-367c2eeaf03d.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the second cell tells us that it's a scikit-learn Bunch object.
    Let's get some more information about that to understand what we are dealing with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to import the base object from scikit-learn utils and print
    the docstring in our Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/52761b21-8607-4edf-9a8d-50cdda5ceafc.png)'
  prefs: []
  type: TYPE_IMG
- en: Reading the resulting docstring suggests that it's basically a dictionary, and
    can essentially be treated as such.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the field names (that is, the keys to the dictionary) by running the
    next cell. We find these fields to be self-explanatory: [`''DESCR''`, `''target''`,
    `''data''`, `''feature_names''`] .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the next cell to print the dataset description contained in boston[`''DESCR''`]
    . Note that in this call, we explicitly want to print the field value so that
    the Notebook renders the content in a more readable format than the string representation
    (that is, if we just type boston[`''DESCR''`] without wrapping it in a print statement).
    We then see the dataset information as we''ve previously summarized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Of particular importance here are the feature descriptions (under `Attribute Information`).
    We will use this as reference during our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to create a Pandas DataFrame that contains the data. This
    is beneficial for a few reasons: all of our data will be contained in one object,
    there are useful and computationally efficient DataFrame methods we can use, and
    other libraries such as Seaborn have tools that integrate nicely with DataFrames.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we will create our DataFrame with the standard constructor method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell where Pandas is imported and the docstring is retrieved for `pd.DataFrame`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0ab67f16-3da3-41d0-87c6-be2fe8d1009c.png)'
  prefs: []
  type: TYPE_IMG
- en: The docstring reveals the DataFrame input parameters. We want to feed in boston[`'data'`]
    for the data and use boston[`'feature_names'`] for the headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next few cells to print the data, its shape, and the feature names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/97e175b0-4092-4f73-93a1-90ed07264741.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the output, we see that our data is in a `2D NumPy array`. Running
    the command boston[`'data'`].shape returns the length (number of samples) and
    the number of features as the first and second outputs, respectively
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the data into a Pandas DataFrame `df` by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In machine learning, the variable that is being modeled is called the target
    variable; it's what you are trying to predict given the features. For this dataset,
    the suggested target is MEDV, the median house value in 1,000s of dollars
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to see the shape of the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ffc414d7-7df4-4df1-aa4f-680e77eb0a19.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that it has the same length as the features, which is what we expect.
    It can therefore be added as a new column to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the target variable to `df` by running the cell with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To distinguish the target from our features, it can be helpful to store it at
    the front of our DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Move the target variable to the front of df by running the cell with the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we introduce a dummy variable y to hold a copy of the target column before
    removing it from the DataFrame. We then use the Pandas concatenation function
    to combine it with the remaining DataFrame along the 1st axis (as opposed to the
    0th axis, which combines rows).
  prefs: []
  type: TYPE_NORMAL
- en: You will often see dot notation used to reference DataFrame columns. For example,
    previously we could have done `y = df.MEDV.copy()` . This does not work for deleting
    columns, however; `del df.MEDV` would raise an error.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data has been loaded in its entirety, let's take a look at the
    DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can do `df.head()` or `df.tail()` to see a glimpse of the data and `len(df)`
    to make sure the number of samples is what we expect. Run the next few cells to
    see the head, tail, and length of `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3407f927-6154-49ed-9a5d-32a9e90fcad9.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5be64859-764c-43ca-8268-1d44953c0bc7.png)'
  prefs: []
  type: TYPE_IMG
- en: Each row is labeled with an index value, as seen in bold on the left side of
    the table.By default, these are a set of integers starting at 0 and incrementing
    by one for each row.
  prefs: []
  type: TYPE_NORMAL
- en: Printing `df.dtypes` will show the datatype contained within each column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the next cell to see the datatypes of each column.
  prefs: []
  type: TYPE_NORMAL
- en: For this dataset, we see that every field is a float and therefore most likely
    a continuous variable, including the target. This means that predicting the target
    variable is a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need to do is clean the data by dealing with any missing
    data, which Pandas automatically sets as `NaN` values. These can be identified
    by running `df.isnull()` , which returns a Boolean DataFrame of the same shape
    as `df.` To get the number of NaN''s per column, we can do `df.isnull().sum()`
    . Run the next cell to calculate the number of `NaN` values in each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d9256fa-2766-47dc-9fc8-9298a704a2b5.png)'
  prefs: []
  type: TYPE_IMG
- en: For this dataset, we see there are no NaN's, which means we have no immediate
    work to do in cleaning the data and can move on.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the analysis, the final thing we'll do before exploration is remove some
    of the columns. We won't bother looking at these, and instead focus on the remainder
    in more detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove some columns by running the cell that contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this is an entirely new dataset that we've never seen before, the first
    goal here is to understand the data. We've already seen the textual description
    of the data, which is important for qualitative understanding. We'll now compute
    a quantitative description.
  prefs: []
  type: TYPE_NORMAL
- en: Explore the Boston housing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Navigate to Subtopic *Data exploration in the Jupyter Notebook* and run the cell
    containing `df.describe()` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5b92da6c-2b53-4acc-a447-113021e353da.png)'
  prefs: []
  type: TYPE_IMG
- en: This computes various properties including the mean, standard deviation, minimum,
    and maximum for each column. This table gives a high-level idea of how everything
    is distributed. Note that we have taken the transform of the result by adding
    a .T to the output; this swaps the rows and columns. Going forward with the analysis,
    we will specify a set of columns to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell where these "focus columns" are defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This subset of columns can be selected from `df` using square brackets. Display
    this subset of the DataFrame by running `df[cols].head()` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d0f04873-7055-41ec-b6ef-4b1a0b7a2b70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a reminder, let''s recall what each of these columns is. From the dataset documentation,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: RM average number of rooms per dwelling
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: AGE proportion of owner-occupied units built prior to 1940
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TAX full-value property-tax rate per $10,000
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTAT % lower status of the population
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MEDV Median value of owner-occupied homes in $1000's
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To look for patterns in this data, we can start by calculating the pairwise correlations
    using `pd.DataFrame.corr`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the pairwise correlations for our selected columns by running the
    cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e79bc1bb-6cdf-4d9d-92cd-3be3db54084d.png)'
  prefs: []
  type: TYPE_IMG
- en: This resulting table shows the correlation score between each set of values.
    Large positive scores indicate a strong positive (that is, in the same direction)
    correlation.As expected, we see maximum values of 1 on the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pearson coefficient is defined as the co-variance between two variables,divided
    by the product of their standard deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c647c0e7-9bc6-4335-b95b-3ec6342cf11c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The co-variance, in turn, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6309711-d7e1-4a6d-ae40-9c909ad8b76d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, n is the number of samples, *x[i]* and *y[i]* are the individual samples
    being summed over, and ![](img/1f9d23ad-d253-4355-9289-7d79761bc52c.png) and ![](img/46020eb6-e869-471d-9fa5-2e3b323b128e.png)  are
    the means of each set.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of straining our eyes to look at the preceding table, it's nicer to
    visualize it with a heatmap. This can be done easily with Seaborn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the next cell to initialize the plotting environment, as discussed earlier
    in the chapter. Then, to create the heatmap, run the cell containing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3ec1a261-49e6-4b0f-819e-f39c15ac38a2.png)'
  prefs: []
  type: TYPE_IMG
- en: We call `sns.heatmap` and pass the pairwise correlation matrix as input. We
    use a custom color palette here to override the Seaborn default. The function
    returns a `matplotlib.axes` object which is referenced by the variable `ax`. The
    final figure is then saved as a high resolution PNG to the `figures` folder.
  prefs: []
  type: TYPE_NORMAL
- en: For the final step in our dataset exploration exercise, we'll visualize our
    data using Seaborn's `pairplot` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualize the DataFrame using Seaborn''s pairplot function. Run the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c86a31f8-972a-4b13-8cbb-e07e5d4a60c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Having previously used a heatmap to visualize a simple overview of the correlations,
    this plot allows us to see the relationships in far more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the histograms on the diagonal, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a: RM and MEDV have the closest shape to normal distributions.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'b: AGE is skewed to the left and LSTAT is skewed to the right (this may seem counter
    intuitive but skew is defined in terms of where the mean is positioned in relation
    to the max).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'c: For TAX, we find a large amount of the distribution is around 700\. This
    is also'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: evident from the scatter plots
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Taking a closer look at the **MEDV** histogram in the bottom right, we actually
    see something similar to **TAX** where there is a large upper-limit bin around
    $50,000\. Recall when we did `df.describe()` , the min and max of **MDEV** was
    5k and 50k, respectively. This suggests that median house values in the dataset
    were capped at 50k.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Predictive Analytics with Jupyter Notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuing our analysis of the Boston housing dataset, we can see that it presents
    us with a regression problem where we predict a continuous target variable given
    a set of features. In particular, we'll be predicting the median house value (**MEDV**).
    We'll train models that take only one feature as input to make this prediction.
    This way, the models will be conceptually simple to understand and we can focus
    more on the technical details of the scikit-learn API. Then, in the next chapter,
    you'll be more comfortable dealing with the relatively complicated models.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models with Seaborn and scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scroll to Subtopic`Introduction to predictive analytics` in the Jupyter Notebook
    and look just above at the pairplot we created in the previous section. In particular,
    look at the scatter plots in the bottom-left corner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7eb25257-8cc2-4530-af19-ec880503a76e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note how the number of rooms per house (**RM**) and the % of the population
    that is lower class (**LSTAT**) are highly correlated with the median house value
    (**MDEV**). Let''s pose the following question: how well can we predict **MDEV**
    given these variables?'
  prefs: []
  type: TYPE_NORMAL
- en: To help answer this, let's first visualize the relationships using Seaborn.
    We will draw the scatter plots along with the line of best fit linear models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw scatter plots along with the linear models by running the cell that contains
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e880df1f-bc8e-4f12-936c-bb362791395d.png)'
  prefs: []
  type: TYPE_IMG
- en: The line of best fit is calculated by minimizing the ordinary least squares
    error function, something Seaborn does automatically when we call the `regplot`
    function. Also note the shaded areas around the lines, which represent 95% confidence
    intervals.
  prefs: []
  type: TYPE_NORMAL
- en: These 95% confidence intervals are calculated by taking the standard deviation
    of data in bins perpendicular to the line of best fit, effectively determining
    the confidence intervals at each point along the line of best fit. In practice,
    this involves Seaborn bootstrapping the data, a process where new data is created
    through random sampling with replacement. The number of bootstrapped samples is
    automatically determined based on the size of the dataset, but can be manually
    set as well by passing the `n_boot` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seaborn can also be used to plot the residuals for these relationships. Plot
    the residuals by running the cell containing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ca395a1c-acc4-465a-b249-23a594b7248c.png)'
  prefs: []
  type: TYPE_IMG
- en: Each point on these residual plots is the difference between that sample (y)
    and the linear model prediction ( ŷ). Residuals greater than zero are data points
    that would be underestimated by the model. Likewise, residuals less than zero
    are data points that would be overestimated by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns in these plots can indicate sub optimal modeling. In each preceding
    case,we see diagonally arranged scatter points in the positive region. These are
    caused by the $50,000 cap on MEDV. The RM data is clustered nicely around 0, which indicates
    a good fit. On the other hand, LSTAT appears to be clustered lower than 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on from visualizations, the fits can be quantified by calculating the
    mean squared error. We''ll do this now using scikit-learn. Defile a function that
    calculates the line of best fit and mean squared error, by running the cell that
    contains the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the `get_mse` function, we first assign the variables y and x to the target
    **MDEV** and the dependent feature, respectively. These are cast as `NumPy` arrays
    by calling the values attribute. The dependent features array is reshaped to the
    format expected by scikit-learn; this is only necessary when modeling a one-dimensional
    feature space. The model is then instantiated and fitted on the data. For linear
    regression, the fitting consists of computing the model parameters using the ordinary
    least squares method (minimizing the sum of squared errors for each sample). Finally,
    after determining the parameters, we predict the target variable and use the results
    to calculate the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `get_mse` function for both RM and LSTAT, by running the cell containing the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2f6eb116-2c27-4a0b-9655-bc72b31b0631.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the **MSE**, it turns out the error is slightly lower for **LSTAT**.
    Looking back to the scatter plots, however, it appears that we might have even
    better success using a polynomial model for LSTAT. In the next activity, we will
    test this by computing a third order polynomial model with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forgetting about our Boston housing dataset for a minute, consider another
    real-world situation where you might employ polynomial regression. The following
    example is modeling weather data. In the following plot, we see temperatures (lines)
    and precipitations (bars) for Vancouver, BC, Canada:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03049e0f-c8d0-41a5-94fa-18f6412ef434.png)'
  prefs: []
  type: TYPE_IMG
- en: Any of these fields are likely to be fit quite well by a fourth-order polynomial.
    This would be a very valuable model to have, for example, if you were interested
    in predicting the temperature or precipitation for a continuous range of dates.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the data source for this here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://climate.weather.gc.ca/climate_normals/results_e.html?stnID=888.](http://climate.weather.gc.ca/climate_normals/results_e.html?stnID=888.)'
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Building a Third-Order Polynomial Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shifting our attention back to the Boston housing dataset, we would like to
    build a third order polynomial model to compare against the linear one. Recall
    the actual problem we are trying to solve: predicting the median house value,
    given the lower class population percentage. This model could benefit a prospective
    Boston house purchaser who cares about how much of their community would be lower
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: Use scikit-learn to fit a polynomial regression model to predict the median
    house value (MEDV), given the LSTAT values. We are hoping to build a model that
    has a lower meansquared error (MSE).
  prefs: []
  type: TYPE_NORMAL
- en: Linear models with Seaborn and scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scroll to the empty cells at the bottom of `Subtopic Introduction to Predictive
    Analysis in your Jupyter Notebook`.These will be found beneath the linear-model
    MSE calculation cell under the Activity heading.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should fill these empty cells in with code as we complete the activity.
    You may need to insert new cells as these become filled up; please do so as needed!
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that our data is contained in the DataFrame `df`, we will fist pull out
    our dependent feature and target variable using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is identical to what we did earlier for the linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out what x looks like by printing the fist few samples with print(x[:3])
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8abfeb62-7431-408c-8833-3eccf8787534.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how each element in the array is itself an array with length 1\. This
    is what` reshape(-1,1)` does, and it is the form expected by scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to transform x into "polynomial features". The rationale
    for this may not be immediately obvious but will be explained shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the appropriate transformation tool from scikit-learn and instantiate
    the third-degree polynomial feature transformer:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we simply have an instance of our feature transformer. Now, let's
    use it to transform the LSTAT feature (as stored in the variable x) by running
    the `fit_transform` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build the polynomial feature set by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Check out what `x_poly` looks like by printing the fist few samples with print(`x_poly[:3]`)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f3370ee-c31c-4727-a552-c84ce7b6b512.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike x, the arrays in each row now have length 4, where the values have been calculated
    as x⁰, x¹, x² and x³.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now going to use this data to fit a linear model. Labeling the features
    as a, b, c, and d, we will calculate the coefficients α[0], α[1], α[2], and α[3]
    and of the linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3d84ebf-c30f-4b47-9a2b-838a2180dc86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can plug in the definitions of a, b, c, and d, to get the following polynomial model,
    where the coefficients are the same as the previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6c3cd61-d065-4a26-92df-1986f0851a40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll import the Linear Regression class and build our linear classification
    model the same way as before, when we calculated the **MSE**. Run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the coefficients and print the polynomial model using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9b204ff8-27d6-41b5-b1c9-ac4b3b250c55.png)'
  prefs: []
  type: TYPE_IMG
- en: To get the actual model intercept, we have to add the `intercept_ and coef_[0]`attributes.
    The higher-order coefficients are then given by the remaining values of `coef_.`
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine the predicted values for each sample and calculate the residuals
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Print some of the residual values by running print(`resid_MEDV[:10]`) :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f2de5a79-3841-4241-a9a0-b08db4d56950.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll plot these soon to compare with the linear model residuals, but first
    we will calculate the **MSE**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code to print the MSE for the third-order polynomial model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/63360ff2-9188-47ac-9d96-324b2956b255.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, the **MSE** is significantly less for the polynomial model compared
    to the linear model (which was 38.5). This error metric can be converted to an
    average error in dollars by taking the square root. Doing this for the polynomial
    model, we find the average error for the median house value is only $5,300.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll visualize the model by plotting the polynomial line of best fit along
    with the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the polynomial model along with the samples by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cad9c13c-603c-49df-9042-deea5d0a3073.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we are plotting the red curve by calculating the polynomial model predictions
    on an array of x values. The array of x values was created using `np.linspace`,
    resulting in 50 values arranged evenly between 2 and 38.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll plot the corresponding residuals. Whereas we used Seaborn for this
    earlier, we'll have to do it manually to show results for a scikit-learn model.
    Since we already calculated the residuals earlier, as reference by the `resid_MEDV`
    variable, we simply need to plot this list of values on a scatter chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the residuals by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6f2a0295-d952-4279-b9f0-6b3c9783d272.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to the linear model LSTAT residual plot, the polynomial model residuals appear
    to be more closely clustered around y - ŷ = 0\. Note that y is the sample MEDV
    and ŷ is the predicted value. There are still clear patterns, such as the cluster near
    x = 7 and y = -7 that indicates suboptimal modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Having successfully modeled the data using a polynomial model, let's finish
    up this chapter by looking at categorical features. In particular, we are going
    to build a set of categorical features and use them to explore the dataset in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using Categorical Features for Segmentation Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, we find datasets where there are a mix of continuous and categorical
    fields. In such cases, we can learn about our data and find patterns by segmenting
    the continuous variables with the categorical fields.
  prefs: []
  type: TYPE_NORMAL
- en: As a specific example, imagine you are evaluating the return on investment from
    an ad campaign. The data you have access to contain measures of some calculated
    **return on investment (ROI)** metric. These values were calculated and recorded
    daily and you are analyzing data from the previous year. You have been tasked
    with finding data-driven insights on ways to improve the ad campaign. Looking
    at the ROI daily time series, you see a weekly oscillation in the data. Segmenting
    by day of the week, you find the following ROI distributions (where 0 represents
    the fist day of the week and 6 represents the last).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5d7241b-a0c9-4edc-94fa-5587503134ce.png)'
  prefs: []
  type: TYPE_IMG
- en: As a specific example, imagine you are evaluating the return on investment from
    an ad campaign. The data you have access to contain measures of some calculated
    return on investment (**ROI**) metric. These values were calculated and recorded
    daily and you are analyzing data from the previous year. You have been tasked
    with finding data-driven insights on ways to improve the ad campaign. Looking
    at the ROI daily time series, you see a weekly oscillation in the data. Segmenting
    by day of the week, you find the following ROI distributions (where 0 represents
    the fist day of the week and 6 represents the last).
  prefs: []
  type: TYPE_NORMAL
- en: Since we don't have any categorical fields in the Boston housing dataset we
    are working with, we'll create one by effectively discretizing a continuous field.
    In our case, this will involve binning the data into "low", "medium", and "high"
    categories. It's important to note that we are not simply creating a categorical
    data field to illustrate the data analysis concepts in this section. As will be
    seen, doing this can reveal insights from the data that would otherwise be difficult
    to notice or altogether unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Create categorical filelds from continuous variables and make segmented visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scroll up to the pair plot in the Jupyter Notebook where we compared MEDV, LSTAT,
    TAX, AGE, and RM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/23eae59d-883a-422a-a632-3517aba32418.png)'
  prefs: []
  type: TYPE_IMG
- en: Take a look at the panels containing AGE. As a reminder, this feature is defined
    as the *proportion of owner-occupied units built prior to 1940*. We are going
    to convert this feature to a categorical variable. Once it's been converted, we'll
    be able to replot this figure with each panel segmented by color according to
    the age category.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to Subtopic `Building and exploring categorical features` and click
    into the first cell. Type and execute the following to plot the AGE cumulative
    distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c7a01b08-8166-44f5-9c8b-f3e35f74ee26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we set `kde_kws={''lw'': 0}` in order to bypass plotting the kernel
    density estimate in the preceding figure.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the plot, there are very few samples with low AGE, whereas there
    are far more with a very large AGE. This is indicated by the steepness of the
    distribution on the far right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: The red lines indicate 1/3 and 2/3 points in the distribution. Looking at the
    places where our distribution intercepts these horizontal lines, we can see that
    only about 33% of the samples have AGE less than 55 and 33% of the samples have
    AGE greater than 90! In other words, a third of the housing communities have less
    than 55% of homes built prior to 1940\. These would be considered relatively new
    communities. On the other end of the spectrum, another third of the housing communities
    have over 90% of homes built prior to 1940\. These would be considered very old.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll use the places where the red horizontal lines intercept the distribution
    as a guide to split the feature into categories: **Relatively New**, R**elatively
    Old**, and **Very Old**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the segmentation points as 50 and 85, create a new categorical feature
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the very handy Pandas method apply, which applies a function
    to a given column or set of columns. The function being applied, in this case
    `get_ age_category`, should take one argument representing a row of data and return
    one value for the new column. In this case, the row of data being passed is just
    a single value, the AGE of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: The apply method is great because it can solve a variety of problems and allows
    for easily readable code. Often though, vectorized methods such as `pd.Series.str`
    can accomplish the same thing much faster. Therefore, it's advised to avoid using
    it if possible, especially when working with large datasets. We'll see some examples
    of vectorized methods in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Check on how many samples we've grouped into each age category by typing  `df.groupby('AGE_category').size()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: into a new cell and running
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/4f1ec257-1b37-4971-b906-e3830e0efde2.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the result, it can be seen that two class sizes are fairly equal,
    and the Very Old group is about 40% larger. We are interested in keeping the classes
    comparable in size, so that each is well-represented and it's straightforward
    to make inferences from the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may not always be possible to assign samples into classes evenly, and in
    real-world situations, it''s very common to find highly imbalanced classes. In
    such cases, it''s important to keep in mind that it will be difficult to make
    statistically significant claims with respect to the under-represented class.
    Predictive analytics with imbalanced classes can be particularly difficult. The
    following blog post offers an excellent summary on methods for handling imbalanced
    classes when doing machine learning: [https://svds.com/learning-imbalanced-classes/](https://svds.com/learning-imbalanced-classes/).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the target variable is distributed when segmented by our new feature `AGE_category`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a violin plot by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6ed1d55-c026-4cb8-b99e-9800c8c75191.png)'
  prefs: []
  type: TYPE_IMG
- en: The violin plot shows a kernel density estimate of the median house value distribution
    for each age category. We see that they all resemble a normal distribution. The
    Very Old group contains the lowest median house value samples and has a relatively
    large width, whereas the other groups are more tightly centered around their average.
    The young group is skewed to the high end, which is evident from the enlarged
    right half and position of the white dot in the thick black line within the body
    of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This white dot represents the mean and the thick black line spans roughly 50%
    of the population (it fills to the first quantile on either side of the white
    dot). The thin black line represents boxplot whiskers and spans 95% of the population.
    This inner visualization can be modified to show the individual data points instead
    by passing `inner='point' to sns.violinplot()` . Let's do that now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Redo the violin plot adding the inner=''point'' argument to the sns.violinplot call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/79627d7e-5d70-4123-bac1-dbc55c3ed7e1.png)'
  prefs: []
  type: TYPE_IMG
- en: It's good to make plots like this for test purposes in order to see how the
    underlying data connects to the visual. We can see, for example, how there are
    no median house values lower than roughly $16,000 for the Relatively New segment,
    and therefore the distribution tail actually contains no data. Due to the small
    size of our dataset (only about 500 rows), we can see this is the case for each
    segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Re-do the pairplot from earlier, but now include color labels for each AGE
    category.This is done by simply passing the hue argument, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3387bd48-7536-44ce-b68f-5fbaeb4dd8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the histograms, the underlying distributions of each segment appear
    similar for **RM** and **TAX**. The **LSTAT** distributions, on the other hand,
    look more distinct. We can focus on them in more detail by again using a violin
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a violin plot comparing the LSTAT distributions for each `AGE_category` segment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/284438ae-9698-441c-8476-b4b918a30101.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the **MEDV** violin plot, where each distribution had roughly the same
    width, here we see the width increasing along with **AGE**. Communities with primarily
    old houses (the Very Old segment) contain anywhere from very few to many lower
    class residents, whereas Relatively New communities are much more likely to be
    predominantly higher class, with over 95% of samples having less lower class percentages
    than the Very Old communities. This makes sense, because Relatively New neighborhoods
    would be more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen the fundamentals of data analysis in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: We began with usage instructions and features of Jupyter such as magic functions
    and tab completion. Then, transitioning to data-science-specific material, we
    introduced the most important libraries for data science with Python.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of the chapter, we ran an exploratory analysis in a live
    Jupyter Notebook. Here, we used visual assists such as scatter plots, histograms,
    and violin plots to deepen our understanding of the data. We also performed simple
    predictive modeling, a topic which will be the focus of the following chapter
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to approach predictive analytics, what
    things to consider when preparing the data for modeling, and how to implement
    and compare a variety of models using Jupyter Notebooks.
  prefs: []
  type: TYPE_NORMAL
