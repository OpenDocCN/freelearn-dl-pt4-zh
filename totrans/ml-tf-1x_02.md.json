["```py\ncd ~/workdir\nwget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz\ntar xvf notMNIST_small.tar.gz\n```", "```py\ncd ~/workdir\ngit clone https://github.com/mlwithtf/MLwithTF\ncd chapter_02\n```", "```py\npython download.py\n```", "```py\n from __future__ import print_function \n import os \n from six.moves.urllib.request import urlretrieve \n import datetime \n def downloadFile(fileURL, expected_size): \n    timeStampedDir=datetime.datetime.now()\n     .strftime(\"%Y.%m.%d_%I.%M.%S\") \n    os.makedirs(timeStampedDir) \n    fileNameLocal = timeStampedDir + \"/\" +     \n    fileURL.split('/')[-1] \n    print ('Attempting to download ' + fileURL) \n    print ('File will be stored in ' + fileNameLocal) \n    filename, _ = urlretrieve(fileURL, fileNameLocal) \n    statinfo = os.stat(filename) \n    if statinfo.st_size == expected_size: \n        print('Found and verified', filename) \n    else: \n        raise Exception('Could not get ' + filename) \n    return filename \n```", "```py\n tst_set = \n downloadFile('http://yaroslavvb.com/upload/notMNIST/notMNIST_small\n .tar.gz', 8458043) \n```", "```py\n import os, sys, tarfile \n from os.path import basename \n\n def extractFile(filename): \n    timeStampedDir=datetime.datetime.now()\n     .strftime(\"%Y.%m.%d_%I.%M.%S\") \n    tar = tarfile.open(filename) \n    sys.stdout.flush() \n    tar.extractall(timeStampedDir) \n    tar.close() \n    return timeStampedDir + \"/\" + os.listdir(timeStampedDir)[0] \n```", "```py\n tst_src='http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.\n gz' \n tst_set = downloadFile(tst_src, 8458043) \n print ('Test set stored in: ' + tst_set) \n tst_files = extractFile(tst_set) \n print ('Test file set stored in: ' + tst_files) \n```", "```py\nsudo apt-get install python-numpy python-scipy python-matplotlib \nipython ipython-notebook python-pandas python-sympy python-nose\n```", "```py\nsudo pip install ndimage\nsudo apt-get install libatlas3-base-dev gcc gfortran g++\n```", "```py\n import numpy as np \n from IPython.display import display, Image \n from scipy import ndimage \n\n image_size = 28  # Pixel width and height. \n pixel_depth = 255.0  # Number of levels per pixel. \n def loadClass(folder): \n  image_files = os.listdir(folder) \n  dataset = np.ndarray(shape=(len(image_files), \n  image_size, \n   image_size), dtype=np.float32) \n  image_index = 0 \n  print(folder) \n  for image in os.listdir(folder): \n    image_file = os.path.join(folder, image) \n    try: \n      image_data =  \n     (ndimage.imread(image_file).astype(float) -  \n                    pixel_depth / 2) / pixel_depth \n      if image_data.shape != (image_size, image_size): \n        raise Exception('Unexpected image shape: %s' % \n     str(image_data.shape)) \n      dataset[image_index, :, :] = image_data \n      image_index += 1 \n     except IOError as e: l\n      print('Could not read:', image_file, ':', e, '-   \n      it\\'s ok, \n       skipping.') \n     return dataset[0:image_index, :, :] \n```", "```py\n classFolders = [os.path.join(tst_files, d) for d in \n os.listdir(tst_files) if os.path.isdir(os.path.join(tst_files, \n d))] \n print (classFolders) \n for cf in classFolders: \n    print (\"\\n\\nExaming class folder \" + cf) \n    dataset=loadClass(cf) \n    print (dataset.shape) \n```", "```py\n def makePickle(imgSrcPath): \n    data_folders = [os.path.join(tst_files, d) for d in \n     os.listdir(tst_files) if os.path.isdir(os.path.join(tst_files, \n     d))] \n    dataset_names = [] \n    for folder in data_folders: \n        set_filename = folder + '.pickle' \n        dataset_names.append(set_filename) \n        print('Pickling %s.' % set_filename) \n        dataset = loadClass(folder) \n        try: \n            with open(set_filename, 'wb') as f: \n                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) \n        except Exception as e: \n            print('Unable to save data to', set_filename, ':', e) \n    return dataset_names \n```", "```py\n def randomize(dataset, labels): \n    permutation = np.random.permutation(labels.shape[0]) \n    shuffled_dataset = dataset[permutation, :, :] \n    shuffled_labels = labels[permutation] \n    return shuffled_dataset, shuffled_labels  \n\n def make_arrays(nb_rows, img_size): \n    if nb_rows: \n        dataset = np.ndarray((nb_rows, img_size, img_size),   \n dtype=np.float32) \n        labels = np.ndarray(nb_rows, dtype=np.int32) \n    else: \n        dataset, labels = None, None \n    return dataset, labels \n\n def merge_datasets(pickle_files, train_size, valid_size=0): \n  num_classes = len(pickle_files) \n  valid_dataset, valid_labels = make_arrays(valid_size,  \n  image_size) \n  train_dataset, train_labels = make_arrays(train_size,  \n  image_size) \n  vsize_per_class = valid_size // num_classes \n  tsize_per_class = train_size // num_classes \n\n  start_v, start_t = 0, 0 \n  end_v, end_t = vsize_per_class, tsize_per_class \n  end_l = vsize_per_class+tsize_per_class \n  for label, pickle_file in enumerate(pickle_files): \n    try: \n      with open(pickle_file, 'rb') as f: \n        letter_set = pickle.load(f) \n        np.random.shuffle(letter_set) \n        if valid_dataset is not None: \n          valid_letter = letter_set[:vsize_per_class, :, :] \n          valid_dataset[start_v:end_v, :, :] = valid_letter \n          valid_labels[start_v:end_v] = label \n          start_v += vsize_per_class \n          end_v += vsize_per_class \n\n        train_letter = letter_set[vsize_per_class:end_l, :, :] \n        train_dataset[start_t:end_t, :, :] = train_letter \n        train_labels[start_t:end_t] = label \n        start_t += tsize_per_class \n        end_t += tsize_per_class \n    except Exception as e: \n      print('Unable to process data from', pickle_file, ':', e) \n      raise \n\n  return valid_dataset, valid_labels, train_dataset, train_labels \n```", "```py\n     train_size = 200000 \n     valid_size = 10000 \n     test_size = 10000 \n```", "```py\n valid_dataset, valid_labels, train_dataset, train_labels = \n  merge_datasets( \n   picklenamesTrn, train_size, valid_size) \n _, _, test_dataset, test_labels = merge_datasets(picklenamesTst, \n  test_size) \n train_dataset, train_labels = randomize(train_dataset, \n  train_labels) \n test_dataset, test_labels = randomize(test_dataset, test_labels) \n valid_dataset, valid_labels = randomize(valid_dataset, \n  valid_labels) \n```", "```py\n print('Training:', train_dataset.shape, train_labels.shape) \n print('Validation:', valid_dataset.shape, valid_labels.shape) \n print('Testing:', test_dataset.shape, test_labels.shape) \n```", "```py\n pickle_file = 'notMNIST.pickle' \n\n try: \n   f = open(pickle_file, 'wb') \n   save = { \n      'datTrn': train_dataset, \n    'labTrn': train_labels, \n    'datVal': valid_dataset, \n    'labVal': valid_labels, \n    'datTst': test_dataset, \n    'labTst': test_labels, \n     } \n   pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) \n   f.close() \n except Exception as e: \n   print('Unable to save data to', pickle_file, ':', e) \n   raise \n\n statinfo = os.stat(pickle_file) \n print('Compressed pickle size:', statinfo.st_size) \n```", "```py\n def reformat(dataset, labels): \n   dataset = dataset.reshape((-1, image_size * \n    image_size)).astype(np.float32) \n   labels = (np.arange(num_labels) == \n    labels[:,None]).astype(np.float32) \n   return dataset, labels \n```", "```py\n with open(pickle_file, 'rb') as f: \n   pkl = pickle.load(f) \n   train_dataset, train_labels = reformat(pkl['datTrn'], \n    pkl['labTrn']) \n   valid_dataset, valid_labels = reformat(pkl['datVal'], \n    pkl['labVal']) \n   test_dataset, test_labels = reformat(pkl['datTst'], \n    pkl['labTst']) \n```", "```py\n import sys, os\n import tensorflow as tf\n import numpy as np\n sys.path.append(os.path.realpath('..'))\n import data_utils\n import logmanager \n```", "```py\n batch_size = 128\n num_steps = 10000\n learning_rate = 0.3\n data_showing_step = 500\n```", "```py\n dataset, image_size, num_of_classes, num_of_channels =  \n data_utils.prepare_not_mnist_dataset(root_dir=\"..\")\n dataset = data_utils.reformat(dataset, image_size, num_of_channels,   \n num_of_classes)\n print('Training set', dataset.train_dataset.shape,  \n dataset.train_labels.shape)\n print('Validation set', dataset.valid_dataset.shape,  \n dataset.valid_labels.shape)\n print('Test set', dataset.test_dataset.shape,  \n dataset.test_labels.shape)\n```", "```py\n graph = tf.Graph()\n with graph.as_default():\n # Input data. For the training data, we use a placeholder that will  \n be fed\n # at run time with a training minibatch.\n tf_train_dataset = tf.placeholder(tf.float32,\n shape=(batch_size, image_size * image_size * num_of_channels))\n tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,  \n num_of_classes))\n tf_valid_dataset = tf.constant(dataset.valid_dataset)\n tf_test_dataset = tf.constant(dataset.test_dataset)\n # Variables.\n weights = {\n 'fc1': tf.Variable(tf.truncated_normal([image_size * image_size *  \n num_of_channels, num_of_classes])),\n 'fc2': tf.Variable(tf.truncated_normal([num_of_classes,  \n num_of_classes]))\n }\n biases = {\n 'fc1': tf.Variable(tf.zeros([num_of_classes])),\n 'fc2': tf.Variable(tf.zeros([num_of_classes]))\n }\n # Training computation.\n logits = nn_model(tf_train_dataset, weights, biases)\n loss = tf.reduce_mean(\n tf.nn.softmax_cross_entropy_with_logits(logits=logits,  \n labels=tf_train_labels))\n # Optimizer.\n optimizer =  \n tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n # Predictions for the training, validation, and test data.\n train_prediction = tf.nn.softmax(logits)\n valid_prediction = tf.nn.softmax(nn_model(tf_valid_dataset,  \n weights, biases))\n test_prediction = tf.nn.softmax(nn_model(tf_test_dataset, weights,  \n biases))\n The most important line here is the nn_model where the neural  \n network is defined:\n def nn_model(data, weights, biases):\n layer_fc1 = tf.matmul(data, weights['fc1']) + biases['fc1']\n relu_layer = tf.nn.relu(layer_fc1)\n return tf.matmul(relu_layer, weights['fc2']) + biases['fc2']\n```", "```py\n loss = tf.reduce_mean(\n tf.nn.softmax_cross_entropy_with_logits(logits=logits,  \n labels=tf_train_labels))\n # Optimizer.\n optimizer =  \n tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n```", "```py\npython training.py\n```", "```py\n def accuracy(predictions, labels): \n  return (100.0 * np.sum(np.argmax(predictions, 1) == \n   np.argmax(labels, 1)) \n          / predictions.shape[0])\n```", "```py\n saver = tf.train.Saver() \n```", "```py\n ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir) \n if ckpt and ckpt.model_checkpoint_path: \n saver.restore(sess, ckpt.model_checkpoint_path) \n```"]