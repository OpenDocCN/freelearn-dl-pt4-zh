["```py\nfrom __future__ import print_function, division\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\"\"\"\ndefine all the constants\n\"\"\"\nnumEpochs = 10\nseriesLength = 50000\nbackpropagationLength = 15\nstateSize = 4\nnumClasses = 2\nechoStep = 3\nbatchSize = 5\nnum_batches = seriesLength // batchSize // backpropagationLength\n\n\"\"\"\ngenerate data\n\"\"\"\ndef generateData():\n    x = np.array(np.random.choice(2, seriesLength, p=[0.5, 0.5]))\n    y = np.roll(x, echoStep)\n    y[0:echoStep] = 0\n\n    x = x.reshape((batchSize, -1))\n    y = y.reshape((batchSize, -1))\n\n    return (x, y)\n\n\"\"\"\nstart computational graph\n\"\"\"\nbatchXHolder = tf.placeholder(tf.float32, [batchSize, backpropagationLength], name=\"x_input\")\nbatchYHolder = tf.placeholder(tf.int32, [batchSize, backpropagationLength], name=\"y_input\")\n\ninitState = tf.placeholder(tf.float32, [batchSize, stateSize], \"rnn_init_state\")\n\nW = tf.Variable(np.random.rand(stateSize+1, stateSize), dtype=tf.float32, name=\"weight1\")\nbias1 = tf.Variable(np.zeros((1,stateSize)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(stateSize, numClasses),dtype=tf.float32, name=\"weight2\")\nbias2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n\ntf.summary.histogram(name=\"weights\", values=W)\n\n# Unpack columns\ninputsSeries = tf.unstack(batchXHolder, axis=1, name=\"input_series\")\nlabelsSeries = tf.unstack(batchYHolder, axis=1, name=\"labels_series\")\n\n# Forward pass\ncurrentState = initState\nstatesSeries = []\nfor currentInput in inputsSeries:\n    currentInput = tf.reshape(currentInput, [batchSize, 1], name=\"current_input\")\n    inputAndStateConcatenated = tf.concat([currentInput, currentState], 1, name=\"input_state_concat\")\n\n    nextState = tf.tanh(tf.matmul(inputAndStateConcatenated, W) + bias1, name=\"next_state\")\n    statesSeries.append(nextState)\n    currentState = nextState\n\n# calculate loss\nlogits_series = [tf.matmul(state, W2) + bias2 for state in statesSeries]\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits) for logits, labels in zip(logits_series,labelsSeries)]\ntotal_loss = tf.reduce_mean(losses, name=\"total_loss\")\n\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss, name=\"training\")\n\n\"\"\"\nplot computation\n\"\"\"\ndef plot(loss_list, predictions_series, batchX, batchY):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batchSeriesIdx in range(5):\n        oneHotOutputSeries = np.array(predictions_series)[:, batchSeriesIdx, :]\n        singleOutputSeries = np.array([(1 if out[0] < 0.5 else 0) for out in oneHotOutputSeries])\n\n        plt.subplot(2, 3, batchSeriesIdx + 2)\n        plt.cla()\n        plt.axis([0, backpropagationLength, 0, 2])\n        left_offset = range(backpropagationLength)\n        plt.bar(left_offset, batchX[batchSeriesIdx, :], width=1, color=\"blue\")\n        plt.bar(left_offset, batchY[batchSeriesIdx, :] * 0.5, width=1, color=\"red\")\n        plt.bar(left_offset, singleOutputSeries * 0.3, width=1, color=\"green\")\n\n    plt.draw()\n    plt.pause(0.0001)\n\n\"\"\"\nrun the graph\n\"\"\"\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"logs\", graph=tf.get_default_graph())\n    sess.run(tf.global_variables_initializer())\n    plt.ion()\n    plt.figure()\n    plt.show()\n    loss_list = []\n\n    for epoch_idx in range(numEpochs):\n        x,y = generateData()\n        _current_state = np.zeros((batchSize, stateSize))\n\n        print(\"New data, epoch\", epoch_idx)\n\n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * backpropagationLength\n            end_idx = start_idx + backpropagationLength\n\n            batchX = x[:,start_idx:end_idx]\n            batchY = y[:,start_idx:end_idx]\n\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                [total_loss, train_step, currentState, predictions_series],\n                feed_dict={\n                    batchXHolder:batchX,\n                    batchYHolder:batchY,\n                    initState:_current_state\n                })\n\n            loss_list.append(_total_loss)\n\n            # fix the cost summary later\n            tf.summary.scalar(name=\"totalloss\", tensor=_total_loss)\n\n            if batch_idx%100 == 0:\n                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n                plot(loss_list, _predictions_series, batchX, batchY)\n\nplt.ioff()\nplt.show()\n```", "```py\nNew data, epoch 0\nStep 0 Loss 0.777418\nStep 600 Loss 0.693907\nNew data, epoch 1\nStep 0 Loss 0.690996\nStep 600 Loss 0.691115\nNew data, epoch 2\nStep 0 Loss 0.69259\nStep 600 Loss 0.685826\nNew data, epoch 3\nStep 0 Loss 0.684189\nStep 600 Loss 0.690608\nNew data, epoch 4\nStep 0 Loss 0.691302\nStep 600 Loss 0.691309\nNew data, epoch 5\nStep 0 Loss 0.69172\nStep 600 Loss 0.694034\nNew data, epoch 6\nStep 0 Loss 0.692927\nStep 600 Loss 0.42796\nNew data, epoch 7\nStep 0 Loss 0.42423\nStep 600 Loss 0.00845207\nNew data, epoch 8\nStep 0 Loss 0.188478\nStep 500 Loss 0.00427217\n```", "```py\nfrom __future__ import print_function, division\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\"\"\"\ndefine all the constants\n\"\"\"\nnumEpochs = 10\nseriesLength = 50000\nbackpropagationLength = 15\nstateSize = 4\nnumClasses = 2\nechoStep = 3\nbatchSize = 5\nnum_batches = seriesLength // batchSize // backpropagationLength\n\n\"\"\"\ngenerate data\n\"\"\"\ndef generateData():\n    x = np.array(np.random.choice(2, seriesLength, p=[0.5, 0.5]))\n    y = np.roll(x, echoStep)\n    y[0:echoStep] = 0\n\n    x = x.reshape((batchSize, -1))\n    y = y.reshape((batchSize, -1))\n\n    return (x, y)\n\n\"\"\"\nstart computational graph\n\"\"\"\nbatchXHolder = tf.placeholder(tf.float32, [batchSize, backpropagationLength], name=\"x_input\")\nbatchYHolder = tf.placeholder(tf.int32, [batchSize, backpropagationLength], name=\"y_input\")\n\ninitState = tf.placeholder(tf.float32, [batchSize, stateSize], \"rnn_init_state\")\n\nW = tf.Variable(np.random.rand(stateSize+1, stateSize), dtype=tf.float32, name=\"weight1\")\nbias1 = tf.Variable(np.zeros((1,stateSize)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(stateSize, numClasses),dtype=tf.float32, name=\"weight2\")\nbias2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n\ntf.summary.histogram(name=\"weights\", values=W)\n\n# Unpack columns\ninputsSeries = tf.split(axis=1, num_or_size_splits=backpropagationLength, value=batchXHolder)\nlabelsSeries = tf.unstack(batchYHolder, axis=1)\n\n# Forward passes\nfrom tensorflow.contrib import rnn\ncell = rnn.BasicRNNCell(stateSize)\nstatesSeries, currentState = rnn.static_rnn(cell, inputsSeries, initState)\n\n# calculate loss\nlogits_series = [tf.matmul(state, W2) + bias2 for state in statesSeries]\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits) for logits, labels in zip(logits_series,labelsSeries)]\ntotal_loss = tf.reduce_mean(losses, name=\"total_loss\")\n\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss, name=\"training\")\n\n\"\"\"\nplot computation\n\"\"\"\ndef plot(loss_list, predictions_series, batchX, batchY):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batchSeriesIdx in range(5):\n        oneHotOutputSeries = np.array(predictions_series)[:, batchSeriesIdx, :]\n        singleOutputSeries = np.array([(1 if out[0] < 0.5 else 0) for out in oneHotOutputSeries])\n\n        plt.subplot(2, 3, batchSeriesIdx + 2)\n        plt.cla()\n        plt.axis([0, backpropagationLength, 0, 2])\n        left_offset = range(backpropagationLength)\n        plt.bar(left_offset, batchX[batchSeriesIdx, :], width=1, color=\"blue\")\n        plt.bar(left_offset, batchY[batchSeriesIdx, :] * 0.5, width=1, color=\"red\")\n        plt.bar(left_offset, singleOutputSeries * 0.3, width=1, color=\"green\")\n\n    plt.draw()\n    plt.pause(0.0001)\n\n\"\"\"\nrun the graph\n\"\"\"\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"logs\", graph=tf.get_default_graph())\n    sess.run(tf.global_variables_initializer())\n    plt.ion()\n    plt.figure()\n    plt.show()\n    loss_list = []\n\n    for epoch_idx in range(numEpochs):\n        x,y = generateData()\n        _current_state = np.zeros((batchSize, stateSize))\n\n        print(\"New data, epoch\", epoch_idx)\n\n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * backpropagationLength\n            end_idx = start_idx + backpropagationLength\n\n            batchX = x[:,start_idx:end_idx]\n            batchY = y[:,start_idx:end_idx]\n\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                [total_loss, train_step, currentState, predictions_series],\n                feed_dict={\n                    batchXHolder:batchX,\n                    batchYHolder:batchY,\n                    initState:_current_state\n                })\n\n            loss_list.append(_total_loss)\n\n            # fix the cost summary later\n            tf.summary.scalar(name=\"totalloss\", tensor=_total_loss)\n\n            if batch_idx%100 == 0:\n                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n                plot(loss_list, _predictions_series, batchX, batchY)\n\nplt.ioff()\nplt.show()\n```", "```py\nNew data, epoch 0\nStep 0 Loss 0.688437\nStep 600 Loss 0.00107078\nNew data, epoch 1\nStep 0 Loss 0.214923\nStep 600 Loss 0.00111716\nNew data, epoch 2\nStep 0 Loss 0.214962\nStep 600 Loss 0.000730697\nNew data, epoch 3\nStep 0 Loss 0.276177\nStep 600 Loss 0.000362316\nNew data, epoch 4\nStep 0 Loss 0.1641\nStep 600 Loss 0.00025342\nNew data, epoch 5\nStep 0 Loss 0.0947087\nStep 600 Loss 0.000276762\n```", "```py\nfrom __future__ import print_function, division\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.contrib import rnn\n\n\"\"\"\ndefine all the constants\n\"\"\"\nnumEpochs = 10\nseriesLength = 50000\nbackpropagationLength = 15\nstateSize = 4\nnumClasses = 2\nechoStep = 3\nbatchSize = 5\nnum_batches = seriesLength // batchSize // backpropagationLength\n\n\"\"\"\ngenerate data\n\"\"\"\ndef generateData():\n    x = np.array(np.random.choice(2, seriesLength, p=[0.5, 0.5]))\n    y = np.roll(x, echoStep)\n    y[0:echoStep] = 0\n\n    x = x.reshape((batchSize, -1))\n    y = y.reshape((batchSize, -1))\n\n    return (x, y)\n\n\"\"\"\nstart computational graph\n\"\"\"\nbatchXHolder = tf.placeholder(tf.float32, [batchSize, backpropagationLength], name=\"x_input\")\nbatchYHolder = tf.placeholder(tf.int32, [batchSize, backpropagationLength], name=\"y_input\")\n\n# rnn replace\n#initState = tf.placeholder(tf.float32, [batchSize, stateSize], \"rnn_init_state\")\n\ncellState = tf.placeholder(tf.float32, [batchSize, stateSize])\nhiddenState = tf.placeholder(tf.float32, [batchSize, stateSize])\ninitState = rnn.LSTMStateTuple(cellState, hiddenState)\n\nW = tf.Variable(np.random.rand(stateSize+1, stateSize), dtype=tf.float32, name=\"weight1\")\nbias1 = tf.Variable(np.zeros((1,stateSize)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(stateSize, numClasses),dtype=tf.float32, name=\"weight2\")\nbias2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n\ntf.summary.histogram(name=\"weights\", values=W)\n\n# Unpack columns\ninputsSeries = tf.split(axis=1, num_or_size_splits=backpropagationLength, value=batchXHolder)\nlabelsSeries = tf.unstack(batchYHolder, axis=1)\n\n# Forward passes\n\n# rnn replace\n# cell = rnn.BasicRNNCell(stateSize)\n# statesSeries, currentState = rnn.static_rnn(cell, inputsSeries, initState)\n\ncell = rnn.BasicLSTMCell(stateSize, state_is_tuple=True)\nstatesSeries, currentState = rnn.static_rnn(cell, inputsSeries, initState)\n\n# calculate loss\nlogits_series = [tf.matmul(state, W2) + bias2 for state in statesSeries]\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits) for logits, labels in zip(logits_series,labelsSeries)]\ntotal_loss = tf.reduce_mean(losses, name=\"total_loss\")\n\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss, name=\"training\")\n\n\"\"\"\nplot computation\n\"\"\"\ndef plot(loss_list, predictions_series, batchX, batchY):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batchSeriesIdx in range(5):\n        oneHotOutputSeries = np.array(predictions_series)[:, batchSeriesIdx, :]\n        singleOutputSeries = np.array([(1 if out[0] < 0.5 else 0) for out in oneHotOutputSeries])\n\n        plt.subplot(2, 3, batchSeriesIdx + 2)\n        plt.cla()\n        plt.axis([0, backpropagationLength, 0, 2])\n        left_offset = range(backpropagationLength)\n        plt.bar(left_offset, batchX[batchSeriesIdx, :], width=1, color=\"blue\")\n        plt.bar(left_offset, batchY[batchSeriesIdx, :] * 0.5, width=1, color=\"red\")\n        plt.bar(left_offset, singleOutputSeries * 0.3, width=1, color=\"green\")\n\n    plt.draw()\n    plt.pause(0.0001)\n\n\"\"\"\nrun the graph\n\"\"\"\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"logs\", graph=tf.get_default_graph())\n    sess.run(tf.global_variables_initializer())\n    plt.ion()\n    plt.figure()\n    plt.show()\n    loss_list = []\n\n    for epoch_idx in range(numEpochs):\n        x,y = generateData()\n\n        # rnn remove\n        # _current_state = np.zeros((batchSize, stateSize))\n\n        _current_cell_state = np.zeros((batchSize, stateSize))\n        _current_hidden_state = np.zeros((batchSize, stateSize))\n\n        print(\"New data, epoch\", epoch_idx)\n\n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * backpropagationLength\n            end_idx = start_idx + backpropagationLength\n\n            batchX = x[:,start_idx:end_idx]\n            batchY = y[:,start_idx:end_idx]\n\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                [total_loss, train_step, currentState, predictions_series],\n                feed_dict={\n                    batchXHolder:batchX,\n                    batchYHolder:batchY,\n                    cellState: _current_cell_state,\n                    hiddenState: _current_hidden_state\n                })\n\n            _current_cell_state, _current_hidden_state = _current_state\n\n            loss_list.append(_total_loss)\n\n            # fix the cost summary later\n            tf.summary.scalar(name=\"totalloss\", tensor=_total_loss)\n\n            if batch_idx%100 == 0:\n                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n                plot(loss_list, _predictions_series, batchX, batchY)\n\nplt.ioff()\nplt.show()\n```", "```py\nNew data, epoch 0\nStep 0 Loss 0.696803\nStep 600 Loss 0.00743465\nNew data, epoch 1\nStep 0 Loss 0.404039\nStep 600 Loss 0.00243205\nNew data, epoch 2\nStep 0 Loss 1.11536\nStep 600 Loss 0.00140995\nNew data, epoch 3\nStep 0 Loss 0.858743\nStep 600 Loss 0.00141037\n```", "```py\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport utility\nfrom tqdm import tqdm\nfrom urllib.request import urlretrieve\nfrom os.path import isfile, isdir\nimport zipfile\nfrom collections import Counter\nimport random\n\ndataDir = 'data'\ndataFile = 'text8.zip'\ndatasetName = 'text 8 data set'\n\n'''\ntrack progress of file download\n'''\n\nclass DownloadProgress(tqdm):\n    lastBlock = 0\n\n    def hook(self, blockNum=1, blockSize=1, totalSize=None):\n        self.total = totalSize\n        self.update((blockNum - self.lastBlock) * blockSize)\n        self.lastBlock = blockNum\n\nif not isfile(dataFile):\n    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc=datasetName) as progressBar:\n        urlretrieve('http://mattmahoney.net/dc/text8.zip', dataFile, progressBar.hook)\n\nif not isdir(dataDir):\n    with zipfile.ZipFile(dataFile) as zipRef:\n        zipRef.extractall(dataDir)\n\nwith open('data/text8') as f:\n    text = f.read()\n\n'''\npre process the downloaded wiki text\n'''\nwords = utility.preProcess(text)\nprint(words[:30])\n\nprint('Total words: {}'.format(len(words)))\nprint('Unique words: {}'.format(len(set(words))))\n\n'''\nconvert words to integers\n'''\nint2vocab, vocab2int = utility.lookupTable(words)\nintWords = [vocab2int[word] for word in words]\nprint('test')\n\n'''\nsub sampling (***think of words as int's***)\n'''\nthreshold = 1e-5\nwordCounts = Counter(intWords)\ntotalCount = len(intWords)\nfrequency = {word: count / totalCount for word, count in wordCounts.items()}\nprobOfWords = {word: 1 - np.sqrt(threshold / frequency[word]) for word in wordCounts}\ntrainWords = [word for word in intWords if random.random() < (1 - probOfWords[word])]\n\n'''\nget window batches\n'''\n\ndef getTarget(words, index, windowSize=5):\n    rNum = np.random.randint(1, windowSize + 1)\n    start = index - rNum if (index - rNum) > 0 else 0\n    stop = index + rNum\n    targetWords = set(words[start:index] + words[index + 1:stop + 1])\n\n    return list(targetWords)\n\n'''\nCreate a generator of word batches as a tuple (inputs, targets)\n'''\n\ndef getBatches(words, batchSize, windowSize=5):\n    nBatches = len(words) // batchSize\n    print('no. of batches {}'.format(nBatches))\n\n    # only full batches\n    words = words[:nBatches * batchSize]\n\n    start = 0\n    for index in range(0, len(words), batchSize):\n        x = []\n        y = []\n        stop = start + batchSize\n        batchWords = words[start:stop]\n        for idx in range(0, len(batchWords), 1):\n            yBatch = getTarget(batchWords, idx, windowSize)\n            y.extend(yBatch)\n            x.extend([batchWords[idx]] * len(yBatch))\n        start = stop + 1\n        yield x, y\n\n'''\nstart computational graph\n'''\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    netInputs = tf.placeholder(tf.int32, [None], name='inputS')\n    netLabels = tf.placeholder(tf.int32, [None, None], name='labelS')\n\n'''\ncreate embedding layer\n'''\nnVocab = len(int2vocab)\nnEmbedding = 300\nwith train_graph.as_default():\n    embedding = tf.Variable(tf.random_uniform((nVocab, nEmbedding), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, netInputs)\n\n'''\nBelow, create weights and biases for the softmax layer. Then, use tf.nn.sampled_softmax_loss to calculate the loss\n'''\nn_sampled = 100\nwith train_graph.as_default():\n    soft_W = tf.Variable(tf.truncated_normal((nVocab, nEmbedding)))\n    soft_b = tf.Variable(tf.zeros(nVocab), name=\"softmax_bias\")\n\n    # Calculate the loss using negative sampling\n    loss = tf.nn.sampled_softmax_loss(\n        weights=soft_W,\n        biases=soft_b,\n        labels=netLabels,\n        inputs=embed,\n        num_sampled=n_sampled,\n        num_classes=nVocab)\n\n    cost = tf.reduce_mean(loss)\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n\n'''\nHere we're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them. \nIt's a nice way to check that our embedding table is grouping together words with similar semantic meanings.\n'''\nwith train_graph.as_default():\n    validSize = 16\n    validWindow = 100\n\n    validExamples = np.array(random.sample(range(validWindow), validSize // 2))\n    validExamples = np.append(validExamples,\n                               random.sample(range(1000, 1000 + validWindow), validSize // 2))\n\n    validDataset = tf.constant(validExamples, dtype=tf.int32)\n\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n    normalizedEmbedding = embedding / norm\n    valid_embedding = tf.nn.embedding_lookup(normalizedEmbedding, validDataset)\n    similarity = tf.matmul(valid_embedding, tf.transpose(normalizedEmbedding))\n\n'''\nTrain the network. Every 100 batches it reports the training loss. Every 1000 batches, it'll print out the validation\nwords.\n'''\nepochs = 10\nbatch_size = 1000\nwindow_size = 10\n\nwith train_graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=train_graph) as sess:\n    iteration = 1\n    loss = 0\n    sess.run(tf.global_variables_initializer())\n\n    for e in range(1, epochs + 1):\n        batches = getBatches(trainWords, batch_size, window_size)\n        start = time.time()\n        for x, y in batches:\n\n            feed = {netInputs: x,\n                    netLabels: np.array(y)[:, None]}\n            trainLoss, _ = sess.run([cost, optimizer], feed_dict=feed)\n\n            loss += trainLoss\n\n            if iteration % 100 == 0:\n                end = time.time()\n                print(\"Epoch {}/{}\".format(e, epochs),\n                      \"Iteration: {}\".format(iteration),\n                      \"Avg. Training loss: {:.4f}\".format(loss / 100),\n                      \"{:.4f} sec/batch\".format((end - start) / 100))\n                loss = 0\n                start = time.time()\n\n            if iteration % 1000 == 0:\n                sim = similarity.eval()\n                for i in range(validSize):\n                    validWord = int2vocab[validExamples[i]]\n                    topK = 8\n                    nearest = (-sim[i, :]).argsort()[1:topK + 1]\n                    log = 'Nearest to %s:' % validWord\n                    for k in range(topK):\n                        closeWord = int2vocab[nearest[k]]\n                        logStatement = '%s %s,' % (log, closeWord)\n                    print(logStatement)\n\n            iteration += 1\n    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n    embed_mat = sess.run(normalizedEmbedding)\n\n'''\nRestore the trained network if you need to\n'''\nwith train_graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    embed_mat = sess.run(embedding)\n\n'''\nBelow we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project \nthese vectors into two dimensions while preserving local structure. \n'''\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nvizWords = 500\ntsne = TSNE()\nembedTSNE = tsne.fit_transform(embed_mat[:vizWords, :])\n\nfig, ax = plt.subplots(figsize=(14, 14))\nfor idx in range(vizWords):\n    plt.scatter(*embedTSNE[idx, :], color='steelblue')\n    plt.annotate(int2vocab[idx], (embedTSNE[idx, 0], embedTSNE[idx, 1]), alpha=0.7)\n```", "```py\nTotal words: 16680599\n Unique words: 63641\n no. of batches 4626\nEpoch 1/10 Iteration: 100 Avg. Training loss: 21.7284 0.3363 sec/batch\n Epoch 1/10 Iteration: 1000 Avg. Training loss: 20.2269 0.3668 sec/batch\n\nNearest to but: universities, hungry, kyu, grandiose, edema, patty, stores, psychometrics,\n Nearest to three: sulla, monuc, conjuring, ontological, auf, grimoire, unpredictably, frenetic,\n\nNearest to world: turkle, spectroscopic, jules, servicio, sportswriter, kamikazes, act, earns,\nEpoch 1/10 Iteration: 1100 Avg. Training loss: 20.1983 0.3650 sec/batch\n Epoch 1/10 Iteration: 2000 Avg. Training loss: 19.1581 0.3767 sec/batch\n\nNearest to but: universities, hungry, edema, kyu, grandiose, stores, patty, psychometrics,\n Nearest to three: monuc, sulla, unpredictably, grimoire, hickey, ontological, conjuring, rays,\n Nearest to world: turkle, spectroscopic, jules, sportswriter, kamikazes, alfons, servicio, act,\n ...... \n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom string import punctuation\nfrom collections import Counter\n\n'''\nmovie review dataset for sentiment analysis\n'''\nwith open('data/reviews.txt', 'r') as f:\n    movieReviews = f.read()\nwith open('data/labels.txt', 'r') as f:\n    labels = f.read()\n\n'''\ndata cleansing - remove punctuations\n'''\ntext = ''.join([c for c in movieReviews if c not in punctuation])\nmovieReviews = text.split('\\n')\n\ntext = ' '.join(movieReviews)\nwords = text.split()\n\nprint(text[:500])\nprint(words[:100])\n\n'''\nbuild a dictionary that maps words to integers\n'''\ncounts = Counter(words)\nvocabulary = sorted(counts, key=counts.get, reverse=True)\nvocab2int = {word: i for i, word in enumerate(vocabulary, 1)}\n\nreviewsInts = []\nfor review in movieReviews:\n    reviewsInts.append([vocab2int[word] for word in review.split()])\n\n'''\nconvert labels from positive and negative to 1 and 0 respectively\n'''\nlabels = labels.split('\\n')\nlabels = np.array([1 if label == 'positive' else 0 for label in labels])\n\nreviewLengths = Counter([len(x) for x in reviewsInts])\nprint(\"Min review length are: {}\".format(reviewLengths[0]))\nprint(\"Maximum review length are: {}\".format(max(reviewLengths)))\n\n'''\nremove the review with zero length from the reviewsInts list\n'''\nnonZeroIndex = [i for i, review in enumerate(reviewsInts) if len(review) != 0]\nprint(len(nonZeroIndex))\n\n'''\nturns out its the final review that has zero length. But that might not always be the case, so let's make it more\ngeneral.\n'''\nreviewsInts = [reviewsInts[i] for i in nonZeroIndex]\nlabels = np.array([labels[i] for i in nonZeroIndex])\n\n'''\ncreate an array features that contains the data we'll pass to the network. The data should come from reviewInts, since\nwe want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, \nleft pad with 0s. That is, if the review is ['best', 'movie', 'renaira'], [100, 40, 20] as integers, the row will look \nlike [0, 0, 0, ..., 0, 100, 40, 20]. For reviews longer than 200, use on the first 200 words as the feature vector.\n'''\nseqLen = 200\nfeatures = np.zeros((len(reviewsInts), seqLen), dtype=int)\nfor i, row in enumerate(reviewsInts):\n    features[i, -len(row):] = np.array(row)[:seqLen]\n\nprint(features[:10,:100])\n\n'''\nlets create training, validation and test data sets. trainX and trainY for example. \nalso define a split percentage function 'splitPerc' as the percentage of data to keep in the training \nset. usually this is 0.8 or 0.9.\n'''\nsplitPrec = 0.8\nsplitIndex = int(len(features)*0.8)\ntrainX, valX = features[:splitIndex], features[splitIndex:]\ntrainY, valY = labels[:splitIndex], labels[splitIndex:]\n\ntestIndex = int(len(valX)*0.5)\nvalX, testX = valX[:testIndex], valX[testIndex:]\nvalY, testY = valY[:testIndex], valY[testIndex:]\n\nprint(\"Train set: {}\".format(trainX.shape), \"\\nValidation set: {}\".format(valX.shape), \"\\nTest set: {}\".format(testX.shape))\nprint(\"label set: {}\".format(trainY.shape), \"\\nValidation label set: {}\".format(valY.shape), \"\\nTest label set: {}\".format(testY.shape))\n\n'''\ntensor-flow computational graph\n'''\nlstmSize = 256\nlstmLayers = 1\nbatchSize = 500\nlearningRate = 0.001\n\nnWords = len(vocab2int) + 1\n\n# create graph object and add nodes to the graph\ngraph = tf.Graph()\n\nwith graph.as_default():\n    inputData = tf.placeholder(tf.int32, [None, None], name='inputData')\n    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n    keepProb = tf.placeholder(tf.float32, name='keepProb')\n\n'''\nlet us create the embedding layer (word2vec)\n'''\n# number of neurons in hidden or embedding layer\nembedSize = 300\n\nwith graph.as_default():\n    embedding = tf.Variable(tf.random_uniform((nWords, embedSize), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, inputData)\n\n'''\nlets use tf.contrib.rnn.BasicLSTMCell to create an LSTM cell, later add drop out to it with \ntf.contrib.rnn.DropoutWrapper. and finally create multiple LSTM layers with tf.contrib.rnn.MultiRNNCell.\n'''\nwith graph.as_default():\n    with tf.name_scope(\"RNNLayers\"):\n        def createLSTMCell():\n            lstm = tf.contrib.rnn.BasicLSTMCell(lstmSize, reuse=tf.get_variable_scope().reuse)\n            return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keepProb)\n\n        cell = tf.contrib.rnn.MultiRNNCell([createLSTMCell() for _ in range(lstmLayers)])\n\n        initialState = cell.zero_state(batchSize, tf.float32)\n\n'''\nset tf.nn.dynamic_rnn to add the forward pass through the RNN. here we're actually passing in vectors from the \nembedding layer 'embed'.\n'''\nwith graph.as_default():\n    outputs, finalState = tf.nn.dynamic_rnn(cell, embed, initial_state=initialState)\n\n'''\nfinal output will carry the sentiment prediction, therefore lets get the last output with outputs[:, -1], \nthe we calculate the cost from that and labels.\n'''\nwith graph.as_default():\n    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n    cost = tf.losses.mean_squared_error(labels, predictions)\n\n    optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost)\n\n'''\nnow we can add a few nodes to calculate the accuracy which we'll use in the validation pass.\n'''\nwith graph.as_default():\n    correctPred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n\n'''\nget batches\n'''\ndef getBatches(x, y, batchSize=100):\n    nBatches = len(x) // batchSize\n    x, y = x[:nBatches * batchSize], y[:nBatches * batchSize]\n    for i in range(0, len(x), batchSize):\n        yield x[i:i + batchSize], y[i:i + batchSize]\n\n'''\ntraining phase\n'''\nepochs = 1\n\nwith graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as sess:\n    writer = tf.summary.FileWriter(\"logs\", graph=tf.get_default_graph())\n    sess.run(tf.global_variables_initializer())\n    iteration = 1\n    for e in range(epochs):\n        state = sess.run(initialState)\n\n        for i, (x, y) in enumerate(getBatches(trainX, trainY, batchSize), 1):\n            feed = {inputData: x, labels: y[:, None], keepProb: 0.5, initialState: state}\n\n            loss, state, _ = sess.run([cost, finalState, optimizer], feed_dict=feed)\n\n            if iteration % 5 == 0:\n                print(\"Epoch are: {}/{}\".format(e, epochs), \"Iteration is: {}\".format(iteration), \"Train loss is: {:.3f}\".format(loss))\n\n            if iteration % 25 == 0:\n                valAcc = []\n                valState = sess.run(cell.zero_state(batchSize, tf.float32))\n                for x, y in getBatches(valX, valY, batchSize):\n                    feed = {inputData: x, labels: y[:, None], keepProb: 1, initialState: valState}\n                    batchAcc, valState = sess.run([accuracy, finalState], feed_dict=feed)\n                    valAcc.append(batchAcc)\n                print(\"Val acc: {:.3f}\".format(np.mean(valAcc)))\n            iteration += 1\n            saver.save(sess, \"checkpoints/sentimentanalysis.ckpt\")\n    saver.save(sess, \"checkpoints/sentimentanalysis.ckpt\")\n\n'''\ntesting phase\n'''\ntestAcc = []\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, \"checkpoints/sentiment.ckpt\")\n\n    testState = sess.run(cell.zero_state(batchSize, tf.float32))\n    for i, (x, y) in enumerate(getBatches(testY, testY, batchSize), 1):\n        feed = {inputData: x,\n                labels: y[:, None],\n                keepProb: 1,\n                initialState: testState}\n        batchAcc, testState = sess.run([accuracy, finalState], feed_dict=feed)\n        testAcc.append(batchAcc)\n    print(\"Test accuracy is: {:.3f}\".format(np.mean(testAcc)))\n```", "```py\nTrain set: (20000, 200)\n Validation set: (2500, 200)\n Test set: (2500, 200)\n label set: (20000,)\n Validation label set: (2500,)\n Test label set: (2500,)\nVal acc: 0.682\n Val acc: 0.692\n Val acc: 0.714\n Val acc: 0.808\n Val acc: 0.763\n Val acc: 0.826\n Val acc: 0.854\n Val acc: 0.872\n```"]