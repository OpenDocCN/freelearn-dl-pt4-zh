<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Parameter Inference Using the Bayesian Approach</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed inferring the parameters using the maximum-likelihood approach. In this chapter, we will explore the same issue through a Bayesian approach. The main topics are as follows:</p>
<ul>
<li>Introduction to Bayesian learning</li>
<li>Bayesian learning in HMMs</li>
<li>Approximate algorithms for estimating distributions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian learning</h1>
                </header>
            
            <article>
                
<p>In the maximum-likelihood approach to learning, we try to find the most optimal parameters for our model that maximizes our likelihood function. But data in real life is usually really noisy, and in most cases, it doesn't represent the true underlying distribution. In such cases, the maximum-likelihood approach fails. For example, consider tossing a fair coin a few times. It is possible that all of our tosses result in either heads or tails. If we use a maximum-likelihood approach on this data, it will assign a probability of 1 to either heads or tails, which would suggest that we would never get the other side of the coin. Or, let's take a less extreme case: let's say we toss a coin 10 times and get three heads and seven tails. In this case, a maximum-likelihood approach will assign a probability of 0.3 to heads and 0.7 to tails, which is not the true distribution of a fair coin. This problem is also commonly known as <strong>overfitting</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Bayesian learning takes a slightly different approach to learn these parameters. We start by assigning a prior distribution over the parameters of our model. The prior makes our assumptions about the model explicit. In the case of tossing the coin, we can start by using a prior that assigns equal probabilities to both heads and tails. Then we apply the Bayes theorem to compute the posterior distribution over our parameters based on the data. This allows us to shift our belief (prior) toward where the data points to, and this makes us do a less extreme estimate of the parameters. And in this way, Bayesian learning can solve one of the major drawbacks of maximum likelihood.</p>
<p>In more general terms, in the case of Bayesian learning, we try to learn a distribution over the parameters of our model instead of learning a single parameter that maximizes the likelihood. For learning this distribution over the parameters, we use the Bayes theorem, given by the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f629c8c-5429-46a3-a693-2f2d0b609354.png" style="width:11.83em;height:3.08em;"/></div>
<p>Here, <em>P(θ)</em> is our prior over the parameters of the model, <em>P(D|θ)</em> is the likelihood of the data given the parameters, and <em>P(D)</em> is the probability of the observed data. <em>P(D)</em> can also be written in terms of prior and likelihood as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dbdfc5df-65e2-41d4-a980-28e5f75354ec.png" style="width:20.00em;height:2.83em;"/></div>
<p>Now let's talk about each of these terms separately and see how can we compute them. The prior, <em>P(θ)</em>, is a probability distribution over the parameters representing our belief about the values of the parameters. For example, in the case of coin tossing, we can have our initial belief as <em>θ</em> is in between 0 and 1 and is uniformly distributed. The likelihood term, <em>P(D|θ)</em>, is the same term that we tried to maximize in <a href="8d06a68a-e427-4f7d-9472-9be25b5351c0.xhtml" target="_blank">Chapter 4</a>, <em>Parameter Inference using Maximum Likelihood</em>. It represents how likely our observed data is, given the parameters of the model. The next term, <em>P(D)</em>, is the probability of observing our data and it acts as the normalizing term. It is computationally difficult to compute because it requires us to sum over all the possible values of <em>θ</em> and, for any sufficiently large number of parameters, it quickly becomes intractable. In the next sections of this chapter, we will see the different algorithms that we can use to approximate these values. The term that we are trying to compute, <em>P(D|θ),</em> is known as the <strong>posterior</strong>. It represents our final probability distribution over the parameters of the model given our observed data. Basically, our prior is updated using the likelihood term to give the final distribution. </p>
<p>Another problem that Bayesian learning solves is the model selection. Since Bayesian learning gives a distribution over the different possible models rather than a single model, we have a couple of options of how we want to do predictions from these models. The first method is to just select a specific model that has the maximum probability, which is also commonly known as the <strong>Maximum Aposteriori</strong> (<strong>MAP</strong>) estimate. The other possible way is to compute the expectation of the prediction from all the models based on the posterior distribution. This allows us to regularize our predictions since we are computing expectation over all possible models. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting the priors</h1>
                </header>
            
            <article>
                
<p>A common question when doing Bayesian learning is how to select the appropriate prior. As David Mackay has said, <em>there is no inference without assumptions, we need to make a guess for the prior</em>. Our prior should be representative of what we think the most likely parameters are for our model. A huge benefit of using our own prior is that we make our assumption about the model explicit. Once we start applying Bayes, theorem using our prior and the observed data, our posterior would be a shift from our prior toward a distribution that represents our data better. </p>
<p>Theoretically, this sounds good as we can probably select very complex priors that capture our idea of the model, but for applying the Bayes theorem, we need to multiply our prior with the likelihood, and for complex distributions, it very quickly becomes computationally intractable. Therefore, in practice, we usually select a prior that is a conjugate distribution to our likelihood. A conjugate prior allows us to have a closed-form solution to the Bayes theorem. Because of this, Gaussian distributions are used for priors and likelihoods as multiplying a Gaussian distribution with another Gaussian distribution results in a Gaussian distribution. Also computations it's not expensive to compute the product of two Gaussians.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intractability</h1>
                </header>
            
            <article>
                
<p>Apart from selecting difficult priors, another source of intractability in Bayesian learning is the denominator term of Bayes' theorem:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/94eab2cf-86ad-4a4d-8333-2fe419d34fd4.png" style="width:19.42em;height:2.75em;"/></div>
<p>As we can see in the preceding equation for computing <em>P(D)</em>, we need to compute a summation over all the possible values of <em>θ</em>, which is the set of all the parameters of our model. If we have a lot of parameters in our model, it is computationally intractable to compute this term since the size of the term grows exponentially with the number of parameters. A lot of work has been done to approximate this value, as we will see in the next section of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian learning in HMM</h1>
                </header>
            
            <article>
                
<p>As we saw in the previous section, in the case of Bayesian learning we assume all the variables as a random variable, assign a prior to it, and then try to compute the posterior based on that. Therefore, in the case of HMM, we can assign a prior on our transition probabilities, emission probabilities, or the number of observation states. </p>
<p class="mce-root">Therefore, the first problem that we need to solve is to select the prior. Theoretically, a prior can be any distribution over the parameters of the model, but in practice, we usually try to use a conjugate prior to the likelihood, so that we have a closed-form solution to the equation. For example, in the case when the output of the HMM is discrete, a common choice of prior is the Dirichlet distribution. It is mainly for two reasons, the first of which is that the Dirichlet distribution is a conjugate distribution to multinomial distribution which allows us to multiply them easily. </p>
<div class="packt_infobox"><strong>Conjugate distribution</strong>: A family of priors is said to be conjugate to a family of likelihoods if the posterior obtained by multiplying the prior by the likelihood is in the same family of distribution as the prior distribution.</div>
<p>For example, since the likelihood of the initial state given the <em>π</em> parameter vector is multinomial:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d5960650-e1ce-46e9-8598-7c16419a6e9c.png" style="width:9.25em;height:3.67em;"/></div>
<p>And if the prior probability of<span> </span><em>π</em><span> </span>is Dirichlet:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1b0a0141-4f7d-4935-a5f1-661689aafd0f.png" style="width:11.75em;height:4.42em;"/></div>
<p>Where <em>u = [u<sub>1</sub>, u<sub>2</sub>, ..., u<sub>K</sub>]</em> is the hyperparameter vector and <em>Z</em> is the normalizing constant. We can now compute the posterior from the likelihood and the prior, which is given as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/df4cf2c1-e1bf-4876-91f9-e7338aa89ada.png" style="width:17.83em;height:4.83em;"/></div>
<p class="mce-root">And we can see that the posterior is also a Dirichlet distribution. Hence we can say that the Dirichlet distribution is a conjugate prior to the multinomial distribution. And in a similar way, we can set up Dirichlet priors for our transition matrix and emission matrix.</p>
<p>The second reason for choosing a Dirichlet prior is that it has the desirable property that its hyperparameters can be interpreted as a hypothetical count of observations. In the preceding example, if <em>u<sub>i</sub> = 2</em> and <em>u<sub>j</sub> = 1</em> for <em>j ≠ i</em>, the MAP estimate of <em>π</em> would be the same as a maximum-likelihood estimation with the assumption that the training data had an extra data point with the initial state being in state <em>i</em>. This conjugate property allows us to do MAP estimation in the case of Dirichlet priors by doing a minor variation in the Baum-Welch algorithm. It also gives theoretical justification for the seemingly ad hoc but very common regularization method for HMMs, which just adds a small positive number to all elements of the parameter vector.</p>
<p>In the last couple of paragraphs, we talked specifically about the case when the output is discrete. But the same concepts can be extended to the case of continuous output as well. Conjugate distributions exist in the case of continuous distributions as well. One of the most commonly used distributions is the Gaussian distribution as it stays in the same family after different operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approximating required integrals</h1>
                </header>
            
            <article>
                
<p>As we discussed before, the Bayesian approach treats all unknown quantities as random variables. We assign prior distributions to these variables and then estimate the posterior distribution over these after the data is observed. In the case of HMMs, the unknown quantities comprise the structure of the HMM, that is, the number of states, the parameters of the network, and the hidden states. Unlike maximum-likelihood or MAP estimations, in which we find point estimates for these parameters, we now have distributions over these parameters. This allows us to compare between model structures, but for doing that we need to integrate over both the parameters and the hidden states of the model. This is commonly known as Bayesian integration.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since these integrations are computationally intractable, we resort to approximate methods to compute these values. In the next few subsections, we will give an overview of some of these methods. A detailed analysis of these methods is outside the scope of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling methods</h1>
                </header>
            
            <article>
                
<p>Sampling methods are one of the most common ways to estimate intractable distributions. The general idea is to sample points from the distribution space in a way such that we get more samples from high-probability areas. And then based on these samples we estimate the distributions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Laplace approximations</h1>
                </header>
            
            <article>
                
<p>Laplace approximations use the central limit theorem, which from well-behaved priors and data asserts that the posterior parameter will converge in the limit of a large number of training samples to a Gaussian around the MAP estimate of the parameters. To estimate the evidence using the Laplace approximation, MAP parameters are found in the usual optimization routines and then the Hessian of the log-likelihood is computed at the MAP estimate. The evidence is approximated by evaluating the <em><span>P(</span>θ,D<span>)</span>/<span>P(</span>θ|D<span>)</span></em> ratio at the MAP estimate of <em>θ</em>, using the Gaussian approximation in the denominator. The Laplace approximation suffers from several disadvantages:</p>
<ul>
<li>Computing the Hessian matrix from the parameters is usually very costly</li>
<li>The Gaussian approximation is not very good for models with parameters that are positive and sum to 1, especially when there are many parameters relative to the size of the dataset</li>
</ul>
<p>For these reasons, the Laplace approximation is usually not used for HMMs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stolke and Omohundro's method</h1>
                </header>
            
            <article>
                
<p>In the famous paper <em>HMM induction by Bayesian model merging</em> Stolke and Omohundro present a new technique for approximating the Bayesian integrals of HMMs. Consider the case of having all the states of the HMM to be observed and the priors to be Dirichlet distributions. In this case, when learning the parameters using Bayesian learning, the posteriors are also going to be Dirichlet distributions, and then the evidence integral can be represented as a product of Dirichlet integrals, which can be easily computed. Therefore, in a sense, we can say that the reason for the intractability of evidence integrals is the fact that the states and parameters are hidden.</p>
<p class="mce-root"/>
<p>Stolke and Omohundro's method proposed to find the single most likely sequence of hidden states using a Viterbi-like algorithm and using this sequence as observed states. Using these observed values, we can easily do evidence integrals. The method proposes to iterate between these two steps, incrementally searching over model structures, merging or splitting states based on comparisons of this approximate evidence. In their paper, Stolke and Omohundro show that this method of trading off integration over hidden variables by integrating over parameters is able to get good results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variational methods</h1>
                </header>
            
            <article>
                
<p>Variational methods are another very common method used for approximating distributions. The general idea is to start by choosing a simpler family of distributions and then try to find the hyperparameters of this distribution such that the distribution is as close as possible to our original distribution. There are different metrics that are used to determine the closeness of two distributions; the most commonly used metric is Kullback-Leiber divergence. This method basically converts an inference problem into an optimization problem where we try to minimize our divergence metric. </p>
<p>In the case of HMMs, we usually make an assumption that the hidden states are independent of the parameters of the model. This allows us to approximate distributions over both the hidden states and parameters simultaneously. More specifically, the evidence can be lower bounded by applying Jenson's inequality twice:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f6441a3c-4b9b-4f5f-b46e-47a3ede9e751.png" style="width:33.17em;height:13.67em;"/></div>
<p>The variational Bayesian approach iteratively maximizes <img class="fm-editor-equation" src="assets/a53434d0-4c21-4db9-9f3f-611212895d68.png" style="width:0.92em;height:1.00em;"/> as a functional of the two free distributions, <em>Q(S) </em>and <em>Q(θ)</em>. In the preceding equations, we can see that this maximization is equivalent to minimizing the KL divergence between <em>Q(S)<span>Q(θ)</span></em> and the joint posterior over hidden states and the <em><span>P(S,θ|D,M)</span></em> parameters. David MacKay first presented a variational Bayesian approach to learning in HMMs. He assumed the prior to be a Dirichlet distribution, making the assumption that the parameters are independent of the hidden states, he showed that the optimal <em><span>Q(θ)</span></em><span> </span>is a Dirichlet distribution. Furthermore, he showed that the optimal <em>Q(S)</em> could be obtained by applying the forward-backward algorithm to an HMM with pseudo-parameters given by <img class="fm-editor-equation" src="assets/881223eb-6a9e-4209-a083-d9fa362eac42.png" style="width:9.33em;height:2.00em;"/>, which can be evaluated for Dirichlet distributions. Thus the whole variational Bayesian method can be implemented as a simple modification of the Baum-Welch algorithm. Essentially we can state that the variational Bayesian method is a combination of special cases of both the MAP approach and Stolke and Omohundro's approach. This is very promising, especially given that it has been used successfully for non-trivial model-structure learning in other models; its potential has not been fully explored for HMMs and their extensions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code</h1>
                </header>
            
            <article>
                
<p>Currently, there are no packages in Python that support learning using Bayesian learning and it would be really difficult to write the complete code to fit in this book. And even though there are a lot of advantages to using Bayesian learning, it is usually computationally infeasible in a lot of cases. For these reasons, we are skipping the code for Bayesian learning in HMMs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we talked about applying Bayesian learning in the case of learning parameters in HMMs. Bayesian learning has a few benefits over the maximum-likelihood estimator, but it turns out to be computationally quite expensive except when we have closed-form solutions. Closed-form solutions are only possible when we use conjugate priors. In the following chapters, we will discuss detailed applications of HMMs for a wide variety of problems.</p>


            </article>

            
        </section>
    </body></html>