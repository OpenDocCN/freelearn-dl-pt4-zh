- en: Deep Learning Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we created some machine learning models using neural
    network packages in R. This chapter will look at some of the fundamentals of neural
    networks and deep learning by creating a neural network using basic mathematical
    and matrix operations. This application sample will be useful for explaining some
    key parameters in deep learning algorithms and some of the optimizations that
    allow them to train on large datasets. We will also demonstrate how to evaluate
    different hyper-parameters for models to find the best set. In the previous chapter,
    we briefly looked at the problem of overfitting; this chapter goes into that topic
    in more depth and looks at how you can overcome this problem. It includes an example
    use case using dropout, the most common regularization technique in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building neural networks from scratch in R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common parameters in deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some key components in deep learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using regularization to overcome overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case—improving out-of-sample model performance using dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building neural networks from scratch in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have already used some neural network algorithms, it's time to dig
    a bit deeper into how they work. This section demonstrates how to code a neural
    network from scratch. It might surprise you to see that the core code for a neural
    network can be written in fewer than 80 lines! The code for this chapter does
    just that using an interactive web application written in R. It should give you
    more of an intuitive understanding of neural networks. First we will look at the
    web application, then we will delve more deeply into the code for the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network web application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will look at an R Shiny web application. I encourage you to run the
    application and follow the examples as it will really help you to get a better
    understanding of how neural networks work. In order to run it, you will have to
    open the `Chapter3` project in RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is R Shiny**?'
  prefs: []
  type: TYPE_NORMAL
- en: R Shiny is an R package from the RStudio company that allows you to create interactive
    web apps using only R code. You can build dashboards and visualizations, and use
    the full functionality of R. You can extend R Shiny apps with CSS, widgets, and
    JavaScript. It is also possible to host your applications online. It is a great
    tool with which to showcase data science applications and I encourage you to look
    into it if you are not already familiar with it. For more information, see [https://shiny.rstudio.com/](https://shiny.rstudio.com/),
    and, for examples of what is possible with R Shiny, see [https://shiny.rstudio.com/gallery/](https://shiny.rstudio.com/gallery/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `server.R` file in RStudio and click on the Run App button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fb1268b3-83d8-4a8a-816f-88ead0e115a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: How to run an R Shiny application'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you click on the Run App button, you should get a pop-up screen for your
    web application. The following is a screenshot of the web application after it
    starts up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bd66fb12-3ed7-40ef-8407-b5b2a8d444d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2: R Shiny application on startup
  prefs: []
  type: TYPE_NORMAL
- en: 'This web application can be used in the pop-up window or opened in a browser.
    On the left, there is a set of input choices; these are parameters for the neural
    network. These are known as hyper-parameters, in order to distinguish between
    the *parameters* that the model is trying to optimize. From top to bottom, these hyper-parameters
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select data**: There are four different datasets that you can use as training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes in hidden layer**: The number of nodes in the hidden layer. The neural
    network has only one hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**# Epochs**: The number of times that the algorithm iterates over the data
    during model-building.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: The learning rate applied during backpropagation. The learning
    rate affects how much the algorithm changes the weights during every epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: The activation function applied to the output of each
    node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Run NN Model button trains a model with the selection of input. The Reset
    button restores input choices to the default values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are four different datasets to choose from, each with a different data
    distribution; you can select them from the drop-down box. They have descriptive
    names; for example, the data that is plotted in *Figure 3.2* is called `bulls_eye`.
    These datasets are from another R package that is used to test clustering algorithms.
    The data has two classes of equal size and is composed of various geometric shapes.
    You can explore these datasets using the web application. The only change we make
    to the data is to randomly switches labels for 5% of the data. When you run the
    application, you will notice that there are some red points in the inner circle
    and some blue points in the outer circle. This is done so that our models should
    only achieve a maximum accuracy of 0.95 (95%). This gives us confidence that the
    model is working correctly. If the accuracy is higher than this, the model could
    be overfitting because the function it has learned is too complex. We will discuss
    overfitting again in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first steps in machine learning should be to establish a benchmark
    score, this is useful for gauging your progress. A benchmark score could be a
    rule of thumb, or a simple machine learning algorithm; it should not be something
    that you spend a lot of time working on. In this application, we use a basic logistic
    regression model as a benchmark. We can see that in the previous screenshot, the
    accuracy for the logistic regression model is only 0.6075, or 60.75% accuracy.
    This is not much over 50%, but recall that logistic regression can only fit a
    straight line and this data cannot be separated using a straight line. A neural
    network should improve on the logistic regression benchmark, so if we get an accuracy
    of less than 0.6075 on this dataset, something is wrong with our model and we
    should review it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s begin! Click on the Run NN Model button, which runs a neural network
    model on the data using the input choices. After a few seconds, the application
    should change to resemble the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a81d0b71-cb02-4770-8b2c-f98b0a58ef3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Neural network model execution with default settings'
  prefs: []
  type: TYPE_NORMAL
- en: The application takes a few seconds and then it creates a graph of the cost
    function over the **# Epochs** and outputs cost function values as the algorithm
    iterates over the data. The text output also includes the final accuracy for the
    model in the text at the bottom right of the screen. In the diagnostic messages
    in the bottom right, we can see that the cost decreases during training and we
    achieved a final accuracy rate of 0.825\. The cost is what the model is trying
    to minimize – a lower cost means better accuracy. It took some time for the cost
    to start decreasing as the model struggled initially to get the right weights.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning models, weights and biases should be not initialized with random
    values. If random values are used, this can lead to problems with training, such
    as vanishing or exploding gradients. This is where the weights get too small or
    too large and the model fails to train successfully. Also, if the weights are
    not correctly initialized, the model will take longer to train, as we saw earlier.
    Two of the most popular techniques to initialize weights to avoid these problems
    are the Xavier initialization and the He initialization (named after their inventors).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see in *Figure 3.3* that the cost has not plateaued, the last few values
    show it is still decreasing. This indicates that the model can be improved if
    we train it for longer. Change **# Epochs** to **7000** and click the **Run NN
    Model** button again; the screen will change to resemble the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d8c44aa-f4e8-4abc-b034-ad19f339540c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4: Neural network model execution with more epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we get an accuracy of 0.95, which is the maximum possible accuracy rate.
    We notice that the cost values have plateaued (that is, are not decreasing further)
    to around 0.21\. This indicates that training the model for longer (that is, more
    epochs) will probably not improve the results, regardless of the current accuracy
    number. If the model is under training and the cost values have plateaued, we
    would need to consider changing the architecture of the model or getting more
    data to improve our accuracy. Let''s look at changing the number of nodes in our
    model. Click the Reset button to change the input values to their defaults, then
    change the number of nodes to 7, and click the **Run NN Model** button. Now the
    screen will change to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8b24308-5c5f-4579-84a9-d4464c3bfaf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5: Neural network model execution with more nodes
  prefs: []
  type: TYPE_NORMAL
- en: Our accuracy here is 0.68, but compare this to the earlier examples, when we
    used the same input and only three nodes. We actually get worse performance with
    more nodes! This is because our data has a relatively simple pattern, and a model
    with seven nodes might be too complex and will take longer to train. Adding more
    nodes to a layer will increase training time but does not always improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the **Learning rate**. Click the **Reset** button to change
    the input values to their defaults, then change the **Learning rate** to around
    **5**, and click the **Run NN Model** button again to replicate the following
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/763dd827-fdfa-48c6-9dcc-42e3465a985a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6: Neural network model execution with larger learning rate
  prefs: []
  type: TYPE_NORMAL
- en: We get 0.95 accuracy again, which is the best possible accuracy. If we compare
    it to the previous examples, we can see that the model *converged* (that is, the
    length of time it took for the cost function to plateau) much quicker, after just
    **500** epochs. We needed fewer epochs, so we can see an inverse relationship
    between learning rates and training epochs. A higher learning rate may mean you
    need fewer epochs. But are bigger learning rates always better? Well, no.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Reset** button to change the input values to their defaults, then
    change the **Learning rate** to the maximum value (**20**), and click the **Run
    NN Model** button again. When you do, you will get similar output to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68feee2c-d3dc-44ca-9f99-7cf61160a1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7: Neural network model execution with too great a learning rate
  prefs: []
  type: TYPE_NORMAL
- en: We get an accuracy rate of 0.83\. What just happened? By selecting a huge learning
    rate, our model failed to converge at all. We can see that the cost function actually
    increases at the start of training, which indicates that the Learning rate is
    too high. Our cost function graph seems to have repeating values, which indicates
    that the gradient-descent algorithm is overshooting the minima at times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can look at how the choice of activation function affects model
    training. By changing the activation function, you may also need to change the
    **Learning rate**. Click the **Reset** button to change the input values to their
    defaults and select `tanh` for the activation function. When we select `tanh`
    as the activation function and 1.5 as the **Learning rate**, the cost gets `stuck` at
    0.4 from epochs 500-3,500 before suddenly decreasing to 0.2\. This can occur in
    neural networks when they get stuck in local optima. This phenomena can be seen
    in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/205bbcd5-1d22-4e25-a98b-39cfc4ef1baa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8: Neural network model execution with the tanh activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, using relu activation results in the model training faster. The
    following is an example where we only run 1,500 epochs with the relu activation
    to get the maximum possible accuracy of 0.95:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06a0a37f-e52a-4037-a549-b455d75dd459.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9: Neural network model execution with the relu activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'I encourage you to experiment with the other datasets. For reference purposes,
    here is the max accuracy I got for each of those datasets. An interesting experiment
    is to see how different activation functions and learning rates work with these
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**worms (accuracy=0.95)**: 3 nodes, 3,000 epochs, Learning rate = 0.5, activation
    = tanh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**moon (accuracy=0.95)**: 5 nodes, 5,000 epochs, Learning rate = 5, activation
    = sigmoid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**blocks (accuracy=0.9025)**: 5 nodes, 5,000 epochs, Learning rate = 10, activation
    = sigmoid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using more epochs means a longer training time, which may not always be needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model has not achieved the best accuracy and the cost function has plateaued
    (that is, it is not decreasing by much) toward the end of the training, then running
    it longer (that is, more epochs) or increasing the learning rate is unlikely to
    improve performance. Instead, look at changing the model''s architecture, such
    as by changing the # layers (not an option in this demo), adding more nodes, or
    changing the activation functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate must be selected carefully. If the value selected is too low,
    it will take a long time for the model to train. If the value selected is too
    high, the model will fail to train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the web application is useful to see the output of the neural network,
    we can also run the code for the neural network to really see how it works. The
    code in `Chapter3/nnet.R` allows us to do just that. This code has the same hyper-parameters
    as in the web application; this file allows you to run the neural network from
    the RStudio IDE. The following is the code that loads the data and sets the initial
    hyper-parameters for the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should not be too difficult to understand, it loads a dataset and
    sets some variables. The data is created in the `getData` function from the `Chapter3/nnet_functions.R`
    file. The data is created from functions in the `clustersim` package. The `Chapter3/nnet_functions.R`
    file contains the core functionality of our neural network that we will look at
    here. Once we load our data, the next step is to initialize our weights and biases.
    The `hidden` variable controls the number of nodes in the hidden layer; we set
    it to 3\. We need two sets of weights and biases, one for the hidden layer and
    one for the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates matrices for the `(weights1, bias1)` hidden layer and the `(weights2,
    bias2)` output layer. We need to ensure our matrices have the correct dimensions.
    For example, the `weights1` matrix should have the same number of columns as the
    input layer and the same number of rows as the hidden layer. Now we move on to
    the actual processing loop of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We first run the forward-propagation function, then calculate a cost. We then
    call a backward-propagation step that calculates our derivatives, `(dweights1,
    dbias1, dweights2, dbias2)`. Then we update the weights and biases, `(weights1, bias1, weights2, bias2)`,
    using our Learning rate, `(lr)`. We run this loop for the number of `epochs (3000)`
    and print out a diagnostic message every 500 `epochs`. This describes how every
    neural network and deep learning model works: first call forward-propagation,
    then calculate costs and derivative values, use those to update the weights through
    back-propagation and repeat.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at some of the functions in the `nnet_functions.R` file. The
    following is the `forward` propagation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you looked at the code carefully, you may have noticed that the assignment
    to the `activation1`, `activation2`, `Z1`, and `Z2` variables uses `<<-` rather
    than `<-`. This makes those variables global in scope; we also want to use these
    values during back propagation. Using global variables is generally frowned upon
    and I could have returned a list, but it is acceptable here to use them because
    this application is for learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The two for loops expand the bias vectors into matrices, then repeat the vector
    n times. The interesting code starts with the `Z1` assignment. `Z1` is a matrix
    multiplication, followed by an addition. We call the `activation_function` function
    on that value. We then use that output value and perform a similar operation for
    `Z2`. Finally, we apply a sigmoid activation to our output layer because our problem
    is binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for the activation function; the first parameter
    decides which function to use (`sigmoid`, `tanh`, or `relu`). The second parameter
    is the value to be used as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the `cost` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As a reminder, the output of the `cost` function is what we are trying to minimize.
    There are many types of `cost` functions; in this application we are using binary
    cross-entropy. The formula for binary cross-entropy is *-1/m ∑ log(ȳ[i]) * y[i] +
    (log(1 -ȳ[i]) * (1-y[i])*. Our target values (*y[i]*) are always either *1* or
    *0*, so for instances where *y[i] = 1*, this reduces to *∑**log(ȳ[i]**)*. If we
    have two rows where *y[i] = 1* and suppose that our model predicts *1.0* for the
    first row and the *0.0001* for the second row, then the costs for the rows are *log(1)=0*
    and *log(0.0001)=-9.1*, respectively. We can see that the closer to *1* the prediction
    is for these rows, the lower the `cost` value. Similarly, for rows where y[i] =
    0, this reduces to log(1-ȳ[i]), so the closer to 0 the prediction is for these
    rows, the lower the `cost` value.
  prefs: []
  type: TYPE_NORMAL
- en: If we are trying to maximize accuracy, why don't we just use what during model
    training? Binary cross-entropy is a better `cost` function because our model does
    not just output 0 or 1, but instead outputs continuous values from 0.0 to 1.0\.
    For example, if two input rows had a target value=1 (that is, y=1), and our model
    gave probabilities of 0.51 and 0.99, then binary cross-entropy would give them
    a cost of 0.67 and 0.01, respectively. It assigns a higher cost to the first row
    because the model is unsure about it (the probability is close to 0.5). If instead
    we just looked at accuracy, we might decide that both rows have the same cost
    value because they are classified correctly (assuming we assign class=0 where
    predicted values < 0.5, and class=1 where predicted values >= 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for the backward-propagation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Backward propagation processes the network in reverse, starting at the last
    hidden layer and finishing at the first hidden layer, that is, in the direction
    of the output layer to the input layer. In our case, we only have one hidden layer,
    so it first calculates the loss from the output layer and calculates `dweight2`
    and `dbias2`. It then calculates the `derivative` of the `activation1` value,
    which was calculated during the forward-propagation step. The `derivative` function
    is similar to the activation function, but instead of calling an activation function,
    it calculates the `derivative` of that function. For example, the `derivative`
    of `sigmoid(x)` is *sigmoid(x) * (1 - sigmoid(x))*. The `derivative` values of
    simple functions can be found in any calculus reference or online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! A working neural network using basic R code. It can fit complex
    functions and performs better than logistic regression. You might not get all
    the parts at once, that''s OK. The following is a quick recap of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run a forward-propagation step, which involves multiplying the weights by the
    input for each layer and passing the output to the next layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the output from the final layer using the `cost` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the error rate, use backpropagation to make small adjustments to the
    weights in the nodes in each layer. The learning rate controls how much of an
    adjustment we make each time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1-3, maybe thousands of times, until the `cost` function begins
    to plateau, which indicates our model is trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many of the concepts in the previous section apply to deep learning because
    deep learning is simply neural networks with two or more hidden layers. To demonstrate
    this, let''s look at the following code in R that loads the `mxnet` deep learning
    library and calls the help command on the function in that library that trains
    a deep learning model. Even though we have not trained any models using this library
    yet, we have already seen many of the parameters in this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you get errors saying the `mxnet` package is unavailable, see [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting
    Started with Deep Learning*, for installation instructions. However, we are not
    running any `mxnet` code in this chapter, we only want to display the help page
    for a function. So feel free to just continue reading and you can install the
    package later when we use it in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up the help page for the `FeedForward` function in the `mxnet`
    library, which is the forward-propagation/model train function. `mxnet` and most
    deep learning libraries do not have a specific *backward-*propagation function,
    they handle this implicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We will see more of this function in subsequent chapters; for now we will just
    look at the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The symbol, X, y, and ctx parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The symbol parameter defines the deep learning architecture; X and y are the
    input and output data structures. The ctx parameter controls which device (for
    example, CPU/GPU) the model is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: The num.round and begin.round parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`num.round` is equivalent to epochs in our code; that is, however many times
    we iterate over the data. `begin.round` is where we resume training the model
    if we paused training previously. If we pause training, we can save the partially-trained
    model, reload it later, and resume training.'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our implementation of neural networks used gradient descent. When researchers
    started creating more complicated multilayer neural network models, they found
    that they took an extraordinarily long time to train. This is because the basic
    gradient-descent algorithm with no optimization is not very efficient; it makes
    small steps towards its goal in each epoch regardless of what occurred in previous
    epochs. We can compare it with a guessing game: one person has to guess a number
    in a range and for each guess, they are told to go higher or lower (assuming they
    do not guess the correct number!). The higher/lower instruction is similar to
    the derivative value, it indicates the direction we must travel. Now let''s say
    that the range of possible numbers is 1 to 1,000,000 and the first guess is 1,000\.
    The person is told to go higher, which should they do:'
  prefs: []
  type: TYPE_NORMAL
- en: Try 1001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the difference between the guess and the max value and divide by 2\. Add
    this value to the previous guess.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option is much better and should mean the person gets to the right
    answer in 20 guesses or fewer. If you have a background in computer science, you
    may recognize this as the binary-search algorithm. The first option, guessing
    1,001, 1,002, ...., 1,000,000, is a terrible choice and will probably fail as
    one party will give up! But this is similar to how gradient descent works. It
    moves incrementally towards the target. If you try increasing the learning rate
    to overcome this problem, you can overshoot the target and the model fails to
    converge.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers came up with some clever optimizations to speed up training. One
    of the first optimizers was called momentum, and it does exactly what its name
    states. It looks at the extent of the derivative and takes bigger *steps* for
    each epoch if the previous steps were all in the same direction. It should mean
    that the model trains much quicker. There are other algorithms that are enhancements
    of these, such as RMS-Prop and Adam. You don't usually need to know how they work,
    just that, when you change the optimizer, you may also have to adjust other hyper-parameters,
    such as the learning rate. In general, look for previous examples done by others
    and copy those hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We actually used one of these optimizers in an example in the previous chapter.
    In that chapter, we had 2 models with a similar architecture (40 hidden nodes).
    The first model (`digits.m3`) used the `nnet` library and took 40 minutes to train.
    The second model (`digits.m3`) used resilient backpropagation and took 3 minutes
    to train. This shows the benefit of using an optimizer in neural networks and
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The initializer parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we created the initial values for our weights and biases (that is, model
    parameters), we used random numbers, but limited them to the values of -0.005
    to +0.005\. If you go back and review some of the graphs of the `cost` functions,
    you see that it took 2,000 epochs before the `cost` function began to decline.
    This is because the initial values were not in the right range and it took 2,000
    epochs to get to the correct magnitude. Fortunately, we do not have to worry about
    how to set these parameters in the `mxnet` library because this parameter controls
    how the weights and biases are initialized before training.
  prefs: []
  type: TYPE_NORMAL
- en: The eval.metric and eval.data parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These two parameters control what data and which metric are used to evaluate the
    model. `eval.metric` is equivalent to the `cost` function we used in our code. `eval.data`
    is used if you want to evaluate the model on a holdout dataset that is not used
    in training.
  prefs: []
  type: TYPE_NORMAL
- en: The epoch.end.callback parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a `callback` function that allows you to register another function that
    is called after *n* epochs. Deep learning models take a long time to train, so
    you need some feedback to know they are working correctly! You can write a custom
    `callback` function to do whatever you need, but usually it outputs to the screen
    or log after *n* epochs. This is equivalent to the code in our neural network
    that printed a diagnostic message every *500* epochs. The `callback` function
    can also be used to save the model to disk, for example, if you wanted to save
    the model before it begins to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: The array.batch.size parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We only had 400 instances (rows) in our data, which can easily fit into memory.
    However, if your input data has millions of instances, the data needs to be split
    into batches during training in order to fit in the memory of the CPU/GPU. The
    number of instances you train at a time is the batch size. Note, you still iterate
    over all the data for the number of epochs, you just split the data into batches
    during each iteration and run the forward-propagation, backpropagation step over
    each batch for each epoch. For example, if you had 100 instances and selected
    a batch size of *32* with *6* epochs, you would need *4* batches for each epoch
    (*100/32 = 3.125*, so we need *4* batches to process all the data), for a total
    of *24* loops.
  prefs: []
  type: TYPE_NORMAL
- en: There is a trade-off in choosing the batch size. If you choose too low a value,
    the model will take a longer time to train because it's running more operations
    and batches will have more variability because of the small size. You cannot choose
    an enormous batch size either, this might cause your model to crash because it
    loads too much data into either the CPU or GPU. In most cases, you either take
    a sensible default that works from another deep learning model, or you set it
    at some value (for example, 1,024) and if your model crashes, then try again with
    a value of half the previous value (512).
  prefs: []
  type: TYPE_NORMAL
- en: There is a relationship between **Batch size**, **Learning rates**, and **#
    Epochs** for training. But there are no hard and fast rules in selecting values.
    However, in general, consider changing these values together and do not use an
    extreme value for one of these hyper-parameters. For example, picking a large
    Learning rate should mean fewer epochs, but if your batch size is too small, the
    model may fail to train. The best advice is to look at similar architectures and
    pick a similar set and range of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we can see that deep learning still uses many of the concepts from
    neural networks, we will move on to talk about an important issue that you will
    probably encounter with every deep learning model: overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Using regularization to overcome overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw the diminishing returns from further training
    iterations on neural networks in terms of their predictive ability on holdout
    or test data (that is, data not used to train the model). This is because complex
    models may memorize some of the noise in the data rather than learning the general
    patterns. These models then perform much worse when predicting new data. There
    are some methods we can apply to make our model generalize, that is, fit the overall
    patterns. These are called **regularization** and aim to reduce testing errors
    so that the model performs well on new data.
  prefs: []
  type: TYPE_NORMAL
- en: The most common regularization technique used in deep learning is dropout. However,
    we will also discuss two other regularization techniques that have a basis in
    regression and deep learning. These two regularization techniques are **L1 penalty**,
    which is also known as **Lasso**, and **L2 penalty**, which is also known as **Ridge**.
  prefs: []
  type: TYPE_NORMAL
- en: L1 penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic concept of the **L1 penalty**, also known as the **least-absolute
    shrinkage and selection operator** (**Lasso**–Hastie, T., Tibshirani, R., and
    Friedman, J. (2009)), is that a penalty is used to shrink weights toward zero.
    The penalty term uses the sum of the absolute weights, so some weights may get
    shrunken to zero. This means that Lasso can also be used as a type of variable
    selection. The strength of the penalty is controlled by a hyper-parameter, alpha
    (λ), which multiplies the sum of the absolute weights, and it can be a fixed value
    or, as with other hyper-parameters, optimized using cross-validation or some similar
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to describe Lasso if we use an **ordinary least squares** (**OLS**)
    regression model. In regression, a set of coefficients or model weights is estimated
    using the least-squared error criterion, where the weight/coefficient vector,
    *Θ*, is estimated such that it minimizes *∑(y[i] - ȳ[i])* where *ȳ[i]=b+Θx, y[i]*
    is the target value we want to predict and *ȳ[i]* is the predicted value. Lasso
    regression adds an additional penalty term that now tries to minimize ∑(y[i] -
    ȳ[i]) + λ⌊Θ⌋, where ⌊Θ⌋ is the absolute value of *Θ*. Typically, the intercept
    or offset term is excluded from this constraint.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of practical implications for Lasso regression. First, the
    effect of the penalty depends on the size of the weights, and the size of the
    weights depends on the scale of the data. Therefore, data is typically standardized
    to have unit variance first (or at least to make the variance of each variable
    equal). The L1 penalty has a tendency to shrink small weights to zero (for explanations
    as to why this happens, see Hastie, T., Tibshirani, R., and Friedman, J. (2009)).
    If you only consider variables for which the L1 penalty leaves non-zero weights,
    it can essentially function as feature-selection. The tendency for the L1 penalty
    to shrink small coefficients to zero can also be convenient for simplifying the
    interpretation of the model results.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the L1 penalty to neural networks works exactly the same for neural
    networks as it does for regression. If *X* represents the input, *Y* is the outcome
    or dependent variable, *B* the parameters, and *F* the objective function that
    will be optimized to obtain *B*, that is, we want to minimize *F(B; X, Y)*. The
    L1 penalty modifies the objective function to be F(B; X, Y) + λ⌊Θ⌋, where *Θ*
    represents the weights (typically offsets are ignored). The L1 penalty tends to
    result in a sparse solution (that is, more zero weights) as small and larger weights
    result in equal penalties, so that at each update of the gradient, the weights
    are moved toward zero.
  prefs: []
  type: TYPE_NORMAL
- en: We have only considered the case where *λ* is a constant, controlling the degree
    of penalty or regularization. However, it is possible to set different values
    with deep neural networks, where varying degrees of regularization can be applied
    to different layers. One reason for considering such differential regularization
    is that it is sometimes desirable to allow a greater number of parameters (say
    by including more neurons in a particular layer) but then counteract this somewhat
    through stronger regularization. However, this approach can be quite computationally
    demanding if we are allowing the L1 penalty to vary for every layer of a deep
    neural network and using cross-validation to optimize all possible combinations
    of the L1 penalty. Therefore, usually a single value is used across the entire
    model.
  prefs: []
  type: TYPE_NORMAL
- en: L1 penalty in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To see how the L1 penalty works, we can use a simulated linear regression problem.
    The code for the rest of this chapter is in `Chapter3/overfitting.R`. We simulate
    the data, using a correlated set of predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can fit an OLS regression model to the first 100 cases, and then use `lasso`.
    To use `lasso`, we use the `glmnet()` function from the `glmnet` package. This
    function can actually fit the L1 or the L2 (discussed in the next section) penalties,
    and which one occurs is determined by the argument, alpha. When `alpha = 1`, it
    is the L1 penalty (that is, `lasso`), and when `alpha = 0`, it is the L2 penalty
    (that is, ridge regression). Further, because we don''t know which value of `lambda`
    we should pick, we can evaluate a range of options and tune this hyper-parameter
    automatically using cross-validation, which is the `cv.glmnet()` function. We
    can then plot the `lasso` object to see the mean squared error for a variety of
    `lambda` values to allow us to select the correct level of regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d0b8aa13-f250-4111-9d27-c7b13b088466.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Lasso regularization'
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that we can see from the graph is that, when the penalty gets too
    high, the cross-validated model increases. Indeed, `lasso` seems to do well with
    very low lambda values, perhaps indicating `lasso` does not help improve out-of-sample
    performance/generalizability much for this dataset. For the sake of this example,
    we will continue but in actual use, this might give us pause to consider whether `lasso`
    was really helping. Finally, we can compare the coefficients with those from `lasso`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the OLS coefficients are noisier and also that, in `lasso`, predictor
    5 is penalized to 0\. Recall from the simulated data that the true coefficients
    are 3, 1, 1, 1, 1, and 0\. The OLS estimates have much too low a value for the
    first predictor and much too high a value for the second, whereas `lasso` has
    more accurate values for each. This demonstrates that `lasso` regression generalizes
    better than OLS regression for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: L2 penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **L2 penalty**, also known as **ridge regression**, is similar in many ways
    to the L1 penalty, but instead of adding a penalty based on the sum of the absolute
    weights, the penalty is based on the squared weights. This means that larger absolute
    weights are penalized more. In the context of neural networks, this is sometimes
    referred to as weight decay. If you examine the gradient of the regularized objective
    function, there is a penalty such that, at every update, there is a multiplicative
    penalty to the weights. As for the L1 penalty, although they could be included,
    biases or offsets are usually excluded from this.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of a linear regression problem, the L2 penalty is a modification
    to the objective function minimized, from *∑(y[i] - ȳ[i]) *to* ∑(y[i] - ȳ[i]) + λΘ²*.
  prefs: []
  type: TYPE_NORMAL
- en: L2 penalty in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To see how the L2 penalty works, we can use the same simulated linear regression
    problem we used for the Ll penalty. To fit a ridge regression model, we use the
    `glmnet()` function from the `glmnet` package. As mentioned previously, this function
    can actually fit the L1 or the L2 penalties, and which one occurs is determined
    by the argument, alpha. When `alpha = 1`, it fits `lasso`, and when `alpha = 0`,
    it fits ridge regression. This time, we choose `alpha = 0`. Again, we evaluate
    a range of lambda options and tune this hyper-parameter automatically using cross-validation.
    This is accomplished by using the `cv.glmnet()` function. We plot the ridge regression
    object to see the error for a variety of lambda values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e89c9b9a-ea90-4458-9471-b6a6ea584165.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Ridge regularization'
  prefs: []
  type: TYPE_NORMAL
- en: Although the shape is different from `lasso` in that the error appears to asymptote
    for higher lambda values, it is still clear that, when the penalty gets too high,
    the cross-validated model error increases. As with `lasso`, the ridge regression
    model seems to do well with very low lambda values, perhaps indicating the L2
    penalty does not improve out-of-sample performance/generalizability by much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can compare the OLS coefficients with those from `lasso` and the
    ridge regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Although ridge regression does not shrink the coefficient for the fifth predictor
    to exactly 0, it is smaller than in the OLS, and the remaining parameters are
    all slightly shrunken, but quite close to their true values of 3, 1, 1, 1, 1,
    and 0.
  prefs: []
  type: TYPE_NORMAL
- en: Weight decay (L2 penalty in neural networks)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already unknowingly used regularization in the previous chapter. The
    neural network we trained using the `caret` and `nnet` package used a weight decay
    of `0.10`. We can investigate the use of weight decay by varying it, and tuning
    it using cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the data as before. Then we create a local cluster to run the cross-validation
    in parallel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a neural network on the digit classification, and vary the weight-decay
    penalty at `0` (no penalty) and `0.10`. We also loop through two sets of the number
    of iterations allowed: `100` or `150`. Note that this code is computationally
    intensive and takes some time to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining the results, we see that, when we limit to only `100` iterations,
    both the non-­regularized model and regularized model have the same accuracy at
    `0.56`, based on cross-validated results, which is not very good on this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Examine the model with `150` iterations to see whether the regularized or non-regularized
    model performs better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Overall, the model with more iterations outperforms the model with fewer iterations,
    regardless of the regularization. However, comparing both models with 150 iterations,
    the regularized model is superior (`accuracy= 0.66`) to the non-regularized model
    (`accuracy= 0.65`), although here the difference is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: These results highlight that regularization is often most useful for more complex
    models that have greater flexibility to fit (and overfit) the data. In models
    that are appropriate or overly simplistic for the data, regularization will probably
    decrease performance. When developing a new model architecture, you should avoid
    adding regularization until the model is performing well on the training data. If
    you add regularization beforehand and the model performs poorly on the training
    data, you will not know whether the problem is with the model's architecture or
    because of the regularization. In the next section, we'll discuss ensemble and
    model averaging techniques, the last forms of regularization that are highlighted
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles and model-averaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to regularization involves creating multiple models (ensembles)
    and combining them, such as by model-averaging or some other algorithm for combining
    individual model results. There is a rich history of using ensemble techniques
    in machine learning, such as bagging, boosting, and random forest, that use this
    technique. The general idea is that, if you build different models using the training
    data, each model has different errors in the predicted values. Where one model
    predicts too high a value, another may predict too low a value, and when averaged,
    some of the errors cancel out, resulting in a more accurate prediction than would
    have been otherwise obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key to ensemble methods is that the different models must have some variability
    in their predictions. If the predictions from the different models are highly
    correlated, then using ensemble techniques will not be beneficial. If the predictions
    from the different models have very low correlations, then the average will be
    far more accurate as it gains the strengths of each model. The following code
    gives an example using simulated data. This small example illustrates the point
    with just three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the predictive value of each model, at least in the training
    data, varies quite a bit. Evaluating the correlations among fitted values in the
    training data can also help to indicate how much overlap there is among the model
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate predicted values for the testing data, the average of the
    predicted values, and again correlate the predictions along with reality in the
    testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From the results, we can see that the average of the three models' predictions
    performs better than any of the models individually. However, this is not always
    the case; one good model may have better predictions than the average predictions. In
    general, it is good to check that the models being averaged perform similarly,
    at least in the training data. The second lesson is that, given models with similar
    performance, it is desirable to have lower correlations between model predictions,
    as this will result in the best performing average.
  prefs: []
  type: TYPE_NORMAL
- en: There are other forms of ensemble methods that are included in other machine
    learning algorithms, for example, bagging and boosting. Bagging is used in random
    forests, where many models are generated, each having different samples of the
    data. The models are deliberately designed to be small, incomplete models. By
    averaging the predictions of lots of undertrained models that use only a portion
    of the data, we should get a more powerful model. An example of boosting includes
    gradient-boosted models (GBMs), which also use multiple models, but this time
    each model focuses on the instances that were incorrectly predicted in the previous
    model. Both random forests and GBMs have proven to be very successful with structured
    data because they reduce variance, that is, avoid overfitting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and model-averaging are not used as frequently in deep neural networks
    because the computational cost of training each model can be quite high, and thus
    repeating the process many times becomes prohibitively expensive in terms of time
    and compute resources. Nevertheless, it is still possible to use model averaging
    in the context of deep neural networks, even if perhaps it is on only a handful
    of models rather than hundreds, as is common in random forests and some other
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – improving out-of-sample model performance using dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout is a novel approach to regularization that is particularly valuable
    for large and complex deep neural networks. For a much more detailed exploration
    of dropout in deep neural networks, see Srivastava, N., Hinton, G., Krizhevsky,
    A., Sutskever, I., and Salakhutdinav, R. (2014). The concept behind dropout is
    actually quite straightforward. During the training of the model, units (for example,
    input and hidden neurons) are probabilistically dropped along with all connections
    to and from them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following diagram is an example of what might happen at each
    step of training for a model where hidden neurons and their connections are dropped
    with a probability of 1/3 for each epoch. Once a node is dropped, its connections
    to the next layer are also dropped. In the the following diagram, the grayed-out
    nodes and dashed connections are the ones that were dropped. It is important to
    note that the choice of nodes that are dropped changes for each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d34f15bc-117f-450b-a667-8d2809c2ae95.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Dropout applied to a layer for different epochs'
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about dropout is that it forces models to be more robust to
    perturbations. Although many neurons are included in the full model, during training
    they are not all simultaneously present, and so neurons must operate somewhat
    more independently than they would otherwise. Another way of viewing dropout is
    that, if you have a large model with N weights between hidden neurons, but 50%
    are dropped during training, although all N weights will be used during some stages
    of training, you have effectively halved the total model complexity as the average
    number of weights will be halved. This reduces model complexity, and hence helps
    to prevent the overfitting of the data. Because of this feature, if the proportion
    of dropout is p, Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and
    Salakhutdinov, R. (2014) recommend scaling up the target model complexity by 1/p
    in order to end up with a roughly equally complex model.
  prefs: []
  type: TYPE_NORMAL
- en: During model testing/scoring, neurons are not usually dropped because it is
    computationally inconvenient. Instead, we can use an approximate average based
    on scaling the weights from a single neural network based on each weight's probability
    of being included (that is, 1/p). This is usually taken care of by the deep learning
    library.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to working well, this approximate weight re-scaling is a fairly
    trivial calculation. Thus, the primary computational cost of dropout comes from
    the fact that a model with more neurons and weights must be used because so many
    (a commonly recommended value is around 50% for hidden neurons) are dropped during
    each training update.
  prefs: []
  type: TYPE_NORMAL
- en: Although dropout is easy to implement, a larger model may be required to compensate.
    To speed up training, a higher learning rate can be used so that fewer epochs
    are required. One potential downside of combining these approaches is that, with
    fewer neurons and a faster learning rate, some weights may become quite large.
    Fortunately, it is possible to use dropout along with other forms of regularization,
    such as the L1 or L2 penalty. Taken together, the result is a larger model that
    that can quickly (a faster Learning rate) explore a broader parameter space, but
    is regularized through dropout and a penalty to keep the weights in check.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show the use of dropout in a neural network, we will return to the **Modified
    National Institute of Standards and Technology** (**MNIST**) dataset (which we
    downloaded in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training
    a Prediction Model*) we worked with previously. We will use the `nn.train()` function
    from the `deepnet` package, as it allows for dropout. As in the previous chapter,
    we will run the four models in parallel to reduce the time it takes. Specifically,
    we compare four models, two with and two without dropout regularization and with
    either 40 or 80 hidden neurons. For dropout, we specify the proportion to dropout
    separately for the hidden and visible units. Based on the rule of thumb that about
    50% of hidden units (and 80% of observed units) should be kept, we specify the
    dropout proportions at `0.5` and `0.2`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can loop through the models to obtain predicted values and get the
    overall model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When evaluating the models in the in-sample training data, it seems those without
    regularization perform better those with regularization. Of course, the real test
    comes with the testing or holdout data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The testing data highlights that the in-sample performance was overly optimistic
    (accuracy = 0.9622 versus accuracy = 0.8520 for the 80-neuron, non-regularized
    model in the training and testing data, respectively). We can see the advantage
    of the regularized models for both the 40- and the 80-neuron models. Although
    both still perform worse in the testing data than they did in the training data,
    they perform on a par with, or better than, the equivalent non-regularized models
    in the testing data. This difference is particularly important for the 80-neuron
    model as the best performing model on the test data is the regularized model.
  prefs: []
  type: TYPE_NORMAL
- en: Although these numbers are by no means record-setting, they do show the value
    of using dropout, or regularization more generally, and how one might go about
    trying to tune the model and dropout parameters to improve the ultimate testing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter began by showing you how to program a neural network from scratch.
    We demonstrated the neural network in a web application created by just using
    R code. We delved into how the neural network actually worked, showing how to
    code forward-propagation, `cost` functions, and backpropagation. Then we looked
    at how the parameters for our neural network apply to modern deep learning libraries
    by looking at the `mx.model.FeedForward.create` function from the `mxnet` deep
    learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Then we covered overfitting, demonstrating several approaches to preventing
    overfitting, including common penalties, the Ll penalty and L2 penalty, ensembles
    of simpler models, and dropout, where variables and/or cases are dropped to make
    the model noisy. We examined the role of penalties in regression problems and
    neural networks. In the next chapter, we will move into deep learning and deep
    neural networks, and see how to push the accuracy and performance of our predictive
    models even further.
  prefs: []
  type: TYPE_NORMAL
