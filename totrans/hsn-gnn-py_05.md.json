["```py\n    from torch_geometric.datasets import Planetoid\n    ```", "```py\n    dataset = Planetoid(root=\".\", name=\"Cora\")\n    ```", "```py\n    data = dataset[0]\n    ```", "```py\n    print(f'Dataset: {dataset}')\n    print('---------------')\n    print(f'Number of graphs: {len(dataset)}')\n    print(f'Number of nodes: {data.x.shape[0]}')\n    print(f'Number of features: {dataset.num_features}')\n    print(f'Number of classes: {dataset.num_classes}')\n    ```", "```py\n    Dataset: Cora()\n    ---------------\n    Number of graphs: 1\n    Number of nodes: 2708\n    Number of features: 1433\n    Number of classes: 7\n    ```", "```py\n    print(f'Graph:')\n    print('------')\n    print(f'Edges are directed: {data.is_directed()}')\n    print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n    print(f'Graph has loops: {data.has_self_loops()}')\n    ```", "```py\n    Graph:\n    ------\n    Edges are directed: False\n    Graph has isolated nodes: False\n    Graph has loops: False\n    ```", "```py\n    from torch_geometric.datasets import FacebookPagePage\n    ```", "```py\n    dataset = FacebookPagePage(root=\".\")\n    ```", "```py\n    data = dataset[0]\n    ```", "```py\n    print(f'Dataset: {dataset}')\n    print('-----------------------')\n    print(f'Number of graphs: {len(dataset)}')\n    print(f'Number of nodes: {data.x.shape[0]}')\n    print(f'Number of features: {dataset.num_features}')\n    print(f'Number of classes: {dataset.num_classes}')\n    ```", "```py\n    Dataset: FacebookPagePage()\n    -----------------------\n    Number of graphs: 1\n    Number of nodes: 22470\n    Number of features: 128\n    Number of classes: 4\n    ```", "```py\n    print(f'\\nGraph:')\n    print('------')\n    print(f'Edges are directed: {data.is_directed()}')\n    print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n    print(f'Graph has loops: {data.has_self_loops()}')\n    ```", "```py\nGraph:\n------\nEdges are directed: False\nGraph has isolated nodes: False\nGraph has loops: True\n```", "```py\n    data.train_mask = range(18000)\n    data.val_mask = range(18001, 20000)\n    data.test_mask = range(20001, 22470)\n    ```", "```py\nimport torch_geometric.transforms as T\ndataset = Planetoid(root=\".\", name=\"Cora\")\ndata = dataset[0]\n```", "```py\nimport pandas as pd\ndf_x = pd.DataFrame(data.x.numpy())\ndf_x['label'] = pd.DataFrame(data.y)\n```", "```py\ndef accuracy(y_pred, y_true):\n    return torch.sum(y_pred == y_true) / len(y_true)\n```", "```py\n    import torch\n    from torch.nn import Linear\n    import torch.nn.functional as F\n    ```", "```py\n    class MLP(torch.nn.Module):\n    ```", "```py\n        def __init__(self, dim_in, dim_h, dim_out):\n            super().__init__()\n            self.linear1 = Linear(dim_in, dim_h)\n            self.linear2 = Linear(dim_h, dim_out)\n    ```", "```py\n        def forward(self, x):\n            x = self.linear1(x)\n            x = torch.relu(x)\n            x = self.linear2(x)\n            return F.log_softmax(x, dim=1)\n    ```", "```py\n        def fit(self, data, epochs):\n            criterion = torch.nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n    ```", "```py\n            self.train()\n            for epoch in range(epochs+1):\n                optimizer.zero_grad()\n                out = self(data.x)\n                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n                acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n                loss.backward()\n                optimizer.step()\n    ```", "```py\n                if epoch % 20 == 0:\n                    val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n                    val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n                    print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n    ```", "```py\n        def test(self, data):\n            self.eval()\n            out = self(data.x)\n            acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n            return acc\n    ```", "```py\n    mlp = MLP(dataset.num_features, 16, dataset.num_classes)\n    print(mlp)\n    ```", "```py\n    MLP(\n      (linear1): Linear(in_features=1433, out_features=16, bias=True)\n      (linear2): Linear(in_features=16, out_features=7, bias=True)\n    )\n    ```", "```py\n    mlp.fit(data, epochs=100)\n    ```", "```py\n    Epoch   0 | Train Loss: 1.954 | Train Acc: 14.29% | Val Loss: 1.93 | Val Acc: 30.80%\n    Epoch  20 | Train Loss: 0.120 | Train Acc: 100.00% | Val Loss: 1.42 | Val Acc: 49.40%\n    Epoch  40 | Train Loss: 0.015 | Train Acc: 100.00% | Val Loss: 1.46 | Val Acc: 50.40%\n    Epoch  60 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 53.40%\n    Epoch  80 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.40 | Val Acc: 54.60%\n    Epoch 100 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.39 | Val Acc: 54.20%\n    ```", "```py\n    acc = mlp.test(data)\n    print(f'MLP test accuracy: {acc*100:.2f}%')\n    ```", "```py\n    MLP test accuracy: 52.50%\n    ```", "```py\n    Epoch   0 | Train Loss: 1.398 | Train Acc: 23.94% | Val Loss: 1.40 | Val Acc: 24.21%\n    Epoch  20 | Train Loss: 0.652 | Train Acc: 74.52% | Val Loss: 0.67 | Val Acc: 72.64%\n    Epoch  40 | Train Loss: 0.577 | Train Acc: 77.07% | Val Loss: 0.61 | Val Acc: 73.84%\n    Epoch  60 | Train Loss: 0.550 | Train Acc: 78.30% | Val Loss: 0.60 | Val Acc: 75.09%\n    Epoch  80 | Train Loss: 0.533 | Train Acc: 78.89% | Val Loss: 0.60 | Val Acc: 74.79%\n    Epoch 100 | Train Loss: 0.520 | Train Acc: 79.49% | Val Loss: 0.61 | Val Acc: 74.94%\n    MLP test accuracy: 74.52%\n    ```", "```py\n    class VanillaGNNLayer(torch.nn.Module):\n    ```", "```py\n        def __init__(self, dim_in, dim_out):\n            super().__init__()\n            self.linear = Linear(dim_in, dim_out, bias=False)\n    ```", "```py\n        def forward(self, x, adjacency):\n            x = self.linear(x)\n            x = torch.sparse.mm(adjacency, x)\n            return x\n    ```", "```py\n    from torch_geometric.utils import to_dense_adj\n    adjacency = to_dense_adj(data.edge_index)[0]\n    adjacency += torch.eye(len(adjacency))\n    adjacency\n    ```", "```py\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n```", "```py\n    class VanillaGNN(torch.nn.Module):\n        def __init__(self, dim_in, dim_h, dim_out):\n            super().__init__()\n            self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n            self.gnn2 = VanillaGNNLayer(dim_h, dim_out)\n    ```", "```py\n        def forward(self, x, adjacency):\n            h = self.gnn1(x, adjacency)\n            h = torch.relu(h)\n            h = self.gnn2(h, adjacency)\n            return F.log_softmax(h, dim=1)\n    ```", "```py\n        def fit(self, data, epochs):\n            criterion = torch.nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n            self.train()\n            for epoch in range(epochs+1):\n                optimizer.zero_grad()\n                out = self(data.x, adjacency)\n                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n                acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n                loss.backward()\n                optimizer.step()\n                if epoch % 20 == 0:\n                    val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n                    val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n                    print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n        def test(self, data):\n            self.eval()\n            out = self(data.x, adjacency)\n            acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n            return acc\n    ```", "```py\n    gnn = VanillaGNN(dataset.num_features, 16, dataset.num_classes)\n    print(gnn)\n    gnn.fit(data, epochs=100)\n    acc = gnn.test(data)\n    print(f'\\nGNN test accuracy: {acc*100:.2f}%')\n    ```", "```py\n    VanillaGNN(\n      (gnn1): VanillaGNNLayer(\n        (linear): Linear(in_features=1433, out_features=16, bias=False)\n      )\n      (gnn2): VanillaGNNLayer(\n        (linear): Linear(in_features=16, out_features=7, bias=False)\n      )\n    )\n    Epoch   0 | Train Loss: 2.008 | Train Acc: 20.00% | Val Loss: 1.96 | Val Acc: 23.40%\n    Epoch  20 | Train Loss: 0.047 | Train Acc: 100.00% | Val Loss: 2.04 | Val Acc: 74.60%\n    Epoch  40 | Train Loss: 0.004 | Train Acc: 100.00% | Val Loss: 2.49 | Val Acc: 75.20%\n    Epoch  60 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 2.61 | Val Acc: 74.60%\n    Epoch  80 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.61 | Val Acc: 75.20%\n    Epoch 100 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.56 | Val Acc: 75.00%\n    GNN test accuracy: 76.80%\n    ```"]