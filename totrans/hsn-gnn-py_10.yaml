- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Links with Graph Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Link prediction** is one of the most popular tasks performed with graphs.
    It is defined as the problem of predicting the existence of a link between two
    nodes. This ability is at the core of social networks and recommender systems.
    A good example is how social media networks display friends and followers you
    have in common with others. Intuitively, if this number is high, you are more
    likely to connect with these people. This likelihood is precisely what link prediction
    tries to estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first see how to perform link prediction without any
    machine learning. These traditional techniques are essential to understanding
    what GNNs learn. We will then refer to previous chapters about `DeepWalk` and
    `Node2Vec` to link prediction through **matrix factorization**. Unfortunately,
    these techniques have significant limitations, which is why we will transition
    to GNN-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore three methods from two different families. The first family
    is based on node embeddings and performs a GNN-based matrix factorization. The
    second method focuses on subgraph representation. The neighborhood around each
    link (fake or real) is considered an input to predict the link probability. Finally,
    we will implement a model of each family in PyTorch Geometric.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to implement various link prediction
    techniques. Given a link prediction problem, you will know which technique is
    the best suited to address it – heuristics, matrix factorization, GNN-based embeddings,
    or subgraph-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting links with traditional methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting links with node embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting links with SEAL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter10.
  prefs: []
  type: TYPE_NORMAL
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting links with traditional methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The link prediction problem has been around for a long time, which is why numerous
    techniques have been proposed to solve it. First, this section will describe popular
    heuristics based on local and global neighborhoods. Then, we will introduce matrix
    factorization and its connection to DeepWalk and Node2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristic techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heuristic techniques are a simple and practical way to predict links between
    nodes. They are easy to implement and offer strong baselines for this task. We
    can classify them based on the number of hops they perform (see *Figure 10**.1*).
    Some of them only require 1-hop neighbors that are adjacent to the target nodes.
    More complex techniques also consider 2-hop neighbors or an entire graph. In this
    section, we will divide them into two categories – *local* (1-hop and 2-hop) and
    *global* heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Graph with 1-hop, 2-hop, and 3-hop neighbors](img/B19153_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Graph with 1-hop, 2-hop, and 3-hop neighbors
  prefs: []
  type: TYPE_NORMAL
- en: 'Local heuristics measure the similarity between two nodes by considering their
    local neighborhoods. We use ![](img/Formula_B19153_10_001.png) to denote the neighbors
    of node ![](img/Formula_B19153_10_002.png). Here are three examples of popular
    local heuristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Common neighbors** simply counts the number of neighbors two nodes have in
    common (1-hop neighbors). The idea is similar to our previous example with social
    networks – the more neighbors you have in common, the more likely you are to be
    connected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Jaccard’s coefficient** measures the proportion of neighbors shared by two
    nodes (1-hop neighbors). It relies on the same idea as common neighbors but normalizes
    the result by the total number of neighbors. This rewards nodes with few interconnected
    neighbors instead of nodes with high degrees:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Adamic–Adar index** sums the inverse logarithmic degree of neighbors
    shared by the two target nodes (2-hop neighbors). The idea is that common neighbors
    with large neighborhoods are less significant than those with small neighborhoods.
    This is why they should have less importance in the final score:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All these techniques rely on neighbors’ node degrees, whether they are direct
    (common neighbors or Jaccard’s coefficient) or indirect (the Adamic–Adar index).
    This is beneficial for speed and explainability but also limits the complexity
    of the relationships they can capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Global heuristics offer a solution to this problem by considering an entire
    network instead of a local neighborhood. Here are two well-known examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Katz index**computes the weighted sum of every possible path between two
    nodes. Weights correspond to a discount factor, ![](img/Formula_B19153_10_006.png)
    (usually between 0.8 and 0.9), to penalize longer paths. With this definition,
    two nodes are more likely to be connected if there are many (preferably short)
    paths between them. Paths of any length can be calculated using adjacency matrix
    powers, ![](img/Formula_B19153_10_007.png), which is why the Katz index is defined
    as follows:![](img/Formula_B19153_10_008.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DeepWalk` and `Node2Vec` algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global heuristics are usually more accurate but require knowing the entirety
    of a graph. However, they are not the only way to predict links with this knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Matrix factorization for link prediction is inspired by the previous work on
    recommender systems [2]. With this technique, we indirectly predict links by predicting
    the entire adjacency matrix, ![](img/Formula_B19153_10_010.png). This is performed
    using node embeddings – similar nodes, ![](img/Formula_B19153_10_011.png) and
    ![](img/Formula_B19153_10_012.png), should have similar embeddings, ![](img/Formula_B19153_10_013.png)
    and ![](img/Formula_B19153_10_014.png) respectively. Using the dot product, we
    can write it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If these nodes are similar, ![](img/Formula_B19153_10_015.png) should be maximal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these nodes are different, ![](img/Formula_B19153_10_016.png) should be minimal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have assumed that similar nodes should be connected. This is why
    we can use this dot product to approximate each element (link) of the adjacency
    matrix, ![](img/Formula_B19153_10_017.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In terms of matrix multiplication, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_B19153_10_020.png) is the node embedding matrix. The
    following figure shows a visual explanation of how matrix factorization works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Matrix multiplication with node embeddings](img/B19153_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Matrix multiplication with node embeddings
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is called matrix factorization because the adjacency matrix,
    ![](img/Formula_B19153_10_021.png), is decomposed into a product of two matrices.
    The goal is to learn relevant node embeddings that minimize the L2 norm between
    true and predicted elements, ![](img/Formula_B19153_10_022.png), for the graph,
    ![](img/Formula_B19153_10_023.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are more advanced variants of matrix factorization that include the Laplacian
    matrix and powers of ![](img/Formula_B19153_10_025.png). Another solution consists
    of using models such as `DeepWalk` and `Node2Vec`. They produce node embeddings
    that can be paired to create link representations. According to Qiu, et al. [3],
    these algorithms implicitly approximate and factorize complex matrices. For example,
    here is the matrix computed by `DeepWalk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_B19153_10_027.png) is the parameter for negative sampling.
    The same can be said for similar algorithms, such as LINE and PTE. Although they
    can capture more complex relationships, they suffer from the same limitations
    that we saw in *Chapters 3* and *4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**They cannot use node features**: They only use topological information to
    create embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They have no inductive capabilities**: They cannot generalize to nodes that
    were not in the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They cannot capture structural similarity**: Structurally similar nodes in
    the graph can obtain vastly different embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limitations motivate the need for GNN-based techniques, as we will see
    in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting links with node embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how to use GNNs to produce node embeddings.
    A popular link prediction technique consists of using these embeddings to perform
    matrix factorization. This section will discuss two GNN architectures for link
    prediction – the **Graph Autoencoder** (**GAE**) and the **Variational Graph**
    **Autoencoder** (**VGAE**).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Graph Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both architectures were introduced by Kipf and Welling in 2016 [5] in a three-page
    paper. They represent the GNN counterparts of two popular neural network architectures
    – the autoencoder and the variational autoencoder. Prior knowledge about these
    architectures is helpful but not necessary. For ease of understanding, we will
    first focus on the GAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GAE is composed of two modules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **encoder** is a classic two-layer GCN that computes node embeddings as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **decoder** approximates the adjacency matrix, ![](img/Formula_B19153_10_029.png),
    using matrix factorization and a sigmoid function, ![](img/Formula_B19153_10_030.png),
    to output probabilities:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we are not trying to classify nodes or graphs. The goal is to predict
    a probability (between 0 and 1) for each element of the adjacency matrix, ![](img/Formula_B19153_10_032.png).
    This is why the GAE is trained using the binary cross-entropy loss (negative log-likelihood)
    between the elements of both adjacency matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, adjacency matrices are often very sparse, which biases the GAE toward
    predicting zero values. There are two simple techniques to fix this bias. First,
    we can add a weight to favor ![](img/Formula_B19153_10_034.png) in the previous
    loss function. Secondly, we can sample fewer zero values during training, making
    labels more balanced. The latter technique is the one implemented by Kipf and
    Welling.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture is flexible – the encoder can be replaced with another type
    of GNN (GraphSAGE, for example), and an MLP can take the role of a decoder, for
    instance. Another possible improvement involves transforming the GAE into a probabilistic
    variant – the Variational GAE.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing VGAEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The difference between GAEs and VGAEs is the same as between autoencoders and
    variational autoencoders. Instead of directly learning node embeddings, VGAEs
    learn normal distributions that are then sampled to produce embeddings. They are
    also divided into two modules:'
  prefs: []
  type: TYPE_NORMAL
- en: The **encoder** is composed of two GCNs that share their first layer. The objective
    is to learn the parameters of each latent normal distribution – a mean, ![](img/Formula_B19153_10_035.png)
    (learned by ![](img/Formula_B19153_10_036.png)), and a variance, ![](img/Formula_B19153_10_037.png)
    (in practice, ![](img/Formula_B19153_10_038.png) learned by ![](img/Formula_B19153_10_039.png)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **decoder** samples embeddings, ![](img/Formula_B19153_10_040.png), from
    the learned distributions, ![](img/Formula_B19153_10_041.png), using the reparametrization
    trick [4]. Then, it uses the same inner product between latent variables to approximate
    the adjacency matrix, ![](img/Formula_B19153_10_042.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With VGAEs, it is important to ensure that the encoder’s output follows a normal
    distribution. This is why we add a new term to the loss function – the **Kullback-Leibler**
    (**KL**) divergence, which measures the divergence between two distributions.
    We obtain the following loss, also called the **evidence lower** **bound** (**ELBO**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B19153_10_044.png) represents the encoder and ![](img/Formula_B19153_10_045.png)
    is the prior distribution of ![](img/Formula_B19153_10_046.png).
  prefs: []
  type: TYPE_NORMAL
- en: The model’s performance is generally evaluated using two metrics – the area
    under the ROC (**AUROC**) curve and the **average** **precision** (**AP**).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how to implement a VGAE using PyTorch Geometric.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a VGAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main differences with previous GNN implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: We will preprocess the dataset to remove links to predict randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will create an encoder model that we will feed to a `VGAE` class, instead
    of directly implementing a VGAE from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code is inspired by PyTorch Geometric’s VGAE example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We try to use the GPU if it’s available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a `transform` object that normalizes input features, directly performs
    tensor device conversion, and randomly splits links. In this example, we have
    an 85/5/10 split. The `add_negative_train_samples` parameter is set to `False`
    because the model already performs negative sampling, so it is not needed in the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the `Cora` dataset with the previous `transform` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `RandomLinkSplit` produces a train/val/test split by design. We store these
    splits as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s implement the encoder. First, we need to import `GCNConv` and `VGAE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We declare a new class. In this class, we want three GCN layers – a shared
    layer, a second layer to approximate mean values, ![](img/Formula_B19153_10_047.png),
    and a third layer to approximate variance values (in practice, the log standard
    deviation, ![](img/Formula_B19153_10_048.png)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can initialize our VGAE and give the encoder as input. By default, it will
    use the inner product as a decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train()` function includes two important steps. First, the embedding matrix,
    ![](img/Formula_B19153_10_049.png), is computed using `model.encode()`; the name
    might be counter-intuitive, but this function does sample embeddings from the
    learned distributions. Then, the ELBO loss is computed with `model.recon_loss()`
    (binary cross-entropy loss) and `model.kl_loss()` (KL divergence). The decoder
    is implicitly called to calculate the cross-entropy loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `test()` function simply calls the VGAE''s dedicated method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train this model for 301 epochs and print the two built-in metrics – the
    AUC and the AP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtain the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We evaluate our model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can manually calculate the approximated adjacency matrix, ![](img/Formula_B19153_10_050.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training a VGAE is fast and outputs results that are easily understandable.
    However, we saw that the GCN is not the most expressive operator. In order to
    improve the model’s expressiveness, we need to incorporate better techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting links with SEAL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section introduced node-based methods, which learn relevant node
    embeddings to compute link likelihoods. Another approach consists of looking at
    the local neighborhood around the target nodes. These techniques are called subgraph-based
    algorithms and were popularized by **SEAL** (which could be said to stand for
    **Subgraphs, Embeddings, and Attributes for Link prediction** – though not always!).
    In this section, we will describe the SEAL framework and implement it using PyTorch
    Geometric.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the SEAL framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in 2018 by Zhang and Chen [6], SEAL is a framework that learns graph
    structure features for link prediction. It defines the subgraph formed by the
    target nodes ![](img/Formula_B19153_10_051.png) and their ![](img/Formula_B19153_10_052.png)-hop
    neighbors as the **enclosing subgraph**. Each enclosing subgraph is used as input
    (instead of the entire graph) to predict a link likelihood. Another way to look
    at it is that SEAL automatically learns a local heuristic for link prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework involves three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enclosing subgraph extraction**, which consists of taking a set of real links
    and a set of fake links (negative sampling) to form the training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Node information matrix construction**, which involves three components –
    node labels, node embeddings, and node features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**GNN training**, which takes the node information matrices as input and outputs
    link likelihoods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are summarized in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The SEAL framework](img/B19153_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – The SEAL framework
  prefs: []
  type: TYPE_NORMAL
- en: The enclosing subgraph extraction is a straightforward process. It consists
    of listing the target nodes and their ![](img/Formula_B19153_10_053.png)-hop neighbors
    to extract their edges and features. A high ![](img/Formula_B19153_10_054.png)
    will improve the quality of the heuristics SEAL can learn, but it also creates
    larger subgraphs that are more computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: The first component of the node information construction is node labeling. This
    process assigns a specific number to each node. Without it, the GNN would be unable
    to differentiate between target and contextual nodes (their neighbors). It also
    embeds distances, which describe nodes’ relative positions and structural importance.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the target nodes, ![](img/Formula_B19153_10_055.png) and ![](img/Formula_B19153_10_056.png),
    must share a unique label to identify them as target nodes. For contextual nodes,
    ![](img/Formula_B19153_10_057.png) and ![](img/Formula_B19153_10_058.png), they
    must share the same label if they have the same distance as the target nodes –
    ![](img/Formula_B19153_10_059.png) and ![](img/Formula_B19153_10_060.png). We
    call this distance the double radius, noted as ![](img/Formula_B19153_10_061.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Different solutions can be considered, but SEAL’s authors propose the **Double-Radius
    Node Labeling** (**DRNL**) algorithm. It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, assign label 1 to ![](img/Formula_B19153_10_062.png) and ![](img/Formula_B19153_10_063.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign label 1 to nodes with a radius – ![](img/Formula_B19153_10_064.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign label 3 to nodes with a radius – ![](img/Formula_B19153_10_065.png) or
    ![](img/Formula_B19153_10_066.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign label 4 to nodes with a radius – ![](img/Formula_B19153_10_067.png),
    ![](img/Formula_B19153_10_068.png), and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The DRNL function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_10_069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B19153_10_070.png), and ![](img/Formula_B19153_10_071.png)
    and ![](img/Formula_B19153_10_072.png) are the integer quotient and remainder
    of ![](img/Formula_B19153_10_073.png) divided by 2 respectively. Finally, these
    node labels are one-hot encoded.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The two other components are easier to obtain. The node embeddings are optional
    but can be calculated using another algorithm, such as `Node2Vec`. Then, they
    are concatenated with the node features and one-hot encoded labels to build the
    final node information matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a GNN is trained to predict links, using enclosing subgraphs’ information
    and adjacency matrices. For this task, SEAL’s authors chose the **Deep Graph Convolutional
    Neural Network** (**DGCNN**) [7]. This architecture performs three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Several GCN layers compute node embeddings that are then concatenated (like
    a GIN).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A global sort pooling layer sorts these embeddings in a consistent order before
    feeding them into convolutional layers, which are not permutation-invariant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Traditional convolutional and dense layers are applied to the sorted graph representations
    and output a link probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DGCNN model is trained using the binary cross-entropy loss and outputs probabilities
    between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the SEAL framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SEAL framework requires extensive preprocessing to extract and label the
    enclosing subgraphs. Let’s implement it using PyTorch Geometric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the `Cora` dataset and apply a link-level random split, like in the
    previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The link-level random split creates new fields in the `Data` object to store
    the labels and index of each positive (real) and negative (fake) edge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a function to process each split and obtain enclosing subgraphs with
    one-hot encoded node labels and node features. We declare a list to store these
    subgraphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each (source and destination) pair in the dataset, we extract the k-hop
    neighbors (here, ![](img/Formula_B19153_10_074.png)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the distances using the DRNL function. First, we remove the target
    nodes from the subgraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compute the adjacency matrices for source and destination nodes based on
    the previous subgraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the distance between every node and the source/destination target
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the node labels, `z`, for every node in the subgraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this example, we will not use node embeddings, but we still concatenate
    features and one-hot-encoded labels to build the node information matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a `Data` object and append it to the list, which is the final output
    of this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use it to extract enclosing subgraphs for each dataset. We separate positive
    and negative examples to get the correct label to predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we merge positive and negative data lists to reconstruct the training,
    validation, and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create data loaders to train the GNN using batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a new class for the DGCNN model. The `k` parameter represents the
    number of nodes to hold for each subgraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create four GCN layers with a fixed hidden dimension of 32:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We instantiate the global sort pooling at the core of the DGCNN architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The node ordering provided by global pooling allows us to use traditional convolutional
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the prediction is managed by an MLP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `forward()` function, we calculate node embeddings for each GCN and
    concatenate the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The global sort pooling, convolutional layers, and dense layers are sequentially
    applied to this result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We instantiate the model on a GPU if available, and train it using the `Adam`
    optimizer and the binary cross-entropy loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a traditional `train()` function for batch training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `test()` function, we calculate the ROC AUC score and the average precision
    to compare the SEAL performance with the VGAE performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train the DGCNN for 31 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we test it on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We obtain results that are similar to those observed using the VGAE (test AUC
    – `0.8833` and test AP – `0.8845`). In theory, subgraph-based methods such as
    SEAL are more expressive than node-based methods such as VGAEs. They capture more
    information by explicitly considering the entire neighborhood around target nodes.
    SEAL’s accuracy can also be improved by increasing the number of neighbors taken
    into account with the `k` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a new task with link prediction. We gave an overview
    of this field by presenting heuristic and matrix factorization techniques. Heuristics
    can be classified according to the k-hop neighbors they consider – from local
    with 1-hop neighbors to global with the knowledge of the entire graph. Conversely,
    matrix factorization approximates the adjacency matrix using node embeddings.
    We also explained how this technique was connected to algorithms described in
    previous chapters (`DeepWalk` and `Node2Vec`).
  prefs: []
  type: TYPE_NORMAL
- en: After this introduction to link prediction, we saw how to implement it using
    GNNs. We outlined two kinds of techniques, based on node embeddings (GAE and VGAE)
    and subgraph representations (SEAL). Finally, we implemented a VGAE and SEAL on
    the `Cora` dataset with an edge-level random split and negative sampling. Both
    models obtained comparable performance, although SEAL is strictly more expressive.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 11*](B19153_11.xhtml#_idTextAnchor131)*, Generating Graphs with
    Graph Neural Networks*, we will see different strategies to produce realistic
    graphs. First, we will describe traditional techniques with the popular Erdős–Rényi
    model. Then, we will see how deep generative methods work by reusing the GVAE
    and introducing a new architecture – the **Graph Recurrent Neural** **Network**
    (**GraphRNN**).
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] H. Tong, C. Faloutsos and J. -y. Pan. “Fast Random Walk with Restart and
    Its Applications” in *Sixth International Conference on Data Mining (ICDM’06)*,
    2006, pp. 613-622, doi: 10.1109/ICDM.2006.70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009\. *Matrix Factorization
    Techniques for Recommender Systems.* Computer 42, 8 (August 2009), 30–37\. https://doi.org/10.1109/MC.2009.263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang. *Network Embedding
    as Matrix Factorization*. Feb. 2018\. doi: 10.1145/3159652.3159706.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. P. Kingma and M. Welling. *Auto-Encoding Variational Bayes.* arXiv,
    2013\. doi: 10.48550/ARXIV.1312.6114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. N. Kipf and M. Welling. *Variational Graph Auto-Encoders*. arXiv, 2016\.
    doi: 10.48550/ARXIV.1611.07308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Zhang and Y. Chen. *Link Prediction Based on Graph Neural Networks*.
    arXiv, 2018\. doi: 10.48550/ARXIV.1802.09691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018\. *An end-to-end
    deep learning architecture for graph classification*. In *Proceedings of the Thirty-Second
    AAAI Conference on Artificial Intelligence* and *Thirtieth Innovative Applications
    of Artificial Intelligence Conference* and *Eighth AAAI Symposium on Educational
    Advances in Artificial Intelligence* (AAAI’18/IAAI’18/EAAI’18). AAAI Press, Article
    544, 4438–4445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
