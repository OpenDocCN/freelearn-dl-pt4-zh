- en: Running Deep Learning Models in the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up till now, we have only briefly discussed the hardware requirements for training
    deep learning models, as almost all of the examples in this book run on any modern
    computer. While you do not need a **GPU** (**Graphical Processing Unit**) based
    computer to run the examples in this book, there is no getting away from the fact
    that training complicated deep learning models requires a computer with a GPU.
    Even if you have a suitable GPU on your machine, installing the necessary software
    to train deep learning models using GPUs is not a trivial task. This section will
    briefly discuss how to install the necessary software to run deep learning models
    on GPUs and also discusses the advantages and disadvantages of using cloud computing
    for deep learning. We will use various cloud providers to create virtual instances
    or access services that will allow us to train deep learning models in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a local computer for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Web Services (AWS) for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Azure for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Google Cloud for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Paperspace for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a local computer for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing this book, it is possible to purchase a computer with
    a GPU card suitable for deep learning for under $1,000. The current on-demand
    cost of the cheapest GPU computer on AWS is $0.90 per hour, which is equivalent
    to using the machine constantly for 46 days. So, if you are just starting with
    deep learning, cloud resources are the cheapest way to begin. Once you have learned
    the basics, then you may decide to get a GPU-based computer, but even then you
    may continue using cloud resources for deep learning. You have much more flexibility
    in the cloud. For example, in AWS, you can get a p3.16xlarge machine with 8 Tesla
    V100 GPU cards for an on-demand price of $24.48 per hour. An equivalent box is
    the DGX-1 from NVIDIA ([https://www.nvidia.com/en-us/data-center/dgx-1/](https://www.nvidia.com/en-us/data-center/dgx-1/)),
    which has 8 Tesla V100 GPU cards and costs $149,000!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are considering using your own computer for deep learning, then one
    of the following applies to you:'
  prefs: []
  type: TYPE_NORMAL
- en: You already have a computer that you can use with a suitable GPU processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will buy a computer to build deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will build a computer to build deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use your local computer for deep learning, you need a suitable
    GPU card, which must be from NVIDIA. The best way to check this is to go to the
    NVIDIA site and check if your graphics card is compatible with CUDA. CUDA is an
    application programming interface (API) that allows programs to use GPU for computing.
    You need to install CUDA to be able to use the GPU for deep learning. The current
    link to check if your graphics card is compatible with CUDA is [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus).
  prefs: []
  type: TYPE_NORMAL
- en: While some companies sell machines that are designed specifically for deep learning,
    they are very expensive. I would not advise getting one of them if you are just
    beginning to, learn deep learning. Instead, I would recommend looking at buying
    a computer that is for high-end computer games. This computer should have an appropriate
    GPU card for deep learning. Again, check that the card is compatible with CUDA
    ([https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus))
    first.
  prefs: []
  type: TYPE_NORMAL
- en: A gaming computer for deep learning? It is not as strange as it seems. GPUs
    were developed to play high-end games on computers, not for deep learning. But
    a machine that is designed for games is likely to have a higher than usual specification,
    for example, an SSD drive, lots of (fast) RAM, and most importantly a GPU card.
    Early deep learning practitioners realized that the matrix operations involved
    in calculating 3D spaces were very similar to the matrix operations used in neural
    networks. NVIDIA released CUDA as an API so that other applications could use
    the GPU as a co-processor. Whether it was luck or foresight, NVIDIA became the
    de facto standard for GPU cards for deep learning and has seen its share price
    grow by 10 times in the past 3 years, largely because of the huge demand for GPU
    cards for artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: The third option is to build your own deep learning computer. If you are considering
    this option, then other than the GPU card, memory, and an SSD drive, you will
    also need to consider the power supply and the motherboard. You may need a bigger
    capacity power supply than what is in a standard computer because of the GPU card
    and fans. For the motherboard, you need to consider if the hardware interface
    between the motherboard and the GPU card may limit the data transfer – these are
    PCIe lanes. A GPU can use 16 PCIe lanes at full capacity. For expansion purposes,
    you may want a motherboard that supports 40 PCIe lanes so that you can support
    two GPU cards and an SSD drive simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the rest of this chapter which discusses using cloud computing
    for deep learning, we should briefly discuss the performance of GPU cards in the
    cloud against what was used for this book. For this book, I used a GTX 1050 Ti
    which has 768 cores and 4 GB RAM. In my experience, the performance of this card
    is about the same as a **p2.xlarge** instance on AWS. I checked this by running
    two models on a local CPU (i5 processor), local GPU (GTX 1050 Ti), and AWS GPU
    (**p2.xlarge**). I ran the test on two models: the binary prediction task from
    [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training Deep Prediction
    Models*, and the LeNet convolutional neural network from [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks*. Both of these models
    were built using MXNet, and ran for 50 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99d7a3cf-186e-4501-86de-fc10925816ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Execution time in seconds for two deep learning networks on CPU,
    local GPU, and AWS GPU'
  prefs: []
  type: TYPE_NORMAL
- en: On my local machine, running the deep learning model for the binary prediction
    task on the GPU is about 20% faster than running on the CPU, and the AWS GPU machine
    is approximately 13% faster than the local GPU. However, there is a much bigger
    difference when running convolutional neural networks, training it on the local
    CPU is almost 16 times slower than training it on the local GPU. In turn, the
    AWS GPU is approximately 16% faster than the local GPU. These results are expected
    and mirror what I have seen in practice and other benchmarks on the web and show
    conclusively that for deep learning computer vision tasks, a GPU is a necessity.
    The GPU card on my local machine (GTX 1050 Ti) is probably the lowest specification
    GPU card you should use for deep learning. It currently costs under $200\. As
    a comparison, a high-end GPU card (GTX 1080 Ti) has 3,584 cores and 11 GB of RAM,
    and currently costs approx $700\. The GTX 1080 Ti is approximately 4-5 times faster
    than the GTX 1050 Ti.
  prefs: []
  type: TYPE_NORMAL
- en: Why does the previous graph just look at AWS and not Azure, Google Cloud, and
    Paperspace? Why did I not benchmark all of them on performance and/or cost? I
    decided not to do so for a few reasons. Firstly, and most importantly, any recommendation
    would have been out of date after a few months—deep learning is very popular and
    the various cloud providers are changing their offerings and prices constantly.
    Another reason is that the examples in this book are relatively small and we are
    using the cheapest GPU instances. Therefore, any comparisons to production use
    cases would be misleading. Finally, when you are starting out, ease of use is
    probably more important than raw cost. All the examples in this book should run
    in under 1 hour in the cloud regardless of which provider you use, so arguing
    that one provider costs $0.55/hour and another costs $0.45/hour is not important.
  prefs: []
  type: TYPE_NORMAL
- en: How do I know if my model is training on a GPU?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One question that many people starting in deep learning ask is, *how do I know
    if my model is training on a GPU?* Fortunately, whether you are using a cloud
    instance or your local machine, you can check if the deep learning model is being
    trained on the GPU or the CPU. There is a utility on the instance that shows the
    GPU''s activity. In Linux, you can type in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In Windows, you can use the following command from a command prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run a script that outputs diagnostic messages about the GPU on the
    computer. If your model is currently training on the GPU, the GPU utility will
    be high. In the following example, we can see that it is 75-78%. We can also see
    that the file called `rsession.exe` is using GPU memory. This confirms that the
    model is being trained on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53b61b85-9ce9-40d7-bdbc-48764803c6df.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2: nvidia-smi utility showing that the GPU card is at 75-85% utilization
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AWS** is the biggest cloud provider, and so it deserves our attention. If
    you know how to use AWS and especially if you are familiar with spot requests,
    it can be a very cost-effective method to train complex deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section gives you a brief introduction to how AWS works. It describes EC2,
    AMIs, and how to create a virtual machine in the cloud. This will not be an exhaustive
    introduction to AWS – there are plenty of tutorials online that will guide you.
  prefs: []
  type: TYPE_NORMAL
- en: AWS is a suite of cloud resources. Another term for it is **Infrastructure as
    a Service** (**IaaS**), as opposed to **Software as a Service** (**SaaS**) or
    **Platform as a Service** (**PaaS**). In IaaS, as opposed to SaaS or PaaS, you
    are supplied with infrastructure (hardware), and it is up to you to use it as
    you wish. This includes installing software and managing security and networking,
    although AWS take care of some aspects of security and networking. AWS has many
    services, but for deep learning, the one you will use is EC2, which is a virtual
    computing environment so that you can launch instances (virtual computers). You
    can control these virtual computers either through web interfaces or by remote
    logging into them to run commands from the shell. When you launch an EC2 instance,
    you can select the operating system (Ubuntu, Linux, Windows, and so on) and the
    type of machine you want.
  prefs: []
  type: TYPE_NORMAL
- en: You can also select to use an **Amazon Machine Image** (**AMI**), which has
    software applications and libraries pre-installed on it. This is a good choice
    for deep learning as it means that you can start an EC2 instance with the deep
    learning libraries already installed and jump straight into deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: One other service you should be familiar with is S3, which is a form of persistent
    storage. A very useful practice that I suggest you to adopt is to consider your
    virtual machines as temporary resources and to keep your data and interim results
    in S3\. We will not discuss this in this chapter because it is an advanced topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we stated that the current on-demand cost of the cheapest
    GPU computer on AWS is $0.90 per hour. *On-demand* is one way to use a virtual
    machine in AWS, but there are three different ways to rent a virtual machine in
    AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On-demand instances**: When you rent an instance as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reserved instances**: When you commit to renting the machine for a certain
    period of time (usually 1-3 years). This is about 50% cheaper than on-demand instances.
    However you are committed to paying for the resource for the period of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spot instances**: In order to deal with fluctuating demand, Amazon has spare
    computing capacity most of the time. You can bid for this unused capacity and,
    depending on the demand for that type of machine, you can usually get it cheaper
    than on-demand and reserved instances. However, once you have the machine, it
    is not guaranteed that you will keep it as long as you need – if the demand for
    the computer goes up, your computer may be terminated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reserve instances are not useful for deep learning. The cost of renting the
    cheapest GPU machine for 1 year would be over $5,000, and you can buy a deep learning
    machine with better performance for much less. On-demand instances guarantee that
    you will have the resource as long as needed, but are expensive. Spot instances
    are an interesting and cost-effective method to use if you know how to use them
    correctly and plan for the chance that your computer will be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the spot price is about 30% of the on-demand price, so the savings
    are significant. Your bid is the maximum amount that you are willing to pay for
    the spot instances, the actual price depends on the market price, which is based
    on the demand.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you should set your bid price higher; I recommend to set it at either
    51%, 76%, or 101% of the on-demand price. The extra 1% from 50%, 75%, and 100%
    is because, similar to any bidding market, humans anchor their bids to round numbers,
    so by avoiding this with an extra 1%, it can make a difference.
  prefs: []
  type: TYPE_NORMAL
- en: The original use case for spot instances was for low-priority batch jobs. Companies
    used spot instances to the avail of themselves cheaper computing resources for
    long-running jobs that could be restarted in the event that they did not finish.
    An example might be running a secondary data ingestion process on data that was
    not critical to operations. However, the demand pattern for GPU based instances
    is different, possibly because of online data mining competitions such as Kaggle.
    Because GPU instances are not very common, demand spikes much more for GPU instances.
    This has led to some strange behavior in spot pricing, where the price that people
    bid for a spot instance can be 10x that of the on-demand price. People do this
    because they believe that this makes it unlikely that they will be outbid. There
    are cases where a p2.16xlarge has had a spot price of $144 per hour while the
    on-demand price is $14.40\. The people who set these bids do not want their machines
    to be terminated and believe that they will still pay less on average for spot
    instances than on-demand instances. This is not something I would encourage if
    you use spot instances as you can get a very nasty surprise if demand goes up!
    However, you should be aware of this pricing quirk – do not think that setting
    the bid price to just above the on-demand price guarantees that your machine will
    not be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS provides you help in setting up your spot request bid by providing pricing
    history charts that advise you on the on-demand price and the bid prices. In the
    following screenshot, we can see how the price has changed over the past 3 months
    for a particular region (us-east). There are 6 availability zones (us-east-1a
    to us-east-1f) and the current spot price of this instance type (**p2.16xlarge**)
    varies from $4.32-$14.40, while the on-demand price is $14.40:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2acf192-d7df-49ce-9267-f6ba038f9a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Pricing history for spot bids for p2.16xlarge instance type'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the preceding graph for this resource, I would consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: I would use the availability zone **us-east-1a** if possible, as it has the
    lowest price volatility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I would set the price to $7.21 per hour, which is just over 50% of the on-demand
    price. I would probably only pay $4.32 per hour as it has been 1 month since the
    bid price in us-east-1a has gone over $4.32 per hour. Setting it at the higher
    price would make it less likely that my spot instance would be terminated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regions and availability zones:** AWS arranges its services in regions (**us-east1**,
    **eu-west1**, and so on). Currently, there are 18 different regions and in each
    region, there are multiple availability zones, which you can consider as physical
    data centers. For some use cases (for example, websites, disaster recovery, and
    so on) and regulatory requirements, regions and availability zones are important.
    For deep learning, they are not so important, as you can usually run your deep
    learning models at any location. The bid price for spot instances is different
    for regions/availability zones, and some resources are more expensive in some
    regions. You also need to be aware that there is a cost in transferring data between
    regions, so keep your data and instances in the same region.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a deep learning GPU instance in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will use AWS to train a deep learning model from [Chapter 9](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml), *Anomaly
    Detection and Recommendation Systems*. This will include setting up the machine,
    accessing the machine, downloading the data, and running the model. We are going
    to use a pre-built AWS AMI from RStudio that has TensorFlow and Keras already
    installed. For details on this AMI, go to this link: [https://aws.amazon.com/marketplace/pp/B0785SXYB2](https://aws.amazon.com/marketplace/pp/B0785SXYB2). You
    will need to sign up for an AWS account if you do not already have one at [https://portal.aws.amazon.com/billing/signup](https://portal.aws.amazon.com/billing/signup).
    Once you have signed up, follow these steps to create a virtual machine that has
    a GPU on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that when you set up an instance in AWS, you will be billed for as long
    as it is running! Always ensure that you shut down your instances, otherwise you
    will continue to be charged. Check the AWS console to ensure you have no running
    instances when you are finished using the virtual instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the AWS console and select EC2\. You should see a screen similar
    to the following. This is the web interface for creating new virtual machines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aba67547-2b69-4a67-b9c5-a36d4b11a68b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: AWS EC2 dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the launch instance button and the following page will load.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **AWS Marketplace** on the left and in the search box type `rstudio` (see
    the following screenshot).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **RStudio Server with Tensorflow-GPU for AWS**. Be aware that there
    is another option with the word **Pro **– this is a paid subscription with additional
    costs, so do not select this AMI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aafcaf7a-2869-471c-acce-f673585f8536.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5: AWS launch instance wizard, Step 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you click **Select**, the following screen may appear with some additional
    information on accessing the instance. Read the instructions carefully, as they
    may have changed from what''s shown in the screenshot that follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d2ec8bac-a577-4bab-895c-35e7461b5a64.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6: RStudio AMI information
  prefs: []
  type: TYPE_NORMAL
- en: 'When you click **Continue**, the following screen will appear for the machine
    type. It is vital to select a machine that has a GPU, so from the **Filter by:** option, select
    GPU compute and then select **p2.xlarge** from the list. Your options should look
    similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ed92abfe-bcc6-468b-98f1-e7c5c15ce603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7: AWS launch instance wizard, Step 2
  prefs: []
  type: TYPE_NORMAL
- en: 'When you click Next, you will get to the following screen with various configuration
    options. The default options are OK, so just press Next again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fd0097b8-c8c5-4039-ae0e-3eddc27cf31b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8: AWS launch instance wizard, Step 3
  prefs: []
  type: TYPE_NORMAL
- en: This screen allows you to change the storage options. You may need to add additional
    storage depending on the size of the data. Storage is relatively cheap, so I recommend
    going with 3x-5x the size of the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Next** to go to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/286b5c73-eb73-4ac7-a270-878f3d9a958b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9: AWS launch instance wizard, Step 4
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screen is not important – tags are used to keep track of resources
    in AWS, but we do not need them. Click Next to go to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e25e132f-c0dd-4f66-b764-9cfa0554e48c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10: AWS launch instance wizard, step 5
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows security options. AWS restricts access to instances,
    so you must open any needed ports. The defaults provided here allow access to
    port `22` (SSH) to access the shell and also for port `8787`, which is the web
    port that RStudio uses. Click Review and Launch to continue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cd54d2b2-35c6-45d5-9046-24345d306457.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11: AWS launch instance wizard, Step 6
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot will appear. Note the warning messages regarding security –
    in a production environment, you would probably want to address these.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Launch** button to continue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/157480e8-9d0b-44f5-b32f-6f5422d462e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12: AWS launch instance wizard, Step 7
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be asked for a key pair. If you have not already created a key pair,
    then select the option to do so. Give it a descriptive name and press the Download
    Key Pair button. Then, click on Launch Instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A key pair is used to access the instance using SSH. You should guard this very
    carefully, as if someone manages to get your private key, then they will be able
    to log in to any of your instances. You should delete your key pair occasionally
    and create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef21c0ea-5bf6-4899-9adc-fb34f76686bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13: AWS launch instance wizard, select key pair
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have completed this, you can go back to the EC2 dashboard and you
    will see that you have 1 Running Instances. Click on that link to move on to the
    details of the instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/34bf01f9-062b-498b-85da-c69914e6fcdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14: AWS EC2 dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you will see the details of the instance. In this case, the IP address
    is `34.227.109.123`. Also note down the instance ID that is highlighted, as this
    is the password that is used to connect to the RStudio instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/27feac10-e148-43ce-8972-9e7784921555.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15: AWS EC2 dashboard, instance details
  prefs: []
  type: TYPE_NORMAL
- en: Open another web page and browse to the IP address of your machine and add `:8787`
    to access the link. In my example, the link is `http://34.227.109.123:8787/`.
    Instructions for logging in are in *Figure 10.6*, that is, use rstudio-user as
    the username and the instance ID as the password. You should also consider changing
    the password as per the instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you log in, you will see a familiar interface – it is similar to the RStudio
    desktop program. One difference you have is the **Upload** button on the bottom-right
    pane, which allows you to upload files. In the following example, I have uploaded
    the data and the script from [Chapter 9](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml),
    *Anomaly Detection and Recommendation Systems*, for the Keras recommender example
    and ran it successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dc6290f6-e4d3-47ae-b27d-9f9571921105.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Accessing deep learning instance in the cloud using RStudio Server'
  prefs: []
  type: TYPE_NORMAL
- en: The web interface in RStudio is similar to using RStudio on your local computer.
    In *Figure 10.16*, you can see data files that I have uploaded (`recomend.csv`, `recomend40.csv`)
    and the R script in the Files in the bottom-left window. We can also see the code
    that was executed in the Console window in the bottom-left.
  prefs: []
  type: TYPE_NORMAL
- en: 'This finishes our example on how to set up a deep learning machine in AWS.
    Again, remember that you will be billed for as long as the computer is running.
    Ensure that your instances are terminated, otherwise you will continue to be charged.
    To do so, go back to the EC2 dashboard, find the instance, and click on the **Actions**
    button. A pop-up menu will appear, where you can select **Instance State** and
    then select **Terminate**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d35019f-f470-4a97-86eb-31bb1b1048d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Terminating the AWS instance'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a deep learning AMI in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we used an **Amazon Machine Image** (**AMI**) that
    was built by RStudio. In AWS, you can also create your own AMI's. When you create
    an AMI, you can install the software you want, load data onto it, and set it up
    as you wish. This section will show you how to use an AMI to use MXNet on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in creating an AMI is to select the base image that you are going
    to use. We could start with a base image that just has the operating system installed,
    but instead we are going to use the **RStudio Server with Tensorflow-GPU for AWS** that
    we used previously and add the MXNet package to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The instructions to install MXNet were adapted from [https://mxnet.incubator.apache.org/install/index.html](https://mxnet.incubator.apache.org/install/index.html).
    The first step is to create the instance from the **RStudio Server with Tensorflow-GPU
    for AWS** AMI as per the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have done this, you need to SSH into the machine. How you do this depends
    on the operating system on your own computer. For Linux and macOS, you can execute
    a local command on the shell, and in Windows you can use Putty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have logged in to the machine, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following line to the end of this file and save the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you are back at the shell, run the following lines one by one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The last command could take up to 2 hours to complete. Once it is done, run
    the last few lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The second line may take up to 30 minutes. The final line may return a warning
    about a missing file, which can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test if everything installed correctly, go to the RStudio page for the instance
    and type in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now go back to the EC2 dashboard, click on **Running Instances**, and select
    the machine in the list. Click on the **Action** button, select **Image** from
    the drop-down menu, and select **Create Image**. This is shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6e79acde-0322-4ea8-bd54-af931a923b29.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18: Creating an AMI
  prefs: []
  type: TYPE_NORMAL
- en: 'The image may take 15-20 minutes to complete. When it is done, click on AMIs
    on the left menu selection to show the list of AMIs associated with your account.
    You should see the AMI you just created. This AMI can then be used to create a
    new on-demand instance or a new spot instance. The following screenshot shows
    the menu option to create a spot instance for the AMI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1a09ca79-e660-464f-84bc-71adffd561a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19: Using an existing AMI for a spot request
  prefs: []
  type: TYPE_NORMAL
- en: This AMI is now available so that you can create new deep learning instances.
    You should be aware that there is an ongoing cost in storing the AMI, even if
    you do not use it.
  prefs: []
  type: TYPE_NORMAL
- en: Using Azure for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure is the brand name for Microsoft's cloud services. You can use Azure for
    deep learning and, similar to AWS, it provides deep learning virtual machines
    that is pre-configured with deep learning libraries installed. In this example,
    we are going to create a Windows instance that can be used for Keras or MXNet.
    This assumes that your local computer is also a Windows computer, as you will
    be using **Remote Desktop Protocol** (**RDP**) to access the cloud instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create an account in Azure and then log in to Azure at
    [https://portal.azure.com](https://portal.azure.com). You will see a screenshot
    similar to the following. Click **Create a resource** and search for **Deep Learning
    Virtual Machine**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/35fda232-a7e7-4c1c-99e6-a719b99d6798.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.20: Azure portal website'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you select **Deep Learning Virtual Machine**, the following screen will
    appear. Click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/865011d0-1c63-4727-913d-39a08bc61ffd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.21: Provisioning a deep learning instance on Azure, step 0'
  prefs: []
  type: TYPE_NORMAL
- en: You will now start a 4-step wizard to create the new instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first step (Basics), asks for some basic details. It is OK to enter the
    same values as I have done, but fill in the username and password carefully as
    you will need them later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b977cee-8553-449a-b482-efa7f5031dec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.22: Provisioning a deep learning instance on Azure, step 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Step 2 (Settings), ensure that the virtual machine size is 1 x Standard
    NC6 (1 GPU), and click **OK** to continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a27d0e4-b492-4566-8383-1d37646b48c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.23: Provisioning a deep learning instance on Azure, step 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Step 3 (Summary), there is a brief validation check. You may be told that
    your account does not have sufficient Compute/VM (cores/vCPUs) resources available,
    which is because Microsoft may have restricted your account when it was first
    created. Create a support ticket to increase your resources and try again. If
    you have passed this step, click **OK** to continue. You are now on the final
    step, so just click **Create**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55fb8721-62c1-4693-a074-b0b9c9a76f10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.24: Provisioning a deep learning instance on Azure, Step 4'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have to wait 30-40 minutes until the resources are created. When this
    is complete, select **All resources** on the left and you will see that all of
    the objects have been created. The following screenshot shows an example of this.
    Click on the one where the type is **Virtual Machine**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b96b0b9e-f5db-449d-846f-c1013f799bd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.25: List of currently provisioned resources on Azure'
  prefs: []
  type: TYPE_NORMAL
- en: You will then see the following screenshot. Click on the **Connect** button
    on the top of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A pane will open up on the right and give you an option to **Download RDP File**.
    Click on that, and when the file is downloaded, double-click on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4f60bb8b-8e00-4906-b815-9940b5a28833.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.26: Downloading the RDP file to connect to the cloud instance in
    Azure'
  prefs: []
  type: TYPE_NORMAL
- en: 'This should bring up a login window to connect to the cloud instance. Enter
    the username and password that you created in step 1 to connect to the instance.
    When you connect, you will see a desktop similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a0697b47-5d20-4e0c-b5f5-9f775c2e5cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.27: The remote desktop of the deep learning instance (Azure)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! RStudio is already installed. Keras is already installed, so any Keras
    deep learning code you have will run. Let''s try and run some MXNet code. Open
    RStudio and run the following commands to install MXNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will not work on the version of R installed. If you want to use MXNet,
    you must download the latest version of R (3.5.1 at the time of writing) and install
    it. Unfortunately, this will disable Keras, so only do this if you want to use
    MXNet instead of Keras. Once you download R from https://cran.r-project.org/,
    then re-run the code above to install MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The software installed on these AMI''s change very frequently. Before
    installing any deep learning library, check the version of CUDA that is installed.
    You need to ensure that they deep learning library is compatible with the version
    of CUDA installed on the machine.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Google Cloud for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud also has GPU instances. At the time of writing this book, the price
    of an instance with an NVIDIA Tesla K80 GPU card (which is also the GPU card in
    an AWS p2.xlarge instance) is $0.45 per hour on-demand. This is significantly
    cheaper than the AWS on-demand price. Further details of Google Cloud's GPU instances
    are at [https://cloud.google.com/gpu/](https://cloud.google.com/gpu/). However,
    for Google Cloud, we are not going to use instances. Instead, we are going to
    use the Google Cloud Machine Learning Engine API to submit machine learning jobs
    to the cloud. One big advantage of this approach over provisioning virtual machines
    is that you only pay for the hardware resources that you use and do not have to
    worry about setting up and terminating instances. More details and pricing can
    be found at [https://cloud.google.com/ml-engine/pricing](https://cloud.google.com/ml-engine/pricing).
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the following steps to sign up for Google Cloud and enable the API:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign up for an account with Google Cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need to login to the portal at [https://console.cloud.google.com](https://console.cloud.google.com)
    and enable the **Cloud Machine Learning Engine** API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **APIs & Services** from the main menu and click on the **Enable APIs
    and services** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The APIs are contained in groups. Select **View All** for the **Machine Learning**
    group, then select **Cloud Machine Learning Engine** and ensure that the API is
    enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the API is enabled, execute the following code from RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This should install the Google Cloud SDK, and you will asked to connect your
    Google account to the SDK. Then, you will be taken through a menu of options in
    the Terminal window. The first option is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09b57ce1-eb63-4e33-a609-82a1d2fe641b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.28: Accessing Google Cloud SDK from RStudio'
  prefs: []
  type: TYPE_NORMAL
- en: For now, do not create any new projects or configurations, just select the ones
    that already exist. Once you have linked your Google account to the Google SDK
    on your machine and enabled the services, you are ready to go. The Cloud Machine
    Learning Engine allows you to submit a job to Google Cloud without having to create
    any instances. All the files in the working folder (R scripts and data) will be
    zipped up and sent to Google Cloud as a package.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, I took a the recommendation file from the project in [Chapter
    8](49a1fa27-1130-4f86-966e-cc73444b88a2.xhtml), *Deep Learning models using TensorFlow
    in R*. I copied this file and the `keras_recommend.R` script into a new directory
    and created a new RStudio project in that directory. I then opened the project
    in RStudio. You can see these two files and the RStudio project file in the previous
    screenshot. Then, I executed the following line in RStudio to submit the deep
    learning job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will collect the files within the current working directory and send them
    to the Cloud Machine Learning Engine. As the job is executed, some progress information
    will be sent back to RStudio. You can also monitor the activity on the console page
    on [https://console.cloud.google.com](https://console.cloud.google.com) by selecting **ML
    Engine** | **Jobs**. Here is a screenshot of this web page showing two finished
    jobs and one that was canceled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8519af01-4cd4-4c31-b2c2-3a1d7d03a2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.29: The ML Engine/Jobs page on the Google Cloud Platform web page'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the job is finished, the logs will be downloaded to your local machine.
    A nice summary web page is automatically created showing statistics for the job,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfd08ae4-afaa-4f7f-b71a-d07a60c1f1d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.30: Web summary page from the machine learning job'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the graph showing the progress of the model during training, the
    model summary, some hyperparameters (**epochs**, **batch_size**, and so on), as
    well as the cost (**ml_units**). The web page also contains the output from the
    R script. Select **Output** from the menu to see it. In the following screenshot,
    we can see the R code and the output from that code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccfca405-fa9f-4dd7-b102-fe39f624f402.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.31: Web summary page from the machine learning job showing the R
    code and output'
  prefs: []
  type: TYPE_NORMAL
- en: This is only a brief introduction to using the Google Cloud Machine Learning
    Engine. There is an excellent tutorial at [https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html](https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html) that
    explains how you can use this service for hyperparameter training. Using this
    service rather than cloud instances for hyperparameter training is simpler and
    probably cheaper than trying to manage it yourself using virtual instances. You
    do not have to monitor it and coordinate the different runs of the model training. More
    information on using this service is available at [https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html](https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html).
  prefs: []
  type: TYPE_NORMAL
- en: Using Paperspace for deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Paperspace** is another interesting way to perform deep learning in the cloud.
    It might be the easiest way to train deep learning models in the cloud. To set
    up a cloud instance with Paperspace, you can log in to their console, provision a
    new machine, and connect to it from your web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by signing up for a Paperspace account, log in to the console, and go
    into the Virtual Machine section by selecting Core or Compute. Paperspace has
    an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0)
    already installed, along with the GPU version of TensorFlow and Keras for R. You
    will see this machine type when you select **Public Templates**, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e81d0c7e-1252-4cc6-9f55-da9d48ab433f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.32: Paperspace portal'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a choice of three GPU instances and the choice of pay by
    the hour or monthly. Select the cheapest option (currently P4000 at $0.40 per
    hour) and the hourly pricing. Scroll down to the bottom of the page and press
    the **Create** button. After a few minutes, your machine will be provisioned and
    you will be able to access it through your browser. An example of an RStudio Paperspace
    instance is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5dac837c-ce96-4cea-ae66-14f848f46040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.33: Accessing the virtual machine''s desktop from a web page and
    running RStudio for a Paperspace instance'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Keras is already installed, so you can go ahead and train deep
    learning models using Keras. However, we are also going to install MXNet on our
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to open RStudio and install a few packages. Execute the following
    commands from RStudio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to access the Terminal (or shell) for the instance you just
    created. You can go back to the console page and do it from there. Alternatively,
    click on the circle target in the top right corner of the desktop (see the previous
    screenshot). This also gives you other options such as synchronizing copy-and-paste
    between your local computer and the VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have logged in to the Terminal for the instance, running the following
    commands will install MXNet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You also need to add the following line to the end of the `.profile` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When you are done, restart the instance. You now have a machine that can train
    Keras and MXNet deep learning models in the cloud. For more details on using RStudio
    in Paperspace, see [https://tensorflow.rstudio.com/tools/cloud_desktop_gpu.html](https://tensorflow.rstudio.com/tools/cloud_desktop_gpu.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of options for training deep learning models in this chapter!
    We discussed options for running it locally and showed the importance of having
    a GPU card. We used the three main cloud providers to train deep learning models
    in R on the cloud. Cloud computing is a fantastic resource – we gave an example
    of a super-computer costing $149,000\. A few years ago, such a resource would
    have been out of reach for practically everyone, but now thanks to cloud computing,
    you can rent a machine like this on an hourly basis.
  prefs: []
  type: TYPE_NORMAL
- en: For AWS, Azure, and Paperspace, we installed MXNet on the cloud resources, giving
    us the option of which deep learning library to use. I encourage you to use the
    examples in the other chapters in this book and try all the different cloud providers
    here. It is amazing to think that you could do so and your total cost could be
    less than $10!
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we build an image classification solution from
    image files. We will demonstrate how to apply transfer learning, which allows
    you to adapt an existing model to a new dataset. We will show how to deploy a
    model to production using a REST API and briefly discuss Generative Adversarial
    Networks, reinforcement learning and provide some further resources if you wish
    to continue on your deep learning quest.
  prefs: []
  type: TYPE_NORMAL
