["```py\n    from torch_geometric.datasets import TUDataset\n    dataset = TUDataset(root='.', name='PROTEINS').shuffle()\n    print(f'Dataset: {dataset}')\n    print('-----------------------')\n    print(f'Number of graphs: {len(dataset)}')\n    print(f'Number of nodes: {dataset[0].x.shape[0]}')\n    print(f'Number of features: {dataset.num_features}')\n    print(f'Number of classes: {dataset.num_classes}')\n    Dataset: PROTEINS(1113)\n    -----------------------\n    Number of graphs: 1113\n    Number of nodes: 30\n    Number of features: 0\n    Number of classes: 2\n    ```", "```py\n    from torch_geometric.loader import DataLoader\n    train_dataset = dataset[:int(len(dataset)*0.8)]\n    val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n    test_dataset  = dataset[int(len(dataset)*0.9):]\n    print(f'Training set   = {len(train_dataset)} graphs')\n    print(f'Validation set = {len(val_dataset)} graphs')\n    print(f'Test set       = {len(test_dataset)} graphs')\n    ```", "```py\n    Training set   = 890 graphs\n    Validation set = 111 graphs\n    Test set       = 112 graphs\n    ```", "```py\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=True)\n    test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=True)\n    ```", "```py\n    print('\\nTrain loader:')\n    for i, batch in enumerate(train_loader):\n        print(f' - Batch {i}: {batch}')\n    print('\\nValidation loader:')\n    for i, batch in enumerate(val_loader):\n        print(f' - Batch {i}: {batch}')\n    print('\\nTest loader:')\n    for i, batch in enumerate(test_loader):\n        print(f' - Batch {i}: {batch}')\n    Train loader:\n     - Batch 0: DataBatch(edge_index=[2, 8622], x=[2365, 0], y=[64], batch=[2365], ptr=[65])\n     - Batch 1: DataBatch(edge_index=[2, 6692], x=[1768, 0], y=[64], batch=[1768], ptr=[65])\n    …\n     - Batch 13: DataBatch(edge_index=[2, 7864], x=[2102, 0], y=[58], batch=[2102], ptr=[59])\n    Validation loader:\n     - Batch 0: DataBatch(edge_index=[2, 8724], x=[2275, 0], y=[64], batch=[2275], ptr=[65])\n     - Batch 1: DataBatch(edge_index=[2, 8388], x=[2257, 0], y=[47], batch=[2257], ptr=[48])\n    Test loader:\n     - Batch 0: DataBatch(edge_index=[2, 7906], x=[2187, 0], y=[64], batch=[2187], ptr=[65])\n     - Batch 1: DataBatch(edge_index=[2, 9442], x=[2518, 0], y=[48], batch=[2518], ptr=[49])\n    ```", "```py\nimport torch\ntorch.manual_seed(0)\nimport torch.nn.functional as F\nfrom torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\nfrom torch_geometric.nn import GINConv\nfrom torch_geometric.nn import global_add_pool\nclass GIN(torch.nn.Module):\n    def __init__(self, dim_h):\n        super(GIN, self).__init__()\n        self.conv1 = GINConv(\n            Sequential(Linear(dataset.num_node_features, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()))\n        self.conv2 = GINConv(\n            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()))\n        self.conv3 = GINConv(\n            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()))\n```", "```py\n            self.lin1 = Linear(dim_h*3, dim_h*3)\n            self.lin2 = Linear(dim_h*3, dataset.num_classes)\n    ```", "```py\n        def forward(self, x, edge_index, batch):\n            # Node embeddings\n            h1 = self.conv1(x, edge_index)\n            h2 = self.conv2(h1, edge_index)\n            h3 = self.conv3(h2, edge_index)\n            # Graph-level readout\n            h1 = global_add_pool(h1, batch)\n            h2 = global_add_pool(h2, batch)\n            h3 = global_add_pool(h3, batch)\n            # Concatenate graph embeddings\n            h = torch.cat((h1, h2, h3), dim=1)\n            # Classifier\n            h = self.lin1(h)\n            h = h.relu()\n            h = F.dropout(h, p=0.5, training=self.training)\n            h = self.lin2(h)\n            return F.log_softmax(h, dim=1)\n    ```", "```py\n    def train(model, loader):\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n        epochs = 100\n        model.train()\n        for epoch in range(epochs+1):\n            total_loss = 0\n            acc = 0\n            val_loss = 0\n            val_acc = 0\n            # Train on batches\n            for data in loader:\n                optimizer.zero_grad()\n                out = model(data.x, data.edge_index, data.batch)\n                loss = criterion(out, data.y)\n                total_loss += loss / len(loader)\n                acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n                loss.backward()\n                optimizer.step()\n                # Validation\n                val_loss, val_acc = test(model, val_loader)\n    ```", "```py\n            # Print metrics every 20 epochs\n            if(epoch % 20 == 0):\n                print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n        return model\n    ```", "```py\n    @torch.no_grad()\n    def test(model, loader):\n        criterion = torch.nn.CrossEntropyLoss()\n        model.eval()\n        loss = 0\n        acc = 0\n        for data in loader:\n            out = model(data.x, data.edge_index, data.batch)\n            loss += criterion(out, data.y) / len(loader)\n            acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n        return loss, acc\n    ```", "```py\n    def accuracy(pred_y, y):\n        return ((pred_y == y).sum() / len(y)).item()\n    ```", "```py\n    gin = GIN(dim_h=32)\n    gin = train(gin, train_loader)\n    Epoch 0 | Train Loss: 1.33 | Train Acc: 58.04% | Val Loss: 0.70 | Val Acc: 59.97%\n    Epoch 20 | Train Loss: 0.54 | Train Acc: 74.50% | Val Loss: 0.55 | Val Acc: 76.86%\n    Epoch 40 | Train Loss: 0.50 | Train Acc: 76.28% | Val Loss: 0.56 | Val Acc: 74.73%\n    Epoch 60 | Train Loss: 0.50 | Train Acc: 76.77% | Val Loss: 0.54 | Val Acc: 72.04%\n    Epoch 80 | Train Loss: 0.49 | Train Acc: 76.95% | Val Loss: 0.57 | Val Acc: 73.67%\n    Epoch 100 | Train Loss: 0.50 | Train Acc: 76.04% | Val Loss: 0.53 | Val Acc: 69.55%\n    ```", "```py\n    test_loss, test_acc = test(gin, test_loader)\n    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n    Test Loss: 0.44 | Test Acc: 81.77%\n    ```", "```py\n    import numpy as np\n    import networkx as nx\n    import matplotlib.pyplot as plt\n    from torch_geometric.utils import to_networkx\n    fig, ax = plt.subplots(4, 4)\n    ```", "```py\n    for i, data in enumerate(dataset[-16:]):\n        out = gcn(data.x, data.edge_index, data.batch)\n        color = \"green\" if out.argmax(dim=1) == data.y else \"red\"\n    ```", "```py\n        ix = np.unravel_index(i, ax.shape)\n        ax[ix].axis('off')\n        G = to_networkx(dataset[i], to_undirected=True)\n        nx.draw_networkx(G,\n                        pos=nx.spring_layout(G, seed=0),\n                        with_labels=False,\n                        node_size=10,\n                        node_color=color,\n                        width=0.8,\n                        ax=ax[ix]\n                        )\n    ```", "```py\n    gcn.eval()\n    gin.eval()\n    acc_gcn = 0\n    acc_gin = 0\n    acc_ens = 0\n    ```", "```py\n    for data in test_loader:\n        out_gcn = gcn(data.x, data.edge_index, data.batch)\n        out_gin = gin(data.x, data.edge_index, data.batch)\n        out_ens = (out_gcn + out_gin)/2\n    ```", "```py\n        acc_gcn += accuracy(out_gcn.argmax(dim=1), data.y) / len(test_loader)\n        acc_gin += accuracy(out_gin.argmax(dim=1), data.y) / len(test_loader)\n        acc_ens += accuracy(out_ens.argmax(dim=1), data.y) / len(test_loader)\n    ```", "```py\n    print(f'GCN accuracy:     {acc_gcn*100:.2f}%')\n    print(f'GIN accuracy:     {acc_gin*100:.2f}%')\n    print(f'GCN+GIN accuracy: {acc_ens*100:.2f}%')\n    GCN accuracy: 72.14%\n    GIN accuracy: 80.99%\n    GCN+GIN accuracy: 81.25%\n    ```"]