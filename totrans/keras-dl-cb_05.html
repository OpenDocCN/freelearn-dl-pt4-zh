<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Recurrent Neural Networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Recurrent Neural Networks (<strong class="calibre7">RNNs</strong>) make use of sequential or time series data. In a regular neural network, we consider that all inputs and outputs are independent of each other. For a task where you want to predict the next word in a given sentence, it's better to know which words have come before it. RNNs are recurrent as the same task is performed for every element in the sequence where the output is dependent on the previous calculations. RNNs can be thought of as having a <strong class="calibre7">memory</strong> that captures information about what has been computed so far.</p>
<p class="calibre4">Going from feedforward neural networks to recurrent neural networks, we will use the concept of sharing parameters across various parts of the model. Parameter sharing will make it possible to extend and apply the model to examples of different forms (different lengths, here) and generalize across them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Introduction to RNNs</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">To understand RNNs, we have to understand the basics of feedforward neural networks. You can refer to <a href="73c6bf42-1089-4504-97d5-a7dafae0e59c.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 3</a>, <em class="calibre17">Optimization for Neural Networks</em>, for details on feedforward networks. Both feedforward and recurrent neural networks are identified from the way they process the information or features through a series of mathematical operations performed at the various nodes of the network. One feeds information straight through (never touching a given node twice), the other cycles it through a loop.</p>
<p class="calibre4">A feedforward neural network is trained on image data until it minimizes the loss or error while predicting or classifying the categories for image types. With the trained set of hyper parameters or weights, the neural network can classify data it has never seen before. A trained feedforward neural network can be shown any random collection of images and the first image it classifies will not alter how it classifies the other images.</p>
<p class="calibre4">In a nutshell, these networks have no notion of order in time or temporal pattern, and the only information they consider is the current example it has been asked to classify.</p>
<p class="calibre4">RNNs take into account the temporal nature of the input data. An input to the RNN cell is both from the current timestep and one step back in time. Details are presented in the following diagram:</p>
<div class="mce-root"><img src="Images/bb938643-d41f-41f3-91ff-52a9a58becbf.png" width="915" height="453" class="calibre137"/></div>
<p class="calibre4">RNNs are recurrent in nature because they perform the same computation for every element in a sequence where the output is dependent on the previous computations. The other way to think about RNNs is that they have memory that can capture the information about what has been computed so far. RNNs can make use of the information or knowledge in long sequences but practically they are restricted to looking back only a few steps.</p>
<p class="calibre4">A typical RNN looks as follows:</p>
<div class="mce-root"><img src="Images/3b910d0e-fd5f-4fc8-9a87-dc5bfad83414.png" width="130" height="173" class="calibre138"/></div>
<p class="calibre4">An unwrapped version of the RNN is shown in the following image; by unwrapping we mean that we write out the neural network for a complete sequence. Consider a sequence of five words; the network will be unwrapped into a five-layer neural network, one layer for each word:</p>
<div class="mce-root"><img src="Images/1ec29cd0-643a-4f88-ab1f-2e9315c103b6.png" width="977" height="272" class="calibre139"/></div>
<p class="calibre4">Computations happening in an RNN are as follows:</p>
<div class="mce-root"><img src="Images/e7e0812f-609b-45b5-a109-9e6e6d6f6ae9.png" width="940" height="377" class="calibre140"/></div>
<ul class="calibre20">
<li class="calibre21"><img class="alignnone" src="Images/64313d85-a18d-42e1-9793-4f567b969e1b.jpg" width="50" height="75"/> denotes the input at timestep <img class="alignnone1" src="Images/2c8b95fa-cac0-44f7-a919-6ad5e8acd3f6.jpg" width="29" height="50"/>.</li>
<li class="calibre21"><img class="alignnone2" src="Images/87c94d1b-1897-42a5-9e5b-c06d45d23745.jpg" width="46" height="75"/> denotes the hidden state at timestep <img class="alignnone1" src="Images/e0b2da69-d7f9-4c8a-86c7-509f4582749b.jpg" width="29" height="50"/>. The hidden state is the memory of the network. <img class="alignnone3" src="Images/deba0343-39ab-45a6-80fa-8e5314797374.jpg" width="46" height="75"/> is computed based on the previously hidden state and the input at the current step, <img class="alignnone4" src="Images/f3bdc7a8-b8f6-4dc4-8921-88c9a6324268.jpg" width="392" height="83"/>.</li>
<li class="calibre21">Function <em class="calibre29">f</em> represents non-linearity such as <em class="calibre29">tanh</em> or ReLU. The first hidden state is typically initialized to all zeroes.</li>
<li class="calibre21"><img class="alignnone" src="Images/64fa0205-2db6-439c-9ba1-09e3ad190341.jpg" width="50" height="75"/> denotes the output at step <img class="alignnone5" src="Images/18f8876a-fc08-4bcb-bf7a-6dff44642c3e.jpg" width="29" height="50"/>. To predict the next word in a given sentence, it will be a vector of probabilities across the vocabulary, <img class="alignnone6" src="Images/738130f6-dc44-4e1e-a2ff-21b10ec81f36.jpg" width="371" height="83"/>.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">RNN implementation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Following program processes, a sequence of numbers and the goal is to predict the next value, with the previous values provided. The input to the RNN network at every time step is the current value and a state vector that represents or stores what the neural network has seen at timesteps before. This state-vector is the encoded memory of the RNN, initially set to zero.</p>
<p class="calibre4">Training data is basically a random binary vector. The output is shifted to the right.</p>
<pre class="calibre26"><span class="calibre5">from </span>__future__ <span class="calibre5">import </span>print_function<span class="calibre5">, </span>division<br class="calibre2"/><span class="calibre5">import </span>tensorflow <span class="calibre5">as </span>tf<br class="calibre2"/><span class="calibre5">import </span>numpy <span class="calibre5">as </span>np<br class="calibre2"/><span class="calibre5">import </span>matplotlib.pyplot <span class="calibre5">as </span>plt<br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">define all the constants<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>numEpochs = <span class="calibre5">10<br class="calibre2"/></span>seriesLength = <span class="calibre5">50000<br class="calibre2"/></span>backpropagationLength = <span class="calibre5">15<br class="calibre2"/></span>stateSize = <span class="calibre5">4<br class="calibre2"/></span>numClasses = <span class="calibre5">2<br class="calibre2"/></span>echoStep = <span class="calibre5">3<br class="calibre2"/></span>batchSize = <span class="calibre5">5<br class="calibre2"/></span>num_batches = seriesLength // batchSize // backpropagationLength<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">generate data<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">generateData</span>():<br class="calibre2"/>    x = np.array(np.random.choice(<span class="calibre5">2</span><span class="calibre5">, </span>seriesLength<span class="calibre5">, </span><span class="calibre5">p</span>=[<span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">0.5</span>]))<br class="calibre2"/>    y = np.roll(x<span class="calibre5">, </span>echoStep)<br class="calibre2"/>    y[<span class="calibre5">0</span>:echoStep] = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">    </span>x = x.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/>    y = y.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">return </span>(x<span class="calibre5">, </span>y)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">start computational graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>batchXHolder = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"x_input"</span>)<br class="calibre2"/>batchYHolder = tf.placeholder(tf.int32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"y_input"</span>)<br class="calibre2"/><br class="calibre2"/>initState = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>stateSize]<span class="calibre5">, </span><span class="calibre5">"rnn_init_state"</span>)<br class="calibre2"/><br class="calibre2"/>W = tf.Variable(np.random.rand(stateSize+<span class="calibre5">1</span><span class="calibre5">, </span>stateSize)<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight1"</span>)<br class="calibre2"/>bias1 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>stateSize))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>W2 = tf.Variable(np.random.rand(stateSize<span class="calibre5">, </span>numClasses)<span class="calibre5">,</span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight2"</span>)<br class="calibre2"/>bias2 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>numClasses))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>tf.summary.histogram(<span class="calibre5">name</span>=<span class="calibre5">"weights"</span><span class="calibre5">, </span><span class="calibre5">values</span>=W)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Unpack columns<br class="calibre2"/></span>inputsSeries = tf.unstack(batchXHolder<span class="calibre5">, </span><span class="calibre5">axis</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"input_series"</span>)<br class="calibre2"/>labelsSeries = tf.unstack(batchYHolder<span class="calibre5">, </span><span class="calibre5">axis</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"labels_series"</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Forward pass<br class="calibre2"/></span>currentState = initState<br class="calibre2"/>statesSeries = []<br class="calibre2"/><span class="calibre5">for </span>currentInput <span class="calibre5">in </span>inputsSeries:<br class="calibre2"/>    currentInput = tf.reshape(currentInput<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span><span class="calibre5">1</span>]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"current_input"</span>)<br class="calibre2"/>    inputAndStateConcatenated = tf.concat([currentInput<span class="calibre5">, </span>currentState]<span class="calibre5">, </span><span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"input_state_concat"</span>)<br class="calibre2"/><br class="calibre2"/>    nextState = tf.tanh(tf.matmul(inputAndStateConcatenated<span class="calibre5">, </span>W) + bias1<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"next_state"</span>)<br class="calibre2"/>    statesSeries.append(nextState)<br class="calibre2"/>    currentState = nextState<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5"># calculate loss<br class="calibre2"/></span>logits_series = [tf.matmul(state<span class="calibre5">, </span>W2) + bias2 <span class="calibre5">for </span>state <span class="calibre5">in </span>statesSeries]<br class="calibre2"/>predictions_series = [tf.nn.softmax(logits) <span class="calibre5">for </span>logits <span class="calibre5">in </span>logits_series]<br class="calibre2"/><br class="calibre2"/>losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="calibre5">labels</span>=labels<span class="calibre5">, </span><span class="calibre5">logits</span>=logits) <span class="calibre5">for </span>logits<span class="calibre5">, </span>labels <span class="calibre5">in </span><span class="calibre5">zip</span>(logits_series<span class="calibre5">,</span>labelsSeries)]<br class="calibre2"/>total_loss = tf.reduce_mean(losses<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"total_loss"</span>)<br class="calibre2"/><br class="calibre2"/>train_step = tf.train.AdagradOptimizer(<span class="calibre5">0.3</span>).minimize(total_loss<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"training"</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">plot computation<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">plot</span>(loss_list<span class="calibre5">, </span>predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY):<br class="calibre2"/>    plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span><span class="calibre5">1</span>)<br class="calibre2"/>    plt.cla()<br class="calibre2"/>    plt.plot(loss_list)<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>batchSeriesIdx <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">5</span>):<br class="calibre2"/>        oneHotOutputSeries = np.array(predictions_series)[:<span class="calibre5">, </span>batchSeriesIdx<span class="calibre5">, </span>:]<br class="calibre2"/>        singleOutputSeries = np.array([(<span class="calibre5">1 </span><span class="calibre5">if </span>out[<span class="calibre5">0</span>] &lt; <span class="calibre5">0.5 </span><span class="calibre5">else </span><span class="calibre5">0</span>) <span class="calibre5">for </span>out <span class="calibre5">in </span>oneHotOutputSeries])<br class="calibre2"/><br class="calibre2"/>        plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span>batchSeriesIdx + <span class="calibre5">2</span>)<br class="calibre2"/>        plt.cla()<br class="calibre2"/>        plt.axis([<span class="calibre5">0</span><span class="calibre5">, </span>backpropagationLength<span class="calibre5">, </span><span class="calibre5">0</span><span class="calibre5">, </span><span class="calibre5">2</span>])<br class="calibre2"/>        left_offset = <span class="calibre5">range</span>(backpropagationLength)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchX[batchSeriesIdx<span class="calibre5">, </span>:]<span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"blue"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchY[batchSeriesIdx<span class="calibre5">, </span>:] * <span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"red"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>singleOutputSeries * <span class="calibre5">0.3</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"green"</span>)<br class="calibre2"/><br class="calibre2"/>    plt.draw()<br class="calibre2"/>    plt.pause(<span class="calibre5">0.0001</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">run the graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>    writer = tf.summary.FileWriter(<span class="calibre5">"logs"</span><span class="calibre5">, </span><span class="calibre5">graph</span>=tf.get_default_graph())<br class="calibre2"/>    sess.run(tf.global_variables_initializer())<br class="calibre2"/>    plt.ion()<br class="calibre2"/>    plt.figure()<br class="calibre2"/>    plt.show()<br class="calibre2"/>    loss_list = []<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>epoch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(numEpochs):<br class="calibre2"/>        x<span class="calibre5">,</span>y = generateData()<br class="calibre2"/>        _current_state = np.zeros((batchSize<span class="calibre5">, </span>stateSize))<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">print</span>(<span class="calibre5">"New data, epoch"</span><span class="calibre5">, </span>epoch_idx)<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">for </span>batch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(num_batches):<br class="calibre2"/>            start_idx = batch_idx * backpropagationLength<br class="calibre2"/>            end_idx = start_idx + backpropagationLength<br class="calibre2"/><br class="calibre2"/>            batchX = x[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/>            batchY = y[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/><br class="calibre2"/>            _total_loss<span class="calibre5">, </span>_train_step<span class="calibre5">, </span>_current_state<span class="calibre5">, </span>_predictions_series = sess.run(<br class="calibre2"/>                [total_loss<span class="calibre5">, </span>train_step<span class="calibre5">, </span>currentState<span class="calibre5">, </span>predictions_series]<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                </span><span class="calibre5">feed_dict</span>={<br class="calibre2"/>                    batchXHolder:batchX<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>batchYHolder:batchY<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>initState:_current_state<br class="calibre2"/>                })<br class="calibre2"/><br class="calibre2"/>            loss_list.append(_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5"># fix the cost summary later<br class="calibre2"/></span><span class="calibre5">            </span>tf.summary.scalar(<span class="calibre5">name</span>=<span class="calibre5">"totalloss"</span><span class="calibre5">, </span><span class="calibre5">tensor</span>=_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>batch_idx%<span class="calibre5">100 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Step"</span><span class="calibre5">,</span>batch_idx<span class="calibre5">, </span><span class="calibre5">"Loss"</span><span class="calibre5">, </span>_total_loss)<br class="calibre2"/>                plot(loss_list<span class="calibre5">, </span>_predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY)<br class="calibre2"/><br class="calibre2"/>plt.ioff()<br class="calibre2"/>plt.show()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Computational graph</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The computational graph is shown as following:</p>
<div class="mce-root"><img src="Images/b454f847-90e3-4a82-b40a-e1bcf2589066.png" class="calibre141"/></div>
<p class="calibre4">The output of the listing is shown as follows:</p>
<pre class="calibre26">New data, epoch 0<br class="calibre2"/>Step 0 Loss 0.777418<br class="calibre2"/>Step 600 Loss 0.693907<br class="calibre2"/>New data, epoch 1<br class="calibre2"/>Step 0 Loss 0.690996<br class="calibre2"/>Step 600 Loss 0.691115<br class="calibre2"/>New data, epoch 2<br class="calibre2"/>Step 0 Loss 0.69259<br class="calibre2"/>Step 600 Loss 0.685826<br class="calibre2"/>New data, epoch 3<br class="calibre2"/>Step 0 Loss 0.684189<br class="calibre2"/>Step 600 Loss 0.690608<br class="calibre2"/>New data, epoch 4<br class="calibre2"/>Step 0 Loss 0.691302<br class="calibre2"/>Step 600 Loss 0.691309<br class="calibre2"/>New data, epoch 5<br class="calibre2"/>Step 0 Loss 0.69172<br class="calibre2"/>Step 600 Loss 0.694034<br class="calibre2"/>New data, epoch 6<br class="calibre2"/>Step 0 Loss 0.692927<br class="calibre2"/>Step 600 Loss 0.42796<br class="calibre2"/>New data, epoch 7<br class="calibre2"/>Step 0 Loss 0.42423<br class="calibre2"/>Step 600 Loss 0.00845207<br class="calibre2"/>New data, epoch 8<br class="calibre2"/>Step 0 Loss 0.188478<br class="calibre2"/>Step 500 Loss 0.00427217</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">RNN implementation with TensorFlow</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">We will now use the TensorFlow API; the inner workings of the RNN are hidden under the hood. The TensorFlow <kbd class="calibre18">rnn</kbd> package unrolls the RNN and creates the graph automatically so that we can remove the for loop:</p>
<pre class="calibre26"><span class="calibre5">from </span>__future__ <span class="calibre5">import </span>print_function<span class="calibre5">, </span>division<br class="calibre2"/><span class="calibre5">import </span>tensorflow <span class="calibre5">as </span>tf<br class="calibre2"/><span class="calibre5">import </span>numpy <span class="calibre5">as </span>np<br class="calibre2"/><span class="calibre5">import </span>matplotlib.pyplot <span class="calibre5">as </span>plt<br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">define all the constants<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>numEpochs = <span class="calibre5">10<br class="calibre2"/></span>seriesLength = <span class="calibre5">50000<br class="calibre2"/></span>backpropagationLength = <span class="calibre5">15<br class="calibre2"/></span>stateSize = <span class="calibre5">4<br class="calibre2"/></span>numClasses = <span class="calibre5">2<br class="calibre2"/></span>echoStep = <span class="calibre5">3<br class="calibre2"/></span>batchSize = <span class="calibre5">5<br class="calibre2"/></span>num_batches = seriesLength // batchSize // backpropagationLength<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">generate data<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">generateData</span>():<br class="calibre2"/>    x = np.array(np.random.choice(<span class="calibre5">2</span><span class="calibre5">, </span>seriesLength<span class="calibre5">, </span><span class="calibre5">p</span>=[<span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">0.5</span>]))<br class="calibre2"/>    y = np.roll(x<span class="calibre5">, </span>echoStep)<br class="calibre2"/>    y[<span class="calibre5">0</span>:echoStep] = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">    </span>x = x.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/>    y = y.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">return </span>(x<span class="calibre5">, </span>y)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">start computational graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>batchXHolder = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"x_input"</span>)<br class="calibre2"/>batchYHolder = tf.placeholder(tf.int32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"y_input"</span>)<br class="calibre2"/><br class="calibre2"/>initState = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>stateSize]<span class="calibre5">, </span><span class="calibre5">"rnn_init_state"</span>)<br class="calibre2"/><br class="calibre2"/>W = tf.Variable(np.random.rand(stateSize+<span class="calibre5">1</span><span class="calibre5">, </span>stateSize)<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight1"</span>)<br class="calibre2"/>bias1 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>stateSize))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>W2 = tf.Variable(np.random.rand(stateSize<span class="calibre5">, </span>numClasses)<span class="calibre5">,</span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight2"</span>)<br class="calibre2"/>bias2 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>numClasses))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>tf.summary.histogram(<span class="calibre5">name</span>=<span class="calibre5">"weights"</span><span class="calibre5">, </span><span class="calibre5">values</span>=W)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Unpack columns<br class="calibre2"/></span>inputsSeries = tf.split(<span class="calibre5">axis</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">num_or_size_splits</span>=backpropagationLength<span class="calibre5">, </span><span class="calibre5">value</span>=batchXHolder)<br class="calibre2"/>labelsSeries = tf.unstack(batchYHolder<span class="calibre5">, </span><span class="calibre5">axis</span>=<span class="calibre5">1</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Forward passes<br class="calibre2"/></span><span class="calibre5">from </span>tensorflow.contrib <span class="calibre5">import </span>rnn<br class="calibre2"/>cell = rnn.BasicRNNCell(stateSize)<br class="calibre2"/>statesSeries<span class="calibre5">, </span>currentState = rnn.static_rnn(cell<span class="calibre5">, </span>inputsSeries<span class="calibre5">, </span>initState)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># calculate loss<br class="calibre2"/></span>logits_series = [tf.matmul(state<span class="calibre5">, </span>W2) + bias2 <span class="calibre5">for </span>state <span class="calibre5">in </span>statesSeries]<br class="calibre2"/>predictions_series = [tf.nn.softmax(logits) <span class="calibre5">for </span>logits <span class="calibre5">in </span>logits_series]<br class="calibre2"/><br class="calibre2"/>losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="calibre5">labels</span>=labels<span class="calibre5">, </span><span class="calibre5">logits</span>=logits) <span class="calibre5">for </span>logits<span class="calibre5">, </span>labels <span class="calibre5">in </span><span class="calibre5">zip</span>(logits_series<span class="calibre5">,</span>labelsSeries)]<br class="calibre2"/>total_loss = tf.reduce_mean(losses<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"total_loss"</span>)<br class="calibre2"/><br class="calibre2"/>train_step = tf.train.AdagradOptimizer(<span class="calibre5">0.3</span>).minimize(total_loss<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"training"</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">plot computation<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">plot</span>(loss_list<span class="calibre5">, </span>predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY):<br class="calibre2"/>    plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span><span class="calibre5">1</span>)<br class="calibre2"/>    plt.cla()<br class="calibre2"/>    plt.plot(loss_list)<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>batchSeriesIdx <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">5</span>):<br class="calibre2"/>        oneHotOutputSeries = np.array(predictions_series)[:<span class="calibre5">, </span>batchSeriesIdx<span class="calibre5">, </span>:]<br class="calibre2"/>        singleOutputSeries = np.array([(<span class="calibre5">1 </span><span class="calibre5">if </span>out[<span class="calibre5">0</span>] &lt; <span class="calibre5">0.5 </span><span class="calibre5">else </span><span class="calibre5">0</span>) <span class="calibre5">for </span>out <span class="calibre5">in </span>oneHotOutputSeries])<br class="calibre2"/><br class="calibre2"/>        plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span>batchSeriesIdx + <span class="calibre5">2</span>)<br class="calibre2"/>        plt.cla()<br class="calibre2"/>        plt.axis([<span class="calibre5">0</span><span class="calibre5">, </span>backpropagationLength<span class="calibre5">, </span><span class="calibre5">0</span><span class="calibre5">, </span><span class="calibre5">2</span>])<br class="calibre2"/>        left_offset = <span class="calibre5">range</span>(backpropagationLength)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchX[batchSeriesIdx<span class="calibre5">, </span>:]<span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"blue"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchY[batchSeriesIdx<span class="calibre5">, </span>:] * <span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"red"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>singleOutputSeries * <span class="calibre5">0.3</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"green"</span>)<br class="calibre2"/><br class="calibre2"/>    plt.draw()<br class="calibre2"/>    plt.pause(<span class="calibre5">0.0001</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">run the graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>    writer = tf.summary.FileWriter(<span class="calibre5">"logs"</span><span class="calibre5">, </span><span class="calibre5">graph</span>=tf.get_default_graph())<br class="calibre2"/>    sess.run(tf.global_variables_initializer())<br class="calibre2"/>    plt.ion()<br class="calibre2"/>    plt.figure()<br class="calibre2"/>    plt.show()<br class="calibre2"/>    loss_list = []<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>epoch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(numEpochs):<br class="calibre2"/>        x<span class="calibre5">,</span>y = generateData()<br class="calibre2"/>        _current_state = np.zeros((batchSize<span class="calibre5">, </span>stateSize))<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">print</span>(<span class="calibre5">"New data, epoch"</span><span class="calibre5">, </span>epoch_idx)<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">for </span>batch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(num_batches):<br class="calibre2"/>            start_idx = batch_idx * backpropagationLength<br class="calibre2"/>            end_idx = start_idx + backpropagationLength<br class="calibre2"/><br class="calibre2"/>            batchX = x[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/>            batchY = y[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/><br class="calibre2"/>            _total_loss<span class="calibre5">, </span>_train_step<span class="calibre5">, </span>_current_state<span class="calibre5">, </span>_predictions_series = sess.run(<br class="calibre2"/>                [total_loss<span class="calibre5">, </span>train_step<span class="calibre5">, </span>currentState<span class="calibre5">, </span>predictions_series]<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                </span><span class="calibre5">feed_dict</span>={<br class="calibre2"/>                    batchXHolder:batchX<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>batchYHolder:batchY<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>initState:_current_state<br class="calibre2"/>                })<br class="calibre2"/><br class="calibre2"/>            loss_list.append(_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5"># fix the cost summary later<br class="calibre2"/></span><span class="calibre5">            </span>tf.summary.scalar(<span class="calibre5">name</span>=<span class="calibre5">"totalloss"</span><span class="calibre5">, </span><span class="calibre5">tensor</span>=_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>batch_idx%<span class="calibre5">100 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Step"</span><span class="calibre5">,</span>batch_idx<span class="calibre5">, </span><span class="calibre5">"Loss"</span><span class="calibre5">, </span>_total_loss)<br class="calibre2"/>                plot(loss_list<span class="calibre5">, </span>_predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY)<br class="calibre2"/><br class="calibre2"/>plt.ioff()<br class="calibre2"/>plt.show()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Computational graph</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The following is the image of computational graph:</p>
<div class="mce-root"><img src="Images/53cc84ec-3960-4956-8431-37935ea2c8a9.png" class="calibre142"/></div>
<p class="calibre4">The output of the listing is shown as follows:</p>
<pre class="calibre26">New data, epoch 0<br class="calibre2"/>Step 0 Loss 0.688437<br class="calibre2"/>Step 600 Loss 0.00107078<br class="calibre2"/>New data, epoch 1<br class="calibre2"/>Step 0 Loss 0.214923<br class="calibre2"/>Step 600 Loss 0.00111716<br class="calibre2"/>New data, epoch 2<br class="calibre2"/>Step 0 Loss 0.214962<br class="calibre2"/>Step 600 Loss 0.000730697<br class="calibre2"/>New data, epoch 3<br class="calibre2"/>Step 0 Loss 0.276177<br class="calibre2"/>Step 600 Loss 0.000362316<br class="calibre2"/>New data, epoch 4<br class="calibre2"/>Step 0 Loss 0.1641<br class="calibre2"/>Step 600 Loss 0.00025342<br class="calibre2"/>New data, epoch 5<br class="calibre2"/>Step 0 Loss 0.0947087<br class="calibre2"/>Step 600 Loss 0.000276762</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Introduction to long short term memory networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The vanishing gradient problem has appeared as the biggest obstacle to recurrent networks.</p>
<p class="calibre4">As the straight line changes along the <em class="calibre17">x</em> axis with a slight change in the <em class="calibre17">y</em> axis, the gradient shows change in all the weights with regard to change in error. If we don't know the gradient, we will not be able to adjust the weights in a direction that will reduce the loss or error, and our neural network ceases to learn.</p>
<p class="calibre4"><strong class="calibre7">Long short term memories</strong> (<strong class="calibre7">LSTMs</strong>) are designed to overcome the vanishing gradient problem. Retaining information for a larger duration of time is effectively their implicit behavior.</p>
<p class="calibre4">In standard RNNs, the repeating cell will have an elementary structure, such as a single <strong class="calibre7">tanh</strong> layer:</p>
<div class="mce-root"><img src="Images/3ce2d1f5-54e4-4b79-9bfd-9ec6fac73002.png" width="984" height="376" class="calibre143"/></div>
<p class="calibre4">As seen in the preceding image, LSTMs also have a chain-like structure, but the recurrent cell has a different structure:</p>
<div class="mce-root"><img src="Images/d56f78b6-5471-44cd-8c0d-b8d983a62edb.png" width="989" height="385" class="calibre144"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Life cycle of LSTM</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The key to LSTMs is the cell state that is like a conveyor belt. It moves down the stream with minor linear interactions. It's straightforward for data to flow as unchanged:</p>
<div class="mce-root"><img src="Images/fe8f82d3-8908-45b9-9aea-178576cd8070.png" width="499" height="290" class="calibre145"/></div>
<p class="calibre4">LSTM networks have the ability to either remove or add information to the cell state that is carefully regulated by structures known as gates.</p>
<ol class="calibre23">
<li class="chapter">The first step in an LSTM network is to determine what information we will be throwing away from the cell state. The decision is made by a sigmoid layer known as the <strong class="calibre3">forget gate</strong> layer. The layer looks at the previous state <em class="calibre29">h(t-1)</em> and current input <em class="calibre29">x(t)</em> and outputs a number between 0 and 1 for each number in the cell state <em class="calibre29">C(t−1)</em>, where 1 represents <strong class="calibre3">absolutely keep this</strong> while a 0 represents <strong class="calibre3">entirely get rid of this</strong>:</li>
</ol>
<div class="mce-root"><img src="Images/20b9c65d-c1a3-4647-a963-46b538b83bfd.png" width="940" height="290" class="calibre146"/></div>
<ol start="2" class="calibre23">
<li class="chapter">The next step is to determine what new information we are going to persist in the cell state. Firstly, a sigmoid layer known as the input gate layer decides which values will be updated. Secondly, a <em class="calibre29">tanh</em> layer generates a vector of new candidate values <em class="calibre29">C̃</em> that could be added to the state.</li>
</ol>
<div class="mce-root"><img src="Images/b0d4fd69-9c56-44db-8493-4f5120847a16.png" width="940" height="290" class="calibre147"/></div>
<ol start="3" class="calibre23">
<li class="chapter"><span class="calibre5">We will now update the old cell state</span> <em class="calibre29">C(t−1)</em> <span class="calibre5">to the new cell state</span> <em class="calibre29">C(t)</em><span class="calibre5">. We multiply the old state by</span> <em class="calibre29">f(t)</em><span class="calibre5">, forgetting the things we decided to forget earlier. Then we add</span> <em class="calibre29">i(t) ∗ C̃</em>; these are the new candidate values scaled by the amount we decided to update each state value.</li>
</ol>
<div class="mce-root"><img src="Images/c262f1f2-8163-43d6-aa07-67fa578add3d.png" width="940" height="290" class="calibre148"/></div>
<ol start="4" class="calibre23">
<li class="chapter">Finally, we decide on the output, which will be based on our cell state but will be a filtered or modified version. Firstly, we execute the sigmoid layer that determines what parts of the cell state we're going to output. Following which, we put the cell state through tanh to push the values to be between −1 and 1, and multiply it by the output of the sigmoid gate so that we only output the parts we decided to.</li>
</ol>
<div class="mce-root"><img src="Images/1f22f442-8d01-441d-9a6a-aea5ec3723e7.png" width="940" height="290" class="calibre149"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">LSTM implementation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">LSTMs remember, forget, and pick what to pass on and then output depending on the current state and input. An LSTM has many more moving parts, but using the native TensorFlow API, it will be quite straightforward:</p>
<pre class="calibre26"><span class="calibre5">from </span>__future__ <span class="calibre5">import </span>print_function<span class="calibre5">, </span>division<br class="calibre2"/><span class="calibre5">import </span>tensorflow <span class="calibre5">as </span>tf<br class="calibre2"/><span class="calibre5">import </span>numpy <span class="calibre5">as </span>np<br class="calibre2"/><span class="calibre5">import </span>matplotlib.pyplot <span class="calibre5">as </span>plt<br class="calibre2"/><span class="calibre5">from </span>tensorflow.contrib <span class="calibre5">import </span>rnn<br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">define all the constants<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>numEpochs = <span class="calibre5">10<br class="calibre2"/></span>seriesLength = <span class="calibre5">50000<br class="calibre2"/></span>backpropagationLength = <span class="calibre5">15<br class="calibre2"/></span>stateSize = <span class="calibre5">4<br class="calibre2"/></span>numClasses = <span class="calibre5">2<br class="calibre2"/></span>echoStep = <span class="calibre5">3<br class="calibre2"/></span>batchSize = <span class="calibre5">5<br class="calibre2"/></span>num_batches = seriesLength // batchSize // backpropagationLength<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">generate data<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">generateData</span>():<br class="calibre2"/>    x = np.array(np.random.choice(<span class="calibre5">2</span><span class="calibre5">, </span>seriesLength<span class="calibre5">, </span><span class="calibre5">p</span>=[<span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">0.5</span>]))<br class="calibre2"/>    y = np.roll(x<span class="calibre5">, </span>echoStep)<br class="calibre2"/>    y[<span class="calibre5">0</span>:echoStep] = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">    </span>x = x.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/>    y = y.reshape((batchSize<span class="calibre5">, </span>-<span class="calibre5">1</span>))<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">return </span>(x<span class="calibre5">, </span>y)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">start computational graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span>batchXHolder = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"x_input"</span>)<br class="calibre2"/>batchYHolder = tf.placeholder(tf.int32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>backpropagationLength]<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"y_input"</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># rnn replace<br class="calibre2"/></span><span class="calibre5">#initState = tf.placeholder(tf.float32, [batchSize, stateSize], "rnn_init_state")<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span>cellState = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>stateSize])<br class="calibre2"/>hiddenState = tf.placeholder(tf.float32<span class="calibre5">, </span>[batchSize<span class="calibre5">, </span>stateSize])<br class="calibre2"/>initState = rnn.LSTMStateTuple(cellState<span class="calibre5">, </span>hiddenState)<br class="calibre2"/><br class="calibre2"/>W = tf.Variable(np.random.rand(stateSize+<span class="calibre5">1</span><span class="calibre5">, </span>stateSize)<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight1"</span>)<br class="calibre2"/>bias1 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>stateSize))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>W2 = tf.Variable(np.random.rand(stateSize<span class="calibre5">, </span>numClasses)<span class="calibre5">,</span><span class="calibre5">dtype</span>=tf.float32<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"weight2"</span>)<br class="calibre2"/>bias2 = tf.Variable(np.zeros((<span class="calibre5">1</span><span class="calibre5">,</span>numClasses))<span class="calibre5">, </span><span class="calibre5">dtype</span>=tf.float32)<br class="calibre2"/><br class="calibre2"/>tf.summary.histogram(<span class="calibre5">name</span>=<span class="calibre5">"weights"</span><span class="calibre5">, </span><span class="calibre5">values</span>=W)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Unpack columns<br class="calibre2"/></span>inputsSeries = tf.split(<span class="calibre5">axis</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">num_or_size_splits</span>=backpropagationLength<span class="calibre5">, </span><span class="calibre5">value</span>=batchXHolder)<br class="calibre2"/>labelsSeries = tf.unstack(batchYHolder<span class="calibre5">, </span><span class="calibre5">axis</span>=<span class="calibre5">1</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># Forward passes<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5"># rnn replace<br class="calibre2"/></span><span class="calibre5"># cell = rnn.BasicRNNCell(stateSize)<br class="calibre2"/></span><span class="calibre5"># statesSeries, currentState = rnn.static_rnn(cell, inputsSeries, initState)<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span>cell = rnn.BasicLSTMCell(stateSize<span class="calibre5">, </span><span class="calibre5">state_is_tuple</span>=<span class="calibre5">True</span>)<br class="calibre2"/>statesSeries<span class="calibre5">, </span><span class="calibre5">currentState</span> = rnn.static_rnn(cell<span class="calibre5">, </span>inputsSeries<span class="calibre5">, </span>initState)<br class="calibre2"/><br class="calibre2"/><span class="calibre5"># calculate loss<br class="calibre2"/></span>logits_series = [tf.matmul(state<span class="calibre5">, </span>W2) + bias2 <span class="calibre5">for </span>state <span class="calibre5">in </span>statesSeries]<br class="calibre2"/>predictions_series = [tf.nn.softmax(logits) <span class="calibre5">for </span>logits <span class="calibre5">in </span>logits_series]<br class="calibre2"/><br class="calibre2"/>losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(<span class="calibre5">labels</span>=labels<span class="calibre5">, </span><span class="calibre5">logits</span>=logits) <span class="calibre5">for </span>logits<span class="calibre5">, </span>labels <span class="calibre5">in </span><span class="calibre5">zip</span>(logits_series<span class="calibre5">,</span>labelsSeries)]<br class="calibre2"/>total_loss = tf.reduce_mean(losses<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"total_loss"</span>)<br class="calibre2"/><br class="calibre2"/>train_step = tf.train.AdagradOptimizer(<span class="calibre5">0.3</span>).minimize(total_loss<span class="calibre5">, </span><span class="calibre5">name</span>=<span class="calibre5">"training"</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">plot computation<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">def </span><span class="calibre5">plot</span>(loss_list<span class="calibre5">, </span>predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY):<br class="calibre2"/>    plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span><span class="calibre5">1</span>)<br class="calibre2"/>    plt.cla()<br class="calibre2"/>    plt.plot(loss_list)<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>batchSeriesIdx <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">5</span>):<br class="calibre2"/>        oneHotOutputSeries = np.array(predictions_series)[:<span class="calibre5">, </span>batchSeriesIdx<span class="calibre5">, </span>:]<br class="calibre2"/>        singleOutputSeries = np.array([(<span class="calibre5">1 </span><span class="calibre5">if </span>out[<span class="calibre5">0</span>] &lt; <span class="calibre5">0.5 </span><span class="calibre5">else </span><span class="calibre5">0</span>) <span class="calibre5">for </span>out <span class="calibre5">in </span>oneHotOutputSeries])<br class="calibre2"/><br class="calibre2"/>        plt.subplot(<span class="calibre5">2</span><span class="calibre5">, </span><span class="calibre5">3</span><span class="calibre5">, </span>batchSeriesIdx + <span class="calibre5">2</span>)<br class="calibre2"/>        plt.cla()<br class="calibre2"/>        plt.axis([<span class="calibre5">0</span><span class="calibre5">, </span>backpropagationLength<span class="calibre5">, </span><span class="calibre5">0</span><span class="calibre5">, </span><span class="calibre5">2</span>])<br class="calibre2"/>        left_offset = <span class="calibre5">range</span>(backpropagationLength)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchX[batchSeriesIdx<span class="calibre5">, </span>:]<span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"blue"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>batchY[batchSeriesIdx<span class="calibre5">, </span>:] * <span class="calibre5">0.5</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"red"</span>)<br class="calibre2"/>        plt.bar(left_offset<span class="calibre5">, </span>singleOutputSeries * <span class="calibre5">0.3</span><span class="calibre5">, </span><span class="calibre5">width</span>=<span class="calibre5">1</span><span class="calibre5">, </span><span class="calibre5">color</span>=<span class="calibre5">"green"</span>)<br class="calibre2"/><br class="calibre2"/>    plt.draw()<br class="calibre2"/>    plt.pause(<span class="calibre5">0.0001</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">run the graph<br class="calibre2"/></span><span class="calibre5">"""<br class="calibre2"/></span><span class="calibre5">with </span>tf.Session() <span class="calibre5">as </span>sess:<br class="calibre2"/>    writer = tf.summary.FileWriter(<span class="calibre5">"logs"</span><span class="calibre5">, </span><span class="calibre5">graph</span>=tf.get_default_graph())<br class="calibre2"/>    sess.run(tf.global_variables_initializer())<br class="calibre2"/>    plt.ion()<br class="calibre2"/>    plt.figure()<br class="calibre2"/>    plt.show()<br class="calibre2"/>    loss_list = []<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>epoch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(numEpochs):<br class="calibre2"/>        x<span class="calibre5">,</span>y = generateData()<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5"># rnn remove<br class="calibre2"/></span><span class="calibre5">        # _current_state = np.zeros((batchSize, stateSize))<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">        </span>_current_cell_state = np.zeros((batchSize<span class="calibre5">, </span>stateSize))<br class="calibre2"/>        _current_hidden_state = np.zeros((batchSize<span class="calibre5">, </span>stateSize))<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">print</span>(<span class="calibre5">"New data, epoch"</span><span class="calibre5">, </span>epoch_idx)<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">for </span>batch_idx <span class="calibre5">in </span><span class="calibre5">range</span>(num_batches):<br class="calibre2"/>            start_idx = batch_idx * backpropagationLength<br class="calibre2"/>            end_idx = start_idx + backpropagationLength<br class="calibre2"/><br class="calibre2"/>            batchX = x[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/>            batchY = y[:<span class="calibre5">,</span>start_idx:end_idx]<br class="calibre2"/><br class="calibre2"/>            _total_loss<span class="calibre5">, </span>_train_step<span class="calibre5">, </span>_current_state<span class="calibre5">, </span>_predictions_series = sess.run(<br class="calibre2"/>                [total_loss<span class="calibre5">, </span>train_step<span class="calibre5">, </span><span class="calibre5">currentState</span><span class="calibre5">, </span>predictions_series]<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                </span><span class="calibre5">feed_dict</span>={<br class="calibre2"/>                    batchXHolder:batchX<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>batchYHolder:batchY<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>cellState: _current_cell_state<span class="calibre5">,<br class="calibre2"/></span><span class="calibre5">                    </span>hiddenState: _current_hidden_state<br class="calibre2"/>                })<br class="calibre2"/><br class="calibre2"/>            _current_cell_state<span class="calibre5">, </span>_current_hidden_state = _current_state<br class="calibre2"/><br class="calibre2"/>            loss_list.append(_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5"># fix the cost summary later<br class="calibre2"/></span><span class="calibre5">            </span>tf.summary.scalar(<span class="calibre5">name</span>=<span class="calibre5">"totalloss"</span><span class="calibre5">, </span><span class="calibre5">tensor</span>=_total_loss)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>batch_idx%<span class="calibre5">100 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Step"</span><span class="calibre5">,</span>batch_idx<span class="calibre5">, </span><span class="calibre5">"Loss"</span><span class="calibre5">, </span>_total_loss)<br class="calibre2"/>                plot(loss_list<span class="calibre5">, </span>_predictions_series<span class="calibre5">, </span>batchX<span class="calibre5">, </span>batchY)<br class="calibre2"/><br class="calibre2"/>plt.ioff()<br class="calibre2"/>plt.show()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Computational graph</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The following computational graph from TensorBoard describes the working of the LSTM network:</p>
<div class="mce-root"><img src="Images/fe1361fa-f533-4aee-8456-c7543d327f24.png" class="calibre150"/></div>
<p class="calibre4">The output of the listing is shown as follows:</p>
<pre class="calibre26">New data, epoch 0<br class="calibre2"/>Step 0 Loss 0.696803<br class="calibre2"/>Step 600 Loss 0.00743465<br class="calibre2"/>New data, epoch 1<br class="calibre2"/>Step 0 Loss 0.404039<br class="calibre2"/>Step 600 Loss 0.00243205<br class="calibre2"/>New data, epoch 2<br class="calibre2"/>Step 0 Loss 1.11536<br class="calibre2"/>Step 600 Loss 0.00140995<br class="calibre2"/>New data, epoch 3<br class="calibre2"/>Step 0 Loss 0.858743<br class="calibre2"/>Step 600 Loss 0.00141037</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Sentiment analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">We will now write an app to predict sentiments of a movie review. Reviews are made up of a sequence of words and the order of words encodes very useful information to predict sentiment. The first step is to map words to word embeddings. The second step is the RNN that receives a sequence of vectors as input and considers the order of the vectors to generate the prediction.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Word embeddings</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">We will now train a neural network for word to vector representation. Given a particular word in the center of a sentence, which is the input word, we look at the words nearby. The network is going to tell us the probability for every word in our vocabulary of being the nearby word that we choose.</p>
<div class="mce-root"><img src="Images/11250af4-f53c-46cb-bf1e-3db2dd354b11.png" class="calibre151"/></div>
<pre class="calibre26"><span class="calibre5">import </span>time<br class="calibre2"/><span class="calibre5">import </span>tensorflow <span class="calibre5">as </span>tf<br class="calibre2"/><span class="calibre5">import </span>numpy <span class="calibre5">as </span>np<br class="calibre2"/><span class="calibre5">import </span>utility<br class="calibre2"/><span class="calibre5">from </span>tqdm <span class="calibre5">import </span>tqdm<br class="calibre2"/><span class="calibre5">from </span>urllib.request <span class="calibre5">import </span>urlretrieve<br class="calibre2"/><span class="calibre5">from </span>os.path <span class="calibre5">import </span>isfile, isdir<br class="calibre2"/><span class="calibre5">import </span>zipfile<br class="calibre2"/><span class="calibre5">from </span>collections <span class="calibre5">import </span>Counter<br class="calibre2"/><span class="calibre5">import </span>random<br class="calibre2"/><br class="calibre2"/>dataDir = <span class="calibre5">'data'<br class="calibre2"/></span>dataFile = <span class="calibre5">'text8.zip'<br class="calibre2"/></span>datasetName = <span class="calibre5">'text 8 data set'<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">track progress of file download<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">class </span>DownloadProgress(tqdm):<br class="calibre2"/>    lastBlock = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">    </span><span class="calibre5">def </span>hook(<span class="calibre5">self</span>, blockNum=<span class="calibre5">1</span>, blockSize=<span class="calibre5">1</span>, totalSize=<span class="calibre5">None</span>):<br class="calibre2"/>        <span class="calibre5">self</span>.total = totalSize<br class="calibre2"/>        <span class="calibre5">self</span>.update((blockNum - <span class="calibre5">self</span>.lastBlock) * blockSize)<br class="calibre2"/>        <span class="calibre5">self</span>.lastBlock = blockNum<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">if not </span>isfile(dataFile):<br class="calibre2"/>    <span class="calibre5">with </span>DownloadProgress(<span class="calibre5">unit</span>=<span class="calibre5">'B'</span>, <span class="calibre5">unit_scale</span>=<span class="calibre5">True</span>, <span class="calibre5">miniters</span>=<span class="calibre5">1</span>, <span class="calibre5">desc</span>=datasetName) <span class="calibre5">as </span>progressBar:<br class="calibre2"/>        urlretrieve(<span class="calibre5">'http://mattmahoney.net/dc/text8.zip'</span>, dataFile, progressBar.hook)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">if not </span>isdir(dataDir):<br class="calibre2"/>    <span class="calibre5">with </span>zipfile.ZipFile(dataFile) <span class="calibre5">as </span>zipRef:<br class="calibre2"/>        zipRef.extractall(dataDir)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span><span class="calibre5">open</span>(<span class="calibre5">'data/text8'</span>) <span class="calibre5">as </span>f:<br class="calibre2"/>    text = f.read()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">pre process the downloaded wiki text<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>words = utility.preProcess(text)<br class="calibre2"/><span class="calibre5">print</span>(words[:<span class="calibre5">30</span>])<br class="calibre2"/><br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">'Total words: {}'</span>.format(<span class="calibre5">len</span>(words)))<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">'Unique words: {}'</span>.format(<span class="calibre5">len</span>(<span class="calibre5">set</span>(words))))<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">convert words to integers<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>int2vocab, vocab2int = utility.lookupTable(words)<br class="calibre2"/>intWords = [vocab2int[word] <span class="calibre5">for </span>word <span class="calibre5">in </span>words]<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">'test'</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">sub sampling (***think of words as int's***)<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>threshold = <span class="calibre5">1e-5<br class="calibre2"/></span>wordCounts = Counter(intWords)<br class="calibre2"/>totalCount = <span class="calibre5">len</span>(intWords)<br class="calibre2"/>frequency = {word: count / totalCount <span class="calibre5">for </span>word, count <span class="calibre5">in </span>wordCounts.items()}<br class="calibre2"/>probOfWords = {word: <span class="calibre5">1 </span>- np.sqrt(threshold / frequency[word]) <span class="calibre5">for </span>word <span class="calibre5">in </span>wordCounts}<br class="calibre2"/>trainWords = [word <span class="calibre5">for </span>word <span class="calibre5">in </span>intWords <span class="calibre5">if </span>random.random() &lt; (<span class="calibre5">1 </span>- probOfWords[word])]<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">get window batches<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">def </span>getTarget(words, index, windowSize=<span class="calibre5">5</span>):<br class="calibre2"/>    rNum = np.random.randint(<span class="calibre5">1</span>, windowSize + <span class="calibre5">1</span>)<br class="calibre2"/>    start = index - rNum <span class="calibre5">if </span>(index - rNum) &gt; <span class="calibre5">0 </span><span class="calibre5">else </span><span class="calibre5">0<br class="calibre2"/></span><span class="calibre5">    </span>stop = index + rNum<br class="calibre2"/>    targetWords = <span class="calibre5">set</span>(words[start:index] + words[index + <span class="calibre5">1</span>:stop + <span class="calibre5">1</span>])<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">return </span><span class="calibre5">list</span>(targetWords)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">Create a generator of word batches as a tuple (inputs, targets)<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">def </span>getBatches(words, batchSize, windowSize=<span class="calibre5">5</span>):<br class="calibre2"/>    nBatches = <span class="calibre5">len</span>(words) // batchSize<br class="calibre2"/>    <span class="calibre5">print</span>(<span class="calibre5">'no. of batches {}'</span>.format(nBatches))<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5"># only full batches<br class="calibre2"/></span><span class="calibre5">    </span>words = words[:nBatches * batchSize]<br class="calibre2"/><br class="calibre2"/>    start = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5">    </span><span class="calibre5">for </span>index <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">0</span>, <span class="calibre5">len</span>(words), batchSize):<br class="calibre2"/>        x = []<br class="calibre2"/>        y = []<br class="calibre2"/>        stop = start + batchSize<br class="calibre2"/>        batchWords = words[start:stop]<br class="calibre2"/>        <span class="calibre5">for </span>idx <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">0</span>, <span class="calibre5">len</span>(batchWords), <span class="calibre5">1</span>):<br class="calibre2"/>            yBatch = getTarget(batchWords, idx, windowSize)<br class="calibre2"/>            y.extend(yBatch)<br class="calibre2"/>            x.extend([batchWords[idx]] * <span class="calibre5">len</span>(yBatch))<br class="calibre2"/>        start = stop + <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5">        </span><span class="calibre5">yield </span>x, y<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">start computational graph<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>train_graph = tf.Graph()<br class="calibre2"/><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    netInputs = tf.placeholder(tf.int32, [<span class="calibre5">None</span>], <span class="calibre5">name</span>=<span class="calibre5">'inputS'</span>)<br class="calibre2"/>    netLabels = tf.placeholder(tf.int32, [<span class="calibre5">None</span>, <span class="calibre5">None</span>], <span class="calibre5">name</span>=<span class="calibre5">'labelS'</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">create embedding layer<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>nVocab = <span class="calibre5">len</span>(int2vocab)<br class="calibre2"/>nEmbedding = <span class="calibre5">300<br class="calibre2"/></span><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    embedding = tf.Variable(tf.random_uniform((nVocab, nEmbedding), -<span class="calibre5">1</span>, <span class="calibre5">1</span>))<br class="calibre2"/>    embed = tf.nn.embedding_lookup(embedding, netInputs)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">Below, create weights and biases for the softmax layer. Then, use tf.nn.sampled_softmax_loss to calculate the loss<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>n_sampled = <span class="calibre5">100<br class="calibre2"/></span><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    soft_W = tf.Variable(tf.truncated_normal((nVocab, nEmbedding)))<br class="calibre2"/>    soft_b = tf.Variable(tf.zeros(nVocab), <span class="calibre5">name</span>=<span class="calibre5">"softmax_bias"</span>)<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5"># Calculate the loss using negative sampling<br class="calibre2"/></span><span class="calibre5">    </span>loss = tf.nn.sampled_softmax_loss(<br class="calibre2"/>        <span class="calibre5">weights</span>=soft_W,<br class="calibre2"/>        <span class="calibre5">biases</span>=soft_b,<br class="calibre2"/>        <span class="calibre5">labels</span>=netLabels,<br class="calibre2"/>        <span class="calibre5">inputs</span>=embed,<br class="calibre2"/>        <span class="calibre5">num_sampled</span>=n_sampled,<br class="calibre2"/>        <span class="calibre5">num_classes</span>=nVocab)<br class="calibre2"/><br class="calibre2"/>    cost = tf.reduce_mean(loss)<br class="calibre2"/>    optimizer = tf.train.AdamOptimizer().minimize(cost)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span>Here we're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them. <br class="calibre2"/>It's a nice way to check that our embedding table is grouping together words with similar semantic meanings.<span class="calibre5"><br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    validSize = <span class="calibre5">16<br class="calibre2"/></span><span class="calibre5">    </span>validWindow = <span class="calibre5">100<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">    </span>validExamples = np.array(random.sample(<span class="calibre5">range</span>(validWindow), validSize // <span class="calibre5">2</span>))<br class="calibre2"/>    validExamples = np.append(validExamples,<br class="calibre2"/>                               random.sample(<span class="calibre5">range</span>(<span class="calibre5">1000</span>, <span class="calibre5">1000 </span>+ validWindow), validSize // <span class="calibre5">2</span>))<br class="calibre2"/><br class="calibre2"/>    validDataset = tf.constant(validExamples, <span class="calibre5">dtype</span>=tf.int32)<br class="calibre2"/><br class="calibre2"/>    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), <span class="calibre5">1</span>, <span class="calibre5">keep_dims</span>=<span class="calibre5">True</span>))<br class="calibre2"/>    normalizedEmbedding = embedding / norm<br class="calibre2"/>    valid_embedding = tf.nn.embedding_lookup(normalizedEmbedding, validDataset)<br class="calibre2"/>    similarity = tf.matmul(valid_embedding, tf.transpose(normalizedEmbedding))<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">Train the network. Every 100 batches it reports the training loss. Every 1000 batches, it'll print out the validation<br class="calibre2"/></span><span class="calibre5">words.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>epochs = <span class="calibre5">10<br class="calibre2"/></span>batch_size = <span class="calibre5">1000<br class="calibre2"/></span>window_size = <span class="calibre5">10<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    saver = tf.train.Saver()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>tf.Session(<span class="calibre5">graph</span>=train_graph) <span class="calibre5">as </span>sess:<br class="calibre2"/>    iteration = <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5">    </span>loss = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5">    </span>sess.run(tf.global_variables_initializer())<br class="calibre2"/><br class="calibre2"/>    <span class="calibre5">for </span>e <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">1</span>, epochs + <span class="calibre5">1</span>):<br class="calibre2"/>        batches = getBatches(trainWords, batch_size, window_size)<br class="calibre2"/>        start = time.time()<br class="calibre2"/>        <span class="calibre5">for </span>x, y <span class="calibre5">in </span>batches:<br class="calibre2"/><br class="calibre2"/>            feed = {netInputs: x,<br class="calibre2"/>                    netLabels: np.array(y)[:, <span class="calibre5">None</span>]}<br class="calibre2"/>            trainLoss, _ = sess.run([cost, optimizer], <span class="calibre5">feed_dict</span>=feed)<br class="calibre2"/><br class="calibre2"/>            loss += trainLoss<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>iteration % <span class="calibre5">100 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                end = time.time()<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Epoch {}/{}"</span>.format(e, epochs),<br class="calibre2"/>                      <span class="calibre5">"Iteration: {}"</span>.format(iteration),<br class="calibre2"/>                      <span class="calibre5">"Avg. Training loss: {:.4f}"</span>.format(loss / <span class="calibre5">100</span>),<br class="calibre2"/>                      <span class="calibre5">"{:.4f} sec/batch"</span>.format((end - start) / <span class="calibre5">100</span>))<br class="calibre2"/>                loss = <span class="calibre5">0<br class="calibre2"/></span><span class="calibre5">                </span>start = time.time()<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>iteration % <span class="calibre5">1000 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                sim = similarity.eval()<br class="calibre2"/>                <span class="calibre5">for </span>i <span class="calibre5">in </span><span class="calibre5">range</span>(validSize):<br class="calibre2"/>                    validWord = int2vocab[validExamples[i]]<br class="calibre2"/>                    topK = <span class="calibre5">8<br class="calibre2"/></span><span class="calibre5">                    </span>nearest = (-sim[i, :]).argsort()[<span class="calibre5">1</span>:topK + <span class="calibre5">1</span>]<br class="calibre2"/>                    log = <span class="calibre5">'Nearest to %s:' </span>% validWord<br class="calibre2"/>                    <span class="calibre5">for </span>k <span class="calibre5">in </span><span class="calibre5">range</span>(topK):<br class="calibre2"/>                        closeWord = int2vocab[nearest[k]]<br class="calibre2"/>                        logStatement = <span class="calibre5">'%s %s,' </span>% (log, closeWord)<br class="calibre2"/>                    <span class="calibre5">print</span>(logStatement)<br class="calibre2"/><br class="calibre2"/>            iteration += <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5">    </span>save_path = saver.save(sess, <span class="calibre5">"checkpoints/text8.ckpt"</span>)<br class="calibre2"/>    embed_mat = sess.run(normalizedEmbedding)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">Restore the trained network if you need to<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>train_graph.as_default():<br class="calibre2"/>    saver = tf.train.Saver()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>tf.Session(<span class="calibre5">graph</span>=train_graph) <span class="calibre5">as </span>sess:<br class="calibre2"/>    saver.restore(sess, tf.train.latest_checkpoint(<span class="calibre5">'checkpoints'</span>))<br class="calibre2"/>    embed_mat = sess.run(embedding)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project <br class="calibre2"/></span><span class="calibre5">these vectors into two dimensions while preserving local structure. <br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">import </span>matplotlib.pyplot <span class="calibre5">as </span>plt<br class="calibre2"/><span class="calibre5">from </span>sklearn.manifold <span class="calibre5">import </span>TSNE<br class="calibre2"/>vizWords = <span class="calibre5">500<br class="calibre2"/></span>tsne = TSNE()<br class="calibre2"/>embedTSNE = tsne.fit_transform(embed_mat[:vizWords, :])<br class="calibre2"/><br class="calibre2"/>fig, ax = plt.subplots(<span class="calibre5">figsize</span>=(<span class="calibre5">14</span>, <span class="calibre5">14</span>))<br class="calibre2"/><span class="calibre5">for </span>idx <span class="calibre5">in </span><span class="calibre5">range</span>(vizWords):<br class="calibre2"/>    plt.scatter(*embedTSNE[idx, :], <span class="calibre5">color</span>=<span class="calibre5">'steelblue'</span>)<br class="calibre2"/>    plt.annotate(int2vocab[idx], (embedTSNE[idx, <span class="calibre5">0</span>], embedTSNE[idx, <span class="calibre5">1</span>]), <span class="calibre5">alpha</span>=<span class="calibre5">0.7</span>)</pre>
<p class="calibre4">The output of the listing is as follows:</p>
<pre class="calibre26">Total words: 16680599<br class="calibre2"/> Unique words: 63641<br class="calibre2"/> no. of batches 4626<br class="calibre2"/>Epoch 1/10 Iteration: 100 Avg. Training loss: 21.7284 0.3363 sec/batch<br class="calibre2"/> Epoch 1/10 Iteration: 1000 Avg. Training loss: 20.2269 0.3668 sec/batch<br class="calibre2"/><br class="calibre2"/>Nearest to but: universities, hungry, kyu, grandiose, edema, patty, stores, psychometrics,<br class="calibre2"/> Nearest to three: sulla, monuc, conjuring, ontological, auf, grimoire, unpredictably, frenetic,<br class="calibre2"/> <br class="calibre2"/>Nearest to world: turkle, spectroscopic, jules, servicio, sportswriter, kamikazes, act, earns,<br class="calibre2"/>Epoch 1/10 Iteration: 1100 Avg. Training loss: 20.1983 0.3650 sec/batch<br class="calibre2"/> Epoch 1/10 Iteration: 2000 Avg. Training loss: 19.1581 0.3767 sec/batch<br class="calibre2"/><br class="calibre2"/>Nearest to but: universities, hungry, edema, kyu, grandiose, stores, patty, psychometrics,<br class="calibre2"/> Nearest to three: monuc, sulla, unpredictably, grimoire, hickey, ontological, conjuring, rays,<br class="calibre2"/> Nearest to world: turkle, spectroscopic, jules, sportswriter, kamikazes, alfons, servicio, act,<br class="calibre2"/> ...... </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Sentiment analysis with an RNN</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The following example shows the implementation of sentiment analysis using an RNN. It has fixed-length movie reviews encoded as integer values, which are then converted to word embedding (embedding vectors) passed to LSTM layers in a recurrent manner that pick the last prediction as the output sentiment:</p>
<pre class="calibre26"><span class="calibre5">import </span>numpy <span class="calibre5">as </span>np<br class="calibre2"/><span class="calibre5">import </span>tensorflow <span class="calibre5">as </span>tf<br class="calibre2"/><span class="calibre5">from </span>string <span class="calibre5">import </span>punctuation<br class="calibre2"/><span class="calibre5">from </span>collections <span class="calibre5">import </span>Counter<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">movie review dataset for sentiment analysis<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span><span class="calibre5">open</span>(<span class="calibre5">'data/reviews.txt'</span>, <span class="calibre5">'r'</span>) <span class="calibre5">as </span>f:<br class="calibre2"/>    movieReviews = f.read()<br class="calibre2"/><span class="calibre5">with </span><span class="calibre5">open</span>(<span class="calibre5">'data/labels.txt'</span>, <span class="calibre5">'r'</span>) <span class="calibre5">as </span>f:<br class="calibre2"/>    labels = f.read()<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">data cleansing - remove punctuations<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>text = <span class="calibre5">''</span>.join([c <span class="calibre5">for </span>c <span class="calibre5">in </span>movieReviews <span class="calibre5">if </span>c <span class="calibre5">not in </span>punctuation])<br class="calibre2"/>movieReviews = text.split(<span class="calibre5">'</span><span class="calibre5">\n</span><span class="calibre5">'</span>)<br class="calibre2"/><br class="calibre2"/>text = <span class="calibre5">' '</span>.join(movieReviews)<br class="calibre2"/>words = text.split()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">print</span>(text[:<span class="calibre5">500</span>])<br class="calibre2"/><span class="calibre5">print</span>(words[:<span class="calibre5">100</span>])<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">build a dictionary that maps words to integers<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>counts = Counter(words)<br class="calibre2"/>vocabulary = <span class="calibre5">sorted</span>(counts, <span class="calibre5">key</span>=counts.get, <span class="calibre5">reverse</span>=<span class="calibre5">True</span>)<br class="calibre2"/>vocab2int = {word: i <span class="calibre5">for </span>i, word <span class="calibre5">in </span><span class="calibre5">enumerate</span>(vocabulary, <span class="calibre5">1</span>)}<br class="calibre2"/><br class="calibre2"/>reviewsInts = []<br class="calibre2"/><span class="calibre5">for </span>review <span class="calibre5">in </span>movieReviews:<br class="calibre2"/>    reviewsInts.append([vocab2int[word] <span class="calibre5">for </span>word <span class="calibre5">in </span>review.split()])<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">convert labels from positive and negative to 1 and 0 respectively<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>labels = labels.split(<span class="calibre5">'</span><span class="calibre5">\n</span><span class="calibre5">'</span>)<br class="calibre2"/>labels = np.array([<span class="calibre5">1 </span><span class="calibre5">if </span>label == <span class="calibre5">'positive' </span><span class="calibre5">else </span><span class="calibre5">0 </span><span class="calibre5">for </span>label <span class="calibre5">in </span>labels])<br class="calibre2"/><br class="calibre2"/>reviewLengths = Counter([<span class="calibre5">len</span>(x) <span class="calibre5">for </span>x <span class="calibre5">in </span>reviewsInts])<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">"Min review length are: {}"</span>.format(reviewLengths[<span class="calibre5">0</span>]))<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">"Maximum review length are: {}"</span>.format(<span class="calibre5">max</span>(reviewLengths)))<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">remove the review with zero length from the reviewsInts list<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>nonZeroIndex = [i <span class="calibre5">for </span>i, review <span class="calibre5">in </span><span class="calibre5">enumerate</span>(reviewsInts) <span class="calibre5">if </span><span class="calibre5">len</span>(review) != <span class="calibre5">0</span>]<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">len</span>(nonZeroIndex))<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">turns out its the final review that has zero length. But that might not always be the case, so let's make it more<br class="calibre2"/></span><span class="calibre5">general.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>reviewsInts = [reviewsInts[i] <span class="calibre5">for </span>i <span class="calibre5">in </span>nonZeroIndex]<br class="calibre2"/>labels = np.array([labels[i] <span class="calibre5">for </span>i <span class="calibre5">in </span>nonZeroIndex])<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">create an array features that contains the data we'll pass to the network. The data should come from reviewInts, since<br class="calibre2"/></span><span class="calibre5">we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, <br class="calibre2"/></span><span class="calibre5">left pad with 0s. That is, if the review is ['best', 'movie', 'renaira'], [100, 40, 20] as integers, the row will look <br class="calibre2"/></span><span class="calibre5">like [0, 0, 0, ..., 0, 100, 40, 20]. For reviews longer than 200, use on the first 200 words as the feature vector.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>seqLen = <span class="calibre5">200<br class="calibre2"/></span>features = np.zeros((<span class="calibre5">len</span>(reviewsInts), seqLen), <span class="calibre5">dtype</span>=<span class="calibre5">int</span>)<br class="calibre2"/><span class="calibre5">for </span>i, row <span class="calibre5">in </span><span class="calibre5">enumerate</span>(reviewsInts):<br class="calibre2"/>    features[i, -<span class="calibre5">len</span>(row):] = np.array(row)[:seqLen]<br class="calibre2"/><br class="calibre2"/><span class="calibre5">print</span>(features[:<span class="calibre5">10</span>,:<span class="calibre5">100</span>])<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">lets create training, validation and test data sets. trainX and trainY for example. <br class="calibre2"/></span><span class="calibre5">also define a split percentage function 'splitPerc' as the percentage of data to keep in the training <br class="calibre2"/></span><span class="calibre5">set. usually this is 0.8 or 0.9.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>splitPrec = <span class="calibre5">0.8<br class="calibre2"/></span>splitIndex = <span class="calibre5">int</span>(<span class="calibre5">len</span>(features)*<span class="calibre5">0.8</span>)<br class="calibre2"/>trainX, valX = features[:splitIndex], features[splitIndex:]<br class="calibre2"/>trainY, valY = labels[:splitIndex], labels[splitIndex:]<br class="calibre2"/><br class="calibre2"/>testIndex = <span class="calibre5">int</span>(<span class="calibre5">len</span>(valX)*<span class="calibre5">0.5</span>)<br class="calibre2"/>valX, testX = valX[:testIndex], valX[testIndex:]<br class="calibre2"/>valY, testY = valY[:testIndex], valY[testIndex:]<br class="calibre2"/><br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">"Train set: {}"</span>.format(trainX.shape), <span class="calibre5">"</span><span class="calibre5">\n</span><span class="calibre5">Validation set: {}"</span>.format(valX.shape), <span class="calibre5">"</span><span class="calibre5">\n</span><span class="calibre5">Test set: {}"</span>.format(testX.shape))<br class="calibre2"/><span class="calibre5">print</span>(<span class="calibre5">"label set: {}"</span>.format(trainY.shape), <span class="calibre5">"</span><span class="calibre5">\n</span><span class="calibre5">Validation label set: {}"</span>.format(valY.shape), <span class="calibre5">"</span><span class="calibre5">\n</span><span class="calibre5">Test label set: {}"</span>.format(testY.shape))<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">tensor-flow computational graph<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>lstmSize = <span class="calibre5">256<br class="calibre2"/></span>lstmLayers = <span class="calibre5">1<br class="calibre2"/></span>batchSize = <span class="calibre5">500<br class="calibre2"/></span>learningRate = <span class="calibre5">0.001<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span>nWords = <span class="calibre5">len</span>(vocab2int) + <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5"># create graph object and add nodes to the graph<br class="calibre2"/></span>graph = tf.Graph()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    inputData = tf.placeholder(tf.int32, [<span class="calibre5">None</span>, <span class="calibre5">None</span>], <span class="calibre5">name</span>=<span class="calibre5">'inputData'</span>)<br class="calibre2"/>    labels = tf.placeholder(tf.int32, [<span class="calibre5">None</span>, <span class="calibre5">None</span>], <span class="calibre5">name</span>=<span class="calibre5">'labels'</span>)<br class="calibre2"/>    keepProb = tf.placeholder(tf.float32, <span class="calibre5">name</span>=<span class="calibre5">'keepProb'</span>)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">let us create the embedding layer (word2vec)<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5"># number of neurons in hidden or embedding layer<br class="calibre2"/></span>embedSize = <span class="calibre5">300<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    embedding = tf.Variable(tf.random_uniform((nWords, embedSize), -<span class="calibre5">1</span>, <span class="calibre5">1</span>))<br class="calibre2"/>    embed = tf.nn.embedding_lookup(embedding, inputData)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">lets use tf.contrib.rnn.BasicLSTMCell to create an LSTM cell, later add drop out to it with <br class="calibre2"/></span><span class="calibre5">tf.contrib.rnn.DropoutWrapper. and finally create multiple LSTM layers with tf.contrib.rnn.MultiRNNCell.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    <span class="calibre5">with </span>tf.name_scope(<span class="calibre5">"RNNLayers"</span>):<br class="calibre2"/>        <span class="calibre5">def </span>createLSTMCell():<br class="calibre2"/>            lstm = tf.contrib.rnn.BasicLSTMCell(lstmSize, <span class="calibre5">reuse</span>=tf.get_variable_scope().reuse)<br class="calibre2"/>            <span class="calibre5">return </span>tf.contrib.rnn.DropoutWrapper(lstm, <span class="calibre5">output_keep_prob</span>=keepProb)<br class="calibre2"/><br class="calibre2"/>        cell = tf.contrib.rnn.MultiRNNCell([createLSTMCell() <span class="calibre5">for </span>_ <span class="calibre5">in </span><span class="calibre5">range</span>(lstmLayers)])<br class="calibre2"/><br class="calibre2"/>        initialState = cell.zero_state(batchSize, tf.float32)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">set tf.nn.dynamic_rnn to add the forward pass through the RNN. here we're actually passing in vectors from the <br class="calibre2"/></span><span class="calibre5">embedding layer 'embed'.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    outputs, finalState = tf.nn.dynamic_rnn(cell, embed, <span class="calibre5">initial_state</span>=initialState)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">final output will carry the sentiment prediction, therefore lets get the last output with outputs[:, -1], <br class="calibre2"/></span><span class="calibre5">the we calculate the cost from that and labels.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    predictions = tf.contrib.layers.fully_connected(outputs[:, -<span class="calibre5">1</span>], <span class="calibre5">1</span>, <span class="calibre5">activation_fn</span>=tf.sigmoid)<br class="calibre2"/>    cost = tf.losses.mean_squared_error(labels, predictions)<br class="calibre2"/><br class="calibre2"/>    optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost)<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">now we can add a few nodes to calculate the accuracy which we'll use in the validation pass.<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    correctPred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)<br class="calibre2"/>    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">get batches<br class="calibre2"/></span><span class="calibre5">'''</span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">def </span>getBatches(x, y, batchSize=<span class="calibre5">100</span>):<br class="calibre2"/>    nBatches = <span class="calibre5">len</span>(x) // batchSize<br class="calibre2"/>    x, y = x[:nBatches * batchSize], y[:nBatches * batchSize]<br class="calibre2"/>    <span class="calibre5">for </span>i <span class="calibre5">in </span><span class="calibre5">range</span>(<span class="calibre5">0</span>, <span class="calibre5">len</span>(x), batchSize):<br class="calibre2"/>        <span class="calibre5">yield </span>x[i:i + batchSize], y[i:i + batchSize]<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">training phase<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>epochs = <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5"><br class="calibre2"/></span><span class="calibre5">with </span>graph.as_default():<br class="calibre2"/>    saver = tf.train.Saver()<br class="calibre2"/><br class="calibre2"/><span class="calibre5">with </span>tf.Session(<span class="calibre5">graph</span>=graph) <span class="calibre5">as </span>sess:<br class="calibre2"/>    writer = tf.summary.FileWriter(<span class="calibre5">"logs"</span>, <span class="calibre5">graph</span>=tf.get_default_graph())<br class="calibre2"/>    sess.run(tf.global_variables_initializer())<br class="calibre2"/>    iteration = <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5">    </span><span class="calibre5">for </span>e <span class="calibre5">in </span><span class="calibre5">range</span>(epochs):<br class="calibre2"/>        state = sess.run(initialState)<br class="calibre2"/><br class="calibre2"/>        <span class="calibre5">for </span>i, (x, y) <span class="calibre5">in </span><span class="calibre5">enumerate</span>(getBatches(trainX, trainY, batchSize), <span class="calibre5">1</span>):<br class="calibre2"/>            feed = {inputData: x, labels: y[:, <span class="calibre5">None</span>], keepProb: <span class="calibre5">0.5</span>, initialState: state}<br class="calibre2"/><br class="calibre2"/>            loss, state, _ = sess.run([cost, finalState, optimizer], <span class="calibre5">feed_dict</span>=feed)<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>iteration % <span class="calibre5">5 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Epoch are: {}/{}"</span>.format(e, epochs), <span class="calibre5">"Iteration is: {}"</span>.format(iteration), <span class="calibre5">"Train loss is: {:.3f}"</span>.format(loss))<br class="calibre2"/><br class="calibre2"/>            <span class="calibre5">if </span>iteration % <span class="calibre5">25 </span>== <span class="calibre5">0</span>:<br class="calibre2"/>                valAcc = []<br class="calibre2"/>                valState = sess.run(cell.zero_state(batchSize, tf.float32))<br class="calibre2"/>                <span class="calibre5">for </span>x, y <span class="calibre5">in </span>getBatches(valX, valY, batchSize):<br class="calibre2"/>                    feed = {inputData: x, labels: y[:, <span class="calibre5">None</span>], keepProb: <span class="calibre5">1</span>, initialState: valState}<br class="calibre2"/>                    batchAcc, valState = sess.run([accuracy, finalState], <span class="calibre5">feed_dict</span>=feed)<br class="calibre2"/>                    valAcc.append(batchAcc)<br class="calibre2"/>                <span class="calibre5">print</span>(<span class="calibre5">"Val acc: {:.3f}"</span>.format(np.mean(valAcc)))<br class="calibre2"/>            iteration += <span class="calibre5">1<br class="calibre2"/></span><span class="calibre5">            </span>saver.save(sess, <span class="calibre5">"checkpoints/sentimentanalysis.ckpt"</span>)<br class="calibre2"/>    saver.save(sess, <span class="calibre5">"checkpoints/sentimentanalysis.ckpt"</span>)<br class="calibre2"/><br class="calibre2"/><span class="calibre5">'''<br class="calibre2"/></span><span class="calibre5">testing phase<br class="calibre2"/></span><span class="calibre5">'''<br class="calibre2"/></span>testAcc = []<br class="calibre2"/><span class="calibre5">with </span>tf.Session(<span class="calibre5">graph</span>=graph) <span class="calibre5">as </span>sess:<br class="calibre2"/>    saver.restore(sess, <span class="calibre5">"checkpoints/sentiment.ckpt"</span>)<br class="calibre2"/><br class="calibre2"/>    testState = sess.run(cell.zero_state(batchSize, tf.float32))<br class="calibre2"/>    <span class="calibre5">for </span>i, (x, y) <span class="calibre5">in </span><span class="calibre5">enumerate</span>(getBatches(testY, testY, batchSize), <span class="calibre5">1</span>):<br class="calibre2"/>        feed = {inputData: x,<br class="calibre2"/>                labels: y[:, <span class="calibre5">None</span>],<br class="calibre2"/>                keepProb: <span class="calibre5">1</span>,<br class="calibre2"/>                initialState: testState}<br class="calibre2"/>        batchAcc, testState = sess.run([accuracy, finalState], <span class="calibre5">feed_dict</span>=feed)<br class="calibre2"/>        testAcc.append(batchAcc)<br class="calibre2"/>    <span class="calibre5">print</span>(<span class="calibre5">"Test accuracy is: {:.3f}"</span>.format(np.mean(testAcc)))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Computational graph</h1>
                </header>
            
            <article class="calibre2">
                
<div class="mce-root"><img src="Images/a365b445-98cc-4ede-b781-2a3fd9ea0efc.png" class="calibre152"/></div>
<p class="calibre4">The output of the listing is shown as follows:</p>
<pre class="calibre26">Train set: (20000, 200)<br class="calibre2"/> Validation set: (2500, 200)<br class="calibre2"/> Test set: (2500, 200)<br class="calibre2"/> label set: (20000,)<br class="calibre2"/> Validation label set: (2500,)<br class="calibre2"/> Test label set: (2500,)<br class="calibre2"/>Val acc: 0.682<br class="calibre2"/> Val acc: 0.692<br class="calibre2"/> Val acc: 0.714<br class="calibre2"/> Val acc: 0.808<br class="calibre2"/> Val acc: 0.763<br class="calibre2"/> Val acc: 0.826<br class="calibre2"/> Val acc: 0.854<br class="calibre2"/> Val acc: 0.872</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, you learned the basics of recurrent neural networks and why it is a useful mechanism for time series data processing. You learned about basic concepts such as states, word embeddings, and long-term memories. This was followed by an example to develop sentiment analysis system. We also implement recurrent neural networks using tensorflow.</p>
<p class="calibre4">In the next chapter, we look at a different kind of neural network called a <strong class="calibre7">Generative Model</strong>.</p>
<p class="calibre4"/>
<p class="calibre4"/>


            </article>

            
        </section>
    </div>



  </body></html>