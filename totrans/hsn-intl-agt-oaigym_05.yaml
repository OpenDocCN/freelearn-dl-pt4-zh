- en: Implementing your First Learning Agent - Solving the Mountain Car problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Well done on making it this far! In previous chapters, we got a good introduction
    to OpenAI Gym, its features, and how to install, configure, and use it in your
    own programs. We also discussed the basics of reinforcement learning and what
    deep reinforcement learning is, and we set up the PyTorch deep learning library
    to develop deep reinforcement learning applications. In this chapter, you will
    start developing your first learning agent! You will develop an intelligent agent
    that will learn how to solve the Mountain Car problem. Gradually in the following
    chapters, we will solve increasingly challenging problems as you get more comfortable
    developing reinforcement learning algorithms to solve problems in OpenAI Gym.
    We will start this chapter by understanding the Mountain Car problem, which has
    been a popular problem in the reinforcement learning and optimal control community.
    We will develop our learning agent from scratch and then train it to solve the
    Mountain Car problem using the Mountain Car environment in the Gym. We will finally
    see how the agent progresses and briefly look at ways we can improve the agent
    to use it for solving more complex problems. The topics we will be covering in
    this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Mountain Car problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a reinforcement learning-based agent to solve the Mountain Car
    problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a reinforcement learning agent at the Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the performance of the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Mountain Car problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any reinforcement learning problem, two fundamental definitions concerning
    the problem are important, irrespective of the learning algorithm we use. They
    are the definitions of the state space and the action space. We mentioned earlier
    in this book that the state and action spaces could be discrete or continuous.
    Typically, in most problems, the state space consists of continuous values and
    is represented as a vector, matrix, or tensor (a multi-dimensional matrix). Problems
    and environments with discrete action spaces are relatively easy compared to continuous
    valued problems and environments. In this book, we will develop learning algorithms
    for a few problems and environments with a mix of state space and action space
    combinations so that you are comfortable dealing with any such variation when
    you start out on your own and develop intelligent agents and algorithms for your
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by understanding the Mountain Car problem with a high-level description,
    before moving on to look at the state and action spaces of the Mountain Car environment.
  prefs: []
  type: TYPE_NORMAL
- en: The Mountain Car problem and environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Mountain Car Gym environment, a car is on a one-dimensional track, positioned
    between two mountains. The goal is to drive the car up the mountain on the right;
    however, the car's engine is not strong enough to drive up the mountain even at
    the maximum speed. Therefore, the only way to succeed is to drive back and forth
    to build up momentum. In short, the Mountain Car problem is to get an under-powered
    car to the top of a hill.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you implement your agent algorithm, it will help tremendously to understand
    the environment, the problem, and the state and action spaces. How do we find
    out the state and action spaces of the Mountain Car environment in the Gym? Well,
    we already know how to do that from [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*. We wrote a script named `get_observation_action_space.py`,
    which will print out the state and observation and action spaces of the environment
    whose name is passed as the first argument to the script. Let''s ask it to print
    the spaces for the `MountainCar-v0` environment with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the command prompt has the `rl_gym_book` prefix, which signifies that
    we have activated the `rl_gym_book` conda Python virtual environment. Also, the
    current directory, `~/rl_gym_book/ch4`, signifies that the script is run from
    the `ch4` directory corresponding to the code for [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12),
    *Exploring the Gym and its Features*, in the code repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command will produce output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From this output, we can see that the state and observation space is a two-dimensional
    box and the action space is three-dimensional and discrete.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a refresher on what **box **and **discrete **spaces mean, you can
    quickly flip to [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12),
    *Exploring the Gym and its Features*, where we discussed these spaces and what
    they mean under the *Spaces in the Gym*section. It is important to understand
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state and action space type, description, and range of allowed values are
    summarized in the following table for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **MountainCar-v0 environment** | **Type** | **Description** | **Range** |'
  prefs: []
  type: TYPE_TB
- en: '| State space | `Box(2,)` | (position, velocity) | Position: -1.2 to 0.6Velocity:
    -0.07 to 0.07 |'
  prefs: []
  type: TYPE_TB
- en: '| Action space | `Discrete(3)` | 0: Go left1: Coast/do-nothing2: Go right |
    0, 1, 2 |'
  prefs: []
  type: TYPE_TB
- en: So for example, the car starts at a random position between *-0.6* and *-0.4*
    with zero velocity, and the goal is to reach the top of the hill on the right
    side, which is at position *0.5*. (The car can technically go beyond *0.5,* up
    to *0.6*, which is also considered.) The environment will send *-1* as a reward
    every time step until the goal position (*0.5*) is reached. The environment will
    terminate the episode. The `done` variable will be equal to `True` if the car
    reaches the *0.5* position or the number of steps taken reaches 200.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Q-learning agent from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start implementing our intelligent agent step-by-step.
    We will be implementing the famous Q-learning algorithm using the `NumPy` library
    and the `MountainCar-V0` environment from the OpenAI Gym library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the reinforcement learning Gym boiler plate code we used in
    [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is a good starting point (aka boilerplate!) for developing our reinforcement
    learning agent. We will first start by changing the environment name from `Qbert-v0`
    to `MountainCar-v0`. Notice in the preceding script that we are setting `MAX_STEPS_PER_EPISODE`.
    This is the number of steps or actions that the agent can take before the episode
    ends. This may be useful in continuing, perpetual, or looping environments, where
    the environment itself does not end the episode. Here, we set a limit for the
    agent to avoid infinite loops. However, most of the environments defined in OpenAI
    Gym have an episode termination condition and once either of them is satisfied,
    the `done` variable returned by the `env.step(...)` function will be set to *True*.
    We saw in the previous section that for the Mountain Car problem we are interested
    in, the environment will terminate the episode if the car reaches the goal position
    (*0.5*) or if the number of steps taken reaches *200*. Therefore, we can further
    simplify the boilerplate code to look like the following for the Mountain Car
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the preceding script, you will see the Mountain Car environment
    come up in a new window and the car moving left and right randomly for 1,000 episodes.
    You will also see the episode number, steps taken, and the total reward obtained
    printed at the end of every episode, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The sample output should look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You should recall from our previous section that the agent gets a reward of
    *-1* for each step and that the `MountainCar-v0` environment will terminate the
    episode after *200* steps; this is why you the agent may sometimes get a total
    reward of *-200!* After all, the agent is taking random actions without thinking
    or learning from its previous actions. Ideally, we would want the agent to figure
    out how to reach the top of the mountain (near the flag, close to, at, or beyond
    position *0.5*) with the minimum number of steps. Don't worry - we will build
    such an intelligent agent by the end of this chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Remember to always activate the `rl_gym_book` conda environment before running
    the scripts! Otherwise, you might run into Module not found errors unnecessarily.
    You can visually confirm whether you have activated the environment by looking
    at the shell prefix, which will show something like this: `(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch5$`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on by having a look at what Q-learning section.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12), *Reinforcement
    Learning and Deep Reinforcement Learning*, we discussed the SARSA and Q-learning
    algorithms. Both of these algorithms provide a systematic way to update the estimate
    of the action-value function denoted by ![](img/00121.jpeg). In particular, we
    saw that Q-learning is an off-policy learning algorithm, which updates the action-value
    estimate of the current state and action towards the maximum obtainable action-value
    in the subsequent state, ![](img/00122.jpeg), which the agent will end up in according
    to its policy. We also saw that the Q-learning update is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will implement a `Q_Learner`class in Python, which implements this
    learning update rule along with the other necessary functions and methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Q-learning agent using Python and NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin implementing our Q-learning agent by implementing the `Q_Learner`
    class. The main methods of this class are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: __init__(self, env)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discretize(self, obs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: get_action(self, obs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learn(self, obs, action, reward, next_obs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will later find that the methods in here are common and exist in almost
    all the agents we will be implementing in this book. This makes it easy for you
    to grasp them, as these methods will be repeated (with some modifications) again
    and again.
  prefs: []
  type: TYPE_NORMAL
- en: The `discretize()` function is not necessary for agent implementations in general,
    but when the state space is large and continuous, it may be better to discretize
    the space into countable bins or ranges of values to simplify the representation.
    This also reduces the number of values that the Q-learning algorithm needs to
    learn, as it now only has to learn a finite set of values, which can be concisely
    represented in tabular formats or by using *n*-dimensional arrays instead of complex
    functions. Moreover, the Q-learning algorithm, used for optimal control, is guaranteed
    to converge for tabular representations of Q-values.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before our `Q_Learner` class declaration, we will initialize a few useful hyperparameters.
    Here are the hyperparameters that we will be using for our `Q_Learner` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EPSILON_MIN`: This is the minimum value of the epsilon value that we want
    the agent to use while following an epsilon-greedy policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_NUM_EPISODES`:The maximum number of episodes that we want the agent to
    interact with the environment for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STEPS_PER_EPISODE`: This is the number of steps in each episode. This could
    be the maximum number of steps that an environment will allow per episode or a
    custom value that we want to limit based on some time budget. Allowing a higher
    number of steps per episode means each episode might take longer to complete and
    in non-terminating environments, the environment won''t be reset until this limit
    is reached, even if the agent is stuck at the same spot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ALPHA`: This is the learning rate that we want the agent to use. This is the
    alpha in the Q-learning update equation listed in the previous section. Some algorithms
    vary the learning rate as the training progresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GAMMA`: This is the discount factor that the agent will use to factor in future
    rewards. This value corresponds to the gamma in the Q-learning update equation
    in the previous section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_DISCRETE_BINS`: This is the number of bins of values that the state space
    will be discretized into. For the Mountain Car environment, we will be discretizing
    the state space into *30* bins. You can play around with higher/lower values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the `MAX_NUM_EPISODES`and `STEPS_PER_EPISODE` have been defined in
    the boilerplate code we went through in one of the previous sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'These hyperparameters are defined in the Python code like this, with some initial
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the Q_Learner class's __init__ method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let us look into the `Q_Learner` class''s member function definitions.
    The `__init__(self, env)` function takes the environment instance, `env`, as an
    input argument and initializes the dimensions/shape of the observation space and
    the action space, and also determines the parameters to discretize the observation
    space based on the `NUM_DISCRETE_BINS` we set. The `__init__(self, env)` function
    also initializes the Q function as a NumPy array, based on the shape of the discretized
    observation space and the action space dimensions. The implementation of `__init__(self,
    env)` is straightforward as we are only initializing the necessary values for
    the agent. Here is our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the Q_Learner class's discretize method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a moment to understand how we are discretizing the observation
    space. The simplest, and yet an effective, way to discretize the observation space
    (and a metric space in general) is to divide the span of the range of values into
    a finite set of values called bins. The span/range of values is given by the difference
    between the maximum possible value and the minimum possible value in each dimension
    of the space. Once we calculate the span, we can divide it by the `NUM_DISCRETE_BINS` that
    we have decided on to get the width of the bin. We calculated the bin width in
    the `__init__` function because it does not change with every new observation.
    The `discretize(self, obs)` function receives every new function and applies the
    discretization step to find the bin that the observation belongs to in the discretized
    space. It is as simple as doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We want it to belong to any *one* of the bins (and not somewhere in between);
    therefore, we convert the previous code into an `integer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we return this discretized observation as a tuple. All of this operation
    can be written in one line of Python code, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the Q_Learner's get_action method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want the agent to taken an action given an observation. `get_action(self,
    obs)`is the function we define to generate an action, given an observation in `obs`.The
    most widely used action selection policy is the epsilon-greedy policy, which takes
    the best action as per the agent''s estimate with a (high) probability of *1-*![](img/00124.jpeg),
    and takes a random action with a (small) probability given by epsilon ![](img/00125.jpeg).
    We implement the epsilon-greedy policy using the `random()` method from NumPy''s
    random module, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Implementing the Q_learner class's learn method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you might have guessed, this is the most important method of the `Q_Learner`
    class, which does the magic of learning the Q-values, which in turn enables the
    agent to take intelligent actions over time! The best part is that it is not that
    complicated to implement! It is merely the implementation of the Q-learning update
    equation that we saw earlier. Don''t believe me when I say it is simple to implement?!
    Alright, here is the implementation of the learning function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now do you agree? :)
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have written the Q learning update rule in one line of code, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: But, calculating each term on a separate line will make it easier to read and
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: Full Q_Learner class implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we put all the method implementations together, we will get a code snippet
    that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So, we have the agent ready. What should we do next, you may ask. Well, we should
    train the agent in the Gym environment! In the next section, we will look at the
    training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Training the reinforcement learning agent at the Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The procedure to train the Q-learning agent may look familiar to you already,
    because it has many of the same lines of code as, and also a similar structure
    to, the boilerplate code that we used before. Instead of choosing a random action
    from the environment''s actions space, we now get the action from the agent using
    the `agent.get_action(obs)` method. We also call the `agent.learn(obs, action,
    reward, next_obs)` method after sending the agent''s action to the environment
    and receiving the feedback. The training function is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Testing and recording the performance of the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we let the agent train at the Gym, we want to be able to measure how well
    it has learned. To do that, we let the agent go through a test. Just like in school! `test(agent,
    env, policy)` takes the agent object, the environment instance, and the agent''s
    policy to test the performance of the agent in the environment, and returns the
    total reward for one full episode. It is similar to the `train(agent, env)` function
    we saw earlier, but it does not let the agent learn or update its Q-value estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A simple and complete Q-Learner implementation for solving the Mountain Car
    problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will put together the whole code into a single Python script
    to initialize the environment, launch the agent's training process, get the trained
    policy, test the performance of the agent, and also record how it acts in the
    environment!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This script is available in the code repository under the `ch5` folder, named `Q_learner_MountainCar.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate the `rl_gym_book` conda environment and launch the script to see it
    in action! When you launch the script, you will see initial output like that shown
    in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: During the initial training episodes, when the agent is just getting started
    learning, you will see that it always ends up with a reward of *-200*. From your
    understanding of how the Gym's Mountain Car environment works, you can see that
    the agent does not reach the mountain top within the *200* time steps, and so
    the environment automatically resets the environment; thus, the agent only gets
    *-200*. You can also observe the **![](img/00127.jpeg) **(**eps**) exploration
    value decaying slowly.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you let the agent learn for long enough, you will see the agent improving
    and learning to reach the top of the mountain in fewer and fewer steps. Here is
    a sample of its progress after *5* minutes of training on a typical laptop hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Once the script finishes running, you will see the recorded videos (along with
    some `.stats.json` and `.meta.json` files) of the agent's performance in the `gym_monitor_output` folder.
    You can watch the videos to see how your agent performed!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a screenshot showing the agent successfully steering the car to the
    top of the mountain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Hooray!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned a lot in this chapter. More importantly, we implemented an agent
    that learned to solve the Mountain Car problem smartly in 7 minutes or so!
  prefs: []
  type: TYPE_NORMAL
- en: We started by understanding the famous Mountain Car problem and looking at how
    the environment, the observation space, the state space, and rewards are designed
    in the Gym's `MountainCar-v0` environment. We revisited the reinforcement learning
    Gym boilerplate code we used in the previous chapter and made some improvements
    to it, which are also available in the code repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We then defined the hyperparameters for our Q-learning agent and started implementing
    a Q-learning algorithm from scratch. We first implemented the agent's initialization
    function to initialize the agent's internal state variables, including the Q value
    representation, using a NumPy *n*-dimensional array. Then, we implemented the
    `discretize` method to discretize the `state space`; the `get_action(...)` method
    to select an action based on an epsilon-greedy policy; and then finally the `learn(...)`
    function, which implements the Q-learning update rule and forms the core of the
    agent. We saw how simple it was to implement them all! We also implemented functions
    to train, test, and evaluate the agent's performance.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you had a lot of fun implementing the agent and watching it solve the
    Mountain Car problem at the Gym! We will get into advanced methods in the next
    chapter to solve a variety of more challenging problems.
  prefs: []
  type: TYPE_NORMAL
