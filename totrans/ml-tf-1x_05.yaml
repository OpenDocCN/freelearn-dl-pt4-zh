- en: Sequence to Sequence Models-Parlez-vous Français?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, much of our work was with images. Working with images is helpful as
    the results are almost uncanny in how quickly and succinctly progress can be made.
    However, the world of machine learning is broader and the next several chapters
    will cover these other aspects. We will start with sequence-to-sequence models.
    The results are just as uncanny, though the setup is a bit more involved and training
    datasets are much larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on several areas, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how sequence-to-sequence models work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the setup required to feed a sequence-to-sequence model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing an English to French translator using sequence-to-sequence models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick preview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes, you read that correctly...we will be writing an English to French translator.
    The pre-machine learning world might have approached this with a series of parsers
    and rules on how to translate words and phrases to others, but our approach will
    be far more elegant, generalizable, and quick. We will just use examples, many
    examples, to train our translator.
  prefs: []
  type: TYPE_NORMAL
- en: The game here will be finding a dataset with enough English sentences translated
    into French (actually, it will work in any language). Translated essays and news
    articles will not be as helpful as we won't necessarily be able to place specific
    sentences from one language to another side by side. So, we will need to be more
    creative. Fortunately, organizations such as the United Nations often need to
    do exactly this—they need to do line by line translations to meet the needs of
    their diverse constituencies. How convenient for us!
  prefs: []
  type: TYPE_NORMAL
- en: The *Workshop on Statistical Machine Translation* had a conference in 2010 that
    released a nice packaged training set, which can be used. The full details are
    available at [http://www.statmt.org/wmt10/](http://www.statmt.org/wmt10/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using specific files just for French, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.statmt.org/wmt10/training-giga-fren.tar](http://www.statmt.org/wmt10/training-giga-fren.tar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.statmt.org/wmt15/dev-v2.tgz](http://www.statmt.org/wmt15/dev-v2.tgz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an excerpt of what the source data looks like on the English
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: Food, where European inflation slipped up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The skyward zoom in food prices is the dominant force behind the speed up in
    eurozone inflation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: November price hikes were higher than expected in the 13 eurozone countries,
    with October's 2.6 percent yr/yr inflation rate followed by 3.1 percent in November,
    the EU's Luxembourg-based statistical office reported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Official forecasts predicted just three percent, Bloomberg said
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As opposed to the US, UK, and Canadian central banks, the **European Central
    Bank** (**ECB**) did not cut interest rates, arguing that a rate drop combined
    with rising raw material prices and declining unemployment would trigger an inflationary
    spiral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ECB wants to hold inflation to under two percent, or somewhere in that vicinity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to one analyst, ECB has been caught in a Catch-22, and it needs to
    **talk down** inflation, to keep from having to take action to push it down later
    in the game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And, here is the French equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: L'inflation, en Europe, a dérapé sur l'alimentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L'inflation accélérée, mesurée dans la zone euro, est due principalement à l'augmentation
    rapide des prix de l'alimentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: En novembre, l'augmentation des prix, dans les 13 pays de la zone euro, a été
    plus importante par rapport aux prévisions, après un taux d'inflation de 2,6 pour
    cent en octobre, une inflation annuelle de 3,1 pour cent a été enregistrée, a
    indiqué le bureau des statistiques de la Communauté Européenne situé à Luxembourg
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Les prévisions officielles n'ont indiqué que 3 pour cent, a communiqué Bloomberg
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrairement aux banques centrales américaine, britannique et canadienne, la
    Banque centrale européenne (BCE) n'a pas baissé le taux d'intérêt directeur en
    disant que la diminution des intérêts, avec la croissance des prix des matières
    premières et la baisse du taux de chômage, conduirait à la génération d'une spirale
    inflationniste
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: La BCE souhaiterait maintenir le taux d'inflation au-dessous mais proche de
    deux pour cent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selon un analyste, c''est le Catch 22 pour la BCE-: "il faut dissuader" l''inflation
    afin de ne plus avoir à intervenir sur ce sujet ultérieurement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is usually good to do a quick sanity check, when possible, to ensure that
    the files do actually line up. We can see the `Catch 22` phrase on line 7 of both
    files, which gives us comfort.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, 7 lines are far from sufficient for a statistical approach. We will
    achieve an elegant, generalizable solution only with mounds of data. And the mounds
    of data we will get for our training set will consist of 20 gigabytes of text,
    translated line by line very much like the preceding excerpts.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did with images, we'll use subsets for training, validation, and
    testing. We will also define a loss function and attempt to minimize that loss.
    Let's start with the data though.
  prefs: []
  type: TYPE_NORMAL
- en: Drinking from the firehose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you did earlier, you should grab the code from [https://github.com/mlwithtf/MLwithTF/](https://github.com/mlwithtf/MLwithTF/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be focusing on the `chapter_05` subfolder that has the following three
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_utils.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`translate.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seq2seq_model.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first file handles our data, so let''s start with that. The `prepare_wmt_dataset`
    function handles that. It is fairly similar to how we grabbed image datasets in
    the past, except now we''re grabbing two data subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`giga-fren.release2.fr.gz`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`giga-fren.release2.en.gz`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, these are the two languages we want to focus on. The beauty of our
    soon-to-be-built translator will be that the approach is entirely generalizable,
    so we can just as easily create a translator for, say, German or Spanish.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot is the specific subset of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a0c104a-e823-49f9-94af-80994af1b221.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will run through the two files of interest from earlier line by line
    and do two things—create vocabularies and tokenize the individual words. These
    are done with the `create_vocabulary` and `data_to_token_ids` functions, which
    we will get to in a moment. For now, let''s observe how to create the vocabulary
    and tokenize on our massive training set as well as a small development set, `newstest2013.fr`
    and `dev/newstest2013.en`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0449ecaa-9e2d-41fa-bdcb-a9a1178d8e61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We created a vocabulary earlier using the following `create_vocabulary` function.
    We will start with an empty vocabulary map, `vocab = {}`, and run through each
    line of the data file and for each line, create a bucket of words using a basic
    tokenizer. (Warning: this is not to be confused with the more important token
    in the following ID function.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we encounter a word we already have in our vocabulary, we will increment
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, we will initialize the count for that word, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will keep doing this until we run out of lines on our training dataset. Next,
    we will sort our vocabulary by order of frequency using `sorted(vocab`, `key=vocab.get`,
    and `reverse=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important because we won''t keep every single word, we''ll only keep
    the *k* most frequent words, where *k* is the vocabulary size we defined (we had
    defined this to 40,000 but you can choose different values and see how the results
    are affected):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8b6bf07-d504-41c8-a06a-f4b76beab673.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While working with sentences and vocabularies is intuitive, this will need
    to get more abstract at this point—we''ll temporarily translate each vocabulary
    word we''ve learned into a simple integer. We will do this line by line using
    the `sequence_to_token_ids` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1a33035-42f6-4cab-8e14-f78af870c26f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will apply this approach to the entire data file using the `data_to_token_ids`
    function, which reads our training file, iterates line by line, and runs the `sequence_to_token_ids`
    function, which then uses our vocabulary listing to translate individual words
    in each sentence to integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb56f8d6-2c8a-4747-9554-dad5f6898326.png)'
  prefs: []
  type: TYPE_IMG
- en: Where does this leave us? With two datasets of just numbers. We have just temporarily
    translated our English to French problem to a numbers to numbers problem with
    two sequences of sentences consisting of numbers mapping to vocabulary words.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we start with `["Brooklyn", "has", "lovely", "homes"]` and generate a `{"Brooklyn":
    1, "has": 3, "lovely": 8, "homes": 17"}` vocabulary, we will end up with `[1,
    3, 8, 17]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the output look like? The following typical file downloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I won't repeat the English section of the dataset processing as it is exactly
    the same. We will read the gigantic file line by line, create a vocabulary, and
    tokenize the words line by line for each of the two language files.
  prefs: []
  type: TYPE_NORMAL
- en: Training day
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The crux of our effort will be the training, which is shown in the second file
    we encountered earlier—`translate.py`. The `prepare_wmt_dataset` function we reviewed
    earlier is, of course, the starting point as it creates our two datasets and tokenizes
    them into nice clean numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training starts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66bd8fe5-2f96-47ac-aba9-0056eb700c57.png)'
  prefs: []
  type: TYPE_IMG
- en: After preparing the data, we will create a TensorFlow session, as usual, and
    construct our model. We'll get to the model later; for now, let's look at our
    preparation and training loop.
  prefs: []
  type: TYPE_NORMAL
- en: We will define a dev set and a training set later, but for now, we will define
    a scale that is a floating point score ranging from 0 to 1\. Nothing complex here;
    the real work comes in the following training loop. This is very different from
    what we've done in previous chapters, so close attention is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main training loop is seeking to minimize our error. There are two key
    statements. Here''s the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And, the second key is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `get_batch` function is essentially used to convert the two sequences into
    batch-major vectors and associated weights. These are then used on the model step,
    which returns our loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t deal with the loss though, we will use `perplexity`, which is `e`
    raised to the power of the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41bc47dd-7cb9-4fb6-aa62-ce0db0ea3d45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At every *X* steps, we will save our progress using `previous_losses.append(loss)`,
    which is important because we will compare our current batch''s loss to previous
    losses. When losses start going up, we will reduce our learning rate using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sess.run(model.learning_rate_decay_op)`, and evaluate the loss on our `dev_set`,
    much like we used our validation set in earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03519901-49f4-4a9b-92c7-6f1d57294cff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will get the following output when we run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see outputs at every 200 steps. This is one of about a dozen settings
    we''re using, which we defined at the top of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will use most of these settings when constructing the model object. That
    is, the final piece of the puzzle is the model itself, so let's look at that.
    We'll return to the third and final of the three files in our project—`seq2seq_model.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall how we created the model at the start of the training process after
    creating the TensorFlow session? Most of the parameters we''ve defined are used
    to initialize the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: However, what the initialize is accomplishing is inside `seq2seq_model.py`,
    so let's jump to that.
  prefs: []
  type: TYPE_NORMAL
- en: You will find that the model is enormous, which is why we won't explain line
    by line but instead chunk by chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first section is the initialization of the model, demonstrated by the following
    two figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e66ffcf-cafd-4922-b7b0-814aa7415731.png)'
  prefs: []
  type: TYPE_IMG
- en: The model starts with an initialization, which sets off the required parameters.
    We'll skip the setting of these parameters as we're already familiar with them—we
    initialized these parameters ourselves before the training, by just passing the
    values into the model construction statement, and they are finally passed into
    internal variables via `self.xyz` assignments.
  prefs: []
  type: TYPE_NORMAL
- en: Recall how we passed in the size of each model layer (size=1024) and the number
    of layers (3). These are pretty important as we construct the weights and biases
    (`proj_w` and `proj_b`). The weights are *A x B* where *A* is the layer size and
    *B* is the vocabulary size of the target language. The biases are just passed
    based on the size of the target vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the weights and biases from our `output_project` tuple - `output_projection
    = (w, b)`- and use the transposed weights and biases to form our `softmax_loss_function`,
    which we''ll use over and over to gauge performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cabd2352-cfe1-44fb-b859-e18a47c3196b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next section is the step function, which is shown in the following figure.
    The first half is just error checking, so we''ll skip through it. Most interesting
    is the construction of the output feed using stochastic gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a210e471-a641-461c-b26f-c0c9030328ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final section of the model is the `get_batch` function, which is shown
    in the following figure. We will explain the individual parts with inline comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e45db696-3d48-4455-b613-e36fb35ec921.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we run this, we can get a perfect training run, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we may find steps where we have reduced our learning rate after
    consistent increases in losses. Either way, we will keep testing on our *development*
    set until our accuracy increases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered sequence-to-sequence networks and wrote a language
    translator using a series of known sentence-by-sentence translations as a training
    set. We were introduced to RNNs as a base for our work and likely crossed the
    threshold of big data as we trained using a 20 GB set of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll jump into tabular data and make predictions on economic and financial
    data. We'll use parts of our prior work so we can hit the ground running, namely
    the initial pipeline work we've written so far to download and prepare training
    data. However, we'll focus on a time series problem, so it will be quite different
    from the image and text work we've done to date.
  prefs: []
  type: TYPE_NORMAL
