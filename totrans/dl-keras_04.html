<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Generative Adversarial Networks and WaveNet</h1>
            </header>

            <article>
                
<p>In this chapter, we will discuss <strong>generative adversarial networks</strong> (<strong>GANs</strong>) and WaveNets. GANs have been defined as <em>the most interesting idea in the last 10 years in ML</em> (<a href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning"><span class="URLPACKT">https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning</span></a>) by Yann LeCun, one of the fathers of deep learning. GANs are able to learn how to reproduce synthetic data that looks real. For instance, computers can learn how to paint and create realistic images. The idea was originally proposed by Ian Goodfellow (for more information refer to: <em>NIPS 2016 Tutorial: Generative Adversarial Networks</em>, by I. Goodfellow, 2016); he was worked with the University of Montreal, Google Brain, and recently OpenAI (<a href="https://openai.com/"><span class="URLPACKT">https://openai.com/</span></a>). WaveNet is a deep generative network proposed by Google DeepMind to teach computers how to reproduce human voices and musical instruments, both with impressive quality.</p>
<p>In this chapter, we will cover cover the following topics:</p>
<ul>
<li>What is GAN?</li>
<li>Deep convolutional GAN</li>
<li>Applications of GAN</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What is a GAN?</h1>
            </header>

            <article>
                
<p>The key intuition of GAN can be easily considered as analogous to <em>art forgery</em>, which is the process of creating works of art (<a href="https://en.wikipedia.org/wiki/Art" target="_blank">https://en.wikipedia.org/wiki/Art</a>) that are falsely credited to other, usually more famous, artists. GANs train two neural nets simultaneously, as shown in the next diagram. The generator <em>G(Z) </em>makes the forgery, and the discriminator <em>D(Y)</em> can judge how realistic the reproductions based on its observations of authentic pieces of arts and copies are. <em>D(Y)</em> takes an input, <em>Y, </em>(for instance, an image) and expresses a vote to judge how real the input is--in general, a value close to zero denotes <em>real</em> and a value close to one denotes <em>forgery</em>. <em>G(Z)</em> takes an input from a random noise, <em>Z</em>, and trains itself to fool <em>D</em> into thinking that whatever <em>G(Z)</em> produces is real. So, the goal of training the discriminator <em>D(Y)</em> is to maximize <em>D(Y)</em> for every image from the true data distribution, and to minimize <em>D(Y</em>) for every image not from the true data distribution. So, <em>G</em> and <em>D</em> play an opposite game; hence the name <em>adversarial training</em>. Note that we train <em>G</em> and <em>D</em> in an alternating manner, where each of their objectives is expressed as a loss function optimized via a gradient descent. The generative model learns how to forge more successfully, and the discriminative model learns how to recognize forgery more successfully. The discriminator network (usually a standard convolutional neural network) tries to classify whether an input image is real or generated. The important new idea is to backpropagate through both the discriminator and the generator to adjust the generator's parameters in such a way that the generator can learn how to fool the the discriminator for an increasing number of situations. At the end, the generator will learn how to produce forged images that are indistinguishable from real ones:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="150" src="assets/B06258_04a_01.png" width="357"/></div>
<p>Of course, GANs require finding the equilibrium in a game with two players. For effective learning it is required that if a player successfully moves downhill in a round of updates, the same update must move the other player downhill too. Think about it! If the forger learns how to fool the judge on every occasion, then the forger himself has nothing more to learn. Sometimes the two players eventually reach an equilibrium, but this is not always guaranteed and the two players can continue playing for a long time. An example of learning from both sides has been provided in the following graph:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="481" src="assets/B06258_04a_001.png" width="571"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Some GAN applications</h1>
            </header>

            <article>
                
<p>We have seen that the generator learns how to forge data. This means that it learns how to create new synthetic data, which is created by the network, that looks real and like it was created by humans. Before going into details of some GAN code, I'd like to share the results of a recent paper: <em>StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks</em>, by Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris Metaxas (the code is available online at: <a href="https://github.com/hanzhanggit/StackGAN" target="_blank">https://github.com/hanzhanggit/StackGAN</a>).<br/>
Here, a GAN has been used to synthesize forged images starting from a text description. The results are impressive. The first column is the real image in the test set, and the rest of the columns contain images generated from the same text description by <span class="packt_screen">Stage-I</span> and <span class="packt_screen">Stage-II</span> of StackGAN. More examples are available on YouTube (<a href="https://www.youtube.com/watch?v=SuRyL5vhCIM&amp;feature=youtu.be" target="_blank">https://www.youtube.com/watch?v=SuRyL5vhCIM&amp;feature=youtu.be</a>):</p>
<p><img class="image-border" src="assets/B06258_04a_002.jpg"/><br/>
<img class="image-border" src="assets/B06258_04a_003.jpg"/></p>
<p>Now let us see how a GAN can learn to <em>forge</em> the MNIST dataset. In this case, there is a combination of GAN and ConvNets (for more information refer to: <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>, by A. Radford, L. Metz, and S. Chintala, arXiv: 1511.06434, 2015) used for the generator and the discriminator networks. At the beginning, the generator creates nothing understandable, but after a few iterations, synthetic forged numbers are progressively clearer and clearer. In the following image, the panels are ordered by increasing training epochs, and you can see the quality improving among panels:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="215" src="assets/B06258_04a_004.png" width="643"/></div>
<p>The following image represents the forged handwritten numbers as the number of iterations increases:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="220" src="assets/B06258_04a_005.png" width="645"/></div>
<p>The following image represents the forged handwritten numbers at the hand of computation. The results are virtually indistinguishable from the original:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="232" src="assets/B06258_04a_006.png" width="677"/></div>
<p>One of the coolest uses of GAN is arithmetic on faces in the generator's vector <em>Z</em>. In other words, if we stay in the space of synthetic forged images, it is possible to see things like this:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><em>[smiling woman] - [neutral woman] + [neutral man] = [smiling man]</em></p>
<p class="packt_figure CDPAlignLeft CDPAlign">Or like this:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><em>[man with glasses] - [man without glasses] + [woman without glasses] = [woman with glasses]</em></p>
<p>The next image is taken from the article, <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>, by A. Radford, L. Metz, and S. Chintala, arXiv: 1511.06434, November, 2015:</p>
<p><img class="image-border" height="573" src="assets/B06258_04a_007.png" width="582"/></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deep convolutional generative adversarial networks</h1>
            </header>

            <article>
                
<p>The <strong>deep convolutional generative adversarial networks</strong> (<strong>DCGAN</strong>) are introduced in the paper: <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>, by A. Radford, L. Metz, and S. Chintala, arXiv: 1511.06434, 2015. The generator uses a 100-dimensional, uniform distribution space, <em>Z,</em> which is then projected into a smaller space by a series of vis-a-vis convolution operations. An example is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="398" src="assets/B06258_04a_008.png" width="684"/></div>
<p>A DCGAN generator can be described by the following Keras code; it is also described by one implementation, available at: <a href="https://github.com/jacobgil/keras-dcgan" target="_blank">https://github.com/jacobgil/keras-dcgan</a>:</p>
<pre>
def generator_model():<br/>    model = Sequential()<br/>    model.add(Dense(input_dim=100, output_dim=1024))<br/>    model.add(Activation('tanh'))<br/>    model.add(Dense(128*7*7))<br/>    model.add(BatchNormalization())<br/>    model.add(Activation('tanh'))<br/>    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))<br/>    model.add(UpSampling2D(size=(2, 2)))<br/>    model.add(Convolution2D(64, 5, 5, border_mode='same'))<br/>    model.add(Activation('tanh'))<br/>    model.add(UpSampling2D(size=(2, 2)))<br/>    model.add(Convolution2D(1, 5, 5, border_mode='same'))<br/>    model.add(Activation('tanh'))<br/>    return model
</pre>
<p>Note that the code runs with Keras 1.x syntax. However, it is possible to run it with Keras 2.0 thanks to the Keras legacy interfaces. In this case a few warnings are reported as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/dcgan-compatibility.png"/></div>
<p>Now let’s see the code. The first dense layer takes a vector of 100 dimensions as input and it produces 1,024 dimensions with the activation function <kbd>tanh</kbd> as the output<em>.</em> We assume that the input is sampled from a uniform distribution in <em>[-1, 1]</em>. The next dense layer produces data of 128 x 7 x 7 in the output using batch normalization (for more information refer to <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>, by S. Ioffe and C. Szegedy, arXiv: 1502.03167, 2014), a technique that can help stabilize learning by normalizing the input to each unit to zero mean and unit variance<em>. </em>Batch normalization has been empirically proven to accelerate the training in many situations, reduce the problems of poor initialization, and more generally produce more accurate results. There is also a <kbd>Reshape()</kbd> module that produces data of 127 x 7 x 7 (127 channels, 7 width, and 7 height), <kbd>dim_ordering</kbd> to <kbd>tf</kbd>, and a <kbd>UpSampling()</kbd> module that produces a repetition of each one into a 2 x 2 square. After that, we have a convolutional layer producing 64 filters on 5 x 5 convolutional kernels with the activation <kbd>tanh</kbd><em>,</em> followed by a new <kbd>UpSampling()</kbd> and a final convolution with one filter, and on 5 x 5 convolutional kernels with the activation <kbd>tanh</kbd><em>.</em> Notice that this ConvNet has no pooling operations. The discriminator can be described with the following code:</p>
<pre>
def discriminator_model():<br/>    model = Sequential()<br/>    model.add(Convolution2D(64, 5, 5, border_mode='same',<br/>    input_shape=(1, 28, 28)))<br/>    model.add(Activation('tanh'))<br/>    model.add(MaxPooling2D(pool_size=(2, 2)))<br/>    model.add(Convolution2D(128, 5, 5))<br/>    model.add(Activation('tanh'))<br/>    model.add(MaxPooling2D(pool_size=(2, 2)))<br/>    model.add(Flatten())<br/>    model.add(Dense(1024))<br/>    model.add(Activation('tanh'))<br/>    model.add(Dense(1))<br/>    model.add(Activation('sigmoid'))<br/>    return model
</pre>
<p>The code takes a standard MNIST image with the shape <kbd>(1, 28, 28)</kbd> and applies a convolution with 64 filters of size 5 x 5 with <kbd>tanh</kbd> as the activation function. This is followed by a max-pooling operation of size 2 x 2 and by a further convolution max-pooling operation. The last two stages are dense, with the final one being the prediction for forgery, which consists of only one neuron with a <kbd>sigmoid</kbd> activation function. For a chosen number of epochs, the generator and discriminator are in turn trained by using <kbd>binary_crossentropy</kbd> as loss function. At each epoch, the generator makes a number of predictions (for example, it creates forged MNIST images) and the discriminator tries to learn after mixing the prediction with real MNIST images. After 32 epochs, the generator learns to forge this set of handwritten numbers. No one has programmed the machine to write but it has learned how to write numbers that are indistinguishable from the ones written by humans. Note that training GANs could be very difficult because it is necessary to find the equilibrium between two players. If you are interested in this topic, I'd advise you to have a look at a series of tricks collected by practitioners (<a href="https://github.com/soumith/ganhacks" target="_blank">https://github.com/soumith/ganhacks</a>):</p>
<div class="CDPAlignCenter"><img class="image-border" height="150" src="assets/Capture.png" width="141"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Keras adversarial GANs for forging MNIST</h1>
            </header>

            <article>
                
<p>Keras adversarial (<a href="https://github.com/bstriner/keras-adversarial" target="_blank">https://github.com/bstriner/keras-adversarial</a>) is an open source Python package for building GANs developed by Ben Striner (<a href="https://github.com/bstriner" target="_blank">https://github.com/bstriner</a> and <a href="https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt" target="_blank">https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt</a>). Since Keras just recently moved to 2.0, I suggest downloading latest Keras adversarial package:</p>
<pre>
<strong>git clone --depth=50 --branch=master https://github.com/bstriner/keras-adversarial.git</strong>
</pre>
<p>And install <kbd>setup.py</kbd>:</p>
<pre>
<strong>python setup.py install</strong>
</pre>
<p>Note that compatibility with Keras 2.0 is tracked in this issue<a href="https://github.com/bstriner/keras-adversarial/issues/11"> https://github.com/bstriner/keras-adversarial/issues/11</a>.</p>
<p> If the generator <em>G</em> and the discriminator <em>D</em> are based on the same model, <em>M,</em> then they can be combined into an adversarial model; it uses the same input, <em>M</em>, but separates targets and metrics for <em>G</em> and <em>D</em>. The library has the following API call:</p>
<pre>
adversarial_model = AdversarialModel(base_model=M,<br/>    player_params=[generator.trainable_weights, discriminator.trainable_weights],<br/>    player_names=["generator", "discriminator"])
</pre>
<p>If the generator <em>G</em> and the discriminator <em>D</em> are based on the two different models, then it is possible to use this API call:</p>
<pre>
adversarial_model = AdversarialModel(player_models=[gan_g, gan_d],<br/>    player_params=[generator.trainable_weights, discriminator.trainable_weights],<br/>    player_names=["generator", "discriminator"])
</pre>
<p>Let's see an example of a computation with MNIST:</p>
<pre>
import matplotlib as mpl<br/># This line allows mpl to run with no DISPLAY defined<br/>mpl.use('Agg')
</pre>
<p>Let us see the open source code (<a href="https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py" target="_blank">https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py</a>). Note that the code uses the syntax of Keras 1.x, but it also runs on the top of Keras 2.x thanks to a convenient set of utility functions contained in <kbd>legacy.py</kbd>. The code for <kbd>legacy.py</kbd> is reported in <a href="c0a1905f-57cc-401c-b485-6bf0854e43e9.xhtml" target="_blank">Appendix</a>, <em>Conclusion</em>, and is available at <a href="https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py">https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py</a>.</p>
<p>First, the open source example imports a number of modules. We have seen all of them previously, with the exception of LeakyReLU, a special version of ReLU that allows a small gradient when the unit is not active. Experimentally, it has been shown that LeakyReLU can improve the performance of GANs (for more information refer to: <em>Empirical Evaluation of Rectified Activations in Convolutional Network</em>, by B. Xu, N. Wang, T. Chen, and M. Li, arXiv:1505.00853, 2014) in a number of situations:</p>
<pre>
from keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU,<br/>    Input, Activation, BatchNormalization<br/>from keras.models import Sequential, Model<br/>from keras.layers.convolutional import Convolution2D, UpSampling2D<br/>from keras.optimizers import Adam<br/>from keras.regularizers import l1, l1l2<br/>from keras.datasets import mnist<br/><br/>import pandas as pd<br/>import numpy as np
</pre>
<p>Then, specific modules for GANs are imported:</p>
<pre>
from keras_adversarial import AdversarialModel, ImageGridCallback,<br/>    simple_gan, gan_targets<br/>from keras_adversarial import AdversarialOptimizerSimultaneous,<br/>    normal_latent_sampling, AdversarialOptimizerAlternating<br/>from image_utils import dim_ordering_fix, dim_ordering_input,<br/>    dim_ordering_reshape, dim_ordering_unfix
</pre>
<p>Adversarial models train for multiplayer games. Given a base model with <em>n</em> targets and <em>k</em> players, create a model with <em>n*k</em> targets, where each player optimizes loss on that player's targets. In addition, <kbd>simple_gan</kbd> generates a GAN with the given <kbd>gan_targets</kbd>. Note that in the library, the labels for generator and discriminator are opposite; intuitively, this is a standard practice for GANs:</p>
<pre>
def gan_targets(n):<br/>    """<br/>    Standard training targets [generator_fake, generator_real, discriminator_fake,     <br/>    discriminator_real] = [1, 0, 0, 1]<br/>    :param n: number of samples<br/>    :return: array of targets<br/>    """<br/>    generator_fake = np.ones((n, 1))<br/>    generator_real = np.zeros((n, 1))<br/>    discriminator_fake = np.zeros((n, 1))<br/>    discriminator_real = np.ones((n, 1))<br/>    return [generator_fake, generator_real, discriminator_fake, discriminator_real]
</pre>
<p>The example defines the generator in a similar way to what we have seen previously. However, in this case, we use the functional syntax—each module in our pipeline is simply passed as input to the following module. So, the first module is dense, initialized by using <kbd>glorot_normal</kbd>. This initialization uses Gaussian noise scaled by the sum of the inputs plus outputs from the node. The same kind of initialization is used for all of the other modules. The <kbd>mode=2</kbd> parameter in <kbd>BatchNormlization</kbd> function produces feature-wise normalization based on per-batch statistics. Experimentally, this produces better results:</p>
<pre>
def model_generator():<br/>    nch = 256<br/>    g_input = Input(shape=[100])<br/>    H = Dense(nch * 14 * 14, init='glorot_normal')(g_input)<br/>    H = BatchNormalization(mode=2)(H)<br/>    H = Activation('relu')(H)<br/>    H = dim_ordering_reshape(nch, 14)(H)<br/>    H = UpSampling2D(size=(2, 2))(H)<br/>    H = Convolution2D(int(nch / 2), 3, 3, border_mode='same', <br/>        init='glorot_uniform')(H)<br/>    H = BatchNormalization(mode=2, axis=1)(H)<br/>    H = Activation('relu')(H)<br/>    H = Convolution2D(int(nch / 4), 3, 3, border_mode='same', <br/>        init='glorot_uniform')(H)<br/>    H = BatchNormalization(mode=2, axis=1)(H)<br/>    H = Activation('relu')(H)<br/>    H = Convolution2D(1, 1, 1, border_mode='same', init='glorot_uniform')(H)<br/>    g_V = Activation('sigmoid')(H)<br/>    return Model(g_input, g_V)
</pre>
<p>The discriminator is very similar to the one defined previously in this chapter. The only major difference is the adoption of <kbd>LeakyReLU</kbd>:</p>
<pre>
def model_discriminator(input_shape=(1, 28, 28), dropout_rate=0.5):<br/>    d_input = dim_ordering_input(input_shape, name="input_x")<br/>    nch = 512<br/>    H = Convolution2D(int(nch / 2), 5, 5, subsample=(2, 2),<br/>        border_mode='same', activation='relu')(d_input)<br/>    H = LeakyReLU(0.2)(H)<br/>    H = Dropout(dropout_rate)(H)<br/>    H = Convolution2D(nch, 5, 5, subsample=(2, 2),<br/>        border_mode='same', activation='relu')(H)<br/>    H = LeakyReLU(0.2)(H)<br/>    H = Dropout(dropout_rate)(H)<br/>    H = Flatten()(H)<br/>    H = Dense(int(nch / 2))(H)<br/>    H = LeakyReLU(0.2)(H)<br/>    H = Dropout(dropout_rate)(H)<br/>    d_V = Dense(1, activation='sigmoid')(H)<br/>    return Model(d_input, d_V)
</pre>
<p>Then, two simple functions for loading and normalizing MNIST data are defined:</p>
<pre>
def mnist_process(x):<br/>    x = x.astype(np.float32) / 255.0<br/>    return x<br/><br/>def mnist_data():<br/>    (xtrain, ytrain), (xtest, ytest) = mnist.load_data()<br/>    return mnist_process(xtrain), mnist_process(xtest)
</pre>
<p>As a next step, the GAN is defined as a combination of generator and discriminator in a joint GAN model. Note that the weights are initialized with <kbd>normal_latent_sampling</kbd>, which samples from a normal Gaussian distribution:</p>
<pre>
if __name__ == "__main__":<br/>    # z in R^100<br/>    latent_dim = 100<br/>    # x in R^{28x28}<br/>    input_shape = (1, 28, 28)<br/>    # generator (z -&gt; x)<br/>    generator = model_generator()<br/>    # discriminator (x -&gt; y)<br/>    discriminator = model_discriminator(input_shape=input_shape)<br/>    # gan (x - &gt; yfake, yreal), z generated on GPU<br/>    gan = simple_gan(generator, discriminator, normal_latent_sampling((latent_dim,)))<br/>    # print summary of models<br/>    generator.summary()<br/>    discriminator.summary()<br/>    gan.summary()
</pre>
<p>After this, the example creates our GAN and it compiles the model trained using the <kbd>Adam</kbd> optimizer, with <kbd>binary_crossentropy</kbd> used as a loss function:</p>
<pre>
# build adversarial model<br/>model = AdversarialModel(base_model=gan,<br/>    player_params=[generator.trainable_weights, discriminator.trainable_weights],<br/>    player_names=["generator", "discriminator"])<br/>model.adversarial_compile(adversarial_optimizer=AdversarialOptimizerSimultaneous(),<br/>    player_optimizers=[Adam(1e-4, decay=1e-4), Adam(1e-3, decay=1e-4)],<br/>    loss='binary_crossentropy')
</pre>
<p>The generator for creating new images that look like real ones is defined. Each epoch will generate a new forged image during training that looks like the original:</p>
<pre>
def generator_sampler():<br/>    zsamples = np.random.normal(size=(10 * 10, latent_dim))<br/>    gen = dim_ordering_unfix(generator.predict(zsamples))<br/>    return gen.reshape((10, 10, 28, 28))<br/><br/>generator_cb = ImageGridCallback(<br/>    "output/gan_convolutional/epoch-{:03d}.png",generator_sampler)<br/>xtrain, xtest = mnist_data()<br/>xtrain = dim_ordering_fix(xtrain.reshape((-1, 1, 28, 28)))<br/>xtest = dim_ordering_fix(xtest.reshape((-1, 1, 28, 28)))<br/>y = gan_targets(xtrain.shape[0])<br/>ytest = gan_targets(xtest.shape[0])<br/>history = model.fit(x=xtrain, y=y,<br/>validation_data=(xtest, ytest), callbacks=[generator_cb], nb_epoch=100,<br/>    batch_size=32)<br/>df = pd.DataFrame(history.history)<br/>df.to_csv("output/gan_convolutional/history.csv")<br/>generator.save("output/gan_convolutional/generator.h5")<br/>discriminator.save("output/gan_convolutional/discriminator.h5")
</pre>
<p>Note that <kbd>dim_ordering_unfix</kbd> is utility function for supporting different image ordering defined in <kbd>image_utils.py</kbd>, as follows:</p>
<pre>
def dim_ordering_fix(x):<br/>    if K.image_dim_ordering() == 'th':<br/>        return x<br/>    else:<br/>        return np.transpose(x, (0, 2, 3, 1))
</pre>
<p>Now let's run the code and see the loss for the generator and discriminator. In the following screenshot, we see a dump of the networks for the discriminator and the generator:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="680" src="assets/keras_adversarial_MNIST.png" width="613"/></div>
<p>The following screenshot, shows the number of sample used for training and for validation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="433" src="assets/B06258_04a_010.png" width="693"/></div>
<p>After 5-6 iterations, we already have acceptable artificial images generated and the computer has learned how to reproduce handwritten characters, as shown in the following image:</p>
<div class="CDPAlignCenter"><img class="image-border" height="164" src="assets/B06258_04a_011.png" width="512"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Keras adversarial GANs for forging CIFAR</h1>
            </header>

            <article>
                
<p>Now we can use a GAN approach to learn how to forge CIFAR-10 and create synthetic images that look real. Let's see the open source code (<a href="https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py" target="_blank">https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py</a>).  Again, note that it uses the syntax of Keras 1.x, but it also runs on the top of Keras 2.x thanks to a convenient set of utility functions contained in <kbd>legacy.py</kbd> (<a href="https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py">https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py</a>). First, the open source example imports a number of packages:</p>
<pre>
import matplotlib as mpl<br/># This line allows mpl to run with no DISPLAY defined<br/>mpl.use('Agg')<br/>import pandas as pd<br/>import numpy as np<br/>import os<br/>from keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, <br/>    Activation, BatchNormalization, SpatialDropout2D<br/>from keras.layers.convolutional import Convolution2D, UpSampling2D, <br/>    MaxPooling2D, AveragePooling2D<br/>from keras.models import Sequential, Model<br/>from keras.optimizers import Adam<br/>from keras.callbacks import TensorBoard<br/>from keras.regularizers import l1l2<br/>from keras_adversarial import AdversarialModel, ImageGridCallback, <br/>    simple_gan, gan_targets<br/>from keras_adversarial import AdversarialOptimizerSimultaneous, <br/>    normal_latent_sampling, fix_names<br/>import keras.backend as K<br/>from cifar10_utils import cifar10_data<br/>from image_utils import dim_ordering_fix, dim_ordering_unfix, <br/>    dim_ordering_shape
</pre>
<p>Next, it defines a generator that uses a combination of convolutions with <kbd>l1</kbd> and <kbd>l2</kbd> regularization, batch normalization, and upsampling. Note that <kbd>axis=1</kbd> says to normalize the dimension of the tensor first and <kbd>mode=0</kbd> says to adopt a feature-wise normalization. This particular net is the result of many fine-tuning experiments, but it is still essentially a sequence of convolution 2D and upsampling operations, which uses a <kbd>Dense</kbd> module at the beginning and a <kbd>sigmoid</kbd> at the end. In addition, each convolution uses a <kbd>LeakyReLU</kbd> activation function and <kbd>BatchNormalization</kbd>:</p>
<pre>
def model_generator():<br/>    model = Sequential()<br/>    nch = 256<br/>    reg = lambda: l1l2(l1=1e-7, l2=1e-7)<br/>    h = 5<br/>    model.add(Dense(input_dim=100, output_dim=nch * 4 * 4, W_regularizer=reg()))<br/>    model.add(BatchNormalization(mode=0))<br/>    model.add(Reshape(dim_ordering_shape((nch, 4, 4))))<br/>    model.add(Convolution2D(nch/2, h, h, border_mode='same', W_regularizer=reg()))<br/>    model.add(BatchNormalization(mode=0, axis=1))<br/>    model.add(LeakyReLU(0.2))<br/>    model.add(UpSampling2D(size=(2, 2)))<br/>    model.add(Convolution2D(nch / 2, h, h, border_mode='same', W_regularizer=reg()))<br/>    model.add(BatchNormalization(mode=0, axis=1))<br/>    model.add(LeakyReLU(0.2))<br/>    model.add(UpSampling2D(size=(2, 2)))<br/>    model.add(Convolution2D(nch / 4, h, h, border_mode='same', W_regularizer=reg()))<br/>    model.add(BatchNormalization(mode=0, axis=1))<br/>    model.add(LeakyReLU(0.2))<br/>    model.add(UpSampling2D(size=(2, 2)))<br/>    model.add(Convolution2D(3, h, h, border_mode='same', W_regularizer=reg()))<br/>    model.add(Activation('sigmoid'))<br/>    return model
</pre>
<p>Then, a discriminator is defined. Again, we have a sequence of convolution 2D operations, and in this case we adopt <kbd>SpatialDropout2D</kbd>, which drops entire 2D feature maps instead of individual elements. We also use <kbd>MaxPooling2D</kbd> and <kbd>AveragePooling2D</kbd> for similar reasons:</p>
<pre>
def model_discriminator():<br/>    nch = 256<br/>    h = 5<br/>    reg = lambda: l1l2(l1=1e-7, l2=1e-7)<br/>    c1 = Convolution2D(nch / 4, h, h, border_mode='same', W_regularizer=reg(),<br/>    input_shape=dim_ordering_shape((3, 32, 32)))<br/>    c2 = Convolution2D(nch / 2, h, h, border_mode='same', W_regularizer=reg())<br/>    c3 = Convolution2D(nch, h, h, border_mode='same', W_regularizer=reg())<br/>    c4 = Convolution2D(1, h, h, border_mode='same', W_regularizer=reg())<br/>    def m(dropout):<br/>        model = Sequential()<br/>        model.add(c1)<br/>        model.add(SpatialDropout2D(dropout))<br/>        model.add(MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(LeakyReLU(0.2))<br/>        model.add(c2)<br/>        model.add(SpatialDropout2D(dropout))<br/>        model.add(MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(LeakyReLU(0.2))<br/>        model.add(c3)<br/>        model.add(SpatialDropout2D(dropout))<br/>        model.add(MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(LeakyReLU(0.2))<br/>        model.add(c4)<br/>        model.add(AveragePooling2D(pool_size=(4, 4), border_mode='valid'))<br/>        model.add(Flatten())<br/>        model.add(Activation('sigmoid'))<br/>        return model<br/>    return m
</pre>
<p>It is now possible to generate proper GANs. The following function takes multiple inputs, including a generator, a discriminator, the number of latent dimensions, and the GAN targets:</p>
<pre>
def example_gan(adversarial_optimizer, path, opt_g, opt_d, nb_epoch, generator,<br/>        discriminator, latent_dim, targets=gan_targets, loss='binary_crossentropy'):<br/>    csvpath = os.path.join(path, "history.csv")<br/>    if os.path.exists(csvpath):<br/>        print("Already exists: {}".format(csvpath))<br/>    return
</pre>
<p>Then two GANs are created, one with dropout and the other without dropout for the discriminator:</p>
<pre>
print("Training: {}".format(csvpath))<br/># gan (x - &gt; yfake, yreal), z is gaussian generated on GPU<br/># can also experiment with uniform_latent_sampling<br/>d_g = discriminator(0)<br/>d_d = discriminator(0.5)<br/>generator.summary()<br/>d_d.summary()<br/>gan_g = simple_gan(generator, d_g, None)<br/>gan_d = simple_gan(generator, d_d, None)<br/>x = gan_g.inputs[1]<br/>z = normal_latent_sampling((latent_dim,))(x)<br/># eliminate z from inputs<br/>gan_g = Model([x], fix_names(gan_g([z, x]), gan_g.output_names))<br/>gan_d = Model([x], fix_names(gan_d([z, x]), gan_d.output_names))
</pre>
<p>The two GANs are now combined into an adversarial model with separate weights, and the model is then compiled:</p>
<pre>
# build adversarial model<br/>model = AdversarialModel(player_models=[gan_g, gan_d],<br/>    player_params=[generator.trainable_weights, d_d.trainable_weights],<br/>    player_names=["generator", "discriminator"])<br/>model.adversarial_compile(adversarial_optimizer=adversarial_optimizer,<br/>    player_optimizers=[opt_g, opt_d], loss=loss)
</pre>
<p>Next, there is a simple callback to sample images and a print on the file where the method <kbd>ImageGridCallback</kbd> is defined:</p>
<pre>
# create callback to generate images<br/>zsamples = np.random.normal(size=(10 * 10, latent_dim))<br/>def generator_sampler():<br/>    xpred = dim_ordering_unfix(generator.predict(zsamples)).transpose((0, 2, 3, 1))<br/>    return xpred.reshape((10, 10) + xpred.shape[1:])<br/>generator_cb =<br/>    ImageGridCallback(os.path.join(path, "epoch-{:03d}.png"),<br/>    generator_sampler, cmap=None)
</pre>
<p>Now, the CIFAR-10 data is loaded and the model is fit. If the backend is TensorFlow, then the loss information is saved into a TensorBoard to check how the loss decreases over time. The history is also conveniently saved into a CVS format, and the models' weights are also stored in an <kbd>h5</kbd> format:</p>
<pre>
# train model<br/>xtrain, xtest = cifar10_data()<br/>y = targets(xtrain.shape[0])<br/>ytest = targets(xtest.shape[0])<br/>callbacks = [generator_cb]<br/>if K.backend() == "tensorflow":<br/>    callbacks.append(TensorBoard(log_dir=os.path.join(path, 'logs'),<br/>        histogram_freq=0, write_graph=True, write_images=True))<br/>history = model.fit(x=dim_ordering_fix(xtrain),y=y,<br/>    validation_data=(dim_ordering_fix(xtest), ytest),<br/>    callbacks=callbacks, nb_epoch=nb_epoch,<br/>    batch_size=32)<br/># save history to CSV<br/>df = pd.DataFrame(history.history)<br/>df.to_csv(csvpath)<br/># save models<br/>generator.save(os.path.join(path, "generator.h5"))<br/>d_d.save(os.path.join(path, "discriminator.h5"))
</pre>
<p>Finally, the whole GANs can be run. The generator samples from a space with 100 latent dimensions, and we've used <kbd>Adam</kbd> as optimizer for both GANs:</p>
<pre>
def main():<br/>    # z in R^100<br/>    latent_dim = 100<br/>    # x in R^{28x28}<br/>    # generator (z -&gt; x)<br/>    generator = model_generator()<br/>    # discriminator (x -&gt; y)<br/>    discriminator = model_discriminator()<br/>    example_gan(AdversarialOptimizerSimultaneous(), "output/gan-cifar10",<br/>        opt_g=Adam(1e-4, decay=1e-5),<br/>        opt_d=Adam(1e-3, decay=1e-5),<br/>        nb_epoch=100, generator=generator, discriminator=discriminator,<br/>        latent_dim=latent_dim)<br/>if __name__ == "__main__":<br/>main()
</pre>
<p>In order to have a complete view on the open source code, we need to include a few simple utility functions for storing the grid of images:</p>
<pre>
from matplotlib import pyplot as plt, gridspec<br/>import os<br/><br/>def write_image_grid(filepath, imgs, figsize=None, cmap='gray'):<br/>    directory = os.path.dirname(filepath)<br/>    if not os.path.exists(directory):<br/>        os.makedirs(directory)<br/>    fig = create_image_grid(imgs, figsize, cmap=cmap)<br/>    fig.savefig(filepath)<br/>    plt.close(fig)<br/><br/>def create_image_grid(imgs, figsize=None, cmap='gray'):<br/>    n = imgs.shape[0]<br/>    m = imgs.shape[1]<br/>    if figsize is None:<br/>        figsize=(n,m)<br/>    fig = plt.figure(figsize=figsize)<br/>    gs1 = gridspec.GridSpec(n, m)<br/>    gs1.update(wspace=0.025, hspace=0.025) # set the spacing between axes.<br/>    for i in range(n):<br/>        for j in range(m):<br/>            ax = plt.subplot(gs1[i, j])<br/>            img = imgs[i, j, :]<br/>    ax.imshow(img, cmap=cmap)<br/>    ax.axis('off')<br/>    return fig
</pre>
<p>In addition, we need some utility methods for dealing with different image ordering (for example, Theano or TensorFlow):</p>
<pre>
import keras.backend as K<br/>import numpy as np<br/>from keras.layers import Input, Reshape<br/><br/>def dim_ordering_fix(x):<br/>    if K.image_dim_ordering() == 'th':<br/>        return x<br/>    else:<br/>        return np.transpose(x, (0, 2, 3, 1))<br/><br/>def dim_ordering_unfix(x):<br/>    if K.image_dim_ordering() == 'th':<br/>        return x<br/>    else:<br/>        return np.transpose(x, (0, 3, 1, 2))<br/><br/>def dim_ordering_shape(input_shape):<br/>    if K.image_dim_ordering() == 'th':<br/>        return input_shape<br/>    else:<br/>        return (input_shape[1], input_shape[2], input_shape[0])<br/><br/>def dim_ordering_input(input_shape, name):<br/>    if K.image_dim_ordering() == 'th':<br/>        return Input(input_shape, name=name)<br/>    else:<br/>        return Input((input_shape[1], input_shape[2], input_shape[0]), name=name)<br/><br/>def dim_ordering_reshape(k, w, **kwargs):<br/>    if K.image_dim_ordering() == 'th':<br/>        return Reshape((k, w, w), **kwargs)<br/>    else:<br/>        return Reshape((w, w, k), **kwargs)<br/><br/># One more utility function is used to fix names<br/>def fix_names(outputs, names):<br/>    if not isinstance(outputs, list):<br/>        outputs = [outputs]<br/>    if not isinstance(names, list):<br/>        names = [names]<br/>    return [Activation('linear', name=name)(output) <br/>        for output, name in zip(outputs, names)]
</pre>
<p>The following screenshot, shows a dump of the defined networks:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="754" src="assets/keras_adversarial_CIFAR.png" width="651"/></div>
<p>If we run the open source code, the very first iteration will generate unrealistic images. However, after 99 iterations, the network will learn to forge images that look like real CIFAR-10 images, as shown here:</p>
<div class="CDPAlignCenter">
<p><img class="image-border" height="299" src="assets/B06258_04a_012.png" width="675"/></p>
</div>
<p>In the following images, we see the real CIFAR-10 image on the right and the forged one on the left:</p>
<table class="a">
<tbody>
<tr>
<td><span>Forged images</span></td>
<td><span>Real CIFAR-10 images</span></td>
</tr>
<tr>
<td>
<div class="packt_figure"><img class="image-border" height="191" src="assets/B06258_04a_013.png" width="307"/></div>
</td>
<td>
<div class="packt_figure"><img class="image-border" height="191" src="assets/B06258_04a_014.png" width="307"/></div>
</td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">WaveNet — a generative model for learning how to produce audio</h1>
            </header>

            <article>
                
<p>WaveNet is a deep generative model for producing raw audio waveforms. This breakthrough technology was introduced (<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>) by Google DeepMind (<a href="https://deepmind.com/" target="_blank">https://deepmind.com/</a>) for teaching users how to speak to computers. The results are truly impressive, and you can find online examples of synthetic voices where the computer learns how to talk with the voices of celebrities such as Matt Damon. So, you might wonder why learning to synthesize audio is so difficult. Well, each digital sound we hear is based on 16,000 samples per second (sometimes, 48,000 or more), and building a predictive model where we learn to reproduce a sample based on all the previous ones is a very difficult challenge. Nevertheless, there are experiments showing that WaveNet has improved current state-of-the-art <strong>text-to-speech</strong> (<strong>TTS</strong>) systems, reducing the difference with human voices by 50% for both US English and Mandarin Chinese. What is even cooler is that DeepMind proved that WaveNet can also be used to teach computers how to generate the sound of musical instruments such as piano music. Now it's time for some definitions. TTS systems are typically divided into two different classes:</p>
<ul>
<li><strong>Concatenative TTS</strong>: This is where single speech voice fragments are first memorized and then recombined when the voice has to be reproduced. However, this approach does not scale because it is only possible to reproduce the memorized voice fragments, and it is not possible to reproduce new speakers or different types of audio without memorizing the fragments from the beginning.</li>
<li><strong>Parametric TTS</strong>: This is where a model is created for storing all the characteristic features of the audio to be synthesized. Before WaveNet, the audio generated with parametric TTS was less natural than concatenative TTS. WaveNet improved the state-of-the-art by modeling directly the production of audio sounds, instead of using intermediate signal processing algorithms that have been used in the past.</li>
</ul>
<p>In principle, WaveNet can be seen as a stack of 1D convolutional layers (we have seen 2D convolution for images in <a href="4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml" target="_blank">Chapter 3</a>, <em>Deep Learning with ConvNets</em>), with a constant stride of one and with no pooling layers. Note that the input and the output have by construction the same dimension, so ConvNet is well-suited to model sequential data such as audio. However, it has been shown that in order to reach a large size for the receptive field (remember that the receptive field of a neuron in a layer is the cross section of the previous layer from which neurons provide inputs) in the output neuron it is necessary to either use a massive number of large filters or prohibitively increase the the depth of the network. For this reason, pure ConvNets are not so effective in learning how to synthesize audio. The key intuition beyond WaveNet is the dilated causal convolutions (for more information refer to the article: <em>Multi-Scale Context Aggregation by Dilated Convolutions</em>, by Fisher Yu, Vladlen Koltun, 2016, available at: <a href="https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48" target="_blank">https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48</a>) or sometime atrous convolution (<em>atrous</em> is the <em>bastardization</em> of the French expression <em>à trous</em>, meaning <em>with holes</em>, so an atrous convolution is a convolution with holes), which simply means that some input values are skipped when the filter of a convolutional layer is applied. As an example, in one dimension, a filter, <em>w</em>, of size <em>3</em> with dilatation <em>1</em> would compute the following sum:</p>
<div class="CDPAlignCenter CDPAlign"><img height="20" src="assets/B06258_04a_015.png" width="262"/></div>
<p>Thanks to this simple idea of introducing <em>holes</em>, it is possible to stack multiple dilated convolutional layers with exponentially increasing filters, and learn long range input dependencies without having an excessively deep network. A WaveNet is therefore a ConvNet where the convolutional layers have various dilation factors, allowing the receptive field to grow exponentially with depth and therefore efficiently cover thousands of audio time-steps. When we train, the input are sounds recorded from human speakers. The waveforms are quantized to a fixed integer range. A WaveNet defines an initial convolutional layer accessing only the current and previous input. Then, there is a stack of dilated ConvNet layers, still accessing only current and previous inputs. At the end, there is a series of dense layers that combine previous results, followed by a softmax activation function for categorical outputs. At each step, a value is predicted from the network and fed back into the input. At the same time, a new prediction for the next step is computed. The loss function is the cross-entropy between the output for the current step and the input at the next step .One Keras implementation developed by Bas Veeling is available at: <a href="https://github.com/basveeling/wavenet">https://github.com/basveeling/wavenet</a> and can be easily installed via <kbd>git</kbd>:</p>
<pre>
<strong>pip install virtualenv<br/>mkdir ~/virtualenvs &amp;&amp; cd ~/virtualenvs<br/>virtualenv wavenet<br/>source wavenet/bin/activate<br/>cd ~<br/>git clone https://github.com/basveeling/wavenet.git<br/>cd wavenet<br/>pip install -r requirements.txt</strong>
</pre>
<p>Note that this code is compatible with Keras 1.x and please check the issue at <a href="https://github.com/basveeling/wavenet/issues/29">https://github.com/basveeling/wavenet/issues/29</a>, to understand what is the progress for porting it on the top of Keras 2.x. Training is very simple but requires a significant amount of computational power (so make sure that you have good GPU support):</p>
<pre>
<strong>$ python wavenet.py with 'data_dir=your_data_dir_name'</strong>
</pre>
<p>Sampling the network after training is equally very easy:</p>
<pre>
<strong>python wavenet.py predict with 'models/[run_folder]/config.json predict_seconds=1'</strong>
</pre>
<p>You can find a large number of hyperparameters online, which can be used for fine-tuning our training process. The network is really deep, as explained by this dump of internal layers. Note that the input waveform are divided into (<kbd>fragment_length = 1152</kbd> and <kbd>nb_output_bins = 256</kbd>), which is the tensor propagating into WaveNet. WaveNet is organized in repeated blocks called residuals, each consisting of a multiplied merge of two dilated convolutional modules (one with <kbd>sigmoid</kbd> and the other with <kbd>tanh</kbd> activation), followed by a sum merged convolutional. Note that each dilated convolution has holes of growing exponential size (<kbd>2 ** i</kbd>) from 1 to 512, as defined in this piece of text:</p>
<pre>
def residual_block(x):<br/>    original_x = x<br/>    tanh_out = CausalAtrousConvolution1D(nb_filters, 2, atrous_rate=2 ** i,<br/>        border_mode='valid', causal=True, bias=use_bias,<br/>        name='dilated_conv_%d_tanh_s%d' % (2 ** i, s), activation='tanh',<br/>        W_regularizer=l2(res_l2))(x)<br/>    sigm_out = CausalAtrousConvolution1D(nb_filters, 2, atrous_rate=2 ** i,<br/>        border_mode='valid', causal=True, bias=use_bias,<br/>        name='dilated_conv_%d_sigm_s%d' % (2 ** i, s), activation='sigmoid',<br/>        W_regularizer=l2(res_l2))(x)<br/>    x = layers.Merge(mode='mul',<br/>        name='gated_activation_%d_s%d' % (i, s))([tanh_out, sigm_out])<br/>        res_x = layers.Convolution1D(nb_filters, 1, border_mode='same', bias=use_bias,<br/>        W_regularizer=l2(res_l2))(x)<br/>    skip_x = layers.Convolution1D(nb_filters, 1, border_mode='same', bias=use_bias,<br/>        W_regularizer=l2(res_l2))(x)<br/>    res_x = layers.Merge(mode='sum')([original_x, res_x])<br/>    return res_x, skip_x
</pre>
<p>After the residual dilated block, there is a sequence of merged convolutional modules, followed by two convolutional modules, followed by a <kbd>softmax</kbd> activation function in <kbd>nb_output_bins</kbd> categories. The full network structure is here:</p>
<pre>
<strong>Layer (type) Output Shape Param # Connected to</strong><br/><strong>====================================================================================================</strong><br/><strong>input_part (InputLayer) (None, 1152, 256) 0</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>initial_causal_conv (CausalAtrou (None, 1152, 256) 131328 input_part[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_1_tanh_s0 (CausalAt (None, 1152, 256) 131072 initial_causal_conv[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_1_sigm_s0 (CausalAt (None, 1152, 256) 131072 initial_causal_conv[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_0_s0 (Merge) (None, 1152, 256) 0 dilated_conv_1_tanh_s0[0][0]</strong><br/><strong>dilated_conv_1_sigm_s0[0][0]</strong><br/><strong>______________________________________________________________________</strong><br/><strong>_____________________________</strong><br/><strong>convolution1d_1 (Convolution1D) (None, 1152, 256) 65536 gated_activation_0_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_1 (Merge) (None, 1152, 256) 0 initial_causal_conv[0][0]</strong><br/><strong>convolution1d_1[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_2_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_1[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_2_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_1[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_1_s0 (Merge) (None, 1152, 256) 0 dilated_conv_2_tanh_s0[0][0]</strong><br/><strong>dilated_conv_2_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_3 (Convolution1D) (None, 1152, 256) 65536 gated_activation_1_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_2 (Merge) (None, 1152, 256) 0 merge_1[0][0]</strong><br/><strong>convolution1d_3[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_4_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_2[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_4_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_2[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_2_s0 (Merge) (None, 1152, 256) 0 dilated_conv_4_tanh_s0[0][0]</strong><br/><strong>dilated_conv_4_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_5 (Convolution1D) (None, 1152, 256) 65536 gated_activation_2_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_3 (Merge) (None, 1152, 256) 0 merge_2[0][0]</strong><br/><strong>convolution1d_5[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_8_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_3[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_8_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_3[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_3_s0 (Merge) (None, 1152, 256) 0 dilated_conv_8_tanh_s0[0][0]</strong><br/><strong>dilated_conv_8_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_7 (Convolution1D) (None, 1152, 256) 65536 gated_activation_3_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_4 (Merge) (None, 1152, 256) 0 merge_3[0][0]</strong><br/><strong>convolution1d_7[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_16_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_4[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_16_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_4[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_4_s0 (Merge) (None, 1152, 256) 0 dilated_conv_16_tanh_s0[0][0]</strong><br/><strong>dilated_conv_16_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_9 (Convolution1D) (None, 1152, 256) 65536 gated_activation_4_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_5 (Merge) (None, 1152, 256) 0 merge_4[0][0]</strong><br/><strong>convolution1d_9[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_32_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_5[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_32_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_5[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_5_s0 (Merge) (None, 1152, 256) 0 dilated_conv_32_tanh_s0[0][0]</strong><br/><strong>dilated_conv_32_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_11 (Convolution1D) (None, 1152, 256) 65536 gated_activation_5_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_6 (Merge) (None, 1152, 256) 0 merge_5[0][0]</strong><br/><strong>convolution1d_11[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_64_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_6[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_64_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_6[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_6_s0 (Merge) (None, 1152, 256) 0 dilated_conv_64_tanh_s0[0][0]</strong><br/><strong>dilated_conv_64_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_13 (Convolution1D) (None, 1152, 256) 65536 gated_activation_6_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_7 (Merge) (None, 1152, 256) 0 merge_6[0][0]</strong><br/><strong>convolution1d_13[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_128_tanh_s0 (Causal (None, 1152, 256) 131072 merge_7[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_128_sigm_s0 (Causal (None, 1152, 256) 131072 merge_7[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_7_s0 (Merge) (None, 1152, 256) 0 dilated_conv_128_tanh_s0[0][0]</strong><br/><strong>dilated_conv_128_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_15 (Convolution1D) (None, 1152, 256) 65536 gated_activation_7_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_8 (Merge) (None, 1152, 256) 0 merge_7[0][0]</strong><br/><strong>convolution1d_15[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_256_tanh_s0 (Causal (None, 1152, 256) 131072 merge_8[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_256_sigm_s0 (Causal (None, 1152, 256) 131072 merge_8[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_8_s0 (Merge) (None, 1152, 256) 0 dilated_conv_256_tanh_s0[0][0]</strong><br/><strong>dilated_conv_256_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_17 (Convolution1D) (None, 1152, 256) 65536 gated_activation_8_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_9 (Merge) (None, 1152, 256) 0 merge_8[0][0]</strong><br/><strong>convolution1d_17[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_512_tanh_s0 (Causal (None, 1152, 256) 131072 merge_9[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>dilated_conv_512_sigm_s0 (Causal (None, 1152, 256) 131072 merge_9[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>gated_activation_9_s0 (Merge) (None, 1152, 256) 0 dilated_conv_512_tanh_s0[0][0]</strong><br/><strong>dilated_conv_512_sigm_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_2 (Convolution1D) (None, 1152, 256) 65536 gated_activation_0_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_4 (Convolution1D) (None, 1152, 256) 65536 gated_activation_1_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_6 (Convolution1D) (None, 1152, 256) 65536 gated_activation_2_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_8 (Convolution1D) (None, 1152, 256) 65536 gated_activation_3_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_10 (Convolution1D) (None, 1152, 256) 65536 gated_activation_4_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_12 (Convolution1D) (None, 1152, 256) 65536 gated_activation_5_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_14 (Convolution1D) (None, 1152, 256) 65536 gated_activation_6_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_16 (Convolution1D) (None, 1152, 256) 65536 gated_activation_7_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_18 (Convolution1D) (None, 1152, 256) 65536 gated_activation_8_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_20 (Convolution1D) (None, 1152, 256) 65536 gated_activation_9_s0[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>merge_11 (Merge) (None, 1152, 256) 0 convolution1d_2[0][0]</strong><br/><strong>convolution1d_4[0][0]</strong><br/><strong>convolution1d_6[0][0]</strong><br/><strong>convolution1d_8[0][0]</strong><br/><strong>convolution1d_10[0][0]</strong><br/><strong>convolution1d_12[0][0]</strong><br/><strong>convolution1d_14[0][0]</strong><br/><strong>convolution1d_16[0][0]</strong><br/><strong>convolution1d_18[0][0]</strong><br/><strong>convolution1d_20[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>activation_1 (Activation) (None, 1152, 256) 0 merge_11[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_21 (Convolution1D) (None, 1152, 256) 65792 activation_1[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>activation_2 (Activation) (None, 1152, 256) 0 convolution1d_21[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>convolution1d_22 (Convolution1D) (None, 1152, 256) 65792 activation_2[0][0]</strong><br/><strong>____________________________________________________________________________________________________</strong><br/><strong>output_softmax (Activation) (None, 1152, 256) 0 convolution1d_22[0][0]</strong><br/><strong>====================================================================================================</strong><br/><strong>Total params: 4,129,536</strong><br/><strong>Trainable params: 4,129,536</strong><br/><strong>Non-trainable params: 0</strong>
</pre>
<p>DeepMind tried to train with data sets including multiple speakers, and this significantly improved the capacity to learn a shared representation of languages and tones and thus receive results close to natural speech. You'll find an amazing collection of examples of synthesized voice online (<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>), and it is interesting to note that the quality of audio improves when WaveNet is conditioned on additional text that is transformed into a sequence of linguistic and phonetic features in addition to audio waveforms. My favorite examples are the ones where the same sentence is pronounced by the net with different tones of voice. Of course, it is also fascinating to hear WaveNet create piano music by itself. Check it out online!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we discussed GANs. A GAN typically consists of two networks; one is trained to forge synthetic data that looks authentic, and the second is trained to discriminate authentic data against forged data. The two networks continuously compete, and in doing so, they keep improving each other. We reviewed an open source code, learning to forge MNIST and CIFAR-10 images that look authentic. In addition, we discussed WaveNet, a deep generative network proposed by Google DeepMind for teaching computers how to reproduce human voices and musical instruments with impressive quality. WaveNet directly generates raw audio with a parametric text-to-speech approach based on dilated convolutional networks. Dilated convolutional networks are a special kind of ConvNets where convolution filters have holes, allowing the receptive field to grow exponentially in depth and therefore efficiently cover thousands of audio time-steps. DeepMind showed how it is possible to use WaveNet to synthesize human voice and musical instruments, and improved previous state-of-the-art. In the next chapter, we will discuss word embeddings—a set of deep learning methodologies for detecting relations among words and grouping together similar words. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>