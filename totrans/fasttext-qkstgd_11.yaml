- en: Notes for the Readers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Windows and Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We would suggest that you use PowerShell for your windows command line as that
    is more powerful then simple `cmd`.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Windows** | **Linux**/**macOS** |'
  prefs: []
  type: TYPE_TB
- en: '| Creating a directory | `mkdir` | `mkdir` |'
  prefs: []
  type: TYPE_TB
- en: '| Change directory | `cd` | `cd` |'
  prefs: []
  type: TYPE_TB
- en: '| Move files | `move` | `mv` |'
  prefs: []
  type: TYPE_TB
- en: '| Unzip files | GUI and double click | `unzip` |'
  prefs: []
  type: TYPE_TB
- en: '| Top of the file | `get-content` | `head` |'
  prefs: []
  type: TYPE_TB
- en: '| Contents of the file | `type` | `cat` |'
  prefs: []
  type: TYPE_TB
- en: '| Piping | `this pipes objects` | `this pipes text` |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom of the file | `-wait` parameter with `get-content` | `tail` |'
  prefs: []
  type: TYPE_TB
- en: python and `perl` commands work the same way in windows as they work in bash
    and hence you can use those files and especially `perl` one liners in similar
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Python 2 and Python 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: fastText works for both Python 2 and Python 3\. There are few differences though
    that you should keep in mind for the particular python version.
  prefs: []
  type: TYPE_NORMAL
- en: '`print` is a statement in Python 2 and a function in Python 3\. This would
    mean that if you are in a Jupyter notebook and trying to see the changes in a
    variable you will need to pass the appropriate print statement in the corresponding
    python version.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fastText handles text as Unicode. Python 3 also handles text as Unicode
    and hence there is no additional overhead if you code in Python 3\. But in case
    you are developing your models in Python 2, you cannot have your data as a string
    instance. You will need to have your data as Unicode. Following is an example
    of text as an instance of the `str` class and `unicode` class in Python 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The fastText command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following is the list of parameters that you can use with fastText command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `supervised`, `skipgram`, and `cbow` commands are for training a model.
    `predict`, `predict-prob` are for predictions on a supervised model. `test`, `print-word-vectors`,
    `print-sentence-vectors`, `print-ngrams`, `nn`, analogies can be used to evaluate
    the model. The `dump` command is basically to find the hyperparameters of the
    model and `quantize` is used to the compress the model.
  prefs: []
  type: TYPE_NORMAL
- en: The list of hyperparameters that you can use for training are listed later.
  prefs: []
  type: TYPE_NORMAL
- en: The fastText supervised
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The fastText skipgram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The fastText cbow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Gensim fastText parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gensim supports the same hyperparameters that are supported in the native implementation
    of fastText. You should be able to set them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sentences`: This can be a list of list of tokens. In general, a stream of
    tokens is recommended, such as `LineSentence` from the word2vec module, as you
    have seen earlier. In the Facebook fastText library this is given by the path
    to the file and is given by the `-input` parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sg`: Either 1 or 0\. 1 means to train a skip-gram model, and 0 means to train
    a CBOW model. In the Facebook fastText library the equivalent is when you pass
    the `skipgram` and `cbow` arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: The dimensions of the word vectors and hence must be an integer. In
    line with the original implementation, 100 is chosen as default. This is similar
    to the `-dim` argument in the Facebook fastText implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window`: The window size that is considered around a word. This is the same
    as `-ws` argument in the original implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: This is the initial learning rate and is a float. It is the same parameter
    as the `-lr` as what you saw in [Chapter 2](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755),
    *Creating Models Using FastText Command Line*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_alpha`: This is the min learning rate to which the learning rate will
    drop to as the training progresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed`: This is for reproducability. For seeding to work the number of threads
    will also need to be 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_count`: Minimum frequency of words in the documents below which the words
    will be discarded. Similar to the `-minCount` parameter in the command line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_vocab_size`: This is to limit the RAM size. In case there are more unique
    words than this will prune the less frequent ones. This needs to be decided based
    on top of the RAM that you have. For example, if you have 2 GB memory then `max_vocab_size`
    needs to be 10M * 2 = 20 million (20 000 000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample`: For down sampling of words. Similar to the "-t" parameter in fasttext
    command line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workers`: Number of threads for training, similar to the `-thread` parameter
    in fastText command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hs`: Either 0 or 1\. If this is 1, then hierarchical softmax will be used
    as the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative`: If you want to use negative sampling as the loss function, then
    set `hs`=0 and negative to a non-zero positive number. Note that, there are only
    two functions that are supported for loss functions, hierarchical softmax and
    negative sampling. Simple softmax is not supported. This parameter, along with
    `hs` is the equivalent of the `-loss` parameter in the `fasttext` command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cbow_mean`: There is a difference from the fastText command here. In the original
    implementation for `cbow` the mean of the vectors are taken. But in this case
    you have the option to use the sum by passing 0 and 1 in case you want to try
    out with the mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hashfxn`: Hash function for randomly initializing the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iter`: Number of iterations or epochs over the samples. This is the same as
    the `-epoch` parameter in the command line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trim_rule`: Function to specify if certain words should be kept in the vocabulary
    or trimmed away.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sorted_vocab`: Accepted values are 1 or 0\. If 1 then the vocabulary will
    be sorted before indexing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_words`: This is the target size of the batches that are passed. The
    default value is 10000\. This is a bit similar to the `-lrUpdateRate` in the command
    line as the number of batches determine when the weights will be updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_n` and `max_n`: Minimum and maximum length of the character n-grams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_ngrams`: Enriches subword information for use in the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bucket: The character n-grams are hashed on to a vector of fixed size. By default
    bucket size of 2 million words are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callbacks`: A list of callback functions to be executed at specific stages
    of the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
