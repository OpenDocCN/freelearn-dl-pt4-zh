- en: Predicting Apple Stock Market Cost with LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stock market predictions have been going on for many years and it has spawned
    an entire industry of prognosticators. It shouldn't come as a surprise since it
    can turn a significant profit if predicted properly. Understanding when is a good
    time to buy or sell a stock is key to getting the upper hand on Wall Street. This
    chapter will focus on creating a deep learning model using LSTM on Keras to predict
    the stock market quote of AAPL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading stock market data for Apple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and visualizing stock market data for Apple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing stock data for model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading stock market data for Apple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many resources for downloading stock market data for Apple. For our
    purposes, we will be using the Yahoo! Finance website.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will require initializing a Spark cluster that will be used for
    all recipes in this chapter. A Spark notebook can be initialized in the terminal
    using `sparknotebook`, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9cd9f51-9207-4cc3-8288-355f18c47dff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A `SparkSession` can be initialized in a Jupyter notebook using the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section walks through the steps for downloading historical stock
    market data for Apple.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the following website to track the daily historical adjusted closing stock
    value for Apple, which has a stock ticker value of AAPL: [https://finance.yahoo.com/quote/AAPL/history](https://finance.yahoo.com/quote/AAPL/history)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set and apply the following parameters to the Historical Data tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Time Period: Jan 01, 2000 - Apr 30, 2018.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Show: Historical prices.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Frequency: Daily.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the dataset with the specified parameter to a `.csv` file by clicking
    on the Download Data link, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75a7842c-6816-4485-8815-282c4f04f4c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Download the file, `AAPL.csv`, and then upload the same dataset to a Spark
    dataframe using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section explains how the stock market data is incorporated into
    a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Yahoo! Finance is a great source for stock market quotes for publicly traded
    companies. The stock quote for Apple, AAPL, is traded on NASDAQ and the historical
    quotes can be captured for model development and analysis purposes. Yahoo! Finance
    gives you the option to capture stock quotes on a daily, weekly, or monthly snapshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The purpose of this chapter is to forecast stock at a daily level, as that would
    pull in the most amount of data into our training model. We can do this by tracing
    data back to January 1, 2000, all the way to April 30, 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once our parameters are set for download, we receive a nicely formatted comma-separated
    value file from Yahoo! Finance that can be easily converted into a Spark dataframe
    with minimal issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dataframe will allow us to view the Date, Open, High, Low, Close, Adj Close,
    and Volume of the stock on a daily basis. The columns in the dataframe track the
    opening and closing stock values as well as the highest and lowest values traded
    during that day. The number of shares traded during the day is also captured.
    The output of the Spark dataframe, `df`, can be shown by executing `df.show()`,
    as you can see in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5f01e2e7-00ae-4314-886f-bec781945611.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python had stock market APIs that allowed you to automatically connect and pull
    back stock market quotes for publicly traded companies such as Apple.   You would
    be required to input parameters and retrieve the data that can be stored in a
    dataframe. However, as of April 2018, the *Yahoo! Finance* API is no longer operational
    and therefore not a reliable solution for extracting data for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Pandas_datareader` is a very powerful library for extracting data from websites
    such as Yahoo! Finance. To learn more about the library and how it may connect
    back to Yahoo! Finance once it is back online, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/pydata/pandas-datareader](https://github.com/pydata/pandas-datareader)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing stock market data for Apple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before any modeling and predictions are performed on the data, it is important
    to first explore and visualize the data at hand for any hidden gems.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pyspark.sql.functions`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section walks through the steps to explore and visualize the stock
    market data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the `Date` column in the dataframe by removing the timestamp using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a for-cycle to add three additional columns to the dataframe. The loop
    breaks apart the `date` field into `year`, `month`, and `day`, as seen in the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Save a subset of the Spark dataframe to a `pandas` dataframe called `df_plot`
    using the following script: `df_plot = df.select('year', 'Adj Close').toPandas()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Graph and visualize the `pandas` dataframe, `df_plot`, inside of the notebook
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Calculate the row and column count of our Spark dataframe using the following
    script: `df.toPandas().shape`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the following script to determine null values in the dataframe: `df.dropna().count()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following script to pull back statistics on `Open`, `High`, `Low`,
    `Close`, and `Adj Close`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section explains the techniques used and insights gained from
    exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The date column in the dataframe is more of a date-time column with the time
    values all ending in 00:00:00\. This is unnecessary for what we will need during
    our modeling and therefore can be removed from the dataset. Luckily for us, PySpark
    has a `to_date` function that can do this quite easily. The dataframe, `df`, is
    transformed using the `withColumn()` function and now only shows the date column
    without the timestamp, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7e4db493-73b8-4a4d-a52d-c918ea13998d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For analysis purposes, we want to extract the `day`, `month`, and `year` from
    the `date` column. We can do this by enumerating through a custom list, `date_breakdown`,
    to split the date by a `-` and then adding a new column for the year, month, and
    day using the `withColumn()` function. The updated dataframe with the newly added
    columns can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f9a7a8a6-a99e-4f5c-8416-cdab2bc52514.png)'
  prefs: []
  type: TYPE_IMG
- en: One important takeaway is that `PySpark` also has a SQL function for dates that
    can extract the day, month, or year from a date timestamp. For example, if we
    were to add a month column to our dataframe, we would use the following script: `df.withColumn("month",f.month("date")).show()`.
    This is to highlight the fact that there are multiple ways to transform data within
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Spark dataframes are more limited in visualization features than `pandas` dataframes.
    Therefore, we will subset two columns from the Spark dataframe, `df`, and convert
    them into a `pandas` dataframe for plotting a line or time-series chart. The y-axis
    will be the adjusted close of the stock and the x-axis will be the year of the
    date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The pandas dataframe, df_plot, is ready to be plotted using matplotlib once
    some formatting features are set, such as the grid visibility, the figure size
    of the plot, and the labels for the title and axes. Additionally, we explicitly
    state that the index of the dataframe needs to point to the year column. Otherwise,
    the default index will appear on the x-axis and not the year. The final time-series
    plot can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/27142593-3862-40c6-80df-ce03e2e90c81.png)'
  prefs: []
  type: TYPE_IMG
- en: Apple has experienced extensive growth over the last 18 years. While a few years
    saw some downward dips, the overall trend has been a steady upward move with the
    last couple of year's stock quotes hovering between $150 and $175.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have made some changes to our dataframe so far, so it is important to get
    an inventory count of the rows and columns total as this will affect how the dataset
    is broken up for testing and training purposes later on in the chapter. As can
    be seen in the following screenshot, we have a total of 10 columns and 4,610 rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8a8ef246-4ce2-414d-8973-3b7d20f45564.png)'
  prefs: []
  type: TYPE_IMG
- en: When executing `df.dropna().count()`, we can see that the row count is still
    4,610, which is identical to the row count from the previous step, indicating
    that none of the rows have any null values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we can get a good read on the row count, mean, standard deviation,
    minimum, and maximum values of each of the columns that will be used in the model.
    This can help to identify whether there are anomalies in the data. One important
    thing to note is that each of the five fields that will be used in the model has
    a standard deviation higher than the mean value, indicating that the data is more
    spread out and not so clustered around the mean. The statistics for Open, High,
    Low, Close, and Adj Close can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2df0f754-2582-417d-8429-2fc79b0cd4dd.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While dataframes in Spark do not have the same native visualization features
    that are found in `pandas` dataframes, there are companies that manage Spark for
    enterprise solutions that allow for advanced visualization capabilities through
    notebooks without having to use libraries such as `matplotlib`. Databricks is
    one such company that offers this feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a visualization using the built-in features
    available in notebooks from Databricks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc9a047b-540b-473d-902e-cd83c02c0743.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about Databricks in general, visit the following website: [https://databricks.com/](https://databricks.com/).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about visualizations in Databricks notebooks, visit the following
    website: [https://docs.databricks.com/user-guide/visualizations/index.html](https://docs.databricks.com/user-guide/visualizations/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about accessing Databricks through a Microsoft Azure subscription,
    visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://azure.microsoft.com/en-us/services/databricks/](https://azure.microsoft.com/en-us/services/databricks/)'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing stock data for model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are almost ready to build a prediction algorithm for the stock value performance
    of Apple. The remaining task at hand is to prepare the data in a manner that ensures
    the best possible predictive outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MinMaxScaler()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps for preparing the stock market data for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to group the year column by the `Adj Close` count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to create two new dataframes for training and
    testing purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the two new dataframes  to `pandas` dataframes to get row and column
    counts with `toPandas()` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did previously with `df`, we visualize `trainDF` and `testDF` using the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We create two new arrays, `trainArray` and `testArray`, based on the dataframes
    with the exception of the date columns using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to scale the arrays between 0 and 1, import `MinMaxScaler` from `sklearn` and
    create a function call, `MinMaxScale`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`MinMaxScaler` is then fit on the `trainArray` and used to create two new arrays
    that are scaled to fit using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Split both `testingArray` and `trainingArray`  into features, `x`, and label,
    `y`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to retrieve a final inventory of the shape of
    all four arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to plot the training array for the quotes `open`,
    `high`, `low`, and `close` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we plot the training array for `volume` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the transformations needed on the data to be used in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first steps to building a model is splitting the data into a training
    and test dataset for model evaluation purposes. Our goal is to use all of the
    stock quotes from 2000 through 2016 to predict stock trends in 2017-2018\. We
    know from previous sections that we have a total of 4,610 days of stock quotes,
    but we don''t know exactly how many fall in each year. We can use the `groupBy()`
    function within the dataframe to get a unique count of stock quotes per year,
    as can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/94a6f06e-e420-4c98-af2d-47734b6fe9b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 2016 and 2017's combined data represents approximately 7% of the total data,
    which is a bit small for a testing dataset. However, for the purposes of this
    model, it should be sufficient. The remaining 93% of the dataset will be used
    for training purposes between 2000 and 2016\. Therefore, two dataframes are created
    using a filter to determine whether to include or exclude rows before or after
    2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now see that the test dataset, `testDF`, contains 333 rows and that
    the training dataset, `trainDF`, contains 4,277 rows. When both are combined,
    we reach our total row count from our original dataframe, `df`, of 4,610\. Finally,
    we see that `testDF` is comprised of 2017 and 2018 data only, which is 251 rows
    for 2017 and 82 rows for 2018 for a total of 333 rows, as can be seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f249625c-a120-4cd6-91a0-59562546f47d.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that anytime we are converting a Spark dataframe to a `pandas` dataframe
    it may not always scale for big data.  While it will work for our specific example
    as we are using a relatively small dataset, the conversion to a `pandas` dataframe
    means that all of the data is loaded into the memory of the driver.  Once this
    conversion occurs, the data is not stored in the Spark worker nodes but is instead
    to the main driver node.  This is not optimal and may produce an out of memory
    error.  If you find that you need to convert to a `pandas` dataframe from Spark
    to visualize data it is recommended to pull a random sample from Spark or to aggregate
    the spark data to a more manageable dataset and then visualize in `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both testing and training dataframes can be visualized using `matplotlib` once
    a subset of the data is converted using `toPandas()` to leverage the built-in
    graphing capabilities of `pandas`. Visualizing the dataframes side by side showcases
    how the graphs appear to be similar when the y-axis for adjusted close is not
    scaled. In reality, we can see that `trainDF_plot` starts close to 0, but `testDF_plot`
    starts closer to 110, as seen in the following two screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bad4a77f-af78-4ec0-8600-1ce08fbd58df.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d0c2e709-8238-47a5-8624-9875b5aaea6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our stock values, as they stand, don''t lend themselves well to deep learning
    modeling because there isn''t a baseline for normalization or standardization.
    When working with neural networks, it is best to keep the values between 0 and
    1 to match outcomes found in sigmoid or step functions that are used for activation.
    In order for us to accomplish this, we must first convert our `pyspark` dataframes,
    `trainDF` and `testDF`, into `numpy` arrays, these being `trainArray` and `testArray`.
    As these are now arrays and not dataframes, we will not be using the date column
    as the neural network is only interested in numerical values. The first values
    in each can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/839b0580-face-47d5-b89e-2d0401361448.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many ways to scale array values to a range between 0 and 1\. It involves
    using the following formula: `scaled array value = (array value - min array value)
    / (max array value - min array value)`. Fortunately, we do not need to manually
    make this calculation on arrays. We can leverage the `MinMaxScaler()` function
    from `sklearn` to scale down both arrays.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `MinMaxScaler()` function is fit on the training array, `trainArray`, and
    is then applied to create two brand new arrays, `trainingArray` and `testingArray`,
    that are scaled to values between 0 and 1\. The first row for each array can be
    seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0ac5852c-0e03-464c-b5cd-3074bf12f275.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now ready to set our label and feature variables by slicing up the array
    into x and y for both testing and training purposes. The first five elements in
    the array are the features or the x values and the last element is the label or
    y value. The features are composed of the values from Open, High, Low, Close,
    and Volume. The label is composed of Adj Close. The breakout of the first row
    for `trainingArray` can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a2e0b0e3-f4b8-49a0-83b8-163b4fcb4f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A final look at the shape of the four arrays that we will be using in the model
    can be used to confirm that we have 4,227 matrix rows of training data, 333 matrix
    rows of test data, 5 elements for features (`x`), and 1 element for the label
    (`y`), as can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b6e38d99-ebe8-48a3-a186-3089a8074a93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values for the training array, `xtrain`, for open, low, high, and close
    can be plotted using the newly adjusted scales between 0 and 1 for the quotes,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/240dd4a3-03f5-48ad-8ec5-079c745060b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, to volume can also be plotted with the scaled volume scores between
    0 and 1, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0e927f61-bd08-42be-9487-cac94c83908f.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we did use `MinMaxScaler` from `sklearn`, it is also important to understand
    that there is also a `MinMaxScaler` function that is available directly through
    `pyspark.ml.feature`. It works exactly the same way by rescaling each feature
    to a value between 0 and 1\. Had we used a machine learning library natively through
    PySpark in this chapter to make our prediction, we would have used `MinMaxScaler`
    from `pyspark.ml.feature`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about `MinMaxScaler` from `sklearn`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about `MinMaxScaler` from `pyspark`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler)'
  prefs: []
  type: TYPE_NORMAL
- en: Building the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is now in a format compatible with model development in Keras for LSTM
    modeling. Therefore, we will spend this section setting up and configuring the
    deep learning model for predicting stock quotes for Apple in 2017 and 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform model management and hyperparameter tuning of our model in
    this section. This will require importing the following libraries in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to setting up and tuning the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries from `keras` using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a `Sequential` model using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the testing and training data sets into three-dimensional arrays
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the  `model` using a variable called `loss` with the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new array, `predicted`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine the `predicted` and `ytest` arrays into a single unified array, `combined_array`,
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the LSTM neural network model is configured to train
    on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the functionality from `keras` used to build the LSTM model will come
    from `models` and `layers`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `LSTM` model that has been built will be defined using a `Sequential` class
    that works well with time series that are sequence dependent. The LSTM model has
    an `input_shape = (1,5)` for one dependent variable and five independent variables
    in our training dataset. Only one `Dense` layer will be used to define the neural
    network as we are looking to keep the model simple. A loss function is required
    when compiling a model in keras, and since we are performing it on a recurrent
    neural network, a `mean_squared_error` calculation is best to determine how close
    the predicted value is to the actual value. Finally, an optimizer is also defined
    when the model is compiled to adjust the weights in the neural network. `adam`
    has given good results, especially when being used with recurrent neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our current arrays, `xtrain` and `xtest`, are currently two-dimensional arrays;
    however, to incorporate them into the LSTM model, they will need to be converted
    to three-dimensional arrays using `reshape()`, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ed6bb442-5b2e-4367-b453-589ade0a6c89.png)'
  prefs: []
  type: TYPE_IMG
- en: The LSTM model is fit with `xtrain` and `ytrain` and the batch size is set to
    10 with 100 epochs. The batch size is the setting that defines the number of objects
    that are trained together. We can go as low or as high as we like in terms of
    setting the batch size, keeping in mind that the lower the number of batches,
    the more memory is required. Additionally, an epoch is a measurement of how often
    the model goes through the entire dataset. Ultimately, these parameters can be
    tuned based on time and memory allotment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The mean squared error loss in each epoch is captured and visualized. After
    the fifth or sixth epoch, we can see that the loss tapers off, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5227945e-55d2-4d8f-869c-bb319f9a88b8.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now create a new array, `predicted`, based on the fitted model applied
    on `xtest` and then combine it with `ytest` to compare them side by side for accuracy
    purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about parameter tuning models within keras, visit the following
    website: [https://keras.io/models/model/](https://keras.io/models/model/)
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the moment of truth: we are going to see if our model is able to give
    us a good prediction for the AAPL stock in 2017 and 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform a model evaluation using the mean squared error. Therefore,
    we will need to import the following library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through visualizing and calculating the predicted vs. actual
    stock quotes for Apple in 2017 and 2018.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a side by side comparison of `Actual` versus `Predicted` stock to compare
    trends using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the mean squared error between the actual `ytest` versus `predicted` stock
    using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the results of the LSTM model's evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a graphical perspective, we can see that our predictions were close to
    the actual stock quotes from 2017-2018, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/59aa966a-f8d8-4990-83ca-adb1e7eacc6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our model shows that the predicted values are closer to the actual values earlier
    on in the days for 2017 and 2018 than later on.  Overall, while it seems that
    our predicted and actual scores are very close, it would be best to get a mean
    squared error calculation to understand how much deviation is between the two.
    As we can see, we have a mean squared error of 0.05841 or approximately 5.8%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/38579afd-997d-47c8-a733-538b06ae4ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to learn more about how the mean squared error is calculated within
    sklearn, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).'
  prefs: []
  type: TYPE_NORMAL
