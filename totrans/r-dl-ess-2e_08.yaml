- en: Deep Learning Models Using TensorFlow in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is about using TensorFlow in R. We have already used TensorFlow
    quite a lot, as Keras is a high-level neural network API that uses either TensorFlow,
    CNTK, or Theano. In R, Keras uses TensorFlow in the background. TensorFlow is
    more difficult to develop deep learning models in. However, there are two interesting
    packages in TensorFlow that could be overlooked: TensorFlow estimators and TensorFlow
    runs. We will cover both of these packages in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building models using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow runs packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the TensorFlow library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is not just a deep learning library, but an expressive programming
    language that can implement various optimization and mathematical transformations
    on data. While it is mainly used to implement deep learning algorithms, it can
    perform much more. In TensorFlow, programs are represented as computational graphs,
    and data in TensorFlow is stored in `tensors`. A **tensor** is an array of data
    that has the same data type, and the rank of a tensor is the number of dimensions.
    Because all the data in a tensor must have the same type, they are more similar
    to R matrices than data frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of tensors of various ranks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A TensorFlow program has two parts. First, you have to build the computational
    graph, which contains the tensors and the operations on those tensors. When they
    have defined the graph, the second part is to create a TensorFlow session to run
    the graph. In the previous example, the first time we print out the value for
    the tensor, `a`, we only get the tensor definition and not the value. All we have
    done is define part of the computation graph. It is only when we call `tf$InteractiveSession`
    that we tell TensorFlow to run the operations on the tensors. A session is responsible
    for running the computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow program is referred to as a graph because the code can be structured
    as a graph. This might not be obvious to us as most of the deep learning models
    that we have built in this book have consisted of sequential operations on layers.
    In TensorFlow (and Keras and MXNet), it is possible to use the output of an operation
    multiple times and to combine inputs in one operation.
  prefs: []
  type: TYPE_NORMAL
- en: As deep learning models get larger, it is increasingly difficult to visualize
    and debug them. In some code blocks, we have printed a summary of the model showing
    the layers, or we have plotted the network. However, neither of these tools would
    be helpful for debugging problems in a model with 10 million+ parameters! Fortunately,
    there is a visualization tool included with TensorFlow to help summarize, debug,
    and fix TensorFlow programs. This is called TensorBoard, and we will cover this
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard to visualize deep learning networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computation graphs in TensorFlow can be very complex, so there is a visualization
    tool called **TensorBoard** to visualize these graphs and assist in debugging.
    TensorBoard can plot a computation graph, display metrics from training, and so
    on. Since Keras uses TensorFlow in the backend, it too can use TensorBoard. Here
    is the MNIST example from Keras with TensorBoard logging enabled. This code can
    be found in the `Chapter8/mnist_keras.R` folder. The first part of the code loads
    the data, pre-processes it, and defines the model architecture. Hopefully, this
    should be familiar to you at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable logging, add a `callbacks` parameter to the `model.fit` function
    to tell Keras/TensorFlow to log events to a directory. The following code will
    output log data to the `/tensorflow_logs` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Warning**: The event logs can take up a lot of space. For 5 epochs on the
    `MNIST` dataset, 1.75 GB of information was created. Most of this was because
    of the image data that was included, so you may consider setting `write_images=0`
    to reduce the size of the logs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard is a web application, and you must start the TensorBoard program
    for it to run. When the model has finished training, follow these steps to start
    the TensorBoard web application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up Command Prompt and enter the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If TensorBoard starts successfully, you should get a message similar to the
    following at Command Prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a web browser to the link that was provided. The web page should be similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2c775aa3-5ea3-40d4-9bdd-b2dfaf3e5786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: TensorBoard – model metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows us the model metrics on the training and validation
    test sets – these are similar to the metrics shown in RStudio during training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c4be3ad7-5e78-4f42-9ef6-3fc0645d3843.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: RStudio – model metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the **Images** option, you will be able to visualize the layers
    in the model and see how they change over epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c9a63540-797c-45c4-bb53-bccdd90bb7ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: TensorBoard – visualizing the model layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the **Graphs** option, it will show the computation graph for
    the model. You can also download it as an image file. Here is the computation
    graph for this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/85274738-c4f0-4804-9588-550900583db8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: TensorBoard – computation graph'
  prefs: []
  type: TYPE_NORMAL
- en: Some of this will seem familiar. We can see our convolutional, max pooling,
    flatten, dense, and dropout layers. The rest are not as obvious. As a higher-level
    abstraction, Keras takes care of a lot of the complexities in creating the computation
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking on the **Histogram** option, you can see how the distribution of
    tensors changes over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7d0685a1-da65-43e7-b00c-6afaa95f4abc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: TensorBoard – histograms'
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to use TensorBoard to debug a model. For example, it would be
    possible to investigate a vanishing gradient or an exploding gradient problem
    to see where the weights of the model were either vanishing to zero or exploding
    to infinity. There is a lot more to TensorBoard, so if you are curious you can
    follow the online documentation on it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use TensorFlow to build a regression model and
    a convolutional neural network.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use TensorFlow to build some machine learning models.
    First, we will build a simple linear regression model and then a convolutional
    neural network model, similar to what we have seen in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code loads the TensorFlow library. We can confirm it loaded successfully
    by setting and accessing a constant string value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Linear regression using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this first Tensorflow example, we will look at regression. The code for
    this section is in the `Chapter8/regression_tf.R` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create some fake data for an an input value, *x*, and an output value,
    *y*. We set *y* to be approximately equal to `0.8 + x * 1.3`. We want the application
    to discover the `beta0` and `beta1` values, which are `0.8` and `1.3`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we set up our `loss` function so that the gradient descent algorithm can
    work:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then set up a TensorFlow session and initialize the variables. Finally,
    we can run the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the model manages to find the values for `beta0` and `beta1`
    that solve the function `y=beta0 + beta1*x`. The next section is a more more complex
    example, where we will build a TensorFlow model for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a TensorFlow model on the MNIST dataset. The
    code has similar layers and parameters to the Lenet model that we saw in [Chapter
    5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification Using Convolutional
    Neural Networks*. However, the code to build the model in TensorFlow is more complicated
    than the code to build the model in Keras or in MXNet. One reason for this is
    that it is the programmer's job to ensure that the sizes of the layers are correctly
    aligned. In the Keras/MXNet models, we can just change the number of nodes in
    a layer in one statement. In TensorFlow, if we change the number of nodes in a
    layer, we must ensure that we also change the inputs in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some ways, programming in TensorFlow is closer to the hand-written neural
    network code we wrote in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),
    *Deep Learning Fundamentals*. Another difference from Keras/MXNet in the training
    loop is that we need to manage the batches rather than just call, asking to iterate
    over all the data *x* times (where *x* is an epoch). The code for this example
    is in the `Chapter8/mnist_tf.R` folder. First, we load the Keras package to get
    the MNIST data, but we train the model using TensorFlow. Here is the first part
    of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We use the `decodeClassLabels` function from the RSNNS library because TensorFlow
    requires a dummy coded matrix, so each possible class is represented as a column
    coded as 0/1, as shown in the preceding code output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code block, we create some placeholders for our input and output
    values in the model. We also reshape the input data into a rank-4 tensor, that
    is, a 4 dimensional data structure. The first dimension (-1L) is for the records
    that will be processed in a batch. The next two dimensions are the dimensions
    of the image files, and the final dimension is the channels, which is the number
    of colors. Since our images are greyscale, there is only 1 channel. If the images
    were color images, there would be 3 channels. The following code block creates
    the placeholders and reshapes the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the model architecture. We will create convolution blocks,
    just like we did previously. However, there are a lot more values that need to
    be set. For example, in the first convolutional layer, we must define the shape,
    initialize the weights, and take care of the bias variable. Here is the code for
    the TensorFlow model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to define the loss equation, define the optimizer to use (Adam),
    and define the accuracy metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train the model over 10 epochs. However, one complication still
    exists, so we must manually manage the batches. We get the number of batches to
    train for and load them in turn. If we have 60,000 images in our train dataset,
    we have 469 batches (60,000/128 = 468.75 and round up to 469) per epoch. We feed
    in every batch and output metrics every 100 batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output for the first epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When training is complete, we can evaluate the model by calculating the accuracy
    on the test set. Again, we have to do this in batches to prevent out-of-memory
    errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We get a final accuracy of `0.9802`. If we compare this code to the MNIST example
    in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),* Image Classification
    Using Convolutional Neural Networks*, the TensorFlow code is more verbose and
    it is easier to make mistakes. We can really see the benefit of using a higher,
    level abstraction, such as MXNet or Keras (which can use TensorFlow as a backend).
    For most deep learning use cases, especially for building deep learning models
    using existing layers as building blocks, there is little to be gained in developing
    code in TensorFlow. For these use cases, it is simpler and more efficient to use
    Keras or MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: After seeing this code, you may want to go back to something more familiar in
    Keras and MXNet. However, the next section looks at TensorFlow estimators and
    TensorFlow runs, which are two useful packages that you should be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow estimators and TensorFlow runs packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow estimators and the TensorFlow runs packages are great packages to
    use for deep learning. In this section, we will use both to train a model based
    on our churn prediction data from [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),
    *Training Deep Prediction Models*.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow estimators** allow you to build TensorFlow models using a simpler
    API interface. In R, the `tfestimators` package allows you to call this API. There
    are different model types, including linear models and neural networks. The following
    estimators are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`linear_regressor()` for linear regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linear_classifier()` for linear classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnn_regressor()` for deep neural network regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnn_classifier()` for deep neural network classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnn_linear_combined_regressor()` for deep neural network linear combined regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnn_linear_combined_classifier()` for deep neural network linear combined
    classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimators hide a lot of the detail in creating a deep learning model, including building
    the graph, initializing variables and layers, and they can also integrate with TensorBoard.
    More details are available at [https://tensorflow.rstudio.com/tfestimators/](https://tensorflow.rstudio.com/tfestimators/).
    We will use `dnn_classifier` with the data from the binary classification task
    from [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),* Training Deep Prediction
    Models*. The following code in the `Chapter8/tf_estimators.R` folder demonstrates TensorFlow
    estimators.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only include the code that is specific to TensorFlow estimators and omit
    the code at the start of the file that loads the data and splits it into train
    and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is trained, the following code plots the training and validation
    metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9b2eefd3-08c8-40d2-ad7d-4ee4b0250033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Training a loss plot for a TensorFlow estimator model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the code calls the `evaluate` function to produce metrics
    for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we got an accuracy of `77.57%`, which is actually almost identical
    to the accuracy we got on the MXNet model in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),
    *Training Deep Prediction Models*, which had a similar architecture. The `dnn_classifier()`
    function hides a lot of the detail, so Tensorflow estimators are a good way to
    use the power of TensorFlow for tasks with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Models created using TensorFlow estimators can be saved onto disk and loaded
    later. The `model_dir()` function shows the location of where the model artifacts
    were saved (usually in a `temp` directory, but it can be copied elsewhere):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Included in the model artifacts are the event logs that can be used by TensorBoard.
    For example, when I load TensorBoard up and point it to the logs directory in
    the `temp` directory, I can see the TensorFlow graph that was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3037cf0-3e7e-4247-8606-2b0b4c550baf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Graph using TensorBoard for a TensorFlow estimator model'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow runs package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tfruns` package is a set of utilities for managing different training runs
    for deep learning models. It can be used as a framework to build multiple deep
    learning models using different hyper-parameters. It can track the hyper-parameters,
    metrics, output, and source code for every training run and allows you to compare
    the best models so that you can see the differences between the training runs.
    This makes hyper-parameter tuning much easier and can be used with any `tfestimator`
    model or `Keras` model. For more details, go to [https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html](https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is in the `Chapter8/hyperparams.R` folder and also uses
    the script we used in the *TensorFlow estimators* section (`Chapter8/tf_estimators.R`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will run the `Chapter8/tf_estimators.R` script with different hyper-parameters.
    The first time, we don't change any hyper-parameters, so it uses the defaults
    included in `Chapter8/tf_estimators.R`. Each time a new model is trained using
    the classification script, it is called a **training r****un**, and the details
    of the training run is stored in the `runs` folder in the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each training run, a new website will pop up with details on the run, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320da535-5b81-4e24-9072-6c2c4ee07181.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: TensorFlow training run – Summary screen'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the progress of the training in the plots, along with the details
    of when the training run occurred and the evaluation metrics. We can also see
    in the bottom right that the **flags** (that is, hyper-parameters) used in the
    training run are also shown. There is another tab for the R code output, which
    includes all of the output from the R code in the inner file (`Chapter8/tf_estimators.R`),
    including plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all of the training runs are complete, the following code shows a summary
    of all the training runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have ordered the results by the column `eval_accuracy`. If you close
    the window showing the summary for the training run, you can display it again by
    calling the `view_run` function and passing in the folder name. For example, to
    show the summary for the best training run, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can also compare two runs. Here, we are comparing the two best
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This brings up a page similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e006024e-8f31-4080-8a20-b5e7f81e6949.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Comparing two TensorFlow runs'
  prefs: []
  type: TYPE_NORMAL
- en: This page shows the evaluation metrics for both training runs and also displays
    the hyper-parameters that were used. As we can see, this makes managing the process
    of tuning deep learning models much easier. This approach to hyper-parameter tuning
    has automatic logging, traceability, and it is easy to compare different sets
    of hyper-parameters. You can see the metrics and the different hyper-parameters
    used for the training runs. There's no more comparing configuration files to try
    and match hyper-parameter settings to output logs! In comparison, the code I wrote
    for hyper-parameter selection for the NLP example in [Chapter 7](03f666ab-60ce-485a-8090-c158b29ef306.xhtml),
    *Natural Language Processing Using Deep Learning*, seems crude in comparison
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we developed some TensorFlow models. We looked at TensorBoard,
    which is a great tool for visualizing and debugging deep learning models. We built
    a couple of models using TensorFlow, including a basic regression model and a Lenet
    model for computer vision models. From these examples, we saw that programming
    in TensorFlow was more complicated and error-prone than using the higher-level
    APIs (MXNet and Keras) that we used elsewhere in this book.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved onto using TensorFlow estimators, which is a much easier interface
    than using TensorFlow. We then used that script in another package called **tfruns**,
    which stands for TensorFlow runs. This package allows us to call a TensorFlow
    estimators or Keras script with different flags each time. We used this for hyper-parameter
    selection, running, and evaluating multiple models. The TensorFlow runs have excellent
    integration with RStudio and we were able to view summaries for each run and compare
    runs to see the difference in the metrics and hyper-parameters that were used.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we we will look at embeddings and auto-encoders. We have
    already seen embeddings in [Chapter 7](03f666ab-60ce-485a-8090-c158b29ef306.xhtml),
    *Natural Language Processing Using Deep Learning*, so in the next chapter we will
    see how embeddings can create a lower level encoding of data. We will also use
    train auto-encoders, which create these embeddings. We will use auto-encoders
    for anomaly detection and also for collaborative filtering (recommender system).
  prefs: []
  type: TYPE_NORMAL
