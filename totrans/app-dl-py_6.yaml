- en: Model Evaluation and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focuses on how to evaluate a neural network model. Different than
    working with other kinds of models, when working with neural networks, we modify
    the network's hyper parameters to improve its performance. However, before altering
    any parameters, we need to measure how the model performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the types of problems addressed by neural networks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore loss functions, accuracy, and error rates
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use TensorBoard
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate metrics and techniques
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add layers and nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore and add epochs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement activation functions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use regularization strategies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In machine learning, it is common to define two distinct terms: parameter and
    hyper **parameter**. Parameters are properties that affect how a model makes predictions
    from data. Hyper parameters refer to how a model learns from data. Parameters
    can be learned from the data and modified dynamically. Hyper parameters are higher-level
    properties and are not typically learned from data. For a more detailed overview,
    refer to the book Python Machine Learning, by Sebastian Raschka and Vahid Mirjalili
    (Packt, 2017).'
  prefs: []
  type: TYPE_NORMAL
- en: Problem Categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally, there are two categories of problems solved by neural networks:
    classification and regression. Classification problems regard the prediction of
    the right categories from data; for instance, if the temperature is hot or cold.
    Regression problems are about the prediction of values in a continuous scalar;
    for instance, what the actual temperature value is?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problems in these two categories are characterized by the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: Problems that are characterized by categories. The categories
    can be different, or not; they can also be about a binary problem. However, they
    must be clearly assigned to each data element. An example of a classification
    problem would be to assign the label *car* or *not car* to an image using a Convolutional
    Neural Network. The MNIST example explored in C*hapter 4*, *Introduction to Neural
    Networks and Deep Learning*, is another example of a classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Problems that are characterized by a continuous variable (that
    is, a scalar). These problems are measured in terms of ranges, and their evaluations
    regard how close to the real values the network is. An example is a time-series
    classification problem in which a Recurrent Neural Network is used to predict
    the future temperature values. The Bitcoin price-prediction problem is another
    example of a regression problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the overall structure of how to evaluate these models is the same for
    both of these problem categories, we employ different techniques for evaluating
    how models perform. In the following section, we explore these techniques for
    either classification or regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: All of the code snippets in this chapter are implemented in *Activities 6 and
    7*. Feel free to follow along, but don't feel that it is mandatory, given that
    they will be repeated in more detail during the activities.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions, Accuracy, and Error Rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks utilize functions that measure how the networks perform when
    compared to a validation set—that is, a part of the data separated to be used
    as part of the training process. These functions are called **loss functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions evaluate how *wrong* a neural network's predictions are; then
    they will propagate those errors back and make adjustments to the network, modifying
    how individual neurons are activated. Loss functions are key components of neural
    networks, and choosing the right loss function can have a significant impact on
    how the network performs.
  prefs: []
  type: TYPE_NORMAL
- en: How are errors propagated to each neuron in a network?
  prefs: []
  type: TYPE_NORMAL
- en: Errors are propagated via a process called back propagation. Back propagation
    is a technique for propagating the errors returned by the loss function back to
    each neuron in a neural network. Propagated errors affect how neurons activate,
    and ultimately, how they influence the output of that network.
  prefs: []
  type: TYPE_NORMAL
- en: Many neural network packages, including Keras, use this technique by default.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the mathematics of backpropagation, please refer
    to *Deep Learning* by Ian Goodfellow et. al., MIT Press, 2016.
  prefs: []
  type: TYPE_NORMAL
- en: We use different loss functions for regression and classification problems.
    For classification problems, we use accuracy functions (that is, the proportion
    of times the predictions were correct). While for regression problems, we use
    error rates (that is, how close the predicted values were to the observed ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides a summary of common loss functions to utilize,
    alongside their common applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Problem Type**  | **Loss Function**  | **Problem**  | **Example**  |'
  prefs: []
  type: TYPE_TB
- en: '| Regression  | Mean Squared Error (MSE)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Predicting a continuous function. That is, predicting value within a range of
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Predicting the temperature in the future using temperature measurements from
    the past.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regression  | Root Mean Squared Error(RMSE)  | Same as preceding, but deals
    with negative values. RMSE typically provides more interpretable results.  | Same
    as preceding.  |'
  prefs: []
  type: TYPE_TB
- en: '| Regression  | Mean Absolute Percentage Error'
  prefs: []
  type: TYPE_NORMAL
- en: (MAPE)
  prefs: []
  type: TYPE_NORMAL
- en: '| Prediction continuous  functions. Has better in performance when working with
    de-normalized ranges.  | Predicting the sales for a product using the product properties
    (for example, price, type, target audience, market conditions).'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification  | Binary Cross entropy  |  Classification between two  categories
    or between two'
  prefs: []
  type: TYPE_NORMAL
- en: values (that is, `true`   or `false`).
  prefs: []
  type: TYPE_NORMAL
- en: '| Predicting if the visitor of a website is male or female based on their browser
    activity.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification  | Categorical Cross-entropy'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification between many categories from a known set'
  prefs: []
  type: TYPE_NORMAL
- en: of categories.
  prefs: []
  type: TYPE_NORMAL
- en: '| Predicting the nationality of a speaker based on their accent when speaking
    a sentence in English.  |'
  prefs: []
  type: TYPE_TB
- en: For regression problems, the MSE function is the most common choice. While for
    classification problems, Binary Cross-entropy (for binary category problems) and
    Categorical Cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems, the MSE function is the most common choice. While for
    classification problems, Binary Cross-entropy (for binary category problems) and
    Categorical Cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  prefs: []
  type: TYPE_NORMAL
- en: The network we develop in C*hapter 5,* *Model Architecture*, uses the MSE as
    its loss function. In the following section, we explore how that function performs
    as the network trains.
  prefs: []
  type: TYPE_NORMAL
- en: Different Loss Functions, Same Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving ahead to the next section, let's explore, in practical terms,
    how these problems are different in the context of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TensorFlow Playground application is made available by the TensorFlow team
    to help us understand how neural networks work. Here, we see a neural network
    represented with its layers: input (on the left), hidden layers (in the middle),
    and output (on the right).'
  prefs: []
  type: TYPE_NORMAL
- en: We can also choose different sample datasets to experiment with on the far-left
    side. And, finally, on the far-right side, we see the output of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7d64db8-e329-4ca4-9b37-4101dc07416b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: TensorFlow Playground web application. Take the parameters for a
    neural network in this visualization to gain some'
  prefs: []
  type: TYPE_NORMAL
- en: intuition on how each parameter affects the model results.
  prefs: []
  type: TYPE_NORMAL
- en: 'This application helps us explore the different problem categories we discussed
    in our previous section. When we choose **Classification** as the **Problem type**
    (upper right-hand corner), the dots in the dataset are colored with only two color
    values: either blue or orange.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we choose **Regression**, the colors of the dots are colored in a range
    of color values between orange and blue. When working on classification problems,
    the network evaluates its loss function based on how many blues and oranges the
    network has gotten wrong; and when working on classification problems, it checks
    how far away to the right color values for each dot the network was, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2eaeef80-5b83-4c9b-b41d-5ac426e2dddb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Detail of the TensorFlow Playground application. Different color
    values are assigned to the dots,'
  prefs: []
  type: TYPE_NORMAL
- en: depending on the problem type.
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on the play button, we notice that the numbers in the Training
    loss area keep going down as the network continuously trains. The numbers are
    very similar in each problem category because the loss functions play the same
    role in both neural networks. However, the actual loss function used for each
    category is different, and is chosen depending on the problem type.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating neural networks is where TensorBoard excels. As explained in C*hapter
    4*, *Introduction to Neural Networks and Deep Learning*, TensorBoard is a suite
    of visualization tools shipped with TensorFlow. Among other things, one can explore
    the results of loss function evaluations after each epoch. A great feature of
    TensorBoard is that one can organize the results of each run separately and compare
    the resulting loss function metrics for each run. One can then make a decision
    on which hyper parameters to tune and have a general sense of how the network
    is performing. The best part is that it is all done in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use TensorBoard with our model, we will use a Keras callback function.
    We do that by importing the `TensorBoard` callback and passing it to our model
    when calling its`fit()` function. The following code shows an example of how it
    would be implemented in the Bitcoin model created in our preceding chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 1*: Snippet that implements a TensorBoard callback in our LSTM model'
  prefs: []
  type: TYPE_NORMAL
- en: Keras callback functions are called at the end of each epoch run. In this case,
    Keras calls the TensorBoard callback to store the results from each run on the
    disk. There are many other useful callback functions available, and one can create
    custom ones using the Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the Keras callback documentation ([https://keras.io/ callbacks/](https://keras.io/callbacks/))
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing the TensorBoard callback, the `loss` function metrics are
    now available in the TensorBoard interface. You can now run a TensorBoard process
    (`with tensorboard --logdir=./logs`) and leave it running while you train your
    network with `fit()` . The main graphic to evaluate is typically called *loss*.
    One can add more metrics by passing known metrics to the metrics parameter in
    the `fit()` function; these will then be available for visualization in TensorBoard,
    but will not be used to adjust the network weights.
  prefs: []
  type: TYPE_NORMAL
- en: The interactive graphics will continue to update in real time, which allows
    you to understand what is happening on every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0edbd6fa-b1b7-4401-a6ab-1be71880004c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Screenshot of a TensorBoard instance showing the loss function results
    alongside other metrics added to the metrics parameter'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Model Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In both regression and classification problems, we split the input dataset
    into three other datasets: train, validation, and test. Both the train and the
    validation sets are used to train the network. The train set is used by the network
    as an input, and the validation set is used by the loss function to compare the
    output of the neural network to the real data, computing how wrong the predictions
    are. Finally, the test set is used after the network has been trained to measure
    how the network can perform on data it has never seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: There isn't a clear rule for determining how the train, validation, and test
    datasets must be divided. It is a common approach to divide the original dataset
    as 80 percent train and 20 percent test, then to further divide the train dataset
    into 80 percent train and 20 percent validation. For more information about this
    problem, please refer to the book *Python Machine Learning*, by Sebastian Raschka
    and Vahid Mirjalili (Packt, 2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification problems, you pass both the data and the labels to the neural
    network as related but distinct data. The network then learns how data is related
    to each label. In regression problems, instead of passing data and labels, one
    passes the variable of interest as one parameter and the variables used for learning
    patterns as another. Keras provides an interface for both of those use cases with
    the `fit()` method. See *Snippet 2* for an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 2*: Snippet that illustrates how to use the `validation_split and validation_data`
    parameters'
  prefs: []
  type: TYPE_NORMAL
- en: The `fit()` method can use either the `validation_split` or the `validation_data`
    parameter, but not both at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions evaluate the progress of models and adjust their weights on every
    run. However, loss functions only describe the relationship between training data
    and validation data. In order to evaluate if a model is performing correctly,
    we typically use a third set of data—which is not used to train the network—and
    compare the predictions made by our model to the values available in that set
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is the role of the test set. Keras provides the method `model.evaluate()`,
    which makes the process of evaluating a trained neural network against a test
    set easy. See the following code for an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 3*: Snippet that illustrates how to use the `evaluate()` method'
  prefs: []
  type: TYPE_NORMAL
- en: The `evaluate()` method returns both the results of the loss function and the
    results of the functions passed to the `metrics` parameter. We will be using that
    function frequently in the Bitcoin problem to test how the model performs on the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the Bitcoin model looks a bit different than the example
    above. That is because we are using an LSTM architecture. LSTMs are designed to
    predict sequences. Because of that, we do not use a set of variables to predict
    a different single variable—even if it is a regression problem. Instead, we use
    previous observations from a single variable (or set of variables) to predict
    future observations of that same variable (or set). The `y` parameter on `Keras.fit()`
    contains the same variable as the `x` parameter, but only the predicted sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Bitcoin Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a test set during our activities in C*hapter 4*, *Introduction to
    Neural Networks and Deep Learning*. That test set has 19 weeks of Bitcoin daily
    price observations, which is equivalent to about 20 percent of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We have also trained our neural network using the other 80 percent of data (that
    is, the train set with 56 weeks of data, minus one for the validation set) in
    C*hapter 5*, *Model Architecture*, and stored the trained network on disk (`bitcoin_lstm_v0`).
    We can now use the `evaluate()` method in each one of the 19 weeks of data from
    the test set and inspect how that fist neural network performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that, though, we have to provide 76 preceding weeks. We have
    to do this because our network has been trained to predict one week of data using
    exactly 76 weeks of continuous data (we will deal with this behavior by re-training
    our network periodically with larger periods in C*hapter 7*, *Productization*,
    when we deploy a neural network as a web application):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 4*: Snippet that implements the `evaluate()` method to evaluate the
    performance of our model in a test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we evaluate each week using Keras'' `model.evaluate()`
    , then store its output in the variable evaluated_weeks. We then plot the resulting
    MSE for each week in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8466d5dd-ea61-4ad9-8b40-71ce88f7b87e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: MSE for each week in the test set; notice that in week 5, the model
    predictions are worse than in any other week'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting MSE from our model suggests that our model performs well during
    most weeks, except for week 5, when its value increases to about `0.08`. Our model
    seems to be performing well for almost all of the other test weeks
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first trained network (`bitcoin_lstm_v0`) may be suffering from a phenomenon
    known as overfitting. Overfitting is when a model is trained to optimize a validation
    set, but it does so at the expense of more generalizable patterns from the phenomenon
    we are interested in predicting. The main issue with overfitting is that a model
    learns how to predict the validation set, but fails to predict new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function used in our model reaches very low levels (about 2.9 * 10-6)
    at the end of our training process. Not only that, but this happens early: the
    MSE loss function used to predict the last week in our data decreases to a stable
    plateau in about epoch 30\. This means that our model is predicting the data from
    week 77 almost perfectly, using the preceding 76 weeks. Could this be the result
    of overfiting?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at *Figure 4* again. We know that our LSTM model reaches extremely
    low values in our validation set (about 2.9 * 10-6), yet it also reaches low values
    in our test set. The key difference, however, is in the scale. The MSE for each
    week in our test set is about 4,000 times bigger (on average) than in the test
    set. This means that the model is performing much worse in our test data than
    in the validation set. This is worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scale, though, hides the power of our LSTM model: even performing much
    worse in our test set, the predictions'' MSE errors are still very, very low.
    That suggests that our model may be learning patterns from the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing is to measure our model comparing MSE errors, and another is to be
    able to interpret its results intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same model, let''s now create a series of predictions for the following
    weeks, using 76 weeks as input. We do that by sliding a window of 76 weeks over
    the complete series (that is, train plus test sets), and making predictions for
    each of those windows. Predictions are done using the Keras `model.predict()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 5*: Snippet that uses the `model.predict()` method for making predictions for
    all the weeks of the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we make predictions using `model.predict()`, then store
    these predictions in the `predicted_weeks` variable. We then plot the resulting
    predictions, making the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7155c427-aa64-4b61-b128-876340bf24dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5*: MSE for each week in the test set. Notice that in week 5, the model
    predictions are worse than in any other week.'
  prefs: []
  type: TYPE_NORMAL
- en: The results of our model (as shown in *Figure 5*) suggest that its performance
    isn't all that bad. By observing the pattern from the Predicted line, one can
    notice that the network has identifiled a fluctuating pattern happening on a weekly
    basis, in which the normalized prices go up in the middle of the week, then down
    by the end of it. With the exception of a few weeks—most notably week 5, the same
    from our previous MSE analysis—most weeks fall close to the correct values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now denormalize the predictions so that we can investigate the prediction
    values using the same scale as the original data (that is, US Dollars). We can
    do this by implementing a denormalization function that uses the day index from
    the predicted data to identify the equivalent week on the test data. After that
    week is identified, the function then takes the fist value of that week and uses
    that value to denormalize the predicted values by using the same point-relative
    normalization technique, but inverted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 6*: De-normalization of data using an inverted point-relative normalization
    technique. The `denormalize()` function takes the first closing price from the
    test''s first day of an equivalent week.'
  prefs: []
  type: TYPE_NORMAL
- en: Our results now compare the predicted values with the test set, using US Dollars.
    As seen in Figure 5, the `bitcoin_lstm_v0` model seems to perform quite well in
    predicting the Bitcoin prices for the following seven days. But, how can we measure
    that performance in interpretable terms?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd53113f-988e-4517-8fbf-de72c73fcf2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: MSE for each week in the test set; notice that in week 5, the model
    predictions are worse than in any other week'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our last step is to add interpretability to our predictions. Figure 6 seems
    to show that our model prediction matches the test data somewhat closely, but
    how closely?
  prefs: []
  type: TYPE_NORMAL
- en: Keras' `model.evaluate()` function is useful for understanding how a model is
    performing at each evaluation step. However, given that we are typically using
    normalized datasets to train neural networks, the metrics generated by the `model.evaluate()`
    method are also hard to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve that problem, we can collect the complete set of predictions
    from our model and compare it with the test set using two other functions from
    *Table 1* that are easier to interpret: MAPE and RMSE, implemented as `mape()`
    and `rmse()` , respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 7*: Implementation of the *mape()* and *rmse()* functions'
  prefs: []
  type: TYPE_NORMAL
- en: These functions are implemented using `NumPy`. Original implementations come
    from [https://stats.stackexchange.com/ questions/58391/mean-absolute-percentage-error-mapein-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)
    (MAPE) and [https://stackoverflow.com/ questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)
    (RMSE).
  prefs: []
  type: TYPE_NORMAL
- en: 'After comparing our test set with our predictions using both of those functions,
    we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Denormalized RMSE: $399.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denormalized MAPE: 8.4 percent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This indicates that our predictions differ, on average, about $399 from real
    data. That represents a difference of about 8.4 percent from real Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: These results facilitate the understanding of our predictions. We will continue
    to use the model.evaluate() method to keep track of how our LSTM model is improving,
    but will also compute both `rmse()` and `mape()` on the complete series on every
    version of our model to interpret how close we are to predicting Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Creating an Active Training Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this activity, we create a training environment for our neural network that
    facilitates both its training and evaluation. This environment is particularly
    important to our next chapter, in which we search for an optimal combination of
    hyperparameters. F
  prefs: []
  type: TYPE_NORMAL
- en: First, we will start both a Jupyter Notebook instance and a TensorBoard instance.
    Both of these instances can remain open for the remainder of this activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your terminal, navigate to the directory chapter_6/activity_6 and execute the
    following code to start a Jupyter Notebook instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the URL provided by the application in your browser and open the Jupyter
    Notebook named  `Activity_6_Creating_an_active_training_environment. ipynb`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f9ad3585-3a3c-440d-b284-4d7acf35007b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Jupyter Notebook highlighting the section Evaluate LSTM Model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also using your terminal, start a TensorBoard instance by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Open the URL that appears on the screen and leave that browser tab open, as
    well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, load both the training (`train_dataset.csv`) and the test set (`test_dataset.
    csv`), and also our previously compiled model (`bitcoin_lstm_v0.h5`), into the
    Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the train and test datasets in the Jupyter Notebook instance using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, load our previously compiled model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let us now evaluate how our model performed against test data. Our model is
    trained using 76 weeks to predict a week into the future—that is, the following
    sequence of seven days. When we built our first model, we divided our original
    dataset between a training and a test set. We will now take a combined version
    of both datasets (let's call it combined set) and move a sliding window of 76
    weeks. At each window, we execute Keras' `model.evaluate()` method to evaluate
    how the network performed on that specific week.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the cells under the header Evaluate LSTM Model. The key concept of
    these cells it to call the model.evaluate() method for each of the weeks in the
    test set. This line is the most important:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Each evaluation result is now stored in the variable `evaluated_weeks`. That
    variable is a simple array containing the sequence of MSE predictions for every
    week in the test set. Go ahead and also plot these results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/20414581-6c03-4a10-8498-f1fda7532ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: As discussed during our chapter, the MSE loss function is difficult to interpret.
    To facilitate our understanding of how our model is performing, we also call the
    method `model.predict()` on each week from the test set and compare its predicted
    results with the set's values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the section **Interpreting Model** Results and execute the code
    cells under the sub-header **Make Predictions**. Notice that we are calling the
    method `model.predict()` , but with a slightly different combination of parameters.
    Instead of using both `X` and `Y` values, we only use `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At each window, we will issue predictions for the following week and store
    the results. We can now plot the normalized results alongside the normalized values from
    the test set, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/778403d5-d6b1-4c38-8d19-f54824595bcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Plotting the normalized values returned from *model.predict()* for
    each week of the test set'
  prefs: []
  type: TYPE_NORMAL
- en: We will also make the same comparisons but using de-normalized values. In order
    to de-normalize our data, we must first identify the equivalent week between the
    test set and the predictions. Then, we take the first price value for that week
    and use it to reverse the point-relative normalization equation from C*hapter
    5*, *Model Architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the header Denormalizing Predictions and execute all cells under
    that header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this section, we defiled the function `denormalize()`, which performs the
    complete de-normalization process. Different than other functions, this function
    takes in a Pandas DataFrame instead of a NumPy array. We do so for using dates
    as an index. This is the most relevant cell block from that header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our de-normalized results (as seen in the following figure) show that our model
    makes predictions that are close to the real Bitcoin prices. But how close?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ac188a1-60c6-4533-ab9f-48d4675e0268.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Plotting the de-normalized values returned from `model.predict()`
    for each week of the test set'
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM network uses MSE values as its loss function. However, as discussed
    during the chapter, MSE values are difficult to interpret. To solve that, we implement
    two functions (loaded from the `script utilities.py`) that implement the functions
    RMSE and MAPE. Those functions add interpretability to our model by returning
    a measurement in the same scale that our original data used, and by comparing
    the difference in scale as a percentage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the header De-normalizing Predictions and load two functions from
    the `utilities.py` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The functions from the script are actually really simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Each function is implemented using NumPy's vector-wise operations. They work
    well in vectors of the same length. They are designed to be applied on a complete
    set of results.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `mape()` function, we can now understand that our model predictions
    are about 8.4 percent away from the prices from the test set. This is equivalent
    to a root mean squared error (calculated using the `rmse()` function) of about
    $399.6.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next section, go back into the Notebook and find the
    header **Re-train Model with TensorBoard**. You may have noticed that we created
    a helper function called `train_model()` . This function is a wrapper around our
    model that trains (`using model.fit()` ) our model, storing its respective results
    under a new directory. Those results are then used by TensorBoard as a discriminator,
    in order to display statistics for different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and modify some of the values for the parameters passed to the `model.
    fit()` function (try epochs, for instance). Now, run the cells that load the model
    into memory from disk (this will replace your trained model):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the `train_model()` function again, but with different parameters,
    indicating a new run version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned how to evaluate a network using loss functions.
    We learned that loss functions are key elements of neural networks, as they evaluate
    the performance of a network at each epoch and are the starting point for the
    propagation of adjustments back into layers and nodes. We also explored why some
    loss functions can be difficult to interpret (for instance, the MSE) and developed
    a strategy using two other functions— RMSE and MAPE—to interpret the predicted
    results from our LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, this chapter concludes with an active training environment.
    We now have a system that can train a deep learning model and evaluate its results
    continuously. This will be key when we move to optimizing our network in the next
    session.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have trained a neural network to predict the next seven days of Bitcoin prices
    using the preceding 76 weeks of prices. On average, that model issues predictions
    that are about 8.4 percent distant from real Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section describes common strategies for improving the performance of neural
    network models:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding or removing layers and changing the number of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing or decreasing the number of training epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with different activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different regularization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will evaluate each modification using the same active learning environment
    developed by the end of the *Model Evaluation* section, measuring how each one
    of these strategies may help us develop a more precise model.
  prefs: []
  type: TYPE_NORMAL
- en: Layers and Nodes - Adding More Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks with single hidden layers can perform fairly well on many problems.
    Our fist Bitcoin model (`bitcoin_lstm_v0`) is a good example: it can predict the
    next seven days of Bitcoin prices (from the test set) with error rates of about
    8.4 percent using a single LSTM layer. However, not all problems can be modeled
    with single layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the function that you are working to predict is, the higher
    the likelihood that you will need to add more layers. A good intuition to determine
    whether adding new layers is a good idea is to understand what their role in a
    neural network is.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer creates a model representation of its input data. Earlier layers
    in the chain create lower-level representations, and later layers, higher-level.
  prefs: []
  type: TYPE_NORMAL
- en: 'While that description may be difficult to translate into real-world problems,
    its practical intuition is simple: when working with complex functions that have
    different levels of representation, you may want to experiment with the addition
    of layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding More Nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of neurons that your layer requires is related to how both the input
    and output data are structured.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you are working to classify a 4 x 4 pixel image into one of
    two categories, one can start with a hidden layer that has 12 neurons (one for
    each available pixel) and an output layer that has only two (one for each predicted
    class).
  prefs: []
  type: TYPE_NORMAL
- en: It is common to add new neurons alongside the addition of new layers. Then,
    one can add a layer that has either the same number of neurons as the previous
    one, or a multiple of the number of neurons from the previous layer. For instance,
    if your fist hidden layer has 12 neurons, you can experiment with adding a second
    layer that has either 12, 6, or 24.
  prefs: []
  type: TYPE_NORMAL
- en: Adding layers and neurons can have significant performance limitations. Feel
    free to experiment with adding layers and nodes. It is common to start with a
    smaller network (that is, a network with a small number of layers and neurons),
    then grow according to its performance gains.
  prefs: []
  type: TYPE_NORMAL
- en: If the above comes across as imprecise, your intuition is right. To quote Aurélien
    Géron, YouTube's former lead for video classification, *Finding the perfect amount
    of neurons is still somewhat of a black art*.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on Machine Learning with Scikit-Learn and TensorFlow, by Aurelién Géron,
    published by O'Reilly, March 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a word of caution: the more layers you add, the more hyper parameters
    you have to tune—and the longer your network will take to train. If your model
    is performing fairly well and not overfitting your data, experiment with the other
    strategies outlined in this chapter before adding new layers to your network.'
  prefs: []
  type: TYPE_NORMAL
- en: Layers and Nodes - Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now modify our original LSTM model by adding more layers. In LSTM models,
    one typically adds LSTM layers in a sequence, making a chain between LSTM layers.
    In our case, the new LSTM layer has the same number of neurons as the original
    layer, so we don't have to configure that parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We will name the modifiled version of our model `bitcoin_lstm_v1`. It is good
    practice to name each one of the models in which one is attempting different hyperparameter
    configurations differently. This helps you to keep track of how each different
    architecture performs, and also to easily compare model differences in TensorBoard.
    We will compare all the different modifiled architectures at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before adding a new LSTM layer, we need to modify the parameter `return_sequences`
    to True on the fist LSTM layer. We do this because the fist layer expects a sequence
    of data with the same input as that of the fist layer. When this parameter is
    set to `False`, the LSTM layer outputs the predicted parameters in a different,
    incompatible output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 8*: Adding a second LSTM layer to the original `bitcoin_lstm_v0 model`,
    making it `bitcoin_lstm_v1`'
  prefs: []
  type: TYPE_NORMAL
- en: Epochs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Epochs are the number of times the network adjust its weights in response to
    data passing through and its loss function. Running a model for more epochs can
    allow it to learn more from data, but you also run the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: When training a model, prefer to increase the epochs exponentially until the
    loss function starts to plateau. In the case of the `bitcoin_lstm_v0` model, its
    loss function plateaus at about 100 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Our LSTM model uses a small amount of data to train, so increasing the number
    of epochs does not affect its performance in significant ways. For instance, if
    one attempts to train it at 103 epochs, the model barely gains any improvements.
    This will not be the case if the model being trained uses enormous amounts of
    data. In those cases, a large number of epochs is crucial to achieve good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'I suggest you use the following association: the larger the date used to train
    your model, the more epochs it will need to achieve good performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Epochs - Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our Bitcoin dataset is rather small, so increasing the epochs that our model
    trains may have only a marginal effect on its performance. In order to have the
    model train for more epochs, one only has to change the epochs parameter in `model.fit()`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 9*: Changing the number of epochs that our model trains for, making
    it `bitcoin_lstm_v2`'
  prefs: []
  type: TYPE_NORMAL
- en: That change bumps our model to v2, effectively making it `bitcoin_lstm_v2`.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Activation functions evaluate how much you need to activate individual neurons.
    They determine the value that each neuron will pass to the next element of the
    network, using both the input from the previous layer and the results from the
    loss function—or if a neuron should pass any values at all.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are a topic of great interest in the scientific community
    researching neural networks. For an overview of research currently being done
    on the topic and a more detailed review on how activation functions work, please
    refer to *Deep Learning* by Ian Goodfellow et. al., MIT Press, 2017.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and Keras provide many activation functions—and new ones are occasionally added.
    As an introduction, three are important to consider; let's explore each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has been greatly inspired by the article *Understanding Activation
    Functions in Neural Networks* by Avinash Sharma V, available at: [https://medium.com/the-theory-of-everything/
    understanding-activation-functions-in-neural-networks- 9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).'
  prefs: []
  type: TYPE_NORMAL
- en: Linear (Identity)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear functions only activate a neuron based on a constant value. They are
    defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7261435b-074a-4e48-a878-ddf7399d9454.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When c = 1, neurons will pass the values as-is, without modification by the
    activation function. The issue with using linear functions is that, due to the
    fact that neurons are activated linearly, chained layers now function as a single
    large layer. In other words, one loses the ability to construct networks with
    many layers, in which the output of one influences the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9929f033-6fa9-461f-8f45-21db82d42fb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Illustration of a linear function'
  prefs: []
  type: TYPE_NORMAL
- en: The use of linear functions is generally considered obsolete for most networks.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic Tangent (Tanh)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tanh is a non-linear function, and is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e1f0388-c59a-4d99-9d23-5722290e7cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that the effect they have on nodes is evaluated continuously. Also,
    because of its non-linearity, one can use this function to change how one layer
    influences the next layer in the chain. When using non-linear functions, layers
    activate neurons in different ways, making it easier to learn different representations
    from data. However, they have a sigmoid-like pattern which penalizes extreme node
    values repeatedly, causing a problem called vanishing gradients. Vanishing gradients
    have negative effects on the ability of a network to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d1b34ee-905d-4c13-affc-466e98ed9533.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Illustration of a `tanh` function'
  prefs: []
  type: TYPE_NORMAL
- en: Tanhs are popular choices, but due to fact that they are computationally expensive,
    ReLUs are often used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Rectifid Linear Unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ReLUs have non-linear properties. They are defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e34ccbc9-8077-452b-8ea7-7fa1695be15c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/69e63f3a-a36c-4cf8-8480-e36c00578657.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Illustration of a ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU functions are often recommended as great starting points before trying
    other functions. ReLUs tend to penalize negative values. So, if the input data
    (for instance, normalized between -1 and 1) contains negative values, those will
    now be penalized by ReLUs. That may not be the intended behavior.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be using ReLU functions in our network because our normalization
    process creates many negative values, yielding a much slower learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions - Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to implement activation functions in Keras is by instantiating
    the Activation() class and adding it to the `Sequential()` model. `Activation()`
    can be instantiated with any activation function available in Keras (for a complete
    list, see [https://keras.io/activations/](https://keras.io/activations/)). In
    our case, we will use the `tanh` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After implementing an activation function, we bump the version of our model
    to `v2`, making it `bitcoin_lstm_v3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 10*: Adding the activation function `tanh` to the `bitcoin_lstm_v2`
    model, making it `bitcoin_lstm_v3`'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of other activation functions worth experimenting with. Both
    TensorFlow and Keras provide a list of implemented functions in their respective
    official documentations. Before implementing your own, start with the ones already
    implemented in both TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are particularly prone to overfitting. Overfitting happens when
    a network learns the patterns of the Neural networks are particularly prone to
    overfitting. Overfitting happens when a network learns the patterns of the training
    data but is unable to find generalizable patterns that can also be applied to
    the test data.he training data but is unable to find generalizable patterns that
    can also be applied to the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization strategies refer to techniques that deal with the problem of
    overfitting by adjusting how the network learns. In this book, we discuss two
    common strategies: L2 and Dropout.'
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: L2 regularization (or weight decay) is a common technique for dealing with overfiting
    models. In some models, certain parameters vary in great magnitudes. The L2 regularization
    penalizes such parameters, reducing the effect of these parameters on the network.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularizations use the![](img/01dfbb24-b718-40fc-840f-8e8529604999.png)
    parameter to determine how much to penalize a model neuron. One typically sets
    that to a very low value (that is, `0.0001`); otherwise, one risks eliminating
    the input from a given neuron completely.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dropout is a regularization technique based on a simple question: if one randomly
    takes away a proportion of nodes from layers, how will the other node adapt? It
    turns out that the remaining neurons adapt, learning to represent patterns that
    were previously handled by those neurons that are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: The dropout strategy is simple to implement and is typically very effective
    to avoid overfitting. This will be our preferred regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Strategies – Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to implement the dropout strategy using Keras, we import the `Dropout()`
    class and add it to our network immediately after each LSTM layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This addition effectively makes our network `bitcoin_lstm_v4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Snippet 11*: In this snippet, we add the `Dropout()` step to our model (`bitcoin_lstm_v3`),
    making it `bitcoin_lstm_v4`'
  prefs: []
  type: TYPE_NORMAL
- en: One could have used the L2 regularization instead of Dropout. In order to do
    that, simply instantiate the `ActivityRegularization()` class with the L2 parameter
    set to a low value (`0.0001`, for instance). Then, place it in the place where
    the Dropout() class is added to the network. Feel free to experiment by adding
    that to the network while keeping both `Dropout()` steps, or simply replace all
    the `Dropout()` instances with `ActivityRegularization()` instead.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All in all, we have created four versions of our model. Three of these versions
    were created by the application of different optimization techniques outlined
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating all these versions, we now have to evaluate which model performs
    best. In order to do that, we use the same metrics used in our fist model: MSE,
    RMSE, and MAPE. MSE is used to compare the error rates of the model on each predicted
    week. RMSE and MAPE are computed to make the model results easier to interpret.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model**  | **MSE (last epoch)**  | **RMSE  (whole series)**  | **MAPE  (whole
    series)**  | **Training Time**  |'
  prefs: []
  type: TYPE_TB
- en: '| bitcoin_lstm_v0  | **-** |  399.6  |  8.4 percent  | ** -** |'
  prefs: []
  type: TYPE_TB
- en: '| bitcoin_lstm_v1  |  7.15*10^(-6)  |  419.3  |  8.8 percent  | 49.3 s  |'
  prefs: []
  type: TYPE_TB
- en: '| bitcoin_lstm_v2  |  3.55*10^(-6)  |  425.4  |  9.0 percent  | 1 min 13s  |'
  prefs: []
  type: TYPE_TB
- en: '| bitcoin_lstm_v3  |  2.8*10^(-4)  |  423.9  |  8.8 percent  | 1 min 19s  |'
  prefs: []
  type: TYPE_TB
- en: '| bitcoin_lstm_v4  |  4.8*10^(-7)  |  442.4  |  8.8 percent  | 1 min 20s  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Model results for all models'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, our fist model (`bitcoin_lstm_v0`) performed the best in nearly
    all defiled metrics. We will be using that model to build our web application
    and continuously predict Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: Activity:Optimizing a Deep Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this activity, we implement different optimization strategies to the model
    created in C*hapter 5*, *Model Architecture* (`bitcoin_lstm_v0`). That model achieves
    a MAPE performance on the complete de-normalization test set of about 8.4 percent.
    We will try to reduce that gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your terminal, start a TensorBoard instance by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the URL that appears on the screen and leave that browser tab open, as
    well. Also, start a Jupyter Notebook instance with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Open the URL that appears in a different browser window.
  prefs: []
  type: TYPE_NORMAL
- en: Now, open the Jupyter Notebook called `Activity_7_Optimizing_a_deep_ learning_model.ipynb`
    and navigate to the title of the Notebook and import all required libraries. We
    will load the train and test data like in previous activities. We will also split
    it into train and test groups using the utility   function `split_lstm_input()`
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each section of this Notebook, we will implement new optimization techniques
    in our model. Each time we do so, we   train a fresh model and store its trained
    instance in a variable that describes the model version. For instance, our first 
    model, `bitcoin_lstm_v0`, is called`model_v0` in this Notebook. At the very end
    of the Notebook, we evaluate all  models using MSE, RMSE, and MAPE.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in the open Jupyter Notebook, navigate to the header **Adding Layers**
    and **Nodes**.You will recognize our fist model in the next cell. This is the
    basic LSTM network that we built in C*hapter 5*, *Model Architecture*. Now, we
    have to add a new LSTM layer to this network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using knowledge from this chapter, go ahead and add a new LSTM layer, compile,
    and train the model. While training your models, remember to frequently visit
    the running TensorBoard instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be able to see each model run and compare the results of their loss
    functions there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c919b2a6-3e37-4d14-90ce-3748169adc08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Running the TensorBoard instance, which is displaying many different
    model runs. TensorBoard is really'
  prefs: []
  type: TYPE_NORMAL
- en: useful for tracking model training in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, navigate to the header Epochs. In this section, we are interested in exploring
    different magnitudes of **epochs**. Use the utility function`train_model()` to
    name different model versions and runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Train the model with a few different epoch parameters.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are interested in making sure the model doesn't overfit the
    training data. You want to avoid this, because if it does, it will not be able
    to predict patterns that are represented in the training data but have different
    representations in the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are done experimenting with epochs, move to the next optimization
    technique: activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, navigate to the header **Activation Functions** in the Notebook. In this
    section, you only need to change the following variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We have used the `tanh` function in this section, but feel free to try other
    activation functions. Review the list available at [https://keras.io/activations/](https://keras.io/activations/)
    and try other possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Our final option is to try different regularization strategies. This is notably
    more complex and may take a few iterations to notice any gains—especially with
    so little data. Also, adding regularization strategies typically increases the
    training time of your network.
  prefs: []
  type: TYPE_NORMAL
- en: Now, navigate to the header **Regularization Strategies** in the Notebook. In
    this section, you need to implement the `Dropout()` regularization strategy. Find
    the right place to place that step and implement it in our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also try the L2 regularization here, as well (or combine both). Do the
    same as with `Dropout()` , but now using `ActivityRegularization`(`l2=0.0001`)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, navigate to the header **Evaluate Models** in the Notebook. In this section,
    we will evaluate the model predictions for the next 19 weeks of data in the test
    set. Then, we will compute the RMSE and MAPE of the predicted series versus the
    test series.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have implemented the same evaluation techniques from Activity 6, all wrapped in
    utility functions. Simply run all the   cells from this section until the end
    of the notebook to see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Take this opportunity to tweak the values for the preceding optimization techniques
    and attempt to beat the performance of that model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to evaluate our model using the metrics mean
    squared error (MSE), squared mean squared error (RMSE), and mean averaged percentage
    error (MAPE). We computed the latter two metrics in a series of 19 week predictions
    made by our fist neural network model. We then learned that it was performing
    well.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to optimize a model. We looked at optimization techniques
    typically used to increase the performance of neural networks. Also, we implemented
    a number of these techniques and created a few more models to predict Bitcoin
    prices with different error rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will be turning our model into a web application that
    does two things: re-trains our model periodically with new data, and is able to
    make predictions using an HTTP API interface.'
  prefs: []
  type: TYPE_NORMAL
