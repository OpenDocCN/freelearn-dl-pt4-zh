<html><head></head><body>
  <div id="_idContainer273">
   <h1 class="chapter-number" id="_idParaDest-115">
    <a id="_idTextAnchor118">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     6
    </span>
   </h1>
   <h1 id="_idParaDest-116">
    <a id="_idTextAnchor119">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Harnessing Large Language Models for Graph Learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     Traditionally,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      graph neural networks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      GNNs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     ) have
    </span>
    <a id="_idIndexMarker408">
    </a>
    <span class="koboSpan" id="kobo.8.1">
     been the workhorse for graph learning tasks, achieving impressive results.
    </span>
    <span class="koboSpan" id="kobo.8.2">
     However, recent research explores the exciting potential of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.9.1">
      large language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.10.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.11.1">
      LLMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.12.1">
     ) in
    </span>
    <a id="_idIndexMarker409">
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.13.1">
      this domain.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.14.1">
     In this chapter, we’ll delve into the intersection of LLMs and graph learning, exploring how these powerful language models can enhance graph-based tasks.
    </span>
    <span class="koboSpan" id="kobo.14.2">
     We’ll begin with an overview of LLMs, followed by a discussion of the limitations of GNNs and the motivations for incorporating LLMs.
    </span>
    <span class="koboSpan" id="kobo.14.3">
     Then, we’ll explore various approaches for utilizing LLMs in graph learning, the intersection of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.15.1">
      retrieval-augmented generation
     </span>
    </strong>
    <span class="koboSpan" id="kobo.16.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.17.1">
      RAG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.18.1">
     ) with
    </span>
    <a id="_idIndexMarker410">
    </a>
    <span class="koboSpan" id="kobo.19.1">
     graphs, and explain the advantages and challenges associated with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.20.1">
      this integration.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.21.1">
     In this chapter, we’ll explore the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.22.1">
      following topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.23.1">
       Understanding LLMs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.24.1">
      Textual data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.25.1">
       in graphs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.26.1">
      LLMs for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.27.1">
       graph learning
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.28.1">
      Integrating RAG with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.29.1">
       graph learning
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.30.1">
      Challenges in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.31.1">
       integrating LLMs
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-117">
    <a id="_idTextAnchor120">
    </a>
    <span class="koboSpan" id="kobo.32.1">
     Understanding LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.33.1">
     LLMs are a
    </span>
    <a id="_idIndexMarker411">
    </a>
    <span class="koboSpan" id="kobo.34.1">
     significant advancement in
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.35.1">
      artificial intelligence
     </span>
    </strong>
    <span class="koboSpan" id="kobo.36.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.37.1">
      AI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.38.1">
     ), particularly in
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.39.1">
      natural language processing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.40.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.41.1">
      NLP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.42.1">
     ) and
    </span>
    <a id="_idIndexMarker412">
    </a>
    <span class="koboSpan" id="kobo.43.1">
     understanding.
    </span>
    <span class="koboSpan" id="kobo.43.2">
     These
    </span>
    <a id="_idIndexMarker413">
    </a>
    <span class="koboSpan" id="kobo.44.1">
     models are designed to understand, generate, and interact with human language in a way that’s both meaningful and contextually relevant.
    </span>
    <span class="koboSpan" id="kobo.44.2">
     The development and evolution of LLMs have been marked by a series of innovations that have expanded their capabilities and applications across
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.45.1">
      various domains.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.46.1">
     At their core, LLMs are trained on vast datasets of text from the internet, books, articles, and other sources of written language.
    </span>
    <span class="koboSpan" id="kobo.46.2">
     This training involves analyzing patterns, structures, and the semantics of language, enabling these models to generate coherent, contextually appropriate text based on the input they receive.
    </span>
    <span class="koboSpan" id="kobo.46.3">
     The training process relies on deep learning techniques, particularly neural networks, which allow the models to improve their language capabilities over time through exposure to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.47.1">
      more data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.48.1">
     One of the key characteristics of LLMs is their size, which is measured in the number of parameters they contain.
    </span>
    <span class="koboSpan" id="kobo.48.2">
     Early models had millions of parameters, but the most advanced models today boast tens or even hundreds of billions of parameters.
    </span>
    <span class="koboSpan" id="kobo.48.3">
     This increase in size has been correlated with a significant improvement in the models’ ability to understand and generate human-like text, making them more effective for a wide range
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.49.1">
      of applications.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.50.1">
     LLMs have a wide array of applications, from simple tasks such as grammar correction and text completion to more complex ones such as writing articles, generating code, translating languages, and even creating poetry or prose.
    </span>
    <span class="koboSpan" id="kobo.50.2">
     They’re also used in conversational agents, providing the backbone for chatbots and virtual assistants, which can engage in more natural and meaningful interactions
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.51.1">
      with users.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.52.1">
     The evolution of LLMs has been marked by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.53.1">
      significant milestones:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.54.1">
       1990s
      </span>
     </strong>
     <span class="koboSpan" id="kobo.55.1">
      : The era
     </span>
     <a id="_idIndexMarker414">
     </a>
     <span class="koboSpan" id="kobo.56.1">
      of
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.57.1">
       statistical language models
      </span>
     </strong>
     <span class="koboSpan" id="kobo.58.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.59.1">
       SLMs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.60.1">
      ) began, utilizing
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.61.1">
       n-gram models
      </span>
     </strong>
     <span class="koboSpan" id="kobo.62.1">
      to
     </span>
     <a id="_idIndexMarker415">
     </a>
     <span class="koboSpan" id="kobo.63.1">
      predict the next word in a sequence based on a few preceding words.
     </span>
     <span class="koboSpan" id="kobo.63.2">
      These models faced challenges with high-order predictions due to data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.64.1">
       sparsity issues.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.65.1">
       Early 2000s
      </span>
     </strong>
     <span class="koboSpan" id="kobo.66.1">
      : The introduction
     </span>
     <a id="_idIndexMarker416">
     </a>
     <span class="koboSpan" id="kobo.67.1">
      of
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.68.1">
       neural language models
      </span>
     </strong>
     <span class="koboSpan" id="kobo.69.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.70.1">
       NLMs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.71.1">
      ), which
     </span>
     <a id="_idIndexMarker417">
     </a>
     <span class="koboSpan" id="kobo.72.1">
      employed neural networks such as
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.73.1">
       multilayer perceptrons
      </span>
     </strong>
     <span class="koboSpan" id="kobo.74.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.75.1">
       MLPs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.76.1">
      ) and
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.77.1">
       recurrent neural networks
      </span>
     </strong>
     <span class="koboSpan" id="kobo.78.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.79.1">
       RNNs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.80.1">
      ), marked
     </span>
     <a id="_idIndexMarker418">
     </a>
     <span class="koboSpan" id="kobo.81.1">
      a shift toward understanding deeper
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.82.1">
       linguistic relationships.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.83.1">
       2010s
      </span>
     </strong>
     <span class="koboSpan" id="kobo.84.1">
      : The development of word embeddings
     </span>
     <a id="_idIndexMarker419">
     </a>
     <span class="koboSpan" id="kobo.85.1">
      such as
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.86.1">
       Word2Vec
      </span>
     </strong>
     <span class="koboSpan" id="kobo.87.1">
      and
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.88.1">
       GloVe
      </span>
     </strong>
     <span class="koboSpan" id="kobo.89.1">
      , which
     </span>
     <a id="_idIndexMarker420">
     </a>
     <span class="koboSpan" id="kobo.90.1">
      represent words in continuous vector spaces, allowed models to capture semantic meaning
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.91.1">
       and context.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.92.1">
       2017
      </span>
     </strong>
     <span class="koboSpan" id="kobo.93.1">
      : Transformer architecture was introduced, leading to a breakthrough in handling sequential data without the need for recurrent processing.
     </span>
     <span class="koboSpan" id="kobo.93.2">
      This architecture is the foundation of many
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.94.1">
       subsequent LLMs.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.95.1">
       2018
      </span>
     </strong>
     <span class="koboSpan" id="kobo.96.1">
      : The
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.97.1">
       Generative Pre-Trained Transformer
      </span>
     </strong>
     <span class="koboSpan" id="kobo.98.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.99.1">
       GPT
      </span>
     </strong>
     <span class="koboSpan" id="kobo.100.1">
      ) was released by OpenAI, showcasing the
     </span>
     <a id="_idIndexMarker421">
     </a>
     <span class="koboSpan" id="kobo.101.1">
      power of transformers in language understanding
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.102.1">
       and generation.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.103.1">
       2019
      </span>
     </strong>
     <span class="koboSpan" id="kobo.104.1">
      :
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.105.1">
       Bidirectional Encoder Representations from Transformers
      </span>
     </strong>
     <span class="koboSpan" id="kobo.106.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.107.1">
       BERT
      </span>
     </strong>
     <span class="koboSpan" id="kobo.108.1">
      ) by Google
     </span>
     <a id="_idIndexMarker422">
     </a>
     <span class="koboSpan" id="kobo.109.1">
      revolutionized the field by introducing a model trained to understand the context from both directions in a piece
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.110.1">
       of text.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.111.1">
       2020s
      </span>
     </strong>
     <span class="koboSpan" id="kobo.112.1">
      : Even larger models began to emerge, such as
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.113.1">
       GPT-3
      </span>
     </strong>
     <span class="koboSpan" id="kobo.114.1">
      , which demonstrated remarkable capabilities in generating human-like text, and models that can integrate LLMs with other AI fields were introduced, such as
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.115.1">
       graph learning.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.116.1">
       2023 – 2024
      </span>
     </strong>
     <span class="koboSpan" id="kobo.117.1">
      : The landscape saw an explosion of powerful LLMs, starting with ChatGPT, followed by
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.118.1">
       GPT-4
      </span>
     </strong>
     <span class="koboSpan" id="kobo.119.1">
      , Claude from Anthropic, Gemini from Google, and Llama from Meta, indicating a trend toward more specialized and powerful
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.120.1">
       language models.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.121.1">
     The remarkable achievements of LLMs have sparked interest in harnessing their capabilities for tasks in graph machine learning.
    </span>
    <span class="koboSpan" id="kobo.121.2">
     On the one hand, the extensive knowledge and logical prowess of LLMs offer promising prospects to improve upon conventional GNN models.
    </span>
    <span class="koboSpan" id="kobo.121.3">
     On the other hand, the organized representations and concrete knowledge
    </span>
    <a id="_idIndexMarker423">
    </a>
    <span class="koboSpan" id="kobo.122.1">
     embedded within graphs hold the potential to mitigate some of the primary shortcomings of LLMs, including their tendency to generate misleading information and their challenges
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.123.1">
      with interpretability.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-118">
    <a id="_idTextAnchor121">
    </a>
    <span class="koboSpan" id="kobo.124.1">
     Textual data in graphs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.125.1">
     One of the fundamental
    </span>
    <a id="_idIndexMarker424">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     hurdles in deploying GNNs lies in acquiring sophisticated
    </span>
    <a id="_idIndexMarker425">
    </a>
    <span class="koboSpan" id="kobo.127.1">
     feature representations for nodes and edges.
    </span>
    <span class="koboSpan" id="kobo.127.2">
     This becomes particularly crucial when these elements are associated with complex textual attributes such as descriptions, titles,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.128.1">
      or abstracts.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.129.1">
     Traditional methods, such as the bag-of-words approach or the utilization of pre-trained word embedding models, have been the norm.
    </span>
    <span class="koboSpan" id="kobo.129.2">
     However, these techniques typically fall short of grasping the subtle semantic intricacies inherent in the text.
    </span>
    <span class="koboSpan" id="kobo.129.3">
     They tend to overlook the context and the interdependencies between words, leading to a loss of critical information that could be essential for the GNN to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.130.1">
      perform optimally.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.131.1">
     To overcome this challenge, there’s a growing need for more advanced methods that can understand and encode the richness of language into the graph structure.
    </span>
    <span class="koboSpan" id="kobo.131.2">
     This is where LLMs come into play.
    </span>
    <span class="koboSpan" id="kobo.131.3">
     With their deep understanding of language nuances and context, LLMs can generate embeddings that capture a broader spectrum of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.132.1">
      linguistic features.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.133.1">
     By integrating LLMs into the feature extraction process for GNNs, you can potentially encode richer, more informative representations that reflect the true semantic content of the textual attributes, thereby enhancing the GNN’s ability to perform tasks such as node classification, link prediction, and graph generation with greater accuracy
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.134.1">
      and efficiency.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.135.1">
     LLMs have capabilities that extend beyond generating textual embeddings as features.
    </span>
    <span class="koboSpan" id="kobo.135.2">
     LLMs are good at generating augmented information from original text attributes.
    </span>
    <span class="koboSpan" id="kobo.135.3">
     They can be used to generate tags/labels and other useful metadata in an
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.136.1">
      unsupervised/semi-supervised way.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.137.1">
     A significant benefit of LLMs is their capacity to adapt to new tasks with minimal or no labeled data, owing to their extensive pre-training on broad text datasets.
    </span>
    <span class="koboSpan" id="kobo.137.2">
     This ability for few-shot learning can help reduce the dependency of GNNs on extensive
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.138.1">
      labeled datasets.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.139.1">
     One strategy involves employing LLMs to directly predict outcomes for graph-related tasks by framing the graph’s structure and the information of its nodes within natural language prompts.
    </span>
    <span class="koboSpan" id="kobo.139.2">
     Techniques such as InstructGLM refine LLMs such as Llama and GPT-4 with well-crafted prompts that detail the graph’s topology, including aspects such as node connections and neighborhoods.
    </span>
    <span class="koboSpan" id="kobo.139.3">
     These optimized LLMs are capable of making predictions for
    </span>
    <a id="_idIndexMarker426">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     tasks
    </span>
    <a id="_idIndexMarker427">
    </a>
    <span class="koboSpan" id="kobo.141.1">
     such as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.142.1">
      node classification
     </span>
    </strong>
    <span class="koboSpan" id="kobo.143.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.144.1">
      link prediction
     </span>
    </strong>
    <span class="koboSpan" id="kobo.145.1">
     without
    </span>
    <a id="_idIndexMarker428">
    </a>
    <span class="koboSpan" id="kobo.146.1">
     requiring any labeled
    </span>
    <a id="_idIndexMarker429">
    </a>
    <span class="koboSpan" id="kobo.147.1">
     examples at the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.148.1">
      inference stage.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-119">
    <a id="_idTextAnchor122">
    </a>
    <span class="koboSpan" id="kobo.149.1">
     Leveraging InstructGLM
    </span>
   </h2>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.150.1">
      InstructGLM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.151.1">
     is a framework
    </span>
    <a id="_idIndexMarker430">
    </a>
    <span class="koboSpan" id="kobo.152.1">
     that leverages natural language to describe both graph structure and node features to a generative LLM, addressing graph-related problems through instruction-tuning.
    </span>
    <span class="koboSpan" id="kobo.152.2">
     It’s a proposed instruction
    </span>
    <a id="_idIndexMarker431">
    </a>
    <span class="koboSpan" id="kobo.153.1">
     fine-tuned
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.154.1">
      graph language model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.155.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.156.1">
      GLM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.157.1">
     ) that utilizes natural language instructions for graph machine learning, offering a powerful NLP interface for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.158.1">
      graph-related tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.159.1">
     The InstructGLM framework involves a multi-task, multi-prompt instructional tuning process to refine LLMs and integrate them with graphs effectively.
    </span>
    <span class="koboSpan" id="kobo.159.2">
     This approach aims to reduce the reliance on labeled data by utilizing self-supervised learning on graphs and leveraging LLMs as text encoders to enhance performance
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.160.1">
      and efficiency.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.161.1">
     The InstructGLM technique employs linguistic cues that articulate the patterns of connections and the characteristics of nodes within a graph.
    </span>
    <span class="koboSpan" id="kobo.161.2">
     These prompts serve as a teaching mechanism, guiding LLMs to comprehend the intricate architecture and inherent meaning
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.162.1">
      of graphs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.163.1">
     As shown in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.164.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.165.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.166.1">
     , the InstructGLM framework presents a sophisticated approach to multi-task, multi-prompt instructional tuning
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.167.1">
      for LLMs:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer271">
     <span class="koboSpan" id="kobo.168.1">
      <img alt="Figure 6.1 – InstructGLM multi-task usage. Source: Ye et al., 2024 (https://arxiv.org/abs/2308.07134)" src="image/B22118_06_1.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.169.1">
     Figure 6.1 – InstructGLM multi-task usage.
    </span>
    <span class="koboSpan" id="kobo.169.2">
     Source: Ye et al., 2024 (https://arxiv.org/abs/2308.07134)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.170.1">
     This figure illustrates
    </span>
    <a id="_idIndexMarker432">
    </a>
    <span class="koboSpan" id="kobo.171.1">
     the core components of InstructGLM, showcasing different types of prompts and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.172.1">
      their applications:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.173.1">
       1-hop prompt with meta node feature
      </span>
     </strong>
     <span class="koboSpan" id="kobo.174.1">
      : This prompt type categorizes central nodes based on their immediate connections, as shown in the blue box at the top left of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.175.1">
       the figure.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.176.1">
       3-hop prompt with intermediate paths
      </span>
     </strong>
     <span class="koboSpan" id="kobo.177.1">
      : Depicted in the green box, this prompt explores connections up to three hops away, providing a broader context for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.178.1">
       node classification.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.179.1">
       Structure-free prompt
      </span>
     </strong>
     <span class="koboSpan" id="kobo.180.1">
      : The yellow box demonstrates how InstructGLM can categorize nodes based on their inherent features without relying on
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.181.1">
       structural information.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.182.1">
       2-hop prompt with meta node feature &amp; intermediate nodes
      </span>
     </strong>
     <span class="koboSpan" id="kobo.183.1">
      : Illustrated in the pink box, this prompt type is used for link prediction tasks, considering connections up to two
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.184.1">
       hops away.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.185.1">
       1-hop prompt without meta node feature
      </span>
     </strong>
     <span class="koboSpan" id="kobo.186.1">
      : Another link prediction prompt, as shown in the orange box, this focuses on immediate connections without additional
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.187.1">
       node information.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.188.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.189.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.190.1">
     also highlights the dual focus of InstructGLM on node classification and link prediction tasks, as indicated by the dotted line separating these two
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.191.1">
      primary functions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.192.1">
     Although utilizing LLMs as opaque predictors has been effective, their accuracy diminishes for more intricate graph tasks where detailed modeling of the structure proves advantageous.
    </span>
    <span class="koboSpan" id="kobo.192.2">
     Consequently, some methods combine LLMs with GNNs, where the GNN maps out the graph structure, and the LLM enriches this with a deeper semantic understanding
    </span>
    <a id="_idIndexMarker433">
    </a>
    <span class="koboSpan" id="kobo.193.1">
     of the nodes based on their
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.194.1">
      textual descriptions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.195.1">
     Next, let’s see how LLMs can help us with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.196.1">
      graph learning.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-120">
    <a id="_idTextAnchor123">
    </a>
    <span class="koboSpan" id="kobo.197.1">
     LLMs for graph learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.198.1">
     Researchers have
    </span>
    <a id="_idIndexMarker434">
    </a>
    <span class="koboSpan" id="kobo.199.1">
     delved into various strategies for incorporating LLMs into the graph learning process.
    </span>
    <span class="koboSpan" id="kobo.199.2">
     Each method presents distinct benefits and potential uses.
    </span>
    <span class="koboSpan" id="kobo.199.3">
     Let’s look at some of the key functions that LLMs
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.200.1">
      can fulfill.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-121">
    <a id="_idTextAnchor124">
    </a>
    <span class="koboSpan" id="kobo.201.1">
     LLMs as enhancers
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.202.1">
     Traditional GNNs rely
    </span>
    <a id="_idIndexMarker435">
    </a>
    <span class="koboSpan" id="kobo.203.1">
     on the quality of initial node features, often with limited textual descriptions.
    </span>
    <span class="koboSpan" id="kobo.203.2">
     LLMs, with their vast knowledge and language comprehension abilities, can bridge this gap.
    </span>
    <span class="koboSpan" id="kobo.203.3">
     By enhancing these features, LLMs empower GNNs to capture intricate relationships and dynamics within the graph, leading to superior performance on tasks such as node classification or
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.204.1">
      link prediction.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.205.1">
     There are two primary methods for harnessing LLMs as enhancers.
    </span>
    <span class="koboSpan" id="kobo.205.2">
     The first is
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.206.1">
      feature-level enhancement
     </span>
    </strong>
    <span class="koboSpan" id="kobo.207.1">
     , which
    </span>
    <a id="_idIndexMarker436">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     can be achieved in various ways
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.209.1">
      using LLMs:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.210.1">
       Synonyms and related concepts
      </span>
     </strong>
     <span class="koboSpan" id="kobo.211.1">
      : The LLM goes beyond the surface level of the text description by recognizing synonyms and semantically related concepts.
     </span>
     <span class="koboSpan" id="kobo.211.2">
      This helps capture a broader range of information that might not be explicitly mentioned.
     </span>
     <span class="koboSpan" id="kobo.211.3">
      For instance, if a product description mentions
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.212.1">
       waterproof
      </span>
     </em>
     <span class="koboSpan" id="kobo.213.1">
      and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.214.1">
       hiking boots
      </span>
     </em>
     <span class="koboSpan" id="kobo.215.1">
      , the LLM can infer that the product is suitable for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.216.1">
       outdoor activities.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.217.1">
       Implicit relationships
      </span>
     </strong>
     <span class="koboSpan" id="kobo.218.1">
      : LLMs can extract implicit relationships from the text.
     </span>
     <span class="koboSpan" id="kobo.218.2">
      These relationships can be crucial for understanding the node’s context within the graph.
     </span>
     <span class="koboSpan" id="kobo.218.3">
      For example, in a social network, the LLM might infer a friendship between two nodes based on their frequent interactions, even if the word
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.219.1">
       friend
      </span>
     </em>
     <span class="koboSpan" id="kobo.220.1">
      is never
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.221.1">
       explicitly mentioned.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.222.1">
       External knowledge integration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.223.1">
      : LLMs can access and integrate external knowledge bases to further enrich the node representation.
     </span>
     <span class="koboSpan" id="kobo.223.2">
      This could involve linking product information to user reviews or connecting protein descriptions (in a biological network) to known
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.224.1">
       protein-protein interactions.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.225.1">
     The other
    </span>
    <a id="_idIndexMarker437">
    </a>
    <span class="koboSpan" id="kobo.226.1">
     method is
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.227.1">
      text-level enhancement
     </span>
    </strong>
    <span class="koboSpan" id="kobo.228.1">
     , which can be employed to create richer, more informative textual descriptions
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.229.1">
      for nodes.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.230.1">
     This approach focuses on
    </span>
    <a id="_idIndexMarker438">
    </a>
    <span class="koboSpan" id="kobo.231.1">
     creating entirely new textual descriptions for the nodes, which is particularly beneficial when the original descriptions are limited or lack context.
    </span>
    <span class="koboSpan" id="kobo.231.2">
     The LLM acts as a content generator, leveraging information about the node and its surrounding context within the graph to create a new, more informative description.
    </span>
    <span class="koboSpan" id="kobo.231.3">
     This context might include the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.232.1">
      following aspects:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.233.1">
      Original
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.234.1">
       text description
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.235.1">
      Labels of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.236.1">
       neighboring nodes
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.237.1">
      The structure of the graph (for example, the number of edges connected to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.238.1">
       the node)
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.239.1">
     The LLM utilizes this information to generate a rich textual description that captures the relationships with neighboring nodes, the overall network structure, and any relevant external knowledge.
    </span>
    <span class="koboSpan" id="kobo.239.2">
     This newly created description becomes the node’s enhanced feature, providing the GNN with a more comprehensive understanding of the node’s role within
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.240.1">
      the
     </span>
    </span>
    <span class="No-Break">
     <a id="_idIndexMarker439">
     </a>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.241.1">
      graph.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.242.1">
     Benefits and challenges of LLM enhancement
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.243.1">
     LLM-based enhancement
    </span>
    <a id="_idIndexMarker440">
    </a>
    <span class="koboSpan" id="kobo.244.1">
     offers several advantages for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.245.1">
      graph learning:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.246.1">
       Improved feature representation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.247.1">
      : The enhanced node features capture a richer and more nuanced understanding of the node’s context within the graph.
     </span>
     <span class="koboSpan" id="kobo.247.2">
      This allows GNNs to learn more complex relationships and patterns, leading to improved performance on tasks such as node classification, link prediction, and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.248.1">
       community detection.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.249.1">
       Handling limited data
      </span>
     </strong>
     <span class="koboSpan" id="kobo.250.1">
      : LLMs can address situations where node descriptions are sparse or lack detail.
     </span>
     <span class="koboSpan" id="kobo.250.2">
      By inferring relationships and leveraging external knowledge, they create more informative representations, mitigating the challenges associated with limited
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.251.1">
       data availability.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.252.1">
       Identifying implicit connections
      </span>
     </strong>
     <span class="koboSpan" id="kobo.253.1">
      : LLMs can go beyond the surface-level information in node features and identify subtle connections based on their understanding of language.
     </span>
     <span class="koboSpan" id="kobo.253.2">
      This can be crucial for tasks such as uncovering hidden communities within a social network or predicting the existence of links between nodes in a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.254.1">
       biological network.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.255.1">
     However, while LLM enhancement offers a compelling path forward, there are a few challenges
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.256.1">
      to navigate:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.257.1">
       The cost of computation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.258.1">
      : Training and running LLMs can be computationally expensive, especially for massive graphs.
     </span>
     <span class="koboSpan" id="kobo.258.2">
      Careful optimization strategies are needed to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.259.1">
       ensure scalability.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.260.1">
       Data bias inheritance
      </span>
     </strong>
     <span class="koboSpan" id="kobo.261.1">
      : LLMs aren’t immune to biases present in their training data.
     </span>
     <span class="koboSpan" id="kobo.261.2">
      It’s crucial to ensure the LLM that’s used for enhancement is trained on high-quality, unbiased data to prevent
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.262.1">
       skewed results.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.263.1">
       The explainability enigma
      </span>
     </strong>
     <span class="koboSpan" id="kobo.264.1">
      : Understanding how LLMs generate enhanced features can be challenging.
     </span>
     <span class="koboSpan" id="kobo.264.2">
      This lack of transparency can make it difficult to interpret the results of GNNs that utilize these features.
     </span>
     <span class="koboSpan" id="kobo.264.3">
      Researchers are actively working on developing methods to make the enhancement process that’s implemented by
     </span>
     <a id="_idIndexMarker441">
     </a>
     <span class="koboSpan" id="kobo.265.1">
      LLMs
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.266.1">
       more transparent.
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.267.1">
     Real-world applications
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.268.1">
     Here are multiple real-world
    </span>
    <a id="_idIndexMarker442">
    </a>
    <span class="koboSpan" id="kobo.269.1">
     examples of how LLMs can enhance the graph
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.270.1">
      learning process.
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.271.1">
       Drug discovery and bioinformatics
      </span>
     </strong>
     <span class="koboSpan" id="kobo.272.1">
      : In drug discovery, predicting adverse
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.273.1">
       drug-drug interactions
      </span>
     </strong>
     <span class="koboSpan" id="kobo.274.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.275.1">
       DDIs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.276.1">
      ) is
     </span>
     <a id="_idIndexMarker443">
     </a>
     <span class="koboSpan" id="kobo.277.1">
      crucial for patient safety.
     </span>
     <span class="koboSpan" id="kobo.277.2">
      Traditional methods often struggle due to the sheer volume of possible drug combinations and interactions.
     </span>
     <span class="koboSpan" id="kobo.277.3">
      GNNs can model these relationships by representing drugs as
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.278.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.279.1">
      and known interactions as
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.280.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.281.1">
      .
     </span>
     <span class="koboSpan" id="kobo.281.2">
      When enhanced with LLMs, which process biomedical literature to extract information about drug mechanisms, side effects, and interactions, these models become significantly more powerful.
     </span>
     <span class="koboSpan" id="kobo.281.3">
      The LLM-generated embeddings enrich the node and edge features in the GNN, leading to more accurate predictions of potential DDIs and ultimately safer
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.282.1">
       medication management.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.283.1">
       Social network analysis
      </span>
     </strong>
     <span class="koboSpan" id="kobo.284.1">
      : Detecting misinformation on social media is another area where LLMs can enhance GNNs.
     </span>
     <span class="koboSpan" id="kobo.284.2">
      GNNs can model social networks with
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.285.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.286.1">
      representing users and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.287.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.288.1">
      representing interactions such as likes, shares, and comments.
     </span>
     <span class="koboSpan" id="kobo.288.2">
      By processing the content of posts, an LLM can extract themes, sentiments, and potentially misleading information, which are then integrated into the graph.
     </span>
     <span class="koboSpan" id="kobo.288.3">
      This enrichment enables the GNN to better identify clusters of misinformation and predict which users are most likely to spread false information, facilitating more effective interventions to maintain
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.289.1">
       information integrity.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.290.1">
       Financial fraud detection
      </span>
     </strong>
     <span class="koboSpan" id="kobo.291.1">
      : Financial fraud detection involves identifying suspicious patterns among millions of transactions.
     </span>
     <span class="koboSpan" id="kobo.291.2">
      GNNs can represent these transactions as graphs, with
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.292.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.293.1">
      as accounts and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.294.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.295.1">
      as transactions.
     </span>
     <span class="koboSpan" id="kobo.295.2">
      An LLM can analyze transaction descriptions and notes to extract keywords and patterns indicative of fraud, enhancing the GNN’s node and edge features.
     </span>
     <span class="koboSpan" id="kobo.295.3">
      This integration allows the GNN to detect fraudulent transactions more accurately by considering both transactional patterns and contextual information from textual descriptions, leading to more robust fraud
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.296.1">
       detection systems.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.297.1">
       Academic research and collaboration networks
      </span>
     </strong>
     <span class="koboSpan" id="kobo.298.1">
      : Identifying potential research collaborators is essential for advancing scientific discovery.
     </span>
     <span class="koboSpan" id="kobo.298.2">
      GNNs can model academic networks, with
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.299.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.300.1">
      representing researchers and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.301.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.302.1">
      representing co-authorship or citation relationships.
     </span>
     <span class="koboSpan" id="kobo.302.2">
      An LLM can analyze publication abstracts, keywords, and research interests, transforming these textual features into
     </span>
     <a id="_idIndexMarker444">
     </a>
     <span class="koboSpan" id="kobo.303.1">
      embeddings that are integrated into the graph.
     </span>
     <span class="koboSpan" id="kobo.303.2">
      This enhancement enables the GNN to recommend potential collaborators by considering both structural relationships and semantic similarities in research interests, fostering more effective
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.304.1">
       scientific collaborations.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.305.1">
       Knowledge graph construction
      </span>
     </strong>
     <span class="koboSpan" id="kobo.306.1">
      : Building comprehensive knowledge graphs involves integrating information from diverse and often unstructured sources.
     </span>
     <span class="koboSpan" id="kobo.306.2">
      GNNs can model knowledge graphs with
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.307.1">
       nodes
      </span>
     </em>
     <span class="koboSpan" id="kobo.308.1">
      representing entities and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.309.1">
       edges
      </span>
     </em>
     <span class="koboSpan" id="kobo.310.1">
      representing relationships.
     </span>
     <span class="koboSpan" id="kobo.310.2">
      An LLM can extract entities and relationships from textual data sources, such as news articles, scientific literature, and web pages, and use these insights to augment the knowledge graph with additional nodes and edges.
     </span>
     <span class="koboSpan" id="kobo.310.3">
      This enhancement allows the GNN to build more complete and accurate knowledge graphs by incorporating detailed and contextually rich information from a wide array of textual sources, facilitating better knowledge representation
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.311.1">
       and discovery.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.312.1">
     The integration of
    </span>
    <a id="_idIndexMarker445">
    </a>
    <span class="koboSpan" id="kobo.313.1">
     LLMs with GNNs provides a powerful approach to enhancing various applications by incorporating rich, contextual information from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.314.1">
      textual data.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-122">
    <a id="_idTextAnchor125">
    </a>
    <span class="koboSpan" id="kobo.315.1">
     LLMs as predictors
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.316.1">
     With the advancement
    </span>
    <a id="_idIndexMarker446">
    </a>
    <span class="koboSpan" id="kobo.317.1">
     in language understanding, thanks to transformer-based LLMs, researchers are exploring how best to represent graph data in text for LLMs.
    </span>
    <span class="koboSpan" id="kobo.317.2">
     A recent paper titled
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.318.1">
      Can Language Models Solve Graph Problems in Natural Language?
     </span>
    </em>
    <span class="koboSpan" id="kobo.319.1">
     (
    </span>
    <a href="https://arxiv.org/html/2305.10037v3">
     <span class="koboSpan" id="kobo.320.1">
      https://arxiv.org/html/2305.10037v3
     </span>
    </a>
    <span class="koboSpan" id="kobo.321.1">
     ) talks about an LLM constructing graphs from text descriptions, enhancing its graph comprehension.
    </span>
    <span class="koboSpan" id="kobo.321.2">
     This paper demonstrates how prompting can help LLM understand graph structure and provide results using a set of instructions (algorithms) provided in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.322.1">
      the prompt.
     </span>
    </span>
   </p>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.323.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.324.1">
      .2
     </span>
    </em>
    <span class="koboSpan" id="kobo.325.1">
     shows three distinct approaches to graph analysis through
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.326.1">
      prompting techniques:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer272">
     <span class="koboSpan" id="kobo.327.1">
      <img alt="Figure 6.2 – Understanding graph structure using prompting. Source: Wang et al., 2024 (https://arxiv.org/html/2305.10037v3)" src="image/B22118_06_2.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.328.1">
     Figure 6.2 – Understanding graph structure using prompting.
    </span>
    <span class="koboSpan" id="kobo.328.2">
     Source: Wang et al., 2024 (https://arxiv.org/html/2305.10037v3)
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.329.1">
     Note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.330.1">
     The figure
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.331.1">
      Overview of Build-a-Graph prompting and Algorithmic prompting
     </span>
    </em>
    <span class="koboSpan" id="kobo.332.1">
     (
    </span>
    <a href="https://arxiv.org/html/2305.10037v3">
     <span class="koboSpan" id="kobo.333.1">
      https://arxiv.org/html/2305.10037v3
     </span>
    </a>
    <span class="koboSpan" id="kobo.334.1">
     ) by Wang et al.
    </span>
    <span class="koboSpan" id="kobo.334.2">
     (2024) is licensed under CC
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.335.1">
      BY 4.0.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.336.1">
     This illustration demonstrates how LLMs can be guided to comprehend and solve graph-related problems using different
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.337.1">
      prompting strategies:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.338.1">
       Standard prompting
      </span>
     </strong>
     <span class="koboSpan" id="kobo.339.1">
      : The first column presents the standard prompting method.
     </span>
     <span class="koboSpan" id="kobo.339.2">
      Here, a simple undirected graph is depicted with nodes numbered from 0 to 4.
     </span>
     <span class="koboSpan" id="kobo.339.3">
      The prompt provides context by describing the graph’s structure, including the weights of edges connecting various nodes.
     </span>
     <span class="koboSpan" id="kobo.339.4">
      The question that’s posed is to find the shortest path from node 0 to node 2, demonstrating a basic graph
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.340.1">
       traversal problem.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.341.1">
       Build-a-graph prompting
      </span>
     </strong>
     <span class="koboSpan" id="kobo.342.1">
      : The middle column introduces a more sophisticated approach called build-a-graph prompting.
     </span>
     <span class="koboSpan" id="kobo.342.2">
      This method begins by instructing the LLM to construct the graph based on the given information.
     </span>
     <span class="koboSpan" id="kobo.342.3">
      It then guides the model through the process of identifying all possible paths between nodes 0 and 2 and calculating their total weights.
     </span>
     <span class="koboSpan" id="kobo.342.4">
      This step-by-step approach helps the LLM to systematically analyze the graph and determine the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.343.1">
       shortest path.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.344.1">
       Algorithmic prompting
      </span>
     </strong>
     <span class="koboSpan" id="kobo.345.1">
      : The rightmost column showcases algorithmic prompting, which is the most advanced technique presented.
     </span>
     <span class="koboSpan" id="kobo.345.2">
      It outlines a
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.346.1">
       depth-first search
      </span>
     </strong>
     <span class="koboSpan" id="kobo.347.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.348.1">
       DFS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.349.1">
      ) algorithm to
     </span>
     <a id="_idIndexMarker447">
     </a>
     <span class="koboSpan" id="kobo.350.1">
      find the shortest path between two nodes in an undirected graph.
     </span>
     <span class="koboSpan" id="kobo.350.2">
      This method provides a detailed explanation of how to implement the algorithm, including tracking distances and backtracking to identify the optimal path.
     </span>
     <span class="koboSpan" id="kobo.350.3">
      The prompt then applies this algorithm to the same graph problem, demonstrating how a more complex analytical approach can be used to solve graph
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.351.1">
       traversal questions.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.352.1">
     The
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.353.1">
      GPT4Graph
     </span>
    </strong>
    <span class="koboSpan" id="kobo.354.1">
     (
    </span>
    <a href="https://arxiv.org/pdf/2305.15066">
     <span class="koboSpan" id="kobo.355.1">
      https://arxiv.org/pdf/2305.15066
     </span>
    </a>
    <span class="koboSpan" id="kobo.356.1">
     ) study
    </span>
    <a id="_idIndexMarker448">
    </a>
    <span class="koboSpan" id="kobo.357.1">
     used graph markup languages and self-prompting
    </span>
    <a id="_idIndexMarker449">
    </a>
    <span class="koboSpan" id="kobo.358.1">
     to improve LLM understanding before generating the final output.
    </span>
    <span class="koboSpan" id="kobo.358.2">
     The main strategy involves inputting graph data, ensuring the LLM understands it, and then querying it.
    </span>
    <span class="koboSpan" id="kobo.358.3">
     However, this method struggles with scalability for large graphs due to context length limitations and requires new prompts for new tasks.
    </span>
    <span class="koboSpan" id="kobo.358.4">
     To address these challenges and improve graph learning capabilities, let’s explore how to integrate RAG with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.359.1">
      graph learning.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-123">
    <a id="_idTextAnchor126">
    </a>
    <span class="koboSpan" id="kobo.360.1">
     Integrating RAG with graph learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.361.1">
     RAG is an AI framework
    </span>
    <a id="_idIndexMarker450">
    </a>
    <span class="koboSpan" id="kobo.362.1">
     that combines
    </span>
    <a id="_idIndexMarker451">
    </a>
    <span class="koboSpan" id="kobo.363.1">
     the power of LLMs with external knowledge retrieval to produce more accurate, relevant, and up-to-date responses.
    </span>
    <span class="koboSpan" id="kobo.363.2">
     Here’s how
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.364.1">
      it works:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.365.1">
       Information retrieval
      </span>
     </strong>
     <span class="koboSpan" id="kobo.366.1">
      : When a query is received, RAG searches a knowledge base or database to find
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.367.1">
       relevant information.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.368.1">
       Context augmentation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.369.1">
      : The retrieved information is then used to augment the input to the language model, providing it with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.370.1">
       additional context.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.371.1">
       Generation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.372.1">
      : The LLM uses this augmented input to generate a response that’s both fluent and grounded in the information that’s
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.373.1">
       been retrieved.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.374.1">
     RAG offers several
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.375.1">
      compelling perks:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.376.1">
       Improved accuracy
      </span>
     </strong>
     <span class="koboSpan" id="kobo.377.1">
      : By grounding responses in retrieved information, RAG reduces hallucinations and improves
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.378.1">
       factual accuracy.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.379.1">
       Up-to-date information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.380.1">
      : The knowledge base can be updated regularly, allowing the system to access current information without having to retrain the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.381.1">
       entire model.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.382.1">
       Transparency
      </span>
     </strong>
     <span class="koboSpan" id="kobo.383.1">
      : RAG can provide sources for its information, increasing trustworthiness and allowing
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.384.1">
       for fact-checking.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.385.1">
     While traditional RAG approaches have proven effective, combining RAG with graph learning techniques offers even more powerful capabilities for information retrieval
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.386.1">
      and generation.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-124">
    <a id="_idTextAnchor127">
    </a>
    <span class="koboSpan" id="kobo.387.1">
     Advantages of graph RAG (GRAG) approaches
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.388.1">
     Graph learning
    </span>
    <a id="_idIndexMarker452">
    </a>
    <span class="koboSpan" id="kobo.389.1">
     leverages the inherent structure and relationships within data, which can significantly enhance the context and relevance of the information that’s retrieved.
    </span>
    <span class="koboSpan" id="kobo.389.2">
     The general benefits of integrating graphs with RAG are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.390.1">
      as follows:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.391.1">
       Structural context
      </span>
     </strong>
     <span class="koboSpan" id="kobo.392.1">
      : Graphs capture complex relationships between entities, providing a richer context than flat
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.393.1">
       text documents.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.394.1">
       Multi-hop reasoning
      </span>
     </strong>
     <span class="koboSpan" id="kobo.395.1">
      : Graph structures allow information to be retrieved across multiple connected entities, facilitating more complex
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.396.1">
       reasoning tasks.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.397.1">
       Improved relevance
      </span>
     </strong>
     <span class="koboSpan" id="kobo.398.1">
      : By considering both textual and topological information, graph-based retrieval can identify more pertinent information for the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.399.1">
       generation process.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.400.1">
     In the following sections, we’ll explore the advantages of a few
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.401.1">
      specific
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.402.1">
      approaches.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.403.1">
     Knowledge graph RAG
    </span>
   </h3>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.404.1">
      Knowledge graph RAG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.405.1">
     leverages
    </span>
    <a id="_idIndexMarker453">
    </a>
    <span class="koboSpan" id="kobo.406.1">
     structured knowledge graphs to enhance the retrieval and generation process.
    </span>
    <span class="koboSpan" id="kobo.406.2">
     This approach offers several
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.407.1">
      key advantages:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.408.1">
       Precise entity and relationship retrieval
      </span>
     </strong>
     <span class="koboSpan" id="kobo.409.1">
      : By utilizing the structured nature of knowledge graphs, this method can retrieve not just relevant entities, but also the relationships between them.
     </span>
     <span class="koboSpan" id="kobo.409.2">
      For example, a biomedical knowledge graph can
     </span>
     <a id="_idIndexMarker454">
     </a>
     <span class="koboSpan" id="kobo.410.1">
      retrieve not only a specific drug, but also its interactions with other drugs, its side effects, and its
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.411.1">
       approved uses.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.412.1">
       Contextual enrichment
      </span>
     </strong>
     <span class="koboSpan" id="kobo.413.1">
      : The retrieved information includes the broader context of entities within the graph.
     </span>
     <span class="koboSpan" id="kobo.413.2">
      This allows the LLM to understand the entity’s place in a larger network of information, leading to more nuanced and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.414.1">
       accurate responses.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.415.1">
       Hierarchical information access
      </span>
     </strong>
     <span class="koboSpan" id="kobo.416.1">
      : Knowledge graphs often contain hierarchical relationships (for example,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.417.1">
       is-a
      </span>
     </em>
     <span class="koboSpan" id="kobo.418.1">
      and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.419.1">
       part-of
      </span>
     </em>
     <span class="koboSpan" id="kobo.420.1">
      ).
     </span>
     <span class="koboSpan" id="kobo.420.2">
      This structure allows for more flexible retrieval, where the system can access both specific details and broader categories
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.421.1">
       as needed.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.422.1">
       Multi-hop reasoning
      </span>
     </strong>
     <span class="koboSpan" id="kobo.423.1">
      : Knowledge graph RAG can facilitate multi-hop reasoning by retrieving paths of connected entities and relationships.
     </span>
     <span class="koboSpan" id="kobo.423.2">
      This is particularly useful for complex queries that require information from multiple sources to be pieced together within
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.424.1">
       the graph.
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.425.1">
     GNNs
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.426.1">
     As we have seen, GNNs are powerful tools
    </span>
    <a id="_idIndexMarker455">
    </a>
    <span class="koboSpan" id="kobo.427.1">
     for learning representations of graph-structured data.
    </span>
    <span class="koboSpan" id="kobo.427.2">
     In the context of RAG, they offer
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.428.1">
      several benefits:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.429.1">
       Learning graph embeddings
      </span>
     </strong>
     <span class="koboSpan" id="kobo.430.1">
      : GNNs can create dense vector representations (embeddings) of nodes, edges, and subgraphs.
     </span>
     <span class="koboSpan" id="kobo.430.2">
      These embeddings capture both local and global structural information, allowing for more effective retrieval of relevant
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.431.1">
       graph components.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.432.1">
       Capturing graph topology
      </span>
     </strong>
     <span class="koboSpan" id="kobo.433.1">
      : Unlike traditional neural networks, GNNs explicitly model the relationships between entities in a graph.
     </span>
     <span class="koboSpan" id="kobo.433.2">
      This allows them to capture complex topological features that are crucial for understanding the context and relevance of information in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.434.1">
       a graph.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.435.1">
       Scalability
      </span>
     </strong>
     <span class="koboSpan" id="kobo.436.1">
      : GNNs can process large-scale graphs efficiently, making them suitable for real-world knowledge bases that often contain millions of entities
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.437.1">
       and relationships.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.438.1">
       Inductive learning
      </span>
     </strong>
     <span class="koboSpan" id="kobo.439.1">
      : Many GNN architectures support inductive learning, allowing them to generalize to unseen nodes or even entirely new graphs.
     </span>
     <span class="koboSpan" id="kobo.439.2">
      This is particularly
     </span>
     <a id="_idIndexMarker456">
     </a>
     <span class="koboSpan" id="kobo.440.1">
      useful for dynamic knowledge bases that are
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.441.1">
       constantly updated.
      </span>
     </span>
    </li>
   </ul>
   <h3>
    <span class="koboSpan" id="kobo.442.1">
     GRAG
    </span>
   </h3>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.443.1">
      Graph RAG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.444.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.445.1">
      GRAG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.446.1">
     ) is an advanced technique that emphasizes the importance of subgraph structures throughout both the retrieval and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.447.1">
      generation processes:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.448.1">
       Subgraph-aware retrieval
      </span>
     </strong>
     <span class="koboSpan" id="kobo.449.1">
      : Instead of retrieving individual entities or relationships, GRAG focuses on retrieving relevant subgraphs.
     </span>
     <span class="koboSpan" id="kobo.449.2">
      This approach preserves the local structure around entities of interest, providing a richer context for the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.450.1">
       generation process.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.451.1">
       Topology-preserving generation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.452.1">
      : GRAG maintains awareness of the graph topology during the generation process.
     </span>
     <span class="koboSpan" id="kobo.452.2">
      This ensures that the generated text respects the structural relationships present in the retrieved subgraphs, leading to more coherent and factually
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.453.1">
       consistent outputs.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.454.1">
       Soft pruning
      </span>
     </strong>
     <span class="koboSpan" id="kobo.455.1">
      : GRAG often employs a soft pruning mechanism to refine retrieved subgraphs.
     </span>
     <span class="koboSpan" id="kobo.455.2">
      This process removes less relevant nodes and edges while maintaining the overall structure, helping to focus the LLM on the most
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.456.1">
       pertinent information.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.457.1">
       Hierarchical text conversion
      </span>
     </strong>
     <span class="koboSpan" id="kobo.458.1">
      : GRAG typically includes methods for converting subgraphs into hierarchical text descriptions.
     </span>
     <span class="koboSpan" id="kobo.458.2">
      This conversion preserves both the textual content and the structural information of the graph, allowing the LLM to work with a rich,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.459.1">
       structured input.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.460.1">
       Multi-hop reasoning support
      </span>
     </strong>
     <span class="koboSpan" id="kobo.461.1">
      : By maintaining subgraph structures, GRAG naturally supports multi-hop reasoning tasks.
     </span>
     <span class="koboSpan" id="kobo.461.2">
      The LLM can traverse the retrieved subgraph to connect distant pieces of information, enabling more
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.462.1">
       complex inferencing.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.463.1">
     Let’s consider how GRAG might be applied in a customer support system for a tech company that utilizes a knowledge base structured as a knowledge graph.
    </span>
    <span class="koboSpan" id="kobo.463.2">
     The knowledge graph contains interconnected information about products, error codes, troubleshooting steps, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.464.1">
      user manuals.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.465.1">
     Suppose a user submits the following query:
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.466.1">
      How do I resolve Error Code E101 on my
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.467.1">
       SmartHome Router?
      </span>
     </em>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.468.1">
     Traditional methods might retrieve isolated entities, like the product name or error code, which could lead to fragmented or incomplete answers.
    </span>
    <span class="koboSpan" id="kobo.468.2">
     GRAG takes a unique approach to provide an accurate and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.469.1">
      context-rich response:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.470.1">
      GRAG focuses on subgraph-aware retrieval, extracting a structured, coherent subgraph that relates to
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.471.1">
       Error Code E101
      </span>
     </em>
     <span class="koboSpan" id="kobo.472.1">
      and the
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.473.1">
       SmartHome Router
      </span>
     </em>
     <span class="koboSpan" id="kobo.474.1">
      .
     </span>
     <span class="koboSpan" id="kobo.474.2">
      This subgraph can include details like the cause of the error (e.g., a
     </span>
     <strong class="bold">
     </strong>
     <span class="koboSpan" id="kobo.475.1">
      network configuration conflict), the troubleshooting steps (e.g., updating firmware, resetting the router, or changing network settings), and related links to the router’s user manual or firmware
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.476.1">
       update pages.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.477.1">
      Once the subgraph is retrieved, GRAG applies a soft pruning mechanism to preserve the integrity of the local graph structure.
     </span>
     <span class="koboSpan" id="kobo.477.2">
      Irrelevant or tangential information, such as troubleshooting steps for other devices or unrelated error codes,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.478.1">
       is removed.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.479.1">
      GRAG transforms the pruned subgraph into hierarchical text descriptions, which maintain the graph’s structure while converting the information into a form that the LLM can seamlessly process.
     </span>
     <span class="koboSpan" id="kobo.479.2">
      Our subgraph is converted into a text hierarchy that organizes information
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.480.1">
       as follows:
      </span>
     </span>
     <ol>
      <li class="upper-roman">
       <span class="koboSpan" id="kobo.481.1">
        An overview of
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.482.1">
         Error Code E101
        </span>
       </em>
       <span class="koboSpan" id="kobo.483.1">
        and its cause (a network
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.484.1">
         configuration conflict)
        </span>
       </span>
      </li>
      <li class="upper-roman">
       <span class="koboSpan" id="kobo.485.1">
        Troubleshooting steps provided in a
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.486.1">
         logical sequence
        </span>
       </span>
      </li>
      <li class="upper-roman">
       <span class="koboSpan" id="kobo.487.1">
        Additional details where relevant, including reports from users linking E101 to recent
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.488.1">
         firmware updates
        </span>
       </span>
      </li>
     </ol>
    </li>
    <li>
     <span class="koboSpan" id="kobo.489.1">
      GRAG employs topology-preserving generation to ensure that the relationships and structure encoded in the subgraph are reflected in the response.
     </span>
     <span class="koboSpan" id="kobo.489.2">
      The system can now generate a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.490.1">
       detailed reply:
      </span>
     </span>
     <p class="list-inset">
      <em class="italic">
       <span class="koboSpan" id="kobo.491.1">
        The Error Code E101 on your SmartHome Router typically occurs due to a network configuration conflict.
       </span>
       <span class="koboSpan" id="kobo.491.2">
        To resolve it, verify if your router’s firmware is updated.
       </span>
       <span class="koboSpan" id="kobo.491.3">
        If it is, reset the router by holding the reset button for 10 seconds.
       </span>
       <span class="koboSpan" id="kobo.491.4">
        You may also need to update the network settings to avoid IP conflicts.
       </span>
       <span class="koboSpan" id="kobo.491.5">
        Note that some users have reported encountering this error after recent firmware updates, so rolling back to a previous version might help if the
       </span>
      </em>
      <span class="No-Break">
       <em class="italic">
        <span class="koboSpan" id="kobo.492.1">
         issue persists.
        </span>
       </em>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.493.1">
       As mentioned, a key strength of GRAG is its multi-hop reasoning support.
      </span>
      <span class="koboSpan" id="kobo.493.2">
       For example, the system might connect
      </span>
      <em class="italic">
       <span class="koboSpan" id="kobo.494.1">
        Error Code E101
       </span>
      </em>
      <span class="koboSpan" id="kobo.495.1">
       to a network configuration conflict and, from there, to specific troubleshooting steps in the user manual.
      </span>
      <span class="koboSpan" id="kobo.495.2">
       This enables the LLM to infer complex relationships and provide a comprehensive answer without missing
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.496.1">
        valuable context.
       </span>
      </span>
     </p>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.497.1">
     These approaches offer unique strengths in leveraging graph structures for RAG.
    </span>
    <span class="koboSpan" id="kobo.497.2">
     The choice between them often depends on the specific requirements of the application, the nature of the knowledge base, and the complexity of the queries being handled.
    </span>
    <span class="koboSpan" id="kobo.497.3">
     Moreover, implementing your chosen approach carefully is crucial because of challenges such as managing large and noisy graphs, maintaining low latency, and adapting LLMs to handle structured
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.498.1">
      graph inputs.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-125">
    <a id="_idTextAnchor128">
    </a>
    <span class="koboSpan" id="kobo.499.1">
     Challenges in integrating LLMs with graph learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.500.1">
     It’s worth noting that
    </span>
    <a id="_idIndexMarker457">
    </a>
    <span class="koboSpan" id="kobo.501.1">
     integrating LLMs with graph learning involves
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.502.1">
      several challenges:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.503.1">
       Efficiency and scalability
      </span>
     </strong>
     <span class="koboSpan" id="kobo.504.1">
      : LLMs require significant computational resources, which poses deployment challenges in real-world applications, particularly on resource-constrained devices.
     </span>
     <span class="koboSpan" id="kobo.504.2">
      Knowledge distillation, where an LLM (teacher model) transfers knowledge to a smaller, efficient GNN (student model), offers a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.505.1">
       promising solution.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.506.1">
       Data leakage and evaluation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.507.1">
      : LLMs pre-trained on vast datasets risk data leakage, potentially inflating performance metrics.
     </span>
     <span class="koboSpan" id="kobo.507.2">
      Mitigating this requires new datasets and careful test data sampling.
     </span>
     <span class="koboSpan" id="kobo.507.3">
      Establishing fair evaluation benchmarks is also crucial for accurate
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.508.1">
       performance assessment.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.509.1">
       Transferability and explainability
      </span>
     </strong>
     <span class="koboSpan" id="kobo.510.1">
      : Enhancing LLMs’ ability to transfer knowledge across diverse graph domains and improving their explainability is vital.
     </span>
     <span class="koboSpan" id="kobo.510.2">
      Techniques such as chain-of-thought prompting can leverage LLMs’ reasoning capabilities for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.511.1">
       better transparency.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.512.1">
       Multimodal integration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.513.1">
      : Graphs often encompass multiple data types, including images, audio, and numeric data.
     </span>
     <span class="koboSpan" id="kobo.513.2">
      Extending LLM integration to these multimodal settings represents an exciting research opportunity.
     </span>
     <span class="koboSpan" id="kobo.513.3">
      With the rapid advancement in the quality of LLM generation, it’s going to play a critical role in augmenting intelligence
     </span>
     <a id="_idIndexMarker458">
     </a>
     <span class="koboSpan" id="kobo.514.1">
      over graph data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.515.1">
       and learning.
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-126">
    <a id="_idTextAnchor129">
    </a>
    <span class="koboSpan" id="kobo.516.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.517.1">
     In this chapter, we explored integrating LLMs with graph learning, highlighting how LLMs can enhance traditional GNNs.
    </span>
    <span class="koboSpan" id="kobo.517.2">
     We discussed the evolution of LLMs, their capabilities in processing textual data within graphs, and their potential to improve node representations and graph-related tasks.
    </span>
    <span class="koboSpan" id="kobo.517.3">
     You learned about various approaches for utilizing LLMs in graph learning, including feature-level and text-level enhancements, as well as using LLMs as predictors through techniques such
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.518.1">
      as InstructGLM.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.519.1">
     We also presented real-world applications in drug discovery, social network analysis, and financial fraud detection to illustrate the practical benefits of this integration.
    </span>
    <span class="koboSpan" id="kobo.519.2">
     Furthermore, you became familiar with the challenges in combining LLMs with graph learning, such as computational costs, data bias, and explainability issues, while learning about the potential for future advancements in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.520.1">
      this field.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.521.1">
     In the next chapter, we’ll explore applying deep learning to graphs in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.522.1">
      more depth.
     </span>
    </span>
   </p>
  </div>
 

  <div class="Content" id="_idContainer274">
   <h1 id="_idParaDest-127" lang="en-US" xml:lang="en-US">
    <a id="_idTextAnchor130">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     Part 3: Practical Applications and Implementation
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.2.1">
     In this part of the book, you will dive into the practical implementation of graph deep learning across various domains.
    </span>
    <span class="koboSpan" id="kobo.2.2">
     You will learn how to apply graph-based approaches to natural language processing, build recommendation systems, and leverage graph structures in computer
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.3.1">
      vision applications.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.4.1">
     This part has the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.5.1">
      following chapters:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <a href="B22118_07.xhtml#_idTextAnchor131">
      <em class="italic">
       <span class="koboSpan" id="kobo.6.1">
        Chapter 7
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.7.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.8.1">
       Graph Deep Learning in Practice
      </span>
     </em>
    </li>
    <li>
     <a href="B22118_08.xhtml#_idTextAnchor138">
      <em class="italic">
       <span class="koboSpan" id="kobo.9.1">
        Chapter 8
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.10.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.11.1">
       Graph Deep Learning for Natural Language Processing
      </span>
     </em>
    </li>
    <li>
     <a href="B22118_09.xhtml#_idTextAnchor156">
      <em class="italic">
       <span class="koboSpan" id="kobo.12.1">
        Chapter 9
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.13.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.14.1">
       Building Recommendation Systems Using Graph Deep Learning
      </span>
     </em>
    </li>
    <li>
     <a href="B22118_10.xhtml#_idTextAnchor182">
      <em class="italic">
       <span class="koboSpan" id="kobo.15.1">
        Chapter 10
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.16.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.17.1">
       Graph Deep Learning for Computer Vision
      </span>
     </em>
    </li>
   </ul>
  </div>
  <div>
   <div id="_idContainer275">
   </div>
  </div>
  <div>
   <div id="_idContainer276">
   </div>
  </div>
  <div>
   <div id="_idContainer277">
   </div>
  </div>
  <div>
   <div id="_idContainer278">
   </div>
  </div>
  <div>
   <div id="_idContainer279">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer280">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer281">
   </div>
  </div>
  <div>
   <div id="_idContainer282">
   </div>
  </div>
  <div>
   <div id="_idContainer283">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer284">
   </div>
  </div>
 </body></html>