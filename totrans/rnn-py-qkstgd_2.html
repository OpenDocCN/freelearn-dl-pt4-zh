<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Your First RNN with TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this chapter, you will gain a hands-on experience of building a <strong>recurrent neural network</strong> (<strong>RNN</strong>). First, you will be introduced to the most widely used machine learning library—TensorFlow. From learning the basics to advancing into some fundamental techniques, you will obtain a reasonable understanding of how to apply this powerful library to your applications. Then, you will take on a fairly simple task of building an actual model. The process will show you how to prepare your data, train the network, and make predictions.</p>
<p>In summary, the topics of this chapter include the following:</p>
<ul>
<li><strong>What are you going to build?</strong>: Introduction of your task</li>
<li><strong>Introduction to TensorFlow</strong>: Taking first steps into learning the TensorFlow framework</li>
<li><strong>Coding the RNN</strong>: You will go through the process of writing your first neural network using TensorFlow. This includes all steps required for a finished solution</li>
</ul>
<p>The prerequisites for this chapter are basic Python programming knowledge and decent understanding of recurrent neural networks captured in the <a href="d6266376-9b8b-4d69-925b-a4e56307951b.xhtml" target="_blank">Chapter 1</a>, <em>Introducing Recurrent Neural Networks</em>. After reading this chapter, you should have a full understanding of how to use TensorFlow with Python and how easy and straightforward it is to build a neural network. </p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What are you going to build?</h1>
                </header>
            
            <article>
                
<p> </p>
<p>Your first steps into the practical world of recurrent neural networks will be to build a simple model which determines the parity (<a href="http://mathworld.wolfram.com/Parity.html">http://mathworld.wolfram.com/Parity.html</a>) of a bit sequence . This is a warm-up exercise released by OpenAI in January 2018 (<a href="https://blog.openai.com/requests-for-research-2/">https://blog.openai.com/requests-for-research-2/</a>). The task can be explained as follows: </p>
<p>Given a binary string of a length of <kbd>50</kbd>, determine whether there is an even or odd number of ones. If that number is even, output <kbd>0</kbd>, otherwise <kbd>1</kbd>.</p>
<p class="p1">Later in this chapter, we will give a detailed explanation of the solution, together with addressing the difficult parts and how to tackle them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is an open source library built by Google, which aims to assist developers in creating machine learning models of any kind. The recent improvements in the deep learning space created the need for an easy and fast way of building neural networks. TensorFlow addresses this problem in an excellent fashion, by providing a wide range of APIs and tools to help developers focus on their specific problem, rather than dealing with mathematical equations and scalability issues. </p>
<p>TensorFlow offers two main ways of programming a model:</p>
<ul>
<li>Graph-based execution</li>
<li>Eager execution</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Graph-based execution</h1>
                </header>
            
            <article>
                
<p>Graph-based execution is an alternative way of representing mathematical equations and functions. Considering the expression <em>a = (b*c) + (d*e),</em> we can use a graph representation as follows:</p>
<ol>
<li>Separate the expression into the following: 
<ul>
<li><em>x = b*c</em></li>
<li><em>y = d*e</em></li>
<li><em>a = x+y</em></li>
</ul>
</li>
</ol>
<ol start="2">
<li>Build the following graph:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-445 image-border" src="assets/ac973fd4-5726-459a-8dbd-10d0aa1924a4.png" style="width:21.92em;height:9.08em;"/></div>
<p>As you can see from the previous example, using graphs lets compute two equations in parallel. This way, the code can be distributed among multiple CPUs/GPUs. </p>
<p>More complex variants of that example are used in TensorFlow for training heavy models. Following that technique, TensorFlow graph-based execution requires a two-step approach when building your neural network. One should first construct the graph architecture and then execute it to receive results. </p>
<p>This approach makes your application run faster and it will be distributed across multiple CPUs, GPUs, and so on. Unfortunately, some complexity comes along with it. Understanding how this way of programming work, and the inability to debug your code in the already familiar way (for example, printing values at any point in your program) makes the graph-based execution (see for more details <a href="http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/">http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/</a>) a bit challenging for beginners. </p>
<p>Even though this technique may introduce a new way of programming, our examples will be based upon it. The reason behind this decision lies in the fact that there are many more resources out there and almost every TensorFlow example you come across is graph-based. In addition, I believe it is of vital importance to understand the fundamentals, even if they introduce unfamiliar techniques. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Eager execution</h1>
                </header>
            
            <article>
                
<p>Eager execution is an approach, recently introduced by Google, which, as stated in the documentation (<a href="https://www.tensorflow.org/guide/eager">https://www.tensorflow.org/guide/eager</a>), uses the following:</p>
<div class="p1 packt_quote">An imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well.</div>
<p>As you can see, there is no overhead to learning the new programming technique and debugging is seamless. For a better understanding, I recommend checking this tutorial from the TensorFlow Conference 2018 (<a href="https://www.youtube.com/watch?v=T8AW0fKP0Hs">https://www.youtube.com/watch?v=T8AW0fKP0Hs</a>).</p>
<div class="packt_tip">I must state that, once you learn how to manipulate the TF API, building models becomes really easy on both graph-based and eager execution. Don't panic if the former seems complicated at first—I can assure you that it is worth investing the time to understand it properly. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding the recurrent neural network</h1>
                </header>
            
            <article>
                
<p>As mentioned before, the aim of our task is to build a recurrent neural network that predicts the parity of a bit sequence. We will approach this problem in a slightly different way. Since the parity of a sequence depends on the number of ones, we will sum up the elements of the sequence and find whether the result is even or not. If it is even, we will output <kbd>0</kbd>, otherwise, <kbd>1</kbd>. </p>
<p>This section of the chapter includes code samples and goes through the following steps:</p>
<ul>
<li>Generating data to train the model</li>
<li>Building the TensorFlow graph (using TensorFlow's built-in functions for recurrent neural networks)</li>
<li>Training the neural network with the generated data</li>
<li>Evaluating the model and determining its accuracy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating data</h1>
                </header>
            
            <article>
                
<p>Let's revisit the OpenAI's task (<a href="https://blog.openai.com/requests-for-research-2/">https://blog.openai.com/requests-for-research-2/</a>). As stated there, we need to generate a dataset of random 100,000 binary strings of length 50. In other words, our training set will be formed of 100,000 examples and the recurrent neural network will accept 50 time steps. The result of the last time step would be counted as the model prediction. </p>
<p>The task of determining the sum of a sequence can be viewed as a classification problem where the result can be any of the classes from <kbd>0</kbd> to <kbd>50</kbd>. A standard practice in machine learning is to encode the data into an easily decodable numeric way. But why is that? Most machine learning algorithms cannot accept anything apart from numeric data, so we need to always encode our input/output. This means that, our predictions will also come out in an encoding format. Thus, it is vital to understand the actual value behind these predictions. This means that we need to be able to easily decode them into a human understandable format.  A popular way of encoding data for classification problems is one-hot encoding.</p>
<p>Here is an example of that technique.</p>
<p>Imagine the predicted output for a specific sequence is <kbd>30</kbd>. We can encode this number by introducing a <kbd>1x50</kbd> array where all numbers, except the one in the 30th position, are <kbd>0s - [0, 0,..., 0, 1, 0, ..., 0, 0, 0]</kbd>. </p>
<p>Before preparing the actual data, we need to import all of the necessary libraries. To do that, follow this link (<a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a>)<sup> </sup>to <span>install Python on your machine</span>. In your command-line/Terminal window install the following packages:</p>
<pre><strong>pip3 install tensorflow</strong></pre>
<p>After you have done that, create a new file called <kbd>ch2_task.py</kbd> and import the following libraries:</p>
<pre>import tensorflow as tf<br/>import random</pre>
<p>Preparing the data requires an input and output value. The input value is a three-dimensional array of a size of <kbd>[100000, 50, 1]</kbd>, with <kbd>100000</kbd> items, each one containing <kbd>50</kbd> one-element arrays (either <kbd>0</kbd> or <kbd>1</kbd>), and is shown in the following example:</p>
<pre class="p1"><span class="s1">[</span> <span class="s1">[ [1], [0], [1], [1], …, [0], [1] ]<br/></span><span class="s1">[ [0], [1], [0], [1], …, [0], [1] ]<br/></span><span class="s1">[ [1], [1], [1], [0], …, [0], [0] ]<br/></span><span class="s1">[ [1], [0], [0], [0], …, [1], [1] ] ]<br/></span></pre>
<p>The following example shows the implementation:</p>
<pre>num_examples = 100000<br/>num_classes = 50<br/><br/>def input_values():<br/>    multiple_values = [map(int, '{0:050b}'.format(i)) for i in range(2**20)]<br/>    random.shuffle(multiple_values)<br/>    final_values = []<br/>    for value in multiple_values[:num_examples]:<br/>        temp = []<br/>        for number in value:<br/>            temp.append([number])<br/>        final_values.append(temp)<br/>    return final_values</pre>
<p>Here, <kbd>num_classes</kbd> is the number of time steps in our RNN (<kbd>50</kbd>, in this example). The preceding code returns a list with the <kbd>100000</kbd> binary sequences. The style is not very Pythonic but, written in this way, it makes it easy to <span>follow and </span>understand. </p>
<p>First, we start with initializing the <kbd>multiple_values</kbd> variable. It contains a binary representation of the first <kbd>2<sup>20 </sup>= <span>1,048,576</span></kbd> numbers, where each binary number is padded with zeros to accommodate the length of <kbd>50</kbd>. Obtaining so many examples minimizes the chance of similarity between any two of them. We use the <kbd>map</kbd> function together with <kbd>int</kbd> in order to convert the produced string into a number.</p>
<p>Here is a quick example of how this works. We want to represent the number <kbd>2</kbd> inside the <kbd>multiple_values</kbd> array. The binary version of <kbd>2</kbd> is <kbd>'10'</kbd>, so the string produced after <kbd>'{0:050b}'.format(i)</kbd> where <kbd>i = 2</kbd>, is <span class="s1"><kbd>'00000000000000000000000000000000000000000000000010'</kbd> (48 zeros at the front to accommodate a length of <kbd>50</kbd>). Finally, the <kbd>map</kbd> function makes the previous string into a number without removing the zeros at the front.</span></p>
<p class="mce-root">Then, we shuffle the <kbd>multiple_values</kbd> array, assuring difference between neighboring elements. This is important during backpropagation when the network is trained, because we are iteratively looping throughout the array and training the network at each step using a single example. Having similar values next to each other inside the array may produce biased results and incorrect future predictions. </p>
<p>Finally, we enter a loop, which traverses over all of the binary elements and builds an array similar to the one we saw previously. An important thing to note is the usage of <kbd>num_examples</kbd>, which slices the array, so we pick only the first <kbd>100,000</kbd> values. </p>
<p>The second part of this section shows how to generate the expected output (the sum of all the elements in each list from the input set). These outputs are used to evaluate the model and tune the <kbd>weight</kbd>/<kbd>biases</kbd> during backpropagation. The following example shows the implementation:</p>
<div>
<pre>def output_values(inputs):<br/>    final_values = []<br/>    for value in inputs:<br/>        output_values = [0 for _ in range(num_classes)]<br/>        count = 0<br/>        for i in value:<br/>            count += i[0]<br/>        if count &lt; num_classes:<br/>            output_values[count] = 1<br/>        final_values.append(output_values)<br/>    return final_values</pre></div>
<p>The <kbd>inputs</kbd> parameter is a result of <kbd>input_values()</kbd> that we declared earlier. The <kbd>output_values()</kbd> function returns a list of one-hot encoded representations of each member in <kbd>inputs</kbd>. If the sum of all of the elements in the <kbd>[[0], [1], [1], [1], [0], ..., [0], [1]]</kbd> sequence is <kbd>48</kbd>, then its corresponding value inside <kbd>output_values</kbd> is <kbd>[0, 0, 0, ..., 1, 0, 0]</kbd> where <kbd>1</kbd> is at position <kbd>48</kbd>. </p>
<p>Finally, we use the <kbd>generate_data()</kbd> function to obtain the final values for the network's input and output, as shown in the following example:</p>
<div>
<pre>def generate_data():<br/>    inputs<span> </span><span>=</span><span> input_values()<br/></span>    return<span> inputs, output_values(inputs)</span></pre></div>
<p>We use the previous function to create these two new variables: <kbd>input_values</kbd>, and <kbd>output_values<span> </span><span>= generate_data()</span></kbd>. <span>One thing to pay attention to is the dimensions of these lists:</span></p>
<ul>
<li><kbd>input_values</kbd> is of a size of <kbd>[num_examples, num_classes, 1]</kbd> </li>
<li><kbd>output_values</kbd> is of a size of <kbd>[num_examples, num_classes]</kbd> </li>
</ul>
<p>Where <kbd>num_examples = 100000</kbd> <span>and</span> <kbd>num_classes = 50</kbd> <span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the TensorFlow graph</h1>
                </header>
            
            <article>
                
<p>Constructing the TensorFlow graph is probably the most complex part of building a neural network. We will precisely examine all of the steps so you can obtain a full understanding. </p>
<p>The TensorFlow graph can be viewed as a direct implementation of the recurrent neural network model, including all equations and algorithms introduced in <a href="d6266376-9b8b-4d69-925b-a4e56307951b.xhtml" target="_blank">Chapter 1</a>, <em>Introducing Recurrent Neural Networks</em>.</p>
<p>First, we start with setting the parameters of the model, as shown in the following example:</p>
<div>
<pre>X = tf.placeholder(tf.float32, shape=[None, num_classes, 1])<br/>Y = tf.placeholder(tf.float32, shape=[None, num_classes])<br/>num_hidden_units = 24<br/>weights = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes]))<br/>biases = tf.Variable(tf.truncated_normal([num_classes]))</pre></div>
<p><kbd>X</kbd> and <kbd>Y</kbd> are declared as <kbd>tf.placeholder</kbd>, which i<span>nserts a placeholder (inside the graph) for a tensor that will be always fed. Placeholders are used for variables that expect data when training the network. They often hold values for the training input and expected output of the network. You might be surprised why one of the dimensions is</span> <kbd>None</kbd><span>. The reason is that we have trained the network using batches. These are collections of several elements from our training data stacked together. When specifying the dimension as None, we let the tensor decide this dimension, calculating it using the other two values.</span></p>
<div class="packt_infobox">According to the TensorFlow documentation: <span>A </span>tensor<span> is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.<br/>
<br/></span> When performing training using batches, we split the training data into several smaller arrays of size—<kbd>batch_size</kbd>. Then, instead of training the network with all examples at once, we use one batch at a time.<br/>
<br/>
The advantages of this are less memory is required and faster learning is achieved.</div>
<p><span><span>The <kbd>weight</kbd> </span></span>and <kbd>biases</kbd><span> </span>are declared as <kbd>tf.Variable</kbd><span>, </span>which holds a certain value during training. This value can be modified. When a variable is first introduced, one should specify an initial value, type, and shape. The type and shape remain constant and cannot be changed.</p>
<p>Next, let's build the RNN cell. If you recall from <a href="d6266376-9b8b-4d69-925b-a4e56307951b.xhtml" target="_blank">Chapter 1</a>, <em>Introducing Recurrent Neural Networks</em>, an input at time step, <em>t,</em> is plugged into an RNN cell to produce an output, <img class="fm-editor-equation" src="assets/f8bef6b8-45e0-40b0-bec8-5c67c9bffec7.png" style="width:1.33em;height:1.33em;"/>, and a hidden state, <img class="fm-editor-equation" src="assets/81fbe1cb-da8c-49f8-885c-0de64e94c48a.png" style="width:1.08em;height:1.17em;"/>. Then, the hidden state and the new input at time step (<kbd><em>t</em>+1</kbd>) are plugged into a new RNN cell (which shares the same weights and biases as the previous). It produces its own output,  <img class="fm-editor-equation" src="assets/ad4e2cd0-a77e-469c-8581-ff28b0f6d3c0.png" style="width:1.75em;height:0.83em;"/>, and hidden state, <img class="fm-editor-equation" src="assets/76cb085f-239b-41b8-bd98-786096161a07.png" style="width:2.17em;height:1.25em;"/>. This pattern is repeated for every time step. </p>
<p>With TensorFlow, the previous operation is just a single line:</p>
<p class="mce-root"><kbd><span>rnn_cell</span> <span>=</span> <span>tf.contrib.rnn.BasicRNNCell(num_units</span><span>=</span><span>num_hidden_units</span><span>)</span></kbd></p>
<p>As you already know, each cell requires an activation function that is applied to the hidden state. By default, TensorFlow chooses <strong>tanh</strong> (perfect for our use case) but you can specify any that you wish. Just add an additional parameter called <kbd>activation</kbd> .</p>
<div>
<p>Both in <kbd>weights</kbd> and in <kbd>rnn_cell</kbd>, you can see a parameter called <span> </span><kbd>num_hidden_units</kbd><span> .</span>  As stated here (<a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell">https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell</a>), the <kbd>num_hidden_units</kbd> <span>is a direct representation of the learning capacity of a neural network. It determines the dimensionality of both the memory state, <img class="fm-editor-equation" src="assets/eaa3fc86-2047-4889-a480-911a7fced27d.png" style="width:1.08em;height:1.25em;"/>, and the output, <img class="fm-editor-equation" src="assets/28afa699-8307-4672-bdb3-518a6fa25397.png" style="width:1.00em;height:1.00em;"/> .</span></p>
<p>The next step is to produce the output of the network. This can also be implemented with a single line:</p>
<div>
<div><kbd><span>outputs, state</span> <span>=</span> <span>tf.nn.dynamic_rnn(rnn_cell, inputs</span><span>=</span><span>X, dtype</span><span>=</span><span>tf.float32)</span></kbd></div>
</div>
<p>Since <kbd>X</kbd> is a batch of input sequences, then <kbd>outputs</kbd> represents a batch of outputs at every time step in all sequences. To evaluate the prediction, we need the value of the last time step for every output in the batch. This happens in three steps, explained in the following bulleted examples:</p>
<ul>
<li>We get the values from the last time step: <kbd>outputs = tf.transpose(outputs, [1, 0, 2])</kbd></li>
</ul>
</div>
<p>This would reshape the output's tensor from (<kbd>1000, 50, 24</kbd>) to (<kbd>50, 1,000, 24</kbd>) so that the outputs from the last time step in every sequence are accessible to be gathered using the following: <kbd><span>last_output</span> <span>=</span> <span>tf.gather(outputs, int(outputs.get_shape()[</span><span>0</span><span>])</span> <span>-</span> <span>1</span><span>)</span></kbd>.</p>
<p class="mce-root">Let's review the following diagram to understand how this <kbd>last_output</kbd> is obtained:</p>
<p>The previous diagram shows how one input example of <kbd>50</kbd> steps is plugged into the network. This operation should be done <kbd>1,000</kbd> times for each individual example having <kbd>50</kbd> steps but, for the sake of simplicity, we are showing only one example. </p>
<p>After iteratively going through each time step, we produce <kbd>50</kbd> outputs, each one having the dimensions (<kbd>24, 1</kbd>). So, for one example of <kbd>50</kbd> input time steps, we produce <kbd>50</kbd> output steps. Presenting all of the outputs mathematically results in a (<kbd>1,000, 50, 24</kbd>) matrix. The height of the matrix is <kbd>1,000</kbd>—the number of individual examples. The width of the matrix is <kbd>50</kbd>—the number of time steps for each example. The depth of the matrix is <kbd>24</kbd>—the dimension of each element. </p>
<p>To make a prediction, we only care about <kbd>output_last</kbd> at each example, and since the number of examples is <kbd>1,000</kbd>, we only need 1,000 output values. As seen in the previous example, we transpose the matrix (<kbd>1000, 50, 24</kbd>) into (<kbd>50, 1000, 24</kbd>), which will make it easier to get <kbd>output_last</kbd> from each example. Then, we use <kbd>tf.gather</kbd> to obtain the <kbd>last_output</kbd> tensor which has size of (<kbd>1000, 24, 1</kbd>).</p>
<p>Final lines of building our graph include:</p>
<ul>
<li><span>We predict the output of the particular sequence: </span></li>
</ul>
<div>
<pre>     prediction = tf.matmul(last_output, weights) + biases</pre>
<p>Using the newly obtained tensor, <kbd>last_output</kbd> , we can calculate a prediction using the weights and biases. </p>
<ul>
<li>We evaluate the output based on the expected value:</li>
</ul>
<pre>    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,    <br/>    logits=prediction)<br/>    total_loss = tf.reduce_mean(loss)</pre>
<p>We can use the popular cross entropy loss function in a combination with <kbd>softmax</kbd>. If you recall from <a href="d6266376-9b8b-4d69-925b-a4e56307951b.xhtml" target="_blank">Chapter 1</a>, <em>Introducing Recurrent Neural Networks</em>, the <kbd>softmax</kbd> function transforms a tensor to <span>emphasize the largest values and suppress values that are significantly below the maximum value. This is done by normalizing the values from the initial array to ones that add up to <kbd>1</kbd>. For example, the input <kbd>[0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3]</kbd> becomes [<kbd>0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153</kbd>]. The cross entropy is a loss function that computes the difference between the <kbd>label</kbd> (expected values) and <kbd>logits</kbd> (predicted values). </span></p>
<p>Since <kbd>tf.nn.softmax_cross_entropy_with_logits_v2</kbd> returns a<span> 1-D tensor </span><span>of a length of <kbd>batch_size</kbd> (declared below), we use <kbd>tf.reduce_mean</kbd> to compute the mean of all elements in that tensor. </span></p>
<p>As a final step, we will see how TensorFlow makes it easy for us to optimize the weights and biases. Once we have obtained the loss function, we need to perform a backpropagation algorithm, adjusting the weights and biases to minimize the loss. This can be done in the following way:</p>
<div>
<pre>learning_rate = 0.001<br/>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=total_loss)</pre></div>
</div>
<p class="mce-root"><kbd>learning_rate</kbd> is one of the model's hyperparameters and is used when optimizing the loss function. Tuning this value is essential for better performance, so feel free to adjust it and evaluate the results. </p>
<p class="mce-root">Minimizing the error of the loss function is done using an Adam optimizer. Here (<a href="https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow">https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow</a>) is a good explanation of why it is preferred over the Gradient <span>descent.</span></p>
<p>We have just built the architecture of our recurrent neural network. Let's put everything together, as shown in the following example:</p>
<div>
<pre>X = tf.placeholder(tf.float32, shape=[None, num_classes, 1])<br/>Y = tf.placeholder(tf.float32, shape=[None, num_classes])<br/><br/>num_hidden_units = 24<br/><br/>weights = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes]))<br/>biases = tf.Variable(tf.truncated_normal([num_classes]))<br/><br/>rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=num_hidden_units, activation=tf.nn.relu)<br/>outputs1, state = tf.nn.dynamic_rnn(rnn_cell, inputs=X, dtype=tf.float32)<br/>outputs = tf.transpose(outputs1, [1, 0, 2])<br/><br/>last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)<br/>prediction = tf.matmul(last_output, weights) + biases<br/><br/>loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=prediction)<br/>total_loss = tf.reduce_mean(loss)<br/><br/>learning_rate = 0.001<br/>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=total_loss)</pre></div>
<p>The next task is to train the neural network <span>using the TensorFlow graph in combination with the previously generated data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the RNN</h1>
                </header>
            
            <article>
                
<p>In this section, we will go through the second part of a TensorFlow program—executing the graph with a predefined data. For this to happen, we will use the <kbd>Session</kbd> object, which encapsulates an environment in which the tensor objects are executed.</p>
<p>The code for our training is shown in the following example:</p>
<pre>batch_size = 1000<br/>number_of_batches = int(num_examples/batch_size)<br/>epoch = 100<br/>with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer()) <br/>    X_train, y_train = generate_data()<br/>    for epoch in range(epoch):<br/>        iter = 0<br/>        for _ in range(number_of_batches):<br/>            training_x = X_train[iter:iter+batch_size]<br/>            training_y = y_train[iter:iter+batch_size]<br/>            iter += batch_size<br/>            _, current_total_loss = sess.run([optimizer, total_loss], <br/>            feed_dict={X: training_x, Y: training_y})<br/>            print("Epoch:", epoch, "Iteration:", iter, "Loss", current_total_loss)<br/>            print("__________________")</pre>
<div>
<p>First, we initialize the batch size. At each training step, the network is tuned, based on examples from the chosen batch. Then, we compute the number of batches as well as the number of epochs—this determines how many times our model should loop through the training set. <kbd>tf.Session()</kbd> encapsulates the code in a TensorFlow <kbd>Session</kbd> and <kbd>sess.run(tf.global_variables_initializer())</kbd> (<a href="https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer">https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer</a>) makes sure all variables hold their values.</p>
<p>Then, we store an individual batch from the training set in <kbd>training_x</kbd><span> and </span><kbd>training_y</kbd><span> .</span></p>
<p>The last, and most, important part of training the network comes with the usage of  <kbd>sess.run()</kbd> . By calling this function, you can compute the value of any tensor. In addition, one can specify as many arguments as you want by ordering them in a list—in our case, we have specified the optimizer and loss function. Remember how, while building the graph, we created placeholders for holding the values of the current batch? These values should be mentioned in the <kbd>feed_dict</kbd> parameter when running <kbd>Session</kbd>.</p>
<p>Training this network can take around four or five hours. You can verify that it is learning by examining the value of the loss function. If its value decreases, then the network is successfully modifying the weights and biases. If the value is not decreasing, you most likely need to make some additional changes to optimize the performance. These will be explained in <a href="" target="_blank">Chapter 6</a>, <em>Improving Your RNN Performance</em>.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the predictions</h1>
                </header>
            
            <article>
                
<p>Testing the model using a fresh new example can be accomplished in the following way:</p>
<div>
<pre>prediction_result = sess.run(prediction, {X: test_example})<br/><span>largest_number_index </span><span class="pl-k">=</span><span> prediction_result[</span><span class="pl-c1">0</span><span>].argsort()[</span><span class="pl-k">-</span><span class="pl-c1">1</span><span>:][::</span><span class="pl-k">-</span><span class="pl-c1">1</span><span>]</span><br/><br/><span class="pl-c1">print</span><span>(</span><span class="pl-s"><span class="pl-pds">"</span>Predicted sum: <span class="pl-pds">"</span></span><span>, largest_number_index, </span><span class="pl-s"><span class="pl-pds">"</span>Actual sum:<span class="pl-pds">"</span></span><span>, </span><span class="pl-c1">30</span><span>)<br/><span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">"</span>The predicted sequence parity is <span class="pl-pds">"</span></span>, largest_number_index <span class="pl-k">%</span> <span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">"</span> and it should be: <span class="pl-pds">"</span></span>, <span class="pl-c1">0</span>)<br/></span></pre></div>
<p>This is where <kbd>test_example</kbd> is an array of a size of  (<kbd>1 x num_classes x 1</kbd>).</p>
<p>Let <kbd>test_example</kbd> be as follows:</p>
<div>
<pre><span>[[[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],<br/></span><span>[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],<br/></span><span>[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>1</span><span>],[</span><span>0</span><span>]]]</span></pre>
<p>The sum of all elements in the above array is equal to <kbd>30</kbd>. <span>With the last line, </span><kbd><span>prediction_result[</span><span>0</span><span>].argsort()[</span><span>-1</span><span>:]</span><span>[::</span><span>-</span><span>1</span></kbd><span><kbd>]</kbd>, </span><span>we can find the index of the largest number. The index would tell us the sum of the sequence. As a last step, we need to find the remainder </span><span>when this number is divided by</span> <kbd>2</kbd><span>. This will give us the parity of the sequence. </span></p>
</div>
<p>Both training and evaluation are done together after you run <kbd>python3 ch2_task.py</kbd>. If you want to only do evaluation, comment out the lines between <kbd>70</kbd> and <kbd>91</kbd> from the program and run it again. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, you explored how to build a simple recurrent neural network to solve the problem of identifying sequence parity. You obtained a brief understanding of the TensorFlow library and how it can be utilized for building deep learning models. I hope the study of this chapter leaves you more confident in your deep learning knowledge, as well as excited to learn and grow more in this field. </p>
<p>In the next chapter, you will go a step further by implementing a more sophisticated neural network for the task of generating text. You will gain both theoretical and practical experience. This will result in you learning about a new type of network, GRU, and understanding how to implement it in TensorFlow. In addition, you will face the challenge of formatting your input text correctly as well as using it for training the TensorFlow graph. </p>
<p>I can assure you that an exciting learning experience is coming, so I cannot wait for you to be part of it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">External links</h1>
                </header>
            
            <article>
                
<ul>
<li>Parity: <a href="http://mathworld.wolfram.com/Parity.html">http://mathworld.wolfram.com/Parity.html</a></li>
<li>Request for Research 2.0 by OpenAI: <a href="https://blog.openai.com/requests-for-research-2/">https://blog.openai.com/requests-for-research-2/ </a></li>
<li>Eager execution documentation:<a href="https://www.tensorflow.org/guide/eager"> https://www.tensorflow.org/guide/eager</a></li>
<li>Eager execution (TensorFlow Conference 2018): <a href="https://www.youtube.com/watch?v=T8AW0fKP0Hs">https://www.youtube.com/watch?v=T8AW0fKP0Hs</a></li>
<li>Python installation: <a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a></li>
<li>Understanding <kbd>num_hidden_units</kbd> : <a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell">https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell</a></li>
<li>Adam versus Gradient descent optimizer: <a href="https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow">https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow</a></li>
<li>Understanding <kbd>sess.run(tf.global_variables_initializer()</kbd>: <a href="https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer">https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer</a></li>
</ul>


            </article>

            
        </section>
    </body></html>