["```py\ndef calculate_n_step_return(self, n_step_rewards, final_state, done, gamma):\n        \"\"\"\n        Calculates the n-step return for each state in the input-trajectory/n_step_transitions\n        :param n_step_rewards: List of rewards for each step\n        :param final_state: Final state in this n_step_transition/trajectory\n        :param done: True rf the final state is a terminal state if not, False\n        :return: The n-step return for each state in the n_step_transitions\n        \"\"\"\n        g_t_n_s = list()\n        with torch.no_grad():\n            g_t_n = torch.tensor([[0]]).float() if done else self.critic(self.preproc_obs(final_state)).cpu()\n            for r_t in n_step_rewards[::-1]: # Reverse order; From r_tpn to r_t\n                g_t_n = torch.tensor(r_t).float() + self.gamma * g_t_n\n                g_t_n_s.insert(0, g_t_n) # n-step returns inserted to the left to maintain correct index order\n            return g_t_n_s\n```", "```py\nclass DeepActorCriticAgent(mp.Process):\n    def __init__(self, id, env_name, agent_params):\n        \"\"\"\n        An Advantage Actor-Critic Agent that uses a Deep Neural Network to represent it's Policy and the Value function\n        :param id: An integer ID to identify the agent in case there are multiple agent instances\n        :param env_name: Name/ID of the environment\n        :param agent_params: Parameters to be used by the agent\n        \"\"\"\n        super(DeepActorCriticAgent, self).__init__()\n        self.id = id\n        self.actor_name = \"actor\" + str(self.id)\n        self.env_name = env_name\n        self.params = agent_params\n        self.policy = self.multi_variate_gaussian_policy\n        self.gamma = self.params['gamma']\n        self.trajectory = [] # Contains the trajectory of the agent as a sequence of Transitions\n        self.rewards = [] # Contains the rewards obtained from the env at every step\n        self.global_step_num = 0\n        self.best_mean_reward = - float(\"inf\") # Agent's personal best mean episode reward\n        self.best_reward = - float(\"inf\")\n        self.saved_params = False # Whether or not the params have been saved along with the model to model_dir\n        self.continuous_action_space = True #Assumption by default unless env.action_space is Discrete\n```", "```py\nfrom function_approximator.shallow import Actor as ShallowActor\nfrom function_approximator.shallow import DiscreteActor as ShallowDiscreteActor\nfrom function_approximator.shallow import Critic as ShallowCritic\nfrom function_approximator.deep import Actor as DeepActor\nfrom function_approximator.deep import DiscreteActor as DeepDiscreteActor\nfrom function_approximator.deep import Critic as DeepCritic\n\ndef run(self):\n        self.env = gym.make(self.env_name)\n        self.state_shape = self.env.observation_space.shape\n        if isinstance(self.env.action_space.sample(), int): # Discrete action space\n            self.action_shape = self.env.action_space.n\n            self.policy = self.discrete_policy\n            self.continuous_action_space = False\n\n        else: # Continuous action space\n            self.action_shape = self.env.action_space.shape[0]\n            self.policy = self.multi_variate_gaussian_policy\n        self.critic_shape = 1\n        if len(self.state_shape) == 3: # Screen image is the input to the agent\n            if self.continuous_action_space:\n                self.actor= DeepActor(self.state_shape, self.action_shape, device).to(device)\n            else: # Discrete action space\n                self.actor = DeepDiscreteActor(self.state_shape, self.action_shape, device).to(device)\n            self.critic = DeepCritic(self.state_shape, self.critic_shape, device).to(device)\n        else: # Input is a (single dimensional) vector\n            if self.continuous_action_space:\n                #self.actor_critic = ShallowActorCritic(self.state_shape, self.action_shape, 1, self.params).to(device)\n                self.actor = ShallowActor(self.state_shape, self.action_shape, device).to(device)\n            else: # Discrete action space\n                self.actor = ShallowDiscreteActor(self.state_shape, self.action_shape, device).to(device)\n            self.critic = ShallowCritic(self.state_shape, self.critic_shape, device).to(device)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.params[\"learning_rate\"])\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.params[\"learning_rate\"])\n```", "```py\ndef calculate_loss(self, trajectory, td_targets):\n        \"\"\"\n        Calculates the critic and actor losses using the td_targets and self.trajectory\n        :param td_targets:\n        :return:\n        \"\"\"\n        n_step_trajectory = Transition(*zip(*trajectory))\n        v_s_batch = n_step_trajectory.value_s\n        log_prob_a_batch = n_step_trajectory.log_prob_a\n        actor_losses, critic_losses = [], []\n        for td_target, critic_prediction, log_p_a in zip(td_targets, v_s_batch, log_prob_a_batch):\n            td_err = td_target - critic_prediction\n            actor_losses.append(- log_p_a * td_err) # td_err is an unbiased estimated of Advantage\n            critic_losses.append(F.smooth_l1_loss(critic_prediction, td_target))\n            #critic_loss.append(F.mse_loss(critic_pred, td_target))\n        if self.params[\"use_entropy_bonus\"]:\n            actor_loss = torch.stack(actor_losses).mean() - self.action_distribution.entropy().mean()\n        else:\n            actor_loss = torch.stack(actor_losses).mean()\n        critic_loss = torch.stack(critic_losses).mean()\n\n        writer.add_scalar(self.actor_name + \"/critic_loss\", critic_loss, self.global_step_num)\n        writer.add_scalar(self.actor_name + \"/actor_loss\", actor_loss, self.global_step_num)\n\n        return actor_loss, critic_loss\n```", "```py\ndef learn(self, n_th_observation, done):\n        td_targets = self.calculate_n_step_return(self.rewards, n_th_observation, done, self.gamma)\n        actor_loss, critic_loss = self.calculate_loss(self.trajectory, td_targets)\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward(retain_graph=True)\n        self.actor_optimizer.step()\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        self.trajectory.clear()\n        self.rewards.clear()\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$ python a2c_agent --env Pendulum-v0\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8/logs$ tensorboard --logdir .\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$ python a2c_agent --env CartPole-v0 --render\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$ python a2c_agent --env LunarLander-v2 --test --render\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$ python async_a2c_agent --env BipedalWalker-v2 \n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$ python a2c_agent --env Carla-v0\n```"]