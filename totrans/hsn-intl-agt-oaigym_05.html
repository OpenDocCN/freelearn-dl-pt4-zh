<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing your First Learning Agent - Solving the Mountain Car problem</h1>
                
            
            <article>
                
<p class="calibre2">Well done on making it this far! In previous chapters, we got a good introduction to OpenAI Gym, its features, and how to install, configure, and use it in your own programs. We also discussed the basics of reinforcement learning and what deep reinforcement learning is, and we set up the PyTorch deep learning library to develop deep reinforcement learning applications. In this chapter, you will start developing your first learning agent! You will develop an intelligent agent that will learn how to solve the Mountain Car problem. Gradually in the following chapters, we will solve increasingly challenging problems as you get more comfortable developing reinforcement learning algorithms to solve problems in OpenAI Gym. We will start this chapter by understanding the Mountain Car problem, which has been a popular problem in the reinforcement learning and optimal control community. We will develop our learning agent from scratch and then train it to solve the Mountain Car problem using the Mountain Car environment in the Gym. We will finally see how the agent progresses and briefly look at ways we can improve the agent to use it for solving more complex problems. The topics we will be covering in this chapter are as follows:</p>
<ul class="calibre10">
<li class="calibre11"><span>Understanding the Mountain Car problem</span></li>
<li class="calibre11">Implementing a reinforcement learning-based agent to solve the Mountain Car problem</li>
<li class="calibre11">Training a reinforcement learning agent at the Gym</li>
<li class="calibre11">Testing the performance of the agent</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Understanding the Mountain Car problem</h1>
                
            
            <article>
                
<p class="calibre2">For any reinforcement learning problem, two fundamental definitions concerning the problem are important, irrespective of the learning algorithm we use. They are the definitions of the state space and the action space. We mentioned earlier in this book that the state and action spaces could be discrete or continuous. Typically, in most problems, the state space <span class="calibre5">consists of continuous values </span>and is represented as a vector, matrix, or tensor (a multi-dimensional matrix). Problems and environments with discrete action spaces are relatively easy compared to continuous valued problems and environments. In this book, we will develop learning algorithms for a few problems and environments with a mix of state space and action space combinations so that you are comfortable dealing with any such variation when you start out on your own and develop intelligent agents and algorithms for your applications.</p>
<p class="calibre2">Let's start by understanding the Mountain Car problem with a high-level description, before moving on to look at the state and action spaces of the Mountain Car environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The Mountain Car problem and environment</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">In the Mountain Car Gym environment, a car is on a one-dimensional track, positioned between two mountains. The goal is to drive the car up the mountain on the right; however, the car's engine is not strong enough to drive up the mountain even at the maximum speed. Therefore, the only way to succeed is to drive back and forth to build up momentum. </span><span class="calibre5">In short, the Mountain Car problem is to get an under-powered car to the top of a hill.</span></p>
<p class="calibre2"><span class="calibre5">Before you implement your agent algorithm, it will help tremendously to understand the environment, the problem, and the state and action spaces. </span>How do we find out the state and action spaces of the Mountain Car environment in the Gym? Well, we already know how to do that from <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 4</a>, <em class="calibre13">Exploring the Gym and its Features</em>. We wrote a script named <kbd class="calibre12">get_observation_action_space.py</kbd>, which will print out the state and observation and action spaces of the environment whose name is passed as the first argument to the script. Let's ask it to print the spaces for the <kbd class="calibre12">MountainCar-v0</kbd> environment with the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch4$ python get_observation_action_space.py 'MountainCar-v0'</strong></pre>
<div class="packt_tip"><span class="packt_screen">Note that the command prompt has the</span> <kbd class="calibre28">rl_gym_book</kbd> prefix, which signifies that we have activated the <kbd class="calibre28">rl_gym_book</kbd> conda Python virtual environment. Also, the current directory, <kbd class="calibre28">~/rl_gym_book/ch4</kbd>, signifies that the script is run from the <kbd class="calibre28">ch4</kbd> directory corresponding to the code for <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre45">Chapter 4</a>, <em class="calibre46">Exploring the Gym and its Features</em>, in the code repository for this book.</div>
<div class="title-page-name">The preceding command will produce output like the following:</div>
<pre class="calibre17">Observation Space:<br class="title-page-name"/>Box(2,)<br class="title-page-name"/><br class="title-page-name"/> space.low: [-1.20000005 -0.07 ]<br class="title-page-name"/><br class="title-page-name"/> space.high: [ 0.60000002 0.07 ]<br class="title-page-name"/>Action Space:<br class="title-page-name"/>Discrete(3)</pre>
<p class="calibre2">From this output, we can see that the state and observation space is a two-dimensional box and the action space is three-dimensional and discrete.</p>
<div class="packt_tip">If you want a refresher on what <strong class="calibre27">box </strong>and <strong class="calibre27">discrete </strong>spaces mean, you can quickly flip to <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre45">Chapter 4</a>, <em class="calibre46">Exploring the Gym and its Features</em>, where we discussed these spaces and what they mean under the <em class="calibre46">Spaces in the Gym</em><strong class="calibre27"> </strong>section. It is important to understand them.</div>
<p class="calibre2">The state and action space type, description, and range of allowed values are summarized in the following table for your reference:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">MountainCar-v0 environment</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Type</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Description</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Range</strong></p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">State space</p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">Box(2,)</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2">(position, velocity)</p>
</td>
<td class="calibre48">
<p class="calibre2">Position: -1.2 to 0.6</p>
<p class="calibre2">Velocity: -0.07 to 0.07</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">Action space</p>
</td>
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12">Discrete(3)</kbd></p>
</td>
<td class="calibre48">
<p class="calibre2">0: Go left</p>
<p class="calibre2">1: Coast/do-nothing</p>
<p class="calibre2">2: Go right</p>
</td>
<td class="calibre48">
<p class="calibre2">0, 1, 2</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">So for example, the car starts at a random position between <em class="calibre13">-0.6</em> and <em class="calibre13">-0.4</em> with zero velocity, and the goal is to reach the top of the hill on the right side, which is at position <em class="calibre13">0.5</em>. (The car can technically go beyond <em class="calibre13">0.5,</em> up to <em class="calibre13">0.6</em>, which is also considered.) The environment will send <em class="calibre13">-1</em> as a reward every time step until the goal position (<em class="calibre13">0.5</em>) is reached. The environment will terminate the episode. The <kbd class="calibre12">done</kbd> variable will be equal to <kbd class="calibre12">True</kbd> if the car reaches the <em class="calibre13">0.5</em> <span class="calibre5">position </span><span class="calibre5">or the number of steps taken reaches 200.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing a Q-learning agent from scratch</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will start implementing our intelligent agent step-by-step. We will be implementing the famous Q-learning algorithm using the <kbd class="calibre12">NumPy</kbd> library and the <kbd class="calibre12">MountainCar-V0</kbd> environment from the OpenAI Gym library.</p>
<p class="calibre2">Let's revisit the reinforcement learning Gym boiler plate code we used in <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 4</a>, <em class="calibre13">Exploring the Gym and its Features</em>, as follows:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>env = gym.make("Qbert-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 10<br class="title-page-name"/>MAX_STEPS_PER_EPISODE = 500<br class="title-page-name"/>for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    for step in range(MAX_STEPS_PER_EPISODE):<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms<br class="title-page-name"/>        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not<br class="title-page-name"/>        obs = next_state<br class="title-page-name"/><br class="title-page-name"/>        if done is True:<br class="title-page-name"/>            print("\n Episode #{} ended in {} steps.".format(episode, step+1))<br class="title-page-name"/>            break</pre>
<p class="calibre2">This code is a good starting point (aka boilerplate!) for developing our reinforcement learning agent. We will first start by changing the environment name from <kbd class="calibre12">Qbert-v0</kbd> to <kbd class="calibre12">MountainCar-v0</kbd>. Notice in the preceding script that we are setting <kbd class="calibre12">MAX_STEPS_PER_EPISODE</kbd>. This is the number of steps or actions that the agent can take before the episode ends. This may be useful in continuing, perpetual, or looping environments, where the environment itself does not end the episode. Here, we set a limit for the agent to avoid infinite loops. However, most of the environments defined in OpenAI Gym have an episode termination condition and once either of them is satisfied, the <kbd class="calibre12">done</kbd> variable returned by the <kbd class="calibre12">env.step(...)</kbd> function will be set to <em class="calibre13">True</em>. We saw in the previous section that for the Mountain Car problem we are interested in, the environment will terminate the episode if the car reaches the goal position (<em class="calibre13">0.5</em>) or if the number of steps taken reaches <em class="calibre13">200</em>. Therefore, we can further simplify the boilerplate code to look like the following for the Mountain Car environment:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>env = gym.make("MountainCar-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 5000<br class="title-page-name"/><br class="title-page-name"/>for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0 # To keep track of the total reward obtained in each episode<br class="title-page-name"/>    step = 0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms<br class="title-page-name"/>        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>        step += 1<br class="title-page-name"/>        obs = next_state<br class="title-page-name"/><br class="title-page-name"/>    print("\n Episode #{} ended in {} steps. total_reward={}".format(episode, step+1, total_reward))<br class="title-page-name"/>env.close()</pre>
<p class="calibre2">If you run the preceding script, you will see the Mountain Car environment come up in a new window and the car moving left and right randomly for 1,000 episodes. You will also see the episode number, steps taken, and the total reward obtained printed at the end of every episode, as shown in the following screenshot:</p>
<p class="cdpaligncenter4"><img src="../images/00119.jpeg" class="calibre62"/></p>
<p class="calibre2">The sample output should look similar to the following screenshot:</p>
<div class="cdpaligncenter"><img src="../images/00120.jpeg" class="calibre63"/><br class="title-page-name"/></div>
<p class="calibre2">You should recall from our previous section that the agent gets a reward of <em class="calibre13">-1</em> for each step and that the <kbd class="calibre12">MountainCar-v0</kbd> environment will terminate the episode after <em class="calibre13">200</em> steps; this is why you the agent may sometimes get a total reward of <em class="calibre13">-200!</em> After all, the agent is taking random actions without thinking or learning from its previous actions. Ideally, we would want the agent to figure out how to reach the top of the mountain (near the flag, close to, at, or beyond position <em class="calibre13">0.5</em>) with the minimum number of steps. Don't worry - we will build such an intelligent agent by the end of this chapter!</p>
<div class="packt_tip">Remember to always activate the <kbd class="calibre28">rl_gym_book</kbd> conda environment before running the scripts! Otherwise, you might run into <span class="packt_screen">Module not found</span> errors unnecessarily. You can visually confirm whether you have activated the environment by looking at the shell prefix, which will show something like this: <kbd class="calibre28">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch5$</kbd>.</div>
<p class="calibre2">Let's move on by having a look at what Q-learning section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Revisiting Q-learning</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">In </span><a target="_blank" href="part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9"><span>Ch</span>apter 2</a>, <em class="calibre13">Reinforcement Learning and Deep Reinforcement Learning</em>, we <span class="calibre5">discussed the SARSA and Q-learning algorithms. Both of these algorithms provide a systematic way to update the estimate of the action-value function denoted by <img class="fm-editor-equation70" src="../images/00121.jpeg"/>. In particular, we saw that Q-learning is an off-policy learning algorithm, which updates the action-value estimate of the current state and action towards the maximum obtainable action-value in the subsequent state, <img class="fm-editor-equation58" src="../images/00122.jpeg"/>, which the agent will end up in according to its policy. We also saw that the Q-learning update is given by the following formula:</span></p>
<div class="cdpaligncenter"><img class="fm-editor-equation71" src="../images/00123.jpeg"/></div>
<p class="calibre2">In the next section, we will implement a <kbd class="calibre12">Q_Learner</kbd><strong class="calibre4"> </strong>class in Python, which implements this learning update rule along with the other necessary functions and methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing a Q-learning agent using Python and NumPy</h1>
                
            
            <article>
                
<p class="calibre2">Let's begin implementing our Q-learning agent by implementing the <kbd class="calibre12">Q_Learner</kbd> class. The main methods of this class are the following:</p>
<ul class="calibre10">
<li class="calibre11">__init__(self, env)</li>
<li class="calibre11">discretize(self, obs)</li>
<li class="calibre11">get_action(self, obs)</li>
<li class="calibre11">learn(self, obs, action, reward, next_obs)</li>
</ul>
<p class="calibre2">You will later find that the methods in here are common and exist in almost all the agents we will be implementing in this book. This makes it easy for you to grasp them, as these methods will be repeated (with some modifications) again and again.</p>
<p class="calibre2">The <kbd class="calibre12">discretize()</kbd> function is not necessary for agent implementations in general, but when the state space is large and continuous, it may be better to discretize the space into countable bins or ranges of values to simplify the representation. This also reduces the number of values that the Q-learning algorithm needs to learn, as it now only has to learn a finite set of values, which can be concisely represented in tabular formats or by using <em class="calibre13">n</em>-dimensional arrays instead of complex functions. Moreover, the Q-learning algorithm, used for optimal control, is guaranteed to converge for tabular representations of Q-values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Defining the hyperparameters</h1>
                
            
            <article>
                
<p class="calibre2">Before our <kbd class="calibre12">Q_Learner</kbd> class declaration, we will initialize a few useful hyperparameters. Here are the hyperparameters that we will be using for our <kbd class="calibre12">Q_Learner</kbd> implementation:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">EPSILON_MIN</kbd>: This is the minimum value of the epsilon value that we want the agent to use while following an epsilon-greedy policy.</li>
<li class="calibre11"><kbd class="calibre12">MAX_NUM_EPISODES</kbd>:<strong class="calibre1"> </strong>The maximum number of episodes that we want the agent to interact with the environment for.</li>
<li class="calibre11"><kbd class="calibre12">STEPS_PER_EPISODE</kbd>: This is the number of steps in each episode. This could be the maximum number of steps that an environment will allow per episode or a custom value that we want to limit based on some time budget. Allowing a higher number of steps per episode means each episode might take longer to complete and in non-terminating environments, the environment won't be reset until this limit is reached, even if the agent is stuck at the same spot.</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">ALPHA</kbd>: This is the learning rate that we want the agent to use. This is the alpha in the Q-learning update equation listed in the previous section. Some algorithms vary the learning rate as the training progresses.</li>
<li class="calibre11"><kbd class="calibre12">GAMMA</kbd>: This is the discount factor that the agent will use to factor in future rewards. This value corresponds to the gamma in the Q-learning update equation in the previous section.</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">NUM_DISCRETE_BINS</kbd>: This is the number of bins of values that the state space will be discretized into. For the Mountain Car environment, we will be discretizing the state space into <em class="calibre25">30</em> bins. You can play around with higher/lower values.</li>
</ul>
<div class="packt_infobox">Note that the <kbd class="calibre28">MAX_NUM_EPISODES</kbd><strong class="calibre27"> </strong>and <kbd class="calibre28">STEPS_PER_EPISODE</kbd> have been defined in the boilerplate code we went through in one of the previous sections of this chapter.</div>
<div class="title-page-name"><span>These hyperparameters are defined in the Python code like this, with some initial values:</span></div>
<pre class="calibre17">EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the Q_Learner class's __init__ method</h1>
                
            
            <article>
                
<p class="calibre2">Next, let us look into the <kbd class="calibre12">Q_Learner</kbd> class's member function definitions. The <kbd class="calibre12">__init__(self, env)</kbd> function takes the environment instance, <kbd class="calibre12">env</kbd>, as an input argument and initializes the dimensions/shape of the observation space and the action space, and also determines the parameters to discretize the observation space based on the <kbd class="calibre12">NUM_DISCRETE_BINS</kbd> we set. The <kbd class="calibre12">__init__(self, env)</kbd> function also initializes the Q function as a NumPy array, based on the shape of the discretized observation space and the action space dimensions. The implementation of <kbd class="calibre12">__init__(self, env)</kbd> is straightforward as we are only initializing the necessary values for the agent. Here is our implementation:</p>
<pre class="calibre17">class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                          self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0</pre>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the Q_Learner class's discretize method</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's take a moment to understand how we are discretizing the observation space. The simplest, and yet an effective, way to discretize the observation space (and a metric space in general) is to divide the span of the range of values into a finite set of values called bins. The span/range of values is given by the difference between the maximum possible value and the minimum possible value in each dimension of the space. Once we calculate the span, we can divide it by the </span><kbd class="calibre12">NUM_DISCRETE_BINS</kbd> that we have decided on to get the width of the bin. We calculated the bin width in the <kbd class="calibre12">__init__</kbd> function because it does not change with every new observation. The <kbd class="calibre12">discretize(self, obs)</kbd> function receives every new function and applies the discretization step to find the bin that the observation belongs to in the discretized space. It is as simple as doing this:</p>
<pre class="calibre17">(obs - self.obs_low) / self.bin_width)</pre>
<p class="calibre2">We want it to belong to any <em class="calibre13">one</em> of the bins (and not somewhere in between); therefore, we convert the previous code into an <kbd class="calibre12">integer</kbd>:</p>
<pre class="calibre17">((obs - self.obs_low) / self.bin_width).astype(int)</pre>
<p class="calibre2">Finally, we return this discretized observation as a tuple. All of this operation can be written in one line of Python code, like this:</p>
<pre class="calibre17">def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the Q_Learner's get_action method</h1>
                
            
            <article>
                
<p class="calibre2">We want the agent to taken an action given an observation. <kbd class="calibre12">get_action(self, obs)</kbd><strong class="calibre4"> </strong>is the function we define to generate an action, given an observation in <kbd class="calibre12">obs</kbd>.<strong class="calibre4"> </strong>The most widely used action selection policy is the epsilon-greedy policy, which takes the best action as per the agent's estimate with a (high) probability of <em class="calibre13">1-</em><img src="../images/00124.jpeg" class="calibre64"/>, and takes a random action with a (small) probability given by epsilon <img class="fm-editor-equation59" src="../images/00125.jpeg"/>. We implement the epsilon-greedy policy using the <kbd class="calibre12">random()</kbd> method from NumPy's random module, like this:</p>
<pre class="calibre17"> def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the Q_learner class's learn method</h1>
                
            
            <article>
                
<p class="calibre2">As you might have guessed, this is the most important method of the <kbd class="calibre12">Q_Learner</kbd> class, which does the magic of learning the Q-values, which in turn enables the agent to take intelligent actions over time! The best part is that it is not that complicated to implement! It is merely the implementation of the Q-learning update equation that we saw earlier. Don't believe me when I say it is simple to implement?! Alright, here is the implementation of the learning function:</p>
<pre class="calibre17"> def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error</pre>
<p class="calibre2"><span class="calibre5">Now do you agree? :) </span></p>
<p class="calibre2">We could have written the Q learning update rule in one line of code, like this: </p>
<pre class="calibre17">self.Q[discretized_obs][action] += self.alpha * (reward + self.gamma * np.max(self.Q[discretized_next_obs] - self.Q[discretized_obs][action]</pre>
<p class="calibre2">But, calculating each term on a separate line will make it easier to read and understand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Full Q_Learner class implementation</h1>
                
            
            <article>
                
<p class="calibre2">If we put all the method implementations together, we will get a code snippet that looks like this:</p>
<pre class="calibre17">EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                           self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error</pre>
<p class="calibre2">So, we have the agent ready. What should we do next, you may ask. Well, we should train the agent in the Gym environment! In the next section, we will look at the training procedure. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training the reinforcement learning agent at the Gym</h1>
                
            
            <article>
                
<p class="calibre2">The procedure to train the Q-learning agent may look familiar to you already, because it has many of the same lines of code as, and also a similar <span class="calibre5">structure</span> to, the boilerplate code that we used before. Instead of choosing a random action from the environment's actions space, we now get the action from the agent using the <kbd class="calibre12">agent.get_action(obs)</kbd> method. We also call the <kbd class="calibre12">agent.learn(obs, action, reward, next_obs)</kbd> method after sending the agent's action to the environment and receiving the feedback. The training function is listed here:</p>
<pre class="calibre17">def train(agent, env):<br class="title-page-name"/>    best_reward = -float('inf')<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        done = False<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>        if total_reward &gt; best_reward:<br class="title-page-name"/>            best_reward = total_reward<br class="title-page-name"/>        print("Episode#:{} reward:{} best_reward:{} eps:{}".format(episode,<br class="title-page-name"/>                                     total_reward, best_reward, agent.epsilon))<br class="title-page-name"/>    # Return the trained policy<br class="title-page-name"/>    return np.argmax(agent.Q, axis=2)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Testing and recording the performance of the agent</h1>
                
            
            <article>
                
<p class="calibre2">Once we let the agent train at the Gym, we want to be able to measure how well it has learned. To do that, we let the agent go through a test. Just like in school! <kbd class="calibre12">test(agent, env, policy)</kbd> takes the agent object, the environment instance, and the agent's policy to test the performance of the agent in the environment, and returns the total reward for one full episode. It is similar to the <kbd class="calibre12">train(agent, env)</kbd> function we saw earlier, but it does not let the agent learn or update its Q-value estimates:</p>
<pre class="calibre17">def test(agent, env, policy):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        action = policy[agent.discretize(obs)]<br class="title-page-name"/>        next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>        obs = next_obs<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>    return total_reward</pre>
<p class="calibre2">Note that the <span class="calibre5"><kbd class="calibre12">test(agent, env, policy)</kbd> function evaluates the agent's performance on one episode and returns the total reward obtained by the agent in that episode. We would want to measure how well the agent performs on several episodes to get a good measure of the agent's actual performance. Also, the Gym provides a handy wrapper function called <em class="calibre13">monitor</em> to record the progress of the agent in video files. The following code snippet illustrates how to test and record the agent's performance on <em class="calibre13">1,000</em> episodes and save the recorded agent's action in the environment as video files in the </span><kbd class="calibre12">gym_monitor_path</kbd> directory:</p>
<pre class="calibre17">if __name__ == "__main__":<br class="title-page-name"/>    env = gym.make('MountainCar-v0')<br class="title-page-name"/>    agent = Q_Learner(env)<br class="title-page-name"/>    learned_policy = train(agent, env)<br class="title-page-name"/>    # Use the Gym Monitor wrapper to evalaute the agent and record video<br class="title-page-name"/>    gym_monitor_path = "./gym_monitor_output"<br class="title-page-name"/>    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)<br class="title-page-name"/>    for _ in range(1000):<br class="title-page-name"/>        test(agent, env, learned_policy)<br class="title-page-name"/>    env.close()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">A simple and complete Q-Learner implementation for solving the Mountain Car problem</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will put together the whole code into a single Python script to initialize the environment, launch the agent's training process, get the trained policy, test the performance of the agent, and also record how it acts in the environment!</p>
<pre class="calibre17">#!/usr/bin/env/ python<br class="title-page-name"/>import gym<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>MAX_NUM_EPISODES = 50000<br class="title-page-name"/>STEPS_PER_EPISODE = 200 #  This is specific to MountainCar. May change with env<br class="title-page-name"/>EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,<br class="title-page-name"/>                           self.action_shape))  # (51 x 51 x 3)<br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            return np.argmax(self.Q[discretized_obs])<br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        discretized_next_obs = self.discretize(next_obs)<br class="title-page-name"/>        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])<br class="title-page-name"/>        td_error = td_target - self.Q[discretized_obs][action]<br class="title-page-name"/>        self.Q[discretized_obs][action] += self.alpha * td_error<br class="title-page-name"/><br class="title-page-name"/>def train(agent, env):<br class="title-page-name"/>    best_reward = -float('inf')<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        done = False<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>        if total_reward &gt; best_reward:<br class="title-page-name"/>            best_reward = total_reward<br class="title-page-name"/>        print("Episode#:{} reward:{} best_reward:{} eps:{}".format(episode,<br class="title-page-name"/>                                     total_reward, best_reward, agent.epsilon))<br class="title-page-name"/>    # Return the trained policy<br class="title-page-name"/>    return np.argmax(agent.Q, axis=2)<br class="title-page-name"/><br class="title-page-name"/>def test(agent, env, policy):<br class="title-page-name"/>    done = False<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    total_reward = 0.0<br class="title-page-name"/>    while not done:<br class="title-page-name"/>        action = policy[agent.discretize(obs)]<br class="title-page-name"/>        next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>        obs = next_obs<br class="title-page-name"/>        total_reward += reward<br class="title-page-name"/>    return total_reward<br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env = gym.make('MountainCar-v0')<br class="title-page-name"/>    agent = Q_Learner(env)<br class="title-page-name"/>    learned_policy = train(agent, env)<br class="title-page-name"/>    # Use the Gym Monitor wrapper to evalaute the agent and record video<br class="title-page-name"/>    gym_monitor_path = "./gym_monitor_output"<br class="title-page-name"/>    env = gym.wrappers.Monitor(env, gym_monitor_path, force=True)<br class="title-page-name"/>    for _ in range(1000):<br class="title-page-name"/>        test(agent, env, learned_policy)<br class="title-page-name"/>    env.close()</pre>
<p class="calibre2">This script is available in the code repository under the <kbd class="calibre12">ch5</kbd> folder, named <kbd class="calibre12">Q_learner_MountainCar.py</kbd>.</p>
<p class="calibre2">Activate the <kbd class="calibre12">rl_gym_book</kbd> conda environment and launch the script to see it in action! When you launch the script, you will see initial output like that shown in this screenshot:</p>
<div class="cdpaligncenter"><img src="../images/00126.jpeg" class="calibre65"/></div>
<p class="calibre2">During the initial training episodes, when the agent is just getting started learning, you will see that it always ends up with a reward of <em class="calibre13">-200</em>. From your understanding of how the Gym's Mountain Car environment works, you can see that the agent does not reach the mountain top within the <em class="calibre13">200</em> time steps, and so the environment automatically resets the environment; thus, the agent only gets <em class="calibre13">-200</em>. You can also observe the <strong class="calibre4"><img class="fm-editor-equation22" src="../images/00127.jpeg"/> </strong>(<strong class="calibre4">eps</strong>) exploration value decaying slowly.</p>
<p class="calibre2">If you let the agent learn for long enough, you will see the agent improving and learning to reach the top of the mountain in fewer and fewer steps. Here is a sample of its progress after <em class="calibre13">5</em> minutes of training on a typical laptop hardware:</p>
<div class="cdpaligncenter"><img src="../images/00128.jpeg" class="calibre66"/></div>
<p class="calibre2">Once the script finishes running, you will see the recorded videos (along with some <kbd class="calibre12">.stats.json</kbd> and <kbd class="calibre12">.meta.json</kbd> files) of the agent's performance in the <kbd class="calibre12">gym_monitor_output</kbd> folder. You can watch the videos to see how your agent performed!</p>
<p class="calibre2">Here is a screenshot showing the agent successfully steering the car to the top of the mountain:</p>
<div class="cdpaligncenter"><img src="../images/00129.jpeg" class="calibre67"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Hooray!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">We learned a lot in this chapter. More importantly, we implemented an agent that learned to solve the Mountain Car problem smartly in 7 minutes or so!</p>
<p class="calibre2">We started by understanding the famous Mountain Car problem and looking at how the environment, the observation space, the state space, and rewards are designed in the Gym's <kbd class="calibre12">MountainCar-v0</kbd> environment. We revisited the reinforcement learning Gym boilerplate code we used in the previous chapter and made some improvements to it, which are also available in the code repository of this book.</p>
<p class="calibre2">We then defined the hyperparameters for our Q-learning agent and started implementing a Q-learning algorithm from scratch. We first implemented the agent's initialization function to initialize the agent's internal state variables, including the Q value representation, using a NumPy <em class="calibre13">n</em>-dimensional array. Then, we implemented the <kbd class="calibre12">discretize</kbd> method to discretize the <kbd class="calibre12">state space</kbd>; the <kbd class="calibre12">get_action(...)</kbd> method to select an action based on an epsilon-greedy policy; and then finally the <kbd class="calibre12">learn(...)</kbd> function, which implements the Q-learning update rule and forms the core of the agent. We saw how simple it was to implement them all! We also implemented functions to train, test, and evaluate the agent's performance. </p>
<p class="calibre2">I hope you had a lot of fun implementing the agent and watching it solve the Mountain Car problem at the Gym! We will get into advanced methods in the next chapter to solve a variety of more challenging problems.</p>


            </article>

            
        </section>
    </body></html>