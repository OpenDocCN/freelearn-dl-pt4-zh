["```py\nimport numpy as np\n```", "```py\n# set seed for reproducibility\nseed_val = 9000\nnp.random.seed(seed_val)\n```", "```py\nfrom keras.datasets import mnist\n```", "```py\n# unpack mnist data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n```", "```py\nprint('Size of the training_set: ', X_train.shape)\nprint('Size of the test_set: ', X_test.shape)\nprint('Shape of each image: ', X_train[0].shape)\nprint('Total number of classes: ', len(np.unique(y_train)))\nprint('Unique class labels: ', np.unique(y_train))\n```", "```py\nimport matplotlib.pyplot as plt\n# Plot of 9 random images\nfor i in range(0, 9):\n    plt.subplot(331+i) # plot of 3 rows and 3 columns\n    plt.axis('off') # turn off axis\n    plt.imshow(X_train[i], cmap='gray') # gray scale\n```", "```py\n# maximum and minimum pixel values\nprint('Maximum pixel value in the training_set: ', np.max(X_train))\nprint('Minimum pixel value in the training_set: ', np.min(X_train))\n```", "```py\n# Number of epochs\nepochs = 20\n\n# Batchsize\nbatch_size = 128\n\n# Optimizer for the generator from keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n\n# Shape of the input image\ninput_shape = (28,28,1)\n```", "```py\nfrom keras.models import Sequential\nmodel = Sequential()\n```", "```py\nfrom keras.layers import Dense\n```", "```py\nmodel.add(Dense(300, input_shape=(784,), activation = 'relu'))\n```", "```py\nmodel.add(Dense(300,  activation='relu'))\n```", "```py\nmodel.add(Dense(10,  activation='softmax'))\n```", "```py\n# compile the model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer , metrics = ['accuracy'])\n```", "```py\n# print model summary\nmodel.summary()\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n# create train and validation data\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.08333, random_state=42)\n\nX_train = X_train.reshape(-1, 784)\nX_val = X_val.reshape(-1, 784)\nX_test = X_test.reshape(-1, 784)\n\nprint('Training Examples', X_train.shape[0])\nprint('Validation Examples', X_val.shape[0])\nprint('Test Examples', X_test.shape[0])\n```", "```py\n# fit the model\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n```", "```py\n# evaluate the model\nloss, acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef loss_plot(history):\n    train_acc = history.history['acc']\n    val_acc = history.history['val_acc']\n\n    plt.figure(figsize=(9,5))\n    plt.plot(np.arange(1,21),train_acc, marker = 'D', label = 'Training Accuracy')\n    plt.plot(np.arange(1,21),val_acc, marker = 'o', label = 'Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Train/Validation Accuracy')\n    plt.legend()\n    plt.margins(0.02)\n    plt.show()\n\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(9,5))\n    plt.plot(np.arange(1,21),train_loss, marker = 'D', label = 'Training Loss')\n    plt.plot(np.arange(1,21),val_loss, marker = 'o', label = 'Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Train/Validation Loss')\n    plt.legend()\n    plt.margins(0.02)\n    plt.show()\n\n# plot training loss\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a simple multi layer perceptron in keras.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 20\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1, 784)\nX_val = X_val.reshape(-1, 784)\nX_test = X_test.reshape(-1, 784)\n\nmodel = Sequential()\nmodel.add(Dense(300, input_shape=(784,), activation = 'relu'))\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer,\n              metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size,\n                    validation_data=(X_val, y_val))\n\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\narray = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\nkernel = np.array([-1, 1, 0])\n```", "```py\narray = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\nkernel = np.array([-1, 1, 0])\n\n# empty feature map\nconv_result = np.zeros(array.shape[0] - kernel.shape[0] +1).astype(int)\n\nfor i in range(array.shape[0] - kernel.shape[0] +1):\n    # convolving\n    conv_result[i] = (kernel * array[i:i+3]).sum()\n    print(kernel, '*', array[i:i+3], '=', conv_result[i])\n\nprint('Feature Map :', conv_result)\n```", "```py\nfrom keras.layers import Conv2D\n```", "```py\n# reshape data \nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nprint('Train data shape:', X_train.shape)\nprint('Val data shape:', X_val.shape)\nprint('Test data shape:', X_test.shape)\n```", "```py\nmodel = Sequential()\n```", "```py\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape, activation = 'relu'))\n```", "```py\nfrom keras.layers import Flatten\nmodel.add(Flatten())\n```", "```py\nmodel.add(Dense(128, activation = 'relu'))\n```", "```py\nmodel.add(Dense(10, activation = 'softmax'))\n```", "```py\n# compile model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n\n# print model summary\nmodel.summary()\n```", "```py\n# fit model\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n```", "```py\n# evaluate model\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\n# plot training loss\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a simple convolution classifier.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 20\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape,\n                 activation = 'relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer,\n              metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size,\n                    validation_data=(X_val, y_val))\n\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\narray = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n```", "```py\n# 1D Max Pooling\narray = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\nresult = np.zeros(len(array)//2)\nfor i in range(len(array)//2):\n    result[i] = np.max(array[2*i:2*i+2])\nresult\n```", "```py\n# 1D Average Pooling\narray = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\nresult = np.zeros(len(array)//2)\nfor i in range(len(array)//2):\n    result[i] = np.mean(array[2*i:2*i+2])\nresult\n```", "```py\nplt.imshow(X_train[0].reshape(28,28), cmap='gray') \n```", "```py\ndef square_max_pool(image, pool_size=2):\n    result = np.zeros((14,14))\n    for i in range(result.shape[0]):\n        for j in range(result.shape[1]):\n            result[i,j] = np.max(image[i*pool_size : i*pool_size+pool_size, j*pool_size : j*pool_size+pool_size])\n\n    return result\n\n# plot a pooled image\nplt.imshow(square_max_pool(X_train[0].reshape(28,28)), cmap='gray')\n```", "```py\nfrom keras.layers import MaxPool2D\n```", "```py\n# model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape, activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(10, activation = 'softmax'))\n\n# compile model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n\n# print model summary\nmodel.summary()\n```", "```py\n# fit model\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n```", "```py\n# evaluate model\nloss, acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\n# plot training loss\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a convolution classifier with maxpool operation.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 20\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape,\n                 activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer,\n              metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size,\n                    validation_data=(X_val, y_val))\n\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\nfrom keras.layers import Dropout\n```", "```py\n# model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape, activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\n# compile model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n\n# model summary\nmodel.summary()\n```", "```py\n# fit model\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n```", "```py\n# evaluate model\nloss, acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\n# plot training loss\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a deep conv classifier with max pool and dropout.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 20\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape,\n                 activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer,\n              metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size,\n                    validation_data=(X_val, y_val))\n\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\n# model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape, activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\n# compile model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n\n# print model summary\nmodel.summary()\n```", "```py\n# fit model\nhistory = model.fit(X_train, y_train, epochs = 40, batch_size=batch_size, validation_data=(X_val, y_val))\n```", "```py\n# evaluate model\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\n# plot training loss\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a deep conv classifier with max pool and dropout.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 20\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.0001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape,\n                 activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer,\n              metrics = ['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size,\n                    validation_data=(X_val, y_val))\n\nloss,acc = model.evaluate(X_test, y_test)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\nfrom keras.preprocessing.image import ImageDataGenerator\n```", "```py\ntrain_datagen = ImageDataGenerator(horizontal_flip=True)  \n```", "```py\n# fit the augmenter\ntrain_datagen.fit(X_train)\n```", "```py\n# transform the data\nfor img, label in train_datagen.flow(X_train, y_train, batch_size=6):\n    for i in range(0, 6):\n        plt.subplot(2,3,i+1)\n        plt.title('Label {}'.format(label[i]))\n        plt.imshow(img[i].reshape(28, 28), cmap='gray')\n    break\nplt.tight_layout()\nplt.show()\n```", "```py\ntrain_datagen = ImageDataGenerator(zoom_range=0.3)\n #fit\ntrain_datagen.fit(X_train)\n\n#transform\nfor img, label in train_datagen.flow(X_train, y_train, batch_size=6):\n    for i in range(0, 6):\n        plt.subplot(2,3,i+1)\n        plt.title('Label {}'.format(label[i]))\n        plt.imshow(img[i].reshape(28, 28), cmap='gray')\n    break\nplt.tight_layout()\nplt.show()\n```", "```py\ntrain_datagen = ImageDataGenerator(\n        rescale = 1./255,\n        zoom_range = 0.2,\n        horizontal_flip = True)  \n```", "```py\ntrain_datagen.fit(X_train)\n```", "```py\n# define model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape, activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\n# compile model\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n```", "```py\n# fit the model on batches with real-time data augmentation\nhistory = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=128), steps_per_epoch=len(X_train) / 128, epochs=10, validation_data=(train_datagen.flow(X_val, y_val)))\n```", "```py\n# transform/augment test data\nfor test_img, test_lab in train_datagen.flow(X_test, y_test, batch_size = X_test.shape[0]):\n    break\n\n# evaluate model on test data \nloss,acc = model.evaluate(test_img, test_lab)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n```", "```py\n# plot the learning\nloss_plot(history)\n```", "```py\n\"\"\"This module implements a deep conv classifier on augmented data.\"\"\"\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import train_test_split\nfrom loss_plot import loss_plot\n\n# Number of epochs\nepochs = 10\n# Batchsize\nbatch_size = 128\n# Optimizer for the generator\nfrom keras.optimizers import Adam\noptimizer = Adam(lr=0.001)\n# Shape of the input image\ninput_shape = (28,28,1)\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  stratify = y_train,\n                                                  test_size = 0.08333,\n                                                  random_state=42)\n\nX_train = X_train.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntrain_datagen.fit(X_train)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape,\n                 activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer= optimizer,\n              metrics = ['accuracy'])\n\n# fits the model on batches with real-time data augmentation:\nhistory = model.fit_generator(train_datagen.flow(X_train, y_train,\n                                                 batch_size=128),\n                              steps_per_epoch=len(X_train) / 128, epochs=epochs,\n                              validation_data=(train_datagen.flow(X_val,\n                                                                  y_val)))\n\nfor test_img, test_lab in train_datagen.flow(X_test, y_test,\n                                             batch_size = X_test.shape[0]):\n    break\n\nloss,acc = model.evaluate(test_img, test_lab)\nprint('Test loss:', loss)\nprint('Accuracy:', acc)\n\nloss_plot(history)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nfrom keras.layers import Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras import backend as k\n\n# for resizing images\nfrom scipy.misc import imresize\n```", "```py\ndef reshape(x):\n    \"\"\"Reshape images to 14*14\"\"\"\n    img = imresize(x.reshape(28,28), (14, 14))\n    return img\n\n# create 14*14 low resolution train and test images\nXX_train = np.array([*map(reshape, X_train.astype(float))])\nXX_test = np.array([*map(reshape, X_test.astype(float))])\n```", "```py\n# scale images to range between 0 and 1\n# 14*14 train images\nXX_train = XX_train/255\n# 28*28 train label images\nX_train = X_train/255\n\n# 14*14 test images\nXX_test = XX_test/255\n# 28*28 test label images\nX_test = X_test/255\n```", "```py\nbatch_size = 128\nepochs = 40\ninput_shape = (14,14,1)\n\n# define autoencoder\ndef make_autoencoder(input_shape):\n\n    generator = Sequential()\n    generator.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n    generator.add(MaxPooling2D(pool_size=(2, 2)))\n\n    generator.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n\n    generator.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    generator.add(UpSampling2D((2, 2)))\n\n    generator.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    generator.add(UpSampling2D((2, 2)))\n\n    generator.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n\n    return generator\n\nautoencoder = make_autoencoder(input_shape)\n\n# compile auto encoder\nautoencoder.compile(loss='mean_squared_error', optimizer = Adam(lr=0.0002, beta_1=0.5))\n\n# auto encoder summary\nautoencoder.summary()\n```", "```py\n# fit autoencoder \nautoencoder_train = autoencoder.fit(XX_train.reshape(-1,14,14,1), X_train.reshape(-1,28,28,1), batch_size=batch_size,\n                                    epochs=epochs, verbose=1,\n                                    validation_split = 0.2)\n```", "```py\nloss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nepochs_ = [x for x in range(epochs)]\nplt.figure()\nplt.plot(epochs_, loss, label='Training loss')\nplt.plot(epochs_, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\nprint('Input')\nplt.figure(figsize=(5,5))\nfor i in range(9):\n    plt.subplot(331 + i)\n    plt.imshow(np.squeeze(XX_test.reshape(-1,14,14)[i]), cmap='gray')\nplt.show()\n\n# Test set results\nprint('GENERATED')\nplt.figure(figsize=(5,5))\nfor i in range(9):\n    pred = autoencoder.predict(XX_test.reshape(-1,14,14,1)[i:i+1], verbose=0)\n    plt.subplot(331 + i)\n    plt.imshow(pred[0].reshape(28,28), cmap='gray')\nplt.show()\n```", "```py\n\"\"\"This module implements a convolution autoencoder on MNIST data.\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nfrom keras.layers import Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras import backend as k\n\n# for resizing images\nfrom scipy.misc import imresize\n\ndef reshape(x):\n    \"\"\"Reshape images to 14*14\"\"\"\n    img = imresize(x.reshape(28,28), (14, 14))\n    return img\n\n# create 14*14 low resolution train and test images\nXX_train = np.array([*map(reshape, X_train.astype(float))])\nXX_test = np.array([*map(reshape, X_test.astype(float))])\n\n# scale images to range between 0 and 1\n#14*14 train images\nXX_train = XX_train/255\n#28*28 train label images\nX_train = X_train/255\n\n#14*14 test images\nXX_test = XX_test/255\n#28*28 test label images\nX_test = X_test/255\n\nbatch_size = 128\nepochs = 40\ninput_shape = (14,14,1)\n\ndef make_autoencoder(input_shape):\n\n    generator = Sequential()\n    generator.add(Conv2D(64, (3, 3), activation='relu', padding='same',\n                         input_shape=input_shape))\n    generator.add(MaxPooling2D(pool_size=(2, 2)))\n\n    generator.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n\n    generator.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    generator.add(UpSampling2D((2, 2)))\n\n    generator.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    generator.add(UpSampling2D((2, 2)))\n\n    generator.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n\n    return generator\n\nautoencoder = make_autoencoder(input_shape)\nautoencoder.compile(loss='mean_squared_error', optimizer = Adam(lr=0.0002,\n                                                                beta_1=0.5))\n\nautoencoder_train = autoencoder.fit(XX_train.reshape(-1,14,14,1),\n                                    X_train.reshape(-1,28,28,1),\n                                    batch_size=batch_size,\n                                    epochs=epochs, verbose=1,\n                                    validation_split = 0.2)\n\nloss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nepochs_ = [x for x in range(epochs)]\nplt.figure()\nplt.plot(epochs_, loss, label='Training loss', marker = 'D')\nplt.plot(epochs_, val_loss, label='Validation loss', marker = 'o')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\nprint('Input')\nplt.figure(figsize=(5,5))\nfor i in range(9):\n    plt.subplot(331 + i)\n    plt.imshow(np.squeeze(XX_test.reshape(-1,14,14)[i]), cmap='gray')\nplt.show()\n\n# Test set results\nprint('GENERATED')\nplt.figure(figsize=(5,5))\nfor i in range(9):\n    pred = autoencoder.predict(XX_test.reshape(-1,14,14,1)[i:i+1], verbose=0)\n    plt.subplot(331 + i)\n    plt.imshow(pred[0].reshape(28,28), cmap='gray')\nplt.show()\n```"]