["```py\nChapter18$ ./riverswim.py \n1:     40 \n2:     39 \n3:     17 \n4:     3 \n5:     1 \n6:     0\n```", "```py\nChapter18$ ./riverswim.py -n 1000 \n1:     441 \n2:     452 \n3:     93 \n4:     12 \n5:     2 \n6:     0\n```", "```py\nChapter18$ ./riverswim.py -n 10000 \n1:     4056 \n2:     4506 \n3:     1095 \n4:     281 \n5:     57 \n6:     5\n```", "```py\n>>> import gymnasium as gym \n>>> e = gym.make(\"MountainCar-v0\") \n>>> e.reset() \n(array([-0.56971574,  0\\.       ], dtype=float32), {}) \n>>> e.observation_space \nBox([-1.2  -0.07], [0.6  0.07], (2,), float32) \n>>> e.action_space \nDiscrete(3) \n>>> e.step(0) \n(array([-0.570371  , -0.00065523], dtype=float32), -1.0, False, False, {}) \n>>> e.step(0) \n(array([-0.57167655, -0.00130558], dtype=float32), -1.0, False, False, {}) \n>>> e.step(0) \n(array([-0.57362276, -0.00194625], dtype=float32), -1.0, False, False, {})\n```", "```py\nMountainCarNoisyNetDQN( \n  (net): Sequential( \n   (0): Linear(in_features=2, out_features=128, bias=True) \n   (1): ReLU() \n   (2): NoisyLinear(in_features=128, out_features=3, bias=True) \n  ) \n)\n```", "```py\nclass PseudoCountRewardWrapper(gym.Wrapper): \n    def __init__(self, env: gym.Env, hash_function = lambda o: o, \n                 reward_scale: float = 1.0): \n        super(PseudoCountRewardWrapper, self).__init__(env) \n        self.hash_function = hash_function \n        self.reward_scale = reward_scale \n        self.counts = collections.Counter()\n```", "```py\n def _count_observation(self, obs) -> float: \n        h = self.hash_function(obs) \n        self.counts[h] += 1 \n        return np.sqrt(1/self.counts[h])\n```", "```py\n def step(self, action): \n        obs, reward, done, is_tr, info = self.env.step(action) \n        extra_reward = self._count_observation(obs) \n        return obs, reward + self.reward_scale * extra_reward, done, is_tr, info\n```", "```py\ndef counts_hash(obs: np.ndarray): \n    r = obs.tolist() \n    return tuple(map(lambda v: round(v, 3), r))\n```", "```py\nMountainCarBasePPO( \n  (actor): Sequential( \n   (0): Linear(in_features=2, out_features=64, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=64, out_features=3, bias=True) \n  ) \n  (critic): Sequential( \n   (0): Linear(in_features=2, out_features=64, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=64, out_features=1, bias=True) \n  ) \n)\n```", "```py\nEpisode 61454: reward=-159.17, steps=168, speed=4581.6 f/s, elapsed=1:37:18 \nEpisode 61455: reward=-158.46, steps=164, speed=4609.0 f/s, elapsed=1:37:18 \nEpisode 61456: reward=-158.41, steps=164, speed=4582.3 f/s, elapsed=1:37:18 \nEpisode 61457: reward=-152.73, steps=158, speed=4556.4 f/s, elapsed=1:37:18 \nEpisode 61458: reward=-154.08, steps=159, speed=4548.1 f/s, elapsed=1:37:18 \nEpisode 61459: reward=-154.85, steps=162, speed=4513.0 f/s, elapsed=1:37:18 \nTest done: got -91.000 reward after 91 steps, avg reward -129.999 \nReward boundary has crossed, stopping training. Congrats!\n```", "```py\nclass MountainCarNetDistillery(nn.Module): \n    def __init__(self, obs_size: int, hid_size: int = 128): \n        super(MountainCarNetDistillery, self).__init__() \n\n        self.ref_net = nn.Sequential( \n            nn.Linear(obs_size, hid_size), \n            nn.ReLU(), \n            nn.Linear(hid_size, hid_size), \n            nn.ReLU(), \n            nn.Linear(hid_size, 1), \n        ) \n        self.ref_net.train(False) \n\n        self.trn_net = nn.Sequential( \n            nn.Linear(obs_size, 1), \n        ) \n\n    def forward(self, x): \n        return self.ref_net(x), self.trn_net(x) \n\n    def extra_reward(self, obs): \n        r1, r2 = self.forward(torch.FloatTensor([obs])) \n        return (r1 - r2).abs().detach().numpy()[0][0] \n\n    def loss(self, obs_t): \n        r1_t, r2_t = self.forward(obs_t) \n        return F.mse_loss(r2_t, r1_t).mean()\n```", "```py\nEpisode 33566: reward=-93.27, steps=149, speed=2962.8 f/s, elapsed=1:23:48 \nEpisode 33567: reward=-82.13, steps=144, speed=2968.6 f/s, elapsed=1:23:48 \nEpisode 33568: reward=-83.77, steps=143, speed=2973.7 f/s, elapsed=1:23:48 \nEpisode 33569: reward=-93.59, steps=160, speed=2974.0 f/s, elapsed=1:23:48 \nEpisode 33570: reward=-83.04, steps=143, speed=2979.7 f/s, elapsed=1:23:48 \nEpisode 33571: reward=-97.96, steps=158, speed=2984.5 f/s, elapsed=1:23:48 \nEpisode 33572: reward=-92.60, steps=150, speed=2989.8 f/s, elapsed=1:23:48 \nTest done: got -87.000 reward after 87 steps, avg reward -129.549 \nReward boundary has crossed, stopping training. Congrats!\n```"]