- en: Using LSTMs in Generative Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will be able to accomplish the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading novels/books that will be used as input text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing and cleansing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and saving the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating similar text using the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the drawbacks of **recurrent neural networks** (**RNNs**) when it comes
    to backpropagation, **Long Short-Term Memory Units** (**LSTMs**) and **Gated Recurrent
    Units** (**GRUs**) have been gaining popularity in recent times when it comes
    to learning sequential input data as they are better suited to tackle problems
    of vanishing and exploding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading novels/books that will be used as input text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will go the steps that we need to download the novels/books
    which we will use as input text for the execution of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Place the input data in the form of a `.txt` file in the working directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input may be any kind of text, such as song lyrics, novels, magazine articles,
    and source code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the classical texts are no longer protected by copyright and may be
    downloaded for free and used in experiments. The best place to get access to free
    books is Project [Gutenberg](http://www.gutenberg.org/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using *The Jungle book* by Rudyard Kipling as the
    input to train our model and generate statistically similar text as output. The
    following screenshot shows you how to download the necessary file in `.txt` format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cefd5b72-ded3-45fc-8617-3ac54c0ca5e8.png)'
  prefs: []
  type: TYPE_IMG
- en: After visiting the website and searching for the required book, click on Plain
    Text UTF-8 and download it. UTF-8 basically specifies the type of encoding. The
    text may be copied and pasted or saved directly to the working directory by clicking
    on the link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before beginning, it always helps to take a look at the data and analyze it.
    After looking at the data, we can see that there are a lot of punctuation marks,
    blank spaces, quotes, and uppercase as well as lowercase letters. We need to prepare
    the data first before performing any kind of analysis on it or feeding it into
    the LSTM network. We require a number of libraries that will make handling data
    easier :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries by issuing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output to the preceding commands looks like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7cf320aa-709e-4ec0-9387-3072025116e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is always a good idea to double check the current working directory and
    choose the required folder as the working directory. In our case, the `.txt` file
    is named `junglebook.txt` and is held in the folder named `Chapter 8`. So, we
    will select that folder as the working directory for the whole chapter. This may
    be done as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/213ed09c-fcfd-4047-984d-dfd893747cc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, load the file into the program''s memory by defining a function named
    `load_document`, which can be done by issuing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the previously defined function to load the document into memory and print
    the first `2000` characters of the text file using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding function as well as the commands produces the output 
    shown in the following screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5d383a5a-8337-473b-ad24-362275e51ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output to the above code is shown in the screenshot here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0437d675-32ce-47d2-8cd6-638188e83b29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot is a continuation of the previous output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b7dafab-2a29-4a29-8081-00fb977e0a41.png)'
  prefs: []
  type: TYPE_IMG
- en: As seen in the preceding screenshots, the first `2000` characters from the `.txt`
    file are printed. It is always a good idea to analyze the data by looking at it
    before performing any preprocessing on it. It will give a better idea of how to
    approach the preprocessing steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `array` function will be used to handle data in the form of arrays. The
    `numpy` library provides this function readily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since our data is only text data, we will require the string library to handle
    all input data as strings before encoding the words as integers, which can be
    fed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `tokenizer` function will be used to split all the sentences into tokens,
    where each token represents a word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pickle library will be required in order to save the dictionary into a pickle
    file by using the `dump` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `to_categorical` function from the `keras` library converts a class vector
    (integers) to a binary class matrix, for example, for use with `categorical_crossentropy`,
    which we will require at a later stage in order to map tokens to unique integers
    and vice versa.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of the other Keras layers required in this chapter are the LSTM layer,
    dense layer, dropout layer, and the embedding layer. The model will be defined
    sequentially, for which we require the sequential model from the `keras` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may also use the same model with different types of texts, such as customer
    reviews on websites, tweets, structured text such as source code, mathematics
    theories, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea of this chapter to understand how LSTMs learn long-term dependencies
    and how they perform better at processing sequential data when compared to recurrent
    neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another good idea would be to input *Pokémon* names into the model and try to
    generate your own Pokémon names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More information about the different libraries used can be found at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.scipy-lectures.org/intro/numpy/array_object.html](https://www.scipy-lectures.org/intro/numpy/array_object.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/2/library/string.html](https://docs.python.org/2/library/string.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/layers/core/](https://keras.io/layers/core/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing and cleansing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section of this chapter will discuss the various data preparation and text
    preprocessing steps involved before feeding it into the model as input. The specific
    way we prepare the data really depends on how we intend to model it, which in
    turn depends on how we intend to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The language model will be based on statistics and predict the probability of
    each word given an input sequence of text. The predicted word will be fed in as
    input to the model, to, in turn, generate the next word.
  prefs: []
  type: TYPE_NORMAL
- en: A key decision is how long the input sequences should be. They need to be long
    enough to allow the model to learn the context for the words to predict. This
    input length will also define the length of the seed text used to generate new
    sequences when we use the model.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of simplicity, we will arbitrarily pick a length of 50 words
    for the length of the input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on reviewing the text (which we did previously), the following are some
    operations that could be performed to clean and preprocess the text in the input
    file. We have presented a few options regarding text preprocessing. However, you
    may want to explore more cleaning operations as an exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace dashes `–` with whitespaces so you can split words better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split words based on whitespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove all punctuation from the input text in order to reduce the number of
    unique characters in the text that is fed into the model (for example, Why? becomes
    Why)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove all words that are not alphabetic to remove standalone punctuation tokens
    and emoticons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert all words from uppercase to lowercase in order to reduce the size of
    the total number of tokens further and remove any discrepancies and data redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary size is a decisive factor in language modeling and deciding the
    training time for the model. A smaller vocabulary results in a more efficient
    model that trains faster. While it is good to have a small vocabulary in some
    cases, it helps to have a larger vocabulary in other cases in order to prevent
    overfitting. In order to preprocess the data, we are going to need a function
    that takes in the entire input text, splits it up based on white spaces, removes
    all punctuation, normalizes all cases, and returns a sequence of tokens. For this
    purpose, define the `clean_document` function by issuing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously defined function will basically take the loaded document/file
    as its argument and return an array of clean tokens, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7fc71d44-55e1-47bf-aa15-a10245e53dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, print out some of the tokens and statistics just to develop a better
    understanding of what the `clean_document` function is doing. This step is done
    by issuing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding set of commands prints the first two hundred tokens
    and is as shown in the following screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/70c89890-2b03-4307-8469-ab2ce81fb726.png)![](img/7ac2d64a-0cb2-4175-812d-a516e1c03953.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, organize all these tokens into sequences, with each sequence containing
    50 words (chosen arbitrarily) using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of sequences formed from the document may be viewed by printing
    them out, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a5eb950-c902-485f-849e-adc3fd169a64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Save all the generated tokens as well as sequences into a file in the working
    directory by defining the `save_doc` function using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To save the sequences, use the following two commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This process is illustrated in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a93679f6-9abc-479d-9718-1634ceb2092c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, load the saved document, which contains all the saved tokens and sequences,
    into the memory using the `load_document` function, which is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b3dc8275-2494-4658-8aa6-5651f8259036.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `clean_document` function removes all whitespaces, punctuation, uppercase
    text, and quotation marks, and splits the entire document into tokens, where each
    token is a word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By printing the total number of tokens and total unique tokens in the document,
    we will note that the `clean_document` function generated 51,473 tokens, out of
    which 5,027 tokens (or words) are unique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `save_document` function then saves all of these tokens as well as unique
    tokens which are required to generate our sequences of 50 words each. Note how,
    by looping through all the generated tokens, we are able to generate a long list
    of 51,422 sequences. These are the same sequences that will be used as input to
    train the language model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before training the model on all 51,422 sequences, it is always a good practice
    to save the tokens as well as sequences to file. Once saved, the file can be loaded
    back into the memory using the defined `load_document` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sequences are organized as 50 input tokens and one output token (which means
    that there are 51 tokens per sequence). For predicting each output token, the
    previous 50 tokens will be used as the input to the model. We can do this by iterating
    over the list of tokens from token 51 onwards and taking the previous 50 tokens
    as a sequence, then repeating this process until the end of the list of all tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visit the following links for a better understanding of data preparation using
    various functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.python.org/3/library/tokenize.html](https://docs.python.org/3/library/tokenize.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/utils/](https://keras.io/utils/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.pythonforbeginners.com/dictionary/python-split](http://www.pythonforbeginners.com/dictionary/python-split)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tutorialspoint.com/python/string_join.htm](https://www.tutorialspoint.com/python/string_join.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tutorialspoint.com/python/string_lower.htm](https://www.tutorialspoint.com/python/string_lower.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before defining and feeding data into an LSTM network it is important that the
    data is converted into a form which can be understood by the neural network. Computers
    understand everything in binary code (0s and 1s) and therefore, the textual or
    data in string format needs to be converted into one hot encoded variables.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For understanding how one hot encoding works, visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ritchieng.com/machinelearning-one-hot-encoding/](https://www.ritchieng.com/machinelearning-one-hot-encoding/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the going through the previous section you should be able to clean the
    entire corpus and split up sentences. The next steps which involve one hot encoding
    and tokenizing sentences can be done in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the tokens and sequences are saved to a file and loaded into memory, they
    have to be encoded as integers since the word embedding layer in the model expects
    input sequences to be comprised of integers and not strings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is done by mapping each word in the vocabulary to a unique integer and
    encoding the input sequences. Later, while making predictions, the predictions
    can be converted (or mapped) back to numbers to look up their associated words
    in the same mapping and reverse map back from integers to words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To perform this encoding, utilize the `Tokenizer` class in the Keras API. Before
    encoding, the tokenizer must be trained on the entire dataset so it finds all
    the unique tokens and assigns each token a unique integer. The commands to do
    so as are  follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You also need to calculate the size of the vocabulary before defining the embedding
    layer later. This is determined by calculating the size of the mapping dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Therefore, when specifying the vocabulary size to the Embedding layer, specify
    it as 1 larger than the actual vocabulary. The vocabulary size is therefore defined
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that once the input sequences have been encoded, they need to be separated
    into input and output elements, which can be done by array slicing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After separating, one hot encode the output word. This means converting it from
    an integer to an n-dimensional vector of 0 values, one for each word in the vocabulary,
    with a 1 to indicate the specific word at the index of the word's integer value. Keras
    provides the `to_categorical()` function, which can be used to one hot encode
    the output words for each input-output sequence pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, specify to the Embedding layer how long input sequences are. We know
    that there are 50 words because the model was designed by specifying the sequence
    length as 50, but a good generic way to specify the sequence length is to use
    the second dimension (number of columns) of the input data’s shape.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be done by issuing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the outputs you must see on executing the commands
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the commands for tokenizing the sentences and calculating vocabulary
    length you must see an output as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fc6fb70e-e203-4bbe-9352-b1f710c3cb26.png)'
  prefs: []
  type: TYPE_IMG
- en: Words are assigned values starting from 1 up to the total number of words (for
    example, 5,027 in this case). The Embedding layer needs to allocate a vector representation
    for each word in this vocabulary from index 1 to the largest index. The index
    of the word at the end of the vocabulary will be 5,027; that means the array must
    be 5,027 + 1 in length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output after array slicing and separating sentences into sequences of 50
    words per sequence must look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a23d1d44-9eef-43c6-aed7-84cd516d9229.png)'
  prefs: []
  type: TYPE_IMG
- en: The `to_categorical()` function is used so that the model learns to predict
    the probability distribution for the next word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More information on reshaping arrays in Python can be found at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and saving the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can now train a statistical language model from the prepared data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model that will be trained is a neural language model. It has a few unique
    characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a distributed representation for words so that different words with
    similar meanings will have a similar representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It learns the representation at the same time as learning the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It learns to predict the probability for the next word using the context of
    the previous 50 words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifically, you will use an Embedding Layer to learn the representation of
    words, and a **Long Short-Term Memory** (**LSTM**) recurrent neural network to
    learn to predict words based on their context.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learned embedding needs to know the size of the vocabulary and the length
    of input sequences as previously discussed. It also has a parameter to specify
    how many dimensions will be used to represent each word. That is the size of the
    embedding vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Common values are 50, 100, and 300\. We will use 100 here, but consider testing
    smaller or larger values and evaluating metrics for those values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network will be comprised of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two LSTM hidden layers with 200 memory cells each. More memory cells and a deeper
    network may achieve better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dropout layer with a dropout of 0.3 or 30%, which will aid the network to
    depend less on each neuron/unit and reduce overfitting the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense fully connected layer with 200 neurons connects to the LSTM hidden layers
    to interpret the features extracted from the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output layer, which predicts the next word as a single vector of the size
    of the vocabulary with a probability for each word in the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A softmax classifier is used in the second dense or fully connected layer to
    ensure the outputs have the characteristics of normalized probabilities (such
    as between 0 and 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model is defined using the following commands and is also illustrated in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/89b30c21-36b6-4d4e-803a-eb80c3dcb512.png)'
  prefs: []
  type: TYPE_IMG
- en: Print the model summary just to ensure that the model is constructed as intended.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile the model, specifying the categorical cross entropy loss needed to
    fit the model. The number of epochs is set to 75 and the model is trained in mini
    batches with a batch size of 250\. This is done using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding commands is illustrated in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a495a034-ddc3-4bb8-a358-30d6a7f422f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the model is done compiling, it is saved using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/78e7b73d-812f-46db-8f49-10f95283d075.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model is built using the `Sequential()` function in the Keras framework.
    The first layer in the model is an embedding layer that takes in the vocabulary
    size, vector dimension, and the input sequence length as its arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next two layers are LSTM layers with 200 memory cells each. More memory
    cells and a deeper network can be experimented with to check if it improves accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next layer is a dropout layer with a dropout probability of 30%, which means
    that there is a 30% chance a certain memory unit is not used during training.
    This prevents overfitting of data. Again, the dropout probabilities can be played
    with and tuned accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final two layers are two fully connected layers. The first one has a `relu`
    activation function and the second has a softmax classifier. The model summary
    is printed to check whether the model is built according to requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice that in this case, the total number of trainable parameters are 2,115,228\.
    The model summary also shows the number of parameters that will be trained by
    each layer in the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained in mini batches of 250 over 75 epochs, in our case, to
    minimize training time. Increasing the number of epochs to over 100 and utilizing
    smaller batches while training greatly improves the model's accuracy while simultaneously
    reducing loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, you will see a summary of performance, including the loss and
    accuracy evaluated from the training data at the end of each batch update. In
    our case, after running the model for 75 epochs, we obtained an accuracy of close
    to 40%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The aim of the model is not to remember the text with 100% accuracy, but rather
    to capture the properties of the input text, such as long-term dependencies and
    structures that exist in natural language and sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model, after it is done training, is saved in the working directory named
    `junglebook_trained.h5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also require the mapping of words to integers when the model is later loaded
    into memory to make predictions. This is present in the `Tokenizer` object, which
    is also saved using the `dump ()` function in the `Pickle` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jason Brownlee''s blogs on Machine Learning Mastery have a lot of useful information
    on developing, training, and tuning machine learning models for natural language
    processing. They can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/deep-learning-for-nlp/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/lstms-with-python/](https://machinelearningmastery.com/lstms-with-python/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/blog/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Further information about different keras layers and other functions used in
    this section can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/optimizers/](https://keras.io/optimizers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keras.io/models/model/](https://keras.io/models/model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating similar text using the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a trained language model, it can be used. In this case, you
    can use it to generate new sequences of text that have the same statistical properties
    as the source text. This is not practical, at least not for this example, but
    it gives a concrete example of what the language model has learned.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by loading the training sequences again. You may do so by using the `load_document()`
    function, which we developed initially. This is done by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c88a273c-a0d6-4be1-8cbb-1b15be720983.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the input filename is now `'junglebook_sequences.txt'`, which will
    load the saved training sequences into the memory. We need the text so that we
    can choose a source sequence as input to the model for generating a new sequence
    of text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model will require 50 words as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Later, the expected length of input needs to be specified. This can be determined
    from the input sequences by calculating the length of one line of the loaded data
    and subtracting 1 for the expected output word that is also on the same line,
    as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sequence_length = len(lines[0].split()) - 1`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, load the trained and saved model into memory by executing the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step in generating text is preparing a seed input. Select a random
    line of text from the input text for this purpose. Once selected, print it so
    that you have some idea of what was used. This is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7b4a194c-a290-49eb-8e61-2d4f94174bdd.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You are now ready to generate new words, one at a time. First, encode the seed
    text to integers using the same tokenizer that was used when training the model,
    which is done using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`encoded = tokenizer.texts_to_sequences([seed_text])[0]`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/8eb637cf-8e57-41ef-a90f-bf076e3f1cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model can predict the next word directly by calling `model.predict_classes()`,
    which will return the index of the word with the highest probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Look up the index in the Tokenizers mapping to get the associated word, as
    shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Append this word to the seed text and repeat the process. Importantly, the
    input sequence is going to get too long. We can truncate it to the desired length
    after the input sequence has been encoded to integers. Keras provides the `pad_sequences()` function
    which we can use to perform this truncation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Wrap all of this into a function called `generate_sequence()` that takes as
    input the model, the tokenizer, the input sequence length, the seed text, and
    the number of words to generate. It then returns a sequence of words generated
    by the model. You may use the following code to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e7d8b604-cebd-4406-bef0-97f6aed4fe00.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to generate a sequence of new words, given that we have some
    seed text :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the model into memory again using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the tokenizer by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Select a seed text randomly by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, a new sequence is generated by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'On printing the generated sequence, you will see an output similar to the one
    shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3d641bd0-01a3-47a7-aa6b-d66f83034217.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model first prints 50 words of the random seed text followed by 50 words
    of the generated text. In this case, the random seed text is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Baskets of dried grass and put grasshoppers in them or catch two praying mantises
    and make them fight or string a necklace of red and black jungle nuts or watch
    a lizard basking on a rock or a snake hunting a frog near the wallows then they
    sing long long songs*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The 50 words of text generated by the model, in this case, are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*with odd native quavers at the end of the review and the hyaena whom he had
    seen the truth they feel twitched to the noises round him for a picture of the
    end of the ravine and snuffing bitten and best of the bulls at the dawn is a native*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note how the model outputs a sequence of random words it generated based on
    what it learned from the input text. You will also notice that the model does
    a reasonably good job of mimicking the input text and generating its own stories.
    Though the text does not make much sense, it gives valuable insight into how the
    model learns to place statistically similar words next to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Upon changing the random seed that was set, the output generated by the network
    also changes. You may not get the exact same output text as the preceding example,
    but it will be very similar to the input used to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some screenshots of different results that were obtained
    by running the generated text piece multiple times:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/18257086-668e-4abb-9f15-4abb5826d268.png)![](img/8d7ad7ba-8b9b-4bd6-9b5b-cc3b54a721a9.png)![](img/beedf83f-1eb8-47cb-972f-fdc105c84b59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model even generates its own version of the project Gutenberg license,
    as can be seen in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c954fc4c-15a4-4076-9d34-2e6512aa3827.png)'
  prefs: []
  type: TYPE_IMG
- en: The model's accuracy can be improved to about 60% by increasing the number of
    epochs from about 100 to 200\. Another method to increase the learning is by training
    the model in mini batches of about 50 and 100\. Try to play around with the different
    hyperparameters and activation functions to see what affects the results in the
    best possible way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model may also be made denser by including more LSTM and dropout layers
    while defining the model. However, know that it will only increase the training
    time if the model is more complex and runs over more epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After much experimentation, the ideal batch size was found to be between 50
    to 100, and the ideal number of epochs to train the model was determined to be
    between 100 and 200.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no definitive way of performing the preceding task. You can also experiment
    with different text inputs to the model such as tweets, customer reviews, or HTML
    code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the other tasks that can be performed include using a simplified vocabulary
    (such as with all the stopwords removed) to further enhance the unique words in
    the dictionary; tuning the size of the embedding layer and the number of memory
    cells in the hidden layers; and extending the model to use a pre-trained model
    such as Google's Word2Vec (pre-trained word model) to see whether it results in
    a better model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More information about the various functions and libraries used in the final
    section of the chapter can be found by visiting the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://keras.io/preprocessing/sequence/](https://keras.io/preprocessing/sequence/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/2/library/random.html](https://docs.python.org/2/library/random.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
