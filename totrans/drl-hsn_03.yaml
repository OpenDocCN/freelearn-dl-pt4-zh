- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Deep Learning with PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行深度学习
- en: In the previous chapter, you became familiar with open source libraries, which
    provided you with a collection of reinforcement learning (RL) environments. However,
    recent developments in RL, and especially its combination with deep learning (DL),
    now make it possible to solve much more challenging problems than ever before.
    This is partly due to the development of DL methods and tools. This chapter is
    dedicated to one such tool, PyTorch, which enables us to implement complex DL
    models with just a bunch of lines of Python code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你已经熟悉了开源库，它们为你提供了一系列强化学习（RL）环境。然而，强化学习的最新发展，特别是与深度学习（DL）结合后，使得现在可以解决比以往更具挑战性的问题。这在某种程度上归功于深度学习方法和工具的发展。本章专门介绍了其中一个工具——PyTorch，它使我们能够用少量的Python代码实现复杂的深度学习模型。
- en: 'The chapter doesn’t pretend to be a complete DL manual, as the field is very
    wide and dynamic; however, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章并不假设自己是一本完整的深度学习手册，因为这一领域非常广泛且动态；然而，我们将涵盖：
- en: The PyTorch library specifics and implementation details (assuming that you
    are already familiar with DL fundamentals)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch库的具体细节和实现方式（假设你已经熟悉深度学习的基础）
- en: Higher-level libraries on top of PyTorch, with the aim of simplifying common
    DL problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于PyTorch的高级库，旨在简化常见的深度学习问题
- en: The PyTorch Ignite library, which will be used in some examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章示例中将使用PyTorch Ignite库
- en: All of the examples in this chapter were updated for the latest(at the time
    of writing) PyTorch 2.3.1, which has changes in comparison to version 1.3.0, which
    was used in the second edition of this book. If you are using the old PyTorch,
    consider upgrading. Throughout this chapter, we will discuss the differences that
    are present in the latest version.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有示例都已更新为最新的（在写作时）PyTorch 2.3.1，相较于第二版书中使用的1.3.0版本有所变化。如果你还在使用旧版PyTorch，建议升级。在本章中，我们将讨论最新版本中的差异。
- en: Tensors
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: A tensor is the fundamental building block of all DL toolkits. The name sounds
    rather mystical, but the underlying idea is that a tensor is just a multi-dimensional
    array. Using the analogy of school math, one single number is like a point, which
    is zero-dimensional, while a vector is one-dimensional like a line segment, and
    a matrix is a two-dimensional object. Three-dimensional number collections can
    be represented by a cuboid of numbers, but they don’t have a separate name in
    the same way as a matrix. We can keep the term “tensor” for collections of higher
    dimensions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是所有深度学习工具包的基本构建块。这个名字听起来有些神秘，但其背后的基本思想是，张量只是一个多维数组。借用学校数学的类比，一个数字像一个点，是零维的；向量像一个线段，是一维的；矩阵是一个二维对象。三维的数字集合可以通过一个立方体的数字表示，但它们不像矩阵那样有一个独立的名称。我们可以保留“张量”这个术语来表示更高维度的集合。
- en: '![19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx
    aaa i,ij,,jk,k,...ii,j ](img/B22150_03_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx
    aaa i,ij,,jk,k,...ii,j ](img/B22150_03_01.png)'
- en: 'Figure 3.1: Going from a single number to an n-dimensional tensor'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：从一个数字到n维张量的转换
- en: Another thing to note about tensors used in DL is that they are only partially
    related to tensors used in tensor calculus or tensor algebra. In DL, a tensor
    is any multi-dimensional array, but in mathematics, a tensor is a mapping between
    vector spaces, which might be represented as a multi-dimensional array in some
    cases, but has much more semantical payload behind it. Mathematicians usually
    frown at anybody who uses well-established mathematical terms to name different
    things, so be warned!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习中使用的张量，还有一个需要注意的点是，它们与张量微积分或张量代数中使用的张量仅部分相关。在深度学习中，张量是任何多维数组，但在数学中，张量是向量空间之间的映射，在某些情况下可能表现为多维数组，但其背后有更丰富的语义负载。数学家通常会对那些用已建立的数学术语命名不同事物的人表示不满，因此需要警惕！
- en: The creation of tensors
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量的创建
- en: As we’ll deal with tensors everywhere in this book, we need to be familiar with
    basic operations on them, and the most basic is how to create one. There are several
    ways to do this, and your choice might influence code readability and performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中会到处使用张量，我们需要熟悉它们的基本操作，而最基本的操作就是如何创建一个张量。创建张量有几种方式，你的选择可能会影响代码的可读性和性能。
- en: 'If you are familiar with the NumPy library (and you should be), then you already
    know that its central purpose is the handling of multi-dimensional arrays in a
    generic way. Even though in NumPy, such arrays aren’t called tensors, they are,
    in fact, tensors. Tensors are used very widely in scientific computations as generic
    storage for data. For example, a color image could be encoded as a 3D tensor with
    the dimensions of width, height, and color plane. Apart from dimensions, a tensor
    is characterized by the type of its elements. There are 13 types supported by
    PyTorch:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Four float types: 16-bit, 32-bit, and 64-bit. 16-bit float has two variants:
    float16 has more bits for precision while bfloat16 has larger exponent part'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three complex types: 32-bit, 64-bit, and 128-bit'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five integer types: 8-bit signed, 8-bit unsigned, 16-bit signed, 32-bit signed,
    and 64-bit signed'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean type
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also four ”quantized number” types, but they are using the preceding
    types, just with different bit representation and interpretation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Tensors of different types are represented by different classes, with the most
    commonly used being torch.FloatTensor (corresponding to a 32-bit float), torch.ByteTensor
    (an 8-bit unsigned integer), and torch.LongTensor (a 64-bit signed integer). You
    can find names of other tensor types in the documentation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to create a tensor in PyTorch:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: By calling a constructor of the required type.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By asking PyTorch to create a tensor with specific data for you. For example,
    you can use the torch.zeros() function to create a tensor filled with zero values.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By converting a NumPy array or a Python list into a tensor. In this case, the
    type will be taken from the array’s type.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To give you examples of these methods, let’s look at a simple session:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we imported both PyTorch and NumPy and created a new float tensor tensor
    of size 3 × 2\. As you can see, PyTorch initializes memory with zeros, which is
    a different behaviour from previous versions. Before, it just allocated memory
    and kept it uninitialized, which is slightly faster but less safe (as it might
    introduce tricky bugs and security issues). But you shouldn’t rely on this behaviour,
    as it might change again (or behave differently on different hardware backends)
    and always initialize the contents of the tensor. To do so, you can either use
    one of the tensor construct operators:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or you can call the tensor modification method:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are two types of operation for tensors: inplace and functional. Inplace
    operations have an underscore appended to their name and operate on the tensor’s
    content. After this, the object itself is returned. The functional equivalent
    creates a copy of the tensor with the performed modification, leaving the original
    tensor untouched. Inplace operations are usually more efficient from a performance
    and memory point of view, but modification of an existing tensor (especially if
    it is shared in different pieces of code) might lead to hidden bugs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to create a tensor by its constructor is to provide a Python iterable
    (for example, a list or tuple), which will be used as the contents of the newly
    created tensor:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we are creating the same tensor with zeros from the NumPy array:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The torch.tensor method accepts the NumPy array as an argument and creates
    a tensor of appropriate shape from it. In the preceding example, we created a
    NumPy array initialized by zeros, which created a double (64-bit float) array
    by default. So, the resulting tensor has the DoubleTensor type (which is shown
    in the example with the dtype value). Usually, in DL, double precision is not
    required and it adds an extra memory and performance overhead. Common practice
    is to use the 32-bit float type, or even the 16-bit float type, which is more
    than enough. To create such a tensor, you need to specify explicitly the type
    of NumPy array:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As an option, the type of the desired tensor could be provided to the torch.tensor
    function in the dtype argument. However, be careful, since this argument expects
    to get a PyTorch type specification and not the NumPy one. PyTorch types are kept
    in the torch package, for example, torch.float32, torch.uint8, and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A note on compatibility
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The torch.tensor() method and explicit PyTorch type specification were added
    in the 0.4.0 release, and this is a step toward the simplification of tensor creation.
    In previous versions, the torch.from_numpy() function was a recommended way to
    convert NumPy arrays, but it had issues with handling the combination of the Python
    list and NumPy arrays. This from_numpy() function is still present for backward
    compatibility, but it is deprecated in favor of the more flexible torch.tensor()
    method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Scalar tensors
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the 0.4.0 release, PyTorch has supported zero-dimensional tensors that
    correspond to scalar values (on the left of Figure [3.1](#x1-54002r1)). Such tensors
    can be the result of some operations, such as summing all values in a tensor.
    Previously, such cases were handled by the creation of a one-dimensional tensor
    (also known as vector) with a single dimension equal to one.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'This solution worked, but it wasn’t very simple, as extra indexation was needed
    to access the value. Now, zero-dimensional tensors are natively supported and
    returned by the appropriate functions, and they can be created by the torch.tensor()
    function. To access the actual Python value of such a tensor, we can use the special
    item() method:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tensor operations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are lots of operations that you can perform on tensors, and there are
    too many to list them all. Usually, it’s enough to search in the PyTorch documentation
    at [http://pytorch.org/docs/](http://pytorch.org/docs/). I need to mention that
    there are two places to look for operations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'The torch package: The function usually accepts the tensor as an argument.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tensor class: The function operates on the called tensor.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, tensor operations in PyTorch are trying to correspond to their
    NumPy equivalent, so if there is some not-very-specialized function in NumPy,
    then there is a good chance that PyTorch will also have it. Examples are torch.stack(),
    torch.transpose(), and torch.cat(). This is very convenient, as NumPy is a very
    widely used library (especially in the scientific community), so your PyTorch
    code becomes readable by anyone familiar with NumPy without looking into the documentation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，PyTorch 中的张量操作都是试图与其 NumPy 对应的功能相匹配，因此，如果 NumPy 中有一些不太特殊的函数，那么很有可能 PyTorch
    也会有类似的函数。比如 torch.stack()、torch.transpose() 和 torch.cat()。这非常方便，因为 NumPy 是一个广泛使用的库（尤其在科学界），因此你的
    PyTorch 代码可以被任何熟悉 NumPy 的人读取，而无需查阅文档。
- en: GPU tensors
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 张量
- en: PyTorch transparently supports CUDA GPUs, which means that all operations have
    two versions — CPU and GPU — that are automatically selected. The decision is
    made based on the type of tensors that you are operating on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 透明地支持 CUDA GPU，这意味着所有操作都有两个版本——CPU 和 GPU——并且会自动选择。这个选择是基于你正在操作的张量类型来决定的。
- en: Every tensor type that I mentioned is for CPU and has its GPU equivalent. The
    only difference is that GPU tensors reside in the torch.cuda package, instead
    of just torch. For example, torch.FloatTensor is a 32-bit float tensor that resides
    in CPU memory, but torch.cuda.FloatTensor is its GPU counterpart.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到的每种张量类型都是针对 CPU 的，并且都有其 GPU 对应版本。唯一的区别是，GPU 张量位于 torch.cuda 包中，而不是仅仅在 torch
    中。例如，torch.FloatTensor 是一个 32 位浮动张量，驻留在 CPU 内存中，但 torch.cuda.FloatTensor 是它的 GPU
    对应张量。
- en: In fact, under the hood, PyTorch supports not just CPU and CUDA; it has a notion
    of backend, which is an abstract computation device with memory. Tensors could
    be allocated in the backend’s memory and computations could be performed on them.
    For example, on Apple hardware, PyTorch supports Metal Performance Shaders (MPS)
    as a backend called mps. In this chapter, we focus on CPU and GPU as the mostly
    used backends, but your PyTorch code could be executed on much more fancier hardware
    without major modifications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在 PyTorch 的底层，不仅支持 CPU 和 CUDA，还引入了后端的概念，这是一种带有内存的抽象计算设备。张量可以分配到后端的内存中，并且可以在其上进行计算。例如，在苹果硬件上，PyTorch
    支持作为名为 mps 的后端的 Metal 性能着色器（MPS）。在本章中，我们将重点讨论 CPU 和 GPU 作为最常用的后端，但你的 PyTorch 代码也可以在更高级的硬件上执行，而无需做重大修改。
- en: To convert from CPU to GPU, there is a tensor method, to(device), that creates
    a copy of the tensor to a specified device (this could be CPU or GPU). If the
    tensor is already on the device, nothing happens and the original tensor will
    be returned. The device type can be specified in different ways. First of all,
    you can just pass a string name of the device, which is "cpu" for CPU memory or
    "cuda" for GPU. A GPU device could have an optional device index specified after
    the colon; for example, the second GPU card in the system could be addressed by
    "cuda:1" (the index is zero-based).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 CPU 转换到 GPU，可以使用张量方法 to(device)，该方法会将张量的副本创建到指定的设备（可以是 CPU 或 GPU）。如果张量已经在该设备上，则什么也不发生，原始张量将被返回。设备类型可以通过不同方式指定。首先，你可以直接传递设备的字符串名称，对于
    CPU 内存是 "cpu"，对于 GPU 是 "cuda"。GPU 设备可以在冒号后面指定一个可选的设备索引；例如，系统中的第二张 GPU 卡可以通过 "cuda:1"
    来表示（索引是从零开始的）。
- en: 'Another slightly more efficient way to specify a device in the to() method
    is by using the torch.device class, which accepts the device name and optional
    index. To access the device that your tensor is currently residing in, it has
    a device property:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 to() 方法中，指定设备的另一种略微更高效的方式是使用 torch.device 类，它接受设备名称和可选的索引。要访问张量当前所在的设备，可以使用设备属性：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we created a tensor on the CPU, then copied it to GPU memory. Both copies
    can be used in computations, and all GPU-specific machinery is transparent to
    the user:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个位于 CPU 上的张量，然后将其复制到 GPU 内存中。两个副本都可以用于计算，并且所有与 GPU 相关的机制对用户是透明的：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The to() method and torch.device class were introduced in 0.4.0\. In previous
    versions, copying between CPU and GPU was performed by separate tensor methods,
    cpu() and cuda(), respectively, which required adding the extra lines of code
    to explicitly convert tensors into their CUDA versions. In newer PyTorch versions,
    you can create a desired torch.device object at the beginning of the program and
    use to(device) on every tensor that you’re creating. The old methods in the tensor,
    cpu() and cuda(), are still present and might be handy if you want to ensure that
    a tensor is in CPU or GPU memory regardless of its original location.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`to()` 方法和 `torch.device` 类在 0.4.0 版本中引入。在早期版本中，CPU 和 GPU 之间的复制是通过单独的张量方法 `cpu()`
    和 `cuda()` 来完成的，这需要添加额外的代码行来显式地将张量转换为它们的 CUDA 版本。在新的 PyTorch 版本中，你可以在程序开始时创建一个所需的
    `torch.device` 对象，并在每个创建的张量上使用 `to(device)`。旧的张量方法 `cpu()` 和 `cuda()` 仍然存在，并且如果你希望确保张量在
    CPU 或 GPU 内存中，不管它原来的位置在哪里，它们仍然可能会派上用场。'
- en: Gradients
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度
- en: Even with transparent GPU support, all of this dancing with tensors isn’t worth
    bothering without one “killer feature” — the automatic computation of gradients.
    This functionality was originally implemented in the Caffe toolkit and then became
    the de facto standard in DL libraries.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有透明的 GPU 支持，所有这些与张量的“跳舞”也毫无意义，除非有一个“杀手级功能” —— 自动计算梯度。这个功能最早在 Caffe 工具包中实现，后来成为了深度学习库中的事实标准。
- en: Earlier, computing gradients manually was extremely painful to implement and
    debug, even for the simplest neural network (NN). You had to calculate derivatives
    for all your functions, apply the chain rule, and then implement the result of
    the calculations, praying that everything was done right. This could be a very
    useful exercise for understanding the nuts and bolts of DL, but it wasn’t something
    that you wanted to repeat over and over again by experimenting with different
    NN architectures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，手动计算梯度是一个非常痛苦的过程，甚至对于最简单的神经网络（NN）来说也是如此。你需要为所有的函数计算导数，应用链式法则，然后实现计算结果，祈祷一切都能正确完成。这可能是理解深度学习核心机制的一个有用练习，但它绝对不是你愿意通过不断尝试不同的神经网络架构来反复做的事。
- en: Luckily, those days have gone now, much like programming your hardware using
    a soldering iron and vacuum tubes! Now, defining an NN of hundreds of layers requires
    nothing more than assembling it from predefined building blocks or, in the extreme
    case of you doing something fancy, defining the transformation expression manually.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，那些日子已经过去了，就像用烙铁和真空管编程硬件一样！现在，定义一个有数百层的神经网络，仅需要将它从预定义的构建块中组装起来，或者在你做一些特别的事情时，手动定义变换表达式。
- en: 'All gradients will be carefully calculated for you, backpropagated, and applied
    to the network. To be able to achieve this, you need to define your network architecture
    using DL library primitives. In Figure [3.2](#x1-59002r2), I have outlined the
    direction of the data and gradients flow during the optimization process:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的梯度将会被仔细计算、反向传播，并应用到网络中。为了实现这一点，你需要使用深度学习库的基本组件来定义你的网络架构。在图 [3.2](#x1-59002r2)
    中，我概述了数据和梯度在优化过程中的流动方向：
- en: '![IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts ](img/B22150_03_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts ](img/B22150_03_02.png)'
- en: 'Figure 3.2: Data and gradients flowing through the NN'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：数据和梯度流经神经网络
- en: 'What can make a fundamental difference is how your gradients are calculated.
    There are two approaches:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 产生根本性差异的因素可能是你计算梯度的方式。这里有两种方法：
- en: 'Static graph: In this method, you need to define your calculations in advance
    and it won’t be possible to change them later. The graph will be processed and
    optimized by the DL library before any computation is made. This model is implemented
    in TensorFlow (versions before 2.0), Theano, and many other DL toolkits.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态图：在这种方法中，你需要提前定义你的计算过程，并且之后无法更改它们。图形将在任何计算执行之前由深度学习库处理和优化。这种模型在 TensorFlow（2.0
    之前的版本）、Theano 和许多其他深度学习工具包中实现。
- en: 'Dynamic graph: You don’t need to define your graph in advance exactly as it
    will be executed; you just need to execute operations that you want to use for
    data transformation on your actual data. During this, the library will record
    the order of the operations performed, and when you ask it to calculate gradients,
    it will unroll its history of operations, accumulating the gradients of the network
    parameters. This method is also called notebook gradients and it is implemented
    in PyTorch, Chainer, and some others.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态图：你不需要提前精确定义你的图形如何执行；你只需要在实际数据上执行你希望用于数据转换的操作。在此过程中，库会记录执行操作的顺序，当你要求它计算梯度时，它会展开其操作历史，累积网络参数的梯度。这个方法也叫做笔记本梯度，它在
    PyTorch、Chainer 和其他一些框架中得到了实现。
- en: Both methods have their strengths and weaknesses. For example, a static graph
    is usually faster, as all computations can be moved to the GPU, minimizing the
    data transfer overhead. Additionally, in a static graph, the library has much
    more freedom in optimizing the order that computations are performed in or even
    removing parts of the graph.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法各有优缺点。例如，静态图通常更快，因为所有计算可以移动到 GPU 上，从而最小化数据传输开销。此外，在静态图中，库在优化计算顺序，甚至删除图形的一部分时，拥有更多的自由度。
- en: On the other hand, although a dynamic graph has a higher computation overhead,
    it gives a developer much more freedom. For example, they can say, “For this piece
    of data, I can apply this network two times, and for this piece of data, I’ll
    use a completely different model with gradients clipped by the batch mean”. Another
    very appealing strength of the dynamic graph model is that it allows you to express
    your transformation more naturally and in a more “Pythonic” way. In the end, it’s
    just a Python library with a bunch of functions, so just call them and let the
    library do the magic.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管动态图具有更高的计算开销，但它为开发者提供了更多的自由度。例如，开发者可以说，“对于这一块数据，我可以应用这个网络两次，而对于另一块数据，我会使用完全不同的模型，并且对梯度进行批均值裁剪”。动态图模型的另一个非常吸引人的优点是，它允许你以更自然、更“Pythonic”的方式表达转换。最终，这不过是一个包含一堆函数的
    Python 库，所以只需要调用它们，让库来完成魔法。
- en: Since version 2.0, PyTorch introduced the torch.compile function, which speeds
    up PyTorch code by JIT-compiling the code into optimized kernels. This is an evolution
    of the TorchScript and FX Tracing compiling methods from earlier versions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2.0 版本以来，PyTorch 引入了 torch.compile 函数，通过 JIT 编译将代码转化为优化后的内核，从而加速 PyTorch 代码的执行。这是早期版本中
    TorchScript 和 FX Tracing 编译方法的演变。
- en: From a historical perspective, this is highly amusing how originally radically
    different approaches of TensorFlow (static graph) and PyTorch (dynamic graph)
    are fusing into each other over time. Nowadays, PyTorch supports compile() and
    TensorFlow has “eager execution mode”.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史角度来看，这非常有趣，最初完全不同的 TensorFlow（静态图）和 PyTorch（动态图）方法如何随着时间推移逐渐融合在一起。如今，PyTorch
    支持 compile()，而 TensorFlow 则有了“急切执行模式”。
- en: Tensors and gradients
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量与梯度
- en: PyTorch tensors have a built-in gradient calculation and tracking machinery,
    so all you need to do is convert the data into tensors and perform computations
    using the tensor methods and functions provided by torch. Of course, if you need
    to access underlying low-level details, you always can, but most of the time,
    PyTorch does what you’re expecting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量具有内建的梯度计算和跟踪机制，所以你只需要将数据转换为张量，并使用 torch 提供的张量方法和函数进行计算。当然，如果你需要访问底层的细节，也可以，但大多数情况下，PyTorch
    会按照你的预期工作。
- en: 'There are several attributes related to gradients that every tensor has:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个张量都有几个与梯度相关的属性：
- en: 'grad: A property that holds a tensor of the same shape containing computed
    gradients.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad：一个属性，保存一个形状相同的张量，包含计算出的梯度。
- en: 'is_leaf: Equals True if this tensor was constructed by the user and False if
    the object is a result of function transformation (in other words, have a parent
    in the computation graph).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: is_leaf：如果该张量是用户构造的，则为 True；如果该对象是函数转换的结果（换句话说，计算图中有父节点），则为 False。
- en: 'requires_grad: Equals True if this tensor requires gradients to be calculated.
    This property is inherited from leaf tensors, which get this value from the tensor
    construction step (torch.zeros() or torch.tensor() and so on). By default, the
    constructor has requires_grad=False, so if you want gradients to be calculated
    for your tensor, then you need to explicitly say so.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make all of this gradient-leaf machinery clearer, let’s consider this session:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we created two tensors. The first requires gradients to be calculated
    and the second doesn’t.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have added both vectors element-wise (which is vector [3, 3]), doubled
    every element, and summed them together:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result is a zero-dimensional tensor with the value 12\. Okay, so far this
    is just a simple math. Now let’s look at the underlying graph that our expressions
    created:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![vv+v×Σv2 12sruesm ](img/B22150_03_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Graph representation of the expression'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check the attributes of our tensors, then we will find that v1 and v2
    are the only leaf nodes and every variable, except v2, requires gradients to be
    calculated:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, the property requires_grad is sort of “sticky”: if one of the
    variables involved in computations has it set to True, all subsequent nodes also
    have it. This is logical behaviour, as we normally need gradients to be calculated
    for all intermediate steps in our computation. But “calculation” doesn’t mean
    they will be preserved in the .grad field. For memory efficiency, gradients are
    stored only for leaf nodes with requires_grad=True. If you want to keep gradients
    in the non-leaf nodes, you need to call their retain_grad() method, which tells
    PyTorch to keep the gradients for non-leaf node.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s tell PyTorch to calculate the gradients of our graph:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By calling the backward function, we asked PyTorch to calculate the numerical
    derivative of the v_res variable with respect to any variable that our graph has.
    In other words, what influence do small changes to the v_res variable have on
    the rest of the graph? In our particular example, the value of 2 in the gradients
    of v1 means that by increasing any element of v1 by one, the resulting value of
    v_res will grow by two.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, PyTorch calculates gradients only for leaf tensors with requires_grad=True.
    Indeed, if we try to check the gradients of v2, we get nothing:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The reason for that is efficiency in terms of computations and memory. In real
    life, our network can have millions of optimized parameters, with hundreds of
    intermediate operations performed on them. During gradient descent optimization,
    we are not interested in gradients of any intermediate matrix multiplication;
    the only things we want to adjust in the model are gradients of loss with respect
    to model parameters (weights). Of course, if you want to calculate the gradients
    of input data (it could be useful if you want to generate some adversarial examples
    to fool the existing NN or adjust pretrained word embeddings), then you can easily
    do so by passing requires_grad=True on tensor creation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Basically, you now have everything needed to implement your own NN optimizer.
    The rest of this chapter is about extra, convenient functions, which will provide
    you with higher-level building blocks of NN architectures, popular optimization
    algorithms, and common loss functions. However, don’t forget that you can easily
    reimplement all of these bells and whistles in any way that you like. This is
    why PyTorch is so popular among DL researchers — for its elegance and flexibility.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Compatibility
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Support of gradients calculation in tensors is one of the major changes in PyTorch
    0.4.0\. In previous versions, graph tracking and gradients accumulation were done
    in a separate, very thin class, Variable. This worked as a wrapper around the
    tensor and automatically saved the history of computations in order to be able
    to backpropagate. This class is still present in 2.2.0 (available in torch.autograd),
    but it is deprecated and will go away soon, so new code should avoid using it.
    From my perspective, this change is great, as the Variable logic was really thin,
    but it still required extra code and the developer’s attention to wrap and unwrap
    tensors. Now, gradients are a built-in tensor property, which makes the API much
    cleaner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: NN building blocks
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the torch.nn package, you will find tons of predefined classes providing
    you with the basic functionality blocks. All of them are designed with practice
    in mind (for example, they support mini-batches, they have sane default values,
    and the weights are properly initialized). All modules follow the convention of
    callable, which means that the instance of any class can act as a function when
    applied to its arguments. For example, the Linear class implements a feed-forward
    layer with optional bias:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we created a randomly initialized feed-forward layer, with two inputs
    and five outputs, and applied it to our float tensor. All classes in the torch.nn
    packages inherit from the nn.Module base class, which you can use to implement
    your own higher-level NN blocks. You will see how you can do this in the next
    section, but, for now, let’s look at useful methods that all nn.Module children
    provide. They are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'parameters(): This function returns an iterator of all variables that require
    gradient computation (that is, module weights).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'zero_grad(): This function initializes all gradients of all parameters to zero.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'to(device): This function moves all module parameters to a given device (CPU
    or GPU).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'state_dict(): This function returns the dictionary with all module parameters
    and is useful for model serialization.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'load_state_dict(): This function initializes the module with the state dictionary.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole list of available classes can be found in the documentation at [http://pytorch.org/docs](http://pytorch.org/docs).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I should mention one very convenient class that allows you to combine
    other layers into the pipe: Sequential. The best way to demonstrate Sequential
    is through an example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we defined a three-layer NN with softmax on output, applied along dimension
    1 (dimension 0 is batch samples), rectified linear unit (ReLU) nonlinearities,
    and dropout. Let’s push something through it:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, our mini-batch of one vector successfully traversed through the network!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Custom layers
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, I briefly mentioned the nn.Module class as a base parent
    for all NN building blocks exposed by PyTorch. It’s not just a unifying parent
    for the existing layers — it’s much more than that. By subclassing the nn.Module
    class, you can create your own building blocks, which can be stacked together,
    reused later, and integrated into the PyTorch framework flawlessly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, the nn.Module provides quite rich functionality to its children:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: It tracks all submodules that the current module includes. For example, your
    building block can have two feed-forward layers used somehow to perform the block’s
    transformation. To keep track of (register) the submodule, you just need to assign
    it to the class’s field.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides functions to deal with all parameters of the registered submodules.
    You can obtain a full list of the module’s parameters (parameters() method), zero
    its gradients (zero_grads() method), move to CPU or GPU (to(device) method), serialize
    and deserialize the module (state_dict() and load_state_dict()), and even perform
    generic transformations using your own callable (apply() method).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It establishes the convention of Module application to data. Every module needs
    to perform its data transformation in the forward() method by overriding it.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some more functions, such as the ability to register a hook function
    to tweak module transformation or gradients flow, but they are more for advanced
    use cases.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These functionalities allow us to nest our submodels into higher-level models
    in a unified way, which is extremely useful when dealing with complexity. It could
    be a simple one-layer linear transformation or a 1001-layer residual NN (ResNet)
    monster, but if they follow the conventions of nn.Module, then both of them could
    be handled in the same way. This is very handy for code reusability and simplification
    (by hiding non-relevant implementation details).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: To make our life simpler, when following the above conventions, PyTorch authors
    simplified the creation of modules through careful design and a good dose of Python
    magic. So, to create a custom module, we usually have to do only two things —
    register submodules and implement the forward() method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how this can be done for our Sequential example from the previous
    section, but in a more generic and reusable way (the full sample is Chapter03/01_modules.py).
    The following is our module class that inherits nn.Module:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the constructor, we pass three parameters: the input size, the output size,
    and the optional dropout probability. The first thing we need to do is call the
    parent’s constructor to let it initialize itself.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In the second step in the preceding code, we create an already familiar nn.Sequential
    with a bunch of layers and assign it to our class field named pipe. By assigning
    a Sequential instance to our object’s field, we will automatically register this
    module (nn.Sequential inherits from nn.Module, as does everything in the nn package).
    To register it, we don’t need to call anything, we just need to assign our submodules
    to fields. After the constructor finishes, all those fields will be registered
    automatically. If you really want to, there is a function in nn.Module to register
    submodules called add_module(). It might be useful if your module can have variable
    number of layers and they need to be created programmatically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must override the forward function with our implementation of data
    transformation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As our module is a very simple wrapper around the Sequential class, we just
    need to ask self.pipe to transform the data. Note that to apply a module to the
    data, we need to call the module as a callable (that is, pretend that the module
    instance is a function and call it with the arguments) and not use the forward()
    function of the nn.Module class. This is because nn.Module overrides the __call__()
    method, which is being used when we treat an instance as callable. This method
    does some nn.Module magic and calls our forward() method. If we call forward()
    directly, we will intervene with the nn.Module duty, which can give wrong results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'So, that’s what we need to do to define our own module. Now, let’s use it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create our module, providing it with the desired number of inputs and outputs,
    then we create a tensor and ask our module to transform it, following the same
    convention of using it as callable. After that, we print our network’s structure
    (nn.Module overrides __str__() and __repr__()) to represent the inner structure
    in a nice way. The last thing we show is the result of the network’s transformation.
    The output of our code should look like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Of course, everything that was said about the dynamic nature of PyTorch is still
    true. The forward() method is called for every batch of data, so if you want to
    do some complex transformations based on the data you need to process, like hierarchical
    softmax or a random choice of network to apply, then nothing can stop you from
    doing so. The count of arguments to your module is also not limited by one parameter.
    So, if you want, you can write a module with multiple required parameters and
    dozens of optional arguments, and it will be fine.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to get familiar with two important pieces of the PyTorch library
    that will simplify our lives: loss functions and optimizers.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions and optimizers
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network that transforms input data into output is not the only thing we
    need for training. We also define our learning objective, which has to be a function
    that accepts two arguments — the network’s output and the desired output. Its
    responsibility is to return to us a single number — how close the network’s prediction
    is from the desired result. This function is called the loss function, and its
    output is the loss value. Using the loss value, we calculate gradients of network
    parameters and adjust them to decrease this loss value, which pushes our model
    to better results in the future. Both the loss function and the method of tweaking
    a network’s parameters by gradient are so common and exist in so many forms that
    both of them form a significant part of the PyTorch library. Let’s start with
    loss functions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loss functions reside in the nn package and are implemented as an nn.Module
    subclass. Usually, they accept two arguments: output from the network (prediction)
    and desired output (ground-truth data, which is also called the label of the data
    sample). At the time of writing, PyTorch 2.3.1 contains over 20 different loss
    functions and, of course, nothing stops you from writing any custom function you
    want to optimize.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used standard loss functions are:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'nn.MSELoss: The mean square error between arguments, which is the standard
    loss for regression problems.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'nn.BCELoss and nn.BCEWithLogits: Binary cross-entropy loss. The first version
    expects a single probability value (usually it’s the output of the Sigmoid layer),
    while the second version assumes raw scores as input and applies Sigmoid itself.
    The second way is usually more numerically stable and efficient. These losses
    (as their names suggest) are frequently used in binary classification problems.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'nn.CrossEntropyLoss and nn.NLLLoss: Famous “maximum likelihood” criteria that
    are used in multi-class classification problems. The first version expects raw
    scores for each class and applies LogSoftmax internally, while the second expects
    to have log probabilities as the input.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other loss functions available and you are always free to write your
    own Module subclass to compare the output and target. Now, let’s look at the second
    piece of the optimization process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The responsibility of the basic optimizer is to take the gradients of model
    parameters and change these parameters in order to decrease the loss value. By
    decreasing the loss value, we are pushing our model toward the desired output,
    which can give us hope for better model performance in the future. Changing parameters
    may sound simple, but there are lots of details here and the optimizer procedure
    is still a hot research topic. In the torch.optim package, PyTorch provides lots
    of popular optimizer implementations, and the most widely known are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD: A vanilla stochastic gradient descent algorithm with an optional momentum
    extension'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMSprop: An optimizer proposed by Geoffrey Hinton'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adagrad: An adaptive gradients optimizer'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adam: A quite successful and popular combination of both RMSprop and Adagrad'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All optimizers expose the unified interface, which makes it easy to experiment
    with different optimization methods (sometimes the optimization method can really
    make a difference in convergence dynamics and the final result). On construction,
    you need to pass an iterable of tensors, which will be modified during the optimization
    process. The usual practice is to pass the result of the params() call of the
    upper-level nn.Module instance, which will return an iterable of all leaf tensors
    with gradients.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s discuss the common blueprint of a training loop:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Usually, you iterate over your data over and over again (one iteration over
    a full set of examples is called an epoch). Data is usually too large to fit into
    CPU or GPU memory at once, so it is split into batches of equal size. Every batch
    includes data samples and target labels, and both of them have to be tensors (lines
    2 and 3).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: You pass data samples to your network (line 4) and feed its output and target
    labels to the loss function (line 5). The result of the loss function shows the
    “badness” of the network result relative to the target labels. As input to the
    network and the network’s weights are tensors, all transformations of your network
    are nothing more than a graph of operations with intermediate tensor instances.
    The same is true for the loss function — its result is also a tensor of one single
    loss value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Every tensor in this computation graph remembers its parent, so to calculate
    gradients for the whole network, all you need to do is call the backward() function
    on a loss function result (line 6). The result of this call will be the unrolling
    of the graph of the performed computations and the calculating of gradients for
    every leaf tensor with require_grad=True. Usually, such tensors are our model’s
    parameters, such as the weights and biases of feed-forward networks, and convolution
    filters. Every time a gradient is calculated, it is accumulated in the tensor.grad
    field, so one tensor can participate in a transformation multiple times and its
    gradients will be properly summed together. For example, one single recurrent
    neural network (RNN) cell could be applied to multiple input items.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: After the loss.backward() call is finished, we have the gradients accumulated,
    and now it’s time for the optimizer to do its job — it takes all gradients from
    the parameters we have passed to it on construction and applies them. All this
    is done with the method step() (line 7).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The last, but not least, piece of the training loop is our responsibility to
    zero gradients of parameters. This can be done by calling zero_grad() on our network,
    but, for our convenience, the optimizer also exposes such a call, which does the
    same thing (line 8). Sometimes, zero_grad() is placed at the beginning of the
    training loop, but it doesn’t matter much.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The preceding scheme is a very flexible way to perform optimization and it can
    fulfill the requirements even in sophisticated research. For example, you can
    have two optimizers tweaking the options of different models on the same data
    (and this is a real-life scenario from generative adversarial network (GAN) training).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: So, we are done with the essential functionality of PyTorch required to train
    NNs. This chapter ends with a practical medium-size example to demonstrate all
    the concepts covered, but before we get to it, we need to discuss one important
    topic that is essential for an NN practitioner — monitoring the learning process.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with TensorBoard
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have ever tried to train an NN on your own, then you will know how painful
    and uncertain it can be. I’m not talking about following the existing tutorials
    and demos, when all the hyperparameters are already tuned for you, but about taking
    some data and creating something from scratch. Even with modern DL high-level
    toolkits, where all best practices, such as proper weights initialization; optimizers’
    betas, gammas, and other options set to sane defaults; and tons of other stuff
    hidden under the hood, there are still lots of decisions that you have to make,
    hence lots of things that could go wrong. As a result, your code almost never
    works from the first run, and this is something that you should get used to.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Of course, with practice and experience, you will develop a strong understanding
    of the possible causes of problems, but this needs input data about what’s going
    on inside your network. So, you need to be able to peek inside your training process
    somehow and observe its dynamics. Even small networks (such as tiny MNIST tutorial
    networks) could have hundreds of thousands of parameters with quite nonlinear
    training dynamics.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'DL practitioners have developed a list of things that you should observe during
    your training, which usually includes the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Loss value, which normally consists of several components like base loss and
    regularization losses. You should monitor both the total loss and the individual
    components over time.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results of validation on training and test datasets.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics about gradients and weights.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values produced by the network. For example, if you are solving a classification
    problem, you definitely want to measure the entropy of predicted class probabilities.
    In the case of a regression problem, raw predicted values can give tons of data
    about the training.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rates and other hyperparameters, if they are adjusted over time.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list could be much longer and include domain-specific metrics, such as word
    embedding projections, audio samples, and images generated by GANs. You also may
    want to monitor values related to training speed, like how long an epoch takes,
    to see the effect of your optimizations or problems with hardware.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: To cut a long story short, you need a generic solution to track lots of values
    over time and represent them for analysis, preferably developed especially for
    DL (just imagine looking at such statistics using an Excel spreadsheet). Luckily,
    such tools exist, and we will explore them next.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard 101
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the first edition of this book was written, there wasn’t too much choice
    for NN monitoring. As time has passed by and new people and companies have become
    involved with the pursuit of ML and DL, more new tools have appeared, for example,
    MLflow [https://mlflow.org/](https://mlflow.org/). In this book, we will still
    focus on the TensorBoard utility from TensorFlow, but you might consider trying
    other alternatives.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first public version, TensorFlow included a special tool called TensorBoard,
    which was developed to solve the problem we are talking about — how to observe
    and analyze various NN characteristics during and after the training. TensorBoard
    is a powerful, generic solution with a large community and it looks quite pretty:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The TensorBoard web interface (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: From the architecture point of view, TensorBoard is a Python web service that
    you can start on your computer, passing it the directory where your training process
    will save values to be analyzed. Then, you point your browser to TensorBoard’s
    port (usually 6006), and it shows you an interactive web interface with values
    updated in real time, as shown in Figure [3.4](#x1-67002r4). It’s nice and convenient,
    especially when your training is performed on a remote machine somewhere in the
    cloud.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Originally, TensorBoard was deployed as a part of TensorFlow, but after some
    time, it has been moved to a separate project (it’s still being maintained by
    Google) and it has its own package name. However, TensorBoard still uses the TensorFlow
    data format, so we will need to write this data from our PyTorch program. Several
    years ago, it required third-party libraries to be installed, but nowadays, PyTorch
    already comes with support of this data format (available in the torch.utils.tensorboard
    package).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Plotting metrics
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give you an impression of how simple is to use TensorBoard, let’s consider
    a small example that is not related to NNs, but is just about writing values into
    TensorBoard (the full example code is in Chapter03/02_tensorboard.py).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we import the required packages, create a writer of
    data, and define functions that we are going to visualize:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By default, SummaryWriter will create a unique directory in the runs directory
    for every launch, to be able to compare different rounds of training. The name
    of the new directory includes the current date and time, and the hostname. To
    override this, you can pass the log_dir argument to SummaryWriter. You can also
    add a suffix to the name of the directory by passing a comment argument, for example,
    to capture different experiments’ semantics, such as dropout=0.3 or strong_regularisation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we loop over angle ranges in degrees:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we convert the angle ranges into radians and calculate our functions’
    values. Every value is added to the writer using the add_scalar function, which
    takes three arguments: the name of the parameter, its value, and the current iteration
    (which has to be an integer). The last thing we need to do after the loop is close
    the writer. Note that the writer does a periodical flush (by default, every two
    minutes), so even in the case of a lengthy optimization process, you will still
    see your values. If you need to flush SummaryWriter data explicitly, it has the
    flush() method.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of running this will be zero output on the console, but you will
    see a new directory created inside the runs directory with a single file. To look
    at the result, we need to start TensorBoard:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you are running TensorBoard on a remote server, you will need to add the
    --bind_all command-line option to make it accessible from other machines. Now
    you can open http://localhost:6006 in your browser to see something like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file10.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Plots produced by the example (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The graphs are interactive, so you can hover over them with your mouse to see
    the actual values and select regions to zoom in and look at details. To zoom out,
    double-click inside the graph. If you run your program several times, then you
    will see several items in the Runs list on the left, which can be enabled and
    disabled in any combination, allowing you to compare the dynamics of several optimizations.
    TensorBoard allows you to analyze not only scalar values but also images, audio,
    text data, and embeddings, and it can even show you the structure of your network.
    Refer to the documentation of TensorBoard for all those features. Now, it’s time
    to combine everything you learned in this chapter and look at a real NN optimization
    problem using PyTorch.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: GAN on Atari images
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost every book about DL uses the MNIST dataset to show you the power of DL,
    which, over the years, has made this dataset extremely boring, like a fruit fly
    for genetic researchers. To break this tradition, and add a bit more fun to the
    book, I’ve tried to avoid well-beaten paths and illustrate PyTorch using something
    different. I briefly referred to generative adversarial networks (GANs) earlier
    in the chapter. In this example, we will train a GAN to generate screenshots of
    various Atari games.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest GAN architecture is this: we have two NNs where the first works
    as a ”cheater” (it is also called the generator), and the other as a ”detective”
    (another name is the discriminator). Both networks compete with each other — the
    generator tries to generate fake data, which will be hard for the discriminator
    to distinguish from your dataset, and the discriminator tries to detect the generated
    data samples. Over time, both networks improve their skills — the generator produces
    more and more realistic data samples, and the discriminator invents more sophisticated
    ways to distinguish the fake items.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Practical usage of GANs includes image quality improvement, realistic image
    generation, and feature learning. In our example, practical usefulness is almost
    zero, but it will be a good showcase about everything we learned about PyTorch
    so far.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s get started. The whole example code is in the file Chapter03/03_atari_gan.py.
    Here, we will look at only the most significant pieces of code, without the import
    section and constants declaration. The following class is a wrapper around a Gym
    game:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding class includes several transformations:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Resize the input image from 210×160 (the standard Atari resolution) to a square
    size of 64 × 64
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move the color plane of the image from the last position to the first, to meet
    the PyTorch convention of convolution layers that input a tensor with the shape
    of the channels, height, and width
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the image from bytes to float
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we define two nn.Module classes: Discriminator and Generator. The first
    takes our scaled color image as input and, by applying five layers of convolutions,
    converts it into a single number passed through a Sigmoid nonlinearity. The output
    from Sigmoid is interpreted as the probability that Discriminator thinks our input
    image is from the real dataset.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Generator takes as input a vector of random numbers (latent vector) and, by
    using the “transposed convolution” operation (it is also known as deconvolution),
    converts this vector into a color image of the original resolution. We will not
    look at those classes here as they are lengthy and not very relevant to our example;
    you can find them in the complete example file.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: As input, we will use screenshots from several Atari games played simultaneously
    by a random agent. Figure [3.6](#x1-69029r6) is an example of what the input data
    looks like.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file11.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Sample screenshots from three Atari games'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are combined in batches that are generated by the following function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This function infinitely samples the environment from the provided list, issues
    random actions, and remembers observations in the batch list. When the batch becomes
    of the required size, we normalize the image, convert it to a tensor, and yield
    from the generator. The check for the non-zero mean of the observation is required
    due to a bug in one of the games to prevent the flickering of an image.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at our main function, which prepares models and runs the training
    loop:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we process the command-line arguments (which could be only one optional
    argument, --dev, which specifies the device to use for computations) and create
    our environment pool with a wrapper applied. This environment array will be passed
    to the iterate_batches function later to generate training data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following piece, we create our classes — a summary writer, both networks,
    a loss function, and two optimizers:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Why do we need two optimizers? It’s because that’s the way that GANs get trained:
    to train the discriminator, we need to show it both real and fake data samples
    with appropriate labels (1 for real and 0 for fake). During this pass, we update
    only the discriminator’s parameters.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: After that, we pass both real and fake samples through the discriminator again,
    but this time, the labels are 1s for all samples and we update only the generator’s
    weights. The second pass teaches the generator how to fool the discriminator and
    confuse real samples with the generated ones.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define arrays, which will be used to accumulate losses, iterator counters,
    and variables with the true and fake labels. We also store the current timestamp
    to report the time elapsed after 100 iterations of training:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At the beginning of the following training loop, we generate a random vector
    and pass it to the Generator network:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then train the discriminator by applying it two times, once to the true
    data samples in our batch and once to the generated ones:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, we need to call the detach() function on the generator’s
    output to prevent gradients of this training pass from flowing into the generator
    (detach() is a method of tensor, which makes a copy of it without connection to
    the parent’s operation, i.e., detaching the tensor from the parent’s graph).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s the generator’s training time:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We pass the generator’s output to the discriminator, but now we don’t stop
    the gradients. Instead, we apply the objective function with True labels. It will
    push our generator in the direction where the samples that it generates make the
    discriminator confuse them with the real data. That was the code related to training,
    and the next couple of lines report losses and feed image samples to TensorBoard:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The training of this example is quite a lengthy process. On a GTX 1080Ti GPU,
    100 iterations take about 2.7 seconds. At the beginning, the generated images
    are completely random noise, but after 10k–20k iterations, the generator becomes
    more and more proficient at its job and the generated images become more and more
    similar to the real game screenshots.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: It also worth noting the performance improvement in software libraries. In the
    first and second editions of the book, exactly the same example ran much slower
    on the same hardware I have. On GTX 1080Ti, 100 iterations took around 40 seconds.
    Now, with PyTorch 2.2.0 on exactly the same GPU, 100 iterations take 2.7 seconds.
    So, instead of 3–4 hours, it now takes about 30 minutes to get good generated
    images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'My experiments gave the following images after 40k–50k of training iterations
    (about half an hour on a 1080 GPU):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file12.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Sample images produced by the generator network'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our network was able to reproduce the Atari screenshots quite
    well. In the next section, we’ll look at how we can simplify our code by using
    the add-on PyTorch library, Ignite.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Ignite
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch is an elegant and flexible library, which makes it a favorite choice
    for thousands of researchers, DL enthusiasts, industry developers, and others.
    But flexibility has its own price: too much code to be written to solve your problem.
    Sometimes, this is very beneficial, such as when implementing some new optimization
    method or DL trick that hasn’t been included in the standard library yet. Then
    you just implement the formulas using Python and PyTorch magic will do all the
    gradient and backpropagation machinery for you. Another example is in situations
    when you have to work on a very low level, fiddling with gradients, optimizer
    details, and the way your data is transformed by the NN.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'However, sometimes you don’t need this flexibility, which happens when you
    work on routine tasks, like the simple supervised training of an image classifier.
    For such tasks, standard PyTorch might be at too low a level when you need to
    deal with the same code over and over again. The following is a non-exhaustive
    list of topics that are an essential part of any DL training procedure, but require
    some code to be written:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation and transformation, and the generation of batches
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculation of training metrics, like loss values, accuracy, and F1-scores
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodical testing of the model being trained on the test and validation datasets
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model checkpointing after some number of iterations or when a new best metric
    is achieved
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending metrics into a monitoring tool like TensorBoard
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters change over time, like a learning rate decrease/increase schedule
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing training progress messages on the console
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are all doable using only PyTorch, of course, but it might require you
    to write a significant amount of code. As those tasks occur in any DL project,
    it quickly becomes cumbersome to write the same code over and over again. The
    normal approach to solving the issue is to write the functionality once, wrap
    it into a library, and reuse it later. If the library is open source and of good
    quality (easy to use, provides a good level of flexibility, written properly,
    and so on), it will become popular as more and more people use it in their projects.
    This process is not DL-specific; it happens everywhere in the software industry.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several libraries for PyTorch that simplify the solving of common
    tasks: ptlearn, fastai, ignite, and some others. The current list of “PyTorch
    ecosystem projects” can be found here: [https://pytorch.org/ecosystem](https://pytorch.org/ecosystem).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: It might be appealing to start using those high-level libraries from the beginning,
    as they allow you to solve common problems with just a couple of lines of code,
    but there is some danger here. If you only know how to use high-level libraries
    without understanding low-level details, you might get stuck on problems that
    can’t be solved solely by standard methods. In the very dynamic field of ML, this
    happens very often.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'The main focus of this book is to ensure that you understand RL methods, their
    implementation, and their applicability, so we will use an incremental approach.
    In the beginning, we will implement methods using only PyTorch code, but with
    more progress, examples will be implemented using high-level libraries. For RL,
    this will be the small library written by me: PTAN ([https://github.com/Shmuma/ptan/](https://github.com/Shmuma/ptan/)),
    and it will be introduced in Chapter [7](ch011.xhtml#x1-1070007).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the amount of DL boilerplate code, we will use a library called PyTorch
    Ignite: [https://pytorch-ignite.ai](https://pytorch-ignite.ai). In this section,
    a small overview of Ignite will be given, then we will check the Atari GAN example
    once it has been rewritten using Ignite.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Ignite concepts
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, Ignite simplifies the writing of the training loop in PyTorch
    DL. Earlier in this chapter (in the Loss functions and optimizers section, you
    saw that the minimal training loop consists of:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Sampling a batch of training data
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying an NN to this batch to calculate the loss function—the single value
    we want to minimize
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running backpropagation of the loss to get gradients on the network’s parameters
    in respect to the loss
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking the optimizer to apply the gradients to the network
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeating until we are happy or bored of waiting
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The central piece of Ignite is the Engine class, which loops over the data
    source, applying the processing function to the data batch. In addition to that,
    Ignite offers the ability to provide functions to be called at specific conditions
    of the training loop. Those conditions are called Events and could be at the:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Beginning/end of the whole training process
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning/end of a training epoch (iteration over the data)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning/end of a single batch processing
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to that, custom events exist and allow you to specify your function
    to be called every N events, for example, if you want to do some calculations
    every 100 batches or every second epoch.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'A very simplistic example of Ignite in action is shown in the following code
    block:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code is not runnable as it misses lots of details, like the data source,
    model, and optimizer creation, but it shows the basic idea of Ignite usage. The
    main benefit of Ignite is in the ability it provides to extend the training loop
    with existing functionality. You want the loss value to be smoothed and written
    in TensorBoard every 100 batches? No problem! Add two lines and it will be done.
    You want to run model validation every 10 epochs? Okay, write a function to run
    a test and attach it to the Engine instance, and it will be called.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'A description of the full Ignite functionality is beyond the scope of the book,
    but you can read the documentation on the official website: [https://pytorch-ignite.ai](https://pytorch-ignite.ai).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: GAN training on Atari using Ignite
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give you an illustration of Ignite, let’s change the example of GAN training
    on Atari images. The full example code is available in Chapter03/04_atari_gan_ignite.py;
    here, I will just show code that differs from the previous section.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import several Ignite classes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The Engine and Events classes have already been outlined. The package ignite.metrics
    contains classes related to working with the performance metrics of the training
    process, such as confusion matrices, precision, and recall. In our example, we
    will use the class RunningAverage, which provides a way to smooth time series
    values. In the previous example, we did this by calling np.mean() on an array
    of losses, but RunningAverage provides a more convenient (and mathematically more
    correct) way of doing this. In addition, we import the TensorBoard logger from
    the Ignite contrib package (the functionality of which is contributed by others).
    We’ll also use the Timer handler, which provides a simple way to calculate time
    elapsed between certain events.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, we need to define our processing function:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This function takes the data batch and does an update of both the discriminator
    and generator models on this batch. This function can return any data to be tracked
    during the training process; in our case, it will be two loss values for both
    models. In this function, we can also save images to be displayed in TensorBoard.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'After this is done, all we need to do is create an Engine instance, attach
    the required handlers, and run the training process:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding code, we create our engine, passing our processing function
    and attaching two RunningAverage transformations for our two loss values. Being
    attached, every RunningAverage produces a so-called “metric” — a derived value
    kept around during the training process. The names of our smoothed metrics are
    avg_loss_gen for smoothed loss from the generator, and avg_loss_dis for smoothed
    loss from the discriminator. Those two values will be written in TensorBoard after
    every iteration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: We also attach the timer, which, being created without any constructor arguments,
    acts as a simple manually-controlled timer (we call its reset() method manually),
    but can work in a more flexible way with different configuration options.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece of code attaches another event handler, which will be our function,
    and is called by the Engine on every iteration completion:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It will write a log line with an iteration index, time taken and values of smoothed
    metrics. The final line starts our engine, passing the already defined function
    as the data source (the iterate_batches function is a generator, returning the
    normal iterator over batches, so, it will be perfectly fine to pass its output
    as a data argument). And that’s it. If you run the Chapter03/04_atari_gan_ignite.py
    example, it will work the same way as our previous example, which might not be
    very impressive for such a small example, but in real projects, Ignite usage normally
    pays off by making your code cleaner and more extensible.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw a quick overview of PyTorch’s functionality and features.
    We talked about basic fundamental pieces, such as tensors and gradients, and you
    saw how an NN can be made from the basic building blocks, before learning how
    to implement those blocks yourself.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: We discussed loss functions and optimizers, as well as the monitoring of training
    dynamics. Finally, you were introduced to PyTorch Ignite, a library used to provide
    a higher-level interface for training loops. The goal of the chapter was to give
    a very quick introduction to PyTorch, which will be used later in the book.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we are ready to start dealing with the main subject of
    this book: RL methods.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
