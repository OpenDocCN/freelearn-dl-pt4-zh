- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you became familiar with open source libraries, which
    provided you with a collection of reinforcement learning (RL) environments. However,
    recent developments in RL, and especially its combination with deep learning (DL),
    now make it possible to solve much more challenging problems than ever before.
    This is partly due to the development of DL methods and tools. This chapter is
    dedicated to one such tool, PyTorch, which enables us to implement complex DL
    models with just a bunch of lines of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter doesn’t pretend to be a complete DL manual, as the field is very
    wide and dynamic; however, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch library specifics and implementation details (assuming that you
    are already familiar with DL fundamentals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-level libraries on top of PyTorch, with the aim of simplifying common
    DL problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch Ignite library, which will be used in some examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the examples in this chapter were updated for the latest(at the time
    of writing) PyTorch 2.3.1, which has changes in comparison to version 1.3.0, which
    was used in the second edition of this book. If you are using the old PyTorch,
    consider upgrading. Throughout this chapter, we will discuss the differences that
    are present in the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A tensor is the fundamental building block of all DL toolkits. The name sounds
    rather mystical, but the underlying idea is that a tensor is just a multi-dimensional
    array. Using the analogy of school math, one single number is like a point, which
    is zero-dimensional, while a vector is one-dimensional like a line segment, and
    a matrix is a two-dimensional object. Three-dimensional number collections can
    be represented by a cuboid of numbers, but they don’t have a separate name in
    the same way as a matrix. We can keep the term “tensor” for collections of higher
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx
    aaa i,ij,,jk,k,...ii,j ](img/B22150_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Going from a single number to an n-dimensional tensor'
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note about tensors used in DL is that they are only partially
    related to tensors used in tensor calculus or tensor algebra. In DL, a tensor
    is any multi-dimensional array, but in mathematics, a tensor is a mapping between
    vector spaces, which might be represented as a multi-dimensional array in some
    cases, but has much more semantical payload behind it. Mathematicians usually
    frown at anybody who uses well-established mathematical terms to name different
    things, so be warned!
  prefs: []
  type: TYPE_NORMAL
- en: The creation of tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ll deal with tensors everywhere in this book, we need to be familiar with
    basic operations on them, and the most basic is how to create one. There are several
    ways to do this, and your choice might influence code readability and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are familiar with the NumPy library (and you should be), then you already
    know that its central purpose is the handling of multi-dimensional arrays in a
    generic way. Even though in NumPy, such arrays aren’t called tensors, they are,
    in fact, tensors. Tensors are used very widely in scientific computations as generic
    storage for data. For example, a color image could be encoded as a 3D tensor with
    the dimensions of width, height, and color plane. Apart from dimensions, a tensor
    is characterized by the type of its elements. There are 13 types supported by
    PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Four float types: 16-bit, 32-bit, and 64-bit. 16-bit float has two variants:
    float16 has more bits for precision while bfloat16 has larger exponent part'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three complex types: 32-bit, 64-bit, and 128-bit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five integer types: 8-bit signed, 8-bit unsigned, 16-bit signed, 32-bit signed,
    and 64-bit signed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boolean type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also four ”quantized number” types, but they are using the preceding
    types, just with different bit representation and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors of different types are represented by different classes, with the most
    commonly used being torch.FloatTensor (corresponding to a 32-bit float), torch.ByteTensor
    (an 8-bit unsigned integer), and torch.LongTensor (a 64-bit signed integer). You
    can find names of other tensor types in the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to create a tensor in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: By calling a constructor of the required type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By asking PyTorch to create a tensor with specific data for you. For example,
    you can use the torch.zeros() function to create a tensor filled with zero values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By converting a NumPy array or a Python list into a tensor. In this case, the
    type will be taken from the array’s type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To give you examples of these methods, let’s look at a simple session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we imported both PyTorch and NumPy and created a new float tensor tensor
    of size 3 × 2\. As you can see, PyTorch initializes memory with zeros, which is
    a different behaviour from previous versions. Before, it just allocated memory
    and kept it uninitialized, which is slightly faster but less safe (as it might
    introduce tricky bugs and security issues). But you shouldn’t rely on this behaviour,
    as it might change again (or behave differently on different hardware backends)
    and always initialize the contents of the tensor. To do so, you can either use
    one of the tensor construct operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you can call the tensor modification method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two types of operation for tensors: inplace and functional. Inplace
    operations have an underscore appended to their name and operate on the tensor’s
    content. After this, the object itself is returned. The functional equivalent
    creates a copy of the tensor with the performed modification, leaving the original
    tensor untouched. Inplace operations are usually more efficient from a performance
    and memory point of view, but modification of an existing tensor (especially if
    it is shared in different pieces of code) might lead to hidden bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to create a tensor by its constructor is to provide a Python iterable
    (for example, a list or tuple), which will be used as the contents of the newly
    created tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are creating the same tensor with zeros from the NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The torch.tensor method accepts the NumPy array as an argument and creates
    a tensor of appropriate shape from it. In the preceding example, we created a
    NumPy array initialized by zeros, which created a double (64-bit float) array
    by default. So, the resulting tensor has the DoubleTensor type (which is shown
    in the example with the dtype value). Usually, in DL, double precision is not
    required and it adds an extra memory and performance overhead. Common practice
    is to use the 32-bit float type, or even the 16-bit float type, which is more
    than enough. To create such a tensor, you need to specify explicitly the type
    of NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As an option, the type of the desired tensor could be provided to the torch.tensor
    function in the dtype argument. However, be careful, since this argument expects
    to get a PyTorch type specification and not the NumPy one. PyTorch types are kept
    in the torch package, for example, torch.float32, torch.uint8, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A note on compatibility
  prefs: []
  type: TYPE_NORMAL
- en: The torch.tensor() method and explicit PyTorch type specification were added
    in the 0.4.0 release, and this is a step toward the simplification of tensor creation.
    In previous versions, the torch.from_numpy() function was a recommended way to
    convert NumPy arrays, but it had issues with handling the combination of the Python
    list and NumPy arrays. This from_numpy() function is still present for backward
    compatibility, but it is deprecated in favor of the more flexible torch.tensor()
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the 0.4.0 release, PyTorch has supported zero-dimensional tensors that
    correspond to scalar values (on the left of Figure [3.1](#x1-54002r1)). Such tensors
    can be the result of some operations, such as summing all values in a tensor.
    Previously, such cases were handled by the creation of a one-dimensional tensor
    (also known as vector) with a single dimension equal to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This solution worked, but it wasn’t very simple, as extra indexation was needed
    to access the value. Now, zero-dimensional tensors are natively supported and
    returned by the appropriate functions, and they can be created by the torch.tensor()
    function. To access the actual Python value of such a tensor, we can use the special
    item() method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tensor operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are lots of operations that you can perform on tensors, and there are
    too many to list them all. Usually, it’s enough to search in the PyTorch documentation
    at [http://pytorch.org/docs/](http://pytorch.org/docs/). I need to mention that
    there are two places to look for operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The torch package: The function usually accepts the tensor as an argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tensor class: The function operates on the called tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the time, tensor operations in PyTorch are trying to correspond to their
    NumPy equivalent, so if there is some not-very-specialized function in NumPy,
    then there is a good chance that PyTorch will also have it. Examples are torch.stack(),
    torch.transpose(), and torch.cat(). This is very convenient, as NumPy is a very
    widely used library (especially in the scientific community), so your PyTorch
    code becomes readable by anyone familiar with NumPy without looking into the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: GPU tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch transparently supports CUDA GPUs, which means that all operations have
    two versions — CPU and GPU — that are automatically selected. The decision is
    made based on the type of tensors that you are operating on.
  prefs: []
  type: TYPE_NORMAL
- en: Every tensor type that I mentioned is for CPU and has its GPU equivalent. The
    only difference is that GPU tensors reside in the torch.cuda package, instead
    of just torch. For example, torch.FloatTensor is a 32-bit float tensor that resides
    in CPU memory, but torch.cuda.FloatTensor is its GPU counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, under the hood, PyTorch supports not just CPU and CUDA; it has a notion
    of backend, which is an abstract computation device with memory. Tensors could
    be allocated in the backend’s memory and computations could be performed on them.
    For example, on Apple hardware, PyTorch supports Metal Performance Shaders (MPS)
    as a backend called mps. In this chapter, we focus on CPU and GPU as the mostly
    used backends, but your PyTorch code could be executed on much more fancier hardware
    without major modifications.
  prefs: []
  type: TYPE_NORMAL
- en: To convert from CPU to GPU, there is a tensor method, to(device), that creates
    a copy of the tensor to a specified device (this could be CPU or GPU). If the
    tensor is already on the device, nothing happens and the original tensor will
    be returned. The device type can be specified in different ways. First of all,
    you can just pass a string name of the device, which is "cpu" for CPU memory or
    "cuda" for GPU. A GPU device could have an optional device index specified after
    the colon; for example, the second GPU card in the system could be addressed by
    "cuda:1" (the index is zero-based).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another slightly more efficient way to specify a device in the to() method
    is by using the torch.device class, which accepts the device name and optional
    index. To access the device that your tensor is currently residing in, it has
    a device property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created a tensor on the CPU, then copied it to GPU memory. Both copies
    can be used in computations, and all GPU-specific machinery is transparent to
    the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The to() method and torch.device class were introduced in 0.4.0\. In previous
    versions, copying between CPU and GPU was performed by separate tensor methods,
    cpu() and cuda(), respectively, which required adding the extra lines of code
    to explicitly convert tensors into their CUDA versions. In newer PyTorch versions,
    you can create a desired torch.device object at the beginning of the program and
    use to(device) on every tensor that you’re creating. The old methods in the tensor,
    cpu() and cuda(), are still present and might be handy if you want to ensure that
    a tensor is in CPU or GPU memory regardless of its original location.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even with transparent GPU support, all of this dancing with tensors isn’t worth
    bothering without one “killer feature” — the automatic computation of gradients.
    This functionality was originally implemented in the Caffe toolkit and then became
    the de facto standard in DL libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, computing gradients manually was extremely painful to implement and
    debug, even for the simplest neural network (NN). You had to calculate derivatives
    for all your functions, apply the chain rule, and then implement the result of
    the calculations, praying that everything was done right. This could be a very
    useful exercise for understanding the nuts and bolts of DL, but it wasn’t something
    that you wanted to repeat over and over again by experimenting with different
    NN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, those days have gone now, much like programming your hardware using
    a soldering iron and vacuum tubes! Now, defining an NN of hundreds of layers requires
    nothing more than assembling it from predefined building blocks or, in the extreme
    case of you doing something fancy, defining the transformation expression manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'All gradients will be carefully calculated for you, backpropagated, and applied
    to the network. To be able to achieve this, you need to define your network architecture
    using DL library primitives. In Figure [3.2](#x1-59002r2), I have outlined the
    direction of the data and gradients flow during the optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts ](img/B22150_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Data and gradients flowing through the NN'
  prefs: []
  type: TYPE_NORMAL
- en: 'What can make a fundamental difference is how your gradients are calculated.
    There are two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static graph: In this method, you need to define your calculations in advance
    and it won’t be possible to change them later. The graph will be processed and
    optimized by the DL library before any computation is made. This model is implemented
    in TensorFlow (versions before 2.0), Theano, and many other DL toolkits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic graph: You don’t need to define your graph in advance exactly as it
    will be executed; you just need to execute operations that you want to use for
    data transformation on your actual data. During this, the library will record
    the order of the operations performed, and when you ask it to calculate gradients,
    it will unroll its history of operations, accumulating the gradients of the network
    parameters. This method is also called notebook gradients and it is implemented
    in PyTorch, Chainer, and some others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both methods have their strengths and weaknesses. For example, a static graph
    is usually faster, as all computations can be moved to the GPU, minimizing the
    data transfer overhead. Additionally, in a static graph, the library has much
    more freedom in optimizing the order that computations are performed in or even
    removing parts of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, although a dynamic graph has a higher computation overhead,
    it gives a developer much more freedom. For example, they can say, “For this piece
    of data, I can apply this network two times, and for this piece of data, I’ll
    use a completely different model with gradients clipped by the batch mean”. Another
    very appealing strength of the dynamic graph model is that it allows you to express
    your transformation more naturally and in a more “Pythonic” way. In the end, it’s
    just a Python library with a bunch of functions, so just call them and let the
    library do the magic.
  prefs: []
  type: TYPE_NORMAL
- en: Since version 2.0, PyTorch introduced the torch.compile function, which speeds
    up PyTorch code by JIT-compiling the code into optimized kernels. This is an evolution
    of the TorchScript and FX Tracing compiling methods from earlier versions.
  prefs: []
  type: TYPE_NORMAL
- en: From a historical perspective, this is highly amusing how originally radically
    different approaches of TensorFlow (static graph) and PyTorch (dynamic graph)
    are fusing into each other over time. Nowadays, PyTorch supports compile() and
    TensorFlow has “eager execution mode”.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors and gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch tensors have a built-in gradient calculation and tracking machinery,
    so all you need to do is convert the data into tensors and perform computations
    using the tensor methods and functions provided by torch. Of course, if you need
    to access underlying low-level details, you always can, but most of the time,
    PyTorch does what you’re expecting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several attributes related to gradients that every tensor has:'
  prefs: []
  type: TYPE_NORMAL
- en: 'grad: A property that holds a tensor of the same shape containing computed
    gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'is_leaf: Equals True if this tensor was constructed by the user and False if
    the object is a result of function transformation (in other words, have a parent
    in the computation graph).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'requires_grad: Equals True if this tensor requires gradients to be calculated.
    This property is inherited from leaf tensors, which get this value from the tensor
    construction step (torch.zeros() or torch.tensor() and so on). By default, the
    constructor has requires_grad=False, so if you want gradients to be calculated
    for your tensor, then you need to explicitly say so.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make all of this gradient-leaf machinery clearer, let’s consider this session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created two tensors. The first requires gradients to be calculated
    and the second doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have added both vectors element-wise (which is vector [3, 3]), doubled
    every element, and summed them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a zero-dimensional tensor with the value 12\. Okay, so far this
    is just a simple math. Now let’s look at the underlying graph that our expressions
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![vv+v×Σv2 12sruesm ](img/B22150_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Graph representation of the expression'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check the attributes of our tensors, then we will find that v1 and v2
    are the only leaf nodes and every variable, except v2, requires gradients to be
    calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the property requires_grad is sort of “sticky”: if one of the
    variables involved in computations has it set to True, all subsequent nodes also
    have it. This is logical behaviour, as we normally need gradients to be calculated
    for all intermediate steps in our computation. But “calculation” doesn’t mean
    they will be preserved in the .grad field. For memory efficiency, gradients are
    stored only for leaf nodes with requires_grad=True. If you want to keep gradients
    in the non-leaf nodes, you need to call their retain_grad() method, which tells
    PyTorch to keep the gradients for non-leaf node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s tell PyTorch to calculate the gradients of our graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: By calling the backward function, we asked PyTorch to calculate the numerical
    derivative of the v_res variable with respect to any variable that our graph has.
    In other words, what influence do small changes to the v_res variable have on
    the rest of the graph? In our particular example, the value of 2 in the gradients
    of v1 means that by increasing any element of v1 by one, the resulting value of
    v_res will grow by two.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, PyTorch calculates gradients only for leaf tensors with requires_grad=True.
    Indeed, if we try to check the gradients of v2, we get nothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The reason for that is efficiency in terms of computations and memory. In real
    life, our network can have millions of optimized parameters, with hundreds of
    intermediate operations performed on them. During gradient descent optimization,
    we are not interested in gradients of any intermediate matrix multiplication;
    the only things we want to adjust in the model are gradients of loss with respect
    to model parameters (weights). Of course, if you want to calculate the gradients
    of input data (it could be useful if you want to generate some adversarial examples
    to fool the existing NN or adjust pretrained word embeddings), then you can easily
    do so by passing requires_grad=True on tensor creation.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, you now have everything needed to implement your own NN optimizer.
    The rest of this chapter is about extra, convenient functions, which will provide
    you with higher-level building blocks of NN architectures, popular optimization
    algorithms, and common loss functions. However, don’t forget that you can easily
    reimplement all of these bells and whistles in any way that you like. This is
    why PyTorch is so popular among DL researchers — for its elegance and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Compatibility
  prefs: []
  type: TYPE_NORMAL
- en: Support of gradients calculation in tensors is one of the major changes in PyTorch
    0.4.0\. In previous versions, graph tracking and gradients accumulation were done
    in a separate, very thin class, Variable. This worked as a wrapper around the
    tensor and automatically saved the history of computations in order to be able
    to backpropagate. This class is still present in 2.2.0 (available in torch.autograd),
    but it is deprecated and will go away soon, so new code should avoid using it.
    From my perspective, this change is great, as the Variable logic was really thin,
    but it still required extra code and the developer’s attention to wrap and unwrap
    tensors. Now, gradients are a built-in tensor property, which makes the API much
    cleaner.
  prefs: []
  type: TYPE_NORMAL
- en: NN building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the torch.nn package, you will find tons of predefined classes providing
    you with the basic functionality blocks. All of them are designed with practice
    in mind (for example, they support mini-batches, they have sane default values,
    and the weights are properly initialized). All modules follow the convention of
    callable, which means that the instance of any class can act as a function when
    applied to its arguments. For example, the Linear class implements a feed-forward
    layer with optional bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created a randomly initialized feed-forward layer, with two inputs
    and five outputs, and applied it to our float tensor. All classes in the torch.nn
    packages inherit from the nn.Module base class, which you can use to implement
    your own higher-level NN blocks. You will see how you can do this in the next
    section, but, for now, let’s look at useful methods that all nn.Module children
    provide. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'parameters(): This function returns an iterator of all variables that require
    gradient computation (that is, module weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'zero_grad(): This function initializes all gradients of all parameters to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'to(device): This function moves all module parameters to a given device (CPU
    or GPU).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'state_dict(): This function returns the dictionary with all module parameters
    and is useful for model serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'load_state_dict(): This function initializes the module with the state dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole list of available classes can be found in the documentation at [http://pytorch.org/docs](http://pytorch.org/docs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I should mention one very convenient class that allows you to combine
    other layers into the pipe: Sequential. The best way to demonstrate Sequential
    is through an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we defined a three-layer NN with softmax on output, applied along dimension
    1 (dimension 0 is batch samples), rectified linear unit (ReLU) nonlinearities,
    and dropout. Let’s push something through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So, our mini-batch of one vector successfully traversed through the network!
  prefs: []
  type: TYPE_NORMAL
- en: Custom layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, I briefly mentioned the nn.Module class as a base parent
    for all NN building blocks exposed by PyTorch. It’s not just a unifying parent
    for the existing layers — it’s much more than that. By subclassing the nn.Module
    class, you can create your own building blocks, which can be stacked together,
    reused later, and integrated into the PyTorch framework flawlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, the nn.Module provides quite rich functionality to its children:'
  prefs: []
  type: TYPE_NORMAL
- en: It tracks all submodules that the current module includes. For example, your
    building block can have two feed-forward layers used somehow to perform the block’s
    transformation. To keep track of (register) the submodule, you just need to assign
    it to the class’s field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides functions to deal with all parameters of the registered submodules.
    You can obtain a full list of the module’s parameters (parameters() method), zero
    its gradients (zero_grads() method), move to CPU or GPU (to(device) method), serialize
    and deserialize the module (state_dict() and load_state_dict()), and even perform
    generic transformations using your own callable (apply() method).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It establishes the convention of Module application to data. Every module needs
    to perform its data transformation in the forward() method by overriding it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some more functions, such as the ability to register a hook function
    to tweak module transformation or gradients flow, but they are more for advanced
    use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These functionalities allow us to nest our submodels into higher-level models
    in a unified way, which is extremely useful when dealing with complexity. It could
    be a simple one-layer linear transformation or a 1001-layer residual NN (ResNet)
    monster, but if they follow the conventions of nn.Module, then both of them could
    be handled in the same way. This is very handy for code reusability and simplification
    (by hiding non-relevant implementation details).
  prefs: []
  type: TYPE_NORMAL
- en: To make our life simpler, when following the above conventions, PyTorch authors
    simplified the creation of modules through careful design and a good dose of Python
    magic. So, to create a custom module, we usually have to do only two things —
    register submodules and implement the forward() method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how this can be done for our Sequential example from the previous
    section, but in a more generic and reusable way (the full sample is Chapter03/01_modules.py).
    The following is our module class that inherits nn.Module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the constructor, we pass three parameters: the input size, the output size,
    and the optional dropout probability. The first thing we need to do is call the
    parent’s constructor to let it initialize itself.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second step in the preceding code, we create an already familiar nn.Sequential
    with a bunch of layers and assign it to our class field named pipe. By assigning
    a Sequential instance to our object’s field, we will automatically register this
    module (nn.Sequential inherits from nn.Module, as does everything in the nn package).
    To register it, we don’t need to call anything, we just need to assign our submodules
    to fields. After the constructor finishes, all those fields will be registered
    automatically. If you really want to, there is a function in nn.Module to register
    submodules called add_module(). It might be useful if your module can have variable
    number of layers and they need to be created programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must override the forward function with our implementation of data
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As our module is a very simple wrapper around the Sequential class, we just
    need to ask self.pipe to transform the data. Note that to apply a module to the
    data, we need to call the module as a callable (that is, pretend that the module
    instance is a function and call it with the arguments) and not use the forward()
    function of the nn.Module class. This is because nn.Module overrides the __call__()
    method, which is being used when we treat an instance as callable. This method
    does some nn.Module magic and calls our forward() method. If we call forward()
    directly, we will intervene with the nn.Module duty, which can give wrong results.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, that’s what we need to do to define our own module. Now, let’s use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We create our module, providing it with the desired number of inputs and outputs,
    then we create a tensor and ask our module to transform it, following the same
    convention of using it as callable. After that, we print our network’s structure
    (nn.Module overrides __str__() and __repr__()) to represent the inner structure
    in a nice way. The last thing we show is the result of the network’s transformation.
    The output of our code should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Of course, everything that was said about the dynamic nature of PyTorch is still
    true. The forward() method is called for every batch of data, so if you want to
    do some complex transformations based on the data you need to process, like hierarchical
    softmax or a random choice of network to apply, then nothing can stop you from
    doing so. The count of arguments to your module is also not limited by one parameter.
    So, if you want, you can write a module with multiple required parameters and
    dozens of optional arguments, and it will be fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to get familiar with two important pieces of the PyTorch library
    that will simplify our lives: loss functions and optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions and optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network that transforms input data into output is not the only thing we
    need for training. We also define our learning objective, which has to be a function
    that accepts two arguments — the network’s output and the desired output. Its
    responsibility is to return to us a single number — how close the network’s prediction
    is from the desired result. This function is called the loss function, and its
    output is the loss value. Using the loss value, we calculate gradients of network
    parameters and adjust them to decrease this loss value, which pushes our model
    to better results in the future. Both the loss function and the method of tweaking
    a network’s parameters by gradient are so common and exist in so many forms that
    both of them form a significant part of the PyTorch library. Let’s start with
    loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loss functions reside in the nn package and are implemented as an nn.Module
    subclass. Usually, they accept two arguments: output from the network (prediction)
    and desired output (ground-truth data, which is also called the label of the data
    sample). At the time of writing, PyTorch 2.3.1 contains over 20 different loss
    functions and, of course, nothing stops you from writing any custom function you
    want to optimize.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used standard loss functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'nn.MSELoss: The mean square error between arguments, which is the standard
    loss for regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'nn.BCELoss and nn.BCEWithLogits: Binary cross-entropy loss. The first version
    expects a single probability value (usually it’s the output of the Sigmoid layer),
    while the second version assumes raw scores as input and applies Sigmoid itself.
    The second way is usually more numerically stable and efficient. These losses
    (as their names suggest) are frequently used in binary classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'nn.CrossEntropyLoss and nn.NLLLoss: Famous “maximum likelihood” criteria that
    are used in multi-class classification problems. The first version expects raw
    scores for each class and applies LogSoftmax internally, while the second expects
    to have log probabilities as the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other loss functions available and you are always free to write your
    own Module subclass to compare the output and target. Now, let’s look at the second
    piece of the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The responsibility of the basic optimizer is to take the gradients of model
    parameters and change these parameters in order to decrease the loss value. By
    decreasing the loss value, we are pushing our model toward the desired output,
    which can give us hope for better model performance in the future. Changing parameters
    may sound simple, but there are lots of details here and the optimizer procedure
    is still a hot research topic. In the torch.optim package, PyTorch provides lots
    of popular optimizer implementations, and the most widely known are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD: A vanilla stochastic gradient descent algorithm with an optional momentum
    extension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMSprop: An optimizer proposed by Geoffrey Hinton'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adagrad: An adaptive gradients optimizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adam: A quite successful and popular combination of both RMSprop and Adagrad'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All optimizers expose the unified interface, which makes it easy to experiment
    with different optimization methods (sometimes the optimization method can really
    make a difference in convergence dynamics and the final result). On construction,
    you need to pass an iterable of tensors, which will be modified during the optimization
    process. The usual practice is to pass the result of the params() call of the
    upper-level nn.Module instance, which will return an iterable of all leaf tensors
    with gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s discuss the common blueprint of a training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Usually, you iterate over your data over and over again (one iteration over
    a full set of examples is called an epoch). Data is usually too large to fit into
    CPU or GPU memory at once, so it is split into batches of equal size. Every batch
    includes data samples and target labels, and both of them have to be tensors (lines
    2 and 3).
  prefs: []
  type: TYPE_NORMAL
- en: You pass data samples to your network (line 4) and feed its output and target
    labels to the loss function (line 5). The result of the loss function shows the
    “badness” of the network result relative to the target labels. As input to the
    network and the network’s weights are tensors, all transformations of your network
    are nothing more than a graph of operations with intermediate tensor instances.
    The same is true for the loss function — its result is also a tensor of one single
    loss value.
  prefs: []
  type: TYPE_NORMAL
- en: Every tensor in this computation graph remembers its parent, so to calculate
    gradients for the whole network, all you need to do is call the backward() function
    on a loss function result (line 6). The result of this call will be the unrolling
    of the graph of the performed computations and the calculating of gradients for
    every leaf tensor with require_grad=True. Usually, such tensors are our model’s
    parameters, such as the weights and biases of feed-forward networks, and convolution
    filters. Every time a gradient is calculated, it is accumulated in the tensor.grad
    field, so one tensor can participate in a transformation multiple times and its
    gradients will be properly summed together. For example, one single recurrent
    neural network (RNN) cell could be applied to multiple input items.
  prefs: []
  type: TYPE_NORMAL
- en: After the loss.backward() call is finished, we have the gradients accumulated,
    and now it’s time for the optimizer to do its job — it takes all gradients from
    the parameters we have passed to it on construction and applies them. All this
    is done with the method step() (line 7).
  prefs: []
  type: TYPE_NORMAL
- en: The last, but not least, piece of the training loop is our responsibility to
    zero gradients of parameters. This can be done by calling zero_grad() on our network,
    but, for our convenience, the optimizer also exposes such a call, which does the
    same thing (line 8). Sometimes, zero_grad() is placed at the beginning of the
    training loop, but it doesn’t matter much.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding scheme is a very flexible way to perform optimization and it can
    fulfill the requirements even in sophisticated research. For example, you can
    have two optimizers tweaking the options of different models on the same data
    (and this is a real-life scenario from generative adversarial network (GAN) training).
  prefs: []
  type: TYPE_NORMAL
- en: So, we are done with the essential functionality of PyTorch required to train
    NNs. This chapter ends with a practical medium-size example to demonstrate all
    the concepts covered, but before we get to it, we need to discuss one important
    topic that is essential for an NN practitioner — monitoring the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have ever tried to train an NN on your own, then you will know how painful
    and uncertain it can be. I’m not talking about following the existing tutorials
    and demos, when all the hyperparameters are already tuned for you, but about taking
    some data and creating something from scratch. Even with modern DL high-level
    toolkits, where all best practices, such as proper weights initialization; optimizers’
    betas, gammas, and other options set to sane defaults; and tons of other stuff
    hidden under the hood, there are still lots of decisions that you have to make,
    hence lots of things that could go wrong. As a result, your code almost never
    works from the first run, and this is something that you should get used to.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, with practice and experience, you will develop a strong understanding
    of the possible causes of problems, but this needs input data about what’s going
    on inside your network. So, you need to be able to peek inside your training process
    somehow and observe its dynamics. Even small networks (such as tiny MNIST tutorial
    networks) could have hundreds of thousands of parameters with quite nonlinear
    training dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'DL practitioners have developed a list of things that you should observe during
    your training, which usually includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loss value, which normally consists of several components like base loss and
    regularization losses. You should monitor both the total loss and the individual
    components over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results of validation on training and test datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics about gradients and weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values produced by the network. For example, if you are solving a classification
    problem, you definitely want to measure the entropy of predicted class probabilities.
    In the case of a regression problem, raw predicted values can give tons of data
    about the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rates and other hyperparameters, if they are adjusted over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list could be much longer and include domain-specific metrics, such as word
    embedding projections, audio samples, and images generated by GANs. You also may
    want to monitor values related to training speed, like how long an epoch takes,
    to see the effect of your optimizations or problems with hardware.
  prefs: []
  type: TYPE_NORMAL
- en: To cut a long story short, you need a generic solution to track lots of values
    over time and represent them for analysis, preferably developed especially for
    DL (just imagine looking at such statistics using an Excel spreadsheet). Luckily,
    such tools exist, and we will explore them next.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard 101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the first edition of this book was written, there wasn’t too much choice
    for NN monitoring. As time has passed by and new people and companies have become
    involved with the pursuit of ML and DL, more new tools have appeared, for example,
    MLflow [https://mlflow.org/](https://mlflow.org/). In this book, we will still
    focus on the TensorBoard utility from TensorFlow, but you might consider trying
    other alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first public version, TensorFlow included a special tool called TensorBoard,
    which was developed to solve the problem we are talking about — how to observe
    and analyze various NN characteristics during and after the training. TensorBoard
    is a powerful, generic solution with a large community and it looks quite pretty:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The TensorBoard web interface (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  prefs: []
  type: TYPE_NORMAL
- en: From the architecture point of view, TensorBoard is a Python web service that
    you can start on your computer, passing it the directory where your training process
    will save values to be analyzed. Then, you point your browser to TensorBoard’s
    port (usually 6006), and it shows you an interactive web interface with values
    updated in real time, as shown in Figure [3.4](#x1-67002r4). It’s nice and convenient,
    especially when your training is performed on a remote machine somewhere in the
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, TensorBoard was deployed as a part of TensorFlow, but after some
    time, it has been moved to a separate project (it’s still being maintained by
    Google) and it has its own package name. However, TensorBoard still uses the TensorFlow
    data format, so we will need to write this data from our PyTorch program. Several
    years ago, it required third-party libraries to be installed, but nowadays, PyTorch
    already comes with support of this data format (available in the torch.utils.tensorboard
    package).
  prefs: []
  type: TYPE_NORMAL
- en: Plotting metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give you an impression of how simple is to use TensorBoard, let’s consider
    a small example that is not related to NNs, but is just about writing values into
    TensorBoard (the full example code is in Chapter03/02_tensorboard.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we import the required packages, create a writer of
    data, and define functions that we are going to visualize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By default, SummaryWriter will create a unique directory in the runs directory
    for every launch, to be able to compare different rounds of training. The name
    of the new directory includes the current date and time, and the hostname. To
    override this, you can pass the log_dir argument to SummaryWriter. You can also
    add a suffix to the name of the directory by passing a comment argument, for example,
    to capture different experiments’ semantics, such as dropout=0.3 or strong_regularisation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we loop over angle ranges in degrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we convert the angle ranges into radians and calculate our functions’
    values. Every value is added to the writer using the add_scalar function, which
    takes three arguments: the name of the parameter, its value, and the current iteration
    (which has to be an integer). The last thing we need to do after the loop is close
    the writer. Note that the writer does a periodical flush (by default, every two
    minutes), so even in the case of a lengthy optimization process, you will still
    see your values. If you need to flush SummaryWriter data explicitly, it has the
    flush() method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of running this will be zero output on the console, but you will
    see a new directory created inside the runs directory with a single file. To look
    at the result, we need to start TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running TensorBoard on a remote server, you will need to add the
    --bind_all command-line option to make it accessible from other machines. Now
    you can open http://localhost:6006 in your browser to see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Plots produced by the example (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  prefs: []
  type: TYPE_NORMAL
- en: The graphs are interactive, so you can hover over them with your mouse to see
    the actual values and select regions to zoom in and look at details. To zoom out,
    double-click inside the graph. If you run your program several times, then you
    will see several items in the Runs list on the left, which can be enabled and
    disabled in any combination, allowing you to compare the dynamics of several optimizations.
    TensorBoard allows you to analyze not only scalar values but also images, audio,
    text data, and embeddings, and it can even show you the structure of your network.
    Refer to the documentation of TensorBoard for all those features. Now, it’s time
    to combine everything you learned in this chapter and look at a real NN optimization
    problem using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: GAN on Atari images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost every book about DL uses the MNIST dataset to show you the power of DL,
    which, over the years, has made this dataset extremely boring, like a fruit fly
    for genetic researchers. To break this tradition, and add a bit more fun to the
    book, I’ve tried to avoid well-beaten paths and illustrate PyTorch using something
    different. I briefly referred to generative adversarial networks (GANs) earlier
    in the chapter. In this example, we will train a GAN to generate screenshots of
    various Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest GAN architecture is this: we have two NNs where the first works
    as a ”cheater” (it is also called the generator), and the other as a ”detective”
    (another name is the discriminator). Both networks compete with each other — the
    generator tries to generate fake data, which will be hard for the discriminator
    to distinguish from your dataset, and the discriminator tries to detect the generated
    data samples. Over time, both networks improve their skills — the generator produces
    more and more realistic data samples, and the discriminator invents more sophisticated
    ways to distinguish the fake items.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical usage of GANs includes image quality improvement, realistic image
    generation, and feature learning. In our example, practical usefulness is almost
    zero, but it will be a good showcase about everything we learned about PyTorch
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s get started. The whole example code is in the file Chapter03/03_atari_gan.py.
    Here, we will look at only the most significant pieces of code, without the import
    section and constants declaration. The following class is a wrapper around a Gym
    game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding class includes several transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Resize the input image from 210×160 (the standard Atari resolution) to a square
    size of 64 × 64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move the color plane of the image from the last position to the first, to meet
    the PyTorch convention of convolution layers that input a tensor with the shape
    of the channels, height, and width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the image from bytes to float
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we define two nn.Module classes: Discriminator and Generator. The first
    takes our scaled color image as input and, by applying five layers of convolutions,
    converts it into a single number passed through a Sigmoid nonlinearity. The output
    from Sigmoid is interpreted as the probability that Discriminator thinks our input
    image is from the real dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Generator takes as input a vector of random numbers (latent vector) and, by
    using the “transposed convolution” operation (it is also known as deconvolution),
    converts this vector into a color image of the original resolution. We will not
    look at those classes here as they are lengthy and not very relevant to our example;
    you can find them in the complete example file.
  prefs: []
  type: TYPE_NORMAL
- en: As input, we will use screenshots from several Atari games played simultaneously
    by a random agent. Figure [3.6](#x1-69029r6) is an example of what the input data
    looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Sample screenshots from three Atari games'
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are combined in batches that are generated by the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This function infinitely samples the environment from the provided list, issues
    random actions, and remembers observations in the batch list. When the batch becomes
    of the required size, we normalize the image, convert it to a tensor, and yield
    from the generator. The check for the non-zero mean of the observation is required
    due to a bug in one of the games to prevent the flickering of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at our main function, which prepares models and runs the training
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we process the command-line arguments (which could be only one optional
    argument, --dev, which specifies the device to use for computations) and create
    our environment pool with a wrapper applied. This environment array will be passed
    to the iterate_batches function later to generate training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following piece, we create our classes — a summary writer, both networks,
    a loss function, and two optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Why do we need two optimizers? It’s because that’s the way that GANs get trained:
    to train the discriminator, we need to show it both real and fake data samples
    with appropriate labels (1 for real and 0 for fake). During this pass, we update
    only the discriminator’s parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we pass both real and fake samples through the discriminator again,
    but this time, the labels are 1s for all samples and we update only the generator’s
    weights. The second pass teaches the generator how to fool the discriminator and
    confuse real samples with the generated ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define arrays, which will be used to accumulate losses, iterator counters,
    and variables with the true and fake labels. We also store the current timestamp
    to report the time elapsed after 100 iterations of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'At the beginning of the following training loop, we generate a random vector
    and pass it to the Generator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the discriminator by applying it two times, once to the true
    data samples in our batch and once to the generated ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we need to call the detach() function on the generator’s
    output to prevent gradients of this training pass from flowing into the generator
    (detach() is a method of tensor, which makes a copy of it without connection to
    the parent’s operation, i.e., detaching the tensor from the parent’s graph).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s the generator’s training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass the generator’s output to the discriminator, but now we don’t stop
    the gradients. Instead, we apply the objective function with True labels. It will
    push our generator in the direction where the samples that it generates make the
    discriminator confuse them with the real data. That was the code related to training,
    and the next couple of lines report losses and feed image samples to TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The training of this example is quite a lengthy process. On a GTX 1080Ti GPU,
    100 iterations take about 2.7 seconds. At the beginning, the generated images
    are completely random noise, but after 10k–20k iterations, the generator becomes
    more and more proficient at its job and the generated images become more and more
    similar to the real game screenshots.
  prefs: []
  type: TYPE_NORMAL
- en: It also worth noting the performance improvement in software libraries. In the
    first and second editions of the book, exactly the same example ran much slower
    on the same hardware I have. On GTX 1080Ti, 100 iterations took around 40 seconds.
    Now, with PyTorch 2.2.0 on exactly the same GPU, 100 iterations take 2.7 seconds.
    So, instead of 3–4 hours, it now takes about 30 minutes to get good generated
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'My experiments gave the following images after 40k–50k of training iterations
    (about half an hour on a 1080 GPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Sample images produced by the generator network'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our network was able to reproduce the Atari screenshots quite
    well. In the next section, we’ll look at how we can simplify our code by using
    the add-on PyTorch library, Ignite.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Ignite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch is an elegant and flexible library, which makes it a favorite choice
    for thousands of researchers, DL enthusiasts, industry developers, and others.
    But flexibility has its own price: too much code to be written to solve your problem.
    Sometimes, this is very beneficial, such as when implementing some new optimization
    method or DL trick that hasn’t been included in the standard library yet. Then
    you just implement the formulas using Python and PyTorch magic will do all the
    gradient and backpropagation machinery for you. Another example is in situations
    when you have to work on a very low level, fiddling with gradients, optimizer
    details, and the way your data is transformed by the NN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, sometimes you don’t need this flexibility, which happens when you
    work on routine tasks, like the simple supervised training of an image classifier.
    For such tasks, standard PyTorch might be at too low a level when you need to
    deal with the same code over and over again. The following is a non-exhaustive
    list of topics that are an essential part of any DL training procedure, but require
    some code to be written:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation and transformation, and the generation of batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculation of training metrics, like loss values, accuracy, and F1-scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodical testing of the model being trained on the test and validation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model checkpointing after some number of iterations or when a new best metric
    is achieved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending metrics into a monitoring tool like TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters change over time, like a learning rate decrease/increase schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing training progress messages on the console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are all doable using only PyTorch, of course, but it might require you
    to write a significant amount of code. As those tasks occur in any DL project,
    it quickly becomes cumbersome to write the same code over and over again. The
    normal approach to solving the issue is to write the functionality once, wrap
    it into a library, and reuse it later. If the library is open source and of good
    quality (easy to use, provides a good level of flexibility, written properly,
    and so on), it will become popular as more and more people use it in their projects.
    This process is not DL-specific; it happens everywhere in the software industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several libraries for PyTorch that simplify the solving of common
    tasks: ptlearn, fastai, ignite, and some others. The current list of “PyTorch
    ecosystem projects” can be found here: [https://pytorch.org/ecosystem](https://pytorch.org/ecosystem).'
  prefs: []
  type: TYPE_NORMAL
- en: It might be appealing to start using those high-level libraries from the beginning,
    as they allow you to solve common problems with just a couple of lines of code,
    but there is some danger here. If you only know how to use high-level libraries
    without understanding low-level details, you might get stuck on problems that
    can’t be solved solely by standard methods. In the very dynamic field of ML, this
    happens very often.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main focus of this book is to ensure that you understand RL methods, their
    implementation, and their applicability, so we will use an incremental approach.
    In the beginning, we will implement methods using only PyTorch code, but with
    more progress, examples will be implemented using high-level libraries. For RL,
    this will be the small library written by me: PTAN ([https://github.com/Shmuma/ptan/](https://github.com/Shmuma/ptan/)),
    and it will be introduced in Chapter [7](ch011.xhtml#x1-1070007).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the amount of DL boilerplate code, we will use a library called PyTorch
    Ignite: [https://pytorch-ignite.ai](https://pytorch-ignite.ai). In this section,
    a small overview of Ignite will be given, then we will check the Atari GAN example
    once it has been rewritten using Ignite.'
  prefs: []
  type: TYPE_NORMAL
- en: Ignite concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, Ignite simplifies the writing of the training loop in PyTorch
    DL. Earlier in this chapter (in the Loss functions and optimizers section, you
    saw that the minimal training loop consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling a batch of training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying an NN to this batch to calculate the loss function—the single value
    we want to minimize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running backpropagation of the loss to get gradients on the network’s parameters
    in respect to the loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking the optimizer to apply the gradients to the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeating until we are happy or bored of waiting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The central piece of Ignite is the Engine class, which loops over the data
    source, applying the processing function to the data batch. In addition to that,
    Ignite offers the ability to provide functions to be called at specific conditions
    of the training loop. Those conditions are called Events and could be at the:'
  prefs: []
  type: TYPE_NORMAL
- en: Beginning/end of the whole training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning/end of a training epoch (iteration over the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning/end of a single batch processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to that, custom events exist and allow you to specify your function
    to be called every N events, for example, if you want to do some calculations
    every 100 batches or every second epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very simplistic example of Ignite in action is shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This code is not runnable as it misses lots of details, like the data source,
    model, and optimizer creation, but it shows the basic idea of Ignite usage. The
    main benefit of Ignite is in the ability it provides to extend the training loop
    with existing functionality. You want the loss value to be smoothed and written
    in TensorBoard every 100 batches? No problem! Add two lines and it will be done.
    You want to run model validation every 10 epochs? Okay, write a function to run
    a test and attach it to the Engine instance, and it will be called.
  prefs: []
  type: TYPE_NORMAL
- en: 'A description of the full Ignite functionality is beyond the scope of the book,
    but you can read the documentation on the official website: [https://pytorch-ignite.ai](https://pytorch-ignite.ai).'
  prefs: []
  type: TYPE_NORMAL
- en: GAN training on Atari using Ignite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To give you an illustration of Ignite, let’s change the example of GAN training
    on Atari images. The full example code is available in Chapter03/04_atari_gan_ignite.py;
    here, I will just show code that differs from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import several Ignite classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The Engine and Events classes have already been outlined. The package ignite.metrics
    contains classes related to working with the performance metrics of the training
    process, such as confusion matrices, precision, and recall. In our example, we
    will use the class RunningAverage, which provides a way to smooth time series
    values. In the previous example, we did this by calling np.mean() on an array
    of losses, but RunningAverage provides a more convenient (and mathematically more
    correct) way of doing this. In addition, we import the TensorBoard logger from
    the Ignite contrib package (the functionality of which is contributed by others).
    We’ll also use the Timer handler, which provides a simple way to calculate time
    elapsed between certain events.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, we need to define our processing function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This function takes the data batch and does an update of both the discriminator
    and generator models on this batch. This function can return any data to be tracked
    during the training process; in our case, it will be two loss values for both
    models. In this function, we can also save images to be displayed in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this is done, all we need to do is create an Engine instance, attach
    the required handlers, and run the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we create our engine, passing our processing function
    and attaching two RunningAverage transformations for our two loss values. Being
    attached, every RunningAverage produces a so-called “metric” — a derived value
    kept around during the training process. The names of our smoothed metrics are
    avg_loss_gen for smoothed loss from the generator, and avg_loss_dis for smoothed
    loss from the discriminator. Those two values will be written in TensorBoard after
    every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: We also attach the timer, which, being created without any constructor arguments,
    acts as a simple manually-controlled timer (we call its reset() method manually),
    but can work in a more flexible way with different configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece of code attaches another event handler, which will be our function,
    and is called by the Engine on every iteration completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It will write a log line with an iteration index, time taken and values of smoothed
    metrics. The final line starts our engine, passing the already defined function
    as the data source (the iterate_batches function is a generator, returning the
    normal iterator over batches, so, it will be perfectly fine to pass its output
    as a data argument). And that’s it. If you run the Chapter03/04_atari_gan_ignite.py
    example, it will work the same way as our previous example, which might not be
    very impressive for such a small example, but in real projects, Ignite usage normally
    pays off by making your code cleaner and more extensible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw a quick overview of PyTorch’s functionality and features.
    We talked about basic fundamental pieces, such as tensors and gradients, and you
    saw how an NN can be made from the basic building blocks, before learning how
    to implement those blocks yourself.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed loss functions and optimizers, as well as the monitoring of training
    dynamics. Finally, you were introduced to PyTorch Ignite, a library used to provide
    a higher-level interface for training loops. The goal of the chapter was to give
    a very quick introduction to PyTorch, which will be used later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we are ready to start dealing with the main subject of
    this book: RL methods.'
  prefs: []
  type: TYPE_NORMAL
