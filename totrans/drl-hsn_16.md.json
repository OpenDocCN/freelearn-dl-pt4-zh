["```py\nHID_SIZE = 64 \n\nclass ModelActor(nn.Module): \n    def __init__(self, obs_size: int, act_size: int): \n        super(ModelActor, self).__init__() \n\n        self.mu = nn.Sequential( \n            nn.Linear(obs_size, HID_SIZE), \n            nn.Tanh(), \n            nn.Linear(HID_SIZE, HID_SIZE), \n            nn.Tanh(), \n            nn.Linear(HID_SIZE, act_size), \n            nn.Tanh(), \n        ) \n        self.logstd = nn.Parameter(torch.zeros(act_size)) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        return self.mu(x)\n```", "```py\nclass ModelCritic(nn.Module): \n    def __init__(self, obs_size: int): \n        super(ModelCritic, self).__init__() \n\n        self.value = nn.Sequential( \n            nn.Linear(obs_size, HID_SIZE), \n            nn.ReLU(), \n            nn.Linear(HID_SIZE, HID_SIZE), \n            nn.ReLU(), \n            nn.Linear(HID_SIZE, 1), \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        return self.value(x)\n```", "```py\nclass AgentA2C(ptan.agent.BaseAgent): \n    def __init__(self, net, device: torch.device): \n        self.net = net \n        self.device = device \n\n    def __call__(self, states: ptan.agent.States, agent_states: ptan.agent.AgentStates): \n        states_v = ptan.agent.float32_preprocessor(states) \n        states_v = states_v.to(self.device) \n\n        mu_v = self.net(states_v) \n        mu = mu_v.data.cpu().numpy() \n        logstd = self.net.logstd.data.cpu().numpy() \n        rnd = np.random.normal(size=logstd.shape) \n        actions = mu + np.exp(logstd) * rnd \n        actions = np.clip(actions, -1, 1) \n        return actions, agent_states\n```", "```py\nGAMMA = 0.99 \nGAE_LAMBDA = 0.95 \n\nTRAJECTORY_SIZE = 2049 \nLEARNING_RATE_ACTOR = 1e-5 \nLEARNING_RATE_CRITIC = 1e-4 \n\nPPO_EPS = 0.2 \nPPO_EPOCHES = 10 \nPPO_BATCH_SIZE = 64\n```", "```py\ndef calc_adv_ref(trajectory: tt.List[ptan.experience.Experience], \n                 net_crt: model.ModelCritic, states_v: torch.Tensor, gamma: float, \n                 gae_lambda: float, device: torch.device): \n    values_v = net_crt(states_v) \n    values = values_v.squeeze().data.cpu().numpy()\n```", "```py\n last_gae = 0.0 \n    result_adv = [] \n    result_ref = [] \n    for val, next_val, (exp,) in zip( \n            reversed(values[:-1]), reversed(values[1:]), reversed(trajectory[:-1])):\n```", "```py\n if exp.done_trunc: \n            delta = exp.reward - val \n            last_gae = delta \n        else: \n            delta = exp.reward + gamma * next_val - val \n            last_gae = delta + gamma * gae_lambda * last_gae\n```", "```py\n result_adv.append(last_gae) \n        result_ref.append(last_gae + val)\n```", "```py\n adv_v = torch.FloatTensor(np.asarray(list(reversed(result_adv)))) \n    ref_v = torch.FloatTensor(np.asarray(list(reversed(result_ref)))) \n    return adv_v.to(device), ref_v.to(device)\n```", "```py\n trajectory.append(exp) \n            if len(trajectory) < TRAJECTORY_SIZE: \n                continue \n\n            traj_states = [t[0].state for t in trajectory] \n            traj_actions = [t[0].action for t in trajectory] \n            traj_states_v = torch.FloatTensor(np.asarray(traj_states)) \n            traj_states_v = traj_states_v.to(device) \n            traj_actions_v = torch.FloatTensor(np.asarray(traj_actions)) \n            traj_actions_v = traj_actions_v.to(device) \n            traj_adv_v, traj_ref_v = common.calc_adv_ref( \n                trajectory, net_crt, traj_states_v, GAMMA, GAE_LAMBDA, device=device)\n```", "```py\n mu_v = net_act(traj_states_v) \n            old_logprob_v = model.calc_logprob(mu_v, net_act.logstd, traj_actions_v) \n\n            traj_adv_v = traj_adv_v - torch.mean(traj_adv_v) \n            traj_adv_v /= torch.std(traj_adv_v)\n```", "```py\n trajectory = trajectory[:-1] \n            old_logprob_v = old_logprob_v[:-1].detach()\n```", "```py\n for epoch in range(PPO_EPOCHES): \n                for batch_ofs in range(0, len(trajectory), PPO_BATCH_SIZE): \n                    batch_l = batch_ofs + PPO_BATCH_SIZE \n                    states_v = traj_states_v[batch_ofs:batch_l] \n                    actions_v = traj_actions_v[batch_ofs:batch_l] \n                    batch_adv_v = traj_adv_v[batch_ofs:batch_l] \n                    batch_adv_v = batch_adv_v.unsqueeze(-1) \n                    batch_ref_v = traj_ref_v[batch_ofs:batch_l] \n                    batch_old_logprob_v = old_logprob_v[batch_ofs:batch_l]\n```", "```py\n opt_crt.zero_grad() \n                    value_v = net_crt(states_v) \n                    loss_value_v = F.mse_loss(value_v.squeeze(-1), batch_ref_v) \n                    loss_value_v.backward() \n                    opt_crt.step()\n```", "```py\n opt_act.zero_grad() \n                    mu_v = net_act(states_v) \n                    logprob_pi_v = model.calc_logprob(mu_v, net_act.logstd, actions_v) \n                    ratio_v = torch.exp(logprob_pi_v - batch_old_logprob_v) \n                    surr_obj_v = batch_adv_v * ratio_v \n                    c_ratio_v = torch.clamp(ratio_v, 1.0 - PPO_EPS, 1.0 + PPO_EPS) \n                    clipped_surr_v = batch_adv_v * c_ratio_v \n                    loss_policy_v = -torch.min(surr_obj_v, clipped_surr_v).mean() \n                    loss_policy_v.backward() \n                    opt_act.step()\n```", "```py\n opt_crt.zero_grad() \n            value_v = net_crt(traj_states_v) \n            loss_value_v = F.mse_loss(value_v.squeeze(-1), traj_ref_v) \n            loss_value_v.backward() \n            opt_crt.step()\n```", "```py\n def get_loss(): \n                mu_v = net_act(traj_states_v) \n                logprob_v = model.calc_logprob(mu_v, net_act.logstd, traj_actions_v) \n                dp_v = torch.exp(logprob_v - old_logprob_v) \n                action_loss_v = -traj_adv_v.unsqueeze(dim=-1)*dp_v \n                return action_loss_v.mean() \n\n            def get_kl(): \n                mu_v = net_act(traj_states_v) \n                logstd_v = net_act.logstd \n                mu0_v = mu_v.detach() \n                logstd0_v = logstd_v.detach() \n                std_v = torch.exp(logstd_v) \n                std0_v = std_v.detach() \n                v = (std0_v ** 2 + (mu0_v - mu_v) ** 2) / (2.0 * std_v ** 2) \n                kl = logstd_v - logstd0_v + v - 0.5 \n                return kl.sum(1, keepdim=True) \n\n            trpo.trpo_step(net_act, get_loss, get_kl, args.maxkl, \n                           TRPO_DAMPING, device=device)\n```", "```py\n@torch.no_grad() \ndef unpack_batch_sac(batch: tt.List[ptan.experience.ExperienceFirstLast], \n                     val_net: model.ModelCritic, twinq_net: model.ModelSACTwinQ, \n                     policy_net: model.ModelActor, gamma: float, ent_alpha: float, \n                     device: torch.device): \n    states_v, actions_v, ref_q_v = unpack_batch_a2c(batch, val_net, gamma, device) \n\n    mu_v = policy_net(states_v) \n    act_dist = distr.Normal(mu_v, torch.exp(policy_net.logstd)) \n    acts_v = act_dist.sample() \n    q1_v, q2_v = twinq_net(states_v, acts_v) \n\n    ref_vals_v = torch.min(q1_v, q2_v).squeeze() - \\ \n                 ent_alpha * act_dist.log_prob(acts_v).sum(dim=1) \n    return states_v, actions_v, ref_vals_v, ref_q_v\n```", "```py\n batch = buffer.sample(BATCH_SIZE) \n                states_v, actions_v, ref_vals_v, ref_q_v = common.unpack_batch_sac( \n                        batch, tgt_crt_net.target_model, twinq_net, act_net, GAMMA, \n                        SAC_ENTROPY_ALPHA, device)\n```", "```py\n twinq_opt.zero_grad() \n                q1_v, q2_v = twinq_net(states_v, actions_v) \n                q1_loss_v = F.mse_loss(q1_v.squeeze(), ref_q_v.detach()) \n                q2_loss_v = F.mse_loss(q2_v.squeeze(), ref_q_v.detach()) \n                q_loss_v = q1_loss_v + q2_loss_v \n                q_loss_v.backward() \n                twinq_opt.step()\n```", "```py\n crt_opt.zero_grad() \n                val_v = crt_net(states_v) \n                v_loss_v = F.mse_loss(val_v.squeeze(), ref_vals_v.detach()) \n                v_loss_v.backward() \n                crt_opt.step()\n```", "```py\n act_opt.zero_grad() \n                acts_v = act_net(states_v) \n                q_out_v, _ = twinq_net(states_v, acts_v) \n                act_loss = -q_out_v.mean() \n                act_loss.backward() \n                act_opt.step()\n```"]