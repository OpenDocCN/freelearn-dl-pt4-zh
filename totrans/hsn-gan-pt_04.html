<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Best Practices for Model Design and Training</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we'll take what we've learned so far and provide some basic information on it to help you move forward. We will look into the overall design of the model architecture and the steps we need to follow when choosing which convolution operation is needed. We will also learn how to adjust and tweak the loss function and learning rate.</p>
<p class="mce-root"><span><span>In this chapter, we will cover the following</span></span> topics:</p>
<ul>
<li>Model design cheat sheet</li>
<li>Model training cheat sheet</li>
<li>Efficient coding in Python</li>
<li>Advice for <span>deep learning </span>beginners</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model design cheat sheet</h1>
                </header>
            
            <article>
                
<p>In this section, we will provide you with an overview of the various choices you can make when it comes to designing the architecture of GAN models, and even deep learning models in general. It is always okay to directly borrow the model architectures you see in papers. It is also imperative to know how to adjust a model and create a brand new model from scratch, according to the practical problems at hand. Other factors, such as GPU memory capacity and expected training time, should also be considered when we design our models. We will talk about the following:</p>
<ul>
<li>Overall model architecture design</li>
<li>Choosing a convolution operation method</li>
<li>Choosing a downsampling operation method</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overall model architecture design</h1>
                </header>
            
            <article>
                
<p>There are mainly two different design processes for deep learning models. They are fit for different scenarios and you should get comfortable with both processes:</p>
<ul>
<li>Design the whole network directly, especially for shallow networks. You can add/remove any layer in your network with ease. With this approach, you can easily notice any bottlenecks in your network (for example, which layer needs more/fewer neurons), which is extremely important when you are designing models that will run on mobile devices.</li>
<li>Design a small block/cell (containing several layers or operations) and repeat the blocks several times to form the whole network. This process is very popular in very deep networks, especially in <strong>network architecture search</strong> (<strong>NAS</strong>). It is a bit harder to spot the weak spot in your model because all you can do is adjust the block, train the whole network for hours, and see if your adjustments lead to higher performance.</li>
</ul>
<p>In some of the chapters in this book, U-Net-shaped (for example, pix2pixm, which we will cover in <a href="209b2357-05d7-48d4-9c91-e061eccf8344.xhtml">Chapter 5</a>, <em>Image-to-Image Translation and Its Applications</em>) and ResNet-shaped (for example, SRGAN, which we will cover in <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, <em>Image Restoration with GANs</em>) networks will be used. Both architectures are designed via a block-based approach and use skip connections to connect non-adjacent layers. There are two different forms of data flow in neural networks:</p>
<ul>
<li><strong>Plain network</strong>: Any layer within the network only has at most one input and one output direction.</li>
<li><strong>Branching network</strong>: At least one of the layers is connected to more than two other layers, such as ResNet and DenseNet.</li>
</ul>
<p>You may have noticed that, in this book, plain networks are often used in discriminators and branching architectures are often used in generators. This is because generator networks are, in general, more difficult to train than discriminators and the branches (for example, skip connections) pass low-level details to deeper layers in the forward pass and help gradients flow better in the backward pass.</p>
<p class="mce-root"/>
<p>When we deal with branches in a network, how several branches are merged (so that the tensors can be passed to another block/cell in the uniform size) also has a great impact on the network's performance. Here are the recommended approaches:</p>
<ul>
<li>Concatenate all the tensors into a list and create another convolution layer to map this list to a smaller tensor. This way, information from all the input branches is reserved and the relationship between them is learned by the convolution layer. However, be careful with this approach when it comes to very deep networks since it costs more memory and more parameters means it's more vulnerable to overfitting.</li>
<li>Directly sum the overall input tensors. This is easy to implement but may not perform well when there are too many input branches.</li>
<li>Assign trainable weight factors to the branches before summing them up. Here, the merged tensor would be the weighted sum of the input tensors. It allows the network to figure out which inputs it should reply to and gives you a chance to remove unnecessary branches if their trained weight factors are too close to 0.</li>
</ul>
<p>In general, if you are dealing with complex data, try using the classic models we have learned about in this book. If the classic models don't serve you well, try building a basic block (such as a residual block) and building a deep network with it. Deeper networks come with more surprises and, of course, take a longer time to train. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing a convolution operation method</h1>
                </header>
            
            <article>
                
<p>There are various types of convolution operations we can choose from, and different configurations in the same convolution layer lead to different results. Here, we will summarize the commonly used convolution operations and talk about their strengths and weaknesses:</p>
<ol>
<li><strong>Vanilla convolution</strong>: This is the most common convolution operation in CNNs. A convolution takes fewer parameters than a fully connected layer (<kbd>nn.Linear</kbd>) with the same input/output size and can be calculated pretty fast with im2col (see <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, <em>Image Restoration with GANs</em>, for more details). You can use the following snippet to create a ReLu-Conv-BN group (of course, feel free to change the order of the three functions):</li>
</ol>
<pre style="padding-left: 60px">class ReLUConvBN(nn.Module):<br/>    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):<br/>        super(ReLUConvBN, self).__init__()<br/>        self.op = nn.Sequential(<br/>            nn.ReLU(inplace=False),<br/>            nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),<br/>            nn.BatchNorm2d(C_out, affine=affine)<br/>        )<br/><br/>    def forward(self, x):<br/>        return self.op(x)</pre>
<ol start="2">
<li><strong>Grouped convolution</strong><span>:</span> Here, the connections between the input/output neurons are separated into groups. You can create a grouped convolution by calling <kbd>nn.Conv2d</kbd> when assigning the <kbd>groups</kbd> argument to an integer larger than 1. It is often followed by another convolution layer with a kernel size of 1 so that the information from different groups can be mixed together. The GroupConv-1x1Conv combination always contains fewer parameters than a vanilla convolution as long as the kernel size is larger than 1.</li>
<li><strong>Depthwise separable convolution</strong><span>:</span> This is a grouped convolution where the group size equals the input channels, followed by a 1 x 1 convolution. It always contains fewer parameters than a vanilla convolution as long as the kernel size is larger than 1. Depthwise separable convolution is extremely popular among tiny networks for mobile devices and NAS (where people try to reach the highest performance under limited hardware resources). It is often used to see whether two depthwise separable convolutions appear together and result in better performance. You can use the following snippet to create a two-layer <span>depthwise separable convolution operation:</span></li>
</ol>
<pre style="padding-left: 60px">class SepConv(nn.Module):<br/>    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):<br/>        super(SepConv, self).__init__()<br/>        self.op = nn.Sequential(<br/>            nn.ReLU(inplace=False),<br/>            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),<br/>            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),<br/>            nn.BatchNorm2d(C_in, affine=affine),<br/>            nn.ReLU(inplace=False),<br/>            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),<br/>            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),<br/>            nn.BatchNorm2d(C_out, affine=affine)<br/>        )<br/>    def forward(self, x):<br/>        return self.op(x)</pre>
<ol start="4">
<li><strong>Dilation convolution</strong>: This has a larger reception field <span>compared to vanilla convolution. For example, a <img class="fm-editor-equation" src="assets/c0c5d15e-1d94-44c6-ae74-b370aa773be8.png" style="width:3.83em;height:1.50em;"/> vanilla convolution has a <img class="fm-editor-equation" src="assets/27e72105-f44e-419d-aa0c-27f5d6fe5fcd.png" style="width:3.83em;height:1.50em;"/> sliding window, but a <img class="fm-editor-equation" src="assets/a4052c0c-f467-46ee-8669-5a566c600cfd.png" style="width:3.83em;height:1.50em;"/> dilation convolution has a <img class="fm-editor-equation" src="assets/a278791c-2919-4deb-9da7-2a32ba12377b.png" style="width:3.83em;height:1.50em;"/> sliding window, in which input pixels are samples – one for every two adjacent steps. However, it is not recommended to use dilation convolution with other types of convolutions (for example, depthwise separable convolution) in the same network. This is because dilation convolutions normally need much smaller learning steps to train, which will dramatically slow down your training process. You can use the following snippet to create a dilation convolution operation:</span></li>
</ol>
<pre style="padding-left: 60px">class DilConv(nn.Module):<br/>    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):<br/>        super(DilConv, self).__init__()<br/>        self.op = nn.Sequential(<br/>            nn.ReLU(inplace=False),<br/>            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),<br/>            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),<br/>            nn.BatchNorm2d(C_out, affine=affine)<br/>            )<br/><br/>    def forward(self, x):<br/>        return self.op(x)</pre>
<p>Normally, vanilla convolutions are already good enough. If your memory capacity is extremely limited, depthwise separable convolution would definitely be your best choice.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing a downsampling operation method</h1>
                </header>
            
            <article>
                
<p>It is often inevitable to increase or decrease the size of tensors (feature maps) in a network. The process of decreasing a tensor's size is called <strong>downsampling</strong> and the process of increasing a tensor's size is called <strong>upsampling</strong>. Downsampling is often trickier than upsampling since we don't want to lose too much useful information in the smaller tensors.</p>
<p class="mce-root"/>
<p>There are several ways to perform downsampling in neural networks, especially in CNNs. You may choose the most suitable one based on your needs:</p>
<ul>
<li><strong>Max-pooling</strong> (for example, <kbd>nn.MaxPool2d</kbd>), which is where you select the maximum value in the sliding window. It is quite popular in the early shallow networks, such as LeNet-5. However, the maximum value is not necessarily the most significant feature in a feature map. For example, what happens to the minimum values? Apparently, the minimum value (<img class="fm-editor-equation" src="assets/b0165383-5175-4b19-b252-4b94e6713e76.png" style="width:2.25em;height:1.00em;"/>) in the <img class="fm-editor-equation" src="assets/d949fbee-fc72-4ce7-a495-1ad11194614c.png" style="width:11.08em;height:1.42em;"/> tensor gives us more information than the maximum value (<img class="fm-editor-equation" src="assets/d2cb2d38-9aef-4168-be6b-ab54a53c6404.png" style="width:2.25em;height:1.50em;"/>) about what kind of pattern this tensor contains.</li>
<li><strong>Average-pooling</strong> (for example, <kbd>nn.AvgPool2d</kbd> or <kbd>nn.AdaptiveAvgPool2d</kbd>), which is where you take the average value over the sliding window. It is becoming more popular than max-pooling. You should certainly choose average-pooling over max-pooling if you want to perform fast downsampling.</li>
<li><strong>Strided convolution</strong>, which is a convolution with a stride size larger than 1. This is actually what most of the models in this book use to perform downsampling since this approach can extract features and decrease the tensor size at the same time. It is worth pointing out that there can be a huge amount of information loss in this approach because the sliding window skips a lot of pixels while it's calculating. A decrease in the feature map size is often accompanied by an increase in the channel size. For example, a mini-batch tensor <img class="fm-editor-equation" src="assets/67c71855-d45e-4a97-a1f0-a09c8ca3d4a9.png" style="width:7.58em;height:1.42em;"/> (the four dimensions denote batch size, channel size, feature map height, and feature map width) is often downsampled to <img class="fm-editor-equation" src="assets/2b61def8-335f-49ff-93fc-5eaa22e4cf8c.png" style="width:8.67em;height:1.50em;"/> so that the output tensor contains a similar amount of information to the input tensor.</li>
<li><strong>Factorized reduction</strong>, which is to perform two strided convolutions with a slight shift. In this approach, the second convolution covers the skipped pixels by the first convolution. Therefore, more information is reserved. It contains more parameters and so takes a longer time to train. You can use the following snippet to perform factorized reduction:</li>
</ul>
<pre style="padding-left: 60px">class FactorizedReduce(nn.Module):<br/>    def __init__(self, C_in, C_out, affine=True):<br/>        super(FactorizedReduce, self).__init__()<br/>        assert C_out % 2 == 0<br/>        self.relu = nn.ReLU(inplace=False)<br/>        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)<br/>        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)<br/>        self.bn = nn.BatchNorm2d(C_out, affine=affine)<br/><br/>    def forward(self, x):<br/>        x = self.relu(x)<br/>        out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)<br/>        out = self.bn(out)<br/>        return out</pre>
<p>If you have more than enough GPU memory to spare, use factorized reduction in your model. If not, using the strided convolution would save you a lot of memory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More on model design</h1>
                </header>
            
            <article>
                
<p>Feel free to check out PyTorch's official document at <kbd>torch.nn</kbd> to find out more about the various layers and operations that are available: <a href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training cheat sheet</h1>
                </header>
            
            <article>
                
<p>Designing a training strategy is just as important <span>– if not more – than</span> model design. Sometimes, a good training strategy can make a poorly designed model shine. Here, we will talk about the following topics:</p>
<ul>
<li>Parameter initialization</li>
<li>Adjusting the loss function</li>
<li>Choosing an optimization method</li>
<li>Adjusting the learning rate</li>
<li>Gradient clipping, weight clipping, and more</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameter initialization</h1>
                </header>
            
            <article>
                
<p>Sometimes, one of the most frustrating things about learning about an optimization method from a book/paper and implementing it with code is that the initial state of the machine learning system (initial values of the parameters) can have a great impact on the model's final performance. It is important to have knowledge of parameter initialization, especially while you're dealing with deep networks. A good parameter initialization also means that you won't always rely on Batch Normalization to keep your parameters in line during training. To quote from the PyTorch documentation, <q><span class="ILfuVd"><span class="e24Kjd">"A PyTorch Tensor</span></span> is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients and is just a generic n-dimensional array to be used for arbitrary numeric computation." </q>This is why there can be so many methods, and there will probably be more in the future. </p>
<p>There are several popular parameter initialization methods. We won't go into great detail about some of the methods since they are rather self-explanatory. Note that the uniform distributions are often used for fully-connected layers and normal distributions are often used for convolution layers. Let's go over some of these now:</p>
<ul>
<li><strong>Uniform</strong> (<kbd>nn.init.uniform_(tensor, a, b)</kbd>): It initializes <kbd>tensor</kbd> with uniform distribution <img class="fm-editor-equation" src="assets/a1d9b168-6637-4d69-939a-080b4a682b7d.png" style="width:3.00em;height:1.25em;"/>.</li>
<li><strong>Normal</strong> (<kbd>nn.init.normal_(tensor, a, b)</kbd>): It initializes <kbd>tensor</kbd> with normal distribution <img class="fm-editor-equation" src="assets/5182f483-c3db-4fbe-b93b-38a492d61d29.png" style="width:4.25em;height:1.50em;"/>.</li>
<li><strong>Xavier-uniform</strong> (<kbd>nn.init.xavier_uniform_(tensor)</kbd>): It initializes <kbd>tensor</kbd> with uniform distribution <img class="fm-editor-equation" src="assets/a437bc40-4715-4245-91d3-274727b03efc.png" style="width:3.92em;height:1.25em;"/>, where we have the following equation:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c3254720-9876-4b51-a55a-8296537fa33d.png" style="width:18.58em;height:3.17em;"/></p>
<ul>
<li><strong>Xavier-normal</strong> (<kbd>nn.init.xavier_normal_(tensor)</kbd>): It initializes <kbd>tensor</kbd> with normal distribution <img class="fm-editor-equation" src="assets/bedac49b-9219-464a-9e7a-8f4daea194ea.png" style="width:5.17em;height:1.83em;"/>, where we have the following equation:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ebbdb2ac-1ca6-400c-9834-e384b0dabdfa.png" style="width:23.75em;height:3.42em;"/></p>
<ul>
<li><strong>He-uniform</strong> (that is, Kaiming-uniform or MSRA-uniform, <kbd>nn.init.kaiming_uniform_(tensor)</kbd>): It initializes <kbd>tensor</kbd> with uniform distribution <img class="fm-editor-equation" src="assets/abb3b28b-b777-4498-ae15-ee5a8c913d40.png" style="width:5.25em;height:1.67em;"/>, where we have the following equation:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2a3dccd9-1f8c-4e3e-bf9e-a17f3a5cd8af.png" style="width:7.50em;height:2.33em;"/></p>
<ul>
<li><strong>He-normal</strong> (that is, Kaiming-normal or MSRA-normal, <kbd>nn.init.kaiming_normal_(tensor)</kbd>): It initializes <kbd>tensor</kbd> with normal distribution <img class="fm-editor-equation" src="assets/ce9eeef0-e4de-4066-8df2-c9aa1440d577.png" style="width:4.92em;height:1.75em;"/>, where we have the following equation:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9af6ad08-48b9-4ebe-af0a-d5a3500fb822.png" style="width:14.42em;height:3.17em;"/></p>
<ul>
<li><strong>Truncated normal</strong>: In this method, all the values that are larger (or smaller) than twice the standard deviation (or negative twice the standard deviation) are discarded and regenerated.</li>
</ul>
<p>Besides using <kbd>torch.nn.init</kbd> to initialize your parameters, you can always create your own custom initializer. For example, here is an initializer that we can use for a convolution layer using <kbd>numpy</kbd> and <kbd>scipy.stats</kbd>:</p>
<pre>import numpy as np<br/>from scipy import stats<br/><br/>def initializer_conv(shape,<br/>                     init='he',<br/>                     dist='truncnorm',<br/>                     dist_scale=1.0):<br/>    w_width = shape[3]<br/>    w_height = shape[2]<br/>    size_in = shape[1]<br/>    size_out = shape[0]<br/><br/>    limit = 0.<br/>    if init == 'xavier':<br/>        limit = math.sqrt(2. / (w_width * w_height * (size_in + size_out))) * dist_scale<br/>    elif init == 'he':<br/>        limit = math.sqrt(2. / (w_width * w_height * size_in)) * dist_scale<br/>    else:<br/>        raise Exception('Arg `init` not recognized.')<br/>    if dist == 'norm':<br/>        var = np.array(stats.norm(loc=0, scale=limit).rvs(shape)).astype(np.float32)<br/>    elif dist == 'truncnorm':<br/>        var = np.array(stats.truncnorm(a=-2, b=2, scale=limit).rvs(shape)).astype(np.float32)<br/>    elif dist == 'uniform':<br/>        var = np.array(stats.uniform(loc=-limit, scale=2*limit).rvs(shape)).astype(np.float32)<br/>    else:<br/>        raise Exception('Arg `dist` not recognized.')<br/>    return var<br/><br/>class Conv2d(nn.Conv2d):<br/>    def __init__(self, in_channels, out_channels, kernel_size,<br/>                 stride=1, padding=0, dilation=1, groups=1, bias=True,<br/>                 init='he', dist='truncnorm', dist_scale=1.0):<br/>        super(Conv2d, self).__init__(<br/>            in_channels, out_channels, kernel_size, stride,<br/>            padding, dilation, groups, bias)<br/>        self.weight = nn.Parameter(torch.Tensor(<br/>            initializer_conv([out_channels, in_channels // groups, kernel_size, kernel_size],<br/>            init=init, dist=dist, dist_scale=dist_scale)))</pre>
<p>There are times when different initialization methods don't make too much of a difference when it comes to the model's final performance, as long as the parameters' magnitudes are kept at a similar level. In those cases, we suggest you try different initialization methods when even the slightest improvement matters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting the loss function</h1>
                </header>
            
            <article>
                
<p>The loss function describes the objective of the training process. We have seen many forms of loss functions in different GAN models, depending on their different goals. Designing the right loss function is crucial for the success of your model's training. Typically, a GAN model comes with two loss functions: one generator loss function and one discriminator loss function. Of course, if there are more than two networks in your model, there can be more loss functions to deal with. Each loss function can have one or more regularization term.<span> The three most common forms are as follows:</span></p>
<ul>
<li><img class="fm-editor-equation" src="assets/dc109b95-8583-44db-a503-760876ae014d.png" style="width:15.08em;height:1.83em;"/></li>
<li><img class="fm-editor-equation" src="assets/119d6d05-eac9-43cc-8c19-7b49a8e4f00b.png" style="width:8.58em;height:1.92em;"/></li>
<li><img class="fm-editor-equation" src="assets/a2085b18-6817-4532-9829-03ce24aa10c1.png" style="width:6.75em;height:1.83em;"/></li>
</ul>
<div class="packt_infobox"><span>In <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, </span><span><em>Image Restoration with GANs</em>, we will discuss different forms of loss functions in GANs at great length. Check it out to find out more.</span></div>
<p>The two most commonly used regularization terms are as follows:</p>
<ul>
<li>L1-loss, <img class="fm-editor-equation" src="assets/bc322e9e-09c8-4b6b-a7f2-30011ccaee06.png" style="width:2.75em;height:2.08em;"/></li>
<li>L2-loss, <img class="fm-editor-equation" src="assets/5cad3b3e-13ac-463e-bbb7-d3445735fccd.png" style="width:3.58em;height:2.42em;"/></li>
</ul>
<p>In L1-loss and L2-loss, <img class="fm-editor-equation" src="assets/256f5683-002c-4645-8c7d-9d42cd57151a.png" style="width:0.83em;height:1.08em;"/> can be many things, for example, the distance between two images or the gradients of an image. L2-loss tends to produce more dense results (where most values are closer to 0) and L1-loss produces more sparse results (where a few outliers with values larger than 0 are tolerated).</p>
<p>It is worth mentioning that L2-regularization (<strong>L2-penalty</strong>) on the parameters is essentially the same as <strong>weight decay</strong>. Here's why:</p>
<p><img class="alignnone size-full wp-image-795 image-border" src="assets/71efea1e-cc65-425f-9385-650162410bb6.png" style="width:458.75em;height:60.50em;"/></p>
<p>The second term in the first equation is L2-penalty and the second term in the second equation is weight decay. Taking derivatives on both sides of the first equation gives us the second equation. Therefore, L2-penalty and weight decay in neural networks are essentially the same thing.</p>
<p>The loss function is also where you bring your algorithm's design to life. For example, if you have extra label information for your dataset, add it to your loss function. If you want your results to be as similar to something as possible, add their distance to your regularization term. If you want the generated images to be smooth, add their gradients to the regularization term.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an optimization method</h1>
                </header>
            
            <article>
                
<p>Here, we will only discuss gradient-based optimization methods, which are most commonly used in GANs. Different gradient methods have their own strengths and weaknesses. There isn't a universal optimization method that can solve every problem. Therefore, we should choose them wisely when it comes to different practical problems. Let's have a look at some now:</p>
<ol>
<li><strong>SGD</strong> (calling <kbd>optim.SGD</kbd> with <kbd>momentum=0</kbd> and <kbd>nesterov=False</kbd>): It works fast and well for shallow networks. However, it can be very slow for deeper networks, and may not even converge for deep networks:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/39953f57-67a5-4361-9607-748c4f572db6.png" style="width:9.83em;height:1.25em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">In this equation, <img class="fm-editor-equation" src="assets/d5adb9ca-98b7-404d-9e65-0e9061eeefcf.png" style="width:1.00em;height:1.33em;"/> is the parameters at iteration step <img class="fm-editor-equation" src="assets/58bcd772-86a0-4daf-86d1-1dc8ce67a0e0.png" style="width:0.58em;height:1.25em;"/>, <img class="fm-editor-equation" src="assets/b3f70bb8-6957-4caa-823a-6341adea1a3e.png" style="width:0.75em;height:1.17em;"/> is the learning rate, and <img class="fm-editor-equation" src="assets/e96cb8f9-3813-4ed4-95e8-2df92711e335.png" style="width:1.58em;height:0.92em;"/> is the gradient of the objective function, <img class="fm-editor-equation" src="assets/adac4014-f788-4884-9b17-495d65b25b74.png" style="width:0.75em;height:1.08em;"/>.</p>
<ol start="2">
<li><strong>Momentm</strong> (calling <kbd>optim.SGD</kbd> with the <kbd>momentum</kbd> argument when it's larger than 0 and <kbd>nestrov=False</kbd>): It is one of the most commonly used optimization methods. This method combines the updates of the previous step with the gradient at the current step so that it takes a smoother trajectory than SGD. The training speed of Momentum is often faster than SGD and it generally works well for both shallow and deep networks:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/eedf5ead-b9dc-4682-8ea9-2e2accbe93d4.png" style="width:14.08em;height:3.17em;"/></p>
<p style="padding-left: 60px">In this equation, <img class="fm-editor-equation" src="assets/2283a896-1c74-4f0b-bb9b-67b63e538597.png" style="width:0.75em;height:1.00em;"/> is called the <strong>momentum term</strong>, which is usually set to a float value between 0.5~0.9.</p>
<ol start="3">
<li><strong>Nesterov</strong> (calling <kbd>optim.SGD</kbd> with the <kbd>momentum</kbd> argument when it's larger than 0 and <kbd>nestrov=True</kbd>); This is a variant of the Momentum method. It calculates a "predicted" gradient of the objective function at iteration step <img class="fm-editor-equation" src="assets/4cba62a2-742b-46c6-a646-923d2dfec5ce.png" style="width:3.08em;height:1.33em;"/>when combining the momentum vector and the gradient vector. In theory, it has a faster convergence speed than Momentum. When your model is having trouble converging with Momentum, you should definitely give Nesterov a try:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d41cb8f2-9c0d-475b-a2c7-e91008105bae.png" style="width:20.92em;height:3.58em;"/></p>
<p class="mce-root"/>
<ol start="4">
<li><strong>AdaGrad</strong> (<kbd>optim.Adagrad</kbd>): This method updates parameters that are updated more frequently with a smaller learning rate and updates the less frequently updated parameters with a larger learning rate. It was used by Google's DistBelief in 2012. However, AdaGrad isn't widely used today because the learning rate keeps getting smaller, which is bad for long-term training in deep models:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/24eecafb-1f9e-40a8-af1e-554c7a23b533.png" style="width:16.42em;height:2.92em;"/></p>
<p style="padding-left: 60px">In this equation, <img class="fm-editor-equation" src="assets/21a28bf7-ca7b-42c2-bdbe-54d2c68ac9cc.png" style="width:2.00em;height:1.83em;"/> is the total sum of the square of gradients starting from iteration step <img class="fm-editor-equation" src="assets/ce6f0af8-a100-4cd0-95b4-4beeb1906a7e.png" style="width:0.83em;height:1.50em;"/> to <img class="fm-editor-equation" src="assets/5d122fc6-f4a2-4233-b956-8745d1d0b82a.png" style="width:0.67em;height:1.42em;"/>, which increases over time and decreases the learning rate, while <img class="fm-editor-equation" src="assets/777b4eb8-7684-46ea-9832-c4b4b78bda66.png" style="width:0.67em;height:1.08em;"/> is a very small value.</p>
<ol start="5">
<li><strong>RMSprop</strong> (<kbd>optim.RMSprop</kbd>): This method is similar to AdaGrad, except that the moving average of squared gradients is taken instead of their sum. This method isn't very common among the various deep learning models. In <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, <em>Image Restoration with GANs</em>, we explicitly point out that RMSprop should be used in Wasserstein GAN:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aed919dc-a392-48da-bae6-5ed6a75702ed.png" style="width:30.25em;height:6.00em;"/></p>
<p style="padding-left: 60px">In this equation, <img class="fm-editor-equation" src="assets/d7a67373-1b4a-4ef3-bb93-f1b6adf104ee.png" style="width:3.58em;height:1.83em;"/> is the moving average of <img class="fm-editor-equation" src="assets/86d957a7-1720-40ae-85c3-33cd7d01c993.png" style="width:1.33em;height:1.75em;"/> until iteration step <img class="fm-editor-equation" src="assets/7d356d72-55bc-46aa-9fb2-5286a9760c74.png" style="width:0.50em;height:1.08em;"/>, while <img class="fm-editor-equation" src="assets/29f55b16-b928-41b8-938a-9a18d8e2922c.png" style="width:0.92em;height:1.42em;"/> is the smoothing term, which is usually set to a value very close to 1; for example, 0.99 or 0.999.</p>
<ol start="6">
<li><strong>Adam</strong> (<kbd>optim.Adam</kbd>): This method, in a sense, combines Momentum and RMSprop via two moment terms. It is one of the most popular and effective optimization methods in deep models. If all of the previous methods don't perform well in your model, Adam is your best chance, especially when your model is very deep and the relationships between the parameters are very complex (for example, you have multiple branching structures in your model):</li>
</ol>
<p>                                               <img class="alignnone size-full wp-image-796 image-border" src="assets/a1de511c-546c-442b-ad43-9e245f69c358.png" style="width:25.33em;height:10.17em;"/></p>
<p>In this equation, the moment coefficients (<img class="fm-editor-equation" src="assets/a73535fa-e377-4ba8-baf9-24d19639b573.png" style="width:1.25em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/985c7a0f-ab2d-49e8-99ed-cb053fc1b4c6.png" style="width:1.17em;height:1.25em;"/>) are normally set to values very close to 1, for example, <img class="fm-editor-equation" src="assets/18e09dee-dcc6-4326-bd05-7d6c10b85c95.png" style="width:4.92em;height:1.42em;"/> and <img class="fm-editor-equation" src="assets/d2bc4997-2449-47af-a539-e1aa9f51749f.png" style="width:6.25em;height:1.42em;"/>. The third line of the equation exists because we don't want the moment terms to be close to 0 at the beginning of the training, especially when they are normally initialized with zeros at <img class="fm-editor-equation" src="assets/938da320-c2b0-4745-ae19-7eb31937618d.png" style="width:2.33em;height:0.92em;"/>. Note that the learning rate for Adam should be dramatically smaller than other methods (such as Momentum).</p>
<p>In summary, you should try using Momentum when you're trying out your training strategy for a new model since it has fewer adjustable hyperparameters and is faster to train. When you feel happy with the model's performance, it is always worth trying Adam to exploit its potential even more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting the learning rate</h1>
                </header>
            
            <article>
                
<p>Now that you have selected an optimization method, you need to set the proper learning rate for the gradient method and start training. Normally, the updates for the parameters are significant enough to be noticeable at the beginning of the training step. After training for a long time, the relations between the parameters are determined and it's time to adjust the parameters subtly with a smaller learning rate. We cannot simply rely on an optimization method (such as RMSprop or Adam) to gradually decrease the learning rate for us. It is far more efficient when we actively decrease the learning rate periodically during training.</p>
<p class="mce-root"/>
<p>You can use <kbd>optim.lr_scheduler</kbd> to set up a <kbd>scheduler</kbd> and call <kbd>scheduler.step()</kbd> after each epoch, as shown in the following code:</p>
<pre>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)<br/>for epoch in range(epochs):<br/>    ...<br/>    scheduler.step()</pre>
<p>You can also create your own custom scheduler, as follows:</p>
<pre>class LRScheduleCosine(object):<br/>    def __init__(self, optimizer, epoch=0, epoch_start=0, lr_max=0.05, lr_min=0.001, t_mul=10):<br/>        self.optimizer = optimizer<br/>        self.epoch = epoch<br/>        self.lr_min = lr_min<br/>        self.lr_max = lr_max<br/>        self.t_start = epoch_start<br/>        self.t_mul = t_mul<br/>        self.lr = lr_max<br/><br/>    def step(self):<br/>        self.epoch += 1<br/>        self.lr = self.lr_min + 0.5*(self.lr_max-self.lr_min)*(1.+math.cos(math.pi*(self.epoch-self.t_start)/self.t_mul))<br/>        if self.optimizer is not None:<br/>            for param_group in self.optimizer.param_groups:<br/>                param_group['lr'] = self.lr<br/>        if self.epoch == self.t_start + self.t_mul:<br/>            self.t_start += self.t_mul<br/>            self.t_mul *= 2<br/>        return self.lr</pre>
<p>This is an implementation of the cosine schedule with warm restarts. To use it in your training, simply call it, as follows:</p>
<pre>scheduler = LRScheduleCosine(optimizer,<br/>                                       lr_max=0.025,<br/>                                       lr_min=0.001,<br/>                                       t_mul=10)<br/>for epoch in range(epochs):<br/>    lr = scheduler.step()<br/>    ...</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The learning rate will decrease from 0.025 to 0.001 in the first 10 epochs, restart at 0.025 and decrease to 0.001 in the next 10 epochs, then restart back at 0.025 and decrease to 0.001 in the next 40 epochs, and so on.</p>
<div class="packt_infobox">You can check out PyTorch's official documentation to find out more about other types of schedulers: <a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient clipping, weight clipping, and more</h1>
                </header>
            
            <article>
                
<p>In the very first chapter of this book, <a href="66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml">Chapter 1</a>, <em>Generative Adversarial Networks Fundamentals</em>, we created a simple GAN with NumPy to generate sine signals using gradient clipping and weight clipping to make sure the training converged. Let's go over why these tricks can be useful for your models:</p>
<ul>
<li><strong>Gradient clipping</strong>: The gradient basically tells us how to update our parameters. Normally, larger gradients lead to bigger changes being applied to our parameters. If, by chance, the loss surface around our search location is steep, large gradient values could mean that we will jump far away from this region at the next iteration step and we'll have to start looking for optimal solutions in a new region. Therefore, clipping gradients and setting limitations on their maximum/minimum values can make sure that we don't jeopardize our previous search results while spending a long time training. You can use <kbd>nn.utils.clip_grad_norm_</kbd> to perform gradient clipping.</li>
<li><strong>Vanishing Gradients</strong>: When the change in the gradients is too small, this can cause problems as well. Often, this is because the inputs are simply too compressed to allow the system to learn correctly. If this seems to be happening, consider using <strong>ReLU</strong> or Leaky ReLU, which we introduced in <a href="66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml">Chapter 1</a>, <em>Generative Adversarial Networks Fundamentals</em>.</li>
<li><strong>Weight clipping</strong>: This is not a widely used technique, besides its application in the Wasserstein GAN (<a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7,</a> <em>Image Restoration with GANs</em>). It is an indirect way to perform gradient clipping. Therefore, it is not necessary to use both techniques in the same model. We only used both in the example in <a href="66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml">Chapter 1</a>, <em>Generative Adversarial Networks Fundamentals</em>, to make sure that nothing went wrong in our model.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Efficient coding in Python</h1>
                </header>
            
            <article>
                
<p>Most of the code you will see in this book is written in Python. Almost all of the popular deep learning tools (PyTorch, TensorFlow, Keras, MXNet, and so on) are also written in Python. Python is easy to learn and easy to use, especially compared to other <strong>object-oriented programming</strong> (<strong>OOP</strong>) languages such as C++ and Java. However, using Python does not excuse us from lazy coding. We should never settle with <em>it works</em>. In deep learning, efficient code may save us hours of training time. In this section, we will give you some tips and advice on writing efficient Python projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinventing the wheel wisely</h1>
                </header>
            
            <article>
                
<p>Innovative developers are not enthusiastic about reinventing the wheel, that is, implementing every tiny component in the project that can be easily grabbed from GitHub or third-party libraries. Deep learning relies on being open source and anyone in the world can learn and do cool things with it. <span>We encourage you to take advantage of any </span><span>available tool you can find to solve your practical problems, as long as it saves your invaluable time. Some of the model implementations in this book come from other people's projects on GitHub. Imagine how long it would take us to figure out all the implementation details based on the papers that have already been published!</span></p>
<p>Here are some websites that may come in handy when you are looking for specific tools or code snippets:</p>
<ul>
<li><a href="https://github.com">https://github.com</a></li>
<li><a href="https://stackoverflow.com">https://stackoverflow.com</a></li>
<li><a href="https://stats.stackexchange.com">https://stats.stackexchange.com</a></li>
<li><a href="https://discuss.pytorch.org">https://discuss.pytorch.org</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning">https://www.reddit.com/r/MachineLearning</a></li>
<li><a href="https://www.kaggle.com/kernels">https://www.kaggle.com/kernels</a></li>
<li>Finally, the most important one: <a href="https://www.google.com">https://www.google.com</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advice for beginners in deep learning</h1>
                </header>
            
            <article>
                
<p>The following is some advice that beginners in deep learning should definitely follow:</p>
<ul>
<li><strong>Set reasonable but solid goals and deadlines</strong>:<strong> </strong>Give yourself plenty of time to research, learn, and experiment with a subject. Start with the goal and then create a series of steps that will achieve that goal. Keep a log of your progress.</li>
<li><strong>Search the web to find information on the project you are working on</strong>: The internet is often the fastest way to gather information about a particular subject. Start with simple but direct search text and then refine your searches to obtain the best resources.</li>
<li><strong>Small steps are better than huge leaps</strong>: As you read an article or chapter on your subject of choice, copy the code into your IDE and run the project. Don't move on until you understand the inputs, outputs, and the code that produces them.</li>
<li><strong>Try to find pretrained models</strong>: Once you have the basic information and understand the model process, use pretrained models to save time and hardware resources. Again, keep the results in your log.</li>
<li><strong>Take the results from your searches and experiment on your own</strong>: It's likely that you will gather ideas about the subject as you do your research and testing. Jot them down and test your ideas against what you have learned.</li>
<li><strong>Get the best hardware that you can afford without breaking the bank</strong>: This is probably the most important tip. A good computer with a good graphics card and GPU with as much memory as possible will potentially cut hours off your process.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the overall design of the model architecture and the steps that are required when it comes to choosing the best convolution operation.</p>
<p>In the next chapter, we will introduce a classic performant GAN model called DCGAN, which is used for generating 2D images.</p>


            </article>

            
        </section>
    </body></html>