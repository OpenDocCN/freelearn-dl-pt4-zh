- en: Implementing an Intelligent - Autonomous Car Driving Agent using Deep Actor-Critic
    Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), Implementing
    an Intelligent Agent for Optimal Control using Deep Q-Learning, we implemented
    agents using deep Q-learning to solve discrete control tasks that involve discrete
    actions or decisions to be made. We saw how they can be trained to play video
    games such as Atari, just like we do: by looking at the game screen and pressing
    the buttons on the game pad/joystick. We can use such agents to pick the best
    choice given a finite set of choices, make decisions, or perform actions where
    the number of possible decisions or actions is finite and typically small. There
    are numerous real-world problems that can be solved with an agent that can learn
    to take optimal through to discrete actions. We saw some examples in [Chapter
    6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing an Intelligent
    Agent for* *Optimal Discrete Control using Deep Q-Learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, there are other classes of problems and tasks that require
    lower-level actions to be performed that are continuous values and not discrete.
    For example, an intelligent temperature control system or a thermostat needs to
    be capable of making fine adjustments to the internal control circuits to maintain
    a room at the specified temperature. The control action signal may include a continuous
    valued real number (such as *1.456*) to control **heating, ventilation, and air
    conditioning** (**HVAC**) systems. Consider another example in which we want to
    develop an intelligent agent to drive a car autonomously. Humans drive a car by
    shifting gears, pressing the accelerator or brake pedal, and steering the car.
    While the current gear is going to be one of a possible set of five to six values,
    depending on the transmission system of the car, if an intelligent software agent
    has to perform all of those actions, it has to be able to produce continuous valued
    real numbers for the throttle (accelerator), braking (brake), and steering.
  prefs: []
  type: TYPE_NORMAL
- en: In cases like these examples, where we need the agent to take continuous valued
    actions, we can use policy gradient-based actor-critic methods to directly learn
    and update the agent's policy in the policy space, rather than through a state
    and/or action value function like in the deep Q-learning agent we saw in [Chapter
    6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing an Intelligent
    Agent for Optimal Discrete Control using Deep Q-Learning*. In this chapter, we
    will start from the basics of an actor-critic algorithm and build our agent gradually,
    while training it to solve various classic control problems using OpenAI Gym environments
    along the way. We will build our agent all the way up to being able to drive a
    car in the CARLA driving simulation environment using the custom Gym interface
    that we implemented in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The deep n-step advantage actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our deep Q-learner-based intelligent agent implementation, we used a deep
    neural network as the function approximator to represent the action-value function.
    The agent then used the action-value function to come up with a policy based on
    the value function. In particular, we used the ![](img/00158.jpeg)-greedy algorithm
    in our implementation. So, we understand that ultimately the agent has to know
    what actions are good to take given an observation/state. Instead of parametrizing
    or approximating a state/action action function and then deriving a policy based
    on that function, can we not parametrize the policy directly? Yes we can! That
    is the exact idea behind policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will briefly look at policy gradient-based
    learning methods and then transition to actor-critic methods that combine and
    make use of both value-based and policy-based learning. We will then look at some
    of the extensions to the actor-critic method that have been shown to improve learning
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In policy gradientbased methods, the policy is represented, for example, by
    using a neural network with parameters ![](img/00159.jpeg), and the goal is to
    find the best set of parameters ![](img/00160.jpeg). This can be intuitively seen
    as an optimization problem where we are trying to optimize the objective of the
    policy to find the best-performing policy. What is the objective of the agent's
    policy ? We know that the agent should achieve maximum rewards in the long term,
    in order to complete the task or achieve the goal. If we can formulate that objective
    mathematically, we can use optimization techniques to find the best policy for
    the agent to follow for the given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the state value function ![](img/00161.jpeg) tells us the expected
    return starting from state ![](img/00162.jpeg)  and following policy ![](img/00163.jpeg) until
    the end of the episode. It tells us how good it is to be in state ![](img/00164.jpeg).
    So ideally, a good policy will have a higher value for the starting state in the
    environment as it represents the expected/mean/average value of being in that
    state and taking actions according to policy ![](img/00165.jpeg) until the end
    of the episode. The higher the value in the starting state, the higher the total
    long-term reward an agent following the policy can achieve. Therefore, in an episodic
    environment—where the environment is an episode; that is, it has a terminal state—we
    can measure how good a policy is based on the value of the start state. Mathematically,
    such an objective function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'But what if the environment is not episodic? This means it doesn''t have a
    terminal state and keeps on going. In such as environment, we can use the average
    value of the states that are visited while following the current policy, ![](img/00167.jpeg).
    Mathematically, the average value objective function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/00169.jpeg) is the stationary distribution of the Markov chain
    for ![](img/00170.jpeg), which gives the probability of visiting state ![](img/00171.jpeg) while
    following policy ![](img/00172.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the average reward obtained per time step in such environments,
    which can be expressed mathematically using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is essentially the expected value of rewards that can be obtained when
    the agent takes actions based on policy ![](img/00174.jpeg), which can be written
    in short form like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To optimize this policy objective function using gradient descent, we would
    take the derivative of the equation with respect to ![](img/00176.jpeg), find
    the gradients, back-propagate, and perform the gradient descent step. From the
    previous equations, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s differentiate the previous equation with respect to  ![](img/00178.jpeg) 
    by expanding the terms and then simplifying it further. Follow the following equations
    from left to right to understand the series of steps involved in arriving at the
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand these equations and how the policy gradient, ![](img/00180.jpeg),
    is equal to the likelihood ratio, ![](img/00181.jpeg), let''s take a step back
    and revisit what our goal is. Our goals is to find the optimal set of parameters ![](img/00182.jpeg) for
    the policy so that the agent following the policy will reap the maximum rewards
    in expectation (i.e on an average average). To achieve that goal, we start with
    a set of parameters and then keep updating the parameters until we reach the optimal
    set of parameters. To figure out which direction in the parameter space the policy
    parameters have to be updated, we make use of the direction indicated by the gradient
    of policy ![](img/00183.jpeg) with respect to parameters ![](img/00184.jpeg).
    Let''s start with the second term in the previous equation, ![](img/00185.jpeg),
    (which was a result of the first term, ![](img/00186.jpeg), by definition):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg) is the gradient of the expected value, under policy ![](img/00188.jpeg),
    of the step reward that resulted from taking action ![](img/00189.jpeg) in state ![](img/00190.jpeg).
    This, by the definition of expectation, can be written as the following sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We'll look at the likelihood ratio trick, which is used in this context to transform
    this equation into a form that makes the computation feasible.
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood ratio trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The policy represented by ![](img/00192.jpeg) is assumed to be a differentiable
    function whenever it is non-zero, but computing the gradient of the policy with
    respect to theta, ![](img/00193.jpeg), may not be straightforward. We can multiply
    and divide by policy ![](img/00194.jpeg) on both sides to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From calculus, we know that the gradient of the log of a function is the gradient
    of the function over the function itself, which is mathematically given by the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we can write the gradient of the policy with respect to its parameters
    in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is called the likelihood ratio trick, or the log derivative trick, in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because policy ![](img/00198.jpeg) is a probability distribution function that
    describes the probability distribution over actions given the state and parameters
    ![](img/00199.jpeg) by definition, the double summation terms over the states
    and the actions can be expressed as the expectation of the score function scaled
    by reward ![](img/00200.jpeg) over distribution ![](img/00201.jpeg). This is mathematically
    equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding equation, ![](img/00203.jpeg) is the step reward
    for taking action ![](img/00204.jpeg) from state ![](img/00205.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy gradient theorem generalizes this approach by replacing the instantaneous
    step reward ![](img/00206.jpeg) with the long-term action value ![](img/00207.jpeg) and
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This result is a very useful one and forms the basis of several variations of
    the policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: With this understanding of policy gradients, we will dive into actor-critic
    algorithms and their variations in the following few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with a diagrammatic representation of the actor-critic architecture,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two components in the actor-critic algorithm, as evident from the
    name and the preceding diagram. The actor is responsible for acting in the environment,
    which involves taking actions, given observations about the environment and based
    on the agent''s policy. The actor can be thought of as the policy holder/maker.
    The critic, on the other hand, takes care of estimating the state-value, or state-action-value,
    or advantage-value function (depending on the variant of the actor-critic algorithm
    used). Let''s consider a case where the critic is trying to estimate the action
    value function ![](img/00210.jpeg). If we use a set of parameters *w* to denote
    the critic''s parameters, the critic''s estimates can be essentially written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Replacing the true action-value function with the critic''s approximation of
    the action-value function (the last equation in the policy gradient theorem section)
    in the results of the policy gradient theorem from the previous section leads
    us to the approximate policy gradient given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In practice, we further approximate the expectation using stochastic gradient
    ascent (or descent with a -ve sign).
  prefs: []
  type: TYPE_NORMAL
- en: Advantage actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The action-value actor-critic algorithm still has high variance. We can reduce
    the variance by subtracting a baseline function, *B(s)*, from the policy gradient.
    A good baseline is the state value function, ![](img/00213.jpeg). With the state
    value function as the baseline, we can rewrite the result of the policy gradient
    theorem as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can define the advantage function ![](img/00215.jpeg) to be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'When used in the previous policy gradient equation with the baseline, this
    gives us the advantage of the actor-critic policy gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall from the previous chapters that the 1-step Temporal Difference (TD)
    error for value function ![](img/00218.jpeg) is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00219.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we compute the expected value of this TD error, we will end up with an equation
    that resembles the definition of the action-value function we saw in [Chapter
    2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12), *Reinforcement Learning
    and Deep Reinforcement Learning*. From that result, we can observe that the TD
    error is in fact an unbiased estimate of the advantage function, as derived in
    this equation from left to right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: With this result and the previous set of equations in this chapter so far, we
    have enough theoretical background to get started with our implementation of our
    agent! Before we get into the code, let's understand the flow of the algorithm
    to get a good picture of it in our minds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest (general/vanilla) form of the advantage actor-critic algorithm
    involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the (stochastic) policy and the value function estimate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a given observation/state ![](img/00221.jpeg), perform the action, ![](img/00222.jpeg),
    prescribed by the current policy, ![](img/00223.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the TD error based on the resulting state, ![](img/00224.jpeg) and the
    reward  ![](img/00225.jpeg) obtained using the 1-step TD learning equation:![](img/00226.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the actor by adjusting the action probabilities for state ![](img/00227.jpeg) based
    on the TD error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If ![](img/00228.jpeg) > 0, increase the probability of taking action ![](img/00229.jpeg) because ![](img/00230.jpeg) was
    a good decision and worked out really well
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If ![](img/00231.jpeg) < 0 , decrease the probability of taking action ![](img/00232.jpeg) because ![](img/00233.jpeg) resulted
    in a poor performance by the agent
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update the critic by adjusting its estimated value of ![](img/00234.jpeg) using
    the TD error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00235.jpeg), where ![](img/00236.jpeg) is the critic''s learning rate'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the next state ![](img/00237.jpeg) to be the current state ![](img/00238.jpeg) and
    repeat step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: n-step advantage actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the advantage actor-critic algorithm section, we looked at the steps involved
    in implementing the algorithm. In step 3, we noticed that we have to calculate
    the TD error based on the 1-step return (TD target). It is like letting the agent
    take a step in the environment and then based on the outcome, calculating the
    error in the critic's estimates and updating the policy of the agent. This sounds
    straightforward and simple, right? But, is there any better way to learn and update
    the policy? As you might have guessed from the title of this section, the idea
    is to use the n-step return, which uses more information to learn and update the
    policy compared to 1-step return-based TD learning. n-step TD learning can be
    seen as a generalized version and the 1-step TD learning used in the actor-critic
    algorithm, as discussed in the previous section, is a special case of the n-step
    TD learning algorithm with n=1\. Let's look at a quick illustration to understand
    the n-step return calculation and then implement a Python method to calculate
    the n-step return, which we will use in our agent implementation.
  prefs: []
  type: TYPE_NORMAL
- en: n-step returns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'n-step returns are a simple but very useful concept known to yield better performance
    for several reinforcement learning algorithms, not just with the advantage actor-critic-based
    algorithm. For example, the best performing algorithm to date on the Atari suite
    of *57* games, which significantly outperforms the second best algorithm, uses
    n-step returns. We will actually discuss that agent algorithm, called Rainbow,
    in [Chapter 10](part0173.html#54VHA0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the learning environment landscape: Roboschool, Gym-Retro, StarCraft-II, DMLab*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first get an intuitive understanding of the n-step return process. Let''s
    use the following diagram to illustrate one step in the environment. Assume that
    the agent is in state ![](img/00239.jpeg) at time t=1 and decides to take action ![](img/00240.jpeg),
    which results in the environment being transitioned to state ![](img/00241.jpeg) at
    time t=t+1= 1+1 = 2 with the agent receiving a reward of ![](img/00242.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the 1-step TD return using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00244.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/00245.jpeg) is the value estimate of state ![](img/00246.jpeg) according
    to the value function (critic). In essence, the agent takes a step and uses the
    received return and the discounted value of the agent's estimate of the value
    of the next/resulting state to calculate the return.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we let the agent continue interacting with the environment for a few more
    steps, the trajectory of the agent can be simplistically represented using the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This diagram shows a 5-step interaction between the agent and the environment.
    Following a similar approach to the 1-step return calculation in the previous
    paragraph, we can calculate the 5-step return using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can then use this as the TD target in step 3 of the advantage actor-critic
    algorithm to improve the performance of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the performance of the advantage actor-critic agent with the
    1-step return compares to the performance with the n-step return by running the
    `ch8/a2c_agent.py` script with the `learning_step_thresh` parameter in the `parameters.json`
    file set to 1 (for the 1-step return) and 5 or 10 (for the n-step return) in any
    of the Gym environments.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can run
  prefs: []
  type: TYPE_NORMAL
- en: '`(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8$python a2c_agent.py --env Pendulum-v0`
    with `learning_step_thresh=1`, monitor its performance using Tensorboard using
    the command'
  prefs: []
  type: TYPE_NORMAL
- en: '`(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch8/logs$tensorboard --logdir=.`, and
    then after a million or so steps you can compare the performance of the agent
    trained with `learning_step_thresh=10`. Note that the trained agent model will
    be saved at `~/HOIAWOG/ch8/trained_models/A2_Pendulum-v0.ptm` . You can rename
    it or move it to a different directory before you start the second run to start
    the training from scratch!'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the concept more explicit, let''s discuss how we will use this in step
    3 and in the advantage actor-critic algorithm. We will first use the n-step return
    as the TD target and calculate the TD error using the following formula (step
    3 of the algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00249.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then follow step 4 in the algorithm discussed in the previous subsection
    and update the critic. Then, in step 5, we will update the critic using the following
    update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00250.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will then move on to step 6 of the algorithm to continue with the next state, ![](img/00251.jpeg),
    using 5-step transitions from ![](img/00252.jpeg) until ![](img/00253.jpeg) and calculating
    the 5-step return, then repeat the procedure for updating ![](img/00254.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the n-step return calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we pause for a moment and analyze what is happening, you might see that
    we are probably not making full use of the 5-step long trajectory. With access
    to the information from the agent''s 5-step long trajectory starting from state ![](img/00255.jpeg),
    we only ended up learning one new piece of information, which is all about ![](img/00256.jpeg)to
    update the actor and the critic (![](img/00257.jpeg)). We can actually make the
    learning process more efficient by using the same 5-step trajectory to calculate
    updates for each of the state values present in the trajectory, with their respective
    *n* values based on the end of the trajectory. For example, in a simplified trajectory
    representation, if we considered state ![](img/00258.jpeg), with the forward-view
    of the trajectory enclosed inside the bubble, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00259.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the information inside the bubble to extract the TD learning target
    for state ![](img/00260.jpeg). In this case, since there is only one step of information
    available from ![](img/00261.jpeg), we will be calculating the 1-step return,
    as shown in this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00262.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we discussed before, we can use this value as the TD target in equation to
    get another TD error value, and use the second value to update the actor and ![](img/00263.jpeg),
    in addition to previous updates (![](img/00264.jpeg)). Now, we have got one more
    piece of information for the agent to learn from!
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply the same intuition and consider state ![](img/00265.jpeg), with
    the forward-view of the trajectory enclosed in the bubble, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00266.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the information inside the bubble to extract the TD learning target
    for ![](img/00267.jpeg). In this case, there are two types of information available
    from ![](img/00268.jpeg); therefore, we will calculate the 2-step return using
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00269.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at this equation and the previous equation, we can observe that
    there is a relationship between ![](img/00270.jpeg) and ![](img/00271.jpeg), which
    is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00272.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us another piece of information for the agent to learn from. Likewise,
    we can extract more information from this one trajectory of the agent. Extending
    the same concept for ![](img/00273.jpeg) and ![](img/00274.jpeg), we can arrive
    at the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00275.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Likewise, in short, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And finally, we can also observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00277.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Simply put, we can start from the last step in the trajectory, calculate the
    n-step return until the end of the trajectory, and then move back to the previous
    step to calculate the return using the previously calculated value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation is straightforward and simple, and it is advisable to try
    to implement this on your own. It is provided as follows for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Deep n-step advantage actor-critic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We observed that the actor-critic algorithm combines value-based methods and
    policy-based methods. The critic estimates the value function and the actor follows
    the policy, and we looked at how we can update the actor and the critic. From
    our previous experience in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for optimal discrete control using Deep Q Learning*, we naturally
    got the idea of using a neural network to approximate the value function and therefore
    the critic. We can also use a neural network to represent policy ![](img/00278.jpeg),
    in which case parameters ![](img/00279.jpeg) are the weights of the neural network.
    Using deep neural networks to approximate the actor and the critic is exactly
    the idea behind deep actor-critic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep n-step advantage actor critic agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have prepared ourselves with all the background information required to implement
    the deep n-step advantage actor-critic (A2C) agent. Let's look at an overview
    of the agent implementation process and then jump right into the hands-on implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the high-level flow of our A2C agent:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the actor's and critic's networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the current policy of the actor to gather n-step experiences from the environment
    and calculate the n-step return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the actor's and critic's losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the stochastic gradent descent optimization step to update the actor
    and critic parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will implement the agent in a Python class named `DeepActorCriticAgent`.
    You will find the full implementation in this book's code repository under 8th
    chapter: `ch8/a2c_agent.py`. We will make this implementation flexible so that
    we can easily extend it further for the batched version, as well make an asynchronous
    version of the n-step advantage actor-critic agent.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the actor and critic networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `DeepActorCriticAgent` class's initialization is straightforward. We will
    quickly have a look into it and then see how we actually define and initialize
    the actor and critic networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent''s initialization function is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may wonder why the `agent` class is inheriting from the `multiprocessing.Process`
    class. Although for our first agent implementation we will be running one agent
    in one process, we can use this flexible interface to enable running several agents
    in parallel to speed up the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to actor and critic implementations using neural networks defined
    with PyTorch operations. Following a similar code structure to what we used in
    our deep Q-learning agent in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Agent for optimal discrete control using Deep Q Learning*,
    in the code base you will see that we are using a module named `function_approximator`
    to contain our neural network-based function approximator implementations. You
    can find the full implementations under the `ch8/function_approximator` folder
    in this book's code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since some environments have small and discrete state spaces, such as the `Pendulum-v0`, `MountainCar-v0`,
    or `CartPole-v0` environments, we will also implement shallow versions of neural
    networks along with the deep versions, so that we can dynamically choose a suitable
    neural network depending on the environment the agent is trained/tested on. When
    you look through the sample implementation of the neural networks for the actor,
    you will notice that in both the `shallow` and the `deep` function approximator
    modules, there is a class called `Actor` and a different class called `DiscreteActor`.
    This is again for generality purposes so that we can let the agent dynamically
    pick and use the neural network most suitable for representing the actor, depending
    on whether the environment''s action space is continuous or discrete. There is
    one more variation for the completeness and generality of our agent implementation
    that you need to be aware of: both the `shallow` and the `deep` function approximator
    modules in our implementations have an `ActorCritic` class, which is a single
    neural network architecture that represents both the actor and the critic. In
    this way, the feature extraction layers are shared between the actor and the critic,
    and different heads (final layers) in the same neural network are used to represent
    the actor and the critic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, the different parts of the implementation might be confusing. To
    help avoid confusion, here is a summary of the various options in our neural network-based
    actor-critic implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Module/class** | **Description** | **Purpose/use case** |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. `function_approximator.shallow` |  Shallow neural network implementations
    for actor-critic representations. | Environments that have low-dimensional state/observation
    spaces. |  |'
  prefs: []
  type: TYPE_TB
- en: '|  1.1 `function_approximator.shallow.Actor` |  Feed-forward neural network
    implementation that produces two continuous values: mu (mean) and sigma for a
    Gaussian distribution-based policy representation. | Low-dimensional state/observation
    space and continuous action space. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1.2 `function_approximator.shallow.DiscreteActor` | Feed-forward neural network
    that produces a logit for each action in the action space. | Low-dimensional state/observation
    space and discrete action space. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1.3 `function_approximator.shallow.Critic` | Feed-forward neural network
    that produces a continuous value. | Used to represent the critic/value function
    for environments with low-dimensional state/observation space |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1.4 `function_approximator.shallow.ActorCritic` | Feed-forward neural network
    that produces mu (mean), sigma for a Gaussian distribution, and a continuous value.
    | Used to represent the actor and the critic in the same network for environments
    with low-dimensional state/observation space. It is possible to modify this to
    a discrete actor-critic network. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. `function_approximator.deep` | Deep neural network implementations for
    actor, critic representation. | Environments that have high-dimensional state/observation
    spaces. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2.1 `function_approximator.deep.Actor` | Deep convolutional neural network
    that produces the mu (mean) and sigma for a Gaussian distribution-based policy
    representation. | High-dimensional state/observation space and continuous action
    space. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2.2 `function_approximator.deep.DiscreteActor` | Deep convolutional neural
    network that produces a logit for each action in the action space. | High-dimensional
    state/observation space and discrete action-space. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2.3 `function_approximator.deep.Critic` | Deep convolutional neural network
    that produces a continuous value. | Used to represent the critic/value-function
    for environments with high-dimensional state/observation space. |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2.4 `function_approximator.deep.ActorCritic` | Deep convolutional neural
    network that produces mu (mean), sigma for a Gaussian distribution as well as
    a continuous value. | Used to represent the actor and the critic in a same network
    for environments with high-dimensional state/observation space. It is possible
    to modify this to a discrete actor-critic network. |  |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s now look at the first part of the `run()` method, where we initialize
    the actor and the critic network based on the type of the environment''s state
    and action spaces, as well as based on whether the state space is low-dimensional
    or high-dimensional based on the previous table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Gathering n-step experiences using the current policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to perform what are called *rollouts* using the current policy
    of the agent to collect `n` number of transitions. This process is basically letting
    the agent interact with the environment and generating new experiences in terms
    of state transitions, usually represented as a tuple containing the state, action,
    reward obtained, and next state, or in short `(![](img/00280.jpeg), ![](img/00281.jpeg), ![](img/00282.jpeg), ![](img/00283.jpeg))`,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00284.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the example shown in the preceding diagram, the agent would fill its `self.trajectory`
    with a list of the five transitions like this: `[T1, T2, T3, T4, T5]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our implementation, we will use a slightly modified transition representation
    to reduce redundant calculations. We will use the following definition to represent
    a transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Transition = namedtuple("Transition", ["s", "value_s", "a", "log_prob_a"])`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `s` is the state, `value_s` is the critic's prediction of the value of
    state `s`, `a` is the action taken, and `log_prob_a` is the logarithm of the probability
    of taking action `a` according to the actor/agent's current policy.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `calculate_n_step_return(self, n_step_rewards, final_state,
    done, gamma)` method we implemented previously to calculate the n-step return
    based on the `n_step_rewards` list containing the scalar reward values obtained
    at each step in the trajectory and the `final_state` used to calculate the critic's
    estimate value of the final/last state in the trajectory, as we discussed earlier
    in the n-step return calculation section.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the actor's and critic's losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the description of the n-step deep actor-critic algorithm we went over
    previously, you may remember that the critic, represented using a neural network,
    is trying to solve a problem that is similar to what we saw in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for Optimal Discrete Control using Deep Q-Learning*, which
    is to represent the value function (similar to the action-value function we used
    in this chapter, but a bit simpler). We can use the standard **Mean Squared Error**
    (**MSE**) loss or the smoother L1 loss/Huber loss, calculated based on the critic's
    predicted values and the n-step returns (TD targets) computed in the previous
    step.
  prefs: []
  type: TYPE_NORMAL
- en: For the actor, we will use the results obtained with the policy gradient theorem,
    and specifically the advantage actor-critic version, where the advantage value
    function is used to guide the gradient updates of the actor policy. We will use
    the TD_error, which is an unbiased estimate of the advantage value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the critic''s and actor''s losses are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*critic_loss = MSE(![](img/00285.jpeg), critic_prediction)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*actor_loss = log(![](img/00286.jpeg)) * TD_error*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the main loss calculation equations captured, we can implement them in
    code using the `calculate_loss(self, trajectory, td_targets)` method, as illustrated
    by the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Updating the actor-critic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we have calculated the losses for the actor and critic, the next and
    final step in the learning process is to update the actor and critic parameters
    based on their losses. Since we use the awesome PyTorch library, which takes care
    of the partial differentiation, back-propagation of errors, and gradient calculations
    automatically, the implementation is simple and straightforward using the results
    from the previous steps, as shown in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tools to save/load, log, visualize, and monitor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we walked through the core part of the agent's learning
    algorithm implementation. Apart from those core parts, there are a few utility
    functions that we will use to train and test the agent in different learning environments.
    We will reuse the components that we already developed in [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12),
    *Implementing an Intelligent Agent for optimal discrete control using Deep Q Learning*,such
    as the `utils.params_manager`, and also the `save()` and `load()` methods, which
    respectively save and load the agent's trained brain or model. We also will make
    use of the logging utilities to log the agent's progress in a format that is usable
    with Tensorboard for a nice and quick visualization, as well as for debugging
    and monitoring to see whether there is something wrong with our agent's training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we can complete our implementation of the n-step advantage actor-critic
    agent! You can find the full implementation in the `ch8/a2c_agent.py` file. Before
    we see how we can train the agent, in the next section we will quickly look at
    one of the extensions we can apply to the deep n-step advantage agent to make
    it perform even better on multi-core machines.
  prefs: []
  type: TYPE_NORMAL
- en: An extension - asynchronous deep n-step advantage actor-critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One easy extension that we can make to our agent implementation is to launch
    several instances of our agent, each with their own instance of the learning environment,
    and send back updates from what they have learned in an asychronous manner, that
    is, whenever they are available, without any need for time syncing. This algorithm is
    popularly known as the A3C algorithm, which is short for asynchronous advantage
    actor-critic.
  prefs: []
  type: TYPE_NORMAL
- en: One of the motivations behind this extension stems from what we learned in [Chapter
    6, ](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12)*Implementing an Intelligent
    Agent for optimal discrete control using Deep Q Learning*, with the use of the
    experience replay memory. Our deep Q-learning agent was able to learn considerably
    better with the addition of experience replay memory, which in essence helped
    in decorrelating the dependencies in the sequential decision making problem, and
    letting the agent extract more juice/information from its past experience. Similarly,
    the idea behind using multiple actor-learner instances running in parallel is
    found to be helpful in breaking the correlation between the transitions, and also
    helps in the exploration of different parts of the state space in the environment,
    as each actor-learner process has its own set of policy parameters and environment
    instance to explore. Once the agent instances running in parallel have some updates
    to send back, they send them over to a shared, global agent instance, which then
    acts as the new parameter source for the other agent instances to sync from.
  prefs: []
  type: TYPE_NORMAL
- en: We can use Python's PyTorch multiprocessing library to implement this extension.
    Yes! You guessed it right. This was the reason our `DeepActorCritic` agent in
    our implementation subclassed `torch.multiprocessing.Process` right from the start,
    so that we can add this extension to it without any significant code refactoring.
    You can look at the `ch8/README.md` file in the book's code repository for more
    resources on exploring this architecture if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily extend our n-step advantage actor-critic agent implementation
    in `a2c_agent.py` to implement the synchronous deep n-step advantage actor-critic
    agent. You can find the asynchronous implementation in `ch8/async_a2c_agent.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Training an intelligent and autonomous driving agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have all the pieces we need to accomplish our goal for this chapter,
    which is to put together an intelligent, autonomous driving agent, and then train
    it to drive a car autonomously in the photo-realistic CARLA driving environment
    that we developed as a learning environment using the Gym interface in the previous
    chapter. The agent training process can take a while. Depending on the hardware
    of the machine that you are going to train the agent on, it may take anywhere
    from a few hours for simpler environments (such as`Pendulum-v0`, `CartPole-v0`,
    and some of the Atari games) to a few days for complex environments (such as the
    CARLA driving environment). In order to first get a good understanding of the
    training process and how to monitor progress while the agent is training, we will
    start with a few simple examples to walk through the whole process of training
    and testing the agent. We will then look at how easily we can move it to the CARLA
    driving environment to train it further.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing the deep n-step advantage actor-critic agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because our agent's implementation is generic (as discussed using the table
    in step 1 in the previous section), we can use any learning environment that has
    Gym-compatible interfaces to train/test the agent. You can experiment and train
    the agent in a variety of environments that we discussed in the initial chapters
    of this book, and we will also be discussing some more interesting learning environments
    in the next chapter. Don't forget about our custom CARLA car driving environment!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will pick a few environments as examples and walk through how you can launch
    the training and testing process to get you started experimenting on your own. First,
    update your fork of the book''s code repository and `cd` to the `ch8` folder,
    where the code for this chapter resides. As always, make sure to activate the
    conda environment we created for this book. After this, you can launch the training
    process for the n-step advantage actor critic agent using the `a2c_agent.py` script,
    as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can replace `Pendulum-v0` with any Gym-compatible learning environment name
    that is set up on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: This should launch the agent's training script, which will use the default parameters
    specified in the `~/HOIAWOG/ch8/parameters.json` file (which you can change to
    experiment). It will also load the trained agent's brain/model for the specified
    environment from the `~/HOIAWOG/ch8/trained_models` directory, if available, and
    continue training. For high-dimensional state space environments, such as the
    Atari games, or other environments where the state/observation is an image of
    the scene or the screen pixels, the deep convolutional neural network we discussed
    in one of the previous sections will be used, which will make use of the GPU on
    your machine, if available, to speed up computations (you can disable this by
    setting `use_cuda = False` in the `parameters.json` file if you want). If you
    have multiple GPUs on your machine and would like to train different agents on
    different GPUs, you can specify the GPU device ID as a command line argument to
    the `a2c_agent.py` script using the `--gpu-id` flag to ask the script to use a
    particular GPU for training/testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training process starts, you can monitor the agent''s process by launching
    `tensorboard` using the following command from the `logs` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After launching `tensorboard` using the preceding command, you can visit the
    web page at `http://localhost:6006` to monitor the progress of the agent. Sample
    screenshots are provided here for your reference; these were from two training
    runs of the n-step advantage actor-critic agent, with different values for *n*
    steps, using the `learning_step_threshold` parameter in the `parameters.json`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actor-critic (using separate actor and critic network):'
  prefs: []
  type: TYPE_NORMAL
- en: '- `Pendulum-v0` ; n-step (learning_step_threshold = 100)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 2.  - `Pendulum-v0`; n-step (learning_step_threshold = 5)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00288.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing 1 (100-step AC in green) and 2 (5-step AC in grey) on `Pendulum-v0`
    for 10 million steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00289.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The training script will also output a summary of the training process to the
    console. If you want to visualize the environment to see what the agent is doing
    or how it is learning, you can add the `--render` flag to the command while launching
    the training script, as illustrated in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have reached a point where you are just one command away
    from training, logging, and visualizing the agent's performance! We have made
    very good progress so far.
  prefs: []
  type: TYPE_NORMAL
- en: You can run several experiments with different sets of parameters for the agent,
    on the same environment or on different environments. The previous example was
    chosen to demonstrate its performance in a simpler environment so that you can
    easily run full-length experiments and reproduce and compare the results, irrespective
    of the hardware resources you may have. As part of the book's code repository,
    trained agent brains/models are provided for some environments so that you can
    quickly start and run the script in test mode to see how a trained agent performs
    at the tasks. They are available in the `ch8/trianed_models` folder in your fork
    of the book's repository, or at the upstream origin here: [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch8/trained_models](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch8/trained_models).
    You will also find other resources, such as illustrations of learning curves in
    other environments and video clips of agents performing in a variety of environments,
    in the book's code repository for your reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are ready to test the agent, either using your own trained agent''s
    brain model or using one of the pre-trained agent brains, you can use the `--test`
    flag to signify that you would like to disable learning and run the agent in testing
    mode. For example, to test the agent in the `LunarLander-v2` environment with
    rendering of the learning environment turned on, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can interchangeably use the asynchronous agent that we discussed as an extension
    to our base agent. Since both the agent implementations follow the same structure
    and configuration, we can easily switch to the asynchronous agent training script
    by just using the `async_a2c_agent.py` script in place of `a2c_agent.py`. They
    even support the same command line arguments to make our work simpler. When using
    the `asyn_a2c_agent.py` script, you should make sure to set the `num_agents` parameter
    in the `parameters.json` file, based on the number of processes or parallel instances
    you would like the agent to use for training. As an example, we can train the
    asynchronous version of our agent in the `BipedalWalker-v2` environment using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you may have realized, our agent implementation is capable of learning to
    act in a variety of different environments, each with its own set of tasks to
    be completed, as well as their own state, observation and action spaces. It is
    this versatility that has made deep reinforcement learning-based agents popular
    and suitable for solving a variety of problems. Now that we are familiar with
    the training process, we can finally move on to training the agent to drive a
    car and follow the lanes in the CARLA driving simulator.
  prefs: []
  type: TYPE_NORMAL
- en: Training the agent to drive a car in the CARLA driving simulator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start training an agent in the CARLA driving environment! First, make
    sure your GitHub fork is up to date with the upstream master so that you have
    the latest code from the book''s repository. Since the CARLA environment we created
    in the previous chapter is compatible with the OpenAI Gym interface, it is actually
    easy to use the CARLA environment for training, just like any other Gym environment.
    You can train the n-step advantage actor-critic agent using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will launch the agent's training process, and like we saw before, a summary
    of the progress will be printed to the console window, along with the logs written
    to the `logs` folder, which can be viewed using `tensorboard`.
  prefs: []
  type: TYPE_NORMAL
- en: During the initial stages of the training process, you will notice that the
    agent is driving the car like crazy!
  prefs: []
  type: TYPE_NORMAL
- en: After several hours of training, you will see that the agent learns to control
    the car and successfully drives down the road while staying in the lane and avoiding
    crashing into other vehicles. A trained model for the autonomous driver agent
    is available in the `ch8/trained_models` folder for you to quickly take the agent
    on a test drive! You will also find more resources and experimental results in
    the book's code repository to help with your learning and experimentation. Happy
    experimenting!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got hands-on with an actor-critic architecture-based deep
    reinforcement learning agent, starting from the basics. We started with the introduction
    to policy gradient-based methods and walked through the step-by-step process of
    representing the objective function for the policy gradient optimization, understanding
    the likelihood ratio trick, and finally deriving the policy gradient theorem.
    We then looked at how the actor-critic architecture makes use of the policy gradient
    theorem and uses an actor component to represent the policy of the agent, and
    a critic component to represent the state/action/advantage value function, depending
    on the implementation of the architecture. With an intuitive understanding of
    the actor-critic architecture, we moved on to the A2C algorithm and discussed
    the six steps involved in it. We then discussed the n-step return calculation
    using a diagram, and saw how easy it is to implement the n-step return calculation
    method in Python. We then moved on to the step-by-step implementation of the deep
    n-step advantage actor-critic agent.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed how we could make the implementation flexible and generic
    to accommodate a variety of environments, which may have different state, observation
    and action space dimensions, and also may be continuous or discrete. We then looked
    at how we can run multiple instances of the agent in parallel on separate processes
    to improve the learning performance. In the last section, we walked through the
    steps involved in the process of training the agents, and once they are trained,
    how we can use the `--test` and `--render` flags to test the agent's performance.
    We started with simpler environments to get accustomed to the training and monitoring
    process, and then finally moved on to accomplishing the goal of this chapter,
    which was to train an intelligent agent to drive a car autonomously in the CARLA
    driving simulator! I hope you learned a lot going through this relatively long
    chapter. At this point, you have experience understanding and implementing two
    broad classes of high-performance learning agent algorithms from this chapter
    and [Chapter 6](part0094.html#2PKKS0-22c7fc7f93b64d07be225c00ead6ce12), *Implementing
    an Intelligent Agent for* *Optimal Discrete Control using Deep Q-Learning*. In
    the next chapter, we will explore the landscape of new and promising learning
    environments, where you can train your custom agents and start making progress
    towards the next level.
  prefs: []
  type: TYPE_NORMAL
