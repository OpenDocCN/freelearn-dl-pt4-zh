["```py\nimport multiprocessing\nimport os , json , requests\nimport re\nimport nltk\nimport gensim.models.word2vec as w2v\nimport sklearn.manifold\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n```", "```py\n\"\"\"**Download NLTK tokenizer models (only the first time)**\"\"\"\n\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\ndef sentence_to_wordlist(raw):\n    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n    words = clean.split()\n    return map(lambda x:x.lower(),words)\n\n```", "```py\n# Article 0on earth from Gutenberg website\nfilepath = 'http://www.gutenberg.org/files/33224/33224-0.txt\n\ncorpus_raw = requests.get(filepath).text\n\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\nraw_sentences = tokenizer.tokenize(corpus_raw)\n\n#sentence where each word is tokenized\nsentences = []\nfor raw_sentence in raw_sentences:\n    if len(raw_sentence) > 0:\n        sentences.append(sentence_to_wordlist(raw_sentence))\n\n```", "```py\nnum_features = 300\n\n# Minimum word count threshold.\nmin_word_count = 3\n\n# Number of threads to run in parallel.\n\n#more workers, faster we train\nnum_workers = multiprocessing.cpu_count()\n\n# Context window length.\ncontext_size = 7\n\n# Downsample setting for frequent words. 0 - 1e-5 is good for this\ndownsampling = 1e-3\n\nseed = 1\n\nmodel2vec = w2v.Word2Vec(\n            sg=1,\n            seed=seed,\n            workers=num_workers,\n            size=num_features,\n            min_count=min_word_count,\n            window=context_size,\n            sample=downsampling\n        )\n\nmodel2vec.build_vocab(sentences)\n```", "```py\n\"\"\"**Start training, this might take a minute or two...**\"\"\"\n\nmodel2vec.train(sentences ,total_examples=model2vec.corpus_count , epochs=100)\n\n\"\"\"**Save to file, can be useful later**\"\"\"\n\nif not os.path.exists(os.path.join(\"trained\",'sample')):\n    os.makedirs(os.path.join(\"trained\",'sample'))\n\nmodel2vec.save(os.path.join(\"trained\",'sample', \".w2v\"))\n```", "```py\nmodel2vec.most_similar(\"earth\")\n\n[(u'crust', 0.6946468353271484),  \n (u'globe', 0.6748907566070557),  \n (u'inequalities', 0.6181437969207764),  \n (u'planet', 0.6092090606689453),  \n (u'orbit', 0.6079996824264526),  \n (u'laboring', 0.6058655977249146),  \n (u'sun', 0.5901342630386353),  \n (u'reduce', 0.5893668532371521),  \n (u'moon', 0.5724939107894897),  \n (u'eccentricity', 0.5709577798843384)]\n```", "```py\nmodel2vec.most_similar(\"human\")\n\n [(u'art', 0.6744576692581177),  \n (u'race', 0.6348963975906372),  \n (u'industry', 0.6203593611717224),  \n (u'man', 0.6148483753204346),  \n (u'population', 0.6090731620788574),  \n (u'mummies', 0.5895125865936279),  \n (u'gods', 0.5859177112579346),  \n (u'domesticated', 0.5857442021369934),  \n (u'lives', 0.5848811864852905),  \n (u'figures', 0.5809590816497803)]\n```", "```py\nmodel2vec.most_similar_cosmul(positive=['earth','moon'], negative=['orbit'])\n\n(u'sun', 0.8161555624008179)\n```", "```py\ntsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n\nall_word_vectors_matrix = model2vec.wv.vectors\n\nall_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n\npoints = pd.DataFrame(\n    [\n        (word, coords[0], coords[1])\n        for word, coords in [\n            (word, all_word_vectors_matrix_2d[model2vec.wv.vocab[word].index])\n            for word in model2vec.wv.vocab\n        ]\n    ],\n    columns=[\"word\", \"x\", \"y\"]\n)\n\nsns.set_context(\"poster\")  \nax = points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))\nfig = ax.get_figure()\n\n```", "```py\nvocab_list = points.word.values.tolist()\nembeddings = all_word_vectors_matrix\n\nembedding_var = tf.Variable(all_word_vectors_matrix, dtype='float32', name='embedding')\nprojector_config = projector.ProjectorConfig()\n\nembedding = projector_config.embeddings.add()\nembedding.tensor_name = embedding_var.name\n\nLOG_DIR='./'\nmetadata_file = os.path.join(\"sample.tsv\")\n\nwith open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n    metadata.writelines(\"%s\\n\" % w.encode('utf-8') for w in vocab_list)\n\nembedding.metadata_path = os.path.join(os.getcwd(), metadata_file)\n\n# Use the same LOG_DIR where you stored your checkpoint.\nsummary_writer = tf.summary.FileWriter(LOG_DIR)\n\n# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n# read this file during startup.\nprojector.visualize_embeddings(summary_writer, projector_config)\n\nsaver = tf.train.Saver([embedding_var])\n\nwith tf.Session() as sess:\n    # Initialize the model\n    sess.run(tf.global_variables_initializer())\n\n    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))\n```", "```py\ntensorboard --logdir=/path/of/the/checkpoint/\n```", "```py\nimport tensorflow as tf\nimport numpy as np\n\nclass TextCNN(object):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    \"\"\"\n\n    def __init__(self,\n                 sequence_length,\n                 num_classes,\n                 vocab_size,\n                 embedding_size,\n                 filter_sizes,\n                 num_filters,\n                 l2_reg_lambda=0.0,\n                 pre_trained=False):\n```", "```py\n# Placeholders for input, output and dropout\nself.input_x = tf.placeholder(\n    tf.int32, [\n        None,\n        sequence_length,\n    ], name=\"input_x\")\nself.input_y = tf.placeholder(\n    tf.float32, [None, num_classes], name=\"input_y\")\nself.dropout_keep_prob = tf.placeholder(\n    tf.float32, name=\"dropout_keep_prob\")\n# Keeping track of l2 regularization loss (optional)\nl2_loss = tf.constant(0.0)\n```", "```py\n# Embedding layer\nwith tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n    if pre_trained:\n        W_ = tf.Variable(\n            tf.constant(0.0, shape=[vocab_size, embedding_size]),\n            trainable=False,\n            name='W')\n        self.embedding_placeholder = tf.placeholder(\n            tf.float32, [vocab_size, embedding_size],\n            name='pre_trained')\n        W = tf.assign(W_, self.embedding_placeholder)\n    else:\n        W = tf.Variable(\n            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n            name=\"W\")\n    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n    self.embedded_chars_expanded = tf.expand_dims(\n        self.embedded_chars, -1)\n```", "```py\n# Create a convolution + maxpool layer for each filter size\npooled_outputs = []\nfor i, filter_size in enumerate(filter_sizes):\n    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n        # Convolution Layer\n        filter_shape = [filter_size, embedding_size, 1, num_filters]\n        W = tf.Variable(\n            tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n        b = tf.Variable(\n            tf.constant(0.1, shape=[num_filters]), name=\"b\")\n        conv = tf.nn.conv2d(\n            self.embedded_chars_expanded,\n            W,\n            strides=[1, 1, 1, 1],\n            padding=\"VALID\",\n            name=\"conv\")\n        # Apply nonlinearity\n        h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n        # Maxpooling over the outputs\n        pooled = tf.nn.max_pool(\n            h,\n            ksize=[1, sequence_length - filter_size + 1, 1, 1],\n            strides=[1, 1, 1, 1],\n            padding='VALID',\n            name=\"pool\")\n        pooled_outputs.append(pooled)\n\n# Combine all the pooled features\nnum_filters_total = num_filters * len(filter_sizes)\nself.h_pool = tf.concat(pooled_outputs, 3)\nself.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n```", "```py\n# Add dropout\nwith tf.name_scope(\"dropout\"):\n    self.h_drop = tf.nn.dropout(self.h_pool_flat,\n                                self.dropout_keep_prob)\n```", "```py\n# Final (unnormalized) scores and predictions\nwith tf.name_scope(\"output\"):\n    W = tf.get_variable(\n        \"W\",\n        shape=[num_filters_total, num_classes],\n        initializer=tf.contrib.layers.xavier_initializer())\n    b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n    l2_loss += tf.nn.l2_loss(W)\n    l2_loss += tf.nn.l2_loss(b)\n    self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n    self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n```", "```py\n# CalculateMean cross-entropy loss\nwith tf.name_scope(\"loss\"):\n    losses = tf.nn.softmax_cross_entropy_with_logits(\n        labels=self.input_y, logits=self.scores)\n    self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n# Accuracy\nwith tf.name_scope(\"accuracy\"):\n    correct_predictions = tf.equal(self.predictions,\n                                   tf.argmax(self.input_y, 1))\n    self.accuracy = tf.reduce_mean(\n        tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n```", "```py\ncheckpoint_dir = \"./runs/1508847544/\"\nembedding = np.load('fasttext_embedding.npy')\n```"]