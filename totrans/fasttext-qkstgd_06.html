<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Sentence Classification in FastText</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Sentence classification</li>
<li class="calibre11">fastText supervised learning:
<ul class="calibre39">
<li class="calibre11">Architecture</li>
<li class="calibre11">Hierarchical softmax architecture</li>
<li class="calibre11">N-grams features and the hashing trick:
<ul class="calibre39">
<li class="calibre11">The <strong class="calibre1">Fowler</strong>-<strong class="calibre1">Noll</strong>-<strong class="calibre1">Vo</strong> (<strong class="calibre1">FNV</strong>) hash</li>
</ul>
</li>
<li class="calibre11">Word embeddings and their use in sentence classification</li>
</ul>
</li>
<li class="calibre11">fastText model quantization:
<ul class="calibre39">
<li class="calibre11">Compression:
<ul class="calibre39">
<li class="calibre11">Quantization</li>
<li class="calibre11">Vector quantization:
<ul class="calibre39">
<li class="calibre11">Finding the codebook for high-dimensional spaces</li>
</ul>
</li>
<li class="calibre11">Product quantization</li>
<li class="calibre11">Additional steps</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Sentence classification</h1>
                
            
            <article>
                
<p class="calibre2"/>
<p class="calibre2">Sentence classification deals with understanding text found in natural languages and determining the classes that it may belong to. In the text classification set of problems, you will have a set of documents <em class="calibre16">d</em> that belongs to the corpus <em class="calibre16">X</em> (which contains all the documents). You will also have a set of finite classes <em class="calibre16">C</em> = <em class="calibre16">{c<sub class="calibre29">1</sub> , c<sub class="calibre29">2</sub>, ..., c<sub class="calibre29">n</sub>}</em>. Classes are also called categories or labels. To train a model, you would need a classifier, which is generally a well-tested algorithm (not necessary but in this case we will be talking about a well-tested algorithm that is used in fastText) and you will need a corpus with documents and associated labeling identifying the classes that each document belongs to.</p>
<p class="calibre2"/>
<p class="calibre2">Text classification has many practical uses, such as the following:</p>
<p class="calibre2"/>
<ul class="calibre10">
<li class="calibre11">Creating spam classifiers in email</li>
<li class="calibre11">Page ranking and indexing in search engines</li>
<li class="calibre11">Sentiment detection in reviews that will give an idea whether customers are happy with the product or not</li>
</ul>
<p class="calibre2"/>
<p class="calibre2">Text classification is generally a way to augment manual classification. Labeling a document is largely subjective and depends on the corpus. A sample document "I like traveling to Hawaii" may be regarded as falling under the class "Travel" by a librarian but may be regarded as "Irrelevant" by a doctor. So the idea is that a set of documents will be labeled by a domain expert, the labeled data will be used to train a text classifier, and then the text classifier can be used to predict new incoming text, saving the time and resources of the domain expert (maybe the domain expert can periodically check and audit the performance of the classifier against incoming text). Also, the proposed idea is for general people and does not apply to crowd-sourced labeling as done by stack overflow when it asks for users to label; most business problems do not have the luxury of such <em class="calibre16">auto</em> labeling and hence you will have to spend some time manually labeling the documents.</p>
<p class="calibre2">To evaluate the performance of a classification model, we divide the training corpus into test and train sets. Only the train set is used for model training. Once done, we classify the test set and compare the predictions with the actual ones and measure the performance. The portion of correctly classified documents to the portion of actual documents is called accuracy. There are two more parameters that we can look at that will give a measure for the model performance. One is the <em class="calibre16">recall</em>, which means the percentage of all the correct labels that we recalled as opposed to the labels that actually existed. We can also look at the <em class="calibre16">precision</em> of the model, which means that we look at all the predicted labels and say which portions of them are the actual labels in the first place.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">fastText supervised learning</h1>
                
            
            <article>
                
<p class="calibre2"/>
<p class="calibre2">A fastText classifier is built on top of a linear classifier, specifically a BoW classifier. In this section, you will get to know the architecture of the fastText classifier and how it works.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Architecture</h1>
                
            
            <article>
                
<p class="calibre2"/>
<p class="calibre2">You can consider that each piece of text and each label is actually a vector in space and the coordinates of that vector are what we are actually trying to tweak and train so that the vector for a text and associated label are really close in space:</p>
<p class="calibre2"/>
<div class="cdpaligncenter"><img src="../images/00055.jpeg" class="calibre40"/></div>
<div class="mce-root">Vector representation of the text</div>
<p class="packt_figref">So, in this example, which is an example shown in 2D space, you have texts that are saying things such as "Nigerian Tommy Thompson is also a relative newcomer to the wrestling scene" and "James scored 20 of his 46 points in the opening quarter" are closer to the "sports" label and not the "travel" label.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The way we can do this is we can take the vector representing the text and the vector representing the label and input it into the scoring function. We then take the score and then we normalize across sum of all the scores between the vector representing the text and the vector representations for every other possible label. And that provides us with the type of probability that the given text above will have the label. The scores are the individual values and we convert them to probabilities using the softmax function which was also discussed in the previous chapter.</p>
<p class="calibre2"/>
<p class="calibre2">The fastText uses similar vector space models for text classification, where the words are reduced to low-dimensional vectors called embeddings. The aim is to train and arrive at a vector space such that the sentence vectors and the label vectors are really close to each other. To apply vector space models to sentences or documents, one must first select an appropriate function, which is a mathematical process for combining multiple words.</p>
<p class="calibre2"/>
<p class="calibre2">Composition functions fall into two classes: unordered or syntactic. Unordered functions treat input texts as <strong class="calibre4">bag of words</strong> (<strong class="calibre4">BoW</strong>) embeddings, while syntactic representations take word order and sentence structure into account. The fastText is mostly an unordered approach since it takes the BoW approach but has a little bit of syntactic representations using the n-grams, as we will see later.</p>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">What you can do next is take the representations and then train a linear classifier on top of them. Good linear classifiers that can be used are logistic regression and <strong class="calibre4">support vector machines</strong> (<strong class="calibre4">SVM</strong>). Linear classifiers, though, have a little caveat. They do not share parameters between features and classes. As a result of this, there is a possibility that this limits the generalization capabilities to those types of classes which do not have many examples to train on. The solution to this problem is to use multilayered neural networks or to factorize the linear classifier into low rank matrices and then run a neural network on top of them.</span></p>
<p class="calibre2"/>
<p class="calibre2">Syntactic representations are more sparse than BoW approach and hence require more training time. This makes them computationally very expensive in case of huge datasets or when you have limited computational resources. For example, if you build a recursive neural network for training on syntactic word representations that again computes costly tensor products, and furthermore there will be non-linearity in every node of a syntactic parse tree, then your training time may stretch to days, which is prohibitive for fast feedback cycles.</p>
<p class="calibre2">So you can use an averaging network, which is an unordered model that can be explained in three simple steps:</p>
<ol class="calibre13">
<li value="1" class="calibre11">Take the vector average of the embeddings associated with an input sequence of tokens.</li>
<li value="2" class="calibre11">Pass the average through one or more feed-forward layers.</li>
<li value="3" class="calibre11">Perform linear classification on the final layer's representation.</li>
</ol>
<p class="calibre2">The model can be improved by applying a novel dropout-inspired regularizer. In this case, for each training instance, some of the token embeddings will be randomly dropped before computing the average. In fastText, this is done by subsampling frequent words, which was also discussed in the previous chapter and is used in the classifier as well.</p>
<p class="calibre2">A general form of the architecture is as follows:</p>
<div class="cdpaligncenter"><img src="../images/00056.jpeg" class="calibre41"/></div>
<p class="calibre2">In this case, we want to map an input sequence of tokens to k labels. We first apply a composition function g to the sequence of word embeddings ν<sub class="calibre29">ω</sub> for ω ∈ X. The output of this composition function is a vector z that serves as input to the logistic regression function. The architecture of the classification is similar to the cbow model that was discussed in the previous chapter.</p>
<p class="calibre2">First, a weighted average of the word embeddings is taken:</p>
<div class="cdpaligncenter"> <img class="fm-editor-equation28" src="../images/00057.jpeg"/></div>
<p class="calibre2">Feeding <em class="calibre16">z</em> to a softmax layer induces estimated probabilities for each output label:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation29" src="../images/00058.jpeg"/></div>
<p class="calibre2">Here, the softmax function is as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation30" src="../images/00059.jpeg"/></div>
<p class="calibre2">Here, <kbd class="calibre12">W_s</kbd> is a <em class="calibre16">k</em> x <em class="calibre16">d</em> matrix for the dataset with <em class="calibre16">k</em> output labels and <em class="calibre16">b</em> is a bias term. fastText uses a generic bias term in the form of an end of sentence character <kbd class="calibre12">&lt;s&gt;</kbd> that gets added to all input examples.</p>
<p class="calibre2">This model can then be trained to minimize cross-entropy error, which for a single training instance with ground truth label <em class="calibre16">y</em> is as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation31" src="../images/00060.jpeg"/></div>
<p class="calibre2">In fastText, this gets translated to computing the probability distribution over the predefined classes. For the set of <em class="calibre16">N</em> documents, this leads to minimizing the negative log likelihood over the classes:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation32" src="../images/00061.jpeg"/></div>
<p class="calibre2">Here, <em class="calibre16">x<sub class="calibre29">n</sub></em> is the normalized bag of features of the nth document, <em class="calibre16">y<sub class="calibre29">n</sub></em> is the label, and A and B are the weight matrices. A is just a lookup table over the words in fastText.</p>
<p class="calibre2">Then we can use stochastic gradient descent to keep tweaking those coordinates until we maximize the probability of the correct label for every piece of text. The fastText trains this model asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate.</p>
<p class="calibre2"/>
<p class="calibre2">The architecture that is used in fastText is as follows:</p>
<div class="mce-root"><img src="../images/00062.gif" class="calibre42"/></div>
<p class="packt_figref">Model architecture of fastText for a sentence with N gram word features <em class="calibre16">x<sub class="calibre43">1</sub></em>, <em class="calibre16">x<sub class="calibre43">2</sub></em>, ..., <em class="calibre16">x<sub class="calibre43">N</sub></em>. The features are embedded and averaged to form the hidden variable.</p>
<p class="calibre2">Now let's summarize the architecture:</p>
<ul class="calibre10">
<li class="calibre11">It starts with word representations, which are averaged into text representations, which are fed into a linear classifier</li>
<li class="calibre11">The classifier is essentially a linear model with a rank constraint and fast loss approximation</li>
<li class="calibre11">The text representation is a hidden state that can be shared among features and classes</li>
<li class="calibre11">A softmax layer is used to obtain a probability distribution over predefined classes</li>
<li class="calibre11">High computational complexity <em class="calibre21">O(kh)</em>, where <em class="calibre21">k</em> is the number of classes and <em class="calibre21">h</em> is the dimension of the text representation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hierarchical softmax architecture</h1>
                
            
            <article>
                
<p class="calibre2">Finding the softmax is computationally quite expensive and prohibitive for a large corpus as this means that we not only have to find the score for the text with the label but the scores for the text with all the labels. So for <em class="calibre16">n</em> text and <em class="calibre16">m</em> labels, this scales with worst case performance of <em class="calibre16">O(n<sup class="calibre23">2</sup>)</em>, which you know is not good.</p>
<p class="calibre2">Also, softmax and <span class="calibre5">other</span><span class="calibre5"> </span><span class="calibre5">similar methods for finding the probabilities do not take into account the semantically meaningful organization of classes. A classifier should know that classifying a dog as a submarine should have a higher penalty than a dog as a wolf. An intuition that we may have is that the target labels are probably not flat but rather a tree.</span></p>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">So now we have</span> <em class="calibre16">k</em> <span class="calibre5">classes that we want each input to be classified into. So let's consider these classes are the leaves of a tree. This tree is organized in such as way that the hierarchy is semantically meaningful. Consider further that our classifier maps an input to an output probability distribution over the leaves. Hopefully, this leaf will be the correct class of the corresponding input. The probability of any node in the tree is the probability of the path from the root to that node. If the node is at depth</span> <em class="calibre16">l + 1</em> <span class="calibre5">with parents</span> <em class="calibre16">n<sub class="calibre29">1</sub></em><span class="calibre5">,</span> <em class="calibre16">n<sub class="calibre29">2</sub></em><span class="calibre5">, ...,</span> <em class="calibre16">n<sub class="calibre29">l</sub></em> <span class="calibre5">then the probability of the node is as follows:</span></p>
<div class="cdpaligncenter"><img class="fm-editor-equation33" src="../images/00063.jpeg"/></div>
<p class="calibre2"><span class="calibre5">A leaf falls under the node if the node is on the path from the root to the leaf. We then define the amount of the "win" or the "winnings" to be the weighted sum of the probabilities of the nodes along its path from the root to the leaf corresponding to the correct classes. During the optimization or the training process, we want to maximize this "winnings" for our model, and conversely minimize the "loss". Loss in this case is considered the negative of the win.</span></p>
<p class="calibre2">So in fastText, what is used is the hierarchical classifier, which is similar to the hierarchical softmax that you saw in the earlier chapter. In this method, it represents the labels in a binary tree and so every node in the binary tree is represented as a probability and so a label is represented by the probability along the path to that given label. In this case, the correct label is generated using the <strong class="calibre4">breadth first search</strong> (<strong class="calibre4">BFS</strong>) algorithm. BFS is quite fast for searching and hence you bring down the complexity to <em class="calibre16">log<sub class="calibre29">2</sub>n</em>. Now we just need to compute the probabilities of the path to the correct label. So when we have a lot of labels, this really increases the speed of computation for all the labels and hence the model training. And as you have seen in the previous chapter, the hierarchical probability representation asymptotes to the softmax probabilities and hence this approximations actually give the same kind of model performance and are vastly faster to train.</p>
<p class="calibre2">As you have seen in the previous chapter, in this case the output of the hierarchical classifier is the label. Similar to training word embeddings, in this case a Huffman tree is formed. Since we have already discussed the internals of the Huffman tree in the previous chapter, in this case we will tinker at little bit with the code and try to see the exact tree that is formed and find the probabilities associated with it.</p>
<p class="calibre2">To keep things simple, we will take a very small dataset with very small number of labels. In this example, the following set of sentences along with the labels are taken and saved in a file named <kbd class="calibre12">labeledtextfile.txt</kbd>:</p>
<pre class="calibre17">__label__sauce How much does potato starch affect a cheese sauce recipe?<br class="title-page-name"/> __label__food-safety Dangerous pathogens capable of growing in acidic environments<br class="title-page-name"/> __label__restaurant Michelin Three Star Restaurant; but if the chef is not there<br class="title-page-name"/> __label__baking how to seperate peanut oil from roasted peanuts at home?<br class="title-page-name"/> __label__baking Fan bake vs bake<br class="title-page-name"/> __label__sauce Regulation and balancing of readymade packed mayonnaise and other sauces</pre>
<p class="calibre2">Since fastText is written in C++, for performance reasons it does not manipulate and work with the direct label strings. To get the Huffman codes of the label, you can change the <kbd class="calibre12"><span>hierarchicalSoftmax</span></kbd> function on line <kbd class="calibre12">81</kbd> of <kbd class="calibre12">model.cc</kbd> to the following:</p>
<pre class="calibre17">real Model::hierarchicalSoftmax(int32_t target, real lr) {<br class="title-page-name"/>  real loss = 0.0;<br class="title-page-name"/>  grad_.zero();<br class="title-page-name"/>  const std::vector&lt;bool&gt;&amp; binaryCode = codes[target];<br class="title-page-name"/><br class="title-page-name"/>  std::cout &lt;&lt; "\ntarget: " &lt;&lt; target &lt;&lt; ", vector: ";<br class="title-page-name"/>  for (std::vector&lt;bool&gt;::const_iterator i = binaryCode.begin(); i != binaryCode.end(); ++i)<br class="title-page-name"/>      std::cout &lt;&lt; *i &lt;&lt; ' ';<br class="title-page-name"/>  std::cout &lt;&lt; '\n';<br class="title-page-name"/>  <br class="title-page-name"/><br class="title-page-name"/>  const std::vector&lt;int32_t&gt;&amp; pathToRoot = paths[target];<br class="title-page-name"/><br class="title-page-name"/>  if (target == 0)<br class="title-page-name"/>  {<br class="title-page-name"/>      std::cout &lt;&lt; "will check the path to root for bakings: " &lt;&lt; ' ';<br class="title-page-name"/>      for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {<br class="title-page-name"/>          std::cout &lt;&lt; pathToRoot[i] &lt;&lt; '_' &lt;&lt; "target_" &lt;&lt; target &lt;&lt; "_Individual loss_" &lt;&lt; binaryLogistic(pathToRoot[i], binaryCode[i], lr) &lt;&lt; ' ';<br class="title-page-name"/>      }<br class="title-page-name"/>      std::cout &lt;&lt; '\n';<br class="title-page-name"/>  }<br class="title-page-name"/><br class="title-page-name"/>  if (target == 1)<br class="title-page-name"/>  {<br class="title-page-name"/>      std::cout &lt;&lt; "will check the path to root for sauce: " &lt;&lt; '\n';<br class="title-page-name"/>      for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {<br class="title-page-name"/>          std::cout &lt;&lt; pathToRoot[i] &lt;&lt; '_' &lt;&lt; "target_" &lt;&lt; target &lt;&lt; "_Individual loss_" &lt;&lt; binaryLogistic(pathToRoot[i], binaryCode[i], lr) &lt;&lt; ' ';<br class="title-page-name"/>      }<br class="title-page-name"/>      std::cout &lt;&lt; '\n';<br class="title-page-name"/>  }<br class="title-page-name"/><br class="title-page-name"/>  if (target == 2)<br class="title-page-name"/>  {<br class="title-page-name"/>      std::cout &lt;&lt; "will check the path to root for sauce: " &lt;&lt; '\n';<br class="title-page-name"/>      for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {<br class="title-page-name"/>          std::cout &lt;&lt; pathToRoot[i] &lt;&lt; '_' &lt;&lt; "target_" &lt;&lt; target &lt;&lt; "_Individual loss_" &lt;&lt; binaryLogistic(pathToRoot[i], binaryCode[i], lr) &lt;&lt; ' ';<br class="title-page-name"/>      }<br class="title-page-name"/>      std::cout &lt;&lt; '\n';<br class="title-page-name"/>  }<br class="title-page-name"/><br class="title-page-name"/>  if (target == 3)<br class="title-page-name"/>  {<br class="title-page-name"/>      std::cout &lt;&lt; "will check the path to root for restaurant: " &lt;&lt; '\n';<br class="title-page-name"/>      for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {<br class="title-page-name"/>          std::cout &lt;&lt; pathToRoot[i] &lt;&lt; '_' &lt;&lt; "target_" &lt;&lt; target &lt;&lt; "_Individual loss_" &lt;&lt; binaryLogistic(pathToRoot[i], binaryCode[i], lr) &lt;&lt; ' ';<br class="title-page-name"/>      }<br class="title-page-name"/>      std::cout &lt;&lt; '\n';<br class="title-page-name"/>  }<br class="title-page-name"/><br class="title-page-name"/>  for (int32_t i = 0; i &lt; pathToRoot.size(); i++) {<br class="title-page-name"/>    loss += binaryLogistic(pathToRoot[i], binaryCode[i], lr);<br class="title-page-name"/>  }<br class="title-page-name"/><br class="title-page-name"/>  // std::cout &lt;&lt; "total loss for target: " &lt;&lt; target &lt;&lt; " is: " &lt;&lt; loss;<br class="title-page-name"/>  // std::cout &lt;&lt; '\n';<br class="title-page-name"/><br class="title-page-name"/>  return loss;<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see, I am listing for multiple labels. But you can choose the label that you want. You will get output similar to this. You will need to get the last occurrence of the target value to get the vector:</p>
<pre class="calibre17">target: 2, vector: 1 0 1<br class="title-page-name"/> will check the path to root for sauce:<br class="title-page-name"/> 0_target_2_Individual loss_0.693147 1_target_2_Individual loss_0.681497 2_target_2_Individual loss_0.693147</pre>
<p class="calibre2">So the <span class="calibre5">similar</span> vector, corresponding to target 2, is 101.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The n-gram features and the hashing trick</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">As you have seen, the BoW of the vocabulary is taken to arrive at the word representation to be used later in the classification process. But the BoW is unordered and does not have any </span><span class="calibre5">syntactic</span><span class="calibre5"> information. Hence, the bag of n-grams are used as additional features to capture some of the </span><span class="calibre5">syntactic information.</span></p>
<p class="calibre2">As we have already discussed, large-scale NLP problems almost always involve using a large corpus. This corpus will always have <em class="calibre16">unbounded</em> number of unique words, as we have seen from the Zipf's law. Words are generally defined as a string of characters separated with a delimiter, such as a space in English. Hence, taking word n-grams is <span class="calibre5">simply</span><span class="calibre5"> not scalable to large corpora, which is essential to come to accurate classifications.</span></p>
<p class="calibre2">Because of these two factors, the matrices that are formed naively are always sparse and high-dimensional. You can try to reduce the dimensions of the matrices using techniques such as PCA but that would still involve doing matrix manipulations that require such a high amount of memory that it would make the whole computation infeasible.</p>
<p class="calibre2">What if you can do something so that you are able to circumvent the creation of the dictionary? A similar problem is tackled with what is known as the kernel trick. The kernel trick enables us to use linear classifiers on non-linear data. In this method, the input data is transformed to a high-dimensional feature space. Interestingly, you just need to specify the kernel for this step, no need to transform all the data to the feature space, and it will work. In other words, when you compute the distance and apply the kernel, you get a number. The number is the same as what you would have got if you expanded your initial points into the higher-order space that your kernel points to and computed their inner product:</p>
<div class="cdpaligncenter"><img src="../images/00064.jpeg" class="calibre44"/></div>
<div class="mce-root"><span> Source: https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78</span></div>
<p class="calibre2"><span class="calibre5">It’s a lot easier to get the inner product in a higher-dimensional space than the actual points in the higher dimensional space.</span></p>
<p class="calibre2">The challenges of text classification are complementary. The original input space is generally linearly separable (because generally humans decide on the features based on tagging) but the training set is prohibitively large in size and very high-dimensional. For this common scenario, a complementary variation to the kernel trick is used. This method is called the hashing trick. Here, the high-dimensional vectors in <em class="calibre16">ℜ<sup class="calibre23">d</sup></em> are mapped into a lower-dimensional feature space <em class="calibre16">ℜ<sup class="calibre23">m</sup></em> such that <em class="calibre16">m &lt;&lt; d</em>. We will train the classifier in <em class="calibre16">ℜ<sup class="calibre23">m</sup></em> space.</p>
<p class="calibre2">The core idea in the hashing trick is the hash functions. So in text classification and similar NLP tasks, we take a non-cryptographic hash such as murmur or FNV (more on this later in the chapter) and map the work into a finite integer (usually 32-bit or 64-bit integers which are modulo of a prime number).</p>
<p class="calibre2">The following are some of the characteristics that define a hash function:</p>
<ul class="calibre10">
<li class="calibre11">The most important one—if you feed the same input to a hash function, it will always give the same output.</li>
<li class="calibre11">The choice of the hash function determines the range of the possible outputs. The range is generally fixed. For example, modern websites use SHA256 hashes that are truncated to 128 bits.</li>
<li class="calibre11">Hash functions are meant to be one way. Given a hash, the input should not be computable.</li>
</ul>
<p class="calibre2">Due to the fact that they have a fixed range, a pleasant side effect of using hash functions is that there are fixed memory requirements. Another advantage is that we get benefits on the <strong class="calibre4">out-of-vocabulary</strong> (<strong class="calibre4">OOV</strong>) front as well. This part is not that obvious so let me explain it. The first step on that note is that we don't have to deal with the vocabulary at all. Instead, when starting with our BoW representations, we will start with a big column vector (<span class="calibre5">2 million </span>in our case) with a lot of elements for each of our training samples:</p>
<pre class="calibre17"><span>FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers<br class="title-page-name"/>-&gt; [0 0 0 0 ... 0 0 0 0] (2000000 elements)</span></pre>
<p class="calibre2">Now we will choose a hash function <em class="calibre16">f</em> that takes in strings as inputs and outputs values. In other words, we are making sure that our hash function will never address an index outside our feature's dimensions.</p>
<p class="calibre2">There is the advantage in terms of OOV words as well, as compared to maintaining a large vocabulary in a <em class="calibre16">naive</em> BoW approach. Because the vector representation is created using a hash function, any string, even OOV words, will have a vector in the hash space. New words will worsen the accuracy of our classifier, true, but it will still work. No need to throw away the new words when predicting.</p>
<p class="calibre2">One reason why this should work comes from the same Zipf's law that we are time and again referring to. Hash collisions that may happen (if any), would probably be between infrequent words or between frequent word and an infrequent word. This is because frequent words by definition will occur earlier and hence tend to occupy the spaces first. Thus the collided feature used for classification will be either unlikely to be selected for feature selection or represent the word that led the classifier to select it.</p>
<p class="calibre2">Now that we have established the benefits of the hashing trick, we need to focus on the hashing function. There are many hash functions that are used for implementing the hashing trick, for example, Vowpal Wabbit and scikit-learn murmurhash v3. A good list of possible non-cryptographic hashes can be found at the following Wiki link:<a href="https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions" class="calibre9"> https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions</a>. FastText uses the FNV-1a hash, which will be discussed below.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The FNV hash</h1>
                
            
            <article>
                
<p class="calibre2">fastText uses the FNV-1a hash, which is the derivative of the FNV hash. To implement this algorithm, start with an initial hash value of <kbd class="calibre12">FNV_offset_basis</kbd>. For each byte in the input, take the XOR of the hash and the byte. Now multiply the result with the FNV prime. In terms of pseudo code:</p>
<pre class="calibre17">hash = FNV_offset_basis
   for each byte_of_data to be hashed
        hash = hash <a href="https://en.wikipedia.org/wiki/XOR" class="calibre45">XOR</a> byte_of_data
        hash = hash × FNV_prime
   return hash</pre>
<p class="calibre2">Source: Wikipedia, <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function" class="calibre9">https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function</a></p>
<p class="calibre2">In fastText, this is implemented in the <kbd class="calibre12">dictionary.cc</kbd> hash function (<a href="https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143" class="calibre9">https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143</a>):</p>
<pre class="calibre17">uint32_t Dictionary::hash(const std::string&amp; str) const {<br class="title-page-name"/>  uint32_t h = 2166136261;<br class="title-page-name"/>  for (size_t i = 0; i &lt; str.size(); i++) {</pre>
<pre class="calibre17"><br class="title-page-name"/>    h = h ^ uint32_t(str[i]);<br class="title-page-name"/>    h = h * 16777619;<br class="title-page-name"/>  }<br class="title-page-name"/>  return h;<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see, the offset basis considered is <kbd class="calibre12">2166136261</kbd> and corresponding prime number is <kbd class="calibre12">16777619</kbd>.</p>
<p class="calibre2">FNV is not cryptographic yet it has a high dispersion quality and a variable size hash result that can be any power of 2 from 32 to 1024 bits. The formula to generate it is among the simplest hash functions ever invented that actually achieve good dispersion. It also has good collision and bias resistance. One of the best properties of the FNV is that although it is not considered cryptographic, subverting bit sizes above 64 bits is mostly unsolvable. As the computational overhead needs to be kept to a minimum, fastText uses 32 bit, which has an FNV offset bias of 2166136261.</p>
<p class="calibre2">Take a look at the Python implementation in the file <kbd class="calibre12">fnv1a.py</kbd> under the <kbd class="calibre12">chapter4</kbd> folder in the repo.</p>
<p class="calibre2">In fastText, word- and character-level n-grams are hashed into a fixed number of buckets. This prevents the memory requirements when training the model. You can change the number of buckets using the <kbd class="calibre12">-buckets</kbd> parameter. By default, the number of buckets is fixed at 2M (2 million).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Word embeddings and their use in sentence classification</h1>
                
            
            <article>
                
<p class="calibre2">As you have seen in the previous chapter, word embeddings are the numerical representation of words in the shape of a vector in <em class="calibre16">ℜ<sup class="calibre23">d</sup></em>. They are unsupervised learned word representation vectors where there should be a correlation on semantic similarity. We also discussed what distributional representations and distributed representations <span class="calibre5">are</span><span class="calibre5"> </span><span class="calibre5">in <a href="part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755" class="calibre9">Chapter 7</a>, <em class="calibre16">Deploying Models to Web and Mobile</em>.</span></p>
<p class="calibre2">When performing sentence classification, there is the hypothesis that one can take an existing, near-state-of-the-art, supervised NLP system and improve it using word embeddings. You can either create your own unsupervised word embeddings using the fastText cbow/skipgram approach as shown in <a href="part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755" class="calibre9">Chapter 2</a>, <em class="calibre16">Creating Models Using FastText Command Line</em>, or you can download them from the <kbd class="calibre12">fasttext.cc</kbd> website.</p>
<p class="calibre2">A question that may arise is whether certain word representations are better for certain tasks. Present research on some specific areas shows that word representations that work well in some tasks do not work well in others. An example that is generally given is that word representations that work in named entity recognition tasks do not work well when the problem domain is search query classification and vice versa.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">fastText model quantization</h1>
                
            
            <article>
                
<p class="calibre2">Due to the efforts of the Facebook AI Research team, there is a way to get vastly smaller models (in terms of the size that they take up in the hard drive), as you have seen in the <em class="calibre16">M</em><span class="calibre5"><em class="calibre16">odel quantization</em> section in </span><a href="part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755" class="calibre9">Chapter 2</a>,<em class="calibre16"> Creating Models Using FastText Command Line</em><span class="calibre5">. Models which take up hundreds of MBs can be quantized to only a couple of MBs. For example, if you see the DBpedia model released by Facebook, which can be accessed at the web page</span><a href="https://fasttext.cc/docs/en/supervised-models.html" class="calibre9"> https://fasttext.cc/docs/en/supervised-models.html</a><span class="calibre5">, notice that the regular model (this is the BIN file) is of 427 MB while the smaller model (the FTZ file) is only 1.7 MB.</span></p>
<p class="calibre2">This reduction in size is achieved by throwing out some of the information that is encoded in the BIN files (or the bigger model). The problem that needs to be solved here is how to keep information that is important and how to identify information that is not that important so that the overall accuracy and performance of the model is not compromised by a significant margin. In this section, we will take a look at the considerations for that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Compression techniques</h1>
                
            
            <article>
                
<p class="calibre2">Since we are interested in understanding how to go about compressing the big fastText model files, let's take a look at some of the compression techniques that can be used.</p>
<p class="calibre2">Different compression techniques can be categorized as the following:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Lossless compression</strong>: As the name suggests, lossless compression techniques are those techniques that will reproduce the same information structure when you compress, then uncompress. There is no loss in information. This type of compression is mostly done using statistical modeling. You have already encountered an algorithm that is used in this type of compression, namely Huffman coding.</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Lossy compression</strong>: Lossy compression is about discarding as much data as possible without discarding the benefits or usefulness of the data as much as possible. This is a good technique when we are not interested in recreating the original data, but more on what the original data represents. As you can correctly infer, you will generally be able to get a higher level of compression using lossy compression.</li>
</ul>
<p class="calibre2">FastText employs a type of lossy compression known as product quantization. In the following sections, we will try to understand what quantization is, then how the idea of vector quantization comes from that and how it implements compression. Then we will take a look at how product quantization is a better variant of vector quantization for this task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quantization</h1>
                
            
            <article>
                
<p class="calibre2">The concept of quantization has been taken from the world of <strong class="calibre4">digital signal processing</strong> (<strong class="calibre4">DSP</strong>). In DSP, when converting an analog signal (for example, sound waves) into a digital signal, the signal is broken down to a series of individual samples based on a bit depth that determines the number of levels that the quantized signal will have. In the case of 8-bit quantization, you will have 2<sup class="calibre23">8</sup>=256 possible combinations for the amplitude signal, and similarly, in 16-bit quantization, you will have 2<sup class="calibre23">16</sup>=65536 possible combinations.</p>
<p class="calibre2">In the following example, a sine wave is quantized to 3-bit levels and hence it will support 8 values for the continuous sine wave. You can see the code to get the following image in the <kbd class="calibre12">product quantization.ipynb</kbd> notebook:</p>
<div class="cdpaligncenter"><img src="../images/00065.jpeg" class="calibre46"/></div>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Vector quantization</h1>
                
            
            <article>
                
<p class="calibre2">Vector quantization is a technique to apply the ideas of quantization to vector spaces. Let's say that you have a vector space <em class="calibre16">ℜ<sub class="calibre29">k</sub></em> and a training set consisting of <em class="calibre16">N</em> samples of <em class="calibre16">k</em>-dimensional vectors on the target vector space. Vector quantization is the process where you map your vectors into a finite set of vectors Y, which are part of the target vector space. Each vector <em class="calibre16">y<sub class="calibre29">i</sub></em> is called a <strong class="calibre4">code vector</strong> or a <strong class="calibre4">codeword</strong> and the set of all <em class="calibre16">codewords</em> is called a <strong class="calibre4">codebook</strong>:</p>
<div class="cdpaligncenter"><img src="../images/00066.jpeg" class="calibre47"/></div>
<p class="calibre2">The amount of compression that is achieved is dependent on the size of the codebook. If we have a codebook of size <em class="calibre16">k</em> and the input vector is of dimension <em class="calibre16">L</em>, we need to specify <img class="fm-editor-equation34" src="../images/00067.jpeg"/> bits to specify which of the codewords are selected from the codebook. Hence, the rate for an L-dimensional vector quantizer with a code book of size k is <img class="fm-editor-equation35" src="../images/00068.jpeg"/>.</p>
<p class="calibre2">Let's understand how that happens in more detail. If you have a vector of <em class="calibre16">L</em> dimension where <em class="calibre16">L=8</em>, it is represented as follows:</p>
<p class="calibre2"><img class="fm-editor-equation36" src="../images/00069.jpeg"/></p>
<p class="calibre2">Now there are 8 numbers and hence you need 3 bits to put it in memory in case you choose to encode in binary. Converting the array into binary you get the following vector:</p>
<p class="calibre2"><img class="fm-editor-equation37" src="../images/00070.jpeg"/></p>
<p class="calibre2">So you need 24 bits to save the previous array in memory if you want to save each number as 3 bits. What if you want to reduce the amount of memory consumption of each number by 1 bit? If you are able to achieve that, you will be able to use only 16 bits to save the previous array in memory and thus save 8 bits, achieving compression. To do that, you can consider the numbers 0, 2, 4, 6 as your codebook which will maps to the vectors 00, 01, 10, 11:</p>
<p class="calibre2"><img class="fm-editor-equation38" src="../images/00071.jpeg"/></p>
<p class="calibre2">During the transformation, all numbers between 0 and 2 are mapped to 00, everything from 2 to 4 is mapped to 01, and so on. Hence your original vector gets changed to the following:</p>
<p class="calibre2"><img class="fm-editor-equation39" src="../images/00072.jpeg"/></p>
<p class="calibre2">The amount of space that this vector occupies in memory is only 8 bits.</p>
<p class="calibre2">Note that we can also target to encode our representation to 1 bit. In that case, only 2 bits of overall memory would be used for the array. But we lose more information about the original distribution. Hence, generalizing this understanding, decreasing the size of the codebook increases the compression ratio, but distortion of the original data increases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Finding the codebook for high-dimensional spaces</h1>
                
            
            <article>
                
<p class="calibre2">The principal goal while designing for vector quantizers is to find a codebook, specifying a decoder, and a rule for specifying the encoder, such that the overall performance of the vector space is optimal.</p>
<p class="calibre2">An important class of quantizers is the Voronoi or nearest-neighbor quantizers. Given a set of L codevectors <img class="fm-editor-equation40" src="../images/00073.jpeg"/>  of size N, along with a distance measure <img class="fm-editor-equation41" src="../images/00074.jpeg"/>, the R<sup class="calibre23">k</sup> space is partitioned into L disjoint regions, known as Voronoi regions, with each codevector associated with each region. A particular Voronoi region Ω<sub class="calibre29">j</sub> associated with the codevector <em class="calibre16">v<sub class="calibre29">j</sub></em> contains all the points in <em class="calibre16">R<sup class="calibre23">k</sup></em> space nearer to <em class="calibre16">v<sub class="calibre29">j</sub></em> than any other codevector and is the nearest-neighbor locus region of <em class="calibre16">v<sub class="calibre29">j</sub></em>. The following is an example Voronoi diagram for a given space with the associated codevectors denoted by points:</p>
<div class="cdpaligncenter"><img src="../images/00075.jpeg" class="calibre48"/></div>
<p class="calibre2"/>
<p class="calibre2">The code for getting the above diagram can also be found in the Jupyter notebook for the repo: <a href="https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb" class="calibre9">https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb</a>.</p>
<p class="calibre2">So, essentially, you can see that there are two steps to the process:</p>
<ol class="calibre13">
<li value="1" class="calibre11"><strong class="calibre1">Constructing the Voronoi space</strong>: A preprocessing phase of constructing the Voronoi space, representing it in the form of a graph data structure and associating with each facet (for instance, vertex, edge, and region), the set of closest codevectors.</li>
<li value="2" class="calibre11"><strong class="calibre1">Finding the codevector</strong>: Given the Voronoi subdivisions, determine the facet of the subdivision which contains the query vector, and the associate code vector is the desired nearest neighbor.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Product quantization</h1>
                
            
            <article>
                
<p class="calibre2">Till now, you might have understood that in vector quantization, you cluster the search space into a number of bins based on the distance to the cluster centroid. If a query vector is quantized to that bin, all other vectors in the bin are good candidates.</p>
<p class="calibre2">Unfortunately, if a query lies at the edge, one has to consider all the neighboring bins as well. This might not seem a big deal until you realize that number of neighbors adjacent to each Voronoi cell increases exponentially with respect to the dimension <em class="calibre16">N</em> of the space. Note that when creating fastText vectors, we routinely deal in high dimensions such as 100, 300, and so on.</p>
<p class="calibre2">The default vectors in fastText are 100-dimensional vectors. A quantizer working on 50-bit codes, which means we only have a half (0.5) bit per component, contains <img class="fm-editor-equation42" src="../images/00076.jpeg"/> centroids (around 150 TB). Product quantization is an efficient solution to address this issue. The input vector <em class="calibre16">x<sub class="calibre29">i</sub></em> is divided into m distinct subvectors j <img class="fm-editor-equation43" src="../images/00077.jpeg"/> of dimension <em class="calibre16">D*</em> = <em class="calibre16">D/m</em>, where <em class="calibre16">D</em> is a multiple of <em class="calibre16">m</em>. The subvectors are quantized separately using m distinct quantizers. A given vector ν is therefore mapped as follows:</p>
<p class="calibre2">_<img class="fm-editor-equation44" src="../images/00078.jpeg"/></p>
<p class="calibre2">Here, <em class="calibre16">q<sub class="calibre29">j</sub></em> is a low-complexity quantizer associated with the <em class="calibre16">j</em>th subvector. With the subquantizer <em class="calibre16">q<sub class="calibre29">j</sub></em>, we associate the index <em class="calibre16">Ι<sub class="calibre29">j</sub></em>, the codebook <em class="calibre16">C<sub class="calibre29">j</sub></em>, and the corresponding reproduction values <em class="calibre16">c<sub class="calibre29">j,i</sub></em>.</p>
<p class="calibre2">A reproduction value of the <strong class="calibre4">product quantizer</strong> (<strong class="calibre4">PQ</strong>) is identified by an element of the product index set <img class="fm-editor-equation45" src="../images/00079.jpeg"/>. The codebook is therefore defined as the Cartesian product:</p>
<p class="calibre2"><img class="fm-editor-equation46" src="../images/00080.jpeg"/></p>
<p class="calibre2">A centroid of this set is the concatenation of the centroids of the m sub-quantizers. From now on, we assume that all sub-quantizers have the same finite number <em class="calibre16">k*</em> of reproduction values. In that case, the total number of centroids is given by the following:</p>
<p class="calibre2"><img class="fm-editor-equation47" src="../images/00081.jpeg"/></p>
<p class="calibre2">In fastText, the two parameters involved in product quantization, namely the number of sub-quantizers m and the number of bits b per quantization index, are typically set to <img class="fm-editor-equation48" src="../images/00082.jpeg"/>, and <em class="calibre16">b</em>=8.</p>
<p class="calibre2">Thus, a PQ can generate an exponentially large codebook at very low memory/time cost. The essence of a PQ is to decompose the high-dimensional vector space into the Cartesian product of the subspaces and then quantize these subspaces separately. The optimal space decomposition is important for good product quantization implementation, and as per current knowledge, this is generally done by minimizing quantization distortions with respect to the space decomposition and quantization codebooks. There are two known ways to solve this optimization problem. One is using iterative quantization. A simple example of iteratively finding the codevectors is shown in the notebook, which can be considered a specific subquantizer. If you are interested, you can take a look at <em class="calibre16">Optimised Product Quantization</em> by Kaiming He et al.</p>
<p class="calibre2">The other method, and one which fastText adopts, is having a Gaussian assumption for the input vectors and finding the k-means through expectation maximization. Take a look at the algorithm in this k-means function: <a href="https://github.com/facebookresearch/fastText/blob/d647be03243d2b83d0b4659a9dbfb01e1d1e1bf7/src/productquantizer.cc#L115" class="calibre9">src/productquantizer.cc#L115<span>.</span></a></p>
<p class="calibre2">After quantizing on the input matrix, retraining is done on the output matrix, keeping the input matrix the same. This is done so that the network readjusts to the quantization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Additional steps</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Following are the additional steps that can be taken:</span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Feature selection and pruning</strong>: Pruning is done on those features that do not have a big influence on the decision of the classifier. During the classification step, only a limited number of <em class="calibre21">K</em> words and n-grams are selected. So for each document, first verification is done if it is covered by a retrained feature and, if not, we add the feature with the highest norm to the set of retrained features.</li>
<li class="calibre11"><strong class="calibre1">Hashing</strong>: Both the words and n-grams are also hashed to further save on the memory.</li>
</ul>
<p class="calibre2">If you decide to implement these techniques discussed in your own model compression methods, there are various ideas here that you can tweak and see if you get better performance in your particular domain:</p>
<ul class="calibre10">
<li class="calibre11">You can explore whether some other distance metrics makes sense for finding the k-means.</li>
<li class="calibre11">You can change the pruning strategy based on neighborhood, entropy, and so on.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">With this chapter, you have completed a deep dive into the theory behind how the fastText model is designed and implemented, the benefits, and the things that you need to consider while implementing it in your ML pipeline.</p>
<p class="calibre2">The next part of the book is about implementation and deployment and we start with how to use fastText in a Python environment in the next chapter.</p>


            </article>

            
        </section>
    </body></html>