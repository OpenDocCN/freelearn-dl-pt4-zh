["```py\n optimizer.zero_grad() \n        logits_v = net(states_v) \n        log_prob_v = F.log_softmax(logits_v, dim=1) \n        log_p_a_v = log_prob_v[range(BATCH_SIZE), batch_actions_t] \n        log_prob_actions_v = batch_scale_v * log_p_a_v \n        loss_policy_v = -log_prob_actions_v.mean()\n```", "```py\n loss_policy_v.backward(retain_graph=True)\n```", "```py\n grads = np.concatenate([p.grad.data.numpy().flatten() \n                                for p in net.parameters() \n                                if p.grad is not None])\n```", "```py\n prob_v = F.softmax(logits_v, dim=1) \n        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean() \n        entropy_loss_v = -ENTROPY_BETA * entropy_v \n        entropy_loss_v.backward() \n        optimizer.step()\n```", "```py\n g_l2 = np.sqrt(np.mean(np.square(grads))) \n        g_max = np.max(np.abs(grads)) \n        writer.add_scalar(\"grad_l2\", g_l2, step_idx) \n        writer.add_scalar(\"grad_max\", g_max, step_idx) \n        writer.add_scalar(\"grad_var\", np.var(grads), step_idx)\n```", "```py\nGAMMA = 0.99 \nLEARNING_RATE = 0.001 \nENTROPY_BETA = 0.01 \nBATCH_SIZE = 128 \nNUM_ENVS = 50 \n\nREWARD_STEPS = 4 \nCLIP_GRAD = 0.1\n```", "```py\nclass AtariA2C(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): \n        super(AtariA2C, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), \n            nn.ReLU(), \n            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, kernel_size=3, stride=1), \n            nn.ReLU(), \n            nn.Flatten(), \n        ) \n\n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.policy = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, n_actions) \n        ) \n        self.value = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, 1) \n        )\n```", "```py\n def forward(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]: \n        xx = x / 255 \n        conv_out = self.conv(xx) \n        return self.policy(conv_out), self.value(conv_out)\n```", "```py\ndef unpack_batch(batch: tt.List[ExperienceFirstLast], net: AtariA2C, \n                 device: torch.device, gamma: float, reward_steps: int): \n    states = [] \n    actions = [] \n    rewards = [] \n    not_done_idx = [] \n    last_states = [] \n    for idx, exp in enumerate(batch): \n        states.append(np.asarray(exp.state)) \n        actions.append(int(exp.action)) \n        rewards.append(exp.reward) \n        if exp.last_state is not None: \n            not_done_idx.append(idx) \n            last_states.append(np.asarray(exp.last_state))\n```", "```py\n states_t = torch.FloatTensor(np.asarray(states)).to(device) \n    actions_t = torch.LongTensor(actions).to(device)\n```", "```py\n rewards_np = np.array(rewards, dtype=np.float32) \n    if not_done_idx: \n        last_states_t = torch.FloatTensor( \n            np.asarray(last_states)).to(device) \n        last_vals_t = net(last_states_t)[1] \n        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0] \n        last_vals_np *= gamma ** reward_steps \n        rewards_np[not_done_idx] += last_vals_np\n```", "```py\n ref_vals_t = torch.FloatTensor(rewards_np).to(device) \n    return states_t, actions_t, ref_vals_t\n```", "```py\nif __name__ == \"__main__\": \n    parser = argparse.ArgumentParser() \n    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\") \n    parser.add_argument(\"--use-async\", default=False, action=’store_true’, \n                        help=\"Use async vector env (A3C mode)\") \n    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\") \n    args = parser.parse_args() \n    device = torch.device(args.dev) \n\n    env_factories = [ \n        lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\")) \n        for _ in range(NUM_ENVS) \n    ] \n    if args.use_async: \n        env = gym.vector.AsyncVectorEnv(env_factories) \n    else: \n        env = gym.vector.SyncVectorEnv(env_factories) \n    writer = SummaryWriter(comment=\"-pong-a2c_\" + args.name)\n```", "```py\n net = common.AtariA2C(env.single_observation_space.shape, \n                          env.single_action_space.n).to(device) \n    print(net) \n\n    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, device=device) \n    exp_source = VectorExperienceSourceFirstLast( \n        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS) \n\n    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n```", "```py\n batch = [] \n\n    with common.RewardTracker(writer, stop_reward=18) as tracker: \n        with TBMeanTracker(writer, batch_size=10) as tb_tracker: \n            for step_idx, exp in enumerate(exp_source): \n                batch.append(exp) \n\n                new_rewards = exp_source.pop_total_rewards() \n                if new_rewards: \n                    if tracker.reward(new_rewards[0], step_idx): \n                        break \n\n                if len(batch) < BATCH_SIZE: \n                    continue\n```", "```py\n states_t, actions_t, vals_ref_t = common.unpack_batch( \n                    batch, net, device=device, gamma=GAMMA, reward_steps=REWARD_STEPS) \n                batch.clear() \n\n                optimizer.zero_grad() \n                logits_t, value_t = net(states_t)\n```", "```py\n loss_value_t = F.mse_loss(value_t.squeeze(-1), vals_ref_t)\n```", "```py\n log_prob_t = F.log_softmax(logits_t, dim=1) \n                adv_t = vals_ref_t - value_t.detach() \n                log_act_t = log_prob_t[range(BATCH_SIZE), actions_t] \n                log_prob_actions_t = adv_t * log_act_t \n                loss_policy_t = -log_prob_actions_t.mean()\n```", "```py\n prob_t = F.softmax(logits_t, dim=1) \n                entropy_loss_t = ENTROPY_BETA * (prob_t * log_prob_t).sum(dim=1).mean()\n```", "```py\n loss_policy_t.backward(retain_graph=True) \n                grads = np.concatenate([ \n                    p.grad.data.cpu().numpy().flatten() \n                    for p in net.parameters() if p.grad is not None \n                ])\n```", "```py\n loss_v = entropy_loss_t + loss_value_t \n                loss_v.backward() \n                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD) \n                optimizer.step() \n                loss_v += loss_policy_t\n```", "```py\n tb_tracker.track(\"advantage\", adv_t, step_idx) \n                tb_tracker.track(\"values\", value_t, step_idx) \n                tb_tracker.track(\"batch_rewards\", vals_ref_t, step_idx) \n                tb_tracker.track(\"loss_entropy\", entropy_loss_t, step_idx) \n                tb_tracker.track(\"loss_policy\", loss_policy_t, step_idx) \n                tb_tracker.track(\"loss_value\", loss_value_t, step_idx) \n                tb_tracker.track(\"loss_total\", loss_v, step_idx) \n                tb_tracker.track(\"grad_l2\", np.sqrt(np.mean(np.square(grads))), step_idx) \n                tb_tracker.track(\"grad_max\", np.max(np.abs(grads)), step_idx) \n                tb_tracker.track(\"grad_var\", np.var(grads), step_idx)\n```", "```py\nChapter12$ ./02_pong_a2c.py --dev cuda -n tt \nA.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) \n[Powered by Stella] \nAtariA2C( \n  (conv): Sequential( \n   (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) \n   (1): ReLU() \n   (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) \n   (3): ReLU() \n   (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) \n   (5): ReLU() \n   (6): Flatten(start_dim=1, end_dim=-1) \n  ) \n  (policy): Sequential( \n   (0): Linear(in_features=3136, out_features=512, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=512, out_features=6, bias=True) \n  ) \n  (value): Sequential( \n   (0): Linear(in_features=3136, out_features=512, bias=True) \n   (1): ReLU() \n   (2): Linear(in_features=512, out_features=1, bias=True) \n  ) \n) \n37850: done 1 games, mean reward -21.000, speed 1090.79 f/s \n39250: done 2 games, mean reward -21.000, speed 1111.24 f/s \n39550: done 3 games, mean reward -21.000, speed 1118.06 f/s \n40000: done 4 games, mean reward -21.000, speed 1083.18 f/s \n40300: done 5 games, mean reward -21.000, speed 1141.46 f/s \n40750: done 6 games, mean reward -21.000, speed 1077.44 f/s \n40850: done 7 games, mean reward -21.000, speed 940.09 f/s \n...\n```", "```py\nGAMMA = 0.99 \nLEARNING_RATE = 0.001 \nENTROPY_BETA = 0.01 \nREWARD_STEPS = 4 \nCLIP_GRAD = 0.1 \n\nPROCESSES_COUNT = 4 \nNUM_ENVS = 8 \nGRAD_BATCH = 64 \nTRAIN_BATCH = 2 \n\nENV_NAME = \"PongNoFrameskip-v4\" \nNAME = ’pong’ \nREWARD_BOUND = 18\n```", "```py\ndef make_env() -> gym.Env: \n    return ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\")) \n\ndef grads_func(proc_name: str, net: common.AtariA2C, device: torch.device, \n               train_queue: mp.Queue): \n    env_factories = [make_env for _ in range(NUM_ENVS)] \n    env = gym.vector.SyncVectorEnv(env_factories) \n\n    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], device=device, apply_softmax=True) \n    exp_source = VectorExperienceSourceFirstLast( \n        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS) \n\n    batch = [] \n    frame_idx = 0 \n    writer = SummaryWriter(comment=proc_name)\n```", "```py\n with common.RewardTracker(writer, REWARD_BOUND) as tracker: \n        with TBMeanTracker(writer, 100) as tb_tracker: \n            for exp in exp_source: \n                frame_idx += 1 \n                new_rewards = exp_source.pop_total_rewards() \n                if new_rewards and tracker.reward(new_rewards[0], frame_idx): \n                    break \n\n                batch.append(exp) \n                if len(batch) < GRAD_BATCH: \n                    continue\n```", "```py\n data = common.unpack_batch(batch, net, device=device, gamma=GAMMA, \n                                           reward_steps=REWARD_STEPS) \n                states_v, actions_t, vals_ref_v = data \n                batch.clear() \n\n                net.zero_grad() \n                logits_v, value_v = net(states_v) \n                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v) \n\n                log_prob_v = F.log_softmax(logits_v, dim=1) \n                adv_v = vals_ref_v - value_v.detach() \n                log_p_a = log_prob_v[range(GRAD_BATCH), actions_t] \n                log_prob_actions_v = adv_v * log_p_a \n                loss_policy_v = -log_prob_actions_v.mean() \n\n                prob_v = F.softmax(logits_v, dim=1) \n                ent = (prob_v * log_prob_v).sum(dim=1).mean() \n                entropy_loss_v = ENTROPY_BETA * ent \n\n                loss_v = entropy_loss_v + loss_value_v + loss_policy_v \n                loss_v.backward()\n```", "```py\n tb_tracker.track(\"advantage\", adv_v, frame_idx) \n                tb_tracker.track(\"values\", value_v, frame_idx) \n                tb_tracker.track(\"batch_rewards\", vals_ref_v, frame_idx) \n                tb_tracker.track(\"loss_entropy\", entropy_loss_v, frame_idx) \n                tb_tracker.track(\"loss_policy\", loss_policy_v, frame_idx) \n                tb_tracker.track(\"loss_value\", loss_value_v, frame_idx) \n                tb_tracker.track(\"loss_total\", loss_v, frame_idx)\n```", "```py\n nn_utils.clip_grad_norm_( \n                    net.parameters(), CLIP_GRAD) \n                grads = [ \n                    param.grad.data.cpu().numpy() if param.grad is not None else None \n                    for param in net.parameters() \n                ] \n                train_queue.put(grads) \n\n    train_queue.put(None)\n```", "```py\nif __name__ == \"__main__\": \n    mp.set_start_method(’spawn’) \n    os.environ[’OMP_NUM_THREADS’] = \"1\" \n    parser = argparse.ArgumentParser() \n    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\") \n    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\") \n    args = parser.parse_args() \n    device = torch.device(args.dev) \n\n    env = make_env() \n    net = common.AtariA2C(env.observation_space.shape, env.action_space.n).to(device) \n    net.share_memory()\n```", "```py\n optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3) \n\n    train_queue = mp.Queue(maxsize=PROCESSES_COUNT) \n    data_proc_list = [] \n    for proc_idx in range(PROCESSES_COUNT): \n        proc_name = f\"-a3c-grad_pong_{args.name}#{proc_idx}\" \n        p_args = (proc_name, net, device, train_queue) \n        data_proc = mp.Process(target=grads_func, args=p_args) \n        data_proc.start() \n        data_proc_list.append(data_proc)\n```", "```py\n batch = [] \n    step_idx = 0 \n    grad_buffer = None \n\n    try: \n        while True: \n            train_entry = train_queue.get() \n            if train_entry is None: \n                break\n```", "```py\n step_idx += 1 \n\n            if grad_buffer is None: \n                grad_buffer = train_entry \n            else: \n                for tgt_grad, grad in zip(grad_buffer, train_entry): \n                    tgt_grad += grad\n```", "```py\n if step_idx % TRAIN_BATCH == 0: \n                for param, grad in zip(net.parameters(), grad_buffer): \n                    param.grad = torch.FloatTensor(grad).to(device) \n\n                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD) \n                optimizer.step() \n                grad_buffer = None\n```", "```py\n finally: \n        for p in data_proc_list: \n            p.terminate() \n            p.join()\n```"]