<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Parameter Learning Using Maximum Likelihood</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed the state inference in the case of a <strong>Hidden Markov Model</strong> (<strong>HMM</strong>). We tried to predict the next state for an HMM using the information of previous state transitions. But in each cases, we had assumed that we already knew the transition and emission probabilities of the model. But in real-life problems, we usually need to learn these parameters from our observations. </p>
<p>In this chapter, we will try to estimate the parameters of our HMM model through data gathered from observations. We will be covering the following topics:</p>
<ul>
<li>Maximum likelihood learning, with examples</li>
<li>Maximum likelihood learning in HMMs</li>
<li>Expectation maximization algorithms</li>
<li>The Baum-Welch algorithm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum likelihood learning</h1>
                </header>
            
            <article>
                
<p>Before diving into learning about <strong>maximum likelihood estimation</strong> (<strong>MLE</strong>) in HMMs, let's try to understand the basic concepts of MLE in generic cases. As the name suggests, MLE tries to select the parameters of the model that maximizes the likelihood of observed data. The likelihood for any model with given parameters is defined as the probability of getting the observed data, and can be written as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b83fac20-1c71-4634-a46f-dac1aaf79d54.png" style="width:9.67em;height:1.33em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here, <em>D={D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, …, D<sub>n</sub>}</em> is the observed data, and <em>θ</em> is the set of parameters governing our model. In most cases, for simplicity, we assume that the datapoints are <strong>independent and identically distributed</strong> (<strong>IID</strong>). With that assumption, we can simplify the definition of our likelihood function as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/57cd5871-7d15-439f-8668-48a8c5630604.png" style="width:16.83em;height:5.58em;"/></p>
<p>Here, we have used the multiplication rule for independent random variables to decompose the joint distribution into product over individual datapoint.</p>
<p class="CDPAlignLeft CDPAlign">Coming back to MLE, MLE tries to find the value of <em>θ</em> for which the value of <em>P(D|θ)</em> is <span>at a maximum</span>. So, basically we now have an optimization problem at hand:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d3b5257-b933-46d8-b328-5a1f5f08da60.png" style="width:10.58em;height:1.92em;"/></p>
<p>In the next couple of subsections, we will try to apply MLE to some simple examples to understand it better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLE in a coin toss</h1>
                </header>
            
            <article>
                
<p>Let's assume that we want to learn a model of a given coin using observations obtained from tossing it. Since a coin can only have two outcomes, heads or tails, it can be modeled using a single parameter. Let's say we define the parameter as <em>θ</em>, which is the probability of getting heads when the coin is tossed. The probability of getting tails will automatically be <em>1-θ</em> because getting either heads or tails are mutually exclusive events. </p>
<p>We have our model ready, so let's move on to computing the likelihood function of this model. Let's assume that we are given some observations of coin tosses as <em>D={H,H,T,H,T,T}</em>. For the given data we can write our likelihood function as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b32bf448-9d8a-4656-b29d-bf6641bb8652.png" style="width:28.33em;height:6.50em;"/></p>
<p>Now, we would like to find the value of <em>θ</em> that would maximize <em>P(D|θ)</em>. For that, we take the derivative of our likelihood function, equate it to <em>0</em>, and then solve it for <em>θ</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e2c3b80e-bf69-4d12-873b-18a90aea69d6.png" style="width:23.25em;height:7.17em;"/></p>
<p>Therefore, our MLE estimator learned that the probability of getting heads on tossing the coin is <em>0.5</em>. Looking at our observations, we would expect the same probability as we have an equal number of heads and tails in our observed data.</p>
<p>Let's now try to write code to learn the parameter <em>θ</em> for our model. But as we know that finding the optimal value can run into numerical issues on a computer, is there a possible way to avoid that and directly be able to compute <em>θ<sub>MLE</sub></em>? If we look closely at our likelihood equation, we realize that we can write a generic formula for the likelihood for this model. If we assume that our data has <em>n</em> heads and <em>m</em> tails, we can write the likelihood as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1c241c3f-451d-4b92-9ec5-a9a6576f5673.png" style="width:10.08em;height:1.25em;"/></p>
<p>Now, we can actually find <em>θ<sub>MLE</sub></em> in a closed-form using this likelihood function and avoid relying on any numerical method to compute the optimum value: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/51def384-ba97-4278-8d00-35a663574cc5.png" style="width:21.50em;height:7.75em;"/></p>
<p>We can see that we have been able to find a closed form solution for the MLE solution to <em>θ</em>. Now, coding this up would be to simply compute the preceding formula as follows:</p>
<pre>import numpy as np<br/><br/><br/>def coin_mle(data):<br/>    """<br/>    Returns the learned probability of getting a heads using MLE.<br/>    <br/>    Parameters<br/>    ----------<br/>    data: list, array-like<br/>        The list of observations. 1 for heads and 0 for tails.<br/>    <br/>    Returns<br/>    -------<br/>    theta: The learned probability of getting a heads.<br/>    """<br/>    data = np.array(data)<br/>    n_heads = np.sum(data)<br/><br/>    return n_heads / data.size</pre>
<p>Now, let's try out our function for different datapoints:</p>
<pre>&gt;&gt;&gt; coin_mle([1, 1, 1, 0, 0])<br/>0.59999999999999998<br/><br/>&gt;&gt;&gt; coin_mle([1, 1, 1, 0, 0, 0])<br/>0.5<br/><br/>&gt;&gt;&gt; coin_mle([1, 1, 1, 0, 0, 0, 0])<br/>0.42857142857142855</pre>
<p>The outputs are as we expect, but one of the drawbacks of the MLE approach is that it is very sensitive to randomness in our data which, in some cases, might lead it to learn the wrong parameters. This is especially true in a case when the dataset is small in size. For example, let's say that we toss a fair coin three times and we get heads in each toss. The MLE approach, in this case, would learn the value of <em>θ</em> to be 1, which is not correct since we had a fair coin. The output is as follows:</p>
<pre>&gt;&gt;&gt; coin_mle([1, 1, 1])<br/>1.0</pre>
<p>In <a href="b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml" target="_blank">Chapter 5</a>, <em>Parameter Inference using Bayesian Approach</em>, we will try to solve this problem of MLE by starting with a prior distribution over the parameters, and it modifies its prior as it sees more and more data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLE for normal distributions</h1>
                </header>
            
            <article>
                
<p>In the previous section, we had a model with a single parameter. In this section, we will apply the same concepts to a slightly more complex model. We will try to learn the parameters of a normal distribution (also known as the <strong>Gaussian distribution</strong>) from a given observed data. As we know, the normal distribution is parametrized by its mean and standard deviation and the distribution is given as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5f1a73b2-ed97-47e4-8352-840dedaf7627.png" style="width:15.25em;height:3.25em;"/></p>
<p>Here, <em>µ</em> is the mean and <em>σ</em> is the standard deviation of the normal distribution.</p>
<p>As we discussed earlier, for estimating parameters using MLE we would need some observed data, which, in this case, we are assuming to be coming from a normal distribution (or that it can be approximated using a normal distribution). Let's assume that we have some observed data: <em>X = {x<sub>1</sub>, x<sub>2</sub>,...,x<sub>N</sub>}</em>. We want to estimate the parameters <em>μ</em> (mean) and <em>σ<sup>2</sup></em> (variance) for our model. </p>
<p>We will follow the same steps as we took in the previous section. We will start by defining the likelihood function for the normal distribution. The likelihood is the probability of the data being observed, given the parameters. So, given the observed data, we can state the likelihood function as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/62dc53b8-f2d5-49fb-b859-d3f8595ab12e.png" style="width:21.67em;height:11.08em;"/></div>
<p>One issue that we usually run into while trying to work with the product of small numbers is that the number can get too small for the computer to work with. To avoid running into this issue, we instead work with the log-likelihood instead of the simple likelihood. Since log is an increasing function, the maximum of the log-likelihood function would be for the same value of parameters as it would have been for the likelihood function. The log-likelihood can be defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/90870fda-be88-45f8-ad5f-82fb52efdefa.png" style="width:22.00em;height:10.42em;"/></div>
<p>We can then find the values of <em>μ<sub>MLE</sub></em> and <em>σ<sub>MLE</sub></em> that maximize the log-likelihood function by taking partial derivatives with respect to each of the variables, equating it to <em>0,</em> and solving the equation. To get the mean value, we need to take the partial derivative of the log-likelihood function with respect to <em>μ </em>while keeping <em>σ</em> as constant, and set it to <em>0</em><em>,</em> which gives us the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e649282d-ea2b-49fb-9c3d-c810b4f0c286.png" style="width:15.42em;height:7.58em;"/></div>
<p>Similarly, the MLE of standard deviation <em>σ<span><sup>2</sup></span></em><em> </em>can be computed by the partial derivative of the log-likelihood function with <em>σ<span><sup>2</sup></span></em> while keeping <em>μ</em> constant, equating it to <em>0,</em> and then solving for <em>σ<span><sup>2</sup></span></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9bd38c12-3bb6-433f-83f2-4fd95a074cf6.png" style="width:17.50em;height:4.33em;"/></div>
<p>As we can see, we have again been able to derive a closed-form solution for the MLE and thus wouldn't need to rely on numerical methods while coding it up. Let's try to code this up and check if our MLE approach has learnt the correct parameters:</p>
<pre>import numpy as np<br/><br/><br/>def gaussian_mle(data):<br/> """<br/> Returns the learned parameters of the Normal Distribution using MLE.<br/> <br/> Parameters<br/> ----------<br/> data: list, array-like<br/> The list of observed variables.<br/> <br/> Returns<br/> -------<br/> \mu: The learned mean of the Normal Distribution.<br/> \sigma: The learned standard deviation of the Normal Distribution.<br/> """<br/> data = np.array(data)</pre>
<pre> mu = np.mean(data)<br/> variance = np.sqrt(np.mean((data - mu)**2))<br/><br/> return mu, variance</pre>
<p>We have our learning function ready, so we can now generate some data from a known distribution and check if our function is able to learn the same parameters from the generated data:</p>
<pre>&gt;&gt;&gt; from numpy.random import normal<br/>&gt;&gt;&gt; data = normal(loc=1, scale=2, size=10)<br/>&gt;&gt;&gt; data<br/>array([ 1.8120102, 2.14363679, 1.49010868, -1.95531206, 1.62449155,<br/>        1.49345327, 1.48957918, -0.67536313, 4.31506202, 4.24883442])<br/><br/>&gt;&gt;&gt; mu, sigma = gaussian_mle(data)<br/>&gt;&gt;&gt; mu<br/>1.5986500906187573<br/>&gt;&gt;&gt; sigma<br/>1.805051208889392</pre>
<p>In this example, we can see that the learned values are not very accurate. This is because of the problem with the MLE being too sensitive to the observed datapoints, as we discussed in the previous section. Let's try to run this same example with more observed data:</p>
<pre>&gt;&gt;&gt; data = normal(loc=1, scale=2, size=1000)<br/>&gt;&gt;&gt; data[:10]<br/>array([ 4.5855015, 1.55162883, -1.61385859, 0.52543984, 0.90247428,<br/>        3.40717092, 1.4078157, 0.01560836, -1.19409859, -0.01641439])<br/><br/>&gt;&gt;&gt; mu, sigma = gaussian_mle(data)<br/>&gt;&gt;&gt; mu<br/> 1.0437186891666821<br/>&gt;&gt;&gt; sigma<br/>1.967211026428509</pre>
<p>In this case, with more data, we can see that the learned values are much closer to our original values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLE for HMMs</h1>
                </header>
            
            <article>
                
<p>Having a basic understanding of MLE, we can now move on to applying these concepts to the case of HMMs. In the next few subsections, we will see two possible scenarios of learning in HMMs, namely, supervised learning and unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>In the case of supervised learning, we use the data generated by sampling the process that we are trying to model. If we are trying to parameterize our HMM model using simple discrete distributions, we can simply apply the MLE to compute the transition and emission distributions by counting the number of transitions from any given state to another state. Similarly, we can compute the emission distribution by counting the output states from different hidden states. Therefore the transition and emission probabilities can be computed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0ebe8117-16f0-4638-a4c4-2235101afd0e.png" style="width:22.33em;height:9.75em;"/></p>
<p>Here, <em>T(i,j)</em> is the transition probability from state <em>i</em> to state <em>j</em>. And <em>E(i,s) </em>is the emission probability of getting state <em>s</em> from state <em>i</em>.</p>
<p>Let's take a very simple example to make this clearer. We want to model the weather and whether or not it would rain over a period of time. Also, we assume that the weather can take three possible states:</p>
<ul>
<li><em>Sunny (S)</em></li>
<li><em>Cloudy (C)</em></li>
<li><em>Windy (W)</em></li>
</ul>
<p><em> </em>And the <em>Rain</em> variable can have two possible states; <em>that it rained (R)</em> or <em>that it didn't rain (NR)</em>. An HMM model would look something like this:</p>
<p>And let's say we have some observed data for this which looks something like <em>D={(S,NR), (S,NR), (C,NR), (C,R), (C,R), (W,NR), (S,NR), (W,R), (C,NR)}</em>. Here, the first element of each datapoint represents the observed weather<em> </em>that day and the second element represents whether it rained or not that day. Now, using the formulas that we derived earlier, we can easily compute the transition and emission probabilities. We will start with computing the transition probability from <em>S</em> to <em>S</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/33c63d6c-3e65-4819-9696-899a449302b0.png" style="width:26.17em;height:5.42em;"/></p>
<p>Similarly, we can compute the transition probabilities for all the other combinations of states:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/db4f5682-7f09-4d02-a7b4-d7149798095c.png" style="width:8.00em;height:11.92em;"/></p>
<p>And, hence, we have our complete transition probability over all the possible states of the weather. We can represent it in tabular form to look nicer:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td><strong>Sunny(S)</strong></td>
<td><strong>Cloudy(C)</strong></td>
<td><strong>Windy(W)</strong></td>
</tr>
<tr>
<td><strong>Sunny(S)</strong></td>
<td>0.33</td>
<td>0.33</td>
<td>0.33</td>
</tr>
<tr>
<td><strong>Cloudy(C)</strong></td>
<td>0</td>
<td>0.66</td>
<td>0.33</td>
</tr>
<tr>
<td><strong>Windy(W)</strong></td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign">Table 1: Transition probability for the weather model</div>
<p>Now, coming to computing the emission probability, we can again just follow the formula derived <span>previously</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/710bbee7-885a-4fb0-8f3c-8fddb48e341c.png" style="width:25.17em;height:6.00em;"/></p>
<p>Similarly, we can compute all the other values in the distribution:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2b4d9790-4cd0-403b-861a-927dcdc42baf.png" style="width:8.00em;height:6.67em;"/></p>
<p>And hence our emission probability can be written in tabular form as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td><strong>Sunny(S)</strong></td>
<td><strong>Cloudy(C)</strong></td>
<td><strong>Windy(W)</strong></td>
</tr>
<tr>
<td><strong>Rain (R)</strong></td>
<td>0</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td><strong>No Rain (NR)</strong></td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign">Table 2: Emission probability for the weather model</div>
<p>In the previous example, we saw how we can compute the parameters of an HMM using MLE and some simple computations. But, because in this case we had assumed the transition and emission probabilities as simple discrete conditional distribution, the computation was much easier. With more complex cases, we will need to estimate more parameters than we did in the previous section in the case of the normal distribution.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code</h1>
                </header>
            
            <article>
                
<p>Let's now try to code up the preceding algorithm:</p>
<pre>def weather_fit(data):<br/>    """<br/>    Learn the transition and emission probabilities from the given data<br/>    for the weather model.<br/><br/>    Parameters<br/>    ----------<br/>    data: 2-D list (array-like)<br/>    Each data point should be a tuple of size 2 with the first element<br/>    representing the state of <em>Weather </em>and the second element representing<br/>    whether it rained or not.<br/>    Sunny = 0, Cloudy = 1, Windy = 2<br/>    Rain = 0, No Rain = 1<br/> <br/>    Returns<br/>    -------<br/>    transition probability: 2-D array<br/>    The conditional distribution representing the transition probability <br/>    of the model.<br/>    emission probability: 2-D array<br/>    The conditional distribution representing the emission probability <br/>    of the model.<br/>    """<br/>    data = np.array(data)<br/> <br/>    transition_counts = np.zeros((3, 3))<br/>    emission_counts = np.zeros((3, 2))<br/><br/>    for index, datapoint in enumerate(data):<br/>        if index != len(data)-1:<br/>            transition_counts[data[index][0], data[index+1][0]] += 1<br/>        emission_counts[data[index][0], data[index][1]] += 1<br/><br/>    transition_prob = transition_counts / np.sum(transition_counts, axis=0)<br/>    emission_prob = (emission_counts.T / np.sum(emission_counts.T, axis=0)).T<br/><br/>    return transition_prob, emission_prob</pre>
<p>Let's generate some data and try learning the parameters using the preceding function:</p>
<pre>&gt;&gt;&gt; import numpy as np<br/>&gt;&gt;&gt; weather_data = np.random.randint(low=0, high=3, size=1000)<br/>&gt;&gt;&gt; rain_data = np.random.randint(low=0, high=2, size=1000)<br/>&gt;&gt;&gt; data = list(zip(weather_data, rain_data))<br/>&gt;&gt;&gt; transition_prob, emission_prob = weather_fit(data)<br/>&gt;&gt;&gt; transition_prob<br/>array([[ 0.3125, 0.38235294, 0.27272727],<br/>       [ 0.28125, 0.38235294, 0.36363636],<br/>       [ 0.40625, 0.23529412, 0.36363636]])<br/><br/>&gt;&gt;&gt; emission_prob<br/>array([[ 0.3125, 0.38235294, 0.27272727],<br/>       [ 0.28125, 0.38235294, 0.36363636],<br/>       [ 0.40625, 0.23529412, 0.36363636]])<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>In the previous section, we saw how we can use supervised learning in a case where we have all the variables observed, <span>including </span>the hidden variables. But that is usually not the case with real-life problems. For such cases, we use unsupervised learning to estimate the parameters of the model.</p>
<p> The two main learning algorithms used for this are the following:</p>
<ul>
<li>The Viterbi learning algorithm</li>
<li>The Baum-Welch algorithm</li>
</ul>
<p>We will discuss these in the next couple of subsections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viterbi learning algorithm</h1>
                </header>
            
            <article>
                
<p>The Viterbi learning algorithm (not to be confused with the Viterbi algorithm for state estimation) takes a set of training observations <em>O<sup>r</sup></em>, with <em>1≤r≤R,</em> and estimates the parameters of a single HMM by iteratively computing Viterbi alignments. When used to initialize a new HMM, the Viterbi segmentation is replaced by a uniform segmentation (that is, each training observation is divided into <em>N</em> equal segments) for the first iteration.</p>
<p>Other than the first iteration on a new model, each training sequence <em>O</em> is segmented using a state alignment procedure which results from maximizing:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a9db8ff8-70d9-4d24-b6cb-b6b75107d1c1.png" style="width:14.92em;height:2.33em;"/></p>
<p>for <em>1&lt;i&lt;N</em> where:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4b6267fd-58d5-4263-b1a1-73900e2f01ea.png" style="width:21.42em;height:3.75em;"/></p>
<p>And the initial conditions are given by:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1892e2c6-f317-44fa-9d68-6d88fa772b0c.png" style="width:5.92em;height:1.58em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d8b3a899-783f-4721-bbad-d2e6f3b07fd0.png" style="width:10.33em;height:1.58em;"/></p>
<p class="mce-root"/>
<p>for <em>1&lt;j&lt;N</em>. And, in the discrete case, the output probability <img class="fm-editor-equation" src="assets/67e61a73-a5d6-43a1-886d-1273ee4fec13.png" style="width:3.17em;height:1.83em;"/>is defined as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d8b2df7f-45e2-48bd-9844-2685f99504f1.png" style="width:14.33em;height:3.67em;"/></p>
<p>where <em>S</em> is the total number of streams, <em>v<sub>s</sub>(O<sub>st</sub>)</em> is the output given, the input <em>O<sub>st,</sub></em> and <em>P<sub>js</sub>[v]</em> is the probability of state <em>j</em> to give an output <em>v</em>.</p>
<p>If <em>A<sub>ij</sub></em> represents the total number of transitions from state <em>i</em> to state <em>j</em> in performing the preceding maximizations, then the transition probabilities can be estimated from the relative frequencies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6d90b2a9-b716-4397-a074-b8f018b14a28.png" style="width:9.75em;height:4.08em;"/></p>
<p>The sequence of states that maximizes <em>∅<sub>N</sub>(T)</em> implies an alignment of training data observations with states. Within each state, a further alignment of observations to mixture components is made. Usually, two mechanisms can be used for this, for each state and each output stream:</p>
<ul>
<li>Use clustering to allocate each observation <em>O<sub>st</sub></em> to one of <em>M<sub>s</sub></em> clusters</li>
<li>Associate each observation <em>O<sub>st</sub></em> with the mixture component with the highest probability</li>
</ul>
<p>In either case, the net result is that every observation is associated with a single unique mixture component. This association can be represented by the indicator function <img class="fm-editor-equation" src="assets/54848bcf-e5b3-42ce-a7d8-7afedce1e71c.png" style="width:2.92em;height:1.25em;"/>, which is <em>1</em> if <img class="fm-editor-equation" src="assets/746625b8-e57b-4c27-b113-12cd1284871f.png" style="width:1.58em;height:1.25em;"/> is associated with a mixture component <em>m</em> of stream <em>s</em> of state <em>j,</em> and is zero otherwise.</p>
<p>The means and variances are then estimated by computing simple means:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/785f9fb8-df5e-4cdc-b8be-d0ea5d87eea2.png" style="width:23.75em;height:4.50em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f00c98ac-1363-4bda-8bee-58585d1e2345.png" style="width:31.08em;height:4.42em;"/></p>
<p>And the mixture weights are based on the number of observations allocated to each component:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/862c8580-5b51-48fc-bdd2-75bcb49eda3c.png" style="width:14.25em;height:4.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Baum-Welch algorithm (expectation maximization)</h1>
                </header>
            
            <article>
                
<p> </p>
<p>The <strong>expectation maximization</strong> (<strong>EM</strong>) algorithm (known as <strong>Baum-Welch</strong> when applied to HMMs) is an iterative method used to find the maximum likelihood or <strong>maximum a posteriori</strong> (<strong>MAP</strong>) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an <strong>expectation</strong> (<strong>E</strong>) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a <strong>maximization </strong>(<strong>M</strong>) step, which computes parameters maximizing the expected log-likelihood found on the <em>E</em> step. These parameter estimates are then used to determine the distribution of the latent variables in the next <em>E</em> step.</p>
<p>The EM algorithm starts with initial value of parameters (<em>θ<sup>old</sup></em>). In the <em>E </em>step, we take these parameters and find the posterior distribution of latent variables <em>P(Z|X,θ<sup>old</sup>)</em>. We then use this posterior distribution to evaluate the expectation of the logarithm of the complete data likelihood function, as a function of the parameters <em>θ</em>, to give the function <em>Q(θ,θ<sup>old</sup>),</em> defined by the following: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b835312c-23b2-4ddd-b44c-87b037870f51.png" style="width:28.33em;height:3.58em;"/></div>
<p>Let's introduce some terms that can help us in the future. <em>γ(Z<sub>n</sub>)</em> to denote the marginal posterior distribution of a latent variable:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e473d34c-bc1d-425f-b461-ebd6bf8a8246.png" style="width:15.00em;height:2.08em;"/></div>
<p><em>ξ(z<sub>n-1</sub>, z<sub>n</sub>)</em> denoting the marginal posterior distribution of two successive latent variables:</p>
<div class="p1 CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/936a734c-181c-4c4b-969b-c593f6048656.png" style="width:22.08em;height:2.08em;"/></div>
<p>Thus, for each value of <em>n</em>, we can store <em>γ(Z<sub>n</sub>)</em> as a vector of <em>K</em> non-negative numbers that sum to <em>1,</em> and, similarly we can use a <em>K×K</em> matrix of non-negative numbers that sum to <em>1</em> to save <em>ξ(z<sub>n-1</sub>, z<sub>n</sub>)</em>.</p>
<p><span>As we have discussed in previous chapters, the latent variable <em>z<sub>n</sub></em></span><span> can be represented as <em>K</em></span><span> dimensional binary variable where <em>z<sub>nk</sub> = 1</em></span><span> when <em>z<sub>n</sub></em></span><span> is in state <em>k</em></span><span>. </span>We can also use it to denote the conditional probability of <em>z<sub>nk</sub><span> </span>= 1,</em> and similarly <em>ξ(z<sub>n-1</sub>, j, z<sub>nk</sub>)</em> to denote the conditional probability of <em>zn-1</em>, <em>j = 1,</em> and <em>z<sub>nk</sub> = 1</em>. As the expectation of a binary random variable is just the probability of its value being <em>1</em>, we can state the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/29aa84f0-6709-4d8e-9eed-2fdbb9d243aa.png" style="width:19.75em;height:3.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cace6a8f-3baa-4e7a-b215-cd7d51a9db15.png" style="width:31.33em;height:3.50em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">As we discussed in the previous chapter, the joint probability distribution of an HMM can be represented as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/ad2a4b0e-4746-4014-994f-7107eadce813.png" style="width:38.25em;height:4.83em;"/></p>
<p>Thus we can write the data likelihood function as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9185967c-2c6d-4a48-a76a-a43632f416ae.png" style="width:56.42em;height:9.08em;"/></div>
<p>In the <em>E</em> step we try to evaluate the quantities<em> γ(z<sub>n</sub>)</em> and <em>ξ(z<sub>n-1</sub>, z<sub>n</sub>)</em> efficiently. <span>For efficient computation of these two terms </span><span>we can use either a forward backward algorithm or the Viterbi algorithm as discussed in the previous chapter.</span> And in the <em>M</em> step, we try to maximize the value of <em>Q(θ, <span>θ</span><sup>old</sup>)</em> with respect to the parameters <span><em>θ={A, π, Φ}</em></span> in which we treat <em>γ(z<sub>n</sub>)</em> and <em><span>ξ</span>(z<sub>n-1</sub>, z<sub>n</sub>)</em> as constants.</p>
<p>In doing so, we get the MLE values of the parameters as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/77ebd97f-a907-45e7-a0f9-03dedef7cabc.png" style="width:10.33em;height:3.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/84bef755-e69e-4bb8-98a4-7397e9e18821.png" style="width:18.08em;height:4.08em;"/></div>
<p>If we assume the emission distribution to be a normal distribution such that <img class="fm-editor-equation" src="assets/dcabab1e-ffd3-473c-a5e6-8d5c9e8f1fd3.png" style="width:16.17em;height:1.92em;"/>, then the maximization of <em>Q(θ, <span>θ</span><sup>old</sup>)</em> with respect to <em>Φ<sub>k</sub></em> would result in the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2920ee83-9f17-4334-a247-16c18f4f1a89.png" style="width:12.42em;height:4.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b922047a-e242-4346-b85e-478a777dfef6.png" style="width:20.92em;height:3.75em;"/></div>
<p>The EM algorithm must be initialized by choosing starting values for <em>π</em> and <em>A</em>, which should, of course, be non-negative and should add up to <em>1</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code</h1>
                </header>
            
            <article>
                
<p>The algorithms for parameter estimation look quite complex but <kbd>hmmlearn</kbd>, a Python package for working with HMMs, has great implementations for it. <kbd>hmmlearn</kbd> is also hosted on PyPI so it can be installed directly using <kbd>pip:<em> </em>pip install hmmlearn</kbd>. For the code example, we will take an example of stock price prediction by learning a Gaussian HMM on stock prices. This example has been taken from the examples page of <kbd>hmmlearn</kbd>.</p>
<p>For the example, we also need the <kbd>matplotlib</kbd> and <kbd>datetime</kbd> packages which can also be installed using <kbd>pip</kbd>:</p>
<p class="mce-root"/>
<pre><strong>pip install matplotlib datetime</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Coming to the code, w<span>e should start by importing all the required packages:</span></p>
<pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function<br/></span><span class="kn"><br/>import</span> <span class="nn">datetime<br/></span><span class="kn"><br/>import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np<br/></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span><span class="p">,</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt<br/></span><span class="kn">from</span> <span class="nn">matplotlib.dates</span> <span class="kn">import</span> <span class="n">YearLocator</span><span class="p">,</span> <span class="n">MonthLocator<br/></span><span class="k"><br/>try</span><span class="p">:<br/>    </span><span class="kn">from</span> <span class="nn">matplotlib.finance</span> <span class="kn">import</span> <span class="n">quotes_historical_yahoo_ochl<br/></span><span class="k">except</span> <span class="ne">ImportError</span><span class="p">:<br/></span>    <span class="c1"># For Matplotlib prior to 1.5.<br/></span>    <span class="kn">from</span> <span class="nn">matplotlib.finance</span> <span class="kn">import</span> <span class="p">(<br/></span>        <span class="n">quotes_historical_yahoo</span> <span class="k">as</span> <span class="n">quotes_historical_yahoo_ochl<br/></span>    <span class="p">)<br/></span><span class="kn"><br/>from</span> <span class="nn">hmmlearn.hmm</span> <span class="kn">import</span> <span class="n">GaussianHMM<br/></span><span class="k"><br/>print</span><span class="p">(</span><span class="vm">__doc__</span><span class="p">)</span></pre>
<p>Next, we will fetch our stock price data from Yahoo! Finance:</p>
<pre><span class="n">quotes</span> <span class="o">=</span> <span class="n">quotes_historical_yahoo_ochl</span><span class="p">(<br/></span>    <span class="s2">"INTC"</span><span class="p">,</span> <span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="p">(</span><span class="mi">1995</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="p">(</span><span class="mi">2012</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))<br/></span><span class="c1"><br/># Unpack quotes<br/></span><span class="n">dates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">quotes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)<br/></span><span class="n">close_v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">q</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">quotes</span><span class="p">])<br/></span><span class="n">volume</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">q</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">quotes</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]<br/></span><span class="c1"><br/># Take diff of close value. Note that this makes<br/></span><span class="c1"># ``len(diff) = len(close_t) - 1``, therefore, other quantities also<br/></span><span class="c1"># need to be shifted by 1.<br/></span><span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">close_v</span><span class="p">)<br/></span><span class="n">dates</span> <span class="o">=</span> <span class="n">dates</span><span class="p">[</span><span class="mi">1</span><span class="p">:]<br/></span><span class="n">close_v</span> <span class="o">=</span> <span class="n">close_v</span><span class="p">[</span><span class="mi">1</span><span class="p">:]<br/></span><span class="c1"><br/># Pack diff and volume for training.<br/></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">diff</span><span class="p">,</span> <span class="n">volume</span><span class="p">])</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Next, we define a Gaussian HMM model and learn the parameters for our data:</p>
<pre><span class="c1"># Make an HMM instance and execute fit<br/></span><span class="n">model</span> <span class="o">=</span> <span class="n">GaussianHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">"diag"</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)<br/></span><span class="c1"><br/># Predict the optimal sequence of internal hidden state<br/></span><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p>We can now print out our learned parameters:</p>
<pre><span class="k">print</span><span class="p">(</span><span class="s2">"Transition matrix"</span><span class="p">)<br/></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">transmat_</span><span class="p">)<br/></span><span class="k">print</span><span class="p">()<br/></span><span class="k"><br/>print</span><span class="p">(</span><span class="s2">"Means and vars of each hidden state"</span><span class="p">)<br/></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">n_components</span><span class="p">):<br/></span>    <span class="k">print</span><span class="p">(</span><span class="s2">"{0}th hidden state"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))<br/></span>    <span class="k">print</span><span class="p">(</span><span class="s2">"mean = "</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">i</span><span class="p">])<br/></span>    <span class="k">print</span><span class="p">(</span><span class="s2">"var = "</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">covars_</span><span class="p">[</span><span class="n">i</span><span class="p">]))<br/></span>    <span class="k">print</span><span class="p">()</span></pre>
<p>The output is as follows:</p>
<pre><span class="n">Transition</span> <span class="n">matrix<br/></span><span class="p">[[</span>  <span class="mf">9.79220773e-01</span>   <span class="mf">2.57382344e-15</span>   <span class="mf">2.72061945e-03</span>   <span class="mf">1.80586073e-02</span><span class="p">]<br/></span> <span class="p">[</span>  <span class="mf">1.12216188e-12</span>   <span class="mf">7.73561269e-01</span>   <span class="mf">1.85019044e-01</span>   <span class="mf">4.14196869e-02</span><span class="p">]<br/></span> <span class="p">[</span>  <span class="mf">3.25313504e-03</span>   <span class="mf">1.12692615e-01</span>   <span class="mf">8.83368021e-01</span>   <span class="mf">6.86228435e-04</span><span class="p">]<br/></span> <span class="p">[</span>  <span class="mf">1.18741799e-01</span>   <span class="mf">4.20310643e-01</span>   <span class="mf">1.18670597e-18</span>   <span class="mf">4.60947557e-01</span><span class="p">]]<br/></span><span class="n"><br/>Means</span> <span class="ow">and</span> <span class="nb">vars</span> <span class="n">of</span> <span class="n">each</span> <span class="n">hidden</span> <span class="n">state<br/></span><span class="mi">0</span><span class="n">th</span> <span class="n">hidden</span> <span class="n">state<br/></span><span class="n">mean</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">2.33331888e-02</span>   <span class="mf">4.97389989e+07</span><span class="p">]<br/></span><span class="n">var</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">6.97748259e-01</span>   <span class="mf">2.49466578e+14</span><span class="p">]<br/></span><span class="mi"><br/></span><span class="mi">1</span><span class="n">st</span> <span class="n">hidden</span> <span class="n">state<br/></span><span class="n">mean</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">2.12401671e-02</span>   <span class="mf">8.81882861e+07</span><span class="p">]<br/></span><span class="n">var</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">1.18665023e-01</span>   <span class="mf">5.64418451e+14</span><span class="p">]<br/></span><span class="mi"><br/>2</span><span class="n">nd</span> <span class="n">hidden</span> <span class="n">state<br/></span><span class="n">mean</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">7.69658065e-03</span>   <span class="mf">5.43135922e+07</span><span class="p">]<br/></span><span class="n">var</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">5.02315562e-02</span>   <span class="mf">1.54569357e+14</span><span class="p">]<br/></span><span class="mi"><br/>3</span><span class="n">rd</span> <span class="n">hidden</span> <span class="n">state<br/></span><span class="n">mean</span> <span class="o">=</span>  <span class="p">[</span> <span class="o">-</span><span class="mf">3.53210673e-01</span>   <span class="mf">1.53080943e+08</span><span class="p">]<br/></span><span class="n">var</span> <span class="o">=</span>  <span class="p">[</span>  <span class="mf">2.55544137e+00</span>   <span class="mf">5.88210257e+15</span><span class="p">]</span></pre>
<p class="mce-root"/>
<p>We can also plot our hidden states over time:</p>
<pre><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)<br/></span><span class="n">colours</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">n_components</span><span class="p">))<br/></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">colour</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">colours</span><span class="p">)):<br/></span>    <span class="c1"># Use fancy indexing to plot data in each state.<br/></span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">==</span> <span class="n">i<br/></span>    <span class="n">ax</span><span class="o">.</span><span class="n">plot_date</span><span class="p">(</span><span class="n">dates</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">close_v</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="s2">".-"</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colour</span><span class="p">)<br/></span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"{0}th hidden state"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))<br/></span><br/>    <span class="c1"># Format the ticks.<br/></span>    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">YearLocator</span><span class="p">())<br/></span>    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">MonthLocator</span><span class="p">())<br/></span><br/>    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)<br/></span><span class="n"><br/>plt</span><span class="o">.</span><span class="n">show</span><span class="p">()<br/></span></pre>
<p>The output of the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/518698e7-beb2-4ed5-a1f0-2ce16f0b085c.png" style="width:34.42em;height:25.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 1: Plot of hidden states over time</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced algorithms for doing parameter estimation of a given HMM model. We started by looking into the basics of MLE and then applied the concepts to HMMs. For HMM training, we looked into two different scenarios: supervised training, when we have the observations for the hidden states, and unsupervised training, when we only have the output observations.</p>
<p>We also talked about the problems with estimation using MLE. In the next chapter, we will introduce algorithms for <span>doing parameter estimation using the Bayesian approach, which tries to solve these issues.</span></p>


            </article>

            
        </section>
    </body></html>