["```py\ndataDirectory <- \"../data\"\nif (!file.exists(paste(dataDirectory,'/train.csv',sep=\"\")))\n{\n  link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'\n  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep=\"\")))\n    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=\"\"))\n  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=\"\"), exdir = dataDirectory)\n  if (file.exists(paste(dataDirectory,'/test.csv',sep=\"\")))\n    file.remove(paste(dataDirectory,'/test.csv',sep=\"\"))\n}\n```", "```py\nif (!file.exists(paste(dataDirectory,'/train.csv',sep=\"\")))\n{\n library(keras)\n mnist <- dataset_mnist()\n c(c(x_train,y_train),c(x_test,y_test)) %<-% dataset_mnist()\n x_train <- array_reshape(x_train,c(dim(x_train)[1],dim(x_train)[2]*dim(x_train)[3]))\n y_train <- array_reshape(y_train,c(length(y_train),1))\n data_mnist <- as.data.frame(cbind(y_train,x_train))\n colnames(data_mnist)[1] <- \"label\"\n colnames(data_mnist)[2:ncol(data_mnist)] <- paste(\"pixel\",seq(1,784),sep=\"\")\n write.csv(data_mnist,paste(dataDirectory,'/train.csv',sep=\"\"),row.names = FALSE)\n}\n```", "```py\ndigits <- read.csv(\"../data/train.csv\")\ndim(digits)\n[1] 42000 785\n\nhead(colnames(digits), 4)\n[1] \"label\" \"pixel0\" \"pixel1\" \"pixel2\"\n\ntail(colnames(digits), 4)\n[1] \"pixel780\" \"pixel781\" \"pixel782\" \"pixel783\"\n\nhead(digits[, 1:4])\n  label pixel0 pixel1 pixel2\n1     1      0      0      0\n2     0      0      0      0\n3     1      0      0      0\n4     4      0      0      0\n5     0      0      0      0\n6     0      0      0      0\n```", "```py\ndigits_nn <- digits[(digits$label==5) | (digits$label==6),]\ndigits_nn$y <- digits_nn$label\ndigits_nn$label <- NULL\ntable(digits_nn$y)\n   5    6 \n3795 4137 \n\ndigits_nn$y <- ifelse(digits_nn$y==5, 0, 1)\ntable(digits_nn$y)\n   0    1 \n3795 4137 \n\nset.seed(42)\nsample <- sample(nrow(digits_nn), nrow(digits_nn)*0.8)\ntest <- setdiff(seq_len(nrow(digits_nn)), sample)\n\ndigits.X2 <- digits_nn[,apply(digits_nn[sample,1:(ncol(digits_nn)-1)], 2, var, na.rm=TRUE) != 0]\nlength(digits.X2)\n[1] 624\n```", "```py\ndf.pca <- prcomp(digits.X2[sample,],center = TRUE,scale. = TRUE) \ns<-summary(df.pca)\ncumprop<-s$importance[3, ]\nplot(cumprop, type = \"l\",main=\"Cumulative sum\",xlab=\"PCA component\")\n```", "```py\nnum_cols <- min(which(cumprop>0.5))\ncumprop[num_cols]\n PC23 \n0.50275 \n\nnewdat<-data.frame(df.pca$x[,1:num_cols])\nnewdat$y<-digits_nn[sample,\"y\"]\ncol_names <- names(newdat)\nf <- as.formula(paste(\"y ~\", paste(col_names[!col_names %in% \"y\"],collapse=\"+\")))\nnn <- neuralnet(f,data=newdat,hidden=c(4,2),linear.output = FALSE)\n```", "```py\nplot(nn)\n```", "```py\ntest.data <- predict(df.pca, newdata=digits_nn[test,colnames(digits.X2)])\ntest.data <- as.data.frame(test.data)\npreds <- compute(nn,test.data[,1:num_cols])\npreds <- ifelse(preds$net.result > 0.5, \"1\", \"0\")\nt<-table(digits_nn[test,\"y\"], preds,dnn=c(\"Actual\", \"Predicted\"))\nacc<-round(100.0*sum(diag(t))/sum(t),2)\nprint(t)\n Predicted\nActual 0   1\n 0   740  17\n 1    17 813\nprint(sprintf(\" accuracy = %1.2f%%\",acc))\n[1] \" accuracy = 97.86%\"\n```", "```py\nsample <- sample(nrow(digits), 6000)\ntrain <- sample[1:5000]\ntest <- sample[5001:6000]\n\ndigits.X <- digits[train, -1]\ndigits.y_n <- digits[train, 1]\ndigits$label <- factor(digits$label, levels = 0:9)\ndigits.y <- digits[train, 1]\n\ndigits.test.X <- digits[test, -1]\ndigits.test.y <- digits[test, 1]\nrm(sample,train,test)\n```", "```py\nbarplot(table(digits.y),main=\"Distribution of y values (train)\")\n```", "```py\nset.seed(42) \ntic <- proc.time()\ndigits.m1 <- caret::train(digits.X, digits.y,\n           method = \"nnet\",\n           tuneGrid = expand.grid(\n             .size = c(5),\n             .decay = 0.1),\n           trControl = trainControl(method = \"none\"),\n           MaxNWts = 10000,\n           maxit = 100)\nprint(proc.time() - tic)\n   user system elapsed \n  54.47 0.06 54.85\n```", "```py\ndigits.yhat1 <- predict(digits.m1,newdata=digits.test.X)\naccuracy <- 100.0*sum(digits.yhat1==digits.test.y)/length(digits.test.y)\nprint(sprintf(\" accuracy = %1.2f%%\",accuracy))\n[1] \" accuracy = 54.80%\"\nbarplot(table(digits.yhat1),main=\"Distribution of y values (model 1)\")\n```", "```py\ncaret::confusionMatrix(xtabs(~digits.yhat1 + digits.test.y))\nConfusion Matrix and Statistics\n\n            digits.test.y\ndigits.yhat1    0   1   2   3   4   5   6   7   8   9\n           0   61   1   0   1   0   2   0   0   0   1\n           1    1 104   0   2   0   4   3   9  12   8\n           2    6   2  91  56   4  20  68   1  41   1\n           3    0   0   0   0   0   0   0   0   0   0\n           4    2   0   4   1  67   1  22   4   2  21\n           5   39   0   6  45   4  46   0   5  30  16\n           6    0   0   0   0   0   0   0   0   0   0\n           7    0   0   0   6   9   0   0  91   2  75\n           8    0   0   0   0   0   0   0   0   0   0\n           9    0   0   0   0   0   0   0   3   0   0\n\nOverall Statistics\n\n               Accuracy : 0.46 \n                 95% CI : (0.4288, 0.4915)\n    No Information Rate : 0.122 \n    P-Value [Acc > NIR] : < 2.2e-16 \n\n                  Kappa : 0.4019 \n Mcnemar's Test P-Value : NA \n\nStatistics by Class:\n\n                     Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6\nSensitivity            0.5596   0.9720   0.9010    0.000   0.7976   0.6301    0.000\nSpecificity            0.9944   0.9563   0.7786    1.000   0.9378   0.8436    1.000\nPos Pred Value         0.9242   0.7273   0.3138      NaN   0.5403   0.2408      NaN\nNeg Pred Value         0.9486   0.9965   0.9859    0.889   0.9806   0.9666    0.907\nPrevalence             0.1090   0.1070   0.1010    0.111   0.0840   0.0730    0.093\nDetection Rate         0.0610   0.1040   0.0910    0.000   0.0670   0.0460    0.000\nDetection Prevalence   0.0660   0.1430   0.2900    0.000   0.1240   0.1910    0.000\nBalanced Accuracy      0.7770   0.9641   0.8398    0.500   0.8677   0.7369    0.500\n                     Class: 7 Class: 8 Class: 9\nSensitivity            0.8053    0.000   0.0000\nSpecificity            0.8963    1.000   0.9966\nPos Pred Value         0.4973      NaN   0.0000\nNeg Pred Value         0.9731    0.913   0.8776\nPrevalence             0.1130    0.087   0.1220\nDetection Rate         0.0910    0.000   0.0000\nDetection Prevalence   0.1830    0.000   0.0030\nBalanced Accuracy      0.8508    0.500   0.4983\n```", "```py\nset.seed(42) \ntic <- proc.time()\ndigits.m2 <- caret::train(digits.X, digits.y,\n           method = \"nnet\",\n           tuneGrid = expand.grid(\n             .size = c(10),\n             .decay = 0.1),\n           trControl = trainControl(method = \"none\"),\n            MaxNWts = 50000,\n            maxit = 100)\nprint(proc.time() - tic)\n   user system elapsed \n 154.49 0.09 155.33 \n\ndigits.yhat2 <- predict(digits.m2,newdata=digits.test.X)\naccuracy <- 100.0*sum(digits.yhat2==digits.test.y)/length(digits.test.y)\nprint(sprintf(\" accuracy = %1.2f%%\",accuracy))\n[1] \" accuracy = 66.30%\"\nbarplot(table(digits.yhat2),main=\"Distribution of y values (model 2)\")\n```", "```py\nset.seed(42) \ntic <- proc.time()\ndigits.m3 <- caret::train(digits.X, digits.y,\n           method = \"nnet\",\n           tuneGrid = expand.grid(\n             .size = c(40),\n             .decay = 0.1),\n           trControl = trainControl(method = \"none\"),\n           MaxNWts = 50000,\n           maxit = 100)\nprint(proc.time() - tic)\n   user system elapsed \n2450.16 0.96 2457.55\n\ndigits.yhat3 <- predict(digits.m3,newdata=digits.test.X)\naccuracy <- 100.0*sum(digits.yhat3==digits.test.y)/length(digits.test.y)\nprint(sprintf(\" accuracy = %1.2f%%\",accuracy))\n[1] \" accuracy = 82.20%\"\nbarplot(table(digits.yhat3),main=\"Distribution of y values (model 3)\")\n```", "```py\nhead(decodeClassLabels(digits.y))\n     0 1 2 3 4 5 6 7 8 9\n[1,] 0 0 0 0 0 0 0 0 0 1\n[2,] 0 0 0 0 1 0 0 0 0 0\n[3,] 1 0 0 0 0 0 0 0 0 0\n[4,] 0 0 0 0 0 1 0 0 0 0\n[5,] 0 0 0 0 1 0 0 0 0 0\n[6,] 0 0 0 1 0 0 0 0 0 0\n```", "```py\nset.seed(42) \ntic <- proc.time()\ndigits.m4 <- mlp(as.matrix(digits.X),\n             decodeClassLabels(digits.y),\n             size = 40,\n             learnFunc = \"Rprop\",\n             shufflePatterns = FALSE,\n             maxit = 80)\nprint(proc.time() - tic)\n   user system elapsed \n 179.71 0.08 180.99 \n\ndigits.yhat4 <- predict(digits.m4,newdata=digits.test.X)\ndigits.yhat4 <- encodeClassLabels(digits.yhat4)\naccuracy <- 100.0*sum(I(digits.yhat4 - 1)==digits.test.y)/length(digits.test.y)\nprint(sprintf(\" accuracy = %1.2f%%\",accuracy))\n[1] \" accuracy = 81.70%\"\nbarplot(table(digits.yhat4),main=\"Distribution of y values (model 4)\")\n```", "```py\ndigits.yhat4_b <- predict(digits.m4,newdata=digits.test.X)\nhead(round(digits.yhat4_b, 2))\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n18986 0.00 0.00 0.00 0.98 0.00 0.02 0.00 0.00 0.00 0.00\n41494 0.00 0.00 0.03 0.00 0.13 0.01 0.95 0.00 0.00 0.00\n21738 0.00 0.00 0.02 0.03 0.00 0.46 0.01 0.00 0.74 0.00\n37086 0.00 0.01 0.00 0.63 0.02 0.01 0.00 0.00 0.03 0.00\n35532 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.99 0.00 0.00\n17889 0.03 0.00 0.00 0.00 0.00 0.34 0.01 0.00 0.00 0.00\n\ntable(encodeClassLabels(digits.yhat4_b,method = \"WTA\", l = 0, h = 0))\n1 2 3 4 5 6 7 8 9 10\n102 116 104 117 93 66 93 127 89 93\n\ntable(encodeClassLabels(digits.yhat4_b,method = \"WTA\", l = 0, h = .5))\n0 1 2 3 4 5 6 7 8 9 10\n141 95 113 86 93 67 53 89 116 73 74\n\ntable(encodeClassLabels(digits.yhat4_b,method = \"WTA\", l = .2, h = .5))\n0 1 2 3 4 5 6 7 8 9 10\n177 91 113 77 91 59 50 88 116 70 68\n\ntable(encodeClassLabels(digits.yhat4_b,method = \"402040\", l = .4, h = .6))\n  0 1 2 3 4 5 6 7 8 9 10 \n254 89 110 71 82 46 41 79 109 65 54 \n```", "```py\ndigits.yhat4.train <- predict(digits.m4)\ndigits.yhat4.train <- encodeClassLabels(digits.yhat4.train)\naccuracy <- 100.0*sum(I(digits.yhat4.train - 1)==digits.y)/length(digits.y)\nprint(sprintf(\" accuracy = %1.2f%%\",accuracy))\n[1] \" accuracy = 84.70%\"\n```", "```py\ndigits.yhat1.train <- predict(digits.m1)\ndigits.yhat2.train <- predict(digits.m2)\ndigits.yhat3.train <- predict(digits.m3)\ndigits.yhat4.train <- predict(digits.m4)\ndigits.yhat4.train <- encodeClassLabels(digits.yhat4.train)\n\nmeasures <- c(\"AccuracyNull\", \"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\")\nn5.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat1.train))\nn5.outsample <- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat1))\nn10.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat2.train))\nn10.outsample <- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat2))\nn40.insample <- caret::confusionMatrix(xtabs(~digits.y + digits.yhat3.train))\nn40.outsample <- caret::confusionMatrix(xtabs(~digits.test.y + digits.yhat3))\nn40b.insample <- caret::confusionMatrix(xtabs(~digits.y + I(digits.yhat4.train - 1)))\nn40b.outsample <- caret::confusionMatrix(xtabs(~ digits.test.y + I(digits.yhat4 - 1)))\n\nshrinkage <- rbind(\n  cbind(Size = 5, Sample = \"In\", as.data.frame(t(n5.insample$overall[measures]))),\n  cbind(Size = 5, Sample = \"Out\", as.data.frame(t(n5.outsample$overall[measures]))),\n  cbind(Size = 10, Sample = \"In\", as.data.frame(t(n10.insample$overall[measures]))),\n  cbind(Size = 10, Sample = \"Out\", as.data.frame(t(n10.outsample$overall[measures]))),\n  cbind(Size = 40, Sample = \"In\", as.data.frame(t(n40.insample$overall[measures]))),\n  cbind(Size = 40, Sample = \"Out\", as.data.frame(t(n40.outsample$overall[measures]))),\n  cbind(Size = 40, Sample = \"In\", as.data.frame(t(n40b.insample$overall[measures]))),\n  cbind(Size = 40, Sample = \"Out\", as.data.frame(t(n40b.outsample$overall[measures])))\n  )\nshrinkage$Pkg <- rep(c(\"nnet\", \"RSNNS\"), c(6, 2))\ndodge <- position_dodge(width=0.4)\n\nggplot(shrinkage, aes(interaction(Size, Pkg, sep = \" : \"), Accuracy,\n                      ymin = AccuracyLower, ymax = AccuracyUpper,\n                      shape = Sample, linetype = Sample)) +\n  geom_point(size = 2.5, position = dodge) +\n  geom_errorbar(width = .25, position = dodge) +\n  xlab(\"\") + ylab(\"Accuracy + 95% CI\") +\n  theme_classic() +\n  theme(legend.key.size = unit(1, \"cm\"), legend.position = c(.8, .2))\n```", "```py\nuse.train.x <- read.table(\"../data/UCI HAR Dataset/train/X_train.txt\")\nuse.train.y <- read.table(\"../data/UCI HAR Dataset/train/y_train.txt\")[[1]]\n\nuse.test.x <- read.table(\"../data/UCI HAR Dataset/test/X_test.txt\")\nuse.test.y <- read.table(\"../data/UCI HAR Dataset/test/y_test.txt\")[[1]]\n\nuse.labels <- read.table(\"../data/UCI HAR Dataset/activity_labels.txt\")\n\nbarplot(table(use.train.y),main=\"Distribution of y values (UCI HAR Dataset)\")\n```", "```py\n## choose tuning parameters\ntuning <- list(\n  size = c(40, 20, 20, 50, 50),\n  maxit = c(60, 100, 100, 100, 100),\n  shuffle = c(FALSE, FALSE, TRUE, FALSE, FALSE),\n  params = list(FALSE, FALSE, FALSE, FALSE, c(0.1, 20, 3)))\n\n## setup cluster using 5 cores\n## load packages, export required data and variables\n## and register as a backend for use with the foreach package\ncl <- makeCluster(5)\nclusterEvalQ(cl, {source(\"cluster_inc.R\")})\nclusterExport(cl,\n  c(\"tuning\", \"use.train.x\", \"use.train.y\",\n    \"use.test.x\", \"use.test.y\")\n  )\nregisterDoSNOW(cl)\n```", "```py\n## train models in parallel\nuse.models <- foreach(i = 1:5, .combine = 'c') %dopar% {\n  if (tuning$params[[i]][1]) {\n    set.seed(42) \n    list(Model = mlp(\n      as.matrix(use.train.x),\n      decodeClassLabels(use.train.y),\n      size = tuning$size[[i]],\n      learnFunc = \"Rprop\",\n      shufflePatterns = tuning$shuffle[[i]],\n      learnFuncParams = tuning$params[[i]],\n      maxit = tuning$maxit[[i]]\n      ))\n  } else {\n    set.seed(42) \n    list(Model = mlp(\n      as.matrix(use.train.x),\n      decodeClassLabels(use.train.y),\n      size = tuning$size[[i]],\n      learnFunc = \"Rprop\",\n      shufflePatterns = tuning$shuffle[[i]],\n      maxit = tuning$maxit[[i]]\n    ))\n  }\n}\n```", "```py\n## export models and calculate both in sample,\n## 'fitted' and out of sample 'predicted' values\nclusterExport(cl, \"use.models\")\nuse.yhat <- foreach(i = 1:5, .combine = 'c') %dopar% {\n  list(list(\n    Insample = encodeClassLabels(fitted.values(use.models[[i]])),\n    Outsample = encodeClassLabels(predict(use.models[[i]],\n                                          newdata = as.matrix(use.test.x)))\n    ))\n}\n```", "```py\nuse.insample <- cbind(Y = use.train.y,\n  do.call(cbind.data.frame, lapply(use.yhat, `[[`, \"Insample\")))\ncolnames(use.insample) <- c(\"Y\", paste0(\"Yhat\", 1:5))\n\nperformance.insample <- do.call(rbind, lapply(1:5, function(i) {\n  f <- substitute(~ Y + x, list(x = as.name(paste0(\"Yhat\", i))))\n  use.dat <- use.insample[use.insample[,paste0(\"Yhat\", i)] != 0, ]\n  use.dat$Y <- factor(use.dat$Y, levels = 1:6)\n  use.dat[, paste0(\"Yhat\", i)] <- factor(use.dat[, paste0(\"Yhat\", i)], levels = 1:6)\n  res <- caret::confusionMatrix(xtabs(f, data = use.dat))\n\n  cbind(Size = tuning$size[[i]],\n        Maxit = tuning$maxit[[i]],\n        Shuffle = tuning$shuffle[[i]],\n        as.data.frame(t(res$overall[c(\"AccuracyNull\", \"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\")])))\n}))\n\nuse.outsample <- cbind(Y = use.test.y,\n  do.call(cbind.data.frame, lapply(use.yhat, `[[`, \"Outsample\")))\ncolnames(use.outsample) <- c(\"Y\", paste0(\"Yhat\", 1:5))\nperformance.outsample <- do.call(rbind, lapply(1:5, function(i) {\n  f <- substitute(~ Y + x, list(x = as.name(paste0(\"Yhat\", i))))\n  use.dat <- use.outsample[use.outsample[,paste0(\"Yhat\", i)] != 0, ]\n  use.dat$Y <- factor(use.dat$Y, levels = 1:6)\n  use.dat[, paste0(\"Yhat\", i)] <- factor(use.dat[, paste0(\"Yhat\", i)], levels = 1:6)\n  res <- caret::confusionMatrix(xtabs(f, data = use.dat))\n\n  cbind(Size = tuning$size[[i]],\n        Maxit = tuning$maxit[[i]],\n        Shuffle = tuning$shuffle[[i]],\n        as.data.frame(t(res$overall[c(\"AccuracyNull\", \"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\")])))\n}))\n```", "```py\noptions(width = 80, digits = 3)\nperformance.insample[,-4]\n  Size Maxit Shuffle Accuracy AccuracyLower AccuracyUpper\n1   40    60   FALSE    0.984         0.981         0.987\n2   20   100   FALSE    0.982         0.978         0.985\n3   20   100    TRUE    0.982         0.978         0.985\n4   50   100   FALSE    0.981         0.978         0.984\n5   50   100   FALSE    1.000         0.999         1.000\n\nperformance.outsample[,-4]\n  Size Maxit Shuffle Accuracy AccuracyLower AccuracyUpper\n1   40    60   FALSE    0.916         0.906         0.926\n2   20   100   FALSE    0.913         0.902         0.923\n3   20   100   TRUE     0.913         0.902         0.923\n4   50   100   FALSE    0.910         0.900         0.920\n5   50   100   FALSE    0.938         0.928         0.946\n```"]