- en: The TensorFlow Toolbox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most machine learning platforms are focused toward scientists and practitioners
    in academic or industrial settings. Accordingly, while quite powerful, they are
    often rough around the edges and have few user-experience features.
  prefs: []
  type: TYPE_NORMAL
- en: Quite a bit of effort goes into peeking at the model at various stages and viewing
    and aggregating performance across models and runs. Even viewing the neural network
    can involve far more effort than expected.
  prefs: []
  type: TYPE_NORMAL
- en: While this was acceptable when neural networks were simple and only a few layers
    deep, today's networks are far deeper. In 2015, Microsoft won the annual **ImageNet**
    competition using a deep network with 152 layers. Visualizing such networks can
    be difficult, and peeking at weights and biases can be overwhelming.
  prefs: []
  type: TYPE_NORMAL
- en: Practitioners started using home-built visualizers and bootstrapped tools to
    analyze their networks and run performance. TensorFlow changed this by releasing
    TensorBoard directly alongside their overall platform release. TensorBoard runs
    out of the box with no additional installations or setup.
  prefs: []
  type: TYPE_NORMAL
- en: Users just need to instrument their code according to what they wish to capture.
    It features plotting of events, learning rate, and loss over time; histograms,
    for weights and biases; and images. The Graph Explorer allows interactive reviews
    of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on several areas, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the instrumentation required to feed TensorBoard using four
    common models and datasets as examples, highlighting the required changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will then review the data captured and ways to interpret it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will review common graphs as visualized by Graph Explorer. This
    will help you visualize common neural network setups, which will be introduced
    in later chapters and projects. It will also be a visual introduction to common
    networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick preview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Without even having TensorFlow installed, you can play with a reference implementation
    of TensorBoard. You can get started here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tensorboard/index.html#graphs.](https://www.tensorflow.org/tensorboard/index.html#graphs)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow along with the code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/model'
  prefs: []
  type: TYPE_NORMAL
- en: s/image/cifar10/cifar10_train.py.](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py)
  prefs: []
  type: TYPE_NORMAL
- en: The example uses the **CIFAR-10** image set. The CIFAR-10 dataset consists of
    60,000 images in 10 classes compiled by Alex Krizhevsky, Vinod Nair, and Geoffrey
    Hinton. The dataset has become one of several standard learning tools and benchmarks
    for machine learning efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the Graph Explorer. We can immediately see a convolutional
    network being used. This is not surprising as we''re trying to classify images
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfe3a5b8-cf8d-4c87-b8de-bc605accc81b.png)'
  prefs: []
  type: TYPE_IMG
- en: This is just one possible view of the graph. You can try the Graph Explorer
    as well. It allows deep dives into individual components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next stop on the quick preview is the EVENTS tab. This tab shows scalar
    data over time. The different statistics are grouped into individual tabs on the
    right-hand side. The following screenshot shows a number of popular scalar statistics,
    such as loss, learning rate, cross entropy, and sparsity across multiple parts
    of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79150dc7-ed50-4625-92d6-ae45e73df412.png)'
  prefs: []
  type: TYPE_IMG
- en: The HISTOGRAMS tab is a close cousin as it shows tensor data over time. Despite
    the name, as of TensorFlow v0.7, it does not actually display histograms. Rather,
    it shows summaries of tensor data using percentiles.
  prefs: []
  type: TYPE_NORMAL
- en: The summary view is shown in the following figure. Just like with the EVENTS
    tab, the data is grouped into tabs on the right-hand side. Different runs can
    be toggled on and off and runs can be shown overlaid, allowing interesting comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: It features three runs, which we can see on the left side, and we'll look at
    just the `softmax` function and associated parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, don''t worry too much about what these mean, we''re just looking at
    what we can achieve for our own classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df6ca2ab-fc79-440f-b643-a7b5e04d0387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the summary view does not do justice to the utility of the HISTOGRAMS
    tab. Instead, we will zoom into a single graph to observe what is going on. This
    is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7a15f4-8379-4e90-8744-27d72f8ba579.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that each histogram chart shows a time series of nine lines. The top
    is the maximum, in middle the median, and the bottom the minimum. The three lines
    directly above and below the median are 1½ standard deviation, 1 standard deviation,
    and ½ standard deviation marks.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this does represent multimodal distributions as it is not a histogram.
    However, it does provide a quick gist of what would otherwise be a mountain of
    data to shift through.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of things to note are how data can be collected and segregated by runs,
    how different data streams can be collected, how we can enlarge the views, and
    how we can zoom into each of the graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Enough of graphics, let's jump into the code so we can run this for ourselves!
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow comes prepackaged with TensorBoard, so it will already be installed.
    It runs as a locally served web application accessible via the browser at `http://0.0.0.0:6006`.
    Conveniently, there is no server-side code or configurations required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on where your paths are, you may be able to run it directly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If your paths are not correct, you may need to prefix the application accordingly,
    as shown in the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux, you can run it in the background and just let it keep running, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Some thought should be put into the directory structure though. The Runs list
    on the left side of the dashboard is driven by subdirectories in the `logdir`
    location. The following image shows two runs--`MNIST_Run1` and `MNIST_Run2`. Having
    an organized `runs` folder will allow plotting successive runs side by side to
    see differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35fdd820-9895-4675-9d99-eb11d699cb78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When initializing `writer`, you will pass in the directory for the log as the
    first parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Consider saving a base location and appending run-specific subdirectories for
    each run. This will help organize outputs without expending more thought on it.
    We'll discuss more about this later.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating hooks into our code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to get started with TensorBoard is by taking existing working examples
    and instrument them with the code required for TensorBoard. We will do this for
    several common training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with the typical Hello World of machine learning with images--the
    MNIST handwritten numeral classification exercise.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database being used has 60,000 images for training and another 10,000
    for testing. It was originally collected by Chris Burges and Corinna Cortes and
    enhanced by Yann LeCun. You can find out more about the dataset on Yann LeCun's
    website ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow conveniently comes with a test script demonstrating a convolutional
    neural network using the MSNIST handwritten, available at [https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py](https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py).
  prefs: []
  type: TYPE_NORMAL
- en: Let's modify this script to allow TensorBoard usage. If you wish to peek ahead,
    download a golden copy or see deltas; our full set of changes is available on
    the book's GitHub repository ([https://github.com/mlwithtf/mlwithtf](https://github.com/mlwithtf/mlwithtf)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: For now, we recommend following along and making changes incrementally to understand
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early on in the `main` class, we will define holders for `convn_weights`, `convn_biases`,
    and other parameters. Directly afterward, we will write the following code to
    add them to the `histogram`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines capture the values for the HISTOGRAMS tab. Notice that
    the captured values form subsections on the HISTOGRAMS tab, which is shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71d949ac-fed1-4f05-9faf-941dace523fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s record some `loss` figures. We have the following code to start
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add a `scalar` summary for the `loss` figures after the preceding line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will start with the standard code calculating the `learning_rate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add a `scalar` summary for the `learning_rate` figures, which is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Just these two preceding lines help us capture these to important scalar metrics
    in our EVENTS tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53e4b291-5aed-43a5-8b61-de36d742b671.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s instruct our script to save the graph setup. Let''s find the
    section of the script which creates the `session`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Just after defining the `sess` handle, we will capture the graph as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to add our `merged` object when running the session. We originally
    had the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add our `merged` object when running the session as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will need to write summaries at specified steps, much like we typically
    output validation set accuracy periodically. So, we do add one more line after
    the `sum_string` is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That is all! We have just captured our loss and learning rates, key intermediate
    parameters on our neural network, and the structure of the graph. We have already
    examined the EVENTS and HISTOGRAMS tab, now let''s look at the GRAPH tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fab51f74-7ad0-49f0-b2fd-c76ae470ce87.png)'
  prefs: []
  type: TYPE_IMG
- en: AlexNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anyone involved in deep learning with images should become familiar with AlexNet.
    The network was introduced in the landmark paper, *ImageNet Classification with
    Deep Convolutional Neural Networks*, by Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton. The paper can be viewed at [http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'This network architecture achieved then-record accuracy on the annual ImageNet
    competition. The architecture is described in their paper, as shown in the following
    image. We will be using this network architecture in later chapters, but for now,
    let''s browse through the network using TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc55270a-7218-4c5c-9351-d3573279583a.png)'
  prefs: []
  type: TYPE_IMG
- en: We will not review line-by-line changes to the existing AlexNet code, but the
    reader can easily see changes by noting differences between the original model
    code provided by Google and the revised code that we have included with the book's
    code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original AlexNet TensorFlow implementation from Google is available at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/models/blob/master/tutorials/image/alexnet/alexnet_benchmark.py.](https://github.com/tensorflow/models/blob/master/tutorials/image/alexnet/alexnet_benchmark.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The revised AlexNet TensorFlow implementation with TensorBoard instrumentation
    can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_03/alexnet_benchmark.py.](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_03/alexnet_benchmark.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The changes introduced are very similar to those done for our MNIST example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, find the location of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, replace it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can run the Python file `alexnet_benchmark.py` and TensorBoard
    command to visualize the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our focus for this section is just the graph. The following figure shows a section
    of the Graph Explorer. We have deep dived into convolutional layer 3 of 5 and
    we are looking at weights and biases for this layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the weights node on the graph is interesting because we see details
    such as the shape: `{"shape":{"dim":[{"size":3},{"size":3},{"size":192},{"size":384}]}}`.
    We can match many of these details right back to the original paper and the previously
    referenced diagram! We can also trace details back to the network setup in the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The details in the Graph Explorer and code are equivalent, but the flow of
    data is very easily visualized using TensorBoard. It is also easy to collapse
    repetitive sections and expand sections of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73088eb4-aab7-46fb-86e4-58655412e4e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The graph is the most interesting part of this section, but of course, you can
    also run our revised script and review the training performance, as well as a
    host of other data we're capturing. You can even capture additional data. Give
    it a try!
  prefs: []
  type: TYPE_NORMAL
- en: Automating runs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When trying to train a classifier, we will often end up with multiple variables
    for which we don't know a good setting. Viewing values used by solutions for similar
    problems is a good starting point. However, we are often left with an array of
    possible values that we need to test. To make things more complicated, we often
    have several such parameters, resulting in numerous combinations that we may need
    to test.
  prefs: []
  type: TYPE_NORMAL
- en: For such situations, we suggest keeping the parameters of interest as values
    that can be passed into the trainer. Then, a `wrapper` script can pass in various
    combinations of the parameters, along with a unique output log subdirectory that
    is possibly tagged with a descriptive name.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will allow an easy comparison of results and intermediate values across
    multiple tests. The following figure shows four runs'' losses plotted together.
    We can easily see the underperforming and overperforming pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09e61965-8300-46e9-a02a-9833e1f82cdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the major areas of TensorBoard--EVENTS, HISTOGRAMS,
    and viewing GRAPH. We modified popular models to see the exact changes required
    before TensorBoard could be up and running. This should have demonstrated the
    fairly minimal effort required to get started with TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we focused on various popular models by viewing their network design.
    We did this by instrumenting the code with TensorBoard hooks and using the TensorBoard
    Graph Explorer to deep dive into the network setups.
  prefs: []
  type: TYPE_NORMAL
- en: The reader should now be able to use TensorBoard more effectively, gauge training
    performance, and plan runs and modify training scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we're going to jump into convolutional networks. We'll use parts of our
    prior work so we can hit the ground running. But, we'll focus on more advanced
    neural network setups to achieve better accuracy. The focus on training accuracy
    reflects the focus of most practitioner's efforts, so it is the time we face the
    challenge.
  prefs: []
  type: TYPE_NORMAL
