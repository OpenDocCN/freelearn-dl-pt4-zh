["```py\nlibrary(keras)\nlibrary(imager)\n# this script loads the cifar_10 data from Keras\n# and saves the data as individual images\n\n# create directories,\n# we will save 8 classes in the data1 folder for model building\n# and use 2 classes for transfer learning\ndata_dir <- \"../data/cifar_10_images/\"\nif (!dir.exists(data_dir))\n  dir.create(data_dir)\nif (!dir.exists(paste(data_dir,\"data1/\",sep=\"\")))\n  dir.create(paste(data_dir,\"data1/\",sep=\"\"))\nif (!dir.exists(paste(data_dir,\"data2/\",sep=\"\")))\n  dir.create(paste(data_dir,\"data2/\",sep=\"\"))\ntrain_dir1 <- paste(data_dir,\"data1/train/\",sep=\"\")\nvalid_dir1 <- paste(data_dir,\"data1/valid/\",sep=\"\")\ntrain_dir2 <- paste(data_dir,\"data2/train/\",sep=\"\")\nvalid_dir2 <- paste(data_dir,\"data2/valid/\",sep=\"\")\n\nif (!dir.exists(train_dir1))\n  dir.create(train_dir1)\nif (!dir.exists(valid_dir1))\n  dir.create(valid_dir1)\nif (!dir.exists(train_dir2))\n  dir.create(train_dir2)\nif (!dir.exists(valid_dir2))\n  dir.create(valid_dir2)\n```", "```py\n# load CIFAR10 dataset\nc(c(x_train,y_train),c(x_test,y_test)) %<-% dataset_cifar10()\n# get the unique categories,\n# note that unique does not mean ordered!\n# save 8 classes in data1 folder\ncategories <- unique(y_train)\nfor (i in categories[1:8])\n{\n  label_dir <- paste(train_dir1,i,sep=\"\")\n  if (!dir.exists(label_dir))\n    dir.create(label_dir)\n  label_dir <- paste(valid_dir1,i,sep=\"\")\n  if (!dir.exists(label_dir))\n    dir.create(label_dir)\n}\n# save 2 classes in data2 folder\nfor (i in categories[9:10])\n{\n  label_dir <- paste(train_dir2,i,sep=\"\")\n  if (!dir.exists(label_dir))\n    dir.create(label_dir)\n  label_dir <- paste(valid_dir2,i,sep=\"\")\n  if (!dir.exists(label_dir))\n    dir.create(label_dir)\n}\n```", "```py\n# loop through train images and save in the correct folder\nfor (i in 1:dim(x_train)[1])\n{\n  img <- x_train[i,,,]\n  label <- y_train[i,1]\n  if (label %in% categories[1:8])\n    image_array_save(img,paste(train_dir1,label,\"/\",i,\".png\",sep=\"\"))\n  else\n    image_array_save(img,paste(train_dir2,label,\"/\",i,\".png\",sep=\"\"))\n  if ((i %% 500)==0)\n    print(i)\n}\n\n# loop through test images and save in the correct folder\nfor (i in 1:dim(x_test)[1])\n{\n  img <- x_test[i,,,]\n  label <- y_test[i,1]\n  if (label %in% categories[1:8])\n    image_array_save(img,paste(valid_dir1,label,\"/\",i,\".png\",sep=\"\"))\n  else\n    image_array_save(img,paste(valid_dir2,label,\"/\",i,\".png\",sep=\"\"))\n  if ((i %% 500)==0)\n    print(i)\n}\n```", "```py\n# plot some images to verify process\nimage_dir <- list.dirs(valid_dir1, full.names=FALSE, recursive=FALSE)[1]\nimage_dir <- paste(valid_dir1,image_dir,sep=\"\")\nimg_paths <- paste(image_dir,list.files(image_dir),sep=\"/\")\n\npar(mfrow = c(3, 3))\npar(mar=c(2,2,2,2))\nfor (i in 1:9)\n{\n  im <- load.image(img_paths[i])\n  plot(im)\n}\n```", "```py\nlibrary(keras)\n\n# train a model from a set of images\n# note: you need to run gen_cifar10_data.R first to create the images!\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(name=\"conv1\", input_shape=c(32, 32, 3),\n    filter=32, kernel_size=c(3,3), padding=\"same\"\n  ) %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(name=\"conv2\",filter=32, kernel_size=c(3,3),\n                padding=\"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_max_pooling_2d(pool_size=c(2,2)) %>%\n  layer_dropout(0.25,name=\"drop1\") %>%\n\n  layer_conv_2d(name=\"conv3\",filter=64, kernel_size=c(3,3),\n                padding=\"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(name=\"conv4\",filter=64, kernel_size=c(3,3),\n                padding=\"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_max_pooling_2d(pool_size=c(2,2)) %>%\n  layer_dropout(0.25,name=\"drop2\") %>%\n\n  layer_flatten() %>%\n  layer_dense(256) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(256) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n\n  layer_dense(8) %>%\n  layer_activation(\"softmax\")\n\nmodel %>% compile(\n  loss=\"categorical_crossentropy\",\n  optimizer=\"adam\",\n  metrics=\"accuracy\"\n)\n```", "```py\n# set up data generators to stream images to the train function\ndata_dir <- \"../data/cifar_10_images/\"\ntrain_dir <- paste(data_dir,\"data1/train/\",sep=\"\")\nvalid_dir <- paste(data_dir,\"data1/valid/\",sep=\"\")\n\n# in CIFAR10\n# there are 50000 images in training set\n# and 10000 images in test set\n# but we are only using 8/10 classes,\n# so its 40000 train and 8000 validation\nnum_train <- 40000\nnum_valid <- 8000\nflow_batch_size <- 50\n# data augmentation\ntrain_gen <- image_data_generator(\n  rotation_range=15,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  horizontal_flip=TRUE,\n  rescale=1.0/255)\n# get images from directory\ntrain_flow <- flow_images_from_directory(\n  train_dir,\n  train_gen,\n  target_size=c(32,32),\n  batch_size=flow_batch_size,\n  class_mode=\"categorical\"\n)\n\n# no augmentation on validation data\nvalid_gen <- image_data_generator(rescale=1.0/255)\nvalid_flow <- flow_images_from_directory(\n  valid_dir,\n  valid_gen,\n  target_size=c(32,32),\n  batch_size=flow_batch_size,\n  class_mode=\"categorical\"\n)\n```", "```py\n# call-backs\ncallbacks <- list(\n  callback_early_stopping(monitor=\"val_acc\",patience=10,mode=\"auto\"),\n  callback_model_checkpoint(filepath=\"cifar_model.h5\",mode=\"auto\",\n                            monitor=\"val_loss\",save_best_only=TRUE)\n)\n```", "```py\n# train the model using the data generators and call-backs defined above\nhistory <- model %>% fit_generator(\n  train_flow,\n  steps_per_epoch=as.integer(num_train/flow_batch_size),\n  epochs=100,\n  callbacks=callbacks,\n  validation_data=valid_flow,\n  validation_steps=as.integer(num_valid/flow_batch_size)\n)\n```", "```py\nlibrary(keras)\n\n# load model trained in build_cifar10_model.R\nmodel <- load_model_hdf5(\"cifar_model.h5\")\n```", "```py\n> valid_dir <-\"../data/cifar_10_images/data1/valid/\"\n> first_dir <- list.dirs(valid_dir, full.names=FALSE, recursive=FALSE)[1]\n> valid_dir <- paste(valid_dir,first_dir,sep=\"\")\n> img_path <- paste(valid_dir,list.files(valid_dir)[7],sep=\"/\")\n\n# load image and convert to shape we can use for prediction\n> img <- image_load(img_path, target_size = c(32,32))\n> x <- image_to_array(img)\n> x <- array_reshape(x, c(1, dim(x)))\n> x <- x / 255.0\n> preds <- model %>% predict(x)\n> preds <- round(preds,3)\n> preds\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7]  [,8]\n[1,] 0.997    0    0    0    0    0    0 0.003\n```", "```py\n> valid_dir <-\"../data/cifar_10_images/data1/valid/\"\n> flow_batch_size <- 50\n> num_valid <- 8000\n> \n> valid_gen <- image_data_generator(rescale=1.0/255)\n> valid_flow <- flow_images_from_directory(\n   valid_dir,\n   valid_gen,\n   target_size=c(32,32),\n   batch_size=flow_batch_size,\n   class_mode=\"categorical\"\n )\n> \n> evaluate_generator(model,valid_flow,\n   steps=as.integer(num_valid/flow_batch_size))\n$`loss`\n[1] 0.5331386\n\n$acc\n[1] 0.808625\n```", "```py\n> preds <- predict_generator(model,valid_flow,\n    steps=as.integer(num_valid/flow_batch_size))\n> dim(preds)\n[1] 8000 8\n\n> # view the predictions,\n> preds <- round(preds,3)\n> head(preds)\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,] 0.000 0.000 0.000 0.000 0.000 0.000 0.999 0.001\n[2,] 0.000 0.007 0.001 0.002 0.990 0.000 0.000 0.000\n[3,] 0.000 0.855 0.069 0.032 0.021 0.017 0.002 0.002\n[4,] 0.134 0.001 0.000 0.000 0.000 0.000 0.001 0.864\n[5,] 0.011 0.064 0.057 0.226 0.051 0.515 0.004 0.073\n[6,] 0.108 0.277 0.135 0.066 0.094 0.091 0.052 0.179\n```", "```py\n> library(keras)\n> model <- application_vgg16(weights = 'imagenet', include_top = TRUE)\n\n> summary(model)\n_________________________________________________________________________\nLayer (type)                     Output Shape                 Param # \n=========================================================================\ninput_1 (InputLayer)             (None, 224, 224, 3)          0 \n_________________________________________________________________________\nblock1_conv1 (Conv2D)            (None, 224, 224, 64)         1792 \n_________________________________________________________________________\nblock1_conv2 (Conv2D)            (None, 224, 224, 64)         36928 \n_________________________________________________________________________\nblock1_pool (MaxPooling2D)       (None, 112, 112, 64)         0 \n_________________________________________________________________________\nblock2_conv1 (Conv2D)            (None, 112, 112, 128)        73856 \n_________________________________________________________________________\nblock2_conv2 (Conv2D)            (None, 112, 112, 128)        147584 \n_________________________________________________________________________\nblock2_pool (MaxPooling2D)       (None, 56, 56, 128)          0 \n_________________________________________________________________________\nblock3_conv1 (Conv2D)            (None, 56, 56, 256)          295168 \n_________________________________________________________________________\nblock3_conv2 (Conv2D)            (None, 56, 56, 256)          590080 \n_________________________________________________________________________\nblock3_conv3 (Conv2D)            (None, 56, 56, 256)          590080 \n_________________________________________________________________________\nblock3_pool (MaxPooling2D)       (None, 28, 28, 256)          0 \n_________________________________________________________________________\nblock4_conv1 (Conv2D)            (None, 28, 28, 512)          1180160 \n_________________________________________________________________________\nblock4_conv2 (Conv2D)            (None, 28, 28, 512)          2359808 \n_________________________________________________________________________\nblock4_conv3 (Conv2D)            (None, 28, 28, 512)          2359808 \n_________________________________________________________________________\nblock4_pool (MaxPooling2D)       (None, 14, 14, 512)          0 \n_________________________________________________________________________\nblock5_conv1 (Conv2D)            (None, 14, 14, 512)          2359808 \n_________________________________________________________________________\nblock5_conv2 (Conv2D)            (None, 14, 14, 512)          2359808 \n_________________________________________________________________________\nblock5_conv3 (Conv2D)            (None, 14, 14, 512)          2359808 \n_________________________________________________________________________\nblock5_pool (MaxPooling2D)       (None, 7, 7, 512)            0 \n_________________________________________________________________________\nflatten (Flatten)                (None, 25088)                0 \n_________________________________________________________________________\nfc1 (Dense)                      (None, 4096)                 102764544 \n_________________________________________________________________________\nfc2 (Dense)                      (None, 4096)                 16781312 \n_________________________________________________________________________\npredictions (Dense)              (None, 1000)                 4097000 \n=========================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________________\n```", "```py\n> img_path <- \"image1.jpg\"\n> img <- image_load(img_path, target_size = c(224,224))\n> x <- image_to_array(img)\n> x <- array_reshape(x, c(1, dim(x)))\n> x <- imagenet_preprocess_input(x)\n\n> preds <- model %>% predict(x)\n> imagenet_decode_predictions(preds, top = 5)\n[[1]]\n  class_name       class_description      score\n1  n02835271   bicycle-built-for-two 0.31723219\n2  n03792782   mountain_bike         0.16578741\n3  n03891332   parking_meter         0.12548350\n4  n04485082   tripod                0.06399463\n5  n09193705   alp                   0.04852912\n```", "```py\nlibrary(keras)\n\n# load model trained in build_cifar10_model.R\nmodel <- load_model_hdf5(\"cifar_model.h5\")\n```", "```py\n> length(model$trainable_weights)\n[1] 14\n```", "```py\nfreeze_weights(model,from=\"conv1\", to=\"drop2\")\nlength(model$trainable_weights)\n[1] 6\n```", "```py\n# remove the softmax layer\npop_layer(model)\npop_layer(model)\n```", "```py\n# add a new layer that has the correct number of nodes for the new task\nmodel %>%\n  layer_dense(name=\"new_dense\",units=2, activation='softmax')\nsummary(model)\n```", "```py\n# compile the model again\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer=\"adam\",\n  metrics=c('accuracy')\n)\n\n# set up data generators to stream images to the train function\ndata_dir <- \"../data/cifar_10_images/\"\ntrain_dir <- paste(data_dir,\"data2/train/\",sep=\"\")\nvalid_dir <- paste(data_dir,\"data2/valid/\",sep=\"\")\n\n# in CIFAR10, # there are 50000 images in training set\n# and 10000 images in test set\n# but we are only using 2/10 classes,\n# so its 10000 train and 2000 validation\nnum_train <- 10000\nnum_valid <- 2000\nflow_batch_size <- 50\n# no data augmentation\ntrain_gen <- image_data_generator(rescale=1.0/255)\n# get images from directory\ntrain_flow <- flow_images_from_directory(\n  train_dir,\n  train_gen,\n  target_size=c(32,32),\n  batch_size=flow_batch_size,\n  class_mode=\"categorical\"\n)\n\n# no augmentation on validation data\nvalid_gen <- image_data_generator(rescale=1.0/255)\nvalid_flow <- flow_images_from_directory(\n  valid_dir,\n  valid_gen,\n  target_size=c(32,32),\n  batch_size=flow_batch_size,\n  class_mode=\"categorical\"\n)\n```", "```py\n> history <- model %>% fit_generator(\n+ train_flow,\n+ steps_per_epoch=as.integer(num_train/flow_batch_size),\n+ epochs=10,\n+ validation_data=valid_flow,\n+ validation_steps=as.integer(num_valid/flow_batch_size)\n+ )\nFound 10000 images belonging to 2 classes.\nFound 2000 images belonging to 2 classes.\nEpoch 1/10\n200/200 [==============================] - 5s 27ms/step - loss: 0.3115 - acc: 0.8811 - val_loss: 0.1529 - val_acc: 0.9425\nEpoch 2/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1971 - acc: 0.9293 - val_loss: 0.1316 - val_acc: 0.9550\nEpoch 3/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1637 - acc: 0.9382 - val_loss: 0.1248 - val_acc: 0.9540\nEpoch 4/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1367 - acc: 0.9497 - val_loss: 0.1200 - val_acc: 0.9575\nEpoch 5/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1227 - acc: 0.9543 - val_loss: 0.1148 - val_acc: 0.9605\nEpoch 6/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1161 - acc: 0.9559 - val_loss: 0.1110 - val_acc: 0.9625\nEpoch 7/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.1022 - acc: 0.9622 - val_loss: 0.1118 - val_acc: 0.9620\nEpoch 8/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.0996 - acc: 0.9655 - val_loss: 0.1068 - val_acc: 0.9645\nEpoch 9/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.0861 - acc: 0.9687 - val_loss: 0.1095 - val_acc: 0.9655\nEpoch 10/10\n200/200 [==============================] - 4s 20ms/step - loss: 0.0849 - acc: 0.9696 - val_loss: 0.1189 - val_acc: 0.9620\n```", "```py\nlibrary(keras)\n#devtools::install_github(\"rstudio/tfdeploy\")\nlibrary(tfdeploy)\n\n# load data\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()\n\n# reshape and rescale\nx_train <- array_reshape(x_train, dim=c(nrow(x_train), 784)) / 255\nx_test <- array_reshape(x_test, dim=c(nrow(x_test), 784)) / 255\n\n# one-hot encode response\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\n# define and compile model\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units=256, activation='relu', input_shape=c(784),name=\"image\") %>%\n  layer_dense(units=128, activation='relu') %>%\n  layer_dense(units=10, activation='softmax',name=\"prediction\") %>%\n  compile(\n    loss='categorical_crossentropy',\n    optimizer=optimizer_rmsprop(),\n    metrics=c('accuracy')\n  )\n\n# train model\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs=10, batch_size=128,\n  validation_split=0.2\n)\npreds <- round(predict(model, x_test[1:5,]),0)\nhead(preds)\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    0    0    0    0    0    0    1    0     0\n[2,]    0    0    1    0    0    0    0    0    0     0\n[3,]    0    1    0    0    0    0    0    0    0     0\n[4,]    1    0    0    0    0    0    0    0    0     0\n[5,]    0    0    0    0    1    0    0    0    0     0\n```", "```py\n\n# create a json file for an image from the test set\njson <- \"{\\\"instances\\\": [{\\\"image_input\\\": [\"\njson <- paste(json,paste(x_test[1,],collapse=\",\"),sep=\"\")\njson <- paste(json,\"]}]}\",sep=\"\")\nwrite.table(json,\"json_image.json\",row.names=FALSE,col.names=FALSE,quote=FALSE)\n```", "```py\nexport_savedmodel(model, \"savedmodel\")\nserve_savedmodel('savedmodel', browse=TRUE)\n```", "```py\ncurl -X POST -H \"Content-Type: application/json\" -d @json_image.json http://localhost:8089/serving_default/predict\n```"]