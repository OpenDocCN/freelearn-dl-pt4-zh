["```py\n<DATE>,<TIME>,<OPEN>,<HIGH>,<LOW>,<CLOSE>,<VOL> \n20160104,100100,1148.90000,1148.90000,1148.90000,1148.90000,0 \n20160104,100200,1148.90000,1148.90000,1148.90000,1148.90000,50 \n20160104,100300,1149.00000,1149.00000,1149.00000,1149.00000,33 \n20160104,100400,1149.00000,1149.00000,1149.00000,1149.00000,4 \n20160104,100500,1153.00000,1153.00000,1153.00000,1153.00000,0 \n20160104,100600,1156.90000,1157.90000,1153.00000,1153.00000,43 \n20160104,100700,1150.60000,1150.60000,1150.40000,1150.40000,5 \n20160104,100800,1150.20000,1150.20000,1150.20000,1150.20000,4 \n20160104,100900,1150.50000,1150.50000,1150.50000,1150.50000,2 \n20160104,101000,1150.00000,1150.00000,1150.00000,1150.00000,43 \n20160104,101100,1149.70000,1149.70000,1149.70000,1149.70000,0 \n20160104,101200,1150.20000,1150.20000,1149.50000,1149.70000,165 \n...\n```", "```py\nimport typing as tt \nimport gymnasium as gym \nfrom gymnasium import spaces \nfrom gymnasium.utils import seeding \nfrom gymnasium.envs.registration import EnvSpec \nimport enum \nimport numpy as np \nfrom . import data \n\nDEFAULT_BARS_COUNT = 10 \nDEFAULT_COMMISSION_PERC = 0.1 \n\nclass Actions(enum.Enum): \n    Skip = 0 \n    Buy = 1 \n    Close = 2\n```", "```py\nclass StocksEnv(gym.Env): \n    spec = EnvSpec(\"StocksEnv-v0\")\n```", "```py\n @classmethod \n    def from_dir(cls, data_dir: str, **kwargs): \n        prices = { \n            file: data.load_relative(file) \n            for file in data.price_files(data_dir) \n        } \n        return StocksEnv(prices, **kwargs)\n```", "```py\n def __init__( \n            self, prices: tt.Dict[str, data.Prices], \n            bars_count: int = DEFAULT_BARS_COUNT, \n            commission: float = DEFAULT_COMMISSION_PERC, \n            reset_on_close: bool = True, state_1d: bool = False, \n            random_ofs_on_reset: bool = True, \n            reward_on_close: bool = False, volumes=False \n    ):\n```", "```py\n self._prices = prices \n        if state_1d: \n            self._state = State1D(bars_count, commission, reset_on_close, \n                                  reward_on_close=reward_on_close, volumes=volumes) \n        else: \n            self._state = State(bars_count, commission, reset_on_close, \n                                reward_on_close=reward_on_close, volumes=volumes) \n        self.action_space = spaces.Discrete(n=len(Actions)) \n        self.observation_space = spaces.Box( \n            low=-np.inf, high=np.inf, shape=self._state.shape, dtype=np.float32) \n         self.random_ofs_on_reset = random_ofs_on_reset\n```", "```py\n def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None): \n        # make selection of the instrument and itâ€™s offset. Then reset the state \n        super().reset(seed=seed, options=options) \n        self._instrument = self.np_random.choice(list(self._prices.keys())) \n        prices = self._prices[self._instrument] \n        bars = self._state.bars_count \n        if self.random_ofs_on_reset: \n            offset = self.np_random.choice(prices.high.shape[0]-bars*10) + bars \n        else: \n            offset = bars \n        self._state.reset(prices, offset) \n        return self._state.encode(), {}\n```", "```py\n def step(self, action_idx: int) -> tt.Tuple[np.ndarray, float, bool, bool, dict]: \n        action = Actions(action_idx) \n        reward, done = self._state.step(action) \n        obs = self._state.encode() \n        info = { \n            \"instrument\": self._instrument, \n            \"offset\": self._state._offset \n        } \n        return obs, reward, done, False, info\n```", "```py\nclass State: \n    def __init__(self, bars_count: int, commission_perc: float, reset_on_close: bool, \n                 reward_on_close: bool = True, volumes: bool = True): \n        assert bars_count > 0 \n        assert commission_perc >= 0.0 \n        self.bars_count = bars_count \n        self.commission_perc = commission_perc \n        self.reset_on_close = reset_on_close \n        self.reward_on_close = reward_on_close \n        self.volumes = volumes \n        self.have_position = False \n        self.open_price = 0.0 \n        self._prices = None \n        self._offset = None\n```", "```py\n def reset(self, prices: data.Prices, offset: int): \n        assert offset >= self.bars_count-1 \n        self.have_position = False \n        self.open_price = 0.0 \n        self._prices = prices \n        self._offset = offset\n```", "```py\n @property \n    def shape(self) -> tt.Tuple[int, ...]: \n        # [h, l, c] * bars + position_flag + rel_profit \n        if self.volumes: \n            return 4 * self.bars_count + 1 + 1, \n        else: \n            return 3 * self.bars_count + 1 + 1,\n```", "```py\n def encode(self) -> np.ndarray: \n        res = np.ndarray(shape=self.shape, dtype=np.float32) \n        shift = 0 \n        for bar_idx in range(-self.bars_count+1, 1): \n            ofs = self._offset + bar_idx \n            res[shift] = self._prices.high[ofs] \n            shift += 1 \n            res[shift] = self._prices.low[ofs] \n            shift += 1 \n            res[shift] = self._prices.close[ofs] \n            shift += 1 \n            if self.volumes: \n                res[shift] = self._prices.volume[ofs] \n                shift += 1 \n        res[shift] = float(self.have_position) \n        shift += 1 \n        if not self.have_position: \n            res[shift] = 0.0 \n        else: \n            res[shift] = self._cur_close() / self.open_price - 1.0 \n        return res\n```", "```py\n def _cur_close(self) -> float: \n        open = self._prices.open[self._offset] \n        rel_close = self._prices.close[self._offset] \n        return open * (1.0 + rel_close)\n```", "```py\n def step(self, action: Actions) -> tt.Tuple[float, bool]: \n        reward = 0.0 \n        done = False \n        close = self._cur_close()\n```", "```py\n if action == Actions.Buy and not self.have_position: \n            self.have_position = True \n            self.open_price = close \n            reward -= self.commission_perc\n```", "```py\n elif action == Actions.Close and self.have_position: \n            reward -= self.commission_perc \n            done |= self.reset_on_close \n            if self.reward_on_close: \n                reward += 100.0 * (close / self.open_price - 1.0) \n            self.have_position = False \n            self.open_price = 0.0\n```", "```py\n self._offset += 1 \n        prev_close = close \n        close = self._cur_close() \n        done |= self._offset >= self._prices.close.shape[0]-1 \n\n        if self.have_position and not self.reward_on_close: \n            reward += 100.0 * (close / prev_close - 1.0) \n\n        return reward, done\n```", "```py\nclass State1D(State): \n    @property \n    def shape(self) -> tt.Tuple[int, ...]: \n        if self.volumes: \n            return 6, self.bars_count \n        else: \n            return 5, self.bars_count\n```", "```py\n def encode(self) -> np.ndarray: \n        res = np.zeros(shape=self.shape, dtype=np.float32) \n        start = self._offset-(self.bars_count-1) \n        stop = self._offset+1 \n        res[0] = self._prices.high[start:stop] \n        res[1] = self._prices.low[start:stop] \n        res[2] = self._prices.close[start:stop] \n        if self.volumes: \n            res[3] = self._prices.volume[start:stop] \n            dst = 4 \n        else: \n            dst = 3 \n        if self.have_position: \n            res[dst] = 1.0 \n            res[dst+1] = self._cur_close() / self.open_price - 1.0 \n        return res\n```", "```py\nclass SimpleFFDQN(nn.Module): \n    def __init__(self, obs_len: int, actions_n: int): \n        super(SimpleFFDQN, self).__init__() \n\n        self.fc_val = nn.Sequential( \n            nn.Linear(obs_len, 512), \n            nn.ReLU(), \n            nn.Linear(512, 512), \n            nn.ReLU(), \n            nn.Linear(512, 1) \n        ) \n\n        self.fc_adv = nn.Sequential( \n            nn.Linear(obs_len, 512), \n            nn.ReLU(), \n            nn.Linear(512, 512), \n            nn.ReLU(), \n            nn.Linear(512, actions_n) \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        val = self.fc_val(x) \n        adv = self.fc_adv(x) \n        return val + (adv - adv.mean(dim=1, keepdim=True))\n```", "```py\nclass DQNConv1D(nn.Module): \n    def __init__(self, shape: tt.Tuple[int, ...], actions_n: int): \n        super(DQNConv1D, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv1d(shape[0], 128, 5), \n            nn.ReLU(), \n            nn.Conv1d(128, 128, 5), \n            nn.ReLU(), \n            nn.Flatten(), \n        ) \n        size = self.conv(torch.zeros(1, *shape)).size()[-1] \n\n        self.fc_val = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, 1) \n        ) \n\n        self.fc_adv = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, actions_n) \n        ) \n\n    def forward(self, x: torch.Tensor) -> torch.Tensor: \n        conv_out = self.conv(x) \n        val = self.fc_val(conv_out) \n        adv = self.fc_adv(conv_out) \n        return val + (adv - adv.mean(dim=1, keepdim=True))\n```", "```py\nChapter10$ ./run_model.py -d data/YNDX_160101_161231.csv -m saves/simple-t1/mean_value-0.277.data -b 10 -n YNDX16\n```"]