<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer714">
<h1 class="chapter-number" id="_idParaDest-182"><a id="_idTextAnchor187"/>16</h1>
<h1 id="_idParaDest-183"><a id="_idTextAnchor188"/>Detecting Anomalies Using Heterogeneous GNNs</h1>
<p>In machine learning, anomaly detection<a id="_idIndexMarker880"/> is a popular task that aims to identify patterns or observations in data that deviate from the expected behavior. This is a fundamental problem that arises in many real-world applications, such as detecting fraud in financial transactions, identifying defective products in a manufacturing process, and detecting cyber attacks in a <span class="No-Break">computer network.</span></p>
<p>GNNs can be trained to learn the normal behavior of a network and then identify nodes or patterns that deviate from that behavior. Indeed, their ability to understand complex relationships makes them particularly appropriate to detect weak signals. Additionally, GNNs can be scaled to large datasets, making them an efficient tool for processing large amounts <span class="No-Break">of data.</span></p>
<p>In this chapter, we will build a GNN application for anomaly detection in computer networks. First, we will introduce the <strong class="source-inline">CIDDS-001</strong> dataset, which contains attacks and benign traffic in a computer network. Next, we will process the dataset, preparing it for input into GNNs. We will then move on to implementing a heterogenous GNN to handle different types of nodes and edges. Finally, we will train the network using the processed dataset and evaluate the results to see how well it detects anomalies in the <span class="No-Break">network traffic.</span></p>
<p>By the end of this chapter, you will know how to implement a GNN for intrusion detection. In addition, you will know how to build relevant features to detect attacks and process them to feed them to a GNN. Finally, you will learn how to implement and evaluate a heterogenous GNN to detect <span class="No-Break">rare attacks.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Exploring the <span class="No-Break">CIDDS-001 dataset</span></li>
<li>Preprocessing the <span class="No-Break">CIDDS-001 dataset</span></li>
<li>Implementing a <span class="No-Break">heterogeneous GNN</span></li>
</ul>
<h1 id="_idParaDest-184"><a id="_idTextAnchor189"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter16"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter16</span></a><span class="No-Break">.</span></p>
<p>The installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> of this book. This chapter requires a large amount of GPU. You can lower it by decreasing the size of the training set in <span class="No-Break">the code.</span></p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor190"/>Exploring the CIDDS-001 dataset</h1>
<p>This section will explore the dataset and get more insights about feature importance <span class="No-Break">and scaling.</span></p>
<p>The <strong class="source-inline">CIDDS-001</strong> dataset [1] is designed<a id="_idIndexMarker881"/> to train and evaluate anomaly-based network intrusion detection systems. It provides realistic traffic that includes up-to-date attacks to assess these systems. It was created by collecting and labeling 8,451,520 traffic flows in a virtual environment using OpenStack. Precisely, each row corresponds to a NetFlow connection, describing <strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) traffic statistics, such as the number of <span class="No-Break">bytes exchanged.</span></p>
<p>The following figure provides an overview of the simulated network environment <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">CIDDS-001</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer707">
<img alt="Figure 16.1 – Overview of the virtual network simulated by CIDDS-001" height="1096" src="image/B19153_16_001.jpg" width="1111"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.1 – Overview of the virtual network simulated by CIDDS-001</p>
<p>We see four<a id="_idIndexMarker882"/> different subnets (developer, office, management, and server) with their respective IP address ranges. All these subnets are linked to a single server connected to the internet through a firewall. An external server is also present and provides two services: a file synchronization service and a web server. Finally, attackers are represented outside of the <span class="No-Break">local network.</span></p>
<p>Connections in <strong class="source-inline">CIDDS-001</strong> were collected from the local and external servers. The goal of this dataset is to correctly classify these connections into five categories: benign (no attack), brute-force, denial of service, ping scan, and <span class="No-Break">port scan.</span></p>
<p>Let’s download the <strong class="source-inline">CIDDS-001</strong> dataset and explore its <span class="No-Break">input features:</span></p>
<ol>
<li>We <span class="No-Break">download </span><span class="No-Break"><strong class="source-inline">CIDDS-001</strong></span><span class="No-Break">:</span><pre class="source-code">
from io import BytesIO
from urllib.request import urlopen
from zipfile import ZipFile
url = 'https://www.hs-coburg.de/fileadmin/hscoburg/WISENT-CIDDS-001.zip'
with urlopen(url) as zurl:
    with ZipFile(BytesIO(zurl.read())) as zfile:
        zfile.extractall('.')</pre></li>
<li>We <a id="_idIndexMarker883"/>import the <span class="No-Break">required libraries:</span><pre class="source-code">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import itertools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PowerTransformer
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from torch_geometric.loader import DataLoader
from torch_geometric.data import HeteroData
from torch.nn import functional as F
from torch.optim import Adam
from torch import nn
import torch</pre></li>
<li>We load the dataset <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">:</span><pre class="source-code">
df = pd.read_csv("CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv")</pre></li>
<li>Let’s look<a id="_idIndexMarker884"/> at the data corresponding to the first <span class="No-Break">five connections:</span><pre class="source-code">
df.head(5)
<strong class="bold">Date first seen Duration Proto Src IP Addr Src Pt Dst IP Addr Dst Pt Packets Bytes Flows Flags Tos class attackType attackID attackDescription</strong>
<strong class="bold">2017-03-15 00:01:16.632 0.000 TCP 192.168.100.5 445 192.168.220.16 58844.0 1 108 1 .AP... 0 normal --- --- ---</strong>
<strong class="bold">2017-03-15 00:01:16.552 0.000 TCP 192.168.100.5 445 192.168.220.15 48888.0 1 108 1 .AP... 0 normal --- --- ---</strong>
<strong class="bold">2017-03-15 00:01:16.551 0.004 TCP 192.168.220.15 48888 192.168.100.5 445.0 2 174 1 .AP... 0 normal --- --- ---</strong>
<strong class="bold">2017-03-15 00:01:16.631 0.004 TCP 192.168.220.16 58844 192.168.100.5 445.0 2 174 1 .AP... 0 normal --- --- ---</strong>
<strong class="bold">2017-03-15 00:01:16.552 0.000 TCP 192.168.100.5 445 192.168.220.15 48888.0 1 108 1 .AP... 0 normal --- --- ---</strong></pre></li>
</ol>
<p>There are a few interesting features we can use for <span class="No-Break">our model:</span></p>
<ul>
<li>The date first seen is a timestamp we can process to extract information about the day of the week and the time of day. In general, network traffic is seasonal, and connections that occur at night or on unusual days <span class="No-Break">are suspicious.</span></li>
<li>IP addresses (such as <strong class="source-inline">192.168.100.5</strong>) are notoriously difficult to process because they are not numerical values and follow a complex set of rules. We could bin them into a few categories since we know how our local network is set up. Another popular and more generalizable solution is to convert them into a binary representation (“192” <span class="No-Break">becomes “11000000”).</span></li>
<li>Duration, the number of packets, and the number of bytes are features that usually display heavy-tailed distributions. Therefore, they will require special processing if that is <span class="No-Break">the case.</span></li>
</ul>
<p>Let’s check this<a id="_idIndexMarker885"/> last point and look closely at the distribution of attacks in <span class="No-Break">this dataset:</span></p>
<ol>
<li>We start by removing features we will not consider in this project: ports, the number of flows, type of service, class, attack ID, and <span class="No-Break">attack description:</span><pre class="source-code">
df = df.drop(columns=['Src Pt', 'Dst Pt', 'Flows', 'Tos', 'class', 'attackID', 'attackDescription'])</pre></li>
<li>We rename the benign class and convert the “date first seen” feature into the timestamp <span class="No-Break">data type:</span><pre class="source-code">
df['attackType'] = df['attackType'].replace('---', 'benign')
df['Date first seen'] = pd.to_datetime(df['Date first seen'])</pre></li>
<li>We count the labels and make a pie chart with the three most represented classes (the two others are <span class="No-Break">under 0.1%):</span><pre class="source-code">
count_labels = df['attackType'].value_counts() / len(df) * 100
plt.pie(count_labels[:3], labels=df['attackType'].unique()[:3], autopct='%.0f%%')</pre></li>
<li>We obtain the <span class="No-Break">following plot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer708">
<img alt="Figure 16.2 – Proportion of each class in the CIDDS-001 dataset" height="804" src="image/B19153_16_002..jpg" width="1022"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.2 – Proportion of each class in the CIDDS-001 dataset</p>
<p>As you can<a id="_idIndexMarker886"/> see, benign traffic represents the immense majority of this dataset. On the contrary, brute-force attacks and ping scans have very few samples in comparison. This imbalanced learning setting could negatively impact the model’s performance when dealing with <span class="No-Break">rare classes.</span></p>
<ol>
<li value="5">Finally, we can display the duration distribution, number of packets, and number of bytes. This allows us to see whether they really need a specific <span class="No-Break">rescaling process:</span><pre class="source-code">
fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(20,5))
df['Duration'].hist(ax=ax1)
ax1.set_xlabel("Duration")
df['Packets'].hist(ax=ax2)
ax2.set_xlabel("Number of packets")
pd.to_numeric(df['Bytes'], errors='coerce').hist(ax=ax3)
ax3.set_xlabel("Number of bytes")
plt.show()</pre></li>
</ol>
<p>It outputs the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer709">
<img alt="Figure 16.3 – Distributions of duration, the number of packets, and number of bytes" height="636" src="image/B19153_16_003..jpg" width="1608"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.3 – Distributions of duration, the number of packets, and number of bytes</p>
<p>We can see that most<a id="_idIndexMarker887"/> values are close to zero, but there is also a long tail of rare values stretching along the <em class="italic">x</em> axes. We will use a power transform to make these features more Gaussian-like, which should help the model <span class="No-Break">during training.</span></p>
<p>Now that we have explored the main characteristics of the <strong class="source-inline">CIDDS-001</strong> dataset, we can move on to the <span class="No-Break">preprocessing stage.</span></p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor191"/>Preprocessing the CIDDS-001 dataset</h1>
<p>In the last section, we identified some<a id="_idIndexMarker888"/> issues with the dataset we need to address to improve the accuracy of <span class="No-Break">our model.</span></p>
<p>The <strong class="source-inline">CIDDS-001</strong> dataset includes diverse types of data: we have numerical values such as duration, categorical features such as protocols (TCP, UDP, ICMP, and IGMP), and others such as timestamps or IP addresses. In the following exercise, we will choose how to represent these data types based on the information from the previous section and <span class="No-Break">expert knowledge:</span></p>
<ol>
<li>First, we can one-hot-encode the day of the week by retrieving this information from the timestamp. We will rename the resulting columns to make them <span class="No-Break">more readable:</span><pre class="source-code">
df['weekday'] = df['Date first seen'].dt.weekday
df = pd.get_dummies(df, columns=['weekday']).rename(columns = {'weekday_0': 'Monday','weekday_1': 'Tuesday','weekday_2': 'Wednesday', 'weekday_3': 'Thursday','weekday_4': 'Friday','weekday_5': 'Saturday','weekday_6': 'Sunday',})</pre></li>
<li>Another important type of information we can get using timestamps is the time of day. We also normalize it between <strong class="source-inline">0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span><pre class="source-code">
df['daytime'] = (df['Date first seen'].dt.second +df['Date first seen'].dt.minute*60 + df['Date first seen'].dt.hour*60*60)/(24*60*60)</pre></li>
<li>We have not talked about TCP flags yet. Each flag indicates a particular state during a TCP connection. For example, <strong class="source-inline">F</strong> or <strong class="source-inline">FIN</strong> signifies that the TCP peer has finished sending data. We can extract each flag, and one-hot-encode them <span class="No-Break">as follows:</span><pre class="source-code">
df = df.reset_index(drop=True)
ohe_flags = one_hot_flags(df['Flags'].to_numpy())
ohe_flags = df['Flags'].apply(one_hot_flags).to_list()
df[['ACK', 'PSH', 'RST', 'SYN', 'FIN']] = pd.DataFrame(ohe_flags, columns=['ACK', 'PSH', 'RST', 'SYN', 'FIN'])</pre></li>
<li>Let’s now<a id="_idIndexMarker889"/> process the IP addresses. In this example, we will use binary encoding. Instead of taking 32 bits to encode the complete IPv4 address, we will only keep the last 16 bits, which are the most significant here. Indeed, the 16 first bits either correspond to <strong class="source-inline">192.168</strong> if the host belongs to the internal network or another value if <span class="No-Break">it’s external:</span><pre class="source-code">
temp = pd.DataFrame()
temp['SrcIP'] = df['Src IP Addr'].astype(str)
temp['SrcIP'][~temp['SrcIP'].str.contains('\d{1,3}\.', regex=True)] = '0.0.0.0'
temp = temp['SrcIP'].str.split('.', expand=True).rename(columns = {2: 'ipsrc3', 3: 'ipsrc4'}).astype(int)[['ipsrc3', 'ipsrc4']]
temp['ipsrc'] = temp['ipsrc3'].apply(lambda x: format(x, "b").zfill(8)) + temp['ipsrc4'].apply(lambda x: format(x, "b").zfill(8))
df = df.join(temp['ipsrc'].str.split('', expand=True)
            .drop(columns=[0, 17])
            .rename(columns=dict(enumerate([f'ipsrc_{i}' for i in range(17)])))
            .astype('int32'))</pre></li>
<li>We repeat this process for destination <span class="No-Break">IP addresses:</span><pre class="source-code">
temp = pd.DataFrame()
temp['DstIP'] = df['Dst IP Addr'].astype(str)
temp['DstIP'][~temp['DstIP'].str.contains('\d{1,3}\.', regex=True)] = '0.0.0.0'
temp = temp['DstIP'].str.split('.', expand=True).rename(columns = {2: 'ipdst3', 3: 'ipdst4'}).astype(int)[['ipdst3', 'ipdst4']]
temp['ipdst'] = temp['ipdst3'].apply(lambda x: format(x, "b").zfill(8)) + temp['ipdst4'].apply(lambda x: format(x, "b").zfill(8))
df = df.join(temp['ipdst'].str.split('', expand=True)
            .drop(columns=[0, 17])
            .rename(columns=dict(enumerate([f'ipdst_{i}' for i in range(17)])))
            .astype('int32'))</pre></li>
<li>There is an <a id="_idIndexMarker890"/>issue with the ‘<strong class="source-inline">Bytes</strong>’ feature: millions are represented as <strong class="source-inline">m</strong> instead of a numerical value. We can fix it by multiplying the numerical part of these non-numerical values by <span class="No-Break">one million:</span><pre class="source-code">
m_index = df[pd.to_numeric(df['Bytes'], errors='coerce').isnull() == True].index
df['Bytes'].loc[m_index] = df['Bytes'].loc[m_index].apply(lambda x: 10e6 * float(x.strip().split()[0]))
df['Bytes'] = pd.to_numeric(df['Bytes'], errors='coerce', downcast='integer')</pre></li>
<li>The last features we need to encode are the easiest ones: categorical features such as protocols and attack types. We use the <strong class="source-inline">get_dummies()</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">:</span><pre class="source-code">
df = pd.get_dummies(df, prefix='', prefix_sep='', columns=['Proto', 'attackType'])</pre></li>
<li>We create a train/validation/test split with <span class="No-Break">80/10/10 ratios:</span><pre class="source-code">
labels = ['benign', 'bruteForce', 'dos', 'pingScan', 'portScan']
df_train, df_test = train_test_split(df, random_state=0, test_size=0.2, stratify=df[labels])
df_val, df_test = train_test_split(df_test, random_state=0, test_size=0.5, stratify=df_test[labels])</pre></li>
<li>Finally, we need to <a id="_idIndexMarker891"/>address the scaling of three features: duration, the number of packets, and the number of bytes. We use <strong class="source-inline">PowerTransformer()</strong> from <strong class="source-inline">scikit-learn</strong> to modify <span class="No-Break">their distributions:</span><pre class="source-code">
scaler = PowerTransformer()
df_train[['Duration', 'Packets', 'Bytes']] = scaler.fit_transform(df_train[['Duration', 'Packets', 'Bytes']])
df_val[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_val[['Duration', 'Packets', 'Bytes']])
df_test[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_test[['Duration', 'Packets', 'Bytes']])</pre></li>
<li>Let’s plot the new distributions to see how <span class="No-Break">they compare:</span><pre class="source-code">
fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(15,5))
df_train['Duration'].hist(ax=ax1)
ax1.set_xlabel("Duration")
df_train['Packets'].hist(ax=ax2)
ax2.set_xlabel("Number of packets")
df_train['Bytes'].hist(ax=ax3)
ax3.set_xlabel("Number of bytes")
plt.show()</pre></li>
</ol>
<p>We obtain the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer710">
<img alt="Figure 16.4 – Rescaled distributions of duration, the number of packets, and the number of bytes" height="636" src="image/B19153_16_004..jpg" width="1610"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.4 – Rescaled distributions of duration, the number of packets, and the number of bytes</p>
<p>These new distributions <a id="_idIndexMarker892"/>are not Gaussian, but the values are more spread out, which should help <span class="No-Break">the model.</span></p>
<p>Note that the dataset we processed is purely tabular. We still need to convert it into a graph dataset before we can feed it to a GNN. In our case, there is no obvious way of converting our traffic flows into nodes. Ideally, flows between the same computers should be connected. This can be achieved using a heterogeneous graph with two types <span class="No-Break">of nodes:</span></p>
<ul>
<li><strong class="bold">Hosts</strong>, which <a id="_idIndexMarker893"/>correspond to computers and use IP addresses as features. If we had more information, we could add other computer-related features, such as logs or <span class="No-Break">CPU utilization.</span></li>
<li><strong class="bold">Flows</strong>, which<a id="_idIndexMarker894"/> correspond to connections between two hosts. They consider all the other features from the dataset. They also have the label we want to predict (a benign or <span class="No-Break">malicious flow).</span></li>
</ul>
<p>In this example, flows are unidirectional, which is why we also define two types of edges: host-to-flow (source), and flow-to-host (destination). A single graph would require too much memory, so we will divide it into subgraphs and place them into <span class="No-Break">data loaders:</span></p>
<ol>
<li>We define the<a id="_idIndexMarker895"/> batch size and the features we want to consider for host and <span class="No-Break">flow nodes:</span><pre class="source-code">
BATCH_SIZE = 16
features_host = [f'ipsrc_{i}' for i in range(1, 17)] + [f'ipdst_{i}' for i in range(1, 17)]
features_flow = ['daytime', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Duration', 'Packets', 'Bytes', 'ACK', 'PSH', 'RST', 'SYN', 'FIN', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']</pre></li>
<li>We define the function that will create our data loaders. It takes two parameters: the tabular DataFrame we created, and the subgraph size (<strong class="source-inline">1024</strong> nodes in <span class="No-Break">this example):</span><pre class="source-code">
def create_dataloader(df, subgraph_size=1024):</pre></li>
<li>We initialize a list called <strong class="source-inline">data</strong> to store our subgraphs, and we count the number of subgraphs we <span class="No-Break">will create:</span><pre class="source-code">
    data = []
    n_subgraphs = len(df) // subgraph_size</pre></li>
<li>For each subgraph, we retrieve the corresponding samples in the DataFrame, the list of source IP addresses, and the list of destination <span class="No-Break">IP addresses:</span><pre class="source-code">
    for i in range(1, n_batches+1):
        subgraph = df[(i-1)*subgraph_size:i*subgraph_size]
        src_ip = subgraph['Src IP Addr'].to_numpy()
        dst_ip = subgraph['Dst IP Addr'].to_numpy()</pre></li>
<li>We create a dictionary that maps the IP addresses to a <span class="No-Break">node index:</span><pre class="source-code">
ip_map = {ip:index for index, ip in enumerate(np.unique(np.append(src_ip, dst_ip)))}</pre></li>
<li>This dictionary will help us to create the edge index from the host to the flow and vice versa. We use a function called <strong class="source-inline">get_connections()</strong>, which we will create after <span class="No-Break">this one.</span><pre class="source-code">
host_to_flow, flow_to_host = get_connections(ip_map, src_ip, dst_ip)</pre></li>
<li>We use all<a id="_idIndexMarker896"/> the data we have collected so far to create a heterogeneous graph for each subgraph and append it to <span class="No-Break">the list:</span><pre class="source-code">
        batch = HeteroData()
        batch['host'].x = torch.Tensor(subgraph[features_host].to_numpy()).float()
        batch['flow'].x = torch.Tensor(subgraph[features_flow].to_numpy()).float()
        batch['flow'].y = torch.Tensor(subgraph[labels].to_numpy()).float()
        batch['host','flow'].edge_index = host_to_flow
        batch['flow','host'].edge_index = flow_to_host
        data.append(batch)</pre></li>
<li>Finally, we return the data loader with the appropriate <span class="No-Break">batch size:</span><pre class="source-code">
return DataLoader(data, batch_size=BATCH_SIZE)</pre></li>
<li>There is one function we still need to implement – <strong class="source-inline">get_connections()</strong> – which calculates two edge indices from the list of source and destination IP addresses and their <span class="No-Break">corresponding map:</span><pre class="source-code">
def get_connections(ip_map, src_ip, dst_ip):</pre></li>
<li>We get indexes from the IP addresses (both source and destination) and <span class="No-Break">stack them:</span><pre class="source-code">
    src1 = [ip_map[ip] for ip in src_ip]
    src2 = [ip_map[ip] for ip in dst_ip]
    src = np.column_stack((src1, src2)).flatten()</pre></li>
<li>The connections are unique, so we can easily index them with the appropriate range <span class="No-Break">of numbers:</span><pre class="source-code">
    dst = list(range(len(src_ip)))
    dst = np.column_stack((dst, dst)).flatten()</pre></li>
<li>Finally, we return the two following <span class="No-Break">edge indexes:</span><pre class="source-code">
return torch.Tensor([src, dst]).int(), torch.Tensor([dst, src]).int()</pre></li>
<li>Now that we have everything we need, we can call the first function to create the training, validation, and test <span class="No-Break">data loaders:</span><pre class="source-code">
train_loader = create_dataloader(df_train)
val_loader = create_dataloader(df_val)
test_loader = create_dataloader(df_test)</pre></li>
<li>We now have<a id="_idIndexMarker897"/> three data loaders corresponding to our training, validation, and test sets. The next step consists of implementing the <span class="No-Break">GNN model.</span></li>
</ol>
<h1 id="_idParaDest-187"><a id="_idTextAnchor192"/>Implementing a heterogeneous GNN</h1>
<p>In this section, we <a id="_idIndexMarker898"/>will implement a heterogeneous GNN using a <strong class="source-inline">GraphSAGE</strong> operator. This architecture will allow us to consider both node types (hosts and flows) to build better embeddings. This is done by duplicating and sharing messages across different layers, as shown in the <span class="No-Break">following figure.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer711">
<img alt="Figure 16.5 – Architecture of the heterogeneous GNN" height="973" src="image/Image98417.jpg" width="891"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.5 – Architecture of the heterogeneous GNN</p>
<p>We will implement<a id="_idIndexMarker899"/> three layers of <strong class="source-inline">SAGEConv</strong> with <strong class="source-inline">LeakyRELU</strong> for each node type. Finally, a linear layer will output a five-dimensional vector, where each dimension corresponds to a class. Furthermore, we will train this model in a supervised way using the cross-entropy loss and the <span class="No-Break"><strong class="source-inline">Adam</strong></span><span class="No-Break"> optimizer:</span></p>
<ol>
<li>We import the relevant neural network layers from <span class="No-Break">PyTorch Geometric:</span><pre class="source-code">
import torch_geometric.transforms as T
from torch_geometric.nn import Linear, HeteroConv, SAGEConv</pre></li>
<li>We define the heterogeneous GNN with three parameters: the number of hidden dimensions, the number of output dimensions, and the number <span class="No-Break">of layers:</span><pre class="source-code">
class HeteroGNN(torch.nn.Module):
    def __init__(self, dim_h, dim_out, num_layers):
        super().__init__()</pre></li>
<li>We define a <a id="_idIndexMarker900"/>heterogenous version of the <strong class="source-inline">GraphSAGE</strong> operator for each layer and edge type. Here, we could apply a different GNN layer to each edge type, such as <strong class="source-inline">GCNConv</strong> or <strong class="source-inline">GATConv</strong>. The <strong class="source-inline">HeteroConv()</strong> wrapper manages the messages between layers, as shown in <span class="No-Break"><em class="italic">Figure 16</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span><pre class="source-code">
        self.convs = torch.nn.ModuleList()
        for _ in range(num_layers):
            conv = HeteroConv({
                ('host', 'to', 'flow'): SAGEConv((-1,-1), dim_h, add_self_loops=False),
                ('flow', 'to', 'host'): SAGEConv((-1,-1), dim_h, add_self_loops=False),
            }, aggr='sum')
            self.convs.append(conv)</pre></li>
<li>We define a linear layer that will output the <span class="No-Break">final classification:</span><pre class="source-code">
        self.lin = Linear(dim_h, dim_out)</pre></li>
<li>We create the <strong class="source-inline">forward()</strong> method, which computes embeddings for host and flow nodes (stored in the <strong class="source-inline">x_dict</strong> dictionary). The flow embeddings are then used to predict <span class="No-Break">a class:</span><pre class="source-code">
    def forward(self, x_dict, edge_index_dict):
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)
            x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}
        return self.lin(x_dict['flow'])</pre></li>
<li>We instantiate the heterogeneous GNN with 64 hidden dimensions, 5 outputs (our 5 classes), and 3 layers. If available, we place it on a GPU and create an <strong class="source-inline">Adam</strong> optimizer with a learning rate <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.001</strong></span><span class="No-Break">:</span><pre class="source-code">
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = HeteroGNN(dim_h=64, dim_out=5, num_layers=3).to(device)
optimizer = Adam(model.parameters(), lr=0.001)</pre></li>
<li>We define<a id="_idIndexMarker901"/> the <strong class="source-inline">test()</strong> function and create arrays to store predictions and <strong class="source-inline">true</strong> labels. We also want to count the number of subgraphs and the total loss, so we create the <span class="No-Break">corresponding variables:</span><pre class="source-code">
@torch.no_grad()
def test(loader):
    model.eval()
    y_pred = []
    y_true = []
    n_subgraphs = 0
    total_loss = 0</pre></li>
<li>We get the model’s prediction for each batch and compute the <span class="No-Break">cross-entropy loss:</span><pre class="source-code">
   for batch in loader:
        batch.to(device)
        out = model(batch.x_dict, batch.edge_index_dict)
        loss = F.cross_entropy(out, batch['flow'].y.float())</pre></li>
<li>We append the predicted class to the list of predictions and do the same with the <span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break"> labels:</span><pre class="source-code">
        y_pred.append(out.argmax(dim=1))
        y_true.append(batch['flow'].y.argmax(dim=1))</pre></li>
<li>We count the number of subgraphs and the total loss <span class="No-Break">as follows:</span><pre class="source-code">
        n_subgraphs += BATCH_SIZE
        total_loss += float(loss) * BATCH_SIZE</pre></li>
<li>Now that the<a id="_idIndexMarker902"/> batch loop is over, we compute the <strong class="source-inline">F1</strong> score (macro) using the prediction and <strong class="source-inline">true</strong> label lists. The macro-averaged <strong class="source-inline">F1</strong> score is a good metric in this imbalanced learning setting because it treats all classes equally regardless of the number <span class="No-Break">of samples:</span><pre class="source-code">
    y_pred = torch.cat(y_pred).cpu()
    y_true = torch.cat(y_true).cpu()
    f1score = f1_score(y_true, y_pred, average='macro')</pre></li>
<li>We return the final loss, the macro-averaged <strong class="source-inline">F1</strong> score, the list of predictions, and the list of <span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break"> labels:</span><pre class="source-code">
    return total_loss/n_subgraphs, f1score, y_pred, y_true</pre></li>
<li>We create the training loop to train the model for <span class="No-Break"><strong class="source-inline">101</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
model.train()
for epoch in range(101):
    n_subgraphs = 0
    total_loss = 0</pre></li>
<li>We train the heterogenous GNN on each batch using the <span class="No-Break">cross-entropy loss:</span><pre class="source-code">
    for batch in train_loader:
        optimizer.zero_grad()
        batch.to(device)
        out = model(batch.x_dict, batch.edge_index_dict)
        loss = F.cross_entropy(out, batch['flow'].y.float())
        loss.backward()
        optimizer.step()
        n_subgraphs += BATCH_SIZE
        total_loss += float(loss) * BATCH_SIZE</pre></li>
<li>Every 10 epochs, we <a id="_idIndexMarker903"/>evaluate the model on the validation set and display relevant metrics (the training loss, validation loss, and validation macro-averaged <span class="No-Break"><strong class="source-inline">F1</strong></span><span class="No-Break"> score):</span><pre class="source-code">
    if epoch % 10 == 0:
        val_loss, f1score, _, _ = test(val_loader)
        print(f'Epoch {epoch} | Loss: {total_loss/n_subgraphs:.4f} | Val loss: {val_loss:.4f} | Val F1 score: {f1score:.4f}')</pre></li>
</ol>
<p>We obtain the following output <span class="No-Break">during training:</span></p>
<pre class="source-code">
<strong class="bold">Epoch 0 | Loss: 0.1006 | Val loss: 0.0072 | Val F1 score: 0.6044</strong>
<strong class="bold">Epoch 10 | Loss: 0.0020 | Val loss: 0.0021 | Val F1-score: 0.8899</strong>
<strong class="bold">Epoch 20 | Loss: 0.0015 | Val loss: 0.0015 | Val F1-score: 0.9211</strong>
<strong class="bold">...</strong>
<strong class="bold">Epoch 90 | Loss: 0.0004 | Val loss: 0.0008 | Val F1-score: 0.9753</strong>
<strong class="bold">Epoch 100 | Loss: 0.0004 | Val loss: 0.0009 | Val F1-score: 0.9785</strong></pre>
<ol>
<li value="16">Finally, we <a id="_idIndexMarker904"/>evaluate the model on the test set. We also print <strong class="source-inline">scikit-learn</strong>’s classification report, which includes the macro-averaged <span class="No-Break">F1 score:</span><pre class="source-code">
_, _, y_pred, y_true = test(test_loader)
print(classification_report(y_true, y_pred, target_names=labels, digits=4))
<strong class="bold">              precision    recall  f1-score   support</strong>
<strong class="bold">      benign     0.9999    0.9999    0.9999    700791</strong>
<strong class="bold">  bruteForce     0.9811    0.9630    0.9720       162</strong>
<strong class="bold">         dos     1.0000    1.0000    1.0000    125164</strong>
<strong class="bold">    pingScan     0.9413    0.9554    0.9483       336</strong>
<strong class="bold">    portScan     0.9947    0.9955    0.9951     18347</strong>
<strong class="bold">    accuracy                         0.9998    844800</strong>
<strong class="bold">   macro avg     0.9834    0.9827    0.9831    844800</strong>
<strong class="bold">weighted avg     0.9998    0.9998    0.9998    844800</strong></pre></li>
</ol>
<p>We obtained a macro-averaged <strong class="source-inline">F1</strong> score of <strong class="source-inline">0.9831</strong>. This excellent result shows that our model has learned to predict each <span class="No-Break">class reliably.</span></p>
<p>The approach we adopted would be even more relevant if we could access more host-related features, but it shows how you can expand it to fit your needs. The other main advantage of GNNs is their ability to process large amounts of data. This approach makes even more sense when dealing with millions of flows. To finish this project, let’s plot the model’s errors to see how we could <span class="No-Break">improve it.</span></p>
<p>We create <a id="_idIndexMarker905"/>a dataframe to store the predictions (<strong class="source-inline">y_pred</strong>) and the true labels (<strong class="source-inline">y_true</strong>). We use this new dataframe to plot the proportion of <span class="No-Break">misclassified samples:</span></p>
<pre class="source-code">
df_pred = pd.DataFrame([y_pred.numpy(), y_true.numpy()]).T
df_pred.columns = ['pred', 'true']
plt.pie(df_pred['true'][df_pred['pred'] != df_pred['true']].value_counts(), labels=labels, autopct='%.0f%%')</pre>
<p>This gives us the <span class="No-Break">following chart:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer712">
<img alt="Figure 16.6 – Proportion of each misclassified class" height="850" src="image/B19153_16_006..jpg" width="1036"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.6 – Proportion of each misclassified class</p>
<p>If we compare this pie chart to the original proportions in the dataset, we see that the model performs better for the majority classes. This is not surprising since minority classes are harder to learn (fewer samples), and not detecting them is less penalizing (with 700,000 benign flows versus 336 ping scans). Port and ping scan detection could be improved with techniques such as oversampling and introducing class weights <span class="No-Break">during training.</span></p>
<p>We can gather even<a id="_idIndexMarker906"/> more information by looking at the confusion matrix (the code can be found <span class="No-Break">on GitHub).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer713">
<img alt="Figure 16.7 – Confusion matrix for multi-class flow classification" height="813" src="image/B19153_16_007..jpg" width="1061"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.7 – Confusion matrix for multi-class flow classification</p>
<p>This confusion matrix displays interesting results, such as a bias toward the benign class or errors between ping and port scans. These errors can be attributed to the similarity between these attacks. Engineering additional features could help the model distinguish <span class="No-Break">these classes.</span></p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor193"/>Summary</h1>
<p>In this chapter, we explored the use of GNNs for detecting anomalies in a new dataset, the <strong class="source-inline">CIDDS-001</strong> dataset. First, we preprocessed the dataset and converted it into a graph representation, allowing us to capture the complex relationships between the different components of the network. We then implemented a heterogeneous GNN with <strong class="source-inline">GraphSAGE</strong> operators. It captured the heterogeneity of the graph and allowed us to classify the flows as benign <span class="No-Break">or malicious.</span></p>
<p>The application of GNNs in network security has shown promising results and opened up new avenues for research. As technology continues to advance and the amount of network data increases, GNNs will become an increasingly important tool for detecting and preventing <span class="No-Break">security breaches.</span></p>
<p>In <a href="B19153_17.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic">Chapter 17</em></span></a>, <em class="italic">Recommending Books Using LightGCN</em>, we will explore the most popular application of GNNs with recommender systems. We will implement a lightweight GNN on a large dataset and produce book recommendations for <span class="No-Break">given users.</span></p>
<h1 id="_idParaDest-189"><a id="_idTextAnchor194"/>Further reading</h1>
<ul>
<li>[1] M. Ring, S. Wunderlich, D. Grüdl, D. Landes, and A. Hotho, <em class="italic">Flow-based benchmark data sets for intrusion detection</em>, in <em class="italic">Proceedings of the 16th European Conference on Cyber Warfare and Security</em> (ECCWS), ACPI, 2017, <span class="No-Break">pp. 361–369.</span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer715">
</div>
</div>
</div></body></html>