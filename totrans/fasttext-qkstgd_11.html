<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Notes for the Readers</h1>
                
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Windows and Linux</h1>
                
            
            <article>
                
<p class="calibre2">We would suggest that you use PowerShell for your windows command line as that is more powerful then simple <kbd class="calibre12">cmd</kbd>.</p>
<table border="1" class="calibre69">
<tbody class="calibre70">
<tr class="calibre71">
<td class="calibre72"><strong class="calibre1">Task</strong></td>
<td class="calibre72"><strong class="calibre1">Windows</strong></td>
<td class="calibre72"><strong class="calibre1">Linux</strong>/<strong class="calibre1">macOS</strong></td>
</tr>
<tr class="calibre71">
<td class="calibre72">Creating a directory</td>
<td class="calibre72"><kbd class="calibre12">mkdir</kbd></td>
<td class="calibre72"><kbd class="calibre12">mkdir</kbd></td>
</tr>
<tr class="calibre71">
<td class="calibre72">Change directory</td>
<td class="calibre72"><kbd class="calibre12">cd</kbd></td>
<td class="calibre72"><kbd class="calibre12">cd</kbd></td>
</tr>
<tr class="calibre71">
<td class="calibre72">Move files</td>
<td class="calibre72"><kbd class="calibre12">move</kbd></td>
<td class="calibre72"><kbd class="calibre12">mv</kbd></td>
</tr>
<tr class="calibre71">
<td class="calibre72">Unzip files</td>
<td class="calibre72">GUI and double click</td>
<td class="calibre72"><kbd class="calibre12">unzip</kbd></td>
</tr>
<tr class="calibre71">
<td class="calibre72">Top of the file</td>
<td class="calibre72"><kbd class="calibre12">get-content</kbd></td>
<td class="calibre72">
<p class="calibre2"><kbd class="calibre12">head</kbd></p>
</td>
</tr>
<tr class="calibre71">
<td class="calibre72">Contents of the file</td>
<td class="calibre72"><kbd class="calibre12">type</kbd></td>
<td class="calibre72">
<p class="calibre2"><kbd class="calibre12">cat</kbd></p>
</td>
</tr>
<tr class="calibre71">
<td class="calibre72">Piping</td>
<td class="calibre72"><kbd class="calibre12">this pipes objects</kbd></td>
<td class="calibre72">
<p class="calibre2"><kbd class="calibre12">this pipes text</kbd></p>
</td>
</tr>
<tr class="calibre71">
<td class="calibre72">Bottom of the file</td>
<td class="calibre72"><span><kbd class="calibre12">-wait</kbd> parameter with <kbd class="calibre12">get-content</kbd></span></td>
<td class="calibre72">
<p class="calibre2"><kbd class="calibre12">tail</kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">python and <kbd class="calibre12">perl</kbd> commands work the same way in windows as they work in bash and hence you can use those files and especially <kbd class="calibre12">perl</kbd> one liners in similar way.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Python 2 and Python 3</h1>
                
            
            <article>
                
<p class="calibre2">fastText works for both Python 2 and Python 3. There are few differences though that you should keep in mind for the particular python version.</p>
<ol class="calibre13">
<li value="1" class="calibre11"><kbd class="calibre12">print</kbd> is a statement in Python 2 and a function in Python 3. This would mean that if you are in a Jupyter notebook and trying to see the changes in a variable you will need to pass the appropriate print statement in the corresponding python version.</li>
<li value="2" class="calibre11">The fastText handles text as Unicode. Python 3 also handles text as Unicode and hence there is no additional overhead if you code in Python 3. But in case you are developing your models in Python 2, you cannot have your data as a string instance. You will need to have your data as Unicode. Following is an example of text as an instance of the <kbd class="calibre12">str</kbd> class and <kbd class="calibre12">unicode</kbd> class in Python 2.</li>
</ol>
<pre class="calibre17">&gt;&gt;&gt; text1 = "some text" # this will not work for fastText<br class="title-page-name"/>&gt;&gt;&gt; type(text1)<br class="title-page-name"/>&lt;type 'str'&gt;<br class="title-page-name"/>&gt;&gt;&gt; text2 = unicode("some text") # in fastText you will need to use this.<br class="title-page-name"/>&gt;&gt;&gt; type(text2)<br class="title-page-name"/>&lt;type 'unicode'&gt;<br class="title-page-name"/>&gt;&gt;&gt;</pre>
<p class="calibre2"> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The fastText command line</h1>
                
            
            <article>
                
<p class="calibre2">Following is the list of parameters that you can use with fastText command line:</p>
<pre class="calibre17"><strong class="calibre1">$ ./fasttext</strong><br class="title-page-name"/><strong class="calibre1">usage: fasttext &lt;command&gt; &lt;args&gt;</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The commands supported by fasttext are:</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">  supervised train a supervised classifier</strong><br class="title-page-name"/><strong class="calibre1">  quantize quantize a model to reduce the memory usage</strong><br class="title-page-name"/><strong class="calibre1">  test evaluate a supervised classifier</strong><br class="title-page-name"/><strong class="calibre1">  predict predict most likely labels</strong><br class="title-page-name"/><strong class="calibre1">  predict-prob predict most likely labels with probabilities</strong><br class="title-page-name"/><strong class="calibre1">  skipgram train a skipgram model</strong><br class="title-page-name"/><strong class="calibre1">  cbow train a cbow model</strong><br class="title-page-name"/><strong class="calibre1">  print-word-vectors print word vectors given a trained model</strong><br class="title-page-name"/><strong class="calibre1">  print-sentence-vectors print sentence vectors given a trained model</strong><br class="title-page-name"/><strong class="calibre1">  print-ngrams print ngrams given a trained model and word</strong><br class="title-page-name"/><strong class="calibre1">  nn query for nearest neighbors</strong><br class="title-page-name"/><strong class="calibre1">  analogies query for analogies</strong><br class="title-page-name"/><strong class="calibre1">  dump dump arguments,dictionary,input/output vectors</strong></pre>
<p class="calibre2">The <kbd class="calibre12">supervised</kbd>, <kbd class="calibre12">skipgram</kbd>, and <kbd class="calibre12">cbow</kbd> commands are for training a model. <kbd class="calibre12">predict</kbd>, <kbd class="calibre12">predict-prob</kbd> are for predictions on a supervised model. <kbd class="calibre12">test</kbd>, <kbd class="calibre12">print-word-vectors</kbd>, <kbd class="calibre12">print-sentence-vectors</kbd>, <kbd class="calibre12">print-ngrams</kbd>, <kbd class="calibre12">nn</kbd>, analogies can be used to evaluate the model. The <kbd class="calibre12">dump</kbd> command is basically to find the hyperparameters of the model and <kbd class="calibre12">quantize</kbd> is used to the compress the model.</p>
<p class="calibre2">The list of hyperparameters that you can use for training are listed later.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The fastText supervised</h1>
                
            
            <article>
                
<pre class="calibre17"><strong class="calibre1">$ ./fasttext supervised</strong><br class="title-page-name"/><strong class="calibre1">Empty input or output path.</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are mandatory:</strong><br class="title-page-name"/><strong class="calibre1">  -input training file path</strong><br class="title-page-name"/><strong class="calibre1">  -output output file path</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -verbose verbosity level [2]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for the dictionary are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -minCount minimal number of word occurences [1]</strong><br class="title-page-name"/><strong class="calibre1">  -minCountLabel minimal number of label occurences [0]</strong><br class="title-page-name"/><strong class="calibre1">  -wordNgrams max length of word ngram [1]</strong><br class="title-page-name"/><strong class="calibre1">  -bucket number of buckets [2000000]</strong><br class="title-page-name"/><strong class="calibre1">  -minn min length of char ngram [0]</strong><br class="title-page-name"/><strong class="calibre1">  -maxn max length of char ngram [0]</strong><br class="title-page-name"/><strong class="calibre1">  -t sampling threshold [0.0001]</strong><br class="title-page-name"/><strong class="calibre1">  -label labels prefix [__label__]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for training are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -lr learning rate [0.1]</strong><br class="title-page-name"/><strong class="calibre1">  -lrUpdateRate change the rate of updates for the learning rate [100]</strong><br class="title-page-name"/><strong class="calibre1">  -dim size of word vectors [100]</strong><br class="title-page-name"/><strong class="calibre1">  -ws size of the context window [5]</strong><br class="title-page-name"/><strong class="calibre1">  -epoch number of epochs [5]</strong><br class="title-page-name"/><strong class="calibre1">  -neg number of negatives sampled [5]</strong><br class="title-page-name"/><strong class="calibre1">  -loss loss function {ns, hs, softmax} [softmax]</strong><br class="title-page-name"/><strong class="calibre1">  -thread number of threads [12]</strong><br class="title-page-name"/><strong class="calibre1">  -pretrainedVectors pretrained word vectors for supervised learning []</strong><br class="title-page-name"/><strong class="calibre1">  -saveOutput whether output params should be saved [false]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for quantization are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -cutoff number of words and ngrams to retain [0]</strong><br class="title-page-name"/><strong class="calibre1">  -retrain whether embeddings are finetuned if a cutoff is applied [false]</strong><br class="title-page-name"/><strong class="calibre1">  -qnorm whether the norm is quantized separately [false]</strong><br class="title-page-name"/><strong class="calibre1">  -qout whether the classifier is quantized [false]</strong><br class="title-page-name"/><strong class="calibre1">  -dsub size of each sub-vector [2]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The fastText skipgram </h1>
                
            
            <article>
                
<pre class="calibre17"><strong class="calibre1">$ ./fasttext skipgram</strong><br class="title-page-name"/><strong class="calibre1">Empty input or output path.</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are mandatory:</strong><br class="title-page-name"/><strong class="calibre1">  -input training file path</strong><br class="title-page-name"/><strong class="calibre1">  -output output file path</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -verbose verbosity level [2]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for the dictionary are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -minCount minimal number of word occurences [5]</strong><br class="title-page-name"/><strong class="calibre1">  -minCountLabel minimal number of label occurences [0]</strong><br class="title-page-name"/><strong class="calibre1">  -wordNgrams max length of word ngram [1]</strong><br class="title-page-name"/><strong class="calibre1">  -bucket number of buckets [2000000]</strong><br class="title-page-name"/><strong class="calibre1">  -minn min length of char ngram [3]</strong><br class="title-page-name"/><strong class="calibre1">  -maxn max length of char ngram [6]</strong><br class="title-page-name"/><strong class="calibre1">  -t sampling threshold [0.0001]</strong><br class="title-page-name"/><strong class="calibre1">  -label labels prefix [__label__]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for training are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -lr learning rate [0.05]</strong><br class="title-page-name"/><strong class="calibre1">  -lrUpdateRate change the rate of updates for the learning rate [100]</strong><br class="title-page-name"/><strong class="calibre1">  -dim size of word vectors [100]</strong><br class="title-page-name"/><strong class="calibre1">  -ws size of the context window [5]</strong><br class="title-page-name"/><strong class="calibre1">  -epoch number of epochs [5]</strong><br class="title-page-name"/><strong class="calibre1">  -neg number of negatives sampled [5]</strong><br class="title-page-name"/><strong class="calibre1">  -loss loss function {ns, hs, softmax} [ns]</strong><br class="title-page-name"/><strong class="calibre1">  -thread number of threads [12]</strong><br class="title-page-name"/><strong class="calibre1">  -pretrainedVectors pretrained word vectors for supervised learning []</strong><br class="title-page-name"/><strong class="calibre1">  -saveOutput whether output params should be saved [false]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for quantization are optional:</strong><br class="title-page-name"/><strong class="calibre1">  -cutoff number of words and ngrams to retain [0]</strong><br class="title-page-name"/><strong class="calibre1">  -retrain whether embeddings are finetuned if a cutoff is applied [false]</strong><br class="title-page-name"/><strong class="calibre1">  -qnorm whether the norm is quantized separately [false]</strong><br class="title-page-name"/><strong class="calibre1">  -qout whether the classifier is quantized [false]</strong><br class="title-page-name"/><strong class="calibre1">  -dsub size of each sub-vector [2]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The fastText cbow</h1>
                
            
            <article>
                
<pre class="calibre17"><strong class="calibre1">$ ./fasttext cbow</strong><br class="title-page-name"/><strong class="calibre1">Empty input or output path.</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are mandatory:</strong><br class="title-page-name"/><strong class="calibre1"> -input training file path</strong><br class="title-page-name"/><strong class="calibre1"> -output output file path</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments are optional:</strong><br class="title-page-name"/><strong class="calibre1"> -verbose verbosity level [2]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for the dictionary are optional:</strong><br class="title-page-name"/><strong class="calibre1"> -minCount minimal number of word occurences [5]</strong><br class="title-page-name"/><strong class="calibre1"> -minCountLabel minimal number of label occurences [0]</strong><br class="title-page-name"/><strong class="calibre1"> -wordNgrams max length of word ngram [1]</strong><br class="title-page-name"/><strong class="calibre1"> -bucket number of buckets [2000000]</strong><br class="title-page-name"/><strong class="calibre1"> -minn min length of char ngram [3]</strong><br class="title-page-name"/><strong class="calibre1"> -maxn max length of char ngram [6]</strong><br class="title-page-name"/><strong class="calibre1"> -t sampling threshold [0.0001]</strong><br class="title-page-name"/><strong class="calibre1"> -label labels prefix [__label__]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for training are optional:</strong><br class="title-page-name"/><strong class="calibre1"> -lr learning rate [0.05]</strong><br class="title-page-name"/><strong class="calibre1"> -lrUpdateRate change the rate of updates for the learning rate [100]</strong><br class="title-page-name"/><strong class="calibre1"> -dim size of word vectors [100]</strong><br class="title-page-name"/><strong class="calibre1"> -ws size of the context window [5]</strong><br class="title-page-name"/><strong class="calibre1"> -epoch number of epochs [5]</strong><br class="title-page-name"/><strong class="calibre1"> -neg number of negatives sampled [5]</strong><br class="title-page-name"/><strong class="calibre1"> -loss loss function {ns, hs, softmax} [ns]</strong><br class="title-page-name"/><strong class="calibre1"> -thread number of threads [12]</strong><br class="title-page-name"/><strong class="calibre1"> -pretrainedVectors pretrained word vectors for supervised learning []</strong><br class="title-page-name"/><strong class="calibre1"> -saveOutput whether output params should be saved [false]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">The following arguments for quantization are optional:</strong><br class="title-page-name"/><strong class="calibre1"> -cutoff number of words and ngrams to retain [0]</strong><br class="title-page-name"/><strong class="calibre1"> -retrain whether embeddings are finetuned if a cutoff is applied [false]</strong><br class="title-page-name"/><strong class="calibre1"> -qnorm whether the norm is quantized separately [false]</strong><br class="title-page-name"/><strong class="calibre1"> -qout whether the classifier is quantized [false]</strong><br class="title-page-name"/><strong class="calibre1"> -dsub size of each sub-vector [2]</strong></pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Gensim fastText parameters</h1>
                
            
            <article>
                
<p class="calibre2">Gensim supports the same hyperparameters that are supported in the native implementation of fastText. You should be able to set them as follows:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">sentences</kbd>: This can be a list of list of tokens. In general, a stream of tokens is recommended, such as <kbd class="calibre12">LineSentence</kbd> from the word2vec module, as you have seen earlier. In the Facebook fastText library this is given by the path to the file and is given by the <kbd class="calibre12">-input</kbd> parameter.</li>
<li class="calibre11"><kbd class="calibre12">sg</kbd>: Either 1 or 0. 1 means to train a skip-gram model, and 0 means to train a CBOW model. In the Facebook fastText library the equivalent is when you pass the <kbd class="calibre12">skipgram</kbd> and <kbd class="calibre12">cbow</kbd> arguments.</li>
<li class="calibre11"><kbd class="calibre12">size</kbd>: The dimensions of the word vectors and hence must be an integer. In line with the original implementation, 100 is chosen as default. This is similar to the <kbd class="calibre12">-dim</kbd> argument in the Facebook fastText implementation.</li>
<li class="calibre11"><kbd class="calibre12">window</kbd>: The window size that is considered around a word. This is the same as <kbd class="calibre12">-ws</kbd> argument in the original implementation.</li>
<li class="calibre11"><kbd class="calibre12">alpha</kbd>: This is the initial learning rate and is a float. It is the same parameter as the <kbd class="calibre12">-lr</kbd> as what you saw in <a href="part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755" class="calibre9">Chapter 2</a>, <em class="calibre21">Creating Models Using FastText Command Line</em>.</li>
<li class="calibre11"><kbd class="calibre12">min_alpha</kbd>: This is the min learning rate to which the learning rate will drop to as the training progresses.</li>
<li class="calibre11"><kbd class="calibre12">seed</kbd>: This is for reproducability. For seeding to work the number of threads will also need to be 1.</li>
<li class="calibre11"><kbd class="calibre12">min_count</kbd>: Minimum frequency of words in the documents below which the words will be discarded. Similar to the <kbd class="calibre12">-minCount</kbd> parameter in the command line.</li>
<li class="calibre11"><kbd class="calibre12">max_vocab_size</kbd>: This is to limit the RAM size. In case there are more unique words than this will prune the less frequent ones. This needs to be decided based on top of the RAM that you have. For example, if you have 2 GB memory then <kbd class="calibre12">max_vocab_size</kbd> needs to be 10M * 2 = 20 million (20 000 000).</li>
<li class="calibre11"><kbd class="calibre12">sample</kbd>: For down sampling of words. Similar to the "-t" parameter in fasttext command line.</li>
<li class="calibre11"><kbd class="calibre12">workers</kbd>: Number of threads for training, similar to the <kbd class="calibre12">-thread</kbd> parameter in fastText command.</li>
<li class="calibre11"><kbd class="calibre12">hs</kbd>: Either 0 or 1. If this is 1, then hierarchical softmax will be used as the loss function. </li>
<li class="calibre11"><kbd class="calibre12">negative</kbd>: If you want to use negative sampling as the loss function, then set <kbd class="calibre12">hs</kbd>=0 and negative to a non-zero positive number. Note that, there are only two functions that are supported for loss functions, hierarchical softmax and negative sampling. Simple softmax is not supported. This parameter, along with <kbd class="calibre12">hs</kbd> is the equivalent of the <kbd class="calibre12">-loss</kbd> parameter in the <kbd class="calibre12">fasttext</kbd> command.</li>
<li class="calibre11"><kbd class="calibre12">cbow_mean</kbd>: There is a difference from the fastText command here. In the original implementation for <kbd class="calibre12">cbow</kbd> the mean of the vectors are taken. But in this case you have the option to use the sum by passing 0 and 1 in case you want to try out with the mean.</li>
<li class="calibre11"><kbd class="calibre12">hashfxn</kbd>: Hash function for randomly initializing the weights.</li>
<li class="calibre11"><kbd class="calibre12">iter</kbd>: Number of iterations or epochs over the samples. This is the same as the <kbd class="calibre12">-epoch</kbd> parameter in the command line.</li>
<li class="calibre11"><kbd class="calibre12">trim_rule</kbd>: Function to specify if certain words should be kept in the vocabulary or trimmed away.</li>
<li class="calibre11"><kbd class="calibre12">sorted_vocab</kbd>: Accepted values are 1 or 0. If 1 then the vocabulary will be sorted before indexing.</li>
<li class="calibre11"><kbd class="calibre12">batch_words</kbd>: This is the target size of the batches that are passed. The default value is 10000. This is a bit similar to the <kbd class="calibre12">-lrUpdateRate</kbd> in the command line as the number of batches determine when the weights will be updated.</li>
<li class="calibre11"><kbd class="calibre12">min_n</kbd> and <kbd class="calibre12">max_n</kbd>: Minimum and maximum length of the character n-grams. </li>
<li class="calibre11"><kbd class="calibre12">word_ngrams</kbd>: Enriches subword information for use in the training process.</li>
<li class="calibre11">bucket: The character n-grams are hashed on to a vector of fixed size. By default bucket size of 2 million words are used.</li>
<li class="calibre11"><kbd class="calibre12">callbacks</kbd>: A list of callback functions to be executed at specific stages of the training process.</li>
</ul>


            </article>

            
        </section>
    </body></html>