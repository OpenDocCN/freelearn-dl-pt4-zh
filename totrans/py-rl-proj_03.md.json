["```py\npip install gym[atari]\n```", "```py\nimport gym\natari = gym.make('Breakout-v0')\natari.reset()\natari.render()\n```", "```py\nactions = atari.env.get_action_meanings()\n```", "```py\n[0, 1, 3, 4] or ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n```", "```py\nnum_actions = atari.env.action_space.n\n```", "```py\nobservation, reward, done, info = atari.step(a)\n```", "```py\npip install pynput\n```", "```py\nimport gym\nimport queue, threading, time\nfrom pynput.keyboard import Key, Listener\n```", "```py\ndef keyboard(queue):\n\n    def on_press(key):\n        if key == Key.esc:\n            queue.put(-1)\n        elif key == Key.space:\n            queue.put(ord(' '))\n        else:\n            key = str(key).replace(\"'\", '')\n            if key in ['w', 'a', 's', 'd']:\n                queue.put(ord(key))\n\n    def on_release(key):\n        if key == Key.esc:\n            return False\n\n    with Listener(on_press=on_press, on_release=on_release) as listener:\n        listener.join()\n```", "```py\ndef start_game(queue):\n\n    atari = gym.make('Breakout-v0')\n    key_to_act = atari.env.get_keys_to_action()\n    key_to_act = {k[0]: a for k, a in key_to_act.items() if len(k) > 0}\n    observation = atari.reset()\n\n    import numpy\n    from PIL import Image\n    img = numpy.dot(observation, [0.2126, 0.7152, 0.0722])\n    img = cv2_resize_image(img)\n    img = Image.fromarray(img)\n    img.save('save/{}.jpg'.format(0))\n\n    while True:\n        atari.render()\n        action = 0 if queue.empty() else queue.get(block=False)\n        if action == -1:\n            break\n        action = key_to_act.get(action, 0)\n        observation, reward, done, _ = atari.step(action)\n        if action != 0:\n            print(\"Action {}, reward {}\".format(action, reward))\n        if done:\n            print(\"Game finished\")\n            break\n        time.sleep(0.05)\n```", "```py\nif __name__ == \"__main__\":\n    queue = queue.Queue(maxsize=10)\n    game = threading.Thread(target=start_game, args=(queue,))\n    game.start()\n    keyboard(queue)\n```", "```py\ndef rgb_to_gray(self, im):\n    return numpy.dot(im, [0.2126, 0.7152, 0.0722])\n```", "```py\ndef cv2_resize_image(image, resized_shape=(84, 84), \n                     method='crop', crop_offset=8):\n\n    height, width = image.shape\n    resized_height, resized_width = resized_shape\n\n    if method == 'crop':\n        h = int(round(float(height) * resized_width / width))\n        resized = cv2.resize(image, \n                             (resized_width, h), \n                             interpolation=cv2.INTER_LINEAR)\n        crop_y_cutoff = h - crop_offset - resized_height\n        cropped = resized[crop_y_cutoff:crop_y_cutoff+resized_height, :]\n        return numpy.asarray(cropped, dtype=numpy.uint8)\n    elif method == 'scale':\n        return numpy.asarray(cv2.resize(image, \n                                        (resized_width, resized_height), \n                                        interpolation=cv2.INTER_LINEAR), \n                                        dtype=numpy.uint8)\n    else:\n        raise ValueError('Unrecognized image resize method.')\n```", "```py\nInitialize  to zero and set parameters ,;\nRepeat for each episode:\n   Randomly select an initial state ;\n   While the goal state hasn't been reached:\n       Select action  among all the possible actions in state  (e.g., using greedy);\n       Take action  and observe reward , next state ;\n       Update ;\n       Set the current state ;\n   End while\n```", "```py\nimport random, numpy\n\ndef Q_learning_demo():\n\n    alpha = 1.0\n    gamma = 0.8\n    epsilon = 0.2\n    num_episodes = 100\n\n    R = numpy.array([\n        [-1, 0, -1, -1, -1, -1],\n        [ 0, -1, 0, -1, 0, -1],\n        [-1, 0, -1, -50, -1, -1],\n        [-1, -1, 0, -1, -1, -1],\n        [-1, 0, -1, -1, -1, 100],\n        [-1, -1, -1, -1, -1, -1]\n        ])\n    # Initialize Q\n    Q = numpy.zeros((6, 6))\n    # Run for each episode\n    for _ in range(num_episodes):\n        # Randomly choose an initial state\n        s = numpy.random.choice(5)\n        while s != 5:\n            # Get all the possible actions\n            actions = [a for a in range(6) if R[s][a] != -1]\n            # Epsilon-greedy\n            if numpy.random.binomial(1, epsilon) == 1:\n                a = random.choice(actions)\n            else:\n                a = actions[numpy.argmax(Q[s][actions])]\n            next_state = a\n            # Update Q(s,a)\n            Q[s][a] += alpha * (R[s][a] + gamma * numpy.max(Q[next_state]) - Q[s][a])\n            # Go to the next state\n            s = next_state\n    return Q\n```", "```py\nInitialize replay memory  to capacity ;\nInitialize the Q-network  with random weights ;\nRepeat for each episode:\n    Set time step ;\n    Receive an initial screen image  and do preprocessing ;\n    While the terminal state hasn't been reached:\n        Select an action at via greedy, i.e., select a random action with probability , otherwise select ;\n        Execute action at in the emulator and observe reward  and image ;\n        Set  and store transition  into replay memory ;\n        Randomly sample a batch of transitions  from ;\n        Set  if  is a terminal state or  if  is a non-terminal state;\n        Perform a gradient descent step on ;\n    End while\n```", "```py\nInitialize replay memory  to capacity ;\nInitialize the Q-network  with random weights ;\nInitialize the target network  with weights ;\nRepeat for each episode:\n\n```", "```py\nSet time step ;\n    Receive an initial screen image  and do preprocessing ;\n    While the terminal state hasn't been reached:\n        Select an action at via greedy, i.e., select a random action with probability , otherwise select ;\n        Execute action at in the emulator and observe reward  and image ;\n        Set  and store transition  into replay memory ;\n        Randomly sample a batch of transitions  from ;\n        Set  if  is a terminal state or  if  is a non-terminal state;\n        Perform a gradient descent step on ;\n```", "```py\n\n        Set  for every  steps;\n End while\n```", "```py\nclass QNetwork:\n\n    def __init__(self, input_shape=(84, 84, 4), n_outputs=4, \n                 network_type='cnn', scope='q_network'):\n\n        self.width = input_shape[0]\n        self.height = input_shape[1]\n        self.channel = input_shape[2]\n        self.n_outputs = n_outputs\n        self.network_type = network_type\n        self.scope = scope\n\n        # Frame images\n        self.x = tf.placeholder(dtype=tf.float32, \n                                shape=(None, self.channel, \n                                       self.width, self.height))\n        # Estimates of Q-value\n        self.y = tf.placeholder(dtype=tf.float32, shape=(None,))\n        # Selected actions\n        self.a = tf.placeholder(dtype=tf.int32, shape=(None,))\n\n        with tf.variable_scope(scope):\n            self.build()\n            self.build_loss()\n```", "```py\n    def build(self):\n\n        self.net = {}\n        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))\n\n        init_b = tf.constant_initializer(0.01)\n        if self.network_type == 'cnn':\n            self.net['conv1'] = conv2d(self.net['input'], 32, \n                                       kernel=(8, 8), stride=(4, 4), \n                                       init_b=init_b, name='conv1')\n            self.net['conv2'] = conv2d(self.net['input'], 64, \n                                       kernel=(4, 4), stride=(2, 2), \n                                       init_b=init_b, name='conv2')\n            self.net['conv3'] = conv2d(self.net['input'], 64, \n                                       kernel=(3, 3), stride=(1, 1), \n                                       init_b=init_b, name='conv3')\n            self.net['feature'] = dense(self.net['conv2'], 512, \n                                        init_b=init_b, name='fc1')\n        elif self.network_type == 'cnn_nips':\n            self.net['conv1'] = conv2d(self.net['input'], 16, \n                                       kernel=(8, 8), stride=(4, 4), \n                                       init_b=init_b, name='conv1')\n            self.net['conv2'] = conv2d(self.net['conv1'], 32, \n                                       kernel=(4, 4), stride=(2, 2), \n                                       init_b=init_b, name='conv2')\n            self.net['feature'] = dense(self.net['conv2'], 256, \n                                        init_b=init_b, name='fc1')\n        elif self.network_type == 'mlp':\n            self.net['fc1'] = dense(self.net['input'], 50, \n                                    init_b=init_b), name='fc1')\n            self.net['feature'] = dense(self.net['fc1'], 50, \n                                        init_b=init_b, name='fc2')\n        else:\n            raise NotImplementedError('Unknown network type')\n\n        self.net['values'] = dense(self.net['feature'], \n                                   self.n_outputs, activation=None,\n                                   init_b=init_b, name='values')\n\n        self.net['q_value'] = tf.reduce_max(self.net['values'], \n                                            axis=1, name='q_value')\n        self.net['q_action'] = tf.argmax(self.net['values'], \n                                         axis=1, name='q_action', \n                                         output_type=tf.int32)\n\n        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                      tf.get_variable_scope().name)\n```", "```py\n    def build_loss(self):\n\n        indices = tf.transpose(tf.stack([tf.range(tf.shape(self.a)[0]), \n                                         self.a], axis=0))\n        value = tf.gather_nd(self.net['values'], indices)\n\n        self.loss = 0.5 * tf.reduce_mean(tf.square((value - self.y)))\n        self.gradient = tf.gradients(self.loss, self.vars)\n\n        tf.summary.scalar(\"loss\", self.loss, collections=['q_network'])\n        self.summary_op = tf.summary.merge_all('q_network')\n```", "```py\nclass ReplayMemory:\n\n    def __init__(self, history_len=4, capacity=1000000, \n                 batch_size=32, input_scale=255.0):\n\n        self.capacity = capacity\n        self.history_length = history_len\n        self.batch_size = batch_size\n        self.input_scale = input_scale\n\n        self.frames = deque([])\n        self.others = deque([])\n```", "```py\n    def add(self, frame, action, r, termination):\n\n        if len(self.frames) == self.capacity:\n            self.frames.popleft()\n            self.others.popleft()\n        self.frames.append(frame)\n        self.others.append((action, r, termination))\n\n    def add_nullops(self, init_frame):\n        for _ in range(self.history_length):\n            self.add(init_frame, 0, 0, 0)\n```", "```py\n    def phi(self, new_frame):\n        assert len(self.frames) > self.history_length\n        images = [new_frame] + [self.frames[-1-i] for i in range(self.history_length-1)]\n        return numpy.concatenate(images, axis=0)\n```", "```py\n    def sample(self):\n\n        while True:\n\n            index = random.randint(a=self.history_length-1, \n                                   b=len(self.frames)-2)\n            infos = [self.others[index-i] for i in range(self.history_length)]\n            # Check if termination=1 before \"index\"\n            flag = False\n            for i in range(1, self.history_length):\n                if infos[i][2] == 1:\n                    flag = True\n                    break\n            if flag:\n                continue\n\n            state = self._phi(index)\n            new_state = self._phi(index+1)\n            action, r, termination = self.others[index]\n            state = numpy.asarray(state / self.input_scale, \n                                  dtype=numpy.float32)\n            new_state = numpy.asarray(new_state / self.input_scale, \n                                      dtype=numpy.float32)\n\n            return (state, action, r, new_state, termination)\n```", "```py\n    def _phi(self, index):\n        images = [self.frames[index-i] for i in range(self.history_length)]\n        return numpy.concatenate(images, axis=0)\n```", "```py\nclass Optimizer:\n\n    def __init__(self, config, feedback_size, \n                 q_network, target_network, replay_memory):\n\n        self.feedback_size = feedback_size\n        self.q_network = q_network\n        self.target_network = target_network\n        self.replay_memory = replay_memory\n        self.summary_writer = None\n\n        self.gamma = config['gamma']\n        self.num_frames = config['num_frames']\n\n        optimizer = create_optimizer(config['optimizer'], \n                                     config['learning_rate'], \n                                     config['rho'], \n                                     config['rmsprop_epsilon'])\n\n        self.train_op = optimizer.apply_gradients(\n                 zip(self.q_network.gradient, \n                 self.q_network.vars))\n```", "```py\n    def sample_transitions(self, sess, batch_size):\n\n        w, h = self.feedback_size\n        states = numpy.zeros((batch_size, self.num_frames, w, h), \n                             dtype=numpy.float32)\n        new_states = numpy.zeros((batch_size, self.num_frames, w, h), \n                                 dtype=numpy.float32)\n        targets = numpy.zeros(batch_size, dtype=numpy.float32)\n        actions = numpy.zeros(batch_size, dtype=numpy.int32)\n        terminations = numpy.zeros(batch_size, dtype=numpy.int32)\n\n        for i in range(batch_size):\n            state, action, r, new_state, t = self.replay_memory.sample()\n            states[i] = state\n            new_states[i] = new_state\n            actions[i] = action\n            targets[i] = r\n            terminations[i] = t\n\n        targets += self.gamma * (1 - terminations) * self.target_network.get_q_value(sess, new_states)\n        return states, actions, targets\n```", "```py\n    def train_one_step(self, sess, step, batch_size):\n\n        states, actions, targets = self.sample_transitions(sess, batch_size)\n        feed_dict = self.q_network.get_feed_dict(states, actions, targets)\n\n        if self.summary_writer and step % 1000 == 0:\n            summary_str, _, = sess.run([self.q_network.summary_op, \n                                        self.train_op], \n                                       feed_dict=feed_dict)\n            self.summary_writer.add_summary(summary_str, step)\n            self.summary_writer.flush()\n        else:\n            sess.run(self.train_op, feed_dict=feed_dict)\n```", "```py\nclass DQN:\n\n    def __init__(self, config, game, directory, \n                 callback=None, summary_writer=None):\n\n        self.game = game\n        self.actions = game.get_available_actions()\n        self.feedback_size = game.get_feedback_size()\n        self.callback = callback\n        self.summary_writer = summary_writer\n\n        self.config = config\n        self.batch_size = config['batch_size']\n        self.n_episode = config['num_episode']\n        self.capacity = config['capacity']\n        self.epsilon_decay = config['epsilon_decay']\n        self.epsilon_min = config['epsilon_min']\n        self.num_frames = config['num_frames']\n        self.num_nullops = config['num_nullops']\n        self.time_between_two_copies = config['time_between_two_copies']\n        self.input_scale = config['input_scale']\n        self.update_interval = config['update_interval']\n        self.directory = directory\n\n        self._init_modules()\n```", "```py\n    def train(self, sess, saver=None):\n\n        num_of_trials = -1\n        for episode in range(self.n_episode):\n            self.game.reset()\n            frame = self.game.get_current_feedback()\n            for _ in range(self.num_nullops):\n                r, new_frame, termination = self.play(action=0)\n                self.replay_memory.add(frame, 0, r, termination)\n                frame = new_frame\n\n            for _ in range(self.config['T']):\n                num_of_trials += 1\n                epsilon_greedy = self.epsilon_min + \\\n                    max(self.epsilon_decay - num_of_trials, 0) / \\\n                    self.epsilon_decay * (1 - self.epsilon_min)\n\n                if num_of_trials % self.update_interval == 0:\n                    self.optimizer.train_one_step(sess, \n                                                  num_of_trials, \n                                                  self.batch_size)\n\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, epsilon_greedy) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if num_of_trials % self.time_between_two_copies == 0:\n                    self.update_target_network(sess)\n                    self.save(sess, saver)\n\n                if self.callback:\n                    self.callback()\n                if termination:\n                    score = self.game.get_total_reward()\n                    summary_str = sess.run(self.summary_op, \n                                           feed_dict={self.t_score: score})\n                    self.summary_writer.add_summary(summary_str, \n                                                    num_of_trials)\n                    self.summary_writer.flush()\n                    break\n```", "```py\n    def evaluate(self, sess):\n\n        for episode in range(self.n_episode):\n            self.game.reset()\n            frame = self.game.get_current_feedback()\n            for _ in range(self.num_nullops):\n                r, new_frame, termination = self.play(action=0)\n                self.replay_memory.add(frame, 0, r, termination)\n                frame = new_frame\n\n            for _ in range(self.config['T']):\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, self.epsilon_min) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if self.callback:\n                    self.callback()\n                    if termination:\n                        break\n```", "```py\npython train.py -g Breakout -d gpu\n```", "```py\npython train.py -g demo -d cpu\n```", "```py\ntensorboard --logdir=log/demo/train\n```"]