<html><head></head><body>
  <div id="_idContainer015">
   <h1 class="chapter-number" id="_idParaDest-15">
    <a id="_idTextAnchor014">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     1
    </span>
   </h1>
   <h1 id="_idParaDest-16">
    <a id="_idTextAnchor015">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     What is ChatGPT and What are LLMs?
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     The world has been strongly influenced by the recent advancements in AI, especially
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      large language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      LLMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     ) such
    </span>
    <a id="_idIndexMarker000">
    </a>
    <span class="koboSpan" id="kobo.8.1">
     as ChatGPT and Gemini (formerly Bard).
    </span>
    <span class="koboSpan" id="kobo.8.2">
     We’ve witnessed stories such as OpenAI reaching one million users in five days, huge tech company lay-offs, history-revising image scandals, more tech companies getting multi-trillion dollar valuations (Microsoft and NVIDIA), a call for funding of $5–7 trillion for the next stage of technology, and talks of revolutions in how
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.9.1">
      everything
     </span>
    </em>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.10.1">
      is done!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.11.1">
     Yes, these are all because of new AI technologies, especially
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.12.1">
      LLM tech.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.13.1">
     LLMs are large in multiple ways: not just large training sets and large training costs but also large impacts on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.14.1">
      the world!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.15.1">
     This book is about harnessing that power effectively, for your benefit, if you are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.16.1">
      a coder.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.17.1">
     Coding has changed, and we must all keep up or else our skills will become redundant or outdated.
    </span>
    <span class="koboSpan" id="kobo.17.2">
     In this book are tools needed by coders to quickly generate code and do it well, to comment, debug, document, and stay ethical and on the right side of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.18.1">
      the law.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.19.1">
     If you’re a programmer or coder, this is for you.
    </span>
    <span class="koboSpan" id="kobo.19.2">
     Software, especially AI/machine learning, is changing everything at ever-accelerating rates, so you’ll have to learn this stuff quickly, and then use it to create and understand
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.20.1">
      future technologies.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.21.1">
     I don’t want to delay you any longer, so let’s get into the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.22.1">
      first chapter.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.23.1">
     In this chapter, we’ll cover some basics of ChatGPT, Gemini, and other LLMs, where they come from, who develops them, and what the architectures entail.
    </span>
    <span class="koboSpan" id="kobo.23.2">
     We’ll introduce some organizations that use LLMs and their services.
    </span>
    <span class="koboSpan" id="kobo.23.3">
     We’ll also briefly touch on some mathematics that go into LLMs.
    </span>
    <span class="koboSpan" id="kobo.23.4">
     Lastly, we’ll check out some of the competition and applications of LLMs in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.24.1">
      the field.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.25.1">
     This chapter covers the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.26.1">
      following topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.27.1">
      Introduction
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.28.1">
       to LLMs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.29.1">
      Origins
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.30.1">
       of LLMs
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.31.1">
       Early LLMs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.32.1">
      Exploring
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.33.1">
       modern LLMs
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.34.1">
      How
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.35.1">
       transformers work
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.36.1">
      Applications
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.37.1">
       of LLMs
      </span>
     </span>
     <a id="_idTextAnchor016">
     </a>
    </li>
   </ul>
   <h1 id="_idParaDest-17">
    <a id="_idTextAnchor017">
    </a>
    <span class="koboSpan" id="kobo.38.1">
     Introduction to LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.39.1">
     ChatGPT is
    </span>
    <a id="_idIndexMarker001">
    </a>
    <span class="koboSpan" id="kobo.40.1">
     an LLM.
    </span>
    <span class="koboSpan" id="kobo.40.2">
     LLMs
    </span>
    <a id="_idIndexMarker002">
    </a>
    <span class="koboSpan" id="kobo.41.1">
     can be used to answer questions and generate emails, marketing materials, blogs, video scripts, code, and even books that look a lot like they’ve been written by humans.
    </span>
    <span class="koboSpan" id="kobo.41.2">
     However, you probably want to know about
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.42.1">
      the technology.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.43.1">
     Let’s start with what an
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.44.1">
      LLM is.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.45.1">
     LLMs are deep learning models, specifically, transformer
    </span>
    <a id="_idIndexMarker003">
    </a>
    <span class="koboSpan" id="kobo.46.1">
     networks or just “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.47.1">
      transformers
     </span>
    </em>
    <span class="koboSpan" id="kobo.48.1">
     .”
    </span>
    <span class="koboSpan" id="kobo.48.2">
     Transformers certainly have transformed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.49.1">
      our culture!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.50.1">
     An LLM is trained on huge amounts of text data, petabytes (thousands of terabytes) of data, and predicts the next word or words.
    </span>
    <span class="koboSpan" id="kobo.50.2">
     Due to the way LLMs operate, they are not perfect at outputting text; they can give alternative facts, facts that
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.51.1">
      are “hallucinated.”
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.52.1">
     ChatGPT is, as of the time of writing, the most popular and famous LLM, created and managed by OpenAI.
    </span>
    <span class="koboSpan" id="kobo.52.2">
     OpenAI is a charity and a capped-profit organization based in San Francisco [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.53.1">
       OpenAI_LP
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.54.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.55.1">
       OpenAIStructure
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.56.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.57.1">
     ChatGPT is now widely used for multiple purposes by a huge number of people around the world.
    </span>
    <span class="koboSpan" id="kobo.57.2">
     Of course, there’s GPT-4 and now GPT-4 Turbo, which are paid, more powerful, and do more things, as well as taking more text
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.58.1">
      in prompts.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.59.1">
     It’s called
    </span>
    <a id="_idIndexMarker004">
    </a>
    <span class="koboSpan" id="kobo.60.1">
     ChatGPT:
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.61.1">
      Chat
     </span>
    </em>
    <span class="koboSpan" id="kobo.62.1">
     because that’s what you do with it, it’s a chatbot, and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.63.1">
      GPT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.64.1">
     is the technology
    </span>
    <a id="_idIndexMarker005">
    </a>
    <span class="koboSpan" id="kobo.65.1">
     and stands for
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.66.1">
      generative pre-trained transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.67.1">
     .
    </span>
    <span class="koboSpan" id="kobo.67.2">
     We will get more into that in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.68.1">
      GPT
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.69.1">
       lineage
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.70.1">
      subsection.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.71.1">
     A transformer is a type of neural network architecture, and a transformer is the basis of the most successful LLMs today (2024).
    </span>
    <span class="koboSpan" id="kobo.71.2">
     GPT is a Generative Pre-trained Transformer.
    </span>
    <span class="koboSpan" id="kobo.71.3">
     Gemini is a
    </span>
    <a id="_idIndexMarker006">
    </a>
    <span class="koboSpan" id="kobo.72.1">
     transformer [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.73.1">
      ChatGPT
     </span>
    </em>
    <span class="koboSpan" id="kobo.74.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.75.1">
      Gemini
     </span>
    </em>
    <span class="koboSpan" id="kobo.76.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.77.1">
      Menon
     </span>
    </em>
    <span class="koboSpan" id="kobo.78.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.79.1">
      HuggingFace
     </span>
    </em>
    <span class="koboSpan" id="kobo.80.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.80.2">
     OpenAI’s GPT-4 is a remarkable advancement in the field of AI.
    </span>
    <span class="koboSpan" id="kobo.80.3">
     This model, which is the fourth iteration of the GPT series, has introduced a new feature: the ability to generate images alongside text.
    </span>
    <span class="koboSpan" id="kobo.80.4">
     This is a significant leap from its predecessors, which were primarily
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.81.1">
      text-based models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.82.1">
     OpenAI also
    </span>
    <a id="_idIndexMarker007">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     has an image generation AI, DALL-E, and an AI that can connect images and text and does image recognition, called CLIP (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.84.1">
      OpenAI_CLIP
     </span>
    </em>
    <span class="koboSpan" id="kobo.85.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.85.2">
     The image generation
    </span>
    <a id="_idIndexMarker008">
    </a>
    <span class="koboSpan" id="kobo.86.1">
     capability of DALL-E is achieved by training the transformer model on image data.
    </span>
    <span class="koboSpan" id="kobo.86.2">
     This means that the model has been exposed to a vast array of images during its training phase, enabling it to understand and generate visual
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.87.1">
      content [
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.88.1">
       OpenAI_DALL.E
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.89.1">
      ]
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.90.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.91.1">
     Furthermore, since images can be sequenced to form videos, DALL.E can also be considered a video generator.
    </span>
    <span class="koboSpan" id="kobo.91.2">
     This opens up a plethora of possibilities for content creation, ranging from static images to dynamic videos.
    </span>
    <span class="koboSpan" id="kobo.91.3">
     It’s a testament to the versatility and power of transformer models, and a glimpse into the future of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.92.1">
      AI capabilities.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.93.1">
     In essence, tools from OpenAI are not just text generators but a comprehensive suite of content generators, capable of producing a diverse range of outputs.
    </span>
    <span class="koboSpan" id="kobo.93.2">
     It’s called being
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.94.1">
      multi-modal
     </span>
    </strong>
    <span class="koboSpan" id="kobo.95.1">
     .
    </span>
    <span class="koboSpan" id="kobo.95.2">
     This
    </span>
    <a id="_idIndexMarker009">
    </a>
    <span class="koboSpan" id="kobo.96.1">
     makes these tools invaluable in numerous applications, from content creation and graphic design to research and development.
    </span>
    <span class="koboSpan" id="kobo.96.2">
     The evolution from GPT-3 to GPT-4 signifies a major milestone in AI development, pushing the boundaries of what AI models
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.97.1">
      can
     </span>
     <a id="_idTextAnchor018">
     </a>
     <span class="koboSpan" id="kobo.98.1">
      achieve.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-18">
    <a id="_idTextAnchor019">
    </a>
    <span class="koboSpan" id="kobo.99.1">
     Origins of LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.100.1">
     Earlier neural
    </span>
    <a id="_idIndexMarker010">
    </a>
    <span class="koboSpan" id="kobo.101.1">
     networks with their ability to read sentences and predict the next word could only read one word at a time and were called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.102.1">
      recurrent neural networks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.103.1">
     , (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.104.1">
      RNNs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.105.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.105.2">
     RNNs
    </span>
    <a id="_idIndexMarker011">
    </a>
    <span class="koboSpan" id="kobo.106.1">
     attempted to mimic human-like sequential processing of words and sentences but faced challenges in handling long-term dependencies between words and sentences due to very limited
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.107.1">
      memory capacity.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.108.1">
     In 1925, the groundwork was laid by Wilhelm Lenz and Ernst Ising with their non-learning Ising model, considered an early RNN architecture [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.109.1">
       Brush
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.110.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.111.1">
       Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.112.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.113.1">
     In 1972, Shun’ichi Amari made this architecture adaptive, paving the way for learning RNNs.
    </span>
    <span class="koboSpan" id="kobo.113.2">
     This work was later popularized by John Hopfield in 1982 [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.114.1">
       Amari
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.115.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.116.1">
       Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.117.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.118.1">
     Due to this, there has been a fair amount of research to find ways to stretch this memory to include more text to get more context.
    </span>
    <span class="koboSpan" id="kobo.118.2">
     RNNs are transformers.
    </span>
    <span class="koboSpan" id="kobo.118.3">
     There are other
    </span>
    <a id="_idIndexMarker012">
    </a>
    <span class="koboSpan" id="kobo.119.1">
     transformers, including
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.120.1">
      LSTMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.121.1">
     , which are
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.122.1">
      long short-term memory
     </span>
    </strong>
    <span class="koboSpan" id="kobo.123.1">
     neural networks that are based on a more advanced version of RNNs, but we won’t go into that here [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.124.1">
      Brownlee_LLMs
     </span>
    </em>
    <span class="koboSpan" id="kobo.125.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.126.1">
      Gemini
     </span>
    </em>
    <span class="koboSpan" id="kobo.127.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.127.2">
     LSTMs were invented by Hochreiter and Schmidhuber in 1997 [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.128.1">
       Wiki_LSTM
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.129.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.130.1">
       Hochreiter1997
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.131.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.132.1">
     There is
    </span>
    <a id="_idIndexMarker013">
    </a>
    <span class="koboSpan" id="kobo.133.1">
     another network called the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.134.1">
      convolutional neural network
     </span>
    </strong>
    <span class="koboSpan" id="kobo.135.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.136.1">
      CNN
     </span>
    </strong>
    <span class="koboSpan" id="kobo.137.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.137.2">
     Without
    </span>
    <a id="_idIndexMarker014">
    </a>
    <span class="koboSpan" id="kobo.138.1">
     going into much detail, CNNs are very good at images and lead the world in image recognition and similar jobs.
    </span>
    <span class="koboSpan" id="kobo.138.2">
     CNNs (or ConvNets) were invented in 1980 by Kunihiko Fukushima and developed by Yann LeCun, but they only really became popular in the 2000s, when GPUs became available.
    </span>
    <span class="koboSpan" id="kobo.138.3">
     Chellapilla
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.139.1">
      et al
     </span>
    </em>
    <span class="koboSpan" id="kobo.140.1">
     .
    </span>
    <span class="koboSpan" id="kobo.140.2">
     tested the speeds of training CNNs on CPUs and GPUs and found the network trained on GPUs 4.1 times faster [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.141.1">
      Fukushima1980
     </span>
    </em>
    <span class="koboSpan" id="kobo.142.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.143.1">
      LeCun1989
     </span>
    </em>
    <span class="koboSpan" id="kobo.144.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.145.1">
      Chellapilla2006
     </span>
    </em>
    <span class="koboSpan" id="kobo.146.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.146.2">
     Sometimes, your inventions take time to bear fruit, but keep inventing!
    </span>
    <span class="koboSpan" id="kobo.146.3">
     CNNs use many layers or stages to do many different mathematical things to their inputs and try to look at them in different ways: different angles, with detail taken out (dropout layers), pooling nearby regions of each image, zeroing negative numbers, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.147.1">
      other tricks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.148.1">
     What was needed was a model with some form of memory to remember and also generate sentences and longer pieces
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.149.1">
      of writing.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.150.1">
     In 2017, Ashish Vaswani and others published a paper called
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.151.1">
      Attention Is All You Need
     </span>
    </em>
    <span class="koboSpan" id="kobo.152.1">
     , [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.153.1">
      Vaswani
     </span>
    </em>
    <span class="koboSpan" id="kobo.154.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.155.1">
      2017
     </span>
    </em>
    <span class="koboSpan" id="kobo.156.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.156.2">
     In this important paper, the transformer architecture was proposed based on attention mechanisms.
    </span>
    <span class="koboSpan" id="kobo.156.3">
     In other words, this model didn’t use recurrence and convolutions, such as RNNs and CNNs.
    </span>
    <span class="koboSpan" id="kobo.156.4">
     These methods have been very successful and popular AI architectures in their
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.157.1">
      own right.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.158.1">
     Compared to RNNs and CNNs, Vaswani’s Transformer performed faster training and allowed for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.159.1">
      higher parallelizability.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.160.1">
     The Transformer was the benchmark for English-to-German translation and established a new state-of-the-art single model in the WMT 2014 English-to-French translation task.
    </span>
    <span class="koboSpan" id="kobo.160.2">
     It also performed this feat after being trained for a small fraction of the training times of the next best existing models.
    </span>
    <span class="koboSpan" id="kobo.160.3">
     Indeed, Transformers were a groundbreaking advancement in natural language processing [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.161.1">
       Vaswani
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.162.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.163.1">
       2017
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.164.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.165.1">
     Now that we have covered the origins of LLMs, we will check out some of the earliest LLMs that
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.166.1">
      were created.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-19">
    <a id="_idTextAnchor020">
    </a>
    <span class="koboSpan" id="kobo.167.1">
     Early LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.168.1">
     There are
    </span>
    <a id="_idIndexMarker015">
    </a>
    <span class="koboSpan" id="kobo.169.1">
     many LLMs today and they can be put into a family tree; see
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.170.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.171.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.172.1">
     .
    </span>
    <span class="koboSpan" id="kobo.172.2">
     The figure shows the evolution from word2vec to the most advanced LLMs in 2023: GPT-4 and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.173.1">
      Gemini [
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.174.1">
       Bard
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.175.1">
      ].
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer007">
     <span class="koboSpan" id="kobo.176.1">
      <img alt="Figure 1.1: Family tree of LLMs from word2vec to GPT-4 and Bard, from Yang2023 with permission" src="image/B21009_01_1.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.177.1">
     Figure 1.1: Family tree of LLMs from word2vec to GPT-4 and Bard, from Yang2023 with permission
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.178.1">
     So, that’s
    </span>
    <a id="_idIndexMarker016">
    </a>
    <span class="koboSpan" id="kobo.179.1">
     all of them but, for now, we’ll look at the earlier LLMs that lead to the most advanced technologies today.
    </span>
    <span class="koboSpan" id="kobo.179.2">
     We’ll
    </span>
    <a id="_idTextAnchor021">
    </a>
    <span class="koboSpan" id="kobo.180.1">
     start
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.181.1">
      with GPT.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-20">
    <a id="_idTextAnchor022">
    </a>
    <span class="koboSpan" id="kobo.182.1">
     GPT lineage
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.183.1">
     The
    </span>
    <a id="_idIndexMarker017">
    </a>
    <span class="koboSpan" id="kobo.184.1">
     development of GPT is a constantly changing and iterative process, with each new model building upon the strengths and weaknesses of its ancestors.
    </span>
    <span class="koboSpan" id="kobo.184.2">
     The GPT series, initiated by OpenAI, has undergone a great deal of evolution, leading to
    </span>
    <a id="_idIndexMarker018">
    </a>
    <span class="koboSpan" id="kobo.185.1">
     advancements in
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.186.1">
      natural language processing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.187.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.188.1">
      NLP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.189.1">
     )
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.190.1">
      and understanding.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.191.1">
     GPT-3, the
    </span>
    <a id="_idIndexMarker019">
    </a>
    <span class="koboSpan" id="kobo.192.1">
     third iteration, brought a significant leap in terms of size and complexity, with an impressive 175 billion parameters.
    </span>
    <span class="koboSpan" id="kobo.192.2">
     This allowed it to generate pretty human-like text across a wide range of topics and subjects [
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.193.1">
       Wiki_GPT3
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.194.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.195.1">
       ProjectPro
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.196.1">
      ].
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.197.1">
     As the GPT series progressed, OpenAI continued to refine and enhance the architecture.
    </span>
    <span class="koboSpan" id="kobo.197.2">
     In subsequent iterations, GPT-4 and GPT-4 Turbo
    </span>
    <a id="_idIndexMarker020">
    </a>
    <span class="koboSpan" id="kobo.198.1">
     have further pushed back the boundaries
    </span>
    <a id="_idIndexMarker021">
    </a>
    <span class="koboSpan" id="kobo.199.1">
     of what these LLMs can achieve.
    </span>
    <span class="koboSpan" id="kobo.199.2">
     The iterative development process focuses on increasing model size and improving fine-tuning capabilities, enabling more nuanced and contextually
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.200.1">
      relevant outputs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.201.1">
     Further to this, there are more modalities, such as GPT-4 with vision
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.202.1">
      and text-to-speech.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.203.1">
     GPT model iteration is not solely about scaling up the number of parameters; it also involves addressing the limitations observed in earlier versions.
    </span>
    <span class="koboSpan" id="kobo.203.2">
     Feedback from user interactions, research findings, and technological advancements contribute to the iterative nature of the GPT series.
    </span>
    <span class="koboSpan" id="kobo.203.3">
     OpenAI is constantly working to reduce the amount of inaccurate information and incoherent outputs (hallucinations) that its chatbots produce.
    </span>
    <span class="koboSpan" id="kobo.203.4">
     Also, each iteration of the chatbot takes on board the lessons learned from real-world applications and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.204.1">
      user feedback.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.205.1">
     GPT models are trained and fine-tuned on very large, diverse datasets to make sure the chatbots can adapt to many different contexts, industries, and user requirements.
    </span>
    <span class="koboSpan" id="kobo.205.2">
     The iterative development approach ensures that later GPT models are better equipped to understand and generate human-like text, making them extremely valuable tools for a huge number of applications, including content creation such as blogs, scripts for videos, and copywriting (writing the text in adverts) as well as conversational agents (chatbots and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.206.1">
      AI assistants).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     The way GPT models are developed iteratively shows OpenAI’s commitment to continuous improvement and innovation in the field of LLMs, allowing even more sophisticated
    </span>
    <a id="_idIndexMarker022">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     and capable models to be built from these models in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.209.1">
      the future.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.210.1">
     Here are the dates for when the different versions of GPT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.211.1">
      were launched:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.212.1">
      GPT was first launched in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.213.1">
       June 2018
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.214.1">
      GPT-2 was released in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.215.1">
       February 2019
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.216.1">
      GPT-3
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.217.1">
       in 2020
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.218.1">
      GPT-3.5 in 2022/ChatGPT in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.219.1">
       November 2022
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.220.1">
     There will be more on the GPT family later, in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.221.1">
      GPT-4 /GPT-4
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.222.1">
       Turbo
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.223.1">
      section.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.224.1">
     Here, we will detail the architecture of LLMs
    </span>
    <a id="_idTextAnchor023">
    </a>
    <span class="koboSpan" id="kobo.225.1">
     and how
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.226.1">
      they operate.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-21">
    <a id="_idTextAnchor024">
    </a>
    <span class="koboSpan" id="kobo.227.1">
     BERT
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.228.1">
     To
    </span>
    <a id="_idIndexMarker023">
    </a>
    <span class="koboSpan" id="kobo.229.1">
     comprehend the roots and development of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.230.1">
      Bidirectional Encoder Representations from Transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.231.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.232.1">
      BERT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.233.1">
     ), we must know more about the intricate and fast-moving landscape of neural networks.
    </span>
    <span class="koboSpan" id="kobo.233.2">
     Without
    </span>
    <a id="_idIndexMarker024">
    </a>
    <span class="koboSpan" id="kobo.234.1">
     hyperbole, BERT was a seriously important innovation in NLP, part of the ongoing evolution of AI.
    </span>
    <span class="koboSpan" id="kobo.234.2">
     BERT was the state of the art for a wide range of NLP tasks in October 2018, when it was released [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.235.1">
      Gemini
     </span>
    </em>
    <span class="koboSpan" id="kobo.236.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.236.2">
     This included question answering, sentiment analysis, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.237.1">
      text summarization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.238.1">
     BERT also paved the way for later R&amp;D of LLMs; it played a pivotal role in LLM development.
    </span>
    <span class="koboSpan" id="kobo.238.2">
     BERT, being open source, helped to speed up
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.239.1">
      LLM advancement.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.240.1">
     BERT takes some of its DNA from RNNs (mentioned in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.241.1">
      Origins of LLMs
     </span>
    </em>
    <span class="koboSpan" id="kobo.242.1">
     section), the neural nets that loop back on themselves to create a kind of memory, although rather
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.243.1">
      limited memory.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.244.1">
     The invention of the first transformer architecture was key to the origin of BERT.
    </span>
    <span class="koboSpan" id="kobo.244.2">
     The creation of BERT as a bidirectional encoder (these go backward and forward along a sentence) drew inspiration from the transformer’s attention-based mechanism, allowing it to capture contextual relationships between words in both directions within
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.245.1">
      a sentence.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.246.1">
     So, BERT’s attention is bidirectional (left-to-right and right-to-left context).
    </span>
    <span class="koboSpan" id="kobo.246.2">
     At its creation, this was unique, and it enabled BERT to gain a more comprehensive understanding of nuanced
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.247.1">
      language semantics.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.248.1">
     While
    </span>
    <a id="_idIndexMarker025">
    </a>
    <span class="koboSpan" id="kobo.249.1">
     BERT’s foundations are in transformer architecture, its
    </span>
    <a id="_idIndexMarker026">
    </a>
    <span class="koboSpan" id="kobo.250.1">
     characteristics have evolved with further research and development, though it is not currently in development.
    </span>
    <span class="koboSpan" id="kobo.250.2">
     Each iteration of BERT refined and expanded
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.251.1">
      its capabilities.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.252.1">
     The
    </span>
    <a id="_idIndexMarker027">
    </a>
    <span class="koboSpan" id="kobo.253.1">
     BERT LLM was a stage of the ongoing innovation in AI.
    </span>
    <span class="koboSpan" id="kobo.253.2">
     BERT’s ability to understand language bidirectionally, drawing insights from both preceding and succeeding words, is part of the endeavors taken to achieve the creation of an AI with a sufficiently deep awareness of the intricacies of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.254.1">
      natural language.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer008">
     <span class="koboSpan" id="kobo.255.1">
      <img alt="Figure 1.2: Architecture of BERT, a bidirectional encoder (reprodu﻿ced from GeekCultureBERT)" src="image/B21009_01_2.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.256.1">
     Figure 1.2: Architecture of BERT, a bidirectional encoder (reprodu
    </span>
    <a id="_idTextAnchor025">
    </a>
    <span class="koboSpan" id="kobo.257.1">
     ced from GeekCultureBERT)
    </span>
   </p>
   <h2 id="_idParaDest-22">
    <a id="_idTextAnchor026">
    </a>
    <span class="koboSpan" id="kobo.258.1">
     LaMDA
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.259.1">
     Understanding the ancestry of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.260.1">
      Language Model for Dialogue Applications
     </span>
    </strong>
    <span class="koboSpan" id="kobo.261.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.262.1">
      LaMDA
     </span>
    </strong>
    <span class="koboSpan" id="kobo.263.1">
     ) involves
    </span>
    <a id="_idIndexMarker028">
    </a>
    <span class="koboSpan" id="kobo.264.1">
     tracing the roots of its architectural design and the evolutionary path it followed in the landscape of NLP.
    </span>
    <span class="koboSpan" id="kobo.264.2">
     LaMDA, like
    </span>
    <a id="_idIndexMarker029">
    </a>
    <span class="koboSpan" id="kobo.265.1">
     its counterparts, emerges from a family of models that have collectively revolutionized how machines comprehend and generate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.266.1">
      human-like text.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.267.1">
     RNNs, mentioned in this chapter’s first section, play a pivotal role in LaMDA’s
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.268.1">
      family tree.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.269.1">
     The breakthrough came with the invention of transformer architectures, and LaMDA owes a significant debt to the transformative
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.270.1">
      Attention Is All You Need
     </span>
    </em>
    <span class="koboSpan" id="kobo.271.1">
     paper [
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.272.1">
      Vaswani
     </span>
    </em>
    <em class="italic">
     <span class="koboSpan" id="kobo.273.1">
      2017
     </span>
    </em>
    <span class="koboSpan" id="kobo.274.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.275.1">
      2023
     </span>
    </em>
    <span class="koboSpan" id="kobo.276.1">
     ].
    </span>
    <span class="koboSpan" id="kobo.276.2">
     This paper laid the groundwork for a novel approach, moving away from sequential processing to a more parallelized and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.277.1">
      attention-based mechanism.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.278.1">
     The LaMDA LLM
    </span>
    <a id="_idIndexMarker030">
    </a>
    <span class="koboSpan" id="kobo.279.1">
     inherits its core architecture from the transformer family and was developed by Google.
    </span>
    <span class="koboSpan" id="kobo.279.2">
     These models learn very well how words in a sentence relate to each other.
    </span>
    <span class="koboSpan" id="kobo.279.3">
     This allows a transformer to have a richer understanding of language.
    </span>
    <span class="koboSpan" id="kobo.279.4">
     This change from using traditional processing in sequence was a paradigm shift in NLP, enabling LaMDA to more effectively grasp nuanced interactions and dependencies
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.280.1">
      within texts.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.281.1">
     While the origins lie in the transformer architecture, LaMDA’s unique characteristics may have been fine-tuned and evolved through subsequent research and development efforts.
    </span>
    <span class="koboSpan" id="kobo.281.2">
     LaMDA’s lineage is not just a linear progression but a family tree, a branching exploration of many possibilities, with each iteration refining and expanding its capabilities.
    </span>
    <span class="koboSpan" id="kobo.281.3">
     In
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.282.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.283.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.284.1">
     , LaMDA is near ERNIE 3.0, Gopher, and PaLM on the right of the main, vertical
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.285.1">
      blue branch.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.286.1">
     Simply put, LaMDA is a product of ongoing innovation and refinement in the field of AI, standing on the shoulders of earlier models and research breakthroughs.
    </span>
    <span class="koboSpan" id="kobo.286.2">
     Its ability to comprehend and generate language is deeply rooted in an evolutionary process of learning from vast amounts of text data, mimicking the way humans process and understand language on a grand,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.287.1">
      digital scale.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.288.1">
     LaM
    </span>
    <a id="_idTextAnchor027">
    </a>
    <span class="koboSpan" id="kobo.289.1">
     DA was launched in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.290.1">
      May 2021.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-23">
    <a id="_idTextAnchor028">
    </a>
    <span class="koboSpan" id="kobo.291.1">
     LLaMA‘s family tree
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.292.1">
     LLaMA is
    </span>
    <a id="_idIndexMarker031">
    </a>
    <span class="koboSpan" id="kobo.293.1">
     the AI brainchild of Meta AI.
    </span>
    <span class="koboSpan" id="kobo.293.2">
     It might not be one you’ve heard the most about but its lineage holds stories of innovation and evolution, tracing a fascinating path through the history of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.294.1">
      AI communication.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.295.1">
     Like the other chatbot LLMs, LLaMA’s roots are also in transformer architectures.
    </span>
    <span class="koboSpan" id="kobo.295.2">
     These models rely on intricate attention mechanisms, allowing them to analyze relationships between words, not just
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.296.1">
      their sequence.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.297.1">
     Trained on massive datasets of text and code, LLaMA learned to generate basic responses, translate languages, and even write different kinds of creative
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.298.1">
      text formats.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.299.1">
     However, like a newborn foal, their capabilities were limited.
    </span>
    <span class="koboSpan" id="kobo.299.2">
     They stumbled with complex contexts, lacked common sense reasoning, and sometimes sputtered out
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.300.1">
      nonsensical strings.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.301.1">
     Yet their potential was undeniable.
    </span>
    <span class="koboSpan" id="kobo.301.2">
     The ability to learn and adapt from data made them valuable tools for researchers.
    </span>
    <span class="koboSpan" id="kobo.301.3">
     Meta AI nurtured these nascent models, carefully tweaking their architecture and feeding them richer datasets.
    </span>
    <span class="koboSpan" id="kobo.301.4">
     They delved deeper into the understanding of human language, acquiring skills such as factual grounding, reasoning, and the ability to engage in multi-turn
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.302.1">
      conversations (Wiki_llama).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.303.1">
     The Llama family tree is not a linear progression but, rather, a family of multiple branches of exploration.
    </span>
    <span class="koboSpan" id="kobo.303.2">
     Different versions explored specific avenues: Code Llama focused on code generation, while Megatron-Turing NLG 530 B was trained on filling in missing words, reading comprehension, and common-sense reasoning, among other things (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.304.1">
      CodeLlama 2023
     </span>
    </em>
    <span class="koboSpan" id="kobo.305.1">
     ,
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.306.1">
       Megatron-Turing 2022
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.307.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.308.1">
     For an idea of how LLaMA fits into the evolutionary tree, see
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.309.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.310.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.311.1">
     at the top left of the vertical blue branch, near
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.312.1">
      Bard (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.313.1">
       Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.314.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.315.1">
     Each experiment, each successful leap forward, contributed valuable DNA to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.316.1">
      future generations.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.317.1">
     Why the name
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.318.1">
      Megatron-Turing NLG 530 B
     </span>
    </em>
    <span class="koboSpan" id="kobo.319.1">
     ?
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.320.1">
      Megatron
     </span>
    </em>
    <span class="koboSpan" id="kobo.321.1">
     because it represents a powerful hardware and software framework.
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.322.1">
      Turing
     </span>
    </em>
    <span class="koboSpan" id="kobo.323.1">
     to honor Alan Turing, the first AI researcher, and the
    </span>
    <a id="_idIndexMarker032">
    </a>
    <span class="koboSpan" id="kobo.324.1">
     originator of AI and ML.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.325.1">
      NLG
     </span>
    </strong>
    <span class="koboSpan" id="kobo.326.1">
     stands for
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.327.1">
      natural language generation
     </span>
    </strong>
    <span class="koboSpan" id="kobo.328.1">
     , and it has 530
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.329.1">
      billion parameters.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.330.1">
     Meta AI continues to shepherd the Llama family, and the future promises more
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.331.1">
      exciting developments.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.332.1">
     Llama LLM was
    </span>
    <a id="_idIndexMarker033">
    </a>
    <span class="koboSpan" id="kobo.333.1">
     launched in February 2023, while Megatron-Turing NLG 530 B was released in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.334.1">
      January 2022.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.335.1">
     Now that we have covered the origins and explored the early stages of LLMs, let us fast-forward and talk about modern LLMs in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.336.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-24">
    <a id="_idTextAnchor029">
    </a>
    <span class="koboSpan" id="kobo.337.1">
     Exploring modern LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.338.1">
     After the
    </span>
    <a id="_idIndexMarker034">
    </a>
    <span class="koboSpan" id="kobo.339.1">
     explosive take-off of ChatGPT in late 2022, with 1 million active users in 5 days and 100 million active users in January 2023 (about 2 months), 2023 was a pretty hot year for LLMs, AI research, and the use of AI
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.340.1">
      in general.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.341.1">
     Most tech companies have worked on their own LLMs or transformer models to use and make publicly available.
    </span>
    <span class="koboSpan" id="kobo.341.2">
     Many companies, organizations, and individuals (students included) have used
    </span>
    <a id="_idIndexMarker035">
    </a>
    <span class="koboSpan" id="kobo.342.1">
     LLMs for a multitude of tasks.
    </span>
    <span class="koboSpan" id="kobo.342.2">
     OpenAI keeps updating its GPT family and Google keeps updating its Bard version.
    </span>
    <span class="koboSpan" id="kobo.342.3">
     Bard became Gemini in February 2024, so all references to Bard have changed to Gemini.
    </span>
    <span class="koboSpan" id="kobo.342.4">
     Many companies use ChatGPT or GPT-4 as the core of their offering, just creating a wrapper and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.343.1">
      selling it.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.344.1">
     This might change as OpenAI keeps adding modalities (speech, image, etc.) to the GPTs and even a new marketplace platform where users can create and sell their own GPT agents right on OpenAI servers.
    </span>
    <span class="koboSpan" id="kobo.344.2">
     This was launched in early January 2024 to paid users ($20/month before VAT).
    </span>
    <span class="koboSpan" id="kobo.344.3">
     We’ll cover some of the latest LLMs that companies have wor
    </span>
    <a id="_idTextAnchor030">
    </a>
    <span class="koboSpan" id="kobo.345.1">
     ked on in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.346.1">
      following sections.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-25">
    <a id="_idTextAnchor031">
    </a>
    <span class="koboSpan" id="kobo.347.1">
     GPT-4
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.348.1">
     GPT-4 Turbo, OpenAI’s
    </span>
    <a id="_idIndexMarker036">
    </a>
    <span class="koboSpan" id="kobo.349.1">
     latest
    </span>
    <a id="_idIndexMarker037">
    </a>
    <span class="koboSpan" id="kobo.350.1">
     hot chatbot, is another big upgrade.
    </span>
    <span class="koboSpan" id="kobo.350.2">
     It’s the GPT-4 you know, but on steroids, with 10 times more memory and a newfound understanding
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.351.1">
      of images.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.352.1">
     If GPT-4
    </span>
    <a id="_idIndexMarker038">
    </a>
    <span class="koboSpan" id="kobo.353.1">
     was a gifted writer, GPT-4 Turbo is a multimedia polymath.
    </span>
    <span class="koboSpan" id="kobo.353.2">
     It can not only spin captivating stories and poems but also decipher images, paint vivid digital landscapes, and even caption photos with witty remarks.
    </span>
    <span class="koboSpan" id="kobo.353.3">
     Forget outdated information – Turbo’s knowledge base refreshes constantly, keeping it as sharp as a tack on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.354.1">
      current events.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.355.1">
     But it’s not
    </span>
    <a id="_idIndexMarker039">
    </a>
    <span class="koboSpan" id="kobo.356.1">
     just about flashy tricks.
    </span>
    <span class="koboSpan" id="kobo.356.2">
     Turbo is a stickler for facts.
    </span>
    <span class="koboSpan" id="kobo.356.3">
     It taps into external knowledge bases and employs sophisticated reasoning, ensuring its responses are accurate and reliable.
    </span>
    <span class="koboSpan" id="kobo.356.4">
     Gone are the days of biased or misleading outputs – Turbo strives for truth and clarity, making it a trustworthy companion for learning
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.357.1">
      and exploration.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.358.1">
     The best part?
    </span>
    <span class="koboSpan" id="kobo.358.2">
     OpenAI isn’t keeping this powerhouse locked away.
    </span>
    <span class="koboSpan" id="kobo.358.3">
     They’ve crafted an API and developer tools, inviting programmers and innovators to customize Turbo for specific tasks and domains.
    </span>
    <span class="koboSpan" id="kobo.358.4">
     This democratization of advanced language processing opens doors to a future where everyone, from artists to scientists, can harness the power of language models to create, analyze, and understand the world
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.359.1">
      around them.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.360.1">
     GPT-4 Turbo is
    </span>
    <a id="_idIndexMarker040">
    </a>
    <span class="koboSpan" id="kobo.361.1">
     probably widely considered the pinnacle of technology at the moment, showing us the breathtaking potential of LLMs.
    </span>
    <span class="koboSpan" id="kobo.361.2">
     It’s not just a language model; it’s a glimpse into a future where machines understand and interact with us like never before.
    </span>
    <span class="koboSpan" id="kobo.361.3">
     So, buckle up!
    </span>
    <span class="koboSpan" id="kobo.361.4">
     The future of language is here, and it’s powered by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.362.1">
      GPT-4 Turbo.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.363.1">
     GPT-4 was launched in March 2023 and GPT-4 Turbo in November 2023 (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.364.1">
      Wiki_GPT4
     </span>
    </em>
    <span class="koboSpan" id="kobo.365.1">
     ,
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.366.1">
       OpenAI_GPT4Turbo
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.367.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.368.1">
       Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.369.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.370.1">
     GPT-4o or GPT-4 omni was released in May 2024, and it can understand multiple formats of data.
    </span>
    <span class="koboSpan" id="kobo.370.2">
     Omni is
    </span>
    <a id="_idIndexMarker041">
    </a>
    <span class="koboSpan" id="kobo.371.1">
     faster than previous models and can respond to speech in 0.32 seconds on average, similar to human response times, while Turbo takes about 5.4 seconds to respond in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.372.1">
      Voice Mode.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.373.1">
     This is partially because, while Turbo takes in text, transcribed from the audio by a simple model, and a third model converts the text back into audio response, omni is a single model that understands audio, video, and text.
    </span>
    <span class="koboSpan" id="kobo.373.2">
     The three models for Turbo are slower than omni and a lot of information is lost to GPT-4 Turbo due
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.374.1">
      to transcription.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.375.1">
     GPT-4o is much better than GPT-4 Turbo in non-English
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.376.1">
      human languages.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.377.1">
     The Omni API is also half the cost of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.378.1">
      Turbo (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.379.1">
       OpenAI-GPT-4o
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.380.1">
      )!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.381.1">
     GPT-4o does very well on code generation versus Claude 3 Opus and Gemini 1.5 Pro.
    </span>
    <span class="koboSpan" id="kobo.381.2">
     Claude is moderate, Gemini is judged to be very
    </span>
    <a id="_idTextAnchor032">
    </a>
    <span class="koboSpan" id="kobo.382.1">
     good, and GPT-4o is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.383.1">
      excellent [
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.384.1">
       encord
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.385.1">
      ].
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.386.1">
     GPT-4 architecture
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.387.1">
     OpenAI has
    </span>
    <a id="_idIndexMarker042">
    </a>
    <span class="koboSpan" id="kobo.388.1">
     not released details of the architecture and full details of GPT-4, proprietary information for now, but we can piece together elements from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.389.1">
      similar work.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.390.1">
     GPT-4 has 1.75 trillion parameters (1.75 million
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.391.1">
      million) (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.392.1">
       MotiveX_Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.393.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.394.1">
     The vision transformer will likely involve some encoder-decoder architecture: image and video inputs for the encoder, then the decoder will generate output such as text descriptions or captions as well as
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.395.1">
      images (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.396.1">
       Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.397.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.398.1">
     It will have an attention mechanism because “attention is all
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.399.1">
      you need.”
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.400.1">
     The vision components will probably multi-head to process various aspects of the input simultaneously.
    </span>
    <span class="koboSpan" id="kobo.400.2">
     There should also be positional encoding, image pro-processing layers, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.401.1">
      modality fusion.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.402.1">
     Modality fusion is where the vision capabilities are combined with the faculties to process text.
    </span>
    <span class="koboSpan" id="kobo.402.2">
     From this, it would need to generate a unified understanding of the inputs or the scene given
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.403.1">
      to it.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.404.1">
     So, GPT-4 can understand images, and it’s believed that it uses a combination of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.405.1">
      Vision Transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.406.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.407.1">
      ViT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.408.1">
     ) and Flamingo visual
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.409.1">
      language models.
     </span>
    </span>
   </p>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.410.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.411.1">
      .3
     </span>
    </em>
    <span class="koboSpan" id="kobo.412.1">
     shows the
    </span>
    <a id="_idIndexMarker043">
    </a>
    <span class="koboSpan" id="kobo.413.1">
     architecture of ViT (reproduced
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.414.1">
      from Wagh).
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer009">
     <span class="koboSpan" id="kobo.415.1">
      <img alt="Figure 1.3: This is what the internal workings of ViT involve (reproduced from Wagh)" src="image/B21009_01_3.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.416.1">
     Figure 1.3: This is what the internal workings of ViT involve (reproduced from Wagh)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.417.1">
     So, the inner
    </span>
    <a id="_idIndexMarker044">
    </a>
    <span class="koboSpan" id="kobo.418.1">
     workings of GPT-4 that handle vision processing likely involve visual transformers as shown in the preceding figure, along with the text processors in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.419.1">
      How an LLM processes a
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.420.1">
       sentence
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.421.1">
      subsection.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.422.1">
     You can find out more about ViT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.423.1">
      here:
     </span>
    </span>
    <a href="https://github.com/lucidrains/vit-pytorch">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.424.1">
       https://github.com/lucidrains/vit-pytorch
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.425.1">
      .
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-26">
    <a id="_idTextAnchor033">
    </a>
    <span class="koboSpan" id="kobo.426.1">
     LLaMA-2
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.427.1">
     The
    </span>
    <a id="_idIndexMarker045">
    </a>
    <span class="koboSpan" id="kobo.428.1">
     latest official LLaMA, LLaMA-2, is capable of holding complicated
    </span>
    <a id="_idIndexMarker046">
    </a>
    <span class="koboSpan" id="kobo.429.1">
     conversations, generating various creative text formats, and even adapting its responses to specific
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.430.1">
      user personalities.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.431.1">
     OpenLLaMA is an open source version of LLaMA released by Open LM Research (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.432.1">
      Watson 2023
     </span>
    </em>
    <span class="koboSpan" id="kobo.433.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.434.1">
      OpenLMR
     </span>
    </em>
    <span class="koboSpan" id="kobo.435.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.436.1">
      Gemini
     </span>
    </em>
    <span class="koboSpan" id="kobo.437.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.437.2">
     OpenLLaMA has several versions, each trained on different datasets but the training process was very similar to the original LLaMA.
    </span>
    <span class="koboSpan" id="kobo.437.3">
     Model weights can be found on the HuggingFace Hub and accessed without the need for any additional permission.
    </span>
    <span class="koboSpan" id="kobo.437.4">
     The
    </span>
    <a id="_idIndexMarker047">
    </a>
    <span class="koboSpan" id="kobo.438.1">
     HuggingFace page for Open LLaMA is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.439.1">
      here:
     </span>
    </span>
    <a href="https://huggingface.co/docs/transformers/en/model_doc/open-llama">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.440.1">
       https://huggingface.co/docs/transformers/en/model_doc/open-llama
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.441.1">
      .
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.442.1">
      OpenLLaMA models serve as benchmarks for LLM research.
     </span>
     <span class="koboSpan" id="kobo.442.2">
      Their open source nature makes it possible to compare with other models.
     </span>
     <span class="koboSpan" id="kobo.442.3">
      This is made easier because there are
     </span>
     <a id="_idIndexMarker048">
     </a>
     <span class="koboSpan" id="kobo.443.1">
      PyTorch and TensorFlow
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.444.1">
       formats available.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.445.1">
      LLaMA-2 was released in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.446.1">
       April 2023.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.447.1">
      OpenLLaMA was released in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.448.1">
       June 2023.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.449.1">
      In early 2024, the rumors are that LLaMA-3 will be released
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.450.1">
       this year.
      </span>
     </span>
    </li>
   </ul>
   <h2 id="_idParaDest-27">
    <a id="_idTextAnchor034">
    </a>
    <span class="koboSpan" id="kobo.451.1">
     Gemini (formerly Bard)
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.452.1">
     Google’s Gemini is
    </span>
    <a id="_idIndexMarker049">
    </a>
    <span class="koboSpan" id="kobo.453.1">
     a chatbot LLM with access to the internet
    </span>
    <a id="_idIndexMarker050">
    </a>
    <span class="koboSpan" id="kobo.454.1">
     and just requires a Google login.
    </span>
    <span class="koboSpan" id="kobo.454.2">
     Technically, Gemini is the face and the brain is whatever Google
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.455.1">
      slots in.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.456.1">
     Previously, Gemini was powered by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.457.1">
      PaLM 2.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.458.1">
     As of writing (early February 2024), Bard was earlier powered by Gemini.
    </span>
    <span class="koboSpan" id="kobo.458.2">
     There are three versions of Gemini: Nano, Pro, and Ultra.
    </span>
    <span class="koboSpan" id="kobo.458.3">
     Nano is for mobile devices.
    </span>
    <span class="koboSpan" id="kobo.458.4">
     As Bard is powered by Gemini Pro, the name changed to Gemini.
    </span>
    <span class="koboSpan" id="kobo.458.5">
     There may soon be a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.459.1">
      paid version.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.460.1">
     Gemini was released in March
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.461.1">
      2023 (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.462.1">
       Wiki_Gemini
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.463.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.464.1">
     Gemini has 142.4 million u
    </span>
    <a id="_idTextAnchor035">
    </a>
    <span class="koboSpan" id="kobo.465.1">
     sers, 62.6% of which are in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.466.1">
      USA (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.467.1">
       AnswerIQ
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.468.1">
      ).
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.469.1">
     The architecture of Gemini
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.470.1">
     Gemini is
    </span>
    <a id="_idIndexMarker051">
    </a>
    <span class="koboSpan" id="kobo.471.1">
     one of the LLMs and AIs developed and used by Google/Alphabet.
    </span>
    <span class="koboSpan" id="kobo.471.2">
     Let’s take a peek under the hood to understand what makes
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.472.1">
      Gemini tick!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.473.1">
     Gemini is trained on a vast library of the world’s books, articles, and internet chatter.
    </span>
    <span class="koboSpan" id="kobo.473.2">
     1.56 trillion words are in the Infiniset dataset of Google Gemini; that’s 750 GB of data.
    </span>
    <span class="koboSpan" id="kobo.473.3">
     Gemini has 137 billion parameters, which are the neural network weights (ChatGPT has 175 billion
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.474.1">
      parameters/weights) (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.475.1">
       ProjectPro
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.476.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.477.1">
     In November 2023, Bard got an upgrade and started to be powered by Gemini, a new AI system (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.478.1">
      SkillLeapAI
     </span>
    </em>
    <span class="koboSpan" id="kobo.479.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.479.2">
     Previously, Gemini was powered by LaMDA from March 2023, then PaLM 2 from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.480.1">
      May 2023.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.481.1">
     There are
    </span>
    <a id="_idIndexMarker052">
    </a>
    <span class="koboSpan" id="kobo.482.1">
     three
    </span>
    <a id="_idIndexMarker053">
    </a>
    <span class="koboSpan" id="kobo.483.1">
     models, Gemini Nano, Gemini Pro, and
    </span>
    <a id="_idIndexMarker054">
    </a>
    <span class="koboSpan" id="kobo.484.1">
     Gemini Ultra.
    </span>
    <span class="koboSpan" id="kobo.484.2">
     As of 19th January 2024, Gemini is powered by Gemini Ultra, which was launched in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.485.1">
      December 2023.
     </span>
    </span>
   </p>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.486.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.487.1">
      .4
     </span>
    </em>
    <span class="koboSpan" id="kobo.488.1">
     shows the
    </span>
    <a id="_idIndexMarker055">
    </a>
    <span class="koboSpan" id="kobo.489.1">
     architecture of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.490.1">
      Gemini (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.491.1">
       GeminiTeam
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.492.1">
      ).
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer010">
     <span class="koboSpan" id="kobo.493.1">
      <img alt="Figure 1.4: Bard/Gemini architecture, from the DeepMind GeminiTeam (GeminiTeam)" src="image/B21009_01_4.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.494.1">
     Figure 1.4: Bard/Gemini architecture, from the DeepMind GeminiTeam (GeminiTeam)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.495.1">
     Gemini can deal with combinations of text, images, audio, and video inputs, which are represented as different colors here.
    </span>
    <span class="koboSpan" id="kobo.495.2">
     Outputs can be text and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.496.1">
      images combined.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.497.1">
     The transition to Gemini Ultra signifies a significant leap in Gemini’s capabilities, offering higher performance, greater efficiency, and a wider range of potential applications (Gemini).
    </span>
    <span class="koboSpan" id="kobo.497.2">
     Bard/Gemini Ultra has a complex architecture that is like a sophisticated language processing factory, with each component playing a crucial role in understanding your questions and crafting the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.498.1">
      perfect response.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.499.1">
     The key component is the transformer decoder, the brain of the operation.
    </span>
    <span class="koboSpan" id="kobo.499.2">
     It analyzes the incoming text, dissecting each word’s meaning and its connection to others.
    </span>
    <span class="koboSpan" id="kobo.499.3">
     It’s like a skilled translator, deciphering the message you send and preparing to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.500.1">
      respond fluently.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.501.1">
     The
    </span>
    <a id="_idIndexMarker056">
    </a>
    <span class="koboSpan" id="kobo.502.1">
     Gemini Ultra multimodal encoder can handle more than just text.
    </span>
    <span class="koboSpan" id="kobo.502.2">
     Images, audio, and other data types can be processed, providing a richer context for the decoder.
    </span>
    <span class="koboSpan" id="kobo.502.3">
     This allows Gemini to interpret complex situations, such as describing an image you send or composing music based on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.503.1">
      your mood.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.504.1">
     To polish the
    </span>
    <a id="_idIndexMarker057">
    </a>
    <span class="koboSpan" id="kobo.505.1">
     decoder’s output, pre-activation and post-activation transformers come into play.
    </span>
    <span class="koboSpan" id="kobo.505.2">
     These additional layers refine and smoothen the response, ensuring it’s clear, grammatically correct, and reads like natural, human language.
    </span>
    <span class="koboSpan" id="kobo.505.3">
     With less hallucination, the factual grounding module anchors its responses in the real world.
    </span>
    <span class="koboSpan" id="kobo.505.4">
     Just like a reliable teacher, it ensures Gemini’s information is accurate and unbiased, grounding its creativity in a strong foundation of truth.
    </span>
    <span class="koboSpan" id="kobo.505.5">
     Beyond basic understanding, Gemini Ultra also
    </span>
    <a id="_idIndexMarker058">
    </a>
    <span class="koboSpan" id="kobo.506.1">
     has reasoning abilities.
    </span>
    <span class="koboSpan" id="kobo.506.2">
     It can answer complex questions, draw logical conclusions, and even
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.507.1">
      solve problems.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.508.1">
     The implementation that is Gemini also has a little link to Google to help users to fact-check its responses.
    </span>
    <span class="koboSpan" id="kobo.508.2">
     At the bottom of the output, above the input window, Google enables you to double-check
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.509.1">
      its response.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer011">
     <span class="koboSpan" id="kobo.510.1">
      <img alt="Figure 1.5: Gemini’s Google search button to fact-check the output it gives you" src="image/B21009_01_5.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.511.1">
     Figure 1.5: Gemini’s Google search button to fact-check the output it gives you
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.512.1">
     Click this and it says
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.513.1">
      Google search
     </span>
    </strong>
    <span class="koboSpan" id="kobo.514.1">
     and outputs some search results and a guide to what
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.515.1">
      you’re seeing.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer012">
     <span class="koboSpan" id="kobo.516.1">
      <img alt="Figure 1.6: Google search based on its output" src="image/B21009_01_6.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.517.1">
     Figure 1.6: Google search based on its output
    </span>
   </p>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.518.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.519.1">
      .7
     </span>
    </em>
    <span class="koboSpan" id="kobo.520.1">
     shows what the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.521.1">
      highlighting means.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer013">
     <span class="koboSpan" id="kobo.522.1">
      <img alt="Figure 1.7: Understanding the results of the Google search to help fact-check" src="image/B21009_01_7.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.523.1">
     Figure 1.7: Understanding the results of the Google search to help fact-check
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.524.1">
     On your
    </span>
    <a id="_idIndexMarker059">
    </a>
    <span class="koboSpan" id="kobo.525.1">
     Gemini screen, you’ll see various passages highlighted in brown or green.
    </span>
    <span class="koboSpan" id="kobo.525.2">
     The green-highlighted text has results agreeing, the brown-highlighted text doesn’t agree with the sources, and no highlight means not enough information
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.526.1">
      to confirm.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.527.1">
     This is just a simplified glimpse into Gemini Ultra’s architecture and functioning.
    </span>
    <span class="koboSpan" id="kobo.527.2">
     With its massive parameter count, self-attention mechanisms, and fine-tuning capabilities, it’s a constantly evolving language maestro
    </span>
    <a id="_idTextAnchor036">
    </a>
    <span class="koboSpan" id="kobo.528.1">
     , pushing the boundaries of what LLMs
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.529.1">
      can achieve.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-28">
    <a id="_idTextAnchor037">
    </a>
    <span class="koboSpan" id="kobo.530.1">
     Amazon Olympus
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.531.1">
     Amazon
    </span>
    <a id="_idIndexMarker060">
    </a>
    <span class="koboSpan" id="kobo.532.1">
     has developed an enormous new LLM.
    </span>
    <span class="koboSpan" id="kobo.532.2">
     It’s a hulking beast, dwarfing even OpenAI’s GPT-4 in sheer size.
    </span>
    <span class="koboSpan" id="kobo.532.3">
     But this isn’t just a power contest.
    </span>
    <span class="koboSpan" id="kobo.532.4">
     Olympus aims for something more: a significant leap in coherence, reasoning, and factual accuracy.
    </span>
    <span class="koboSpan" id="kobo.532.5">
     Their chatbot, Metis is powered by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.533.1">
      Olympus:
     </span>
    </span>
    <a href="https://happyfutureai.com/amazons-metis-a-new-ai-chatbot-powered-by-olympus-llm/">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.534.1">
       https://happyfutureai.com/amazons-metis-a-new-ai-chatbot-powered-by-olympus-llm/
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.535.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.536.1">
     With no half-baked ideas, Olympus
    </span>
    <a id="_idIndexMarker061">
    </a>
    <span class="koboSpan" id="kobo.537.1">
     digs deep, thinks logically, and double-checks its facts before uttering a word.
    </span>
    <span class="koboSpan" id="kobo.537.2">
     Amazon is purportedly working to reduce bias and misinformation.
    </span>
    <span class="koboSpan" id="kobo.537.3">
     This LLM strives for high levels of wisdom
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.538.1">
      and reliability.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.539.1">
     It’s not just about
    </span>
    <a id="_idIndexMarker062">
    </a>
    <span class="koboSpan" id="kobo.540.1">
     bragging rights for Amazon.
    </span>
    <span class="koboSpan" id="kobo.540.2">
     Olympus represents a potential turning point for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.541.1">
      language models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.542.1">
     The aim is to be able to tackle complex tasks with pinpoint accuracy, grasp subtle nuances of meaning, and engage in intelligent, fact-based conversations with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.543.1">
      other AI.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.544.1">
     Olympus will, hopefully, be a more thoughtful companion capable of deeper understanding and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.545.1">
      insightful exchange.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.546.1">
     Olympus may not be ready to join your book club just yet, but its story is worth watching.
    </span>
    <span class="koboSpan" id="kobo.546.2">
     Hopefully, Olympus will be a needed advancement for LLMs and not hallucinate, only producing truth and changing what LLMs
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.547.1">
      can do.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.548.1">
     Amazon Olympus should have around two trillion parameters (weights and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.549.1">
      biases) (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.550.1">
       Life_Achritecture
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.551.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.552.1">
     Amazon Olympus is expected in the second half of 2024 but not much information has come out since
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.553.1">
      November 2023.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.554.1">
     Now that we have introduced many of the modern LLMs, let’s look at how they work, including using an example piece
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.555.1">
      of text.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-29">
    <a id="_idTextAnchor038">
    </a>
    <span class="koboSpan" id="kobo.556.1">
     How Transformers work
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.557.1">
     Moving on to the
    </span>
    <a id="_idIndexMarker063">
    </a>
    <span class="koboSpan" id="kobo.558.1">
     general transformers,
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.559.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.560.1">
      .8
     </span>
    </em>
    <span class="koboSpan" id="kobo.561.1">
     shows the structure of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.562.1">
      a Transformer:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer014">
     <span class="koboSpan" id="kobo.563.1">
      <img alt="Figure 1.8: Architecture of a Transformer: an encoder for the inputs and a decoder for the outputs (reproduced from Zahere)" src="image/B21009_01_8.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.564.1">
     Figure 1.8: Architecture of a Transformer: an encoder for the inputs and a decoder for the outputs (reproduced from Zahere)
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.565.1">
     You can see that
    </span>
    <a id="_idIndexMarker064">
    </a>
    <span class="koboSpan" id="kobo.566.1">
     it has an encoder and a decoder.
    </span>
    <span class="koboSpan" id="kobo.566.2">
     The encoder learns the patterns in the data and the decoder tries to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.567.1">
      recreate them.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.568.1">
     The encoder has multiple neural network layers.
    </span>
    <span class="koboSpan" id="kobo.568.2">
     In transformers, each layer uses self-attention, allowing the encoder to understand how the different parts of the sentence fit together and understand
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.569.1">
      the context.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.570.1">
     Here is a quick
    </span>
    <a id="_idIndexMarker065">
    </a>
    <span class="koboSpan" id="kobo.571.1">
     version of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.572.1">
      transformer process:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.573.1">
       Encoder network:
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.574.1">
       Uses multiple layers of
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.575.1">
        neural networks.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.576.1">
       Each layer employs self-attention to understand relationships between sentence parts
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.577.1">
        and context.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.578.1">
       Creates a compressed representation of
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.579.1">
        the input.
       </span>
      </span>
     </p>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.580.1">
       Decoder
      </span>
     </span>
     <span class="No-Break">
      <a id="_idIndexMarker066">
      </a>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.581.1">
       network:
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.582.1">
       Utilizes the encoder’s representation for generating
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.583.1">
        new outputs.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.584.1">
       Employs multiple layers with cross-attention for information exchange with
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.585.1">
        the encoder.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.586.1">
       Generates meaningful outputs such as translations, summaries, or answers based
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.587.1">
        on input.
       </span>
      </span>
     </p>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.588.1">
       Encoder-decoder partnership:
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.589.1">
       Combined, they power the transformer for various tasks with high accuracy
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.590.1">
        and flexibility.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.591.1">
       For example, Microsoft Bing leverages GPT-4, a transformer model, to understand user intent and context beyond keywords for delivering relevant
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.592.1">
        search results.
       </span>
      </span>
     </p>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.593.1">
       Beyond keywords:
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.594.1">
       Bing transforms from a search engine to an AI-powered copilot
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.595.1">
        using GPT-4.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.596.1">
       It interprets questions and requests by analyzing context and intent, not
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.597.1">
        just keywords.
       </span>
      </span>
     </p>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.598.1">
       For example, instead of only providing ingredient lists, it recommends personalized recipes considering dietary needs and
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.599.1">
        skill levels.
       </span>
      </span>
     </p>
    </li>
    <li>
     <span class="koboSpan" id="kobo.600.1">
      From links
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.601.1">
       to understanding:
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.602.1">
       Bing evolves beyond finding links to comprehending user needs and delivering relevant,
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.603.1">
        helpful information
       </span>
       <a id="_idTextAnchor039">
       </a>
       <span class="koboSpan" id="kobo.604.1">
        .
       </span>
      </span>
     </p>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.605.1">
     Next is the detailed version of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.606.1">
      Transformer process.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-30">
    <a id="_idTextAnchor040">
    </a>
    <span class="koboSpan" id="kobo.607.1">
     How an LLM processes a piece of text
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.608.1">
     The encoder
    </span>
    <a id="_idIndexMarker067">
    </a>
    <span class="koboSpan" id="kobo.609.1">
     produces a compressed representation of the input.
    </span>
    <span class="koboSpan" id="kobo.609.2">
     This allows the decoder to not only consider its own outputs
    </span>
    <a id="_idIndexMarker068">
    </a>
    <span class="koboSpan" id="kobo.610.1">
     but also look back at the encoder’s representation, which contains a representation of the whole input sequence for guidance.
    </span>
    <span class="koboSpan" id="kobo.610.2">
     This is used by the decoder for each step of its
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.611.1">
      output generation.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.612.1">
     The decoder uses output from the encoder to generate a new output sequence.
    </span>
    <span class="koboSpan" id="kobo.612.2">
     Because of Transformers, modern LLMs can hold entire sentences or paragraphs in their attention, not just one word at a time
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.613.1">
      like RNNs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.614.1">
     Again, this section has lots of layers but, this time, there
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.615.1">
      is cross-attention.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.616.1">
     This back-and-forth conversation between the decoder and the encoder’s compressed knowledge empowers the decoder to generate meaningful and relevant outputs, such as translating a sentence to another language, summarizing a paragraph, or answering a question based on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.617.1">
      the input.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.618.1">
     Together, the encoder and decoder form the powerhouse of the transformer, enabling it to perform a wide range of tasks with remarkable accuracy
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.619.1">
      and flexibility.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.620.1">
     Microsoft’s Bing search engine uses GPT-4 to deliver more relevant search results, understanding your intent and context beyond
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.621.1">
      just keywords.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.622.1">
     Bing has gone from a search engine to an AI-powered copilot with the help of GPT-4.
    </span>
    <span class="koboSpan" id="kobo.622.2">
     This powerful language model acts as Bing’s brain, understanding your questions and requests not just through keywords, but by analyzing the context
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.623.1">
      and intent.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.624.1">
     You can, for example, ask for a recipe instead of just ingredients; GPT-4 scours the web, considers your dietary needs and skill level, and then presents a personalized selection.
    </span>
    <span class="koboSpan" id="kobo.624.2">
     It’s like having a knowledgeable friend helping you navigate the vast ocean of information.
    </span>
    <span class="koboSpan" id="kobo.624.3">
     So, Bing isn’t just about finding links anymore; it’s about understanding what you truly need and
    </span>
    <a id="_idIndexMarker069">
    </a>
    <span class="koboSpan" id="kobo.625.1">
     delivering it in a way that’s relevant and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.626.1">
      helpful (
     </span>
    </span>
    <a href="https://www.bing.com/">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.627.1">
       https://www.bing.com/
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.628.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.629.1">
     The whole process of getting a paragraph into an LLM goes
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.630.1">
      like this:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.631.1">
       Cleaning
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.632.1">
       Tokenization
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.633.1">
      Word-to-number conversion (words given indices: 1, 2,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.634.1">
       3, 4…)
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.635.1">
      Numbers are turned
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.636.1">
       into vectors
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.637.1">
       Contextual embedding
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.638.1">
      Context vectors
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.639.1">
       are formed
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.640.1">
      Attention vectors are formed and fed into
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.641.1">
       final blocks
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.642.1">
      Subsequent words
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.643.1">
       are predicted
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.644.1">
     (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.645.1">
      ChatGPT
     </span>
    </em>
    <span class="koboSpan" id="kobo.646.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.647.1">
      Gemini
     </span>
    </em>
    <span class="koboSpan" id="kobo.648.1">
     ,
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.649.1">
       Panuganty
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.650.1">
      ,
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.651.1">
       Aakanksha
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.652.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.653.1">
     With this
    </span>
    <a id="_idIndexMarker070">
    </a>
    <span class="koboSpan" id="kobo.654.1">
     framework in your subconscious, we can go
    </span>
    <a id="_idIndexMarker071">
    </a>
    <span class="koboSpan" id="kobo.655.1">
     through the details of the stages.
    </span>
    <span class="koboSpan" id="kobo.655.2">
     When you pay for ChatGPT questions and answers (more for developers), you pay by thousands of tokens.
    </span>
    <span class="koboSpan" id="kobo.655.3">
     Tokens are where the sentences are split up into words and punctuation or tokenized.
    </span>
    <span class="koboSpan" id="kobo.655.4">
     Tokens are turned into numbers (indices) and those are put into vectors, as the maths happens more easily with vectors or context vectors.
    </span>
    <span class="koboSpan" id="kobo.655.5">
     The attention layers show the model where to focus in each sentence, and the next word can
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.656.1">
      be predicted.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.657.1">
     This process is needed to input the words and sentences into the transformer model to train it and to query it to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.658.1">
      get responses.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.659.1">
     Before tokenization, the data (sentence, paragraph, etc.) would need to be cleaned and normalized: remove special characters, lowercase everything, and some other
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.660.1">
      basic cleaning.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.661.1">
     Here is an example of a paragraph
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.662.1">
      for tokenization:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.663.1">
     “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.664.1">
      The game Fallout stands out as a distinctive and immersive gaming experience when compared to Fortnite.
     </span>
     <span class="koboSpan" id="kobo.664.2">
      Fallout’s strength lies in its rich narrative, offering players a post-apocalyptic world filled with intricate storytelling, character development, and meaningful choices.
     </span>
     <span class="koboSpan" id="kobo.664.3">
      Unlike Fortnite’s fast-paced battle royale format, Fallout provides a vast open-world exploration, encouraging players to delve into a detailed and atmospheric environment.
     </span>
     <span class="koboSpan" id="kobo.664.4">
      The emphasis on role-playing and decision-making in Fallout adds layers of complexity, contributing to a more profound
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.665.1">
       gaming engagement.
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.666.1">
      ”
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.667.1">
     Now, here is
    </span>
    <a id="_idIndexMarker072">
    </a>
    <span class="koboSpan" id="kobo.668.1">
     that same paragraph
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.669.1">
      tokenized (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.670.1">
       ChatGPT
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.671.1">
      ):
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.672.1">
["The", "game", "Fallout", "stands", "out", "as", "a", "distinctive", "and", "immersive", "gaming", "experience", "when", "compared", "to", "Fortnite", ".", "Fallout's", "strength", "lies", "in", "its", "rich", "narrative", ",", "offering", "players", "a", "post-apocalyptic", "world", "filled", "with", "intricate", "storytelling", ",", "character", "development", ",", "and", "meaningful", "choices", ".", "Unlike", "Fortnite's", "fast-paced", "battle", "royale", "format", ",", "Fallout", "provides", "a", "vast", "open-world", "exploration", ",", "encouraging", "players", "to", "delve", "into", "a", "detailed", "and", "atmospheric", "environment", ".", "The", "emphasis", "on", "role-playing", "and", "decision-making", "in", "Fallout", "adds", "layers", "of", "complexity", ",", "contributing", "to", "a", "more", "profound", "gaming", "engagement", "."]</span></pre>
   <p>
    <span class="koboSpan" id="kobo.673.1">
     Sentences can be tokenized with
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.674.1">
       BertTokenizer
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.675.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.676.1">
       Metzger
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.677.1">
      ):
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.678.1">
from transformers import BertTokenizer</span></pre>
   <p>
    <span class="koboSpan" id="kobo.679.1">
     Then, a word becomes an index by
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.680.1">
      word-to-number conversion:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.681.1">
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88</span></pre>
   <p>
    <span class="koboSpan" id="kobo.682.1">
     Indices
    </span>
    <a id="_idIndexMarker073">
    </a>
    <span class="koboSpan" id="kobo.683.1">
     become vectors, as defined by pre-trained representations of words that come from training on the huge datasets mentioned earlier.
    </span>
    <span class="koboSpan" id="kobo.683.2">
     This comes from Word2Vec, GloVe or FastText, ELMo, or the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.684.1">
      all-famous BERTs:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.685.1">
       "The": [0.2, 0.8, -
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.686.1">
        0.5, 0.3]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.687.1">
       "game": [0.5, -0.7,
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.688.1">
        0.1, 0.6]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.689.1">
       "Fallout": [0.9, 0.4, -
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.690.1">
        0.2, -0.1]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.691.1">
       "stands": [-0.3, 0.6,
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.692.1">
        0.7, -0.5]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.693.1">
       "out": [-0.7, 0.2, -
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.694.1">
        0.4, 0.9]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.695.1">
       "as": [0.3, 0.1, -
      </span>
     </strong>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.696.1">
        0.6, 0.4]
       </span>
      </strong>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.697.1">
     The
    </span>
    <a id="_idIndexMarker074">
    </a>
    <span class="koboSpan" id="kobo.698.1">
     size of the vectors depends on the number of
    </span>
    <a id="_idIndexMarker075">
    </a>
    <span class="koboSpan" id="kobo.699.1">
     dimensions of the model.
    </span>
    <span class="koboSpan" id="kobo.699.2">
     The preceding model implies a four-dimensional model, which is very small, just for this
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.700.1">
      simple explanation.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.701.1">
     The model with only two dimensions might have
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.702.1">
      "woman"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.703.1">
     in the context of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.704.1">
      "man"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.705.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.706.1">
      "fast"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.707.1">
     in the context
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.708.1">
      of
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.709.1">
       "slow"
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.710.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.711.1">
     Next, we have contextual embedding: what is the environment of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.712.1">
      the word?
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.713.1">
     Here are some examples of the sort of thing that
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.714.1">
      would happen:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.715.1">
      Sentence 1,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.716.1">
       The game Fallout stands out...
      </span>
     </em>
     <span class="koboSpan" id="kobo.717.1">
      : Embedding might emphasize aspects of distinctiveness and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.718.1">
       gaming experience
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.719.1">
      Sentence 2,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.720.1">
       Fallout’s strength lies in its rich narrative...
      </span>
     </em>
     <span class="koboSpan" id="kobo.721.1">
      : Embedding might focus on storytelling and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.722.1">
       narrative elements
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.723.1">
      Sentence 3,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.724.1">
       Unlike Fortnite’s fast-paced format, Fallout provides...
      </span>
     </em>
     <span class="koboSpan" id="kobo.725.1">
      : Embedding might highlight the contrast with another game and world
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.726.1">
       exploration aspects
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.727.1">
     As vectors, that would look
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.728.1">
      like this:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.729.1">
      The: [0.12, 0.34, 0.56, 0.21, -0.05, ..., 0.90] (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.730.1">
       300 values)
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.731.1">
      game: [0.78, 0.21, -0.45, 0.10, 0.83, ..., 0.68] (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.732.1">
       300 values)
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.733.1">
      Fallout: [0.90, -0.10, 0.05, 0.75, 0.43, ..., -0.22] (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.734.1">
       300 values)
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.735.1">
     There are 300 dimensions because that enables the model to capture rather subtle semantic relationships but would also require more training data and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.736.1">
      computational resources.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.737.1">
     This could be done with only 50 dimensions if the dataset were small, and you didn’t want to spend a lot of time and money computing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.738.1">
      it all.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-31">
    <a id="_idTextAnchor041">
    </a>
    <span class="koboSpan" id="kobo.739.1">
     ChatGPT uses reinforcement learning from human feedback
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.740.1">
     ChatGPT
    </span>
    <a id="_idIndexMarker076">
    </a>
    <span class="koboSpan" id="kobo.741.1">
     stands out among other LLMs due to its ability to
    </span>
    <a id="_idIndexMarker077">
    </a>
    <span class="koboSpan" id="kobo.742.1">
     continuously improve through a process called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.743.1">
      reinforcement learning from human feedback
     </span>
    </strong>
    <span class="koboSpan" id="kobo.744.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.745.1">
      RLHF
     </span>
    </strong>
    <span class="koboSpan" id="kobo.746.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.746.2">
     This means it doesn’t just learn
    </span>
    <a id="_idIndexMarker078">
    </a>
    <span class="koboSpan" id="kobo.747.1">
     from massive datasets of text and code but also incorporates direct feedback from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.748.1">
      human users.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.749.1">
     When a new
    </span>
    <a id="_idIndexMarker079">
    </a>
    <span class="koboSpan" id="kobo.750.1">
     GPT model is trained (call it GPT-X for any GPT model), before being released to the public, users interact with GPT-X, asking questions or giving instructions.
    </span>
    <span class="koboSpan" id="kobo.750.2">
     After receiving a response, they can express approval or disapproval through various methods, such as thumbs-up/down ratings or explicit feedback prompts.
    </span>
    <span class="koboSpan" id="kobo.750.3">
     This valuable input directly affects how the GPT-X model refines its internal model, prioritizing responses that resonate with humans and minimizing those that miss
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.751.1">
      the mark.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.752.1">
     Think of it like training a puppy.
    </span>
    <span class="koboSpan" id="kobo.752.2">
     Just as rewards encourage desired behaviors, positive feedback in RLHF reinforces helpful and accurate responses within GPT-X.
    </span>
    <span class="koboSpan" id="kobo.752.3">
     Over time, through countless interactions and feedback loops, GPT-X fine-tunes its responses to be more informative, engaging, and aligned with human preferences.
    </span>
    <span class="koboSpan" id="kobo.752.4">
     This human-in-the-loop approach sets GPT models apart, allowing them to adapt and learn dynamically, continuously evolving their capabilities based on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.753.1">
      real-world interactions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.754.1">
     This is how the researchers and developers attempt to make the AI ethical and moral, according to their understanding of human morals, which will not agree with everybody, but do agree with common culture,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.755.1">
      including laws.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.756.1">
     Other people like to make sure uncensored LLMs exist that don’t encourage the politics of the LLM developers such as Californian
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.757.1">
      tech companies.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.758.1">
     This process should stop the AI from helping anybody to do anything violent/illegal, such as constructing weapons or illegally hacking into an organization’s website
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.759.1">
      or servers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.760.1">
     While the specifics of RLHF implementation remain proprietary, its impact is evident in ChatGPT’s ability to handle diverse conversation styles, generate different creative text formats, and provide
    </span>
    <a id="_idIndexMarker080">
    </a>
    <span class="koboSpan" id="kobo.761.1">
     informative answers.
    </span>
    <span class="koboSpan" id="kobo.761.2">
     As RLHF technologies advance, we can expect LLMs such as ChatGPT to become even more adept at understanding and responding to human needs, blurring the lines between machine and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.762.1">
      human communication.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-32">
    <a id="_idTextAnchor042">
    </a>
    <span class="koboSpan" id="kobo.763.1">
     LLMs are expensive
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.764.1">
     Many tech company players have been working to create and train their own LLMs or chatbots to ride this wave of innovation for money and control.
    </span>
    <span class="koboSpan" id="kobo.764.2">
     LLMs of today, 2024, require an enormous amount of training and this takes enormous piles of cash.
    </span>
    <span class="koboSpan" id="kobo.764.3">
     OpenAI took funding of about $13 billion when Microsoft bought shares in OpenAI, and much of this was likely used on training the GPT family of LLMs on Microsoft’s own Azure cloud
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.765.1">
      servers (
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.766.1">
       Sanman
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.767.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.768.1">
     Cash and cooling (energy) are required to train and run LLMs, so it’s a good thing deep learning models can be used to save energy and reduce pollution.
    </span>
    <span class="koboSpan" id="kobo.768.2">
     DeepMind once saved Google data centers 40% of their cooling bill!
    </span>
    <span class="koboSpan" id="kobo.768.3">
     They did this by developing a deep learning model that made suggestions for how to modify how the cooling systems worked.
    </span>
    <span class="koboSpan" id="kobo.768.4">
     Later, the DeepMind model was set to just run the cooling systems directly [Hooper 2021 and DeepMind].
    </span>
    <span class="koboSpan" id="kobo.768.5">
     These Google data centers have their own dedicated power stations, so this is a lot of energy saved and money and pollution
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.769.1">
      saved too!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.770.1">
     Speaking of numbers and calculations,
    </span>
    <a id="_idTextAnchor043">
    </a>
    <span class="koboSpan" id="kobo.771.1">
     let’s briefly look at what classes of mathematics are involved
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.772.1">
      in LLMs.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-33">
    <a id="_idTextAnchor044">
    </a>
    <span class="koboSpan" id="kobo.773.1">
     A note on the mathematics of LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.774.1">
     Getting into the
    </span>
    <a id="_idIndexMarker081">
    </a>
    <span class="koboSpan" id="kobo.775.1">
     mathematical center of LLMs can be a bit of work, but understanding their core principles reveals a lot about how the most powerful and widely used AIs today function.
    </span>
    <span class="koboSpan" id="kobo.775.2">
     So, if you want to make these AI models and research them, the mathematics is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.776.1">
      very interesting:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.777.1">
       Foundations in linear algebra
      </span>
     </strong>
     <span class="koboSpan" id="kobo.778.1">
      : The bedrock of LLMs lies in linear algebra, where matrices and vectors rule.
     </span>
     <span class="koboSpan" id="kobo.778.2">
      Words are mapped to high-dimensional vectors, capturing their meanings and relationships within a vast semantic space.
     </span>
     <span class="koboSpan" id="kobo.778.3">
      Each word is a point in a multi-dimensional space, with related words clustering
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.779.1">
       closer together.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.780.1">
       Backpropagation and optimization
      </span>
     </strong>
     <span class="koboSpan" id="kobo.781.1">
      : Training LLMs requires massive datasets and sophisticated optimization algorithms.
     </span>
     <span class="koboSpan" id="kobo.781.2">
      One powerful tool is backpropagation, a mathematical technique that calculates the error gradient – how much each parameter in the model contributes to the overall deviation from the
     </span>
     <a id="_idIndexMarker082">
     </a>
     <span class="koboSpan" id="kobo.782.1">
      desired output.
     </span>
     <span class="koboSpan" id="kobo.782.2">
      By iteratively adjusting these parameters based on the error gradient, the LLM learns and improves
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.783.1">
       its predictions.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.784.1">
       Loss functions and metrics
      </span>
     </strong>
     <span class="koboSpan" id="kobo.785.1">
      : To evaluate the performance of an LLM, we need quantitative measures.
     </span>
     <span class="koboSpan" id="kobo.785.2">
      Loss functions define how much the model’s output deviates from the desired outcome, while metrics such as accuracy, perplexity, and BLEU score assess its ability to generate fluent, contextually
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.786.1">
       appropriate text:
      </span>
     </span>
     <ul>
      <li>
       <strong class="bold">
        <span class="koboSpan" id="kobo.787.1">
         BLEU score
        </span>
       </strong>
       <span class="koboSpan" id="kobo.788.1">
        stands for
       </span>
       <strong class="bold">
        <span class="koboSpan" id="kobo.789.1">
         Bilingual Evaluation Understudy score
        </span>
       </strong>
       <span class="koboSpan" id="kobo.790.1">
        , which is
       </span>
       <a id="_idIndexMarker083">
       </a>
       <span class="koboSpan" id="kobo.791.1">
        from translation but can be used as a way to compare AI-generated translations with reference translations.
       </span>
       <span class="koboSpan" id="kobo.791.2">
        It can be calculated with the NLTK code library in Python using the
       </span>
       <strong class="source-inline">
        <span class="koboSpan" id="kobo.792.1">
         sentence_bleu()
        </span>
       </strong>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.793.1">
         function (
        </span>
       </span>
       <span class="No-Break">
        <em class="italic">
         <span class="koboSpan" id="kobo.794.1">
          Brownlee_BLEU
         </span>
        </em>
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.795.1">
         ).
        </span>
       </span>
      </li>
     </ul>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.796.1">
       Beyond basic maths
      </span>
     </strong>
     <span class="koboSpan" id="kobo.797.1">
      : The mathematics of LLMs extends far beyond these core principles.
     </span>
     <span class="koboSpan" id="kobo.797.2">
      Techniques such as regularization, dropout, and gradient clipping help prevent overfitting and improve generalization.
     </span>
     <span class="koboSpan" id="kobo.797.3">
      RNNs add memory capabilities, allowing the model to learn from longer sequences of data.
     </span>
     <span class="koboSpan" id="kobo.797.4">
      The world of mathematics is constantly evolving, pushing the boundaries of what LLMs
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.798.1">
       can achieve.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.799.1">
       Transformers and attention
      </span>
     </strong>
     <span class="koboSpan" id="kobo.800.1">
      : This mathematical architecture forms the engine of modern LLMs.
     </span>
     <span class="koboSpan" id="kobo.800.2">
      In Transformers, the mechanism for calculating attention scores involves dot products between query and key vectors.
     </span>
     <span class="koboSpan" id="kobo.800.3">
      While in LSTMs, each time step acts as both
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.801.1">
       query
      </span>
     </em>
     <span class="koboSpan" id="kobo.802.1">
      and
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.803.1">
       key
      </span>
     </em>
     <span class="koboSpan" id="kobo.804.1">
      , Transformers separate these roles: The query originates from the current token’s representation, while the keys are derived from the value representations of all tokens in the sequence.
     </span>
     <span class="koboSpan" id="kobo.804.2">
      This distinction helps to compute attention scores that indicate how significant or relevant each token is within its context.
     </span>
     <span class="koboSpan" id="kobo.804.3">
      Transformers also use values, which are also derived
     </span>
     <a id="_idIndexMarker084">
     </a>
     <span class="koboSpan" id="kobo.805.1">
      from the word embeddings of all tokens, carrying the actual information from
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.806.1">
       each token:
      </span>
     </span>
     <ul>
      <li>
       <span class="koboSpan" id="kobo.807.1">
        Let’s look at an example sentence,
       </span>
       <em class="italic">
        <span class="koboSpan" id="kobo.808.1">
         The cat played with
        </span>
       </em>
       <span class="No-Break">
        <em class="italic">
         <span class="koboSpan" id="kobo.809.1">
          a ball
         </span>
        </em>
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.810.1">
         .
        </span>
       </span>
      </li>
      <li>
       <span class="koboSpan" id="kobo.811.1">
        In a Transformer’s attention mechanism, the
       </span>
       <span class="No-Break">
        <span class="koboSpan" id="kobo.812.1">
         following applies:
        </span>
       </span>
       <ul>
        <li>
         <span class="koboSpan" id="kobo.813.1">
          Words and their meanings are usually represented numerically using embeddings, such as Word2Vec or
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.814.1">
           GloVe vectors
          </span>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.815.1">
          The query would be derived from the representation of the current token; let’s say it’s the
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.816.1">
           word
          </span>
         </span>
         <span class="No-Break">
          <em class="italic">
           <span class="koboSpan" id="kobo.817.1">
            played
           </span>
          </em>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.818.1">
          The keys are calculated from the value representations of all tokens in the sequence, so we’d have keys for
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.819.1">
           The
          </span>
         </em>
         <span class="koboSpan" id="kobo.820.1">
          ,
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.821.1">
           cat
          </span>
         </em>
         <span class="koboSpan" id="kobo.822.1">
          ,
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.823.1">
           played
          </span>
         </em>
         <span class="koboSpan" id="kobo.824.1">
          ,
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.825.1">
           with
          </span>
         </em>
         <span class="koboSpan" id="kobo.826.1">
          ,
         </span>
         <em class="italic">
          <span class="koboSpan" id="kobo.827.1">
           a
          </span>
         </em>
         <span class="koboSpan" id="kobo.828.1">
          ,
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.829.1">
           and
          </span>
         </span>
         <span class="No-Break">
          <em class="italic">
           <span class="koboSpan" id="kobo.830.1">
            ball
           </span>
          </em>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.831.1">
          Then, each query would do a dot product with every
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.832.1">
           key vector
          </span>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.833.1">
          These dot products would then be used to calculate the
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.834.1">
           attention scores
          </span>
         </span>
        </li>
        <li>
         <span class="koboSpan" id="kobo.835.1">
          Ultimately, this process helps highlight which words in the sequence are most relevant or important within context, enhancing the Transformer’s ability to
         </span>
         <span class="No-Break">
          <span class="koboSpan" id="kobo.836.1">
           understand text
          </span>
         </span>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.837.1">
     While the maths might seem daunting, it’s crucial to remember that it’s just a tool.
    </span>
    <span class="koboSpan" id="kobo.837.2">
     The true power lies in how these algorithms are woven together to create models capable of remarkable feats of language processing.
    </span>
    <span class="koboSpan" id="kobo.837.3">
     As mathematical models evolve and datasets grow, LLMs promise to push the boundaries of language, blurring the lines betwee
    </span>
    <a id="_idTextAnchor045">
    </a>
    <span class="koboSpan" id="kobo.838.1">
     n human and machine communication in ever-fascinating ways [
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.839.1">
      Llama3, Gemini].
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-34">
    <a id="_idTextAnchor046">
    </a>
    <span class="koboSpan" id="kobo.840.1">
     Applications of LLMs
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.841.1">
     The LLM revolution is
    </span>
    <a id="_idIndexMarker085">
    </a>
    <span class="koboSpan" id="kobo.842.1">
     reaching its virtual tentacles into every corner of life, from writing your college essay to generating personalized Coca-Cola ads and customer services.
    </span>
    <span class="koboSpan" id="kobo.842.2">
     Here’s a quick peek into just 16
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.843.1">
      diverse applications:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.844.1">
       DIYVA
      </span>
     </em>
     <span class="koboSpan" id="kobo.845.1">
      : Designs
     </span>
     <a id="_idIndexMarker086">
     </a>
     <span class="koboSpan" id="kobo.846.1">
      stunning visuals and logos, making even the artistically challenged look like
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.847.1">
       Picassos (
      </span>
     </span>
     <a href="https://diyva.life/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.848.1">
        https://diyva.life/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.849.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.850.1">
       LimeWire
      </span>
     </em>
     <span class="koboSpan" id="kobo.851.1">
      : Conjures
     </span>
     <a id="_idIndexMarker087">
     </a>
     <span class="koboSpan" id="kobo.852.1">
      up unique AI-generated artwork, turning your wildest creative visions into reality.
     </span>
     <span class="koboSpan" id="kobo.852.2">
      Start
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.853.1">
       here:
      </span>
     </span>
     <a href="https://limewire.com/studio?referrer=ml736b1k7k">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.854.1">
        https://limewire.com/studio?referrer=ml736b1k7k
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.855.1">
       .
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.856.1">
       Coca-Cola
      </span>
     </em>
     <span class="koboSpan" id="kobo.857.1">
      : Creates
     </span>
     <a id="_idIndexMarker088">
     </a>
     <span class="koboSpan" id="kobo.858.1">
      targeted ad campaigns, crafting personalized marketing messages for each individual
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.859.1">
       Coke-sipper (
      </span>
     </span>
     <a href="https://www.coca-cola.com/gb/en">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.860.1">
        https://www.coca-cola.com/gb/en
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.861.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.862.1">
       Slack
      </span>
     </em>
     <span class="koboSpan" id="kobo.863.1">
      : Transcribes
     </span>
     <a id="_idIndexMarker089">
     </a>
     <span class="koboSpan" id="kobo.864.1">
      meetings and automatically summarizes key points, saving you precious time and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.865.1">
       attention (
      </span>
     </span>
     <a href="https://slack.com/intl/en-gb">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.866.1">
        https://slack.com/intl/en-gb
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.867.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.868.1">
       Octopus Energy
      </span>
     </em>
     <span class="koboSpan" id="kobo.869.1">
      : Predicts
     </span>
     <a id="_idIndexMarker090">
     </a>
     <span class="koboSpan" id="kobo.870.1">
      your energy usage and suggests personalized plans, optimizing your home’s power with LLM
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.871.1">
       intelligence (
      </span>
     </span>
     <a href="https://octopus.energy/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.872.1">
        https://octopus.energy/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.873.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.874.1">
       Cheggmate
      </span>
     </em>
     <span class="koboSpan" id="kobo.875.1">
      : Offers
     </span>
     <a id="_idIndexMarker091">
     </a>
     <span class="koboSpan" id="kobo.876.1">
      AI-powered tutoring tailored to each student’s specific needs, making learning more efficient and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.877.1">
       engaging (
      </span>
     </span>
     <a href="https://www.cheggmate.ai/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.878.1">
        https://www.cheggmate.ai/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.879.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.880.1">
       Freshworks
      </span>
     </em>
     <span class="koboSpan" id="kobo.881.1">
      : Automates
     </span>
     <a id="_idIndexMarker092">
     </a>
     <span class="koboSpan" id="kobo.882.1">
      customer service, analyzing chats and offering solutions before agents even
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.883.1">
       blink (
      </span>
     </span>
     <a href="https://www.cheggmate.ai/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.884.1">
        https://www.cheggmate.ai/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.885.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.886.1">
       Udacity
      </span>
     </em>
     <span class="koboSpan" id="kobo.887.1">
      : Designs
     </span>
     <a id="_idIndexMarker093">
     </a>
     <span class="koboSpan" id="kobo.888.1">
      personalized learning paths, guiding you through the tech jungle with LLM-powered
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.889.1">
       recommendations (
      </span>
     </span>
     <a href="https://www.udacity.com/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.890.1">
        https://www.udacity.com/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.891.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.892.1">
       Zalando
      </span>
     </em>
     <span class="koboSpan" id="kobo.893.1">
      : This
     </span>
     <a id="_idIndexMarker094">
     </a>
     <span class="koboSpan" id="kobo.894.1">
      European
     </span>
     <a id="_idIndexMarker095">
     </a>
     <span class="koboSpan" id="kobo.895.1">
      fashion retailer uses LLMs to generate personalized product recommendations based on user preferences and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.896.1">
       behavior (
      </span>
     </span>
     <a href="https://www.zalando.co.uk/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.897.1">
        https://www.zalando.co.uk/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.898.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.899.1">
       Headspace
      </span>
     </em>
     <span class="koboSpan" id="kobo.900.1">
      : Headspace
     </span>
     <a id="_idIndexMarker096">
     </a>
     <span class="koboSpan" id="kobo.901.1">
      leverages LLMs to personalize guided meditations, adapting practices to your mood, sleep patterns, and personal
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.902.1">
       goals (
      </span>
     </span>
     <a href="https://www.headspace.com/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.903.1">
        https://www.headspace.com/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.904.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.905.1">
       Spotify
      </span>
     </em>
     <span class="koboSpan" id="kobo.906.1">
      : Spotify’s Discover
     </span>
     <a id="_idIndexMarker097">
     </a>
     <span class="koboSpan" id="kobo.907.1">
      Weekly playlists and other personalized recommendations are generated by LLMs, analyzing your listening habits and music preferences to curate an ever-evolving soundtrack for your
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.908.1">
       life (
      </span>
     </span>
     <a href="https://open.spotify.com/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.909.1">
        https://open.spotify.com/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.910.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.911.1">
       Peloton
      </span>
     </em>
     <span class="koboSpan" id="kobo.912.1">
      : Peloton’s AI coaches
     </span>
     <a id="_idIndexMarker098">
     </a>
     <span class="koboSpan" id="kobo.913.1">
      utilize LLMs to deliver dynamic real-time feedback during workouts, tailoring prompts and challenges to your individual performance and fitness
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.914.1">
       goals (
      </span>
     </span>
     <a href="https://www.onepeloton.co.uk/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.915.1">
        https://www.onepeloton.co.uk/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.916.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.917.1">
       Baidu’s WenLan
      </span>
     </em>
     <span class="koboSpan" id="kobo.918.1">
      : Helps
     </span>
     <a id="_idIndexMarker099">
     </a>
     <span class="koboSpan" id="kobo.919.1">
      Chinese businesses analyze customer reviews and personalize marketing campaigns; a local LLM
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.920.1">
       giant (
      </span>
     </span>
     <a href="https://ir.baidu.com/company-overview">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.921.1">
        https://ir.baidu.com/company-overview
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.922.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.923.1">
       NVIDIA Megatron-Turing NLG
      </span>
     </em>
     <span class="koboSpan" id="kobo.924.1">
      : Generates
     </span>
     <a id="_idIndexMarker100">
     </a>
     <span class="koboSpan" id="kobo.925.1">
      different creative text formats such as poems, code, scripts, and so on, pushing the boundaries of LLM
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.926.1">
       expressiveness (
      </span>
     </span>
     <a href="https://gpt3demo.com/apps/mt-nlg-by-microsoft-and-nvidia-ai">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.927.1">
        https://gpt3demo.com/apps/mt-nlg-by-microsoft-and-nvidia-ai
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.928.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.929.1">
       Grammarly
      </span>
     </em>
     <span class="koboSpan" id="kobo.930.1">
      : This
     </span>
     <a id="_idIndexMarker101">
     </a>
     <span class="koboSpan" id="kobo.931.1">
      writing assistant uses
     </span>
     <a id="_idIndexMarker102">
     </a>
     <span class="koboSpan" id="kobo.932.1">
      LLMs to analyze your writing, offering real-time grammar and style suggestions for clearer, more
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.933.1">
       impactful communication
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.934.1">
      (
     </span>
     <a href="https://app.grammarly.com/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.935.1">
        https://app.grammarly.com/
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.936.1">
       ).
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.937.1">
       DeepBrain AI:
      </span>
     </em>
     <span class="koboSpan" id="kobo.938.1">
      Utilizes
     </span>
     <a id="_idIndexMarker103">
     </a>
     <span class="koboSpan" id="kobo.939.1">
      their own LLM, along with sophisticated animation and voice-sy
     </span>
     <a id="_idTextAnchor047">
     </a>
     <span class="koboSpan" id="kobo.940.1">
      nthesis techniques (
     </span>
     <a href="https://www.deepbrain.io/aistudios?via=abtnews">
      <span class="koboSpan" id="kobo.941.1">
       https://www.deepbrain.io/aistudios?via=abtnews
      </span>
     </a>
     <span class="koboSpan" id="kobo.942.1">
      ).
     </span>
     <span class="koboSpan" id="kobo.942.2">
      (
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.943.1">
       ForbesMarr
      </span>
     </em>
     <span class="koboSpan" id="kobo.944.1">
      ,
     </span>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.945.1">
        f6s
       </span>
      </em>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.946.1">
       ,
      </span>
     </span>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.947.1">
        Gemini
       </span>
      </em>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.948.1">
       .)
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-35">
    <a id="_idTextAnchor048">
    </a>
    <span class="koboSpan" id="kobo.949.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.950.1">
     In this chapter, we covered what ChatGPT is and what LLMs in general are, the origins of some widely used LLMs such as BERT, the GPT family, LlaMDA, LlaMA, and modern LLMs such as GPT-4 and Gemini.
    </span>
    <span class="koboSpan" id="kobo.950.2">
     We looked at some architecture of LLMs and transformers.
    </span>
    <span class="koboSpan" id="kobo.950.3">
     We had a go at fully processing a sentence in the way an LLM model would: tokenizing, Word2Vec contextual embedding, and more.
    </span>
    <span class="koboSpan" id="kobo.950.4">
     We also touched on the types of mathematics involved and the applications of this fantastic technology deployed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.951.1">
      by companies.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.952.1">
     Hopefully, you now understand the nature of LLMs such as ChatGPT/Gemini; understand the architectures of LLMs; understand some mathematics of LLMs; and are enlightened about competition in the field and how to teach LLMs
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.953.1">
      to others.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.954.1">
     In
    </span>
    <a href="B21009_02.xhtml#_idTextAnchor051">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.955.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.956.1">
     , we will look at the advantages of coding with L
    </span>
    <a id="_idTextAnchor049">
    </a>
    <span class="koboSpan" id="kobo.957.1">
     LMs, planning your LLM-powered coding, doing some coding with LLMs, and making it work
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.958.1">
      for you.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-36">
    <a id="_idTextAnchor050">
    </a>
    <span class="koboSpan" id="kobo.959.1">
     Bibliography
    </span>
   </h1>
   <ul>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.960.1">
       Amari
      </span>
     </em>
     <span class="koboSpan" id="kobo.961.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.962.1">
       Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements
      </span>
     </em>
     <span class="koboSpan" id="kobo.963.1">
      ”, S.
     </span>
     <span class="koboSpan" id="kobo.963.2">
      I.
     </span>
     <span class="koboSpan" id="kobo.963.3">
      Amari
     </span>
     <a href="https://ieeexplore.ieee.org/document/1672070">
      <span class="koboSpan" id="kobo.964.1">
       https://ieeexplore.ieee.org/document/1672070
      </span>
     </a>
     <span class="koboSpan" id="kobo.965.1">
      in IEEE Transactions on Computers, vol.
     </span>
     <span class="koboSpan" id="kobo.965.2">
      C-21, no.
     </span>
     <span class="koboSpan" id="kobo.965.3">
      11, pp.
     </span>
     <span class="koboSpan" id="kobo.965.4">
      1197-1206, Nov.
     </span>
     <span class="koboSpan" id="kobo.965.5">
      1972,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.966.1">
       doi: 10.1109/T-C.1972.223477
      </span>
     </span>
     <p class="list-inset">
      <span class="koboSpan" id="kobo.967.1">
       keywords: {Associative memory, brain model, concept formation, logic nets of threshold elements, self-organization, sequential recalling, stability of
      </span>
      <span class="No-Break">
       <span class="koboSpan" id="kobo.968.1">
        state transition}
       </span>
      </span>
     </p>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.969.1">
       AnswerIQ
      </span>
     </em>
     <span class="koboSpan" id="kobo.970.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.971.1">
       25+ Google Bard Statistics 2024 (Usage, Traffic &amp; Cost)
      </span>
     </em>
     <span class="koboSpan" id="kobo.972.1">
      ”, Paul Rogers:
     </span>
     <a href="https://www.answeriq.com/google-bard-statistics/">
      <span class="koboSpan" id="kobo.973.1">
       https://www.answeriq.com/google-bard-statistics/
      </span>
     </a>
     <span class="koboSpan" id="kobo.974.1">
      6th
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.975.1">
       Jan 2024
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.976.1">
       Brownlee_LLMs
      </span>
     </em>
     <span class="koboSpan" id="kobo.977.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.978.1">
       What are Large Language Models
      </span>
     </em>
     <span class="koboSpan" id="kobo.979.1">
      ”, Adrian
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.980.1">
       Tam:
      </span>
     </span>
     <a href="https://machinelearningmastery.com/what-are-large-language-models/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.981.1">
        https://machinelearningmastery.com/what-are-large-language-models/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.982.1">
       Brownlee_BLEU
      </span>
     </em>
     <span class="koboSpan" id="kobo.983.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.984.1">
       A Gentle Introduction to Calculating the BLEU Score for Text in Python
      </span>
     </em>
     <span class="koboSpan" id="kobo.985.1">
      ”, Jason
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.986.1">
       Brownlee,
      </span>
     </span>
     <a href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/ ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.987.1">
        https://machinelearningmastery.com/calculate-bleu-score-for-text-python/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.988.1">
       Brush
      </span>
     </em>
     <span class="koboSpan" id="kobo.989.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.990.1">
       History of the Lenz-Ising Model
      </span>
     </em>
     <span class="koboSpan" id="kobo.991.1">
      ”, Stephen G.
     </span>
     <span class="koboSpan" id="kobo.991.2">
      Brush, 1967, Reviews of Modern Physics.
     </span>
     <span class="koboSpan" id="kobo.991.3">
      39 (4):
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.992.1">
       883–893.
      </span>
     </span>
     <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.39.883">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.993.1">
        https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.39.883
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.994.1">
       ChatGPT
      </span>
     </em>
     <span class="koboSpan" id="kobo.995.1">
      :”
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.996.1">
       ChatGPT
      </span>
     </em>
     <span class="koboSpan" id="kobo.997.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.998.1">
       OpenAI,
      </span>
     </span>
     <a href="https://chat.openai.com/ ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.999.1">
        https://chat.openai.com/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1000.1">
       Chellapilla2006
      </span>
     </em>
     <span class="koboSpan" id="kobo.1001.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1002.1">
       High Performance Convolutional Neural Networks for Document Processing
      </span>
     </em>
     <span class="koboSpan" id="kobo.1003.1">
      ”, Kumar Chellapilla; Sid Puri; Patrice Simard (2006).
     </span>
     <span class="koboSpan" id="kobo.1003.2">
      In Lorette, Guy (ed.).
     </span>
     <span class="koboSpan" id="kobo.1003.3">
      Tenth International Workshop on Frontiers in Handwriting Recognition.
     </span>
     <span class="koboSpan" id="kobo.1003.4">
      Suvisoft.
     </span>
     <span class="koboSpan" id="kobo.1003.5">
      Archived from the original on 2020-05-18.
     </span>
     <span class="koboSpan" id="kobo.1003.6">
      Retrieved
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1004.1">
       2016-03-14.
      </span>
     </span>
     <a href="https://inria.hal.science/inria-00112631/document ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1005.1">
        https://inria.hal.science/inria-00112631/document
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1006.1">
       CodeLlama
      </span>
     </em>
     <em class="italic">
      <span class="koboSpan" id="kobo.1007.1">
       2023
      </span>
     </em>
     <span class="koboSpan" id="kobo.1008.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1009.1">
       Introducing Code Llama, an AI Tool for Coding
      </span>
     </em>
     <span class="koboSpan" id="kobo.1010.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1011.1">
       Meta,
      </span>
     </span>
     <a href="https://about.fb.com/news/2023/08/code-llama-ai-for-coding/ ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1012.1">
        https://about.fb.com/news/2023/08/code-llama-ai-for-coding/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1013.1">
       DeepMind
      </span>
     </em>
     <span class="koboSpan" id="kobo.1014.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1015.1">
       DeepMind AI Reduces Google Data Centre Cooling Bill by 40%
      </span>
     </em>
     <span class="koboSpan" id="kobo.1016.1">
      ”, Richard Evans, Jim
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1017.1">
       Gao:
      </span>
     </span>
     <a href="https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1018.1">
        https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1019.1">
       encord
      </span>
     </em>
     <span class="koboSpan" id="kobo.1020.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1021.1">
       Stephen Oladele, GPT-4o vs.
      </span>
      <span class="koboSpan" id="kobo.1021.2">
       Gemini 1.5 Pro vs.
      </span>
      <span class="koboSpan" id="kobo.1021.3">
       Claude 3 Opus: Multimodal AI Model
      </span>
     </em>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.1022.1">
        Comparison
       </span>
      </em>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1023.1">
       ”,
      </span>
     </span>
     <a href="https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/#:~:text=Code%20Generation%20Capability,GPT%2D4o%20in%20this%20domain ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1024.1">
        https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/#:~:text=Code%20Generation%20Capability,GPT%2D4o%20in%20this%20domain
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1025.1">
       ForbesMarr
      </span>
     </em>
     <span class="koboSpan" id="kobo.1026.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1027.1">
       10 Amazing Real-World Examples Of How Companies Are Using ChatGPT In 2023
      </span>
     </em>
     <span class="koboSpan" id="kobo.1028.1">
      ”, Bernard
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1029.1">
       Marr:
      </span>
     </span>
     <a href="https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023/?sh=3fe5f9601441 ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1030.1">
        https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023/?sh=3fe5f9601441
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1031.1">
       f62
      </span>
     </em>
     <span class="koboSpan" id="kobo.1032.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1033.1">
       100 top ChatGPT companies and startups in 2024
      </span>
     </em>
     <span class="koboSpan" id="kobo.1034.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1035.1">
       f62:
      </span>
     </span>
     <a href="https://www.f6s.com/companies/chatgpt/mo ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1036.1">
        https://www.f6s.com/companies/chatgpt/mo
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1037.1">
       Fukushima1980
      </span>
     </em>
     <span class="koboSpan" id="kobo.1038.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1039.1">
       Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position
      </span>
     </em>
     <span class="koboSpan" id="kobo.1040.1">
      ”, Kunihiko Fukushima, J.
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1041.1">
       Biological Cybernetics.,
      </span>
     </span>
     <a href="https://doi.org/10.1007/BF00344251">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1042.1">
        https://doi.org/10.1007/BF00344251
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1043.1">
       GeekCultureBERT
      </span>
     </em>
     <span class="koboSpan" id="kobo.1044.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1045.1">
       4 Crucial Things to Know about GPT-4: You should know these to use GPT-4
      </span>
     </em>
     <span class="koboSpan" id="kobo.1046.1">
      ”, Tirendaz
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1047.1">
       AI,
      </span>
     </span>
     <a href="https://medium.com/geekculture/an-overview-of-gpt-4-in-4-steps-867bb81b31e3 ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1048.1">
        https://medium.com/geekculture/an-overview-of-gpt-4-in-4-steps-867bb81b31e3
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1049.1">
       Gemini
      </span>
     </em>
     <span class="koboSpan" id="kobo.1050.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1051.1">
       Gemini
      </span>
     </em>
     <span class="koboSpan" id="kobo.1052.1">
      ”, Google
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1053.1">
       Research,
      </span>
     </span>
     <a href="https://gemini.google.com/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1054.1">
        https://gemini.google.com/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1055.1">
       GeminiTeam
      </span>
     </em>
     <span class="koboSpan" id="kobo.1056.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1057.1">
       Gemini: A Family of Highly Capable Multimodal Models
      </span>
     </em>
     <span class="koboSpan" id="kobo.1058.1">
      ”, Gemini Team,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1059.1">
       Google,
      </span>
     </span>
     <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1060.1">
        https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1061.1">
       Hochreiter1997
      </span>
     </em>
     <span class="koboSpan" id="kobo.1062.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1063.1">
       Long Short-Term Memory
      </span>
     </em>
     <span class="koboSpan" id="kobo.1064.1">
      ”, Sepp Hochreiter, Sepp; Jürgen Schmidhuber, Jürgen (1997-11-01).
     </span>
     <span class="koboSpan" id="kobo.1064.2">
      Neural Computation.
     </span>
     <span class="koboSpan" id="kobo.1064.3">
      9 (8): 1735–1780.
     </span>
     <span class="koboSpan" id="kobo.1064.4">
      doi:10.1162/neco.1997.9.8.1735.
     </span>
     <span class="koboSpan" id="kobo.1064.5">
      PMID 9377276.
     </span>
     <span class="koboSpan" id="kobo.1064.6">
      S2CID
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1065.1">
       1915014.
      </span>
     </span>
     <a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltexthttps://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1066.1">
        https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltexthttps://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1067.1">
       Hooper
      </span>
     </em>
     <span class="koboSpan" id="kobo.1068.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1069.1">
       How to Spend $1 trillion
      </span>
     </em>
     <span class="koboSpan" id="kobo.1070.1">
      ”, Rowan Hooper (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1071.1">
       2021),
      </span>
     </span>
     <a href="https://www.goodreads.com/en/book/show/54823535">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1072.1">
        https://www.goodreads.com/en/book/show/54823535
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1073.1">
       HuggingFace
      </span>
     </em>
     <span class="koboSpan" id="kobo.1074.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1075.1">
       describeai/gemini
      </span>
     </em>
     <span class="koboSpan" id="kobo.1076.1">
      ”, Hugging
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1077.1">
       Face,
      </span>
     </span>
     <a href="https://huggingface.co/describeai/gemini">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1078.1">
        https://huggingface.co/describeai/gemini
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1079.1">
       Investors.com
      </span>
     </em>
     <span class="koboSpan" id="kobo.1080.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1081.1">
       OpenAI Circus Continues As Ousted Chief Executive Returns As Boss
      </span>
     </em>
     <span class="koboSpan" id="kobo.1082.1">
      ”, Patrick
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1083.1">
       Seitz:
      </span>
     </span>
     <a href="https://www.investors.com/news/technology/microsoft-stock-rises-on-sam-altman-return-to-openai/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1084.1">
        https://www.investors.com/news/technology/microsoft-stock-rises-on-sam-altman-return-to-openai/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1085.1">
       LeCun1989
      </span>
     </em>
     <span class="koboSpan" id="kobo.1086.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1087.1">
       Backpropagation Applied to Handwritten Zip Code Recognition
      </span>
     </em>
     <span class="koboSpan" id="kobo.1088.1">
      ”, Y.
     </span>
     <span class="koboSpan" id="kobo.1088.2">
      LeCun; B.J..S.
     </span>
     <span class="koboSpan" id="kobo.1088.3">
      Boser; J.S.
     </span>
     <span class="koboSpan" id="kobo.1088.4">
      Denker; D.
     </span>
     <span class="koboSpan" id="kobo.1088.5">
      Henderson; R.E.
     </span>
     <span class="koboSpan" id="kobo.1088.6">
      Howard; W.
     </span>
     <span class="koboSpan" id="kobo.1088.7">
      Hubbard; L.D.
     </span>
     <span class="koboSpan" id="kobo.1088.8">
      Jackel (1989) Advances in Neural Information Processing Systems, 1,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1089.1">
       323-331.
      </span>
     </span>
     <a href="https://doi.org/10.1162/neco.1989.1.4.541">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1090.1">
        https://doi.org/10.1162/neco.1989.1.4.541
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1091.1">
       Life_Architecture
      </span>
     </em>
     <span class="koboSpan" id="kobo.1092.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1093.1">
       Amazon Olympus (large language model due 2024H2
      </span>
     </em>
     <span class="koboSpan" id="kobo.1094.1">
      )”, Alan D.
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1095.1">
       Thompson,
      </span>
     </span>
     <a href="https://lifearchitect.ai/olympus/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1096.1">
        https://lifearchitect.ai/olympus/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1097.1">
       Llama3
      </span>
     </em>
     <span class="koboSpan" id="kobo.1098.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1099.1">
       Llama3 8b
      </span>
     </em>
     <span class="koboSpan" id="kobo.1100.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1101.1">
       Meta,
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.1102.1">
        https://llama.meta.com/llama3/
       </span>
      </strong>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1103.1">
       Mandlik
      </span>
     </em>
     <span class="koboSpan" id="kobo.1104.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1105.1">
       How GPT-4 Image Works
      </span>
     </em>
     <span class="koboSpan" id="kobo.1106.1">
      ?”, Sanman
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1107.1">
       Mandlik,
      </span>
     </span>
     <a href="https://sanmancreations.medium.com/how-gpt-4-image-works-4d7a87cf4497#:~:text=GPT%2D4%20Image%3A%20A%20Fusion,a%20pioneering%20advancement%20in%20AI">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1108.1">
        https://sanmancreations.medium.com/how-gpt-4-image-works-4d7a87cf4497#:~:text=GPT%2D4%20Image%3A%20A%20Fusion,a%20pioneering%20advancement%20in%20AI
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1109.1">
       Megatron-Turing 2022
      </span>
     </em>
     <span class="koboSpan" id="kobo.1110.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1111.1">
       Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model
      </span>
     </em>
     <span class="koboSpan" id="kobo.1112.1">
      ”, Shaden Smith; Mostofa Patwary; Brandon Norick; Patrick LeGresley; Samyam Rajbhandari; Jared Casper; Zhun Liu; Shrimai Prabhumoye; George Zerveas; Vijay Korthikanti; Elton Zhang; Rewon Child; Reza Yazdani Aminabadi; Julie Bernauer; Xia Song; Mohammad Shoeybi; Yuxiong He; Michael Houston; Saurabh Tiwary; Bryan
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1113.1">
       Catanzaro:
      </span>
     </span>
     <a href="https://arxiv.org/abs/2201.11990">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1114.1">
        https://arxiv.org/abs/2201.11990
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1115.1">
       Menon
      </span>
     </em>
     <span class="koboSpan" id="kobo.1116.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1117.1">
       Introduction to Large Language Models and the Transformer Architecture
      </span>
     </em>
     <span class="koboSpan" id="kobo.1118.1">
      ”, Pradeep
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1119.1">
       Menon,
      </span>
     </span>
     <a href="https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1120.1">
        https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1121.1">
       Metzger
      </span>
     </em>
     <span class="koboSpan" id="kobo.1122.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1123.1">
       A Beginner’s Guide to Tokens, Vectors, and Embeddings in NLP
      </span>
     </em>
     <span class="koboSpan" id="kobo.1124.1">
      ”, Sascha
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1125.1">
       Metzger,
      </span>
     </span>
     <a href="mailto:https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1126.1">
        https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1127.1">
       MotiveX_Gemini
      </span>
     </em>
     <span class="koboSpan" id="kobo.1128.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1129.1">
       Is GEMINI AI The Best?
      </span>
      <span class="koboSpan" id="kobo.1129.2">
       - SHOCKING Power (GPT-4 HUMBLED)
      </span>
     </em>
     <span class="koboSpan" id="kobo.1130.1">
      ”, MotiveX YouTube channel,
     </span>
     <a href="https://youtu.be/JvA9os8Oq20?t=144">
      <span class="koboSpan" id="kobo.1131.1">
       https://youtu.be/JvA9os8Oq20?t=144
      </span>
     </a>
     <span class="koboSpan" id="kobo.1132.1">
      , 6
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1133.1">
       Feb 2024
      </span>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1134.1">
       OpenAI_CLIP
      </span>
     </em>
     <span class="koboSpan" id="kobo.1135.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1136.1">
       CLIP: Connecting text and images
      </span>
     </em>
     <span class="koboSpan" id="kobo.1137.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1138.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/index/clip/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1139.1">
        https://openai.com/index/clip/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1140.1">
       OpenAI_DALL.E
      </span>
     </em>
     <span class="koboSpan" id="kobo.1141.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1142.1">
       DALL.E: Creating images from text
      </span>
     </em>
     <span class="koboSpan" id="kobo.1143.1">
      ”
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1144.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/index/dall-e/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1145.1">
        https://openai.com/index/dall-e/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1146.1">
       OpenAI-GPT-4o
      </span>
     </em>
     <span class="koboSpan" id="kobo.1147.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1148.1">
       Hello GPT-4o
      </span>
     </em>
     <span class="koboSpan" id="kobo.1149.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1150.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/index/hello-gpt-4o/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1151.1">
        https://openai.com/index/hello-gpt-4o/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1152.1">
       OpenAI_GPT4Turbo
      </span>
     </em>
     <span class="koboSpan" id="kobo.1153.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1154.1">
       New models and developer products announced at DevDay GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more
      </span>
     </em>
     <span class="koboSpan" id="kobo.1155.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1156.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1157.1">
        https://openai.com/blog/new-models-and-developer-products-announced-at-devday
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1158.1">
       OpenAI_LP
      </span>
     </em>
     <span class="koboSpan" id="kobo.1159.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1160.1">
       OpenAI LP
      </span>
     </em>
     <span class="koboSpan" id="kobo.1161.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1162.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/index/openai-lp/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1163.1">
        https://openai.com/index/openai-lp/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1164.1">
       OpenAIStructure
      </span>
     </em>
     <span class="koboSpan" id="kobo.1165.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1166.1">
       Our Structure
      </span>
     </em>
     <span class="koboSpan" id="kobo.1167.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1168.1">
       OpenAI,
      </span>
     </span>
     <a href="https://openai.com/our-structure/">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1169.1">
        https://openai.com/our-structure/
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1170.1">
       OpenLMR
      </span>
     </em>
     <span class="koboSpan" id="kobo.1171.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1172.1">
       OpenLLaMA: An Open Reproduction of LLaMA
      </span>
     </em>
     <span class="koboSpan" id="kobo.1173.1">
      ”, Xinyang (Young) Geng; Hao Liu; Martin,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1174.1">
       Jul,
      </span>
     </span>
     <a href="https://github.com/openlm-research/open_llama">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1175.1">
        https://github.com/openlm-research/open_llama
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1176.1">
       Panuganty
      </span>
     </em>
     <span class="koboSpan" id="kobo.1177.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1178.1">
       From Words to Vectors: Inside the LLM Transformer Architecture
      </span>
     </em>
     <span class="koboSpan" id="kobo.1179.1">
      ”, Harika
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1180.1">
       Panuganty:
      </span>
     </span>
     <a href="mailto:https://medium.com/@harikapanuganty/from-words-to-vectors-inside-the-llm-transformer-architecture-50275c354bc4">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1181.1">
        https://medium.com/@harikapanuganty/from-words-to-vectors-inside-the-llm-transformer-architecture-50275c354bc4
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1182.1">
       Patil
      </span>
     </em>
     <span class="koboSpan" id="kobo.1183.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1184.1">
       Top 5 Pre-trained Word Embeddings
      </span>
     </em>
     <span class="koboSpan" id="kobo.1185.1">
      ”, Aakanksha
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1186.1">
       Patil,
      </span>
     </span>
     <a href="https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1187.1">
        https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1188.1">
       ProjectPro
      </span>
     </em>
     <span class="koboSpan" id="kobo.1189.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1190.1">
       ChatGPT vs Google BARD-Battle of the Large Language Models
      </span>
     </em>
     <span class="koboSpan" id="kobo.1191.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1192.1">
       Manika,
      </span>
     </span>
     <a href="https://www.projectpro.io/article/chatgpt-vs-google-bard/815">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1193.1">
        https://www.projectpro.io/article/chatgpt-vs-google-bard/815
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1194.1">
       SkillLeapAI
      </span>
     </em>
     <span class="koboSpan" id="kobo.1195.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1196.1">
       How to Use Google Gemini in Bard - Including new prompts
      </span>
     </em>
     <span class="koboSpan" id="kobo.1197.1">
      ”, Skill Leap AI YouTube
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1198.1">
       channel,
      </span>
     </span>
     <a href="https://www.youtube.com/watch?v=9qszKWO68wQ">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1199.1">
        https://www.youtube.com/watch?v=9qszKWO68wQ
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1200.1">
       Vaswani
      </span>
     </em>
     <span class="koboSpan" id="kobo.1201.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1202.1">
       Attention Is All You Need
      </span>
     </em>
     <span class="koboSpan" id="kobo.1203.1">
      ”, Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N.
     </span>
     <span class="koboSpan" id="kobo.1203.2">
      Gomez; Lukasz Kaiser and Illia
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1204.1">
       Polosukhin,
      </span>
     </span>
     <a href="https://arxiv.org/abs/1706.03762">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1205.1">
        https://arxiv.org/abs/1706.03762
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1206.1">
       Wagh
      </span>
     </em>
     <span class="koboSpan" id="kobo.1207.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1208.1">
       What’s new in GPT-4: An Overview of the GPT-4 Architecture and Capabilities of Next-Generation AI
      </span>
     </em>
     <span class="koboSpan" id="kobo.1209.1">
      ”, Amol
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1210.1">
       Wagh,
      </span>
     </span>
     <span class="No-Break">
      <span class="P---URL">
      </span>
     </span>
     <a href="mailto:https://medium.com/@amol-wagh/whats-new-in-gpt-4-an-overview-of-the-gpt-4-architecture-and-capabilities-of-next-generation-ai-900c445d5ffe">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1211.1">
        https://medium.com/@amol-wagh/whats-new-in-gpt-4-an-overview-of-the-gpt-4-architecture-and-capabilities-of-next-generation-ai-900c445d5ffe
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1212.1">
       Watson 2023
      </span>
     </em>
     <span class="koboSpan" id="kobo.1213.1">
      : “Op
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1214.1">
       en Llama Unleashed: Revolutionizing AI for Business &amp;
      </span>
     </em>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.1215.1">
        Beyond!
       </span>
      </em>
     </span>
     <span class="No-Break">
     </span>
     <a href="https://medium.com/nextgen-tech/open-llama-unleashed-revolutionizing-ai-for-business-beyond-18de67aa0b9d">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1216.1">
        https://medium.com/nextgen-tech/open-llama-unleashed-revolutionizing-ai-for-business-beyond-18de67aa0b9d
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1217.1">
       Wiki_Gemini
      </span>
     </em>
     <span class="koboSpan" id="kobo.1218.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1219.1">
       Gemini (chatbot)
      </span>
     </em>
     <span class="koboSpan" id="kobo.1220.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1221.1">
       Wikipedia,
      </span>
     </span>
     <a href="https://en.wikipedia.org/wiki/Gemini_(chatbot)">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1222.1">
        https://en.wikipedia.org/wiki/Gemini_(chatbot)
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1223.1">
       Wiki_GPT3
      </span>
     </em>
     <span class="koboSpan" id="kobo.1224.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1225.1">
       GPT-3
      </span>
     </em>
     <span class="koboSpan" id="kobo.1226.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1227.1">
       Wikipedia,
      </span>
     </span>
     <a href="https://en.wikipedia.org/wiki/GPT-3">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1228.1">
        https://en.wikipedia.org/wiki/GPT-3
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1229.1">
       Wiki_GPT4
      </span>
     </em>
     <span class="koboSpan" id="kobo.1230.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1231.1">
       GPT-4
      </span>
     </em>
     <span class="koboSpan" id="kobo.1232.1">
      ”, ,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1233.1">
       Wikipedia,
      </span>
     </span>
     <a href="https://en.wikipedia.org/wiki/GPT-4">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1234.1">
        https://en.wikipedia.org/wiki/GPT-4
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1235.1">
       Wiki_llama
      </span>
     </em>
     <span class="koboSpan" id="kobo.1236.1">
      : (2024), “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1237.1">
       LLaMA
      </span>
     </em>
     <span class="koboSpan" id="kobo.1238.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1239.1">
       Wikipedia,
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.1240.1">
        https://en.wikipedia.org/wiki/LLaMA
       </span>
      </strong>
     </span>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1241.1">
       Wiki_LSTM
      </span>
     </em>
     <span class="koboSpan" id="kobo.1242.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1243.1">
       Recurrent Neural Network
      </span>
     </em>
     <span class="koboSpan" id="kobo.1244.1">
      ”,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1245.1">
       Wikipedia,
      </span>
     </span>
     <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network#:~:text=Long%20short%2Dterm%20memory%20(LSTM,models%20in%20certain%20speech%20applications">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1246.1">
        https://en.wikipedia.org/wiki/Recurrent_neural_network#:~:text=Long%20short%2Dterm%20memory%20(LSTM,models%20in%20certain%20speech%20applications
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1247.1">
       Yang2023
      </span>
     </em>
     <span class="koboSpan" id="kobo.1248.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1249.1">
       Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
      </span>
     </em>
     <span class="koboSpan" id="kobo.1250.1">
      ”, Jingfeng Yang; Hongye Jin; Ruixiang Tang; Xiaotian Han; Qizhang Feng; Haoming Jiang; Bing Yin and Xia
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1251.1">
       Hu,
      </span>
     </span>
     <a href="https://arxiv.org/abs/2304.13712">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1252.1">
        https://arxiv.org/abs/2304.13712
       </span>
      </span>
     </a>
    </li>
    <li>
     <em class="italic">
      <span class="koboSpan" id="kobo.1253.1">
       Zahere
      </span>
     </em>
     <span class="koboSpan" id="kobo.1254.1">
      : “
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.1255.1">
       How ChatGPT Works: The Architectural Details You Need to Know
      </span>
     </em>
     <span class="koboSpan" id="kobo.1256.1">
      ”, Zahiruddin
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.1257.1">
       Tavargere,
      </span>
     </span>
     <a href="https://zahere.com/how-chatgpt-works-the-architectural-details-you-need-to-know">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.1258.1">
        https://zahere.com/how-chatgpt-works-the-architectural-details-you-need-to-know
       </span>
      </span>
     </a>
    </li>
   </ul>
  </div>
 </body></html>