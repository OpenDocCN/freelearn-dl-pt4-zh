["```py\nimport time \nimport gymnasium as gym \nimport miniwob \nfrom miniwob.action import ActionTypes \n\nRENDER_ENV = True \n\nif __name__ == \"__main__\": \n    gym.register_envs(miniwob)\n```", "```py\n env = gym.make(’miniwob/click-test-2-v1’, render_mode=’human’ if RENDER_ENV else None) \n    print(env) \n    try: \n        obs, info = env.reset() \n        print(\"Obs keys:\", list(obs.keys())) \n        print(\"Info dict:\", info) \n        assert obs[\"utterance\"] == \"Click button ONE.\" \n        assert obs[\"fields\"] == ((\"target\", \"ONE\"),) \n        print(\"Screenshot shape:\", obs[’screenshot’].shape)\n```", "```py\n$ python adhoc/01_wob_create.py \n<OrderEnforcing<PassiveEnvChecker<ClickTest2Env<miniwob/click-test-2-v1>>>> \nObs keys: [’utterance’, ’dom_elements’, ’screenshot’, ’fields’] \nInfo dict: {’done’: False, ’env_reward’: 0, ’raw_reward’: 0, ’reason’: None, ’root_dom’: [1] body @ (0, 0) classes=[] children=1} \nScreenshot shape: (210, 160, 3)\n```", "```py\n if RENDER_ENV: \n            time.sleep(2) \n\n        target_elems = [e for e in obs[’dom_elements’] if e[’text’] == \"ONE\"] \n        assert target_elems \n        print(\"Target elem:\", target_elems[0])\n```", "```py\nTarget elem: {’ref’: 4, ’parent’: 3, ’left’: array([80.], dtype=float32), ’top’: array([134.], dtype=float32), ’width’: array([40.], dtype=float32), ’height’: array([40.], dtype=float32), ’tag’: ’button’, ’text’: ’ONE’, ’value’: ’’, ’id’: ’subbtn’, ’classes’: ’’, ’bg_color’: array([0.9372549, 0.9372549, 0.9372549, 1\\.      ], dtype=float32), ’fg_color’: array([0., 0., 0., 1.], dtype=float32), ’flags’: array([0, 0, 0, 1], dtype=int8)}\n```", "```py\n action = env.unwrapped.create_action( \n            ActionTypes.CLICK_ELEMENT, ref=target_elems[0][\"ref\"]) \n        obs, reward, terminated, truncated, info = env.step(action) \n        print(reward, terminated, info) \n    finally: \n        env.close()\n```", "```py\n0.7936 True {’done’: True, ’env_reward’: 0.7936, ’raw_reward’: 1, ’reason’: None, ’elapsed’: 2.066638231277466}\n```", "```py\nWIDTH = 160 \nHEIGHT = 210 \nX_OFS = 0 \nY_OFS = 50 \nBIN_SIZE = 10 \nWOB_SHAPE = (3, HEIGHT, WIDTH) \n\nclass MiniWoBClickWrapper(gym.ObservationWrapper): \n    FULL_OBS_KEY = \"full_obs\" \n\n    def __init__(self, env: gym.Env, keep_text: bool = False, \n                 keep_obs: bool = False, bin_size: int = BIN_SIZE): \n        super(MiniWoBClickWrapper, self).__init__(env) \n        self.bin_size = bin_size \n        self.keep_text = keep_text \n        self.keep_obs = keep_obs \n        img_space = spaces.Box(low=0, high=255, shape=WOB_SHAPE, dtype=np.uint8) \n        if keep_text: \n            self.observation_space = spaces.Tuple( \n                (img_space, spaces.Text(max_length=1024))) \n        else: \n            self.observation_space = img_space \n        self.x_bins = WIDTH // bin_size \n        count = self.x_bins * ((HEIGHT - Y_OFS) // bin_size) \n        self.action_space = spaces.Discrete(count)\n```", "```py\n @classmethod \n    def create(cls, env_name: str, bin_size: int = BIN_SIZE, keep_text: bool = False, \n               keep_obs: bool = False, **kwargs) -> \"MiniWoBClickWrapper\": \n        gym.register_envs(miniwob) \n        x_bins = WIDTH // bin_size \n        y_bins = (HEIGHT - Y_OFS) // bin_size \n        act_cfg = ActionSpaceConfig( \n            action_types=(ActionTypes.CLICK_COORDS, ), coord_bins=(x_bins, y_bins)) \n        env = gym.make(env_name, action_space_config=act_cfg, **kwargs) \n        return MiniWoBClickWrapper( \n            env, keep_text=keep_text, keep_obs=keep_obs, bin_size=bin_size)\n```", "```py\n def _observation(self, observation: dict) -> np.ndarray | tt.Tuple[np.ndarray, str]: \n        text = observation[’utterance’] \n        scr = observation[’screenshot’] \n        scr = np.transpose(scr, (2, 0, 1)) \n        if self.keep_text: \n            return scr, text \n        return scr \n\n    def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None) \\ \n            -> tuple[gym.core.WrapperObsType, dict[str, tt.Any]]: \n        obs, info = self.env.reset(seed=seed, options=options) \n        if self.keep_obs: \n            info[self.FULL_OBS_KEY] = obs \n        return self._observation(obs), info\n```", "```py\n def step(self, action: int) -> tt.Tuple[ \n        gym.core.WrapperObsType, gym.core.SupportsFloat, bool, bool, dict[str, tt.Any] \n    ]: \n        b_x, b_y = action_to_bins(action, self.bin_size) \n        new_act = { \n            \"action_type\": 0, \n            \"coords\": np.array((b_x, b_y), dtype=np.int8), \n        } \n        obs, reward, is_done, is_tr, info = self.env.step(new_act) \n        if self.keep_obs: \n            info[self.FULL_OBS_KEY] = obs \n        return self._observation(obs), reward, is_done, is_tr, info \n\ndef action_to_bins(action: int, bin_size: int = BIN_SIZE) -> tt.Tuple[int, int]: \n    row_bins = WIDTH // bin_size \n    b_y = action // row_bins \n    b_x = action % row_bins \n    return b_x, b_y\n```", "```py\nclass Model(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): \n        super(Model, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 64, 5, stride=5), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, 3, stride=2), \n            nn.ReLU(), \n            nn.Flatten(), \n        ) \n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.policy = nn.Linear(size, n_actions) \n        self.value = nn.Linear(size, 1) \n\n    def forward(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]: \n        xx = x / 255.0 \n        conv_out = self.conv(xx) \n        return self.policy(conv_out), self.value(conv_out)\n```", "```py\n$ ./wob_click_play.py -m saves/best_0.923_45400.dat --verbose \n0 0.0 False {’done’: False, ’env_reward’: 0, ’raw_reward’: 0, ’reason’: None, ’elapsed’: 0.1620042324066162, ’root_dom’: [1] body @ (0, 0) classes=[] children=2} \n1 0.9788 True {’done’: True, ’env_reward’: 0.9788, ’raw_reward’: 1, ’reason’: None, ’elapsed’: 0.19491100311279297} \nRound 0 done \nDone 1 rounds, mean steps 2.00, mean reward 0.979\n```", "```py\nMM_EMBEDDINGS_DIM = 50 \nMM_HIDDEN_SIZE = 128 \nMM_MAX_DICT_SIZE = 100 \n\nTOKEN_UNK = \"#unk\" \n\nclass MultimodalPreprocessor: \n    log = logging.getLogger(\"MulitmodalPreprocessor\") \n\n    def __init__(self, max_dict_size: int = MM_MAX_DICT_SIZE, \n                 device: torch.device = torch.device(’cpu’)): \n        self.max_dict_size = max_dict_size \n        self.token_to_id = {TOKEN_UNK: 0} \n        self.next_id = 1 \n        self.tokenizer = TweetTokenizer(preserve_case=True) \n        self.device = device \n\n    def __len__(self): \n        return len(self.token_to_id)\n```", "```py\n def __call__(self, batch: tt.Tuple[tt.Any, ...] | tt.List[tt.Tuple[tt.Any, ...]]): \n        tokens_batch = [] \n\n        if isinstance(batch, tuple): \n            batch_iter = zip(*batch) \n        else: \n            batch_iter = batch \n        for img_obs, txt_obs in batch_iter: \n            tokens = self.tokenizer.tokenize(txt_obs) \n            idx_obs = self.tokens_to_idx(tokens) \n            tokens_batch.append((img_obs, idx_obs)) \n        tokens_batch.sort(key=lambda p: len(p[1]), reverse=True) \n        img_batch, seq_batch = zip(*tokens_batch) \n        lens = list(map(len, seq_batch))\n```", "```py\n img_v = torch.FloatTensor(np.asarray(img_batch)).to(self.device) \n        seq_arr = np.zeros( \n            shape=(len(seq_batch), max(len(seq_batch[0]), 1)), dtype=np.int64) \n        for idx, seq in enumerate(seq_batch): \n            seq_arr[idx, :len(seq)] = seq \n            if len(seq) == 0: \n                lens[idx] = 1 \n        seq_v = torch.LongTensor(seq_arr).to(self.device) \n        seq_p = rnn_utils.pack_padded_sequence(seq_v, lens, batch_first=True) \n        return img_v, seq_p\n```", "```py\n def tokens_to_idx(self, tokens): \n        res = [] \n        for token in tokens: \n            idx = self.token_to_id.get(token) \n            if idx is None: \n                if self.next_id == self.max_dict_size: \n                    self.log.warning(\"Maximum size of dict reached, token \" \n                                     \"’%s’ converted to #UNK token\", token) \n                    idx = 0 \n                else: \n                    idx = self.next_id \n                    self.next_id += 1 \n                    self.token_to_id[token] = idx \n            res.append(idx) \n        return res\n```", "```py\nclass ModelMultimodal(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int, \n                 max_dict_size: int = MM_MAX_DICT_SIZE): \n        super(ModelMultimodal, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 64, 5, stride=5), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, 3, stride=2), \n            nn.ReLU(), \n            nn.Flatten(), \n        ) \n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n\n        self.emb = nn.Embedding(max_dict_size, MM_EMBEDDINGS_DIM) \n        self.rnn = nn.LSTM(MM_EMBEDDINGS_DIM, MM_HIDDEN_SIZE, batch_first=True) \n        self.policy = nn.Linear(size + MM_HIDDEN_SIZE*2, n_actions) \n        self.value = nn.Linear(size + MM_HIDDEN_SIZE*2, 1)\n```", "```py\n def _concat_features(self, img_out: torch.Tensor, \n                         rnn_hidden: torch.Tensor | tt.Tuple[torch.Tensor, ...]): \n        batch_size = img_out.size()[0] \n        if isinstance(rnn_hidden, tuple): \n            flat_h = list(map(lambda t: t.view(batch_size, -1), rnn_hidden)) \n            rnn_h = torch.cat(flat_h, dim=1) \n        else: \n            rnn_h = rnn_hidden.view(batch_size, -1) \n        return torch.cat((img_out, rnn_h), dim=1)\n```", "```py\n def forward(self, x: tt.Tuple[torch.Tensor, rnn_utils.PackedSequence]): \n        x_img, x_text = x \n\n        emb_out = self.emb(x_text.data) \n        emb_out_seq = rnn_utils.PackedSequence(emb_out, x_text.batch_sizes) \n        rnn_out, rnn_h = self.rnn(emb_out_seq) \n\n        xx = x_img / 255.0 \n        conv_out = self.conv(xx) \n        feats = self._concat_features(conv_out, rnn_h) \n        return self.policy(feats), self.value(feats)\n```", "```py\n$ ./record_demo.py -o demos/test -g tic-tac-toe-v1 -d 1 \nBottle v0.12.25 server starting up (using WSGIRefServer())... \nListening on http://localhost:8032/ \nHit Ctrl-C to quit. \n\nWARNING:root:Cannot call {’action_type’: 0} on instance 0, which is already done \n127.0.0.1 - - [26/Apr/2024 12:19:49] \"POST /record HTTP/1.1\" 200 17 \nSaved in  demos/test/tic-tac-toe_0426101949.json \nNew episode starts in 1 seconds...\n```", "```py\n demo_samples = None \n    if args.demo: \n        demo_samples = demos.load_demo_dir(args.demo, gamma=GAMMA, steps=REWARD_STEPS) \n        print(f\"Loaded {len(demo_samples)} demo samples\")\n```", "```py\n if demo_samples and step_idx < DEMO_FRAMES: \n                    if random.random() < DEMO_PROB: \n                        random.shuffle(demo_samples) \n                        demo_batch = demo_samples[:BATCH_SIZE] \n                        model.train_demo(net, optimizer, demo_batch, writer, \n                                         step_idx, device=device)\n```", "```py\ndef train_demo(net: Model, optimizer: torch.optim.Optimizer, \n               batch: tt.List[ptan.experience.ExperienceFirstLast], writer, step_idx: int, \n               preprocessor=ptan.agent.default_states_preprocessor, \n               device: torch.device = torch.device(\"cpu\")): \n    batch_obs, batch_act = [], [] \n    for e in batch: \n        batch_obs.append(e.state) \n        batch_act.append(e.action) \n    batch_v = preprocessor(batch_obs) \n    if torch.is_tensor(batch_v): \n        batch_v = batch_v.to(device) \n    optimizer.zero_grad() \n    ref_actions_v = torch.LongTensor(batch_act).to(device) \n    policy_v = net(batch_v)[0] \n    loss_v = F.cross_entropy(policy_v, ref_actions_v) \n    loss_v.backward() \n    optimizer.step() \n    writer.add_scalar(\"demo_loss\", loss_v.item(), step_idx)\n```"]