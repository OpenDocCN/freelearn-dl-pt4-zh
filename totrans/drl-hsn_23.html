<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 likechapterHead" id="bibliography">
<h1 class="mainHeading"><span id="x1-425000"/>Bibliography</h1>
<dl>
<dt>[Sut88]</dt>
<dd>
<p><span id="cite.0@sutton1988learning"/>Richard S Sutton. “Learning to predict by the methods of temporal differences”. In: <span class="cmti-10x-x-109">Machine learning </span>3 (1988), pp. 9–44.</p>
</dd>
<dt>[HS96]</dt>
<dd>
<p><span id="cite.0@hochreiter1996lstm"/>Sepp Hochreiter and Jürgen Schmidhuber. “LSTM can solve hard long time lag problems”. In: <span class="cmti-10x-x-109">Advances in neural</span> <span class="cmti-10x-x-109">information processing systems </span>9 (1996).</p>
</dd>
<dt>[RK04]</dt>
<dd>
<p><span id="cite.0@rubinstein2004cross"/>Reuven Y Rubinstein and Dirk P Kroese. <span class="cmti-10x-x-109">The cross-entropy</span> <span class="cmti-10x-x-109">method: a unified approach to combinatorial optimization,</span> <span class="cmti-10x-x-109">Monte-Carlo simulation, and machine learning</span>. Vol. 133. Springer, 2004.</p>
</dd>
<dt>[SL08]</dt>
<dd>
<p><span id="cite.0@strehl2008analysis"/>Alexander L Strehl and Michael L Littman. “An analysis of model-based interval estimation for Markov decision processes”. In: <span class="cmti-10x-x-109">Journal of Computer and System Sciences </span>74.8 (2008), pp. 1309–1331.</p>
</dd>
<dt>[Kro+11]</dt>
<dd>
<p><span id="cite.0@kroese2011cross"/>Dirk P Kroese et al. “Cross-entropy method’”. In: <span class="cmti-10x-x-109">European</span> <span class="cmti-10x-x-109">Journal of Operational Research </span>31 (2011), pp. 276–283.</p>
</dd>
<dt>[LS11]</dt>
<dd>
<p><span id="cite.0@lehman2011abandoning"/>Joel Lehman and Kenneth O Stanley. “Abandoning objectives: Evolution through the search for novelty alone”. In: <span class="cmti-10x-x-109">Evolutionary computation </span>19.2 (2011), pp. 189–223.</p>
</dd>
<dt>[Mni13]</dt>
<dd>
<p><span id="cite.0@mnih2013playing"/>Volodymyr Mnih. “Playing atari with deep reinforcement learning”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1312.5602 </span>(2013).</p>
</dd>
<dt>[Sil+14]</dt>
<dd>
<p><span id="cite.0@silver2014deterministic"/>David Silver et al. “Deterministic policy gradient algorithms”. In: <span class="cmti-10x-x-109">International conference on machine learning</span>. Pmlr. 2014, pp. 387–395.</p>
</dd>
<dt>[Lil15]</dt>
<dd>
<p><span id="cite.0@lillicrap2015continuous"/>TP Lillicrap. “Continuous control with deep reinforcement learning”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1509.02971 </span>(2015).</p>
</dd>
<dt>[MG15]</dt>
<dd>
<p><span id="cite.0@martens2015optimizing"/>James Martens and Roger Grosse. “Optimizing neural networks with kronecker-factored approximate curvature”. In: <span class="cmti-10x-x-109">International conference on machine learning</span>. PMLR. 2015, pp. 2408–2417.</p>
</dd>
<dt>[Mni+15]</dt>
<dd>
<p><span id="cite.0@mnih2015human"/>Volodymyr Mnih et al. “Human-level control through deep reinforcement learning”. In: <span class="cmti-10x-x-109">nature </span>518.7540 (2015), pp. 529–533.</p>
</dd>
<dt>[Sch+15]</dt>
<dd>
<p><span id="cite.0@prioreplay"/>Tom Schaul et al. “Prioritized Experience Replay”. In: (2015). arXiv: <a href="https://arxiv.org/abs/1511.05952">1511.05952 <span class="cmtt-10x-x-109">[cs.LG]</span></a>. <span class="cmcsc-10x-x-109"><span class="small-caps">u</span><span class="small-caps">r</span><span class="small-caps">l</span></span>: <a class="url" href="https://arxiv.org/abs/1511.05952"><span class="cmtt-10x-x-109">https://arxiv.org/abs/1511.05952</span></a>.</p>
</dd>
<dt>[Sch15]</dt>
<dd>
<p><span id="cite.0@schulman2015trust"/>John Schulman. “Trust Region Policy Optimization”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1502.05477 </span>(2015).</p>
</dd>
<dt>[VGS16]</dt>
<dd>
<p><span id="cite.0@van2016deep"/>Hado Van Hasselt, Arthur Guez, and David Silver. “Deep reinforcement learning with double q-learning”. In: <span class="cmti-10x-x-109">Proceedings of the AAAI conference on artificial intelligence</span>. Vol. 30. 1. 2016.</p>
</dd>
<dt>[Wan+16]</dt>
<dd>
<p><span id="cite.0@wang2016dueling"/>Ziyu Wang et al. “Dueling network architectures for deep reinforcement learning”. In: <span class="cmti-10x-x-109">International conference on</span> <span class="cmti-10x-x-109">machine learning</span>. PMLR. 2016, pp. 1995–2003.</p>
</dd>
<dt>[BDM17]</dt>
<dd>
<p><span id="cite.0@bellemare2017distributional"/>Marc G Bellemare, Will Dabney, and Rémi Munos. “A distributional perspective on reinforcement learning”. In: <span class="cmti-10x-x-109">International conference on machine learning</span>. PMLR. 2017, pp. 449–458.</p>
</dd>
<dt>[Chr+17]</dt>
<dd>
<p><span id="cite.0@rlhf"/>Paul Christiano et al. <span class="cmti-10x-x-109">Deep reinforcement learning from</span> <span class="cmti-10x-x-109">human preferences</span>. 2017. eprint: <a class="url" href="#"><span class="cmtt-10x-x-109">arXiv:1706.03741</span></a>.</p>
</dd>
<dt>[For+17]</dt>
<dd>
<p><span id="cite.0@noisynets"/>Meire Fortunato et al. “Noisy Networks for Exploration”. In: (2017). arXiv: <a href="https://arxiv.org/abs/1706.10295">1706.10295 <span class="cmtt-10x-x-109">[cs.LG]</span></a>. <span class="cmcsc-10x-x-109"><span class="small-caps">u</span><span class="small-caps">r</span><span class="small-caps">l</span></span>: <a class="url" href="https://arxiv.org/abs/1706.10295"><span class="cmtt-10x-x-109">https://arxiv.org/abs/1706.10295</span></a>.</p>
</dd>
<dt>[Mar+17]</dt>
<dd>
<p><span id="cite.0@martin2017count"/>Jarryd Martin et al. “Count-based exploration in feature space for reinforcement learning”. In: <span class="cmti-10x-x-109">arXiv preprint</span> <span class="cmti-10x-x-109">arXiv:1706.08090 </span>(2017).</p>
</dd>
<dt>[Ost+17]</dt>
<dd>
<p><span id="cite.0@ostrovski2017count"/>Georg Ostrovski et al. “Count-based exploration with neural density models”. In: <span class="cmti-10x-x-109">International conference on machine</span> <span class="cmti-10x-x-109">learning</span>. PMLR. 2017, pp. 2721–2730.</p>
</dd>
<dt>[Sal+17]</dt>
<dd>
<p><span id="cite.0@salimans2017evolution"/>Tim Salimans et al. “Evolution strategies as a scalable alternative to reinforcement learning”. In: <span class="cmti-10x-x-109">arXiv preprint</span> <span class="cmti-10x-x-109">arXiv:1703.03864 </span>(2017).</p>
</dd>
<dt>[Sch+17]</dt>
<dd>
<p><span id="cite.0@schulman2017proximal"/>John Schulman et al. “Proximal policy optimization algorithms”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1707.06347 </span>(2017).</p>
</dd>
<dt>[SSa17]</dt>
<dd>
<p><span id="cite.0@alphago"/>David Silver, Julian Schrittwieser, and Karen Simonyan et al. <span class="cmti-10x-x-109">Mastering the game of Go without human knowledge</span>. 2017. eprint: <a class="url" href="#"><span class="cmtt-10x-x-109">10.1038/nature24270</span></a>.</p>
</dd>
<dt>[Sil+17]</dt>
<dd>
<p><span id="cite.0@shogi"/>David Silver et al. <span class="cmti-10x-x-109">Mastering Chess and Shogi by</span> <span class="cmti-10x-x-109">Self-Play with a General Reinforcement Learning</span> <span class="cmti-10x-x-109">Algorithm</span>. 2017. arXiv: <a href="https://arxiv.org/abs/1712.01815">1712.01815 <span class="cmtt-10x-x-109">[cs.AI]</span></a>. <span class="cmcsc-10x-x-109"><span class="small-caps">u</span><span class="small-caps">r</span><span class="small-caps">l</span></span>: <a class="url" href="https://arxiv.org/abs/1712.01815"><span class="cmtt-10x-x-109">https://arxiv.org/abs/1712.01815</span></a>.</p>
</dd>
<dt>[Suc+17]</dt>
<dd>
<p><span id="cite.0@such2017deep"/>Felipe Petroski Such et al. “Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning”. In: <span class="cmti-10x-x-109">arXiv</span> <span class="cmti-10x-x-109">preprint arXiv:1712.06567 </span>(2017).</p>
</dd>
<dt>[Vas17]</dt>
<dd>
<p><span id="cite.0@vaswani2017attention"/>A Vaswani. “Attention is all you need”. In: <span class="cmti-10x-x-109">Advances in</span> <span class="cmti-10x-x-109">Neural Information Processing Systems </span>(2017).</p>
</dd>
<dt>[Wu+17]</dt>
<dd>
<p><span id="cite.0@wu2017scalable"/>Yuhuai Wu et al. “Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation”. In: <span class="cmti-10x-x-109">Advances in neural information</span> <span class="cmti-10x-x-109">processing systems </span>30 (2017).</p>
</dd>
<dt>[Bar+18]</dt>
<dd>
<p><span id="cite.0@barth2018distributed"/>Gabriel Barth-Maron et al. “Distributed distributional deterministic policy gradients”. In: <span class="cmti-10x-x-109">arXiv preprint</span> <span class="cmti-10x-x-109">arXiv:1804.08617 </span>(2018).</p>
</dd>
<dt>[Bur+18]</dt>
<dd>
<p><span id="cite.0@burda2018exploration"/>Yuri Burda et al. “Exploration by random network distillation”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1810.12894 </span>(2018).</p>
</dd>
<dt>[Haa+18]</dt>
<dd>
<p><span id="cite.0@haarnoja2018soft"/>Tuomas Haarnoja et al. “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor”. In: <span class="cmti-10x-x-109">International conference on machine</span> <span class="cmti-10x-x-109">learning</span>. PMLR. 2018, pp. 1861–1870.</p>
</dd>
<dt>[Hes+18]</dt>
<dd>
<p><span id="cite.0@hessel2018rainbow"/>Matteo Hessel et al. “Rainbow: Combining improvements in deep reinforcement learning”. In: <span class="cmti-10x-x-109">Proceedings of the AAAI</span> <span class="cmti-10x-x-109">conference on artificial intelligence</span>. Vol. 32. 1. 2018.</p>
</dd>
<dt>[McA+18]</dt>
<dd>
<p><span id="cite.0@rubikcube"/>Stephen McAleer et al. “Solving the Rubik’s cube without human knowledge”. In: <span class="cmti-10x-x-109">arXiv preprint arXiv:1805.07470</span> (2018).</p>
</dd>
<dt>[Bak+20]</dt>
<dd>
<p><span id="cite.0@baker2020emergenttoolusemultiagent"/>Bowen Baker et al. <span class="cmti-10x-x-109">Emergent Tool Use From Multi-Agent</span> <span class="cmti-10x-x-109">Autocurricula</span>. 2020. arXiv: <a href="https://arxiv.org/abs/1909.07528">1909.07528 <span class="cmtt-10x-x-109">[cs.LG]</span></a>. <span class="cmcsc-10x-x-109"><span class="small-caps">u</span><span class="small-caps">r</span><span class="small-caps">l</span></span>: <a class="url" href="https://arxiv.org/abs/1909.07528"><span class="cmtt-10x-x-109">https://arxiv.org/abs/1909.07528</span></a>.</p>
</dd>
<dt>[FS20]</dt>
<dd>
<p><span id="cite.0@cubikmath"/>Alexander H Frey Jr and David Singmaster. “Handbook of cubik math”. In: (2020).</p>
</dd>
<dt>[Sch+20]</dt>
<dd>
<p><span id="cite.0@muzero"/>Julian Schrittwieser et al. “Mastering Atari, Go, chess and shogi by planning with a learned model”. In: <span class="cmti-10x-x-109">Nature </span>588.7839 (Dec. 2020), pp. 604–609. <span class="cmcsc-10x-x-109"><span class="small-caps">i</span><span class="small-caps">s</span><span class="small-caps">s</span><span class="small-caps">n</span></span>: 1476-4687. <span class="cmcsc-10x-x-109"><span class="small-caps">d</span><span class="small-caps">o</span><span class="small-caps">i</span></span>: <a href="https://doi.org/10.1038/s41586-020-03051-4">10.1038/s41586-020-03051-4</a>. <span class="cmcsc-10x-x-109"><span class="small-caps">u</span><span class="small-caps">r</span><span class="small-caps">l</span></span>: <a class="url" href="http://dx.doi.org/10.1038/s41586-020-03051-4"><span class="cmtt-10x-x-109">http://dx.doi.org/10.1038/s41586-020-03051-4</span></a>.</p>
</dd>
<dt>[BDR23]</dt>
<dd>
<p><span id="cite.0@bellemare2023distributional"/>Marc G Bellemare, Will Dabney, and Mark Rowland. <span class="cmti-10x-x-109">Distributional reinforcement learning</span>. MIT Press, 2023.</p>
</dd>
</dl>
<p><img alt="PIC" height="50" src="../Images/file0.png" width="170"/></p>
<p><a class="url" href="https://www.packt.com"><span class="cmtt-10x-x-109">www.packt.com</span></a></p>
<p>Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
<section class="level3 likesectionHead" id="why-subscribe">
<h1 class="heading-1" id="sigil_toc_id_386"><span id="x1-426000"/>Why subscribe?</h1>
<ul>
<li>
<p>Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</p>
</li>
<li>
<p>Improve your learning with Skill Plans built especially for you</p>
</li>
<li>
<p>Get a free eBook or video every month</p>
</li>
<li>
<p>Fully searchable for easy access to vital information</p>
</li>
<li>
<p>Copy and paste, print, and bookmark content</p>
</li>
</ul>
<p>At <a class="url" href="https://www.packt.com"><span class="cmtt-10x-x-109">www.packt.com</span></a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
</section>
</section>
</div>

<div id="sbo-rt-content"><section class="level2 chapterHead" id="other-books-you-may-enjoy">
<p class="eop"/>
<h1 class="mainHeading"><span id="x1-427000"/>Other Books You May Enjoy</h1>
<p>If you enjoyed this book, you may be interested in these other books by Packt:</p>
<p><a href="https://www.packtpub.com/en-us/product/mastering-pytorch-9781801074308"><img alt="PIC" height="250" src="../Images/file350.png" width="200"/></a></p>
<p><span class="cmbx-10x-x-109">Mastering PyTorch</span></p>
<p>Ashish Ranjan Jha</p>
<p>ISBN: 9781801074308</p>
<ul>
<li>
<p>Implement text, vision, and music generation models using PyTorch</p>
</li>
<li>
<p>Build a deep Q-network (DQN) model in PyTorch</p>
</li>
<li>
<p>Deploy PyTorch models on mobile devices (Android and iOS)</p>
</li>
<li>
<p>Become well versed in rapid prototyping using PyTorch with fastai</p>
</li>
<li>
<p>Perform neural architecture search effectively using AutoML</p>
</li>
<li>
<p>Easily interpret machine learning models using Captum</p>
</li>
<li>
<p>Design ResNets, LSTMs, and graph neural networks (GNNs)</p>
</li>
<li>
<p>Create language and vision transformer models using Hugging Face</p>
</li>
</ul><p class="eop"/>
<p><a href="https://www.packtpub.com/en-us/product/python-for-algorithmic-trading-cookbook-9781835084700"><img alt="PIC" height="250" src="../Images/file351.png" width="200"/></a></p>
<p><span class="cmbx-10x-x-109">Python for Algorithmic Trading Cookbook</span></p>
<p>Jason Strimpel</p>
<p>ISBN: 9781835084700</p>
<ul>
<li>
<p>Acquire and process freely available market data with the OpenBB Platform</p>
</li>
<li>
<p>Build a research environment and populate it with financial market data</p>
</li>
<li>
<p>Use machine learning to identify alpha factors and engineer them into signals</p>
</li>
<li>
<p>Use VectorBT to find strategy parameters using walk-forward optimization</p>
</li>
<li>
<p>Build production-ready backtests with Zipline Reloaded and evaluate factor performance</p>
</li>
<li>
<p>Set up the code framework to connect and send an order to Interactive Brokers</p>
</li>
</ul>
<section class="level3 likesectionHead" id="packt-is-searching-for-authors-like-you">
<p class="eop"/>
<h1 class="heading-1" id="sigil_toc_id_387"><span id="x1-428000"/>Packt is searching for authors like you</h1>
<p>If you’re interested in becoming an author for Packt, please visit <a class="url" href="https://authors.packtpub.com"><span class="cmtt-10x-x-109">authors.packtpub.com</span></a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
</section>
</section>
</div></body></html>