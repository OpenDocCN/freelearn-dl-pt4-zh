- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym API and Gymnasium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After talking so much about the theoretical concepts of reinforcement learning
    (RL) in Chapter [1](ch005.xhtml#x1-190001), let’s start doing something practical.
    In this chapter, you will learn the basics of Gymnasium, a library used to provide
    a uniform API for an RL agent and lots of RL environments. Originally, this API
    was implemented in the OpenAI Gym library, but it is no longer maintained. In
    this book, we’ll use Gymnasium—a fork of OpenAI Gym implementing the same API.
    In any case, having a uniform API for environments removes the need to write boilerplate
    code and allows you to implement an agent in a general way without worrying about
    environment details.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also write your first randomly behaving agent and become more familiar
    with the basic concepts of RL that we have covered so far. By the end of the chapter,
    you will have an understanding of:'
  prefs: []
  type: TYPE_NORMAL
- en: The high-level requirements that need to be implemented to plug the agent into
    the RL framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A basic, pure-Python implementation of the random RL agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI Gym API and its implementation – the Gymnasium library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The anatomy of the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you learned in the previous chapter, there are several fundamental concepts
    in RL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent: A thing, or person, that takes an active role. In practice, the
    agent is some piece of code that implements some policy. Basically, this policy
    decides what action is needed at every time step, given our observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The environment: Everything that is external to the agent and has the responsibility
    of providing observations and giving rewards. The environment changes its state
    based on the agent’s actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore how both can be implemented in Python for a simple situation.
    We will define an environment that will give the agent random rewards for a limited
    number of steps, regardless of the agent’s actions. This scenario is not very
    useful in the real world, but it will allow us to focus on specific methods in
    both the environment and agent classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the code snippets shown in this book are not full examples.
    You can find the full examples on the GitHub page: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)
    and run them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we allowed the environment to initialize its internal
    state. In our case, the state is just a counter that limits the number of time
    steps that the agent is allowed to take to interact with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The get_observation() method is supposed to return the current environment’s
    observation to the agent. It is usually implemented as some function of the internal
    state of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re curious about what is meant by -> List[float], that’s an example
    of Python type annotations, which were introduced in Python 3.5\. You can find
    out more in the documentation at [https://docs.python.org/3/library/typing.xhtml](https://docs.python.org/3/library/typing.xhtml).
    In our example, the observation vector is always zero, as the environment basically
    has no internal state. The get_actions() method allows the agent to query the
    set of actions it can execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Normally, the set of actions does not change over time, but some actions can
    become impossible in different states (for example, not every move is possible
    in any position of the tic-tac-toe game). In our simplistic example, there are
    only two actions that the agent can carry out, which are encoded with the integers
    0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following method signals the end of the episode to the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you saw in Chapter [1](ch005.xhtml#x1-190001), the series of environment-agent
    interactions is divided into a sequence of steps called episodes. Episodes can
    be finite, like in a game of chess, or infinite, like the Voyager 2 mission (a
    famous space probe that was launched over 46 years ago and has traveled beyond
    our solar system). To cover both scenarios, the environment provides us with a
    way to detect when an episode is over and there is no way to communicate with
    it anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The action() method is the central piece in the environment’s functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It does two things – handles an agent’s action and returns the reward for this
    action. In our example, the reward is random and its action is discarded. Additionally,
    we update the count of steps and don’t continue the episodes that are over.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when looking at the agent’s part, it is much simpler and includes only
    two methods: the constructor and the method that performs one step in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the constructor, we initialize the counter that will keep the total reward
    accumulated by the agent during the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The step() function accepts the environment instance as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This function allows the agent to perform the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Observe the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a decision about the action to take based on the observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit the action to the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the reward for the current step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our example, the agent is dull and ignores the observations obtained during
    the decision-making process about which action to take. Instead, every action
    is selected randomly. The final piece is the glue code, which creates both classes
    and runs one episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the full code in this book’s GitHub repository at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)
    in the Chapter02/01_agent_anatomy.py file. It has no external dependencies and
    should work with any relatively modern Python version. By running it several times,
    you’ll get different amounts of reward gathered by the agent. The following is
    an output I got on my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The simplicity of the preceding code illustrates the important basic concepts
    of the RL model. The environment could be an extremely complicated physics model,
    and an agent could easily be a large neural network (NN) that implements the latest
    RL algorithm, but the basic pattern will stay the same – at every step, the agent
    will take some observations from the environment, do its calculations, and select
    the action to take. The result of this action will be a reward and a new observation.
  prefs: []
  type: TYPE_NORMAL
- en: You may ask, if the pattern is the same, why do we need to write it from scratch?
    What if it is already implemented by somebody and could be used as a library?
    Of course, such frameworks exist, but before we spend some time discussing them,
    let’s prepare your development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware and software requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples in this book were implemented and tested using Python version 3.11\.
    I assume that you’re already familiar with the language and common concepts such
    as virtual environments, so I won’t cover in detail how to install packages and
    how to do this in an isolated way. The examples will use the previously mentioned
    Python type annotations, which will allow us to provide type signatures for functions
    and class methods.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, there are lots of ML and RL libraries available, but in this book,
    I tried to keep the list of dependencies to a minimum, giving a preference to
    our own implementation of methods over the blind import of third-party libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The external libraries that we will use in this book are open source software,
    and they include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy: This is a library for scientific computing and implementing matrix operations
    and common functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenCV Python bindings: This is a computer vision library and provides many
    functions for image processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gymnasium from the Farama Foundation ([https://farama.org](https://farama.org)):
    This is a maintained fork of the OpenAI Gym library ( [https://github.com/openai/gym](https://github.com/openai/gym))
    and an RL framework that has various environments that can be communicated with
    in a unified way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch: This is a flexible and expressive deep learning (DL) library. A short
    crash course on it will be given in Chapter [3](ch007.xhtml#x1-530003).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch Ignite: This is a set of high-level tools on top of PyTorch used to
    reduce boilerplate code. It will be covered briefly in Chapter [3](ch007.xhtml#x1-530003).
    The full documentation is available here: [https://pytorch-ignite.ai/](https://pytorch-ignite.ai/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PTAN: ([https://github.com/Shmuma/ptan](https://github.com/Shmuma/ptan)): This
    is an open source extension to the OpenAI Gym API that I created to support modern
    deep RL methods and building blocks. All classes used will be described in detail
    together with the source code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other libraries will be used for specific chapters; for example, we will use
    Microsoft TextWorld to play text-based games, PyBullet and MuJoCo for robotic
    simulations, Selenium for browser-based automation problems, and so on. Those
    specialized chapters will include installation instructions for those libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'A significant portion of this book (Parts 2, 3, and 4) is focused on the modern
    deep RL methods that have been developed over the past few years. The word ”deep”
    in this context means that DL is heavily used. You may be aware that DL methods
    are computationally hungry. One modern graphics processing unit (GPU) can be 10
    to 100 times faster than even the fastest multiple central processing unit (CPU)
    systems. In practice, this means that the same code that takes one hour to train
    on a system with a GPU could take from half a day to one week even on the fastest
    CPU system. It doesn’t mean that you can’t try the examples from this book without
    having access to a GPU, but it will take longer. To experiment with the code on
    your own (the most useful way to learn anything), it is better to get access to
    a machine with a GPU. This can be done in various ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Buying a modern GPU suitable for CUDA and supported by the PyTorch framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cloud instances. Both Amazon Web Services and Google Cloud Platform can
    provide you with GPU-powered instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Colab offers free GPU access to its Jupyter notebooks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The instructions on how to set up the system are beyond the scope of this book,
    but there are plenty of manuals available on the Internet. In terms of an operating
    system (OS) , you should use Linux or macOS. Windows is supported by PyTorch and
    Gymnasium, but the examples in the book were not fully tested on the Windows OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you the exact versions of the external dependencies that we will use
    throughout the book, here is a requirements.txt file (please note that it was
    tested on Python 3.11; different versions might require you to tweak the dependencies
    or not work at all):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: All the examples in the book were written and tested with PyTorch 2.5.0, which
    can be installed by following the instructions on the [https://pytorch.org](https://pytorch.org)
    website (normally, that’s just the conda install pytorch torchvision -c pytorch
    or even just pip install torch command, depending on your OS).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go into the details of the OpenAI Gym API, which provides us with
    tons of environments, from trivial to challenging ones.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI Gym API and Gymnasium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python library called Gym was developed by OpenAI ([www.openai.com](https://www.openai.com)).
    The first version was released in 2017 and since then, lots of environments were
    developed or adopted to this original API, which became a de facto standard for
    RL.
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, the team that developed OpenAI Gym moved the development to Gymnasium
    ([github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium))
    – the fork of the original Gym library. Gymnasium provides the same API and is
    supposed to be a “drop-in replacement” for Gym (you can write import gymnasium
    as gym and most likely your code will work).
  prefs: []
  type: TYPE_NORMAL
- en: Examples in this book are using Gymnasium, but in the text, I’ll use ”Gym” for
    brevity. In rare cases when the difference does matter, I’ll use ”Gymnasium.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal of Gym is to provide a rich collection of environments for RL
    experiments using a unified interface. So, it is not surprising that the central
    class in the library is an environment, which is called Env. Instances of this
    class expose several methods and fields that provide the required information
    about its capabilities. At a high level, every environment provides these pieces
    of information and functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: A set of actions that is allowed to be executed in the environment. Gym supports
    both discrete and continuous actions, as well as their combination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape and boundaries of the observations that the environment provides the
    agent with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method called step to execute an action, which returns the current observation,
    the reward, and a flag indicating that the episode is over.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method called reset, which returns the environment to its initial state and
    obtains the first observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now talk about these components of the environment in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The action space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, the actions that an agent can execute can be discrete, continuous,
    or a combination of the two.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete actions are a fixed set of things that an agent can do, for example,
    directions in a grid like left, right, up, or down. Another example is a push
    button, which could be either pressed or released. Both states are mutually exclusive
    and this is the main characteristic of a discrete action space, where only one
    action from a finite set of actions is possible at a time.
  prefs: []
  type: TYPE_NORMAL
- en: A continuous action has a value attached to it, for example, a steering wheel,
    which can be turned at a specific angle, or an accelerator pedal, which can be
    pressed with different levels of force. A description of a continuous action includes
    the boundaries of the value that the action could have. In the case of a steering
    wheel, it could be from −720 degrees to 720 degrees. For an accelerator pedal,
    it’s usually from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are not limited to a single action; the environment could take
    multiple actions, such as pushing multiple buttons simultaneously or steering
    the wheel and pressing two pedals (the brake and the accelerator). To support
    such cases, Gym defines a special container class that allows the nesting of several
    action spaces into one unified action.
  prefs: []
  type: TYPE_NORMAL
- en: The observation space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in Chapter [1](ch005.xhtml#x1-190001), observations are pieces
    of information that an environment provides the agent with, on every timestamp,
    besides the reward. Observations can be as simple as a bunch of numbers or as
    complex as several multidimensional tensors containing color images from several
    cameras. An observation can even be discrete, much like action spaces. An example
    of a discrete observation space is a lightbulb, which could be in two states –
    on or off – given to us as a Boolean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, you can see the similarity between actions and observations, and that is
    how they have been represented in Gym’s classes. Let’s look at a class diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![tsuhpalpee::Dl TsaioSuSTmsnBwpppupc:o:alaplrxcecleeinfleeettos[S[(eapin)tatc,e
    .,..]...] cohnigtha:inflso(axt) seed () ](img/B22150_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The hierarchy of the Space class in Gym'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic abstract Space class includes one property and three methods that
    are relevant to us:'
  prefs: []
  type: TYPE_NORMAL
- en: 'shape: This property contain the shape of the space, identical to NumPy arrays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sample(): This returns a random sample from the space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'contains(x): This checks whether the argument, x, belongs to the space’s domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'seed(): This method allows us to initialize a random number generator for the
    space and all subspaces. This is useful if you want to get reproducible environment
    behavior across several runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All these methods are abstract and reimplemented in each of the Space subclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: The Discrete class represents a mutually exclusive set of items, numbered from
    0 to n-1\. If needed, you can redefine the starting index with the optional constructor
    argument start. The value n is a count of the items our Discrete object describes.
    For example, Discrete(n=4) can be used for an action space of four directions
    to move in [left, right, up, or down].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Box class represents an n-dimensional tensor of rational numbers with intervals
    [low, high]. For instance, this could be an accelerator pedal with one single
    value between 0.0 and 1.0, which could be encoded by Box(low=0.0, high=1.0, shape=(1,),
    dtype=np.float32). Here, the shape argument is assigned a tuple of length 1 with
    a single value of 1, which gives us a one-dimensional tensor with a single value.
    The dtype parameter specifies the space’s value type, and here, we specify it
    as a NumPy 32-bit float. Another example of Box could be an Atari screen observation
    (we will cover lots of Atari environments later), which is an RGB (red, green,
    and blue) image of size 210 × 160: Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8).
    In this case, the shape argument is a tuple of three elements: the first dimension
    is the height of the image, the second is the width, and the third equals 3, which
    all correspond to three color planes for red, green, and blue, respectively. So,
    in total, every observation is a three-dimensional tensor with 100,800 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final child of Space is a Tuple class, which allows us to combine several
    Space class instances together. This enables us to create action and observation
    spaces of any complexity that we want. For example, imagine we want to create
    an action space specification for a car. The car has several controls that can
    be changed at every timestamp, including the steering wheel angle, brake pedal
    position, and accelerator pedal position. These three controls can be specified
    by three float values in one single Box instance. Besides these essential controls,
    the car has extra discrete controls, like a turn signal (which could be off, right,
    or left) or horn (on or off). To combine all of this into one action space specification
    class, we can use the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This flexibility is rarely used; for example, in this book, you will see only
    the Box and Discrete actions and observation spaces, but the Tuple class can be
    handy in some cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are other Space subclasses defined in Gym, for example, Sequence (representing
    variable-length sequences), Text (strings), and Graph (where space is a set of
    nodes with connections between them). But the three that we have described are
    the most useful ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every environment has two members of type Space: the action_space and observation_space.
    This allows us to create generic code that could work with any environment. Of
    course, dealing with the pixels of the screen is different from handling discrete
    observations (as in the former case, we may want to preprocess images with convolutional
    layers or with other methods from the computer vision toolbox); so, most of the
    time, this means optimizing the code for a particular environment or group of
    environments, but Gym doesn’t prevent us from writing generic code.'
  prefs: []
  type: TYPE_NORMAL
- en: The environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The environment is represented in Gym by the Env class, which has the following
    members:'
  prefs: []
  type: TYPE_NORMAL
- en: 'action_space: This is the field of the Space class and provides a specification
    for allowed actions in the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'observation_space: This field has the same Space class, but specifies the observations
    provided by the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'reset(): This resets the environment to its initial state, returning the initial
    observation vector and the dict with extra information from the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'step(): This method allows the agent to take the action and returns information
    about the outcome of the action:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next observation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The local reward
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The end-of-episode flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The flag indicating a truncated episode
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A dictionary with extra information from the environment
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This method is a bit complicated; we will look at it in detail later in this
    section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are extra utility methods in the Env class, such as render(), which allows
    us to obtain the observation in a human-friendly form, but we won’t use them.
    You can find the full list in Gym’s documentation, but let’s focus on the core
    Env methods: reset() and step().'
  prefs: []
  type: TYPE_NORMAL
- en: As reset is much simpler, we will start with it. The reset() method has no arguments;
    it instructs an environment to reset into its initial state and obtain the initial
    observation. Note that you have to call reset() after the creation of the environment.
    As you may remember from Chapter [1](ch005.xhtml#x1-190001), the agent’s communication
    with the environment may have an end (like a “Game Over” screen). Such sessions
    are called episodes, and after the end of the episode, an agent needs to start
    over. The value returned by this method is the first observation of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the observation, reset() returns the second value – the dictionary with
    extra environment-specific information. Most standard environments return nothing
    in this dictionary, but more complicated ones (like TextWorld, an emulator for
    interactive-fiction games; we’ll take a look at it later in the book) might return
    additional information that doesn’t fit into standard observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The step() method is the central piece in the environment’s functionality.
    It does several things in one call, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Telling the environment which action we will execute in the next step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the new observation from the environment after this action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the reward the agent gained with this step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the indication that the episode is over
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the flag which signals an episode truncation (when time limit is enabled,
    for example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the dict with extra environment-specific information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first item in the preceding list (action) is passed as the only argument
    to the step() method, and the rest are returned by this method. More precisely,
    this is a tuple (Python tuple and not the Tuple class we discussed in the previous
    section) of five elements (observation, reward, done, truncated, and info). They
    have these types and meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'observation: This is a NumPy vector or a matrix with observation data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'reward: This is the float value of the reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'done: This is a Boolean indicator, which is True when the episode is over.
    If this value is True, we have to call reset() in the environment, as no more
    actions are possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'truncated: This is a Boolean indicator, which is True when the episode is truncated.
    For most environments, this is a TimeLimit (which is a way to limit length of
    episodes), but might have different meaning in some environments. This flag is
    separated from done, because in some scenarios it might be useful to distinguish
    situations ”agent reached the end of episode” and ”agent has reached the time
    limit of the environment.” If truncated is True, we also have to call reset()
    in the environment, the same as with the done flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'info: This could be anything environment-specific with extra information about
    the environment. The usual practice is to ignore this value in general RL methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have already got the idea of environment usage in an agent’s code –
    in a loop, we call the step() method with an action to perform until the done
    or truncated flags become True. Then, we can call reset() to start over. There
    is only one piece missing – how we create Env objects in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every environment has an unique name of the EnvironmentName-vN form, where N
    is the number used to distinguish between different versions of the same environment
    (when, for example, some bugs get fixed or some other major changes are made).
    To create an environment, the gymnasium package provides the make(name) function,
    whose only argument is the environment’s name in string form.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, Gymnasium version 0.29.1 (being installed with the
    [atari] extension) contains 1,003 environments with different names. Of course,
    all of these are not unique environments, as this list includes all versions of
    an environment. Additionally, the same environment can have different variations
    in the settings and observations spaces. For example, the Atari game Breakout
    has these environment names:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Breakout-v0, Breakout-v4: The original Breakout with a random initial position
    and direction of the ball.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BreakoutDeterministic-v0, BreakoutDeterministic-v4: Breakout with the same
    initial placement and speed vector of the ball.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BreakoutNoFrameskip-v0, BreakoutNoFrameskip-v4: Breakout with every frame displayed
    to the agent. Without this, every action is executed for several consecutive frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breakout-ram-v0, Breakout-ram-v4: Breakout with the observation of the full
    Atari emulation memory (128 bytes) instead of screen pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breakout-ramDeterministic-v0, Breakout-ramDeterministic-v4: Memory observation
    with the same initial state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breakout-ramNoFrameskip-v0, Breakout-ramNoFrameskip-v4: Memory observation
    without frame skipping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In total, there are 12 environments for a single game. In case you’ve never
    seen it before, here is a screenshot of its gameplay:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: The gameplay of Breakout'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even after the removal of such duplicates, Gymnasium comes with an impressive
    list of 198 unique environments, which can be divided into several groups:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classic control problems: These are toy tasks that are used in optimal control
    theory and RL papers as benchmarks or demonstrations. They are usually simple,
    with low-dimension observation and action spaces, but they are useful as quick
    checks when implementing algorithms. Think about them as the ”MNIST for RL” (MNIST
    is a handwriting digit recognition dataset from Yann LeCun, which you can find
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Atari 2600: These are games from the classic game platform from the 1970s.
    There are 63 unique games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Algorithmic: These are problems that aim to perform small computation tasks,
    such as copying the observed sequence or adding numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Box2D: These are environments that use the Box2D physics simulator to learn
    walking or car control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MuJoCo: This is another physics simulator used for several continuous control
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameter tuning: This is RL being used to optimize NN parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Toy text: These are simple grid world text environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the total number of RL environments supporting the Gym API is much
    larger. For example, The Farama Foundation maintains several repositories related
    to special RL topics like multi-agent RL, 3D navigation, robotics, and web automation.
    In addition, there are lots of third-party repositories. To get the idea, you
    can check out [https://gymnasium.farama.org/environments/third_party_environments](https://gymnasium.farama.org/environments/third_party_environments)
    in the Gymnasium documentation.
  prefs: []
  type: TYPE_NORMAL
- en: But enough theory! Let’s now look at a Python session working with one of Gym’s
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: The CartPole session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s apply our knowledge and explore one of the simplest RL environments that
    Gym provides.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have imported the gymnasium package and created an environment called
    CartPole. This environment is from the classic control group and its gist is to
    control the platform with a stick attached to its bottom part (see the following
    figure).
  prefs: []
  type: TYPE_NORMAL
- en: The trickiness is that this stick tends to fall right or left and you need to
    balance it by moving the platform to the right or left at every step.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The CartPole environment'
  prefs: []
  type: TYPE_NORMAL
- en: The observation of this environment is four floating-point numbers containing
    information about the x coordinate of the stick’s center of mass, its speed, its
    angle to the platform, and its angular speed. Of course, by applying some math
    and physics knowledge, it won’t be complicated to convert these numbers into actions
    when we need to balance the stick, but our problem is different – how do we learn
    how to balance this system without knowing the exact meaning of the observed numbers
    and only by getting the reward? The reward in this environment is 1, and it is
    given on every time step. The episode continues until the stick falls, so to get
    a more accumulated reward, we need to balance the platform in a way to avoid the
    stick falling.
  prefs: []
  type: TYPE_NORMAL
- en: This problem may look difficult, but in just two chapters, we will write the
    algorithm that will easily solve CartPole in minutes, without any idea about what
    the observed numbers mean. We will do it only by trial and error and using a bit
    of RL magic.
  prefs: []
  type: TYPE_NORMAL
- en: But now, let’s continue with our session.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we reset the environment and obtained the first observation (we always
    need to reset the newly created environment). As I said, the observation is four
    numbers, so no surprises here. Let’s now examine the action and observation space
    of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The action_space field is of the Discrete type, so our actions will be just
    0 or 1, where 0 means pushing the platform to the left and 1 is pushing to the
    right. The observation space is of Box(4,), which means a vector of four numbers.
    The first list shown in the observation_space field is the low bound and the second
    is the high bound of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re curious, you can peek at the source code of the environment in the
    Gymnasium repository in the cartpole.py file at [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py#L40).
    Documentation strings of the CartPole class provide all the details, including
    semantics of observation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cart position: Value in −4.8…4.8 range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cart velocity: Value in −∞…∞ range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pole angle: Value in radians in −0.418…0.418 range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pole angular velocity: Value in −∞…∞ range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python uses float32 maximum and minimum values to represent infinity, which
    is why some entries in boundary vectors have values of scale 10^(38). All those
    internal details are interesting to know, but absolutely not needed to solve the
    environment using RL methods. Let’s go further and send an action to the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we pushed our platform to the left by executing the action 0 and got
    the tuple of five elements:'
  prefs: []
  type: TYPE_NORMAL
- en: A new observation, which is a new vector of four numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A reward of 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The done flag with value False, which means that the episode is not over yet
    and we are more or less okay with balancing the pole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The truncated flag with value False, meaning that the episode was not truncated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extra information about the environment, which is an empty dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will use the sample() method of the Space class on the action_space
    and observation_space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This method returned a random sample from the underlying space, which in the
    case of our Discrete action space means a random number of 0 or 1, and for the
    observation space means a random vector of four numbers. The random sample of
    the observation space is not very useful, but the sample from the action space
    could be used when we are not sure how to perform an action. This feature is especially
    handy because you don’t know any RL methods yet, but we still want to play around
    with the Gym environment. Now that you know enough to implement your first randomly
    behaving agent for CartPole, let’s do it.
  prefs: []
  type: TYPE_NORMAL
- en: The random CartPole agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the environment is much more complex than our first example in section [2.1](#x1-390002.1),
    the code of the agent is much shorter. This is the power of reusability, abstractions,
    and third-party libraries!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here is the code (you can find it in Chapter02/02_cartpole_random.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created the environment and initialized the counter of steps and the
    reward accumulator. On the last line, we reset the environment to obtain the first
    observation (which we will not use, as our agent is stochastic):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding loop, after sampling a random action, we asked the environment
    to execute it and return to us the next observation (obs), the reward, the is_done,
    and the is_trunc flags. If the episode is over, we stop the loop and show how
    many steps we have taken and how much reward has been accumulated. If you start
    this example, you will see something like this (not exactly, though, due to the
    agent’s randomness):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: On average, our random agent takes 12 to 15 steps before the pole falls and
    the episode ends. Most of the environments in Gym have a ”reward boundary,” which
    is the average reward that the agent should gain during 100 consecutive episodes
    to ”solve” the environment. For CartPole, this boundary is 195, which means that,
    on average, the agent must hold the stick for 195 time steps or longer. Using
    this perspective, our random agent’s performance looks poor. However, don’t be
    disappointed; we are just at the beginning, and soon you will solve CartPole and
    many other much more interesting and challenging environments.
  prefs: []
  type: TYPE_NORMAL
- en: Extra Gym API functionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we have discussed so far covers two-thirds of the Gym core API and the
    essential functions required to start writing agents. The rest of the API you
    can live without, but it will make your life easier and the code cleaner. So,
    let’s briefly cover the rest of the API.
  prefs: []
  type: TYPE_NORMAL
- en: Wrappers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very frequently, you will want to extend the environment’s functionality in
    some generic way. For example, imagine an environment gives you some observations,
    but you want to accumulate them in some buffer and provide to the agent the N
    last observations. This is a common scenario for dynamic computer games, when
    one single frame is just not enough to get the full information about the game
    state. Another example is when you want to be able to crop or preprocess an image’s
    pixels to make it more convenient for the agent to digest, or if you want to normalize
    reward scores somehow. There are many such situations that have the same structure
    – you want to ”wrap” the existing environment and add some extra logic for doing
    something. Gym provides a convenient framework for this – the Wrapper class.
  prefs: []
  type: TYPE_NORMAL
- en: The class structure is shown in Figure [2.4](#x1-49003r4).
  prefs: []
  type: TYPE_NORMAL
- en: '![ObAsRceoetrbwivsaWeoaaerrrnnctrdeEavWtivWwnp:rioaravpaontareEpnWiopdrnp(rnp(veaa(err)por)pbesr)
    unwrapped: Env ](img/B22150_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The hierarchy of the Wrapper classes in Gym'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Wrapper class inherits the Env class. Its constructor accepts the only
    argument – the instance of the Env class to be ”wrapped.” To add extra functionality,
    you need to redefine the methods you want to extend, such as step() or reset().
    The only requirement is to call the original method of the superclass. To simplify
    accessing the environment being wrapped, Wrapper has two properties: env, of the
    immediate environment we’re wrapping (which could be another wrapper as well),
    and property unwrapped, which is an Env without any wrappers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle more specific requirements, such as a Wrapper class that wants to
    process only observations from the environment, or only actions, there are subclasses
    of Wrapper that allow the filtering of only a specific portion of information.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ObservationWrapper: You need to redefine the observation(obs) method of the
    parent. The obs argument is an observation from the wrapped environment, and this
    method should return the observation that will be given to the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RewardWrapper: This exposes the reward(rew) method, which can modify the reward
    value given to the agent, for example, scale it to the needed range, add a discount
    based on some previous actions, or something like this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ActionWrapper: You need to override the action(a) method, which can tweak the
    action passed to the wrapped environment by the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make it slightly more practical, let’s imagine a situation where we want
    to intervene in the stream of actions sent by the agent and, with a probability
    of 10%, replace the current action with a random one. It might look like an unwise
    thing to do, but this simple trick is one of the most practical and powerful methods
    for solving the exploration/exploitation problem that we mentioned in Chapter [1](ch005.xhtml#x1-190001).
    By issuing the random actions, we make our agent explore the environment and from
    time to time drift away from the beaten track of its policy. This is an easy thing
    to do using the ActionWrapper class (a full example is in Chapter02/03_random_action_wrapper.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, we initialized our wrapper by calling a parent’s __init__ method and saving
    epsilon (the probability of a random action).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a method that we need to override from a parent’s class to
    tweak the agent’s actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Every time we roll the die, and with the probability of epsilon, we sample a
    random action from the action space and return it instead of the action the agent
    has sent to us. Note that using action_space and wrapper abstractions, we were
    able to write abstract code, which will work with any environment from Gym. We
    also print the message on the console, just to illustrate that our wrapper is
    working. In the production code, this won’t be necessary, of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s time to apply our wrapper. We will create a normal CartPole environment
    and pass it to our Wrapper constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From here on, we will use our wrapper as a normal Env instance, instead of the
    original CartPole. As the Wrapper class inherits the Env class and exposes the
    same interface, we can nest our wrappers as deep as we want. This is a powerful,
    elegant, and generic solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is almost the same code as in the random agent, except that every time,
    we issue the same action, 0, so our agent is dull and does the same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the code, you should see that the wrapper is indeed working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We should move on now and look at how you can render your environment during
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: Rendering the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another possibility that you should be aware of is rendering the environment.
    It is implemented with two wrappers: HumanRendering and RecordVideo.'
  prefs: []
  type: TYPE_NORMAL
- en: Those two classes replace the original Monitor wrapper in the OpenAI Gym library,
    which was removed. This class was able to record the information about your agent’s
    performance in a file, with an optional video recording of your agent in action.
  prefs: []
  type: TYPE_NORMAL
- en: With the Gymnasium library, you have two classes to check what’s going on inside
    the environment. The first one is HumanRendering, which opens a separate graphical
    window in which the image from the environment is being shown interactively. To
    be able to render the environment (CartPole in our case), it has to be initialized
    with the render_mode="rgb_array" argument. This argument tells the environment
    to return pixels from its render() method, which is being called by the HumanRendering
    wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to use the HumanRenderer wrapper, you need to change the random agent’s
    code (the full code is in Chapter02/04_cartpole_random_monitor.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you start the code, the window with environment rendering will appear. As
    our agent cannot balance the pole for too long (10-30 steps max), the window will
    disappear quite quickly, once the env.close() method is called.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: CartPole environment rendered by HumanRendering'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another wrapper that might be useful is RecordVideo, which captures the pixels
    from the environment and produces a video file of your agent in action. It is
    used in the same way as the human renderer, but requires an extra argument specifying
    the directory to store video files. If the directory doesn’t exist, it will be
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After starting the code, it reports the name of the video produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This wrapper is especially useful in situations when you’re running your agent
    on a remote machine without the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: More wrappers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gymnasium provides lots of other wrappers, which we’ll use in the upcoming chapters.
    It can do standardized preprocessing of Atari game images, do reward normalization,
    stack observation frames, do vectorization of an environment, do time limiting
    and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The full list of available wrappers is available in the documentation, [https://gymnasium.farama.org/api/wrappers/](https://gymnasium.farama.org/api/wrappers/),
    and in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have started to learn about the practical side of RL! In this chapter, we
    experimented with Gymnasium, with its tons of environments to play with. We studied
    its basic API and created a randomly behaving agent.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to extend the functionality of existing environments in
    a modular way and became familiar with a way to render our agent’s activity using
    wrappers. This will be heavily used in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will do a quick DL recap using PyTorch, which is one
    of the most widely used DL toolkits.
  prefs: []
  type: TYPE_NORMAL
