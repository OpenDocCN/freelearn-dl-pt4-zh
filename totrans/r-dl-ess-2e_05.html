<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Classification Using Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>It is not an exaggeration to say that the huge growth of interest in deep learning can be mostly attributed to convolutional neural networks. <strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) are the main building blocks of image classification models in deep learning, and have replaced most techniques that were previously used by specialists in the field. Deep learning models are now the de facto method to perform all large-scale image tasks, including image classification, object detection, detecting artificially generated images, and even attributing text descriptions to images. In this chapter, we will look at some of these techniques.</p>
<p>Why are CNNs so important? To explain why, we can look at the history of the ImageNet competition. The <strong>ImageNet</strong> competition is an open large-scale image classification challenge that has one thousand categories. It can be considered as the unofficial world championship for image classification. Teams, mostly fielded by academics and researchers, compete from around the world. In 2011, an error rate of around 25% was the benchmark. In 2012, a team led by Alex Krizhevsky and advised by Geoffrey Hinton achieved a huge leap by winning the competition with an error rate of 16%. Their solution consisted of 60 million parameters and 650,000 neurons, five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1,000-way softmax layer to do the final classification.</p>
<p>Other researchers built on their techniques in subsequent years, with the result that the original ImageNet competition is essentially considered <em>solved.</em> In 2017, almost all teams achieved an error rate of less than 5%. Most people consider that the 2012 ImageNet victory heralded the dawn of the new deep learning revolution.</p>
<p>In this chapter, we will look at image classification using CNN. We are going to start with the <kbd>MNIST</kbd> dataset, which is considered as the <em>Hello World</em> of deep learning tasks. The <kbd>MNIST</kbd> dataset consists of grayscale images of size 28 x 28 of 10 classes, the numbers 0-9. This is a much easier task than the ImageNet competition; there are 10 categories rather than 1,000, the images are in grayscale rather than color and most importantly, there are no backgrounds in the MNIST images that can potentially confuse the model. Nevertheless, the MNIST task is an important one in its own right; for example, most countries use postal codes containing digits. Every country uses automatic address routing solutions that are more complex variations of this task.</p>
<p>We will use the MXNet library from Amazon for this task. The MXNet library is an excellent library introduction to deep learning as it allows us to code at a higher level than other libraries such as TensorFlow, which we cover later on in this book.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>What are CNNs?</li>
<li>Convolutional layers</li>
<li>Pooling layers</li>
<li>Softmax</li>
<li>Deep learning architectures</li>
<li>Using MXNet for image classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs</h1>
                </header>
            
            <article>
                
<p>CNNs are the cornerstone of image classification in deep learning. This section gives an introduction to them, explains the history of CNNs, and will explain why they are so powerful.</p>
<p>Before we begin, we will look at a simple deep learning architecture. Deep learning models are difficult to train, so using an existing architecture is often the best place to start. An architecture is an existing deep learning model that was state-of-the-art when initially released. Some examples are AlexNet, VGGNet, GoogleNet, and so on. The architecture we will look at is the original LeNet architecture for digit classification from Yann LeCun and others from the mid 1990s. This architecture was used for the <kbd>MNIST</kbd> dataset. This dataset is comprised of <span>grayscale </span>images of <span>28 x 28 size </span>that contain the digits 0 to 9. The following diagram shows the LeNet architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-758 image-border" src="assets/f59bb2d3-51a3-4092-8010-a129b9248d7c.png" style="width:162.50em;height:53.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.1: The LeNet architecture </div>
<p>The original images are 28 x 28 in size. We have a series of hidden layers which are convolution and pooling layers (here, they are labeled <em>subsampling</em>). Each convolutional layer changes structure; for example, when we apply the convolutions in the first hidden layer, our output size is three dimensional. Our final layer is of size 10 x 1, which is the same size as the number of categories. We can apply a <kbd>softmax</kbd> function here to convert the values in this layer to probabilities for each category. The category with the highest probability would be the category prediction for each image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers</h1>
                </header>
            
            <article>
                
<p>This section shows how convolutional layers work in greater depth. At a basic level, convolutional layers are nothing more than a set of filters. When you look at images while wearing glasses with a red tint, everything appears to have a red hue. Now, imagine if these glasses consisted of different tints embedded within them, maybe a red tint with one or more horizontal green tints. If you had such a pair of glasses, the effect would be to highlight certain aspects of the scene in front of you. Any part of the scene that had a green horizontal line would become more focused.</p>
<p>Convolutional layers apply a selection of patches (or convolutions) over the previous layer’s output. For example, for a face recognition task, the first layer’s patches identify basic features in the image, for example, an edge or a diagonal line. The patches are moved across the image to match different parts of the image. Here is an example of a 3 x 3 convolutional block applied across a 6 x 6 image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9e8858f5-faf5-4c81-80d8-23b1caa232f8.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.2: An example of a single convolution applied across an image</div>
<p>The values in the convolutional block are multiplied element by element (that is, not matrix multiplication), and the values are added to give a single value. Here is an example:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3701b84b-891e-4c43-bba3-2dc35c04c6f1.png" style="width:40.42em;height:18.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.3: An example of a convolution block applied to two parts of an input layer</div>
<p>In this example, our convolutional block is a diagonal pattern. The first block in the image (<em>A1:C3</em>) is also a diagonal pattern, so when we multiply the elements and sum them, we get a relatively large value of <strong>6.3</strong>. In comparison, the second block in the image (<em>D4:F6</em>) is a horizontal line pattern, so we get a much smaller value.</p>
<p>It can be difficult to visualize how convolutional layers work across the entire image, so the following R Shiny application will show it more clearly. This application is included in the code for this book in the <kbd>Chapter5/server.R</kbd> file. Open this file in <strong>RStudio</strong> and select <strong>Run app</strong>. Once the application is loaded, select <strong>Convolutional Layers</strong> from the left menu bar. The application loads the first 100 images from the <kbd>MNIST</kbd> dataset, which we will use later for our first deep learning image classification task. The images are grayscale images of size 28 x 28 of handwritten digits 0 to 9. Here is a screenshot of the application with the fourth image selected, which is a four:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4719d54a-cdbd-4ef9-a302-fc9a7b1c9bf2.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.4: The Shiny application <span>showing a horizontal convolutional filter</span></div>
<p>Once loaded, you can use the slider to browse through the images. In the top-right corner, there are four choices of convolutional layers to apply to the image. In the previous screenshot, a horizontal line convolutional layer is selected and we can see what this looks like in the text box in the top right corner. When we apply the convolutional filter to the input image on the left, we can see that the resulting image on the right is almost entirely grey, except for where the horizontal line was in the original image. Our convolutional filter has matched the parts in the image that have a horizontal line. If we change the convolutional filter to a vertical line, we get the following result:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/684ef9f1-9e06-476a-91ce-a339e28b065b.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.5: The Shiny application showing a vertical convolutional filter</div>
<p>Now, we can see that, after the convolution is applied, the vertical lines in the original image are highlighted in the resultant image on the right. In effect, applying these filters is a type of feature extraction. I encourage you to use the application and browse through images and see how different convolutions apply to images of the different categories.</p>
<p>This is the basis of convolutional filters, and while it is a simple concept, it becomes powerful when you start doing two things:</p>
<ul>
<li style="font-weight: 400">Combining many convolutional filters to create convolutional layers</li>
<li style="font-weight: 400">Applying another set of convolutional filters (that is, a convolutional layer) to the output of a previous convolutional layer</li>
</ul>
<p>This may take some time to get your head around. If I apply a filter to an image and then apply a filter to that output, what do I get? And if I then apply that a third time, that is, apply a filter to an image and then apply a filter to that output, and then apply a filter to that output, what do I get? The answer is that each subsequent layer combines identified features from the previous layers to find even more complicated patterns, for example, corners, arcs, and so on. Later layers find even richer features such as a circle with an arc over it, indicating the eye of a person.</p>
<p>There are two parameters that are used to control the movement of the convolution: padding and strides. In the following diagram, we can see that the original image is of size 6 x 6, while there are 4 x 4 subgraphs. We have therefore reduced the data representation from a 6 x 6 matrix to a 4 x 4 matrix. When we apply a convolution of size <em>c1</em>, <em>c2</em> to data of size n, m, the output will be <em>n-c1+1</em>, <em>m-c2+1</em>. If we want our output to be the same size as our input, we can pad the input by adding zeros to borders of the images. For the previous example, we add a 1-pixel border around the entire image. The following diagram shows how the first 3 x 3 convolution would be applied to the image with padding:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/328c6a41-1e2f-4474-bd01-479977cc7e23.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.6 Padding applied before a convolution</div>
<p>The second parameter we can apply to convolutions is strides, which control the movement of the convolution. The default is 1, which means the convolution moves by 1 each time, first to the right and then down. In practice, this value is rarely changed, so we will not consider it further.</p>
<p>We now know that convolutions act like small feature generators, that they are applied across an input layer (which is image data for the first layer), and that subsequent convolution layers find even more complicated features. But how are they calculated? Do we need to carefully craft a set of convolutions manually to apply them to our model? The answer is no; these convolutions are automatically calculated for us through the magic of the gradient descent algorithm. The best patterns are found after many iterations through the training dataset.</p>
<div class="packt_infobox">So, how do convolutions work once we get beyond 2-3 levels of layers? The answer is that it is difficult for anyone to understand the exact mathematics of how convolutional layers work. Even the original designers of these architects may not fully understand what is happening in the hidden layers in a series of CNNs. If this worries you, then recall that the solution that won the ImageNet competition in 2012 had 60 million parameters. With the advance in computing power, deep learning architectures may have hundreds of millions of parameters. It is simply not possible for any person to fully understand what is happening in such a complicated model. This is why they are often called <strong>black-box</strong> models.</div>
<p>It might surprise you at first. How can deep learning achieve human-level performance in image classification and how can we build deep learning models if we do not fully understand how they work? This question has divided the deep learning community, largely along the demarcation between industry and academia. Many (but not all) researchers believe that we should get a more fundamental understanding of how deep learning models work. Some researchers also believe that we can only <span>develop the next generation of artificial intelligence applications </span>by getting a better understanding of how current architectures work. At a recent NIPS conference (one of the oldest and most notable conferences for deep learning), deep learning was unfavorably compared to alchemy. Meanwhile, practitioners in the industry are not concerned with how deep learning works. They are more focused on building ever more complex deep learning architectures to maximize accuracy or performance.</p>
<p>Of course, this is a crude representation of the state of the industry; not all academics are inward looking and not all <span>practitioners are just tweaking models to get small improvements. Deep learning is still relatively new (although the foundation blocks of neural networks have been known about for decades). But this tension does exist and </span>has been around for awhile <span>–</span> for example, a popular deep learning architecture introduced <em>Inception</em> modules, which were named after the <em>Inception</em> movie. In the film, Leonardo DiCaprio leads a team that alter people’s thoughts and opinions by embedding themselves within people’s dreams. Initially, they go one layer deep, but then go deeper, in effect going to dreams within dreams. As they go deeper, the worlds get more complicated and the outcomes less certain. We will not go into detail here about what <em>Inception modules</em> are, but they combine convolutional and max pooling layers in parallel. The authors of the paper acknowledged the memory and computational cost of the model within the paper, but by naming the key component as an <em>Inception module,</em> they were subtly suggesting which side of the argument they were on.</p>
<p><span>After the breakthrough performance of the winner of the 2012 ImageNet competition, two researchers were unsatisfied that there was no insight into how the model worked. They decided to reverse-engineer the algorithm, attempting to show the input pattern that caused a given activation in the feature maps. This was a non-trivial task, as some layers used in the original model (for example, pooling layers) discarded information. Their paper showed the top 9 activations for each layer. Here is the feature visualization for the first layer:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-586 image-border" src="assets/798437be-2e43-4581-99e1-db373bd99ef0.png" style="width:16.08em;height:11.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.7: Feature visualization for the first layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</div>
<p>The image is in two parts; on the left we can see the convolution (the paper only highlights 9 convolutions for each layer). On the right, we can see examples of patterns within images that match that convolution. For example, the convolution in the top-left corner is a diagonal edge detector. Here is the feature visualization for the second layer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-587 image-border" src="assets/8995cb80-262e-4472-81dd-a0201ee82632.png" style="width:43.17em;height:21.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.8: Feature visualization for the second layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</div>
<p>Again, the image on the left is an interpretation of the convolution, while the image on the right shows examples of image patches that activate for that convolution. Here, we are starting to see some combinatorial patterns. For example, in the top-left, we can see patterns with stripes. Even more interesting is the example in the second row and second column. Here, we see circle shapes, which can indicate an eyeball in a person or an animal. Now, let's move on to feature visualization for the third layer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-588 image-border" src="assets/44322ff1-5a85-4c01-af92-00d8d8716b79.png" style="width:55.17em;height:20.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.9: Feature visualization for the third layer in a CNN Source: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</div>
<p>In the third layer, we are seeing some really interesting patterns. In the second row and second column, we have identified parts of a car (wheels). In the third row and third column, we have begun to identify peoples' faces. In the second row and fourth column, we are identifying text within the images.</p>
<p>In the paper, the authors show examples for more layers. I encourage you to read the paper to get further insight into how convolutional layers work.</p>
<div class="packt_infobox">It is important to note that, while deep learning models can achieve human-level performance on image classification, they do not interpret images as humans do. They have no concept of what a cat is or what a dog is. They can only match the patterns given. In the paper, the authors highlight an example where the matched patterns have little in common; the model is matching features in the background (grass) instead of foreground objects.</div>
<p>In another image classification task, the model failed to work in practice. The task was to classify wolves versus dogs. The model failed in practice because the model was trained with data which had wolves in their natural habitat, that is, snow. Therefore, the model assumed its task was to differentiate between <em>snow</em> and <em>dog</em>. Any image of a wolf in another setting was wrongly classified.</p>
<div class="packt_infobox">The lesson from this is that your training data should be varied and closely related to the data that the model will be expected to predict against. This may sound obvious in theory, but it is not always easy to do so in practice. We will discuss this further in the next chapter.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>Pooling layers are used in CNNs to reduce the number of parameters in the model and therefore they reduce overfitting. They can be thought of as a type of dimensionality reduction. Similar to convolutional layers, a pooling layer moves over the previous layer but the operation and return value are different. It returns a single value and the operation is usually the maximum value of the cells in that patch, hence the name max-pooling. You can also perform other operations, for example, average pooling, but this is less common. Here is an example of max-pooling using a 2 x 2 block. The first block has the values 7, 0, 6, 6 and the maximum value of these is 7, so the output is 7. Note that padding is not normally used with max-pooling and that it usually applies a stride parameter to move the block. Here, the stride is 2, so once we get the max of the first block, we move across, 2 cells to the right:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-589 image-border" src="assets/4fdf19a0-cf55-4d13-bccd-91b5b63c0283.png" style="width:25.25em;height:11.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.10: Max-Pooling applied to a matrix</div>
<p>We can see that max-pooling reduces the output by a factor of 4; the input was 6 x 6 and the output is 3 x 3. If you have not seen this before, your first reaction is probably disbelief. Why are we throwing away data? Why do we use max-pooling at all? There are three parts to this answer:</p>
<ul>
<li style="font-weight: 400"><strong>Pooling</strong>: It is normally applied after a convolutional layer, so instead of executing over pixels, we execute over matched patterns. Downsizing after convolutional layers does not discard 75% of the input data; there is still enough signal there to find the pattern if it exists.</li>
<li style="font-weight: 400"><strong>Regularization</strong>: If you have studied machine learning, you will know that many models have problems with correlated features and that you are generally advised to remove correlated features. In image data, features are highly correlated with the spatial pattern around them. Applying max-pooling reduces the data while maintaining the features.</li>
<li style="font-weight: 400"><strong>Execution speed</strong>: When we consider the two earlier reasons, we can see that max-pooling greatly reduces the size of the network without removing too much of the signal. This makes training the model much quicker.</li>
</ul>
<p>It is important to note the difference in the parameters used in the convolutional layer compared to the pooling layer. In general, a convolutional block is bigger (3 x 3) than the pooling block (2 x 2) and they should not overlap. For example, do not use a 4 x 4 convolutional block and a 2 x 2 pooling block. If they did overlap, the pooling block would just operate over the same convolutional blocks and the model would not train correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p><strong>Dropout</strong> is a form of regularization which aims to prevent a model from overfitting. Overfitting is when the model is memorizing parts of the training dataset, but is not as accurate on unseen test data. When you build a model, you can check if overfitting is a problem by looking at the gap between the accuracy on the training set against the accuracy on the test set. If performance is much better on the training dataset, then the model is overfitting. Dropout refers to removing nodes randomly from a network temporarily during training. It is usually only applied to hidden layers, and not input layers. Here is an example of dropout applied to a neural network:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/25613efe-f5b3-4596-8cca-80e3f0bc38e2.jpg" style="width:24.58em;height:54.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.11: An example of dropout in a deep learning model</div>
<p>For each forward pass, a different set of nodes is removed, and therefore the network is different each time. In the original paper, dropout is compared to ensemble techniques, and in a way it is. There are some similarities to how dropout works and how random forest selects a random selection of features for each tree.</p>
<p>Another way to look at dropout is that each node in a layer must learn to work with all the nodes in that layer and the inputs it gets from the previous layer. It prevents one or a small number of nodes in a layer from getting large weights and dominating the outputs from that layer. This means that each node in a layer will work as a group and prevent some nodes from being too lazy and other nodes from being too dominant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flatten layers, dense layers, and softmax</h1>
                </header>
            
            <article>
                
<p>After applying multiple convolutional layers, the resulting data structure is a multi-dimensional matrix (or tensor). We must transform this into a matrix that is in the shape of the required output. For example, if our classification task has 10 classes (for e<span>xample, 10 for the <kbd>MNIST</kbd> example</span>), we need the output of the model to be a 1 x 10 matrix. We do this by taking the results of our convolutional and max-pooling layers and using a Flatten layer to reshape the data. The last layer should have the same number of nodes as the number of classes we wish to predict for. If our task is binary classification, the <kbd>activation</kbd> function in our last layer will be sigmoid. If our task is binary classification, the <kbd>activation</kbd> function in our last layer will be softmax.</p>
<p><span>Before applying the softmax/sigmoid activation, we may optionally apply a number of dense layers. A dense layer is just a normal hidden layer, as we saw in <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>,<em> Getting Started with Deep Learning</em>. </span></p>
<p>We need a softmax layer because the values in the last layer are numeric but range from -infinity to + infinity. We must convert these series of input values into a series of probabilities that says how likely the instance is for each category. The function to transform these numeric values to a series of probabilities must have the following characteristics:</p>
<ul>
<li style="font-weight: 400">Each output value must be between 0.0 to 1.0</li>
<li style="font-weight: 400">The sum of the output values should be exactly 1.0</li>
</ul>
<p>One way to do this is to just rescale the values by dividing each input value by the sum of the absolute input values. That approach has two problems:</p>
<ul>
<li style="font-weight: 400">It does not handle negative values correctly</li>
<li style="font-weight: 400">Rescaling the input values may give us probabilities that are too close to each other</li>
</ul>
<p>These two issues can be solved by first applying <strong>e<sup>x</sup></strong> (where e is 2.71828) to each input value and then rescaling those values. This transforms any negative number to a small positive number, and it also causes the probabilities to be more polarized. This can be demonstrated with an example; here, we can see the result from our dense layers. The values for categories 5 and 6 are quite close at 17.2 and 15.8, respectively. However, when we apply the <kbd>softmax</kbd> function, the probability value for category 5 is 4 times the probability value for category 6. The <kbd>softmax</kbd> function tends to result in probabilities that emphasize one category over all others, which is exactly what we want:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4cbe0564-d081-4f69-9b77-5dc0e94402be.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.12 Example of the softmax function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification using the MXNet library</h1>
                </header>
            
            <article>
                
<p>The MXNet package was introduced in <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, so go back to that chapter for instructions on how to install the package if you have not already done so. We will demonstrate how to get almost 100% accuracy on a classification task for image data. We will use the <kbd>MNIST</kbd> dataset that we introduced in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 2</a>, <em>Image Classification Using Convolutional Neural Networks</em>. This dataset contains <span>images of </span>handwritten digits (0-9), and all images are of size 28 x 28. It is the <em>Hello World!</em> equivalent in deep learning. There’s a long-term competition on Kaggle that uses this dataset. The script <kbd>Chapter5/explore.Rmd</kbd> is an R markdown file that explores this dataset.</p>
<ol>
<li>First, we will check if the data has already been <span>downloaded, and if it has not, we will download it</span><span>.</span> If the data is not available at this link, see the code in <kbd>Chapter2/chapter2.R</kbd> for an alternative way to get the data:</li>
</ol>
<pre style="padding-left: 60px">dataDirectory &lt;- "../data"<br/>if (!file.exists(paste(dataDirectory,'/train.csv',sep="")))<br/>{<br/>  link &lt;- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'<br/>  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep="")))<br/>    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=""))<br/>  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=""), exdir = dataDirectory)<br/>  if (file.exists(paste(dataDirectory,'/test.csv',sep="")))<br/>    file.remove(paste(dataDirectory,'/test.csv',sep=""))<br/>}</pre>
<ol start="2">
<li>Next we read the data into R and check it:</li>
</ol>
<pre style="padding-left: 60px">train &lt;- read.csv(paste(dataDirectory,'/train.csv',sep=""), header=TRUE, nrows=20)</pre>
<p style="padding-left: 60px">We have <kbd>20</kbd> rows and <kbd>785</kbd> columns. Here, we will look at the rows at the tail of the dataset and look at the first 6 columns and the last 6 columns:</p>
<pre style="padding-left: 60px">tail(train[,1:6])<br/>   label pixel0 pixel1 pixel2 pixel3 pixel4<br/>15     3      0      0      0      0      0<br/>16     1      0      0      0      0      0<br/>17     2      0      0      0      0      0<br/>18     0      0      0      0      0      0<br/>19     7      0      0      0      0      0<br/>20     5      0      0      0      0      0<br/><br/>tail(train[,(ncol(train)-5):ncol(train)])<br/>   pixel778 pixel779 pixel780 pixel781 pixel782 pixel783<br/>15        0        0        0        0        0        0<br/>16        0        0        0        0        0        0<br/>17        0        0        0        0        0        0<br/>18        0        0        0        0        0        0<br/>19        0        0        0        0        0        0<br/>20        0        0        0        0        0        0</pre>
<p style="padding-left: 60px">We have <kbd>785</kbd> columns. The first column is the data label, and then we have 784 columns named <kbd>pixel0</kbd>, …, <kbd>pixel783</kbd> with the pixel values. Our images are <em>28 x 28 = 784</em>, so everything looks OK.</p>
<p style="padding-left: 60px">Before we start building models, it is always a good idea to ensure that your data is in the correct format and that your features and labels are aligned correctly. Let's plot the first 9 instances with their data labels.</p>
<ol start="3">
<li>To do this, we will create a <kbd>helper</kbd> function called <kbd>plotInstance</kbd> that takes in the pixel values and outputs the image with an optional header:</li>
</ol>
<pre style="padding-left: 60px">plotInstance &lt;-function (row,title="")<br/> {<br/>  mat &lt;- matrix(row,nrow=28,byrow=TRUE)<br/>  mat &lt;- t(apply(mat, 2, rev))<br/>  image(mat, main = title,axes = FALSE, col = grey(seq(0, 1, length = 256)))<br/> }<br/> par(mfrow = c(3, 3))<br/> par(mar=c(2,2,2,2))<br/> for (i in 1:9)<br/> {<br/>  row &lt;- as.numeric(train[i,2:ncol(train)])<br/>  plotInstance(row, paste("index:",i,", label =",train[i,1]))<br/> }</pre>
<p style="padding-left: 60px">The output of this code shows the first 9 images and their classification:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ca8e8a1a-ba42-4536-8103-7e5ac5b98ee7.png" style="width:39.08em;height:27.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.13: The first 9 images in the MNIST dataset</div>
<p>This completes our data exploration. Now, we can move on to creating some deep learning models using the MXNet library. We will create two models—the first is a standard neural network which we will use as a baseline. The second deep learning model is based on an architecture called <strong>LeNet</strong>. This is an old architecture, but is suitable in this case because our images are low resolution and do not contain backgrounds. Another advantage of LeNet is that it is possible to train quickly, even on CPUs, because it does not have many layers.</p>
<p>The code for this section is in <span> </span><kbd>Chapter5/mnist.Rmd</kbd>. We must read data into R and convert it into matrices. <span>We will split the training data into a train set and test set to get an unbiased </span>estimate of accuracy. Because we have a large number of rows, we can use a split ratio of 90/10:</p>
<pre>require(mxnet)<br/>options(scipen=999)<br/><br/>dfMnist &lt;- read.csv("../data/train.csv", header=TRUE)<br/>yvars &lt;- dfMnist$label<br/>dfMnist$label &lt;- NULL<br/><br/>set.seed(42)<br/>train &lt;- sample(nrow(dfMnist),0.9*nrow(dfMnist))<br/>test &lt;- setdiff(seq_len(nrow(dfMnist)),train)<br/>train.y &lt;- yvars[train]<br/>test.y &lt;- yvars[test]<br/>train &lt;- data.matrix(dfMnist[train,])<br/>test &lt;- data.matrix(dfMnist[test,])<br/><br/>rm(dfMnist,yvars)</pre>
<p>Each image is represented as row of 784 (28 x 28) pixel values. The value of each pixel is in the range 0-255, we linearly transform it into 0-1 by dividing by 255. We also transpose the input matrix to because column major format in order to use it in <span> </span><kbd>mxnet</kbd>.</p>
<pre>train &lt;- t(train / 255.0)<br/>test &lt;- t(test / 255.0)</pre>
<p>Before creating a model, we should check that our dataset is balanced, i.e. the number of instances for each digit is reasonably even:</p>
<pre>table(train.y)<br/>## train.y<br/>##    0    1    2    3    4    5    6    7    8    9<br/>## 3716 4229 3736 3914 3672 3413 3700 3998 3640 3782</pre>
<p>This looks ok, we can now move on to creating some deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Base model (no convolutional layers)</h1>
                </header>
            
            <article>
                
<p>Now that we have explored the data and we are satisfied that it looks OK, the next step is to create our first deep learning model. This is similar to the example we saw in the previous chapter. The code for this is in <kbd>Chapter5/mnist.Rmd</kbd>:</p>
<pre>data &lt;- mx.symbol.Variable("data")<br/>fullconnect1 &lt;- mx.symbol.FullyConnected(data, name="fullconnect1", num_hidden=256)<br/>activation1 &lt;- mx.symbol.Activation(fullconnect1, name="activation1", act_type="relu")<br/>fullconnect2 &lt;- mx.symbol.FullyConnected(activation1, name="fullconnect2", num_hidden=128)<br/>activation2 &lt;- mx.symbol.Activation(fullconnect2, name="activation2", act_type="relu")<br/>fullconnect3 &lt;- mx.symbol.FullyConnected(activation2, name="fullconnect3", num_hidden=10)<br/>softmax &lt;- mx.symbol.SoftmaxOutput(fullconnect3, name="softmax")</pre>
<p>Let's look at this code in detail:</p>
<ol>
<li style="font-weight: 400">In <kbd>mxnet</kbd>, we use its own data type symbol to configure the network.</li>
<li style="font-weight: 400">We create the first hidden layer (<kbd>fullconnect1 &lt;- ....</kbd>). This parameters are the data as input, the layer's name and the number of neurons in the layer.</li>
<li style="font-weight: 400">We apply an activation function to the<span> </span><kbd>fullconnect</kbd>  layer (<kbd>activation1 &lt;- ....</kbd>). The <kbd>mx.symbol.Activation</kbd> function takes the output from the first hidden layer, <kbd>fullconnect1</kbd>.</li>
<li style="font-weight: 400">The second <span>hidden </span>layer <span>(</span><kbd>fullconnect1 &lt;- ....</kbd><span>) </span>takes <kbd>activation1</kbd> as the input.</li>
<li style="font-weight: 400">The second activation is similar to  <kbd>activation1</kbd>.</li>
<li style="font-weight: 400">The <span> </span><kbd>fullconnect3</kbd><span> </span><span> </span>is the output layer. <span>This layer has 10 neurons because this is a multi-classification problem and t</span>here are 10 classes.</li>
<li style="font-weight: 400">Finally, we use a <span>softmax </span>activation to get a probabilistic prediction for each class.</li>
</ol>
<p>Now, let's train the base model. I have a GPU installed, so I can use that. You may need to change the line to <kbd>devices &lt;- mx.cpu()</kbd>:</p>
<pre>devices &lt;- mx.gpu()<br/>mx.set.seed(0)<br/>model &lt;- mx.model.FeedForward.create(softmax, X=train, y=train.y,<br/>                                     ctx=devices,array.batch.size=128,<br/>                                     num.round=10,<br/>                                     learning.rate=0.05, momentum=0.9,<br/>                                     eval.metric=mx.metric.accuracy,<br/>                                     epoch.end.callback=mx.callback.log.train.metric(1))</pre>
<p>To make a prediction, we will call the <kbd>predict</kbd> function. We can then create a confusion matrix and calculate our accuracy level on test data:</p>
<pre>preds1 &lt;- predict(model, test)<br/>pred.label1 &lt;- max.col(t(preds1)) - 1<br/>res1 &lt;- data.frame(cbind(test.y,pred.label1))<br/>table(res1)<br/>##      pred.label1<br/>## test.y   0   1   2   3   4   5   6   7   8   9<br/>##      0 405   0   0   1   1   2   1   1   0   5<br/>##      1   0 449   1   0   0   0   0   4   0   1<br/>##      2   0   0 436   0   0   0   0   3   1   1<br/>##      3   0   0   6 420   0   1   0   2   8   0<br/>##      4   0   1   1   0 388   0   2   0   1   7<br/>##      5   2   0   0   6   1 363   3   0   2   5<br/>##      6   3   1   3   0   2   1 427   0   0   0<br/>##      7   0   2   3   0   1   0   0 394   0   3<br/>##      8   0   4   2   4   0   2   1   1 403   6<br/>##      9   1   0   1   2   7   0   1   1   0 393<br/><br/>accuracy1 &lt;- sum(res1$test.y == res1$pred.label1) / nrow(res1)<br/>accuracy1<br/>## 0.971</pre>
<p>The accuracy of our base model is <kbd>0.971</kbd>. Not bad, but let's see if we can improve on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LeNet</h1>
                </header>
            
            <article>
                
<p>Now, we can create a model based on the LeNet architecture. This is a very simple model; we have two sets of convolutional and pooling layers and then a Flatten layer, and finally two dense layers. <span>The code for this is in <kbd>Chapter5/mnist.Rmd</kbd>. First let's define the model:</span></p>
<pre>data &lt;- mx.symbol.Variable('data')<br/># first convolution layer<br/>convolution1 &lt;- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=64)<br/>activation1 &lt;- mx.symbol.Activation(data=convolution1, act_type="tanh")<br/>pool1 &lt;- mx.symbol.Pooling(data=activation1, pool_type="max",<br/>                           kernel=c(2,2), stride=c(2,2))<br/><br/># second convolution layer<br/>convolution2 &lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=32)<br/>activation2 &lt;- mx.symbol.Activation(data=convolution2, act_type="relu")<br/>pool2 &lt;- mx.symbol.Pooling(data=activation2, pool_type="max",<br/>                           kernel=c(2,2), stride=c(2,2))<br/><br/># flatten layer and then fully connected layers<br/>flatten &lt;- mx.symbol.Flatten(data=pool2)<br/>fullconnect1 &lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=512)<br/>activation3 &lt;- mx.symbol.Activation(data=fullconnect1, act_type="relu")<br/>fullconnect2 &lt;- mx.symbol.FullyConnected(data=activation3, num_hidden=10)<br/># final softmax layer<br/>softmax &lt;- mx.symbol.SoftmaxOutput(data=fullconnect2)<br/><br/></pre>
<p>Now, let's reshape the data so that it can be used in MXNet:</p>
<pre>train.array &lt;- train<br/>dim(train.array) &lt;- c(28,28,1,ncol(train))<br/>test.array &lt;- test<br/>dim(test.array) &lt;- c(28,28,1,ncol(test))</pre>
<p>Finally, we can build the model:</p>
<pre>devices &lt;- mx.gpu()<br/>mx.set.seed(0)<br/>model2 &lt;- mx.model.FeedForward.create(softmax, X=train.array, y=train.y,<br/>                                     ctx=devices,array.batch.size=128,<br/>                                     num.round=10,<br/>                                     learning.rate=0.05, momentum=0.9, wd=0.00001,<br/>                                     eval.metric=mx.metric.accuracy,<br/>                                     epoch.end.callback=mx.callback.log.train.metric(1))</pre>
<p>Finally, let's evaluate the model:</p>
<pre>preds2 &lt;- predict(model2, test.array)<br/>pred.label2 &lt;- max.col(t(preds2)) - 1<br/>res2 &lt;- data.frame(cbind(test.y,pred.label2))<br/>table(res2)<br/>## pred.label2<br/>## test.y   0   1   2   3   4   5   6   7   8   9<br/>##      0 412   0   0   0   0   1   1   1   0   1<br/>##      1   0 447   1   1   1   0   0   4   1   0<br/>##      2   0   0 438   0   0   0   0   3   0   0<br/>##      3   0   0   6 427   0   1   0   1   2   0<br/>##      4   0   0   0   0 395   0   0   1   0   4<br/>##      5   1   0   0   5   0 369   2   0   1   4<br/>##      6   2   0   0   0   1   1 432   0   1   0<br/>##      7   0   0   2   0   0   0   0 399   0   2<br/>##      8   1   0   1   0   1   1   1   1 414   3<br/>##      9   2   0   0   0   4   0   0   1   1 398<br/><br/><br/>accuracy2<br/>## 0.9835714</pre>
<p>The accuracy of our CNN model is <kbd>0.9835714</kbd>, which is quite an improvement over the accuracy of our base model, which was <kbd>0.971</kbd>.</p>
<p>Finally, we can visualize our model in R:</p>
<pre>graph.viz(model2$symbol)</pre>
<p>This produces the following plot, which shows the architecture of the deep learning model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44ca3de2-5605-49eb-9042-80099ff1b36f.png" style="text-align: center;background-color: #ffffff;font-family: Merriweather, serif;font-size: 1em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.14: Convolutional deep learning model (LeNet)</div>
<p>Congratulations! You have built a deep learning model that is over 98% accurate!</p>
<p><span>We saw the architecture of LeNet in <em>Figure 5.1</em> and we have programmed it using the MXNet library. </span>Let's analyze the LeNet architecture in more detail. Essentially, we have two convolutional groups and two fully connected layers. Our convolutional groups have a convolutional layer, followed by an <kbd>activation</kbd> function and then a pooling layer. This combination of layers is very common across many deep learning image classification tasks. The first convolution layer has 64 blocks of 5 x 5 size with no padding. This will possibly miss some features at the edges of the images, but if we look back at our sample images in <em>Figure 5.15</em>, we can see that most images do not have any data around the borders. We use pooling layers with <kbd>pool_type=max</kbd>. Other types are possible; average pooling was commonly used but has fallen out of favor recently. It is another hyper-parameter to try. We calculate our pools in 2 x 2 and then stride <kbd>(“jump”)</kbd> by 2. Therefore, each input value is only used once in the max pool layer.</p>
<p>We use <kbd>tanh</kbd> as an <kbd>activation</kbd> function for our first convolutional block, and then use <span> </span><kbd>relu</kbd><span> </span> for subsequent layers. If you wish, you can try to change these and see what effect they have. Once we have executed our convolutional layers, we can use Flatten to restructure the data into a format that can be used by a fully connected layer. A fully connected layer is just a collection of nodes in a layer, that is, similar to the layers in the base model in the previous code. We have two layers, one with 512 nodes and the other with 10 nodes. We select 10 nodes in our last layer as this is the number of categories in our problem. Finally, we use a softmax to convert the numeric quantities in this layer into a set of probabilities for each category. <span>We have achieved 98.35% accuracy, which is quite an improvement on a <em>normal</em> deep learning model, but </span><span>there is still room for improvement</span><span>. Some models can get 99.5% accuracy on this dataset, that is, 5 wrongly classified records in 1,000. Next, we will look at a different dataset that, while similar to MNIST, is harder than MNIST. This is the Fashion <kbd>MNIST</kbd> dataset, which has grayscale images of the same size as MNIST and also has 10 categories.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification using the fashion MNIST dataset</h1>
                </header>
            
            <article>
                
<p>This dataset is in the same structure as <kbd>MNIST</kbd>, so we can just change our dataset and use the existing boilerplate code we have for loading the data. The script <kbd>Chapter5/explore_Fashion.Rmd</kbd> is an R markdown file that explores this dataset; it is almost identical to the <kbd>explore.Rmd</kbd> that we used for the <kbd>MNIST</kbd> dataset, so we will not repeat it. The only change to the <kbd>explore.Rmd</kbd> is to output the labels. We will look at 16 examples because this is a new dataset. Here are some sample images from this dataset that are created using the same boilerplate code we used to create the example for the <kbd>MNIST</kbd> dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/367f26fc-344b-499f-a1de-5e190590764f.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.15: Some images from the Fashion MNIST dataset</div>
<p>An interesting fact about this dataset is that the company that released it also created a GitHub repository where they tested machine learning libraries against this dataset. The benchmarks are available at <a href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/">http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/</a>. If we look through these results, none of the machine libraries they tried achieved over 90% accuracy (they did not try deep learning). This is the target we want to beat with a deep learning classifier. The deep learning model code is in <kbd>Chapter5/fmnist.R</kbd> and achieves over 91% accuracy on this dataset. There are some small, but significant differences to the model architecture above. Try to spot them without peeking at the explanation.</p>
<p>First, let's define the model architecture.</p>
<pre>data &lt;- mx.symbol.Variable('data')<br/># first convolution layer<br/>convolution1 &lt;- mx.symbol.Convolution(data=data, kernel=c(5,5),<br/>                                      stride=c(1,1), pad=c(2,2), num_filter=64)<br/>activation1 &lt;- mx.symbol.Activation(data=convolution1, act_type=act_type1)<br/>pool1 &lt;- mx.symbol.Pooling(data=activation1, pool_type="max",<br/>                           kernel=c(2,2), stride=c(2,2))<br/> <br/># second convolution layer<br/>convolution2 &lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5),<br/>                                      stride=c(1,1), pad=c(2,2), num_filter=32)<br/>activation2 &lt;- mx.symbol.Activation(data=convolution2, act_type=act_type1)<br/>pool2 &lt;- mx.symbol.Pooling(data=activation2, pool_type="max",<br/>                           kernel=c(2,2), stride=c(2,2))<br/> <br/># flatten layer and then fully connected layers with activation and dropout<br/>flatten &lt;- mx.symbol.Flatten(data=pool2)<br/>fullconnect1 &lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=512)<br/>activation3 &lt;- mx.symbol.Activation(data=fullconnect1, act_type=act_type1)<br/>drop1 &lt;- mx.symbol.Dropout(data=activation3,p=0.4)<br/>fullconnect2 &lt;- mx.symbol.FullyConnected(data=drop1, num_hidden=10)<br/># final softmax layer<br/>softmax &lt;- mx.symbol.SoftmaxOutput(data=fullconnect2)</pre>
<p>Now let's train the model:</p>
<pre>logger &lt;- mx.metric.logger$new()<br/>model2 &lt;- mx.model.FeedForward.create(softmax, X=train.array, y=train.y,<br/>                                     ctx=devices, num.round=20,<br/>                                     array.batch.size=64,<br/>                                     learning.rate=0.05, momentum=0.9,<br/>                                     wd=0.00001,<br/>                                     eval.metric=mx.metric.accuracy,<br/>                                     eval.data=list(data=test.array,labels=test.y),<br/>                                     epoch.end.callback=mx.callback.log.train.metric(100,logger))</pre>
<p>The first change is that we switch to use <kbd>relu</kbd> as an <kbd>Activation</kbd> function for all layers. Another change was that we use padding for the convolutional layers, which we did to capture the features at the borders of the image. We increased the number of nodes in each layer to add depth to the model. We also added a dropout layer to prevent the model from overfitting. We also added logging to our model, which outputs the train and validation metrics for each epoch. We used these to check how our model performs and to decide if it is overfitting.</p>
<p>Here are the accuracy results and the diagnostic plot for this model:</p>
<pre>preds2 &lt;- predict(model2, test.array)<br/>pred.label2 &lt;- max.col(t(preds2)) - 1<br/>res2 &lt;- data.frame(cbind(test.y,pred.label2))<br/>table(res2)<br/>      pred.label2<br/>test.y   0   1   2   3   4   5   6   7   8   9<br/>     0 489   0  12  10   0   0  53   0   3   0<br/>     1   0 586   1   6   1   0   1   0   0   0<br/>     2   8   1 513   7  56   0  31   0   0   0<br/>     3  13   0   3 502  16   0  26   1   1   0<br/>     4   1   1  27  13 517   0  32   0   2   0<br/>     5   1   0   0   0   0 604   0   9   0   3<br/>     6  63   0  47   9  28   0 454   0   3   0<br/>     7   0   0   0   1   0  10   0 575   1  11<br/>     8   0   0   1   0   1   2   1   0 618   0<br/>     9   0   0   0   0   0   1   0  17   1 606<br/>accuracy2 &lt;- sum(res2$test.y == res2$pred.label2) / nrow(res2)<br/>accuracy2<br/># 0.9106667</pre>
<p>One thing to note is that we are using the same validation/test set for showing metrics during training and to evaluate the final model. This is not a good practice, but it is acceptable here because we are not using validation metrics to tune the hyperparameters of the model. The accuracy of our CNN model is <kbd>0.9106667</kbd>.</p>
<p>Let's plot the accuracy of the train and validation sets as the model is trained. <span>The deep learning model code has a <kbd>callback</kbd> function which saves the metrics as the model is trained. We can use this to plot the training and validation metrics for each epoch:</span></p>
<pre># use the log data collected during model training<br/>dfLogger&lt;-as.data.frame(round(logger$train,3))<br/>dfLogger2&lt;-as.data.frame(round(logger$eval,3))<br/>dfLogger$eval&lt;-dfLogger2[,1]<br/>colnames(dfLogger)&lt;-c("train","eval")<br/>dfLogger$epoch&lt;-as.numeric(row.names(dfLogger))<br/> <br/>data_long &lt;- melt(dfLogger, id="epoch")<br/> <br/>ggplot(data=data_long,<br/>       aes(x=epoch, y=value, colour=variable,label=value)) +<br/>  ggtitle("Model Accuracy") +<br/>  ylab("accuracy") +<br/>  geom_line()+geom_point() +<br/>  geom_text(aes(label=value),size=3,hjust=0, vjust=1) +<br/>  theme(legend.title=element_blank()) +<br/>  theme(plot.title = element_text(hjust = 0.5)) +<br/>  scale_x_discrete(limits= 1:nrow(dfLogger))</pre>
<p><span>This shows us how our model is performing after each epoch (or training run). </span>This produces the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b92b126f-4ded-40c8-b316-1fbae5700357.png" style="width:43.00em;height:30.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 5.16: Training and validation accuracy by epoch</span></div>
<p>The two main points to be taken from this graph are that:</p>
<ul>
<li style="font-weight: 400">The model is overfitting. We can see a clear gap between performance on the training set at <strong>0.95xxx</strong> and the validation set at <strong>0.91xxx</strong>.</li>
<li style="font-weight: 400">We could have probably stopped the model training after 8 epochs, as performance did not improve after this point.</li>
</ul>
<p>As we have discussed in previous chapters, deep learning models will almost always overfit by default, but there are methods to negate this. The second issue is related to <em>early stopping</em>, and it is vital that you know how to do this so that you do not waste hours in continuing to train a model which is no longer improving. This is especially relevant if you are building the model using cloud resources. We will look at these and more issues related to building deep learning models in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References/further reading</h1>
                </header>
            
            <article>
                
<p>These papers are classical deep learning papers in this domain. Some of them document winning approaches to ImageNet competitions. I encourage you to download and read all of them. You may not understand them at first, but their importance will become more evident as you continue on your journey in deep learning.</p>
<ul>
<li>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. Advances in neural information processing systems. 2012.</li>
<li>Szegedy, Christian, et al. <em>Going Deeper with Convolutions</em>. Cvpr, 2015.</li>
<li>LeCun, Yann, et al. <em>Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition</em>. Neural networks: the statistical mechanics perspective 261 (1995): 276.</li>
<li>Zeiler, Matthew D., and Rob Fergus. <em>Visualizing and Understanding Convolutional Networks</em>. European conference on computer vision. Springer, Cham, 2014.</li>
<li>Srivastava, Nitish, et al. <em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em>. The Journal of Machine Learning Research 15.1 (2014): 1929-1958.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we used deep learning for image classification. We discussed the different layer types that are used in image classification: convolutional layers, pooling layers, dropout, dense layers, and the softmax activation function. We saw an R-Shiny application that shows how convolutional layers perform feature engineering on image data.</p>
<p>We used <span>the MXNet deep learning library in R to </span>create a base deep learning model which got 97.1% accuracy. We then developed a <span>CNN </span>deep learning model based on the LeNet architecture, which achieved over 98.3% accuracy on test data. We also used a slightly harder dataset (<kbd>Fashion MNIST</kbd>) and created a new model that achieved over 91% accuracy. This accuracy score was better than all of the other scores that used non-deep learning algorithms. In the next chapter, we will build on what we have covered and show you how we can take advantage of pre-trained models for classification and as building blocks for new deep learning models.</p>
<p>In the next chapter, we are going to discuss important topics in deep learning concerning tuning and optimizing your models. This includes how to use the limited data you may have, data pre-processing, data augmentation, and hyperparameter selection.</p>


            </article>

            
        </section>
    </body></html>