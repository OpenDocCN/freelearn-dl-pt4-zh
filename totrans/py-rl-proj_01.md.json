["```py\n# Install virtualenv using pip\n$ pip install virtualenv\n\n# Create a virtualenv\n$ virtualenv rl_projects\n\n# Activate virtualenv\n$ source rl_projects/bin/activate\n\n# cd into the directory with our requirements.txt\n(rl_projects) $ cd /path/to/requirements.txt\n\n# pip install the required packages\n(rl_projects) $ pip install -r requirements.txt\n```", "```py\n- P queries E for some observation \n- P decides to take action  based on observation \n- E receives  and returns reward  based on the action\n- P receives \n- E updates  to  based on  and other factors\n```", "```py\nimport logging\nimport os\nimport sys\n\nlogger = logging.getLogger(__name__)\n\nimport tensorflow as tf\nimport numpy as np\nfrom keras.datasets import fashion_mnist\nfrom keras.utils import np_utils\n```", "```py\nclass SimpleCNN(object):\n\n    def __init__(self, learning_rate, num_epochs, beta, batch_size):\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.beta = beta\n        self.batch_size = batch_size\n        self.save_dir = \"saves\"\n        self.logs_dir = \"logs\"\n        os.makedirs(self.save_dir, exist_ok=True)\n        os.makedirs(self.logs_dir, exist_ok=True)\n        self.save_path = os.path.join(self.save_dir, \"simple_cnn\")\n        self.logs_path = os.path.join(self.logs_dir, \"simple_cnn\")\n```", "```py\ndef build(self, input_tensor, num_classes):\n    \"\"\"\n    Builds a convolutional neural network according to the input shape and the number of classes.\n    Architecture is fixed.\n\n    Args:\n        input_tensor: Tensor of the input\n        num_classes: (int) number of classes\n\n    Returns:\n        The output logits before softmax\n    \"\"\"\n```", "```py\nwith tf.name_scope(\"input_placeholders\"):\n    self.is_training = tf.placeholder_with_default(True, shape=(), name=\"is_training\")\n```", "```py\nwith tf.name_scope(\"convolutional_layers\"):\n    conv_1 = tf.layers.conv2d(\n        input_tensor,\n        filters=16,\n        kernel_size=(5, 5),\n        strides=(1, 1),\n        padding=\"SAME\",\n        activation=tf.nn.relu,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"conv_1\")\n    conv_2 = tf.layers.conv2d(\n        conv_1,\n        filters=32,\n        kernel_size=(3, 3),\n        strides=(1, 1),\n        padding=\"SAME\",\n        activation=tf.nn.relu,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"conv_2\")\n    pool_3 = tf.layers.max_pooling2d(\n        conv_2,\n        pool_size=(2, 2),\n        strides=1,\n        padding=\"SAME\",\n        name=\"pool_3\"\n    )\n    drop_4 = tf.layers.dropout(pool_3, training=self.is_training, name=\"drop_4\")\n\n    conv_5 = tf.layers.conv2d(\n        drop_4,\n        filters=64,\n        kernel_size=(3, 3),\n        strides=(1, 1),\n        padding=\"SAME\",\n        activation=tf.nn.relu,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"conv_5\")\n    conv_6 = tf.layers.conv2d(\n        conv_5,\n        filters=128,\n        kernel_size=(3, 3),\n        strides=(1, 1),\n        padding=\"SAME\",\n        activation=tf.nn.relu,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"conv_6\")\n    pool_7 = tf.layers.max_pooling2d(\n        conv_6,\n        pool_size=(2, 2),\n        strides=1,\n        padding=\"SAME\",\n        name=\"pool_7\"\n    )\n    drop_8 = tf.layers.dropout(pool_7, training=self.is_training, name=\"drop_8\")\n```", "```py\nwith tf.name_scope(\"fully_connected_layers\"):\n    flattened = tf.layers.flatten(drop_8, name=\"flatten\")\n    fc_9 = tf.layers.dense(\n        flattened,\n        units=1024,\n        activation=tf.nn.relu,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"fc_9\"\n    )\n    drop_10 = tf.layers.dropout(fc_9, training=self.is_training, name=\"drop_10\")\n    logits = tf.layers.dense(\n        drop_10,\n        units=num_classes,\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.beta),\n        name=\"logits\"\n    )\n\nreturn logits\n```", "```py\ndef _create_tf_dataset(self, x, y):\n    dataset = tf.data.Dataset.zip((\n            tf.data.Dataset.from_tensor_slices(x),\n            tf.data.Dataset.from_tensor_slices(y)\n        )).shuffle(50).repeat().batch(self.batch_size)\n    return dataset\n\ndef _log_loss_and_acc(self, epoch, loss, acc, suffix):\n    summary = tf.Summary(value=[\n        tf.Summary.Value(tag=\"loss_{}\".format(suffix), simple_value=float(loss)),\n        tf.Summary.Value(tag=\"acc_{}\".format(suffix), simple_value=float(acc))\n    ])\n    self.summary_writer.add_summary(summary, epoch)\n```", "```py\ndef fit(self, X_train, y_train, X_valid, y_valid):\n    \"\"\"\n    Trains a CNN on given data\n\n    Args:\n        numpy.ndarrays representing data and labels respectively\n    \"\"\"\n    graph = tf.Graph()\n    with graph.as_default():\n        sess = tf.Session()\n```", "```py\ntrain_dataset = self._create_tf_dataset(X_train, y_train)\nvalid_dataset = self._create_tf_dataset(X_valid, y_valid)\n\n# Creating a generic iterator\niterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n                                           train_dataset.output_shapes)\nnext_tensor_batch = iterator.get_next()\n\n# Separate training and validation set init ops\ntrain_init_ops = iterator.make_initializer(train_dataset)\nvalid_init_ops = iterator.make_initializer(valid_dataset)\n\ninput_tensor, labels = next_tensor_batch\n```", "```py\nnum_classes = y_train.shape[1]\n\n# Building the network\nlogits = self.build(input_tensor=input_tensor, num_classes=num_classes)\nlogger.info('Built network')\n\nprediction = tf.nn.softmax(logits, name=\"predictions\")\nloss_ops = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n    labels=labels, logits=logits), name=\"loss\")\n```", "```py\noptimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\ntrain_ops = optimizer.minimize(loss_ops)\n\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1), name=\"correct\")\naccuracy_ops = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n```", "```py\ninitializer = tf.global_variables_initializer()\n\nlogger.info('Initializing all variables')\nsess.run(initializer)\nlogger.info('Initialized all variables')\n\nsess.run(train_init_ops)\nlogger.info('Initialized dataset iterator')\nself.saver = tf.train.Saver()\nself.summary_writer = tf.summary.FileWriter(self.logs_path)\n```", "```py\nlogger.info(\"Training CNN for {} epochs\".format(self.num_epochs))\nfor epoch_idx in range(1, self.num_epochs+1):\n    loss, _, accuracy = sess.run([\n        loss_ops, train_ops, accuracy_ops\n    ])\n    self._log_loss_and_acc(epoch_idx, loss, accuracy, \"train\")\n\n    if epoch_idx % 10 == 0:\n        sess.run(valid_init_ops)\n        valid_loss, valid_accuracy = sess.run([\n            loss_ops, accuracy_ops\n        ], feed_dict={self.is_training: False})\n        logger.info(\"=====================> Epoch {}\".format(epoch_idx))\n        logger.info(\"\\tTraining accuracy: {:.3f}\".format(accuracy))\n        logger.info(\"\\tTraining loss: {:.6f}\".format(loss))\n        logger.info(\"\\tValidation accuracy: {:.3f}\".format(valid_accuracy))\n        logger.info(\"\\tValidation loss: {:.6f}\".format(valid_loss))\n        self._log_loss_and_acc(epoch_idx, valid_loss, valid_accuracy, \"valid\")\n\n    # Creating a checkpoint at every epoch\n    self.saver.save(sess, self.save_path)\n```", "```py\nif __name__ == \"__main__\":\n    logging.basicConfig(stream=sys.stdout,\n                        level=logging.DEBUG,\n                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n    logger = logging.getLogger(__name__)\n\n    logger.info(\"Loading Fashion MNIST data\")\n    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n```", "```py\nlogger.info('Shape of training data:')\nlogger.info('Train: {}'.format(X_train.shape))\nlogger.info('Test: {}'.format(X_test.shape))\n\nlogger.info('Adding channel axis to the data')\nX_train = X_train[:,:,:,np.newaxis]\nX_test = X_test[:,:,:,np.newaxis]\n\nlogger.info(\"Simple transformation by dividing pixels by 255\")\nX_train = X_train / 255.\nX_test = X_test / 255.\n\nX_train = X_train.astype(np.float32)\nX_test = X_test.astype(np.float32)\ny_train = y_train.astype(np.float32)\ny_test = y_test.astype(np.float32)\nnum_classes = len(np.unique(y_train))\n\nlogger.info(\"Turning ys into one-hot encodings\")\ny_train = np_utils.to_categorical(y_train, num_classes=num_classes)\ny_test = np_utils.to_categorical(y_test, num_classes=num_classes)\n```", "```py\ncnn_params = {\n    \"learning_rate\": 3e-4,\n    \"num_epochs\": 100,\n    \"beta\": 1e-3,\n    \"batch_size\": 32\n}\n```", "```py\nlogger.info('Initializing CNN')\nsimple_cnn = SimpleCNN(**cnn_params)\nlogger.info('Training CNN')\nsimple_cnn.fit(X_train=X_train,\n               X_valid=X_test,\n               y_train=y_train,\n               y_valid=y_test)\n```", "```py\n$ python cnn.py\n```", "```py\n$ python cnn.py\nUsing TensorFlow backend.\n2018-07-29 21:21:55,423 __main__ INFO Loading Fashion MNIST data\n2018-07-29 21:21:55,686 __main__ INFO Shape of training data:\n2018-07-29 21:21:55,687 __main__ INFO Train: (60000, 28, 28)\n2018-07-29 21:21:55,687 __main__ INFO Test: (10000, 28, 28)\n2018-07-29 21:21:55,687 __main__ INFO Adding channel axis to the data\n2018-07-29 21:21:55,687 __main__ INFO Simple transformation by dividing pixels by 255\n2018-07-29 21:21:55,914 __main__ INFO Turning ys into one-hot encodings\n2018-07-29 21:21:55,914 __main__ INFO Initializing CNN\n2018-07-29 21:21:55,914 __main__ INFO Training CNN\n2018-07-29 21:21:58,365 __main__ INFO Built network\n2018-07-29 21:21:58,562 __main__ INFO Initializing all variables\n2018-07-29 21:21:59,284 __main__ INFO Initialized all variables\n2018-07-29 21:21:59,639 __main__ INFO Initialized dataset iterator\n2018-07-29 21:22:00,880 __main__ INFO Training CNN for 100 epochs\n2018-07-29 21:24:23,781 __main__ INFO =====================> Epoch 10\n2018-07-29 21:24:23,781 __main__ INFO Training accuracy: 0.406\n2018-07-29 21:24:23,781 __main__ INFO Training loss: 1.972021\n2018-07-29 21:24:23,781 __main__ INFO Validation accuracy: 0.500\n2018-07-29 21:24:23,782 __main__ INFO Validation loss: 2.108872\n2018-07-29 21:27:09,541 __main__ INFO =====================> Epoch 20\n2018-07-29 21:27:09,541 __main__ INFO Training accuracy: 0.469\n2018-07-29 21:27:09,541 __main__ INFO Training loss: 1.573592\n2018-07-29 21:27:09,542 __main__ INFO Validation accuracy: 0.500\n2018-07-29 21:27:09,542 __main__ INFO Validation loss: 1.482948\n2018-07-29 21:29:57,750 __main__ INFO =====================> Epoch 30\n2018-07-29 21:29:57,750 __main__ INFO Training accuracy: 0.531\n2018-07-29 21:29:57,750 __main__ INFO Training loss: 1.119335\n2018-07-29 21:29:57,750 __main__ INFO Validation accuracy: 0.625\n2018-07-29 21:29:57,750 __main__ INFO Validation loss: 0.905031\n2018-07-29 21:32:45,921 __main__ INFO =====================> Epoch 40\n2018-07-29 21:32:45,922 __main__ INFO Training accuracy: 0.656\n2018-07-29 21:32:45,922 __main__ INFO Training loss: 0.896715\n2018-07-29 21:32:45,922 __main__ INFO Validation accuracy: 0.719\n2018-07-29 21:32:45,922 __main__ INFO Validation loss: 0.847015\n```", "```py\n$ tensorboard --logdir=logs/\n```"]