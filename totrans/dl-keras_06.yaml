- en: Recurrent Neural Network — RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml), *Deep Learning with
    ConvNets*, we learned about **convolutional neural networks** (CNN) and saw how
    they exploit the spatial geometry of their input. For example, CNNs apply convolution
    and pooling operations in one dimension for audio and text data along the time
    dimension, in two dimensions for images along the (height x width) dimensions
    and in three dimensions, for videos along the (height x width x time) dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about **recurrent neural networks** (**RNN**),
    a class of neural networks that exploit the sequential nature of their input.
    Such inputs could be text, speech, time series, and anything else where the occurrence
    of an element in the sequence is dependent on the elements that appeared before
    it. For example, the next word in the sentence *the dog...* is more likely to
    be *barks* than *car*, therefore, given such a sequence, an RNN is more likely
    to predict *barks* than *car*.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN can be thought of as a graph of RNN cells, where each cell performs the
    same operation on every element in the sequence. RNNs are very flexible and have
    been used to solve problems such as speech recognition, language modeling, machine
    translation, sentiment analysis, and image captioning, to name a few. RNNs can
    be adapted to different types of problems by rearranging the way the cells are
    arranged in the graph. We will see some examples of these configurations and how
    they are used to solve specific problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also learn about a major limitation of the SimpleRNN cell, and how
    two variants of the SimpleRNN cell—**long short term memory** (**LSTM**) and **gated
    recurrent unit** (**GRU**)—overcome this limitation. Both LSTM and GRU are drop-in
    replacements for the SimpleRNN cell, so just replacing the RNN cell with one of
    these variants can often result in a major performance improvement in your network.
    While LSTM and GRU are not the only variants, it has been shown empirically (for
    more information refer to the articles: *An Empirical Exploration of Recurrent
    Network Architectures*, by R. Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015
    and *LSTM: A Search Space Odyssey*, by K. Greff, arXiv:1503.04069, 2015) that
    they are the best choices for most sequence problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will also learn about some tips to improve the performance of our
    RNNs and when and how to apply them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: SimpleRNN cell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic RNN implementation in Keras in generating text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM, GRU, and other RNN variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SimpleRNN cells
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional multilayer perceptron neural networks make the assumption that all
    inputs are independent of each other. This assumption breaks down in the case
    of sequence data. You have already seen the example in the previous section where
    the first two words in the sentence affect the third. The same idea is true of
    speech—if we are having a conversation in a noisy room, I can make reasonable
    guesses about a word I may not have understood based on the words I have heard
    so far. Time series data, such as stock prices or weather, also exhibit a dependence
    on past data, called the secular trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN cells incorporate this dependence by having a hidden state, or memory,
    that holds the essence of what has been seen so far. The value of the hidden state
    at any point in time is a function of the value of the hidden state at the previous
    time step and the value of the input at the current time step, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/rnn-eq1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*h[t]* and *h[t-1]* are the values of the hidden states at the time steps *t*
    and *t-1* respectively, and *x[t]* is the value of the input at time *t*. Notice
    that the equation is recursive, that is, *h[t-1]* can be represented in terms
    of *h[t-2]* and *x[t-1]*, and so on, until the beginning of the sequence. This
    is how RNNs encode and incorporate information from arbitrarily long sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also represent the RNN cell graphically as shown in the following diagram
    on the left. At time *t*, the cell has an input *x[t]* and an output *y[t]*. Part
    of the output *y[t]* (the hidden state *h[t]*) is fed back into the cell for use
    at a later time step *t+1*. Just as a traditional neural network''s parameters
    are contained in its weight matrix, the RNN''s parameters are defined by three
    weight matrices *U*, *V*, and *W*, corresponding to the input, output, and hidden
    state respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/basic-rnn-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Another way to look at an RNN to *unroll* it, as shown in the preceding diagram
    on the right. Unrolling means that we draw the network out for the complete sequence.
    The network shown here is a three-layer RNN, suitable for processing three element
    sequences. Notice that the weight matrices *U*, *V*, and *W* are shared across
    the steps. This is because we are applying the same operation on different inputs
    at each time step. Being able to share these weight vectors across all the time
    steps greatly reduces the number of parameters that the RNN needs to learn.
  prefs: []
  type: TYPE_NORMAL
- en: We can also describe the computations within an RNN in terms of equations. The
    internal state of the RNN at a time *t* is given by the value of the hidden vector
    *h[t]*, which is the sum of the product of the weight matrix *W* and the hidden
    state *h[t-1]* at time *t-1* and the product of the weight matrix *U* and the
    input *x[t]* at time *t*, passed through the *tanh* nonlinearity. The choice of
    *tanh* over other nonlinearities has to do with its second derivative decaying
    very slowly to zero. This keeps the gradients in the linear region of the activation
    function and helps combat the vanishing gradient problem. We will learn more about
    the vanishing gradient problem later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output vector *y[t]* at time *t* is the product of the weight matrix *V*
    and the hidden state *h[t]*, with *softmax* applied to the product so the resulting
    vector is a set of output probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/rnn-eq2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Keras provides the SimpleRNN (for more information refer to: [https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/))
    recurrent layer that incorporates all the logic we have seen so far, as well as
    the more advanced variants such as LSTM and GRU that we will see later in this
    chapter, so it is not strictly necessary to understand how they work in order
    to start building with them. However, an understanding of the structure and equations
    is helpful when you need to compose your own RNN to solve a given problem.'
  prefs: []
  type: TYPE_NORMAL
- en: SimpleRNN with Keras — generating text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs have been used extensively by the **natural language processing** (**NLP**)
    community for various applications. One such application is building language
    models. A language model allows us to predict the probability of a word in a text
    given the previous words. Language models are important for various higher level
    tasks such as machine translation, spelling correction, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A side effect of the ability to predict the next word given previous words is
    a generative model that allows us to generate text by sampling from the output
    probabilities. In language modeling, our input is typically a sequence of words
    and the output is a sequence of predicted words. The training data used is existing
    unlabeled text, where we set the label *y[t]* at time *t* to be the input *x[t+1]*
    at time *t+1*.
  prefs: []
  type: TYPE_NORMAL
- en: For our first example of using Keras for building RNNs, we will train a character
    based language model on the text of *Alice in Wonderland* to predict the next
    character given 10 previous characters. We have chosen to build a character-based
    model here because it has a smaller vocabulary and trains quicker. The idea is
    the same as using a word-based language model, except we use characters instead
    of words. We will then use the trained model to generate some text in the same
    style.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we import the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We read our input text from the text of *Alice in Wonderland* on the Project
    Gutenberg website ([http://www.gutenberg.org/files/11/11-0.txt](http://www.gutenberg.org/files/11/11-0.txt)).
    The file contains line breaks and non-ASCII characters, so we do some preliminary
    cleanup and write out the contents into a variable called `text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are building a character-level RNN, our vocabulary is the set of characters
    that occur in the text. There are 42 of them in our case. Since we will be dealing
    with the indexes to these characters rather than the characters themselves, the
    following code snippet creates the necessary lookup tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create the input and label texts. We do this by stepping
    through the text by a number of characters given by the `STEP` variable (`1` in
    our case) and then extracting a span of text whose size is determined by the `SEQLEN`
    variable (`10` in our case). The next character after the span is our label character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding code, the input and label texts for the text `it turned
    into a pig` would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to vectorize these input and label texts. Each row of the
    input to the RNN corresponds to one of the input texts shown previously. There
    are `SEQLEN` characters in this input, and since our vocabulary size is given
    by `nb_chars`, we represent each input character as a one-hot encoded vector of
    size (`nb_chars`). Thus each input row is a tensor of size (`SEQLEN` and `nb_chars`).
    Our output label is a single character, so similar to the way we represent each
    character of our input, it is represented as a one-hot vector of size (`nb_chars`).
    Thus, the shape of each label is `nb_chars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we are ready to build our model. We define the RNN's output dimension
    to have a size of 128\. This is a hyper-parameter that needs to be determined
    by experimentation. In general, if we choose too small a size, then the model
    does not have sufficient capacity for generating good text, and you will see long
    runs of repeating characters or runs of repeating word groups. On the other hand,
    if the value chosen is too large, the model has too many parameters and needs
    a lot more data to train effectively. We want to return a single character as
    output, not a sequence of characters, so `return_sequences=False`. We have already
    seen that the input to the RNN is of shape (`SEQLEN` and `nb_chars`). In addition,
    we set `unroll=True` because it improves performance on the TensorFlow backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN is connected to a dense (fully connected) layer. The dense layer has
    (`nb_char`) units, which emits scores for each of the characters in the vocabulary.
    The activation on the dense layer is a softmax, which normalizes the scores to
    probabilities. The character with the highest probability is chosen as the prediction.
    We compile the model with the categorical cross-entropy loss function, a good
    loss function for categorical outputs, and the RMSprop optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Our training approach is a little different from what we have seen so far. So
    far our approach has been to train a model for a fixed number of epochs, then
    evaluate it against a portion of held-out test data. Since we don't have any labeled
    data here, we train the model for an epoch (`NUM_EPOCHS_PER_ITERATION=1`) then
    test it. We continue training like this for 25 (`NUM_ITERATIONS=25`) iterations,
    stopping once we see intelligible output. So effectively, we are training for
    `NUM_ITERATIONS` epochs and testing the model after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test consists of generating a character from the model given a random input,
    then dropping the first character from the input and appending the predicted character
    from our previous run, and generating another character from the model. We continue
    this 100 times (`NUM_PREDS_PER_EPOCH=100`) and generate and print the resulting
    string. The string gives us an indication of the quality of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this run is shown as follows. As you can see, the model starts
    out predicting gibberish, but by the end of the 25th epoch, it has learned to
    spell reasonably well, although it has trouble expressing coherent thoughts. The
    amazing thing about this model is that it is character-based and has no knowledge
    of words, yet it learns to spell words that look like they might have come from
    the original text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generating the next character or next word of text is not the only thing you
    can do with this sort of model. This kind of model has been successfully used
    to make stock predictions (for more information refer to the article: *Financial
    Market Time Series Prediction with Recurrent Neural Networks*, by A. Bernal, S.
    Fok, and R. Pidaparthi, 2012) and generate classical music (for more information
    refer to the article: *DeepBach: A Steerable Model for Bach Chorales Generation*,
    by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016), to name a few interesting
    applications. Andrej Karpathy covers a few other fun examples, such as generating
    fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his
    blog post at: *The Unreasonable Effectiveness of Recurrent Neural Networks* at
    [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).'
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this example is available in `alice_chargen_rnn.py` in the
    code download for the chapter. The data is available from Project Gutenberg.
  prefs: []
  type: TYPE_NORMAL
- en: RNN topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The APIs for MLP and CNN architectures are limited. Both architectures accept
    a fixed-size tensor as input and produce a fixed-size tensor as output; and they
    perform the transformation from input to output in a fixed number of steps given
    by the number of layers in the model. RNNs don't have this limitation—you can
    have sequences in the input, the output, or both. This means that RNNs can be
    arranged in many ways to solve specific problems.
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned, RNNs combine the input vector with the previous state vector
    to produce a new state vector. This can be thought of as similar to running a
    program with some inputs and some internal variables. Thus RNNs can be thought
    of as essentially describing computer programs. In fact, it has been shown that
    RNNs are turing complete (for more information refer to the article: *On the Computational
    Power of Neural Nets*, by H. T. Siegelmann and E. D. Sontag, proceedings of the
    fifth annual workshop on computational learning theory, ACM, 1992.) in the sense
    that given the proper weights, they can simulate arbitrary programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This property of being able to work with sequences gives rise to a number of
    common topologies, some of which we''ll discuss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/rnn-toplogies.png)'
  prefs: []
  type: TYPE_IMG
- en: All these different topologies derive from the same basic structure shown in
    the preceding diagram. In this basic topology, all input sequences are of the
    same length and an output is produced at each time step. We have already seen
    an example of this with our character level RNN for generating words in *Alice
    in Wonderland*.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a many to many RNN could be a machine translation network
    shown as **(b)**, part of a general family of networks called sequence-to-sequence
    (for more information refer to: *Grammar as a Foreign Language*, by O. Vinyals,
    Advances in Neural Information Processing Systems, 2015). These take in a sequence
    and produces another sequence. In the case of machine translation, the input could
    be a sequence of English words in a sentence and the output could be the words
    in a translated Spanish sentence. In the case of a model that uses sequence-to-sequence
    to do **part-of-speech** (**POS**) tagging, the input could be the words in a
    sentence and the output could be the corresponding POS tags. It differs from the
    previous topology in that at certain time steps there is no input and at others
    there is no output. We will see an example of such a network later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other variants are the one-to-many network shown as **(c)**, an example of
    which could be an image captioning network (for more information refer to the
    article: *Deep Visual-Semantic Alignments for Generating Image Descriptions*,
    by A. Karpathy, and F. Li, Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, 2015.), where the input is an image and the output a
    sequence of words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, an example of a many-to-one network as shown in **(d)** could be
    a network that does sentiment analysis of sentences, where the input is a sequence
    of words and the output is a positive or negative sentiment (for more information
    refer to the article: *Recursive Deep Models for Semantic Compositionality over
    a Sentiment Treebank*, by R. Socher, Proceedings of the Conference on Empirical
    Methods in Natural Language Processing (EMNLP). Vol. 1631, 2013). We will see
    an (much simplified compared to the cited model) example of this topology as well
    later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like traditional neural networks, training the RNN also involves backpropagation.
    The difference in this case is that since the parameters are shared by all time
    steps, the gradient at each output depends not only on the current time step,
    but also on the previous ones. This process is called **backpropagation through
    time** (**BPTT**) (for more information refer to the article: *Learning Internal
    Representations by Backpropagating errors*, by G. E. Hinton, D. E. Rumelhart,
    and R. J. Williams, Parallel Distributed Processing: Explorations in the Microstructure
    of Cognition 1, 1985):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/rnn-bptt.png)'
  prefs: []
  type: TYPE_IMG
- en: Consider the small three layer RNN shown in the preceding diagram. During the
    forward propagation (shown by the solid lines), the network produces predictions
    that are compared to the labels to compute a loss *L[t]* at each time step. During
    backpropagation (shown by dotted lines), the gradients of the loss with respect
    to the parameters *U*, *V*, and *W* are computed at each time step and the parameters
    are updated with the sum of the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation shows the gradient of the loss with respect to *W*,
    the matrix that encodes weights for the long term dependencies. We focus on this
    part of the update because it is the cause of the vanishing and exploding gradient
    problem. The other two gradients of the loss with respect to the matrices *U*
    and *V* are also summed up across all time steps in a similar way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bptt-eq1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now look at what happens to the gradient of the loss at the last time
    step (*t=3*). As you can see, this gradient can be decomposed to a product of
    three sub gradients using the chain rule. The gradient of the hidden state *h2*
    with respect to *W* can be further decomposed as the sum of the gradient of each
    hidden state with respect to the previous one. Finally, each gradient of the hidden
    state with respect to the previous one can be further decomposed as the product
    of gradients of the current hidden state against the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bptt-eq2.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar calculations are done to compute the gradient of losses *L[1]* and *L[2]*
    (at time steps 1 and 2) with respect to *W* and to sum them into the gradient
    update for *W*. We will not explore the math further in this book. If you want
    to do so on your own, this WILDML blog post ([https://goo.gl/l06lbX](https://goo.gl/l06lbX))
    has a very good explanation of BPTT, including more detailed derivations of the
    mathematics behind the process.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, the final form of the gradient in the equation above tells
    us why RNNs have the problem of vanishing and exploding gradients. Consider the
    case where the individual gradients of a hidden state with respect to the previous
    one is less than one. As we backpropagate across multiple time steps, the product
    of gradients get smaller and smaller, leading to the problem of vanishing gradients.
    Similarly, if the gradients are larger than one, the products get larger and larger,
    leading to the problem of exploding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of vanishing gradients is that the gradients from steps that are
    far away do not contribute anything to the learning process, so the RNN ends up
    not learning long range dependencies. Vanishing gradients can happen for traditional
    neural networks as well, it is just more visible in case of RNNs, since RNNs tend
    to have many more layers (time steps) over which back propagation must occur.
  prefs: []
  type: TYPE_NORMAL
- en: Exploding gradients are more easily detectable, the gradients will become very
    large and then turn into **not a number** (**NaN**) and the training process will
    crash. Exploding gradients can be controlled by clipping them at a predefined
    threshold as discussed in the paper: *On the Difficulty of Training Recurrent
    Neural Networks*, by R. Pascanu, T. Mikolov, and Y. Bengio, ICML, Pp 1310-1318,
    2013.
  prefs: []
  type: TYPE_NORMAL
- en: While there are a few approaches to minimize the problem of vanishing gradients,
    such as proper initialization of the *W* matrix, using a ReLU instead of *tanh*
    layers, and pre-training the layers using unsupervised methods, the most popular
    solution is to use the LSTM or GRU architectures. These architectures have been
    designed to deal with the vanishing gradient problem and learn long term dependencies
    more effectively. We will learn more about LSTM and GRU architectures later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Long short term memory — LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM is a variant of RNN that is capable of learning long term dependencies.
    LSTMs were first proposed by Hochreiter and Schmidhuber and refined by many other
    researchers. They work well on a large variety of problems and are the most widely
    used type of RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen how the SimpleRNN uses the hidden state from the previous time
    step and the current input in a *tanh* layer to implement recurrence. LSTMs also
    implement recurrence in a similar way, but instead of a single *tanh* layer, there
    are four layers interacting in a very specific way. The following diagram illustrates
    the transformations that are applied to the hidden state at time step *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/lstm-cell.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram looks complicated, but let us look at it component by component.
    The line across the top of the diagram is the cell state *c*, and represents the
    internal memory of the unit. The line across the bottom is the hidden state, and
    the *i*, *f*, *o*, and *g* gates are the mechanism by which the LSTM works around
    the vanishing gradient problem. During training, the LSTM learns the parameters
    for these gates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to gain a deeper understanding of how these gates modulate the LSTM''s
    hidden state, let us consider the equations that show how it calculates the hidden
    state *h[t]* at time *t* from the hidden state *h[t-1]* at the previous time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/lstm-eq1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here *i*, *f*, and *o* are the input, forget, and output gates. They are computed
    using the same equations but with different parameter matrices. The sigmoid function
    modulates the output of these gates between zero and one, so the output vector
    produced can be multiplied element-wise with another vector to define how much
    of the second vector can pass through the first one.
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate defines how much of the previous state *h[t-1]* you want to
    allow to pass through. The input gate defines how much of the newly computed state
    for the current input *x[t]* you want to let through, and the output gate defines
    how much of the internal state you want to expose to the next layer. The internal
    hidden state *g* is computed based on the current input *x[t]* and the previous
    hidden state *h[t-1]*. Notice that the equation for *g* is identical to that for
    the SimpleRNN cell, but in this case we will modulate the output by the output
    of the input gate *i*.
  prefs: []
  type: TYPE_NORMAL
- en: Given *i*, *f*, *o*, and *g*, we can now calculate the cell state *c[t]* at
    time *t* in terms of *c[t-1]* at time (*t-1*) multiplied by the forget gate and
    the state *g* multiplied by the input gate *i*. So this is basically a way to
    combine the previous memory and the new input—setting the forget gate to *0* ignores
    the old memory and setting the input gate to *0* ignores the newly computed state.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the hidden state *h[t]* at time *t* is computed by multiplying the
    memory *c[t]* with the output gate.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to realize is that an LSTM is a drop-in replacement for a SimpleRNN
    cell, the only difference is that LSTMs are resistant to the vanishing gradient
    problem. You can replace an RNN cell in a network with an LSTM without worrying
    about any side effects. You should generally see better results along with longer
    training times.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to know more, WILDML blog post has a very detailed explanation
    of these LSTM gates and how they work. For a more visual explanation, take a look
    at Christopher Olah''s blog post: *Understanding LSTMs* ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    where he walks you step by step through these computations, with illustrations
    at each step.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM with Keras — sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras provides an LSTM layer that we will use here to construct and train a
    many-to-one RNN. Our network takes in a sentence (a sequence of words) and outputs
    a sentiment value (positive or negative). Our training set is a dataset of about
    7,000 short sentences from UMICH SI650 sentiment classification competition on
    Kaggle ([https://inclass.kaggle.com/c/si650winter11](https://inclass.kaggle.com/c/si650winter11)).
    Each sentence is labeled *1* or *0* for positive or negative sentiment respectively,
    which our network will learn to predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the imports, as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start, we want to do a bit of exploratory analysis on the data. Specifically
    we need to know how many unique words there are in the corpus and how many words
    are there in each sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this, we get the following estimates for our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using the number of unique words `len(word_freqs)`, we set our vocabulary size
    to a fixed number and treat all the other words as **out of vocabulary** (**OOV**)
    words and replace them with the pseudo-word UNK (for unknown). At prediction time,
    this will allow us to handle previously unseen words as OOV words as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of words in the sentence (`maxlen`) allows us to set a fixed sequence
    length and zero pad shorter sentences and truncate longer sentences to that length
    as appropriate. Even though RNNs handle variable sequence length, this is usually
    achieved either by padding and truncating as above, or by grouping the inputs
    in different batches by sequence length. We will use the former approach here.
    For the latter approach, Keras recommends using batches of size one (for more
    information refer to: [https://github.com/fchollet/keras/issues/40](https://github.com/fchollet/keras/issues/40)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the preceding estimates, we set our `VOCABULARY_SIZE` to `2002`. This
    is 2,000 words from our vocabulary plus the UNK pseudo-word and the PAD pseudo
    word (used for padding sentences to a fixed number of words), in our case 40 given
    by `MAX_SENTENCE_LENGTH`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we need a pair of lookup tables. Each row of input to the RNN is a sequence
    of word indices, where the indices are ordered by most frequent to least frequent
    word in the training set. The two lookup tables allow us to lookup an index given
    the word and the word given the index. This includes the `PAD` and `UNK` pseudo-words
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert our input sentences to word index sequences, pad them to the
    `MAX_SENTENCE_LENGTH` words. Since our output label in this case is binary (positive
    or negative sentiment), we don''t need to process the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we split the training set into a 80-20 training test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the structure of our RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/sentiment-lstm.png)'
  prefs: []
  type: TYPE_IMG
- en: The input for each row is a sequence of word indices. The sequence length is
    given by `MAX_SENTENCE_LENGTH`. The first dimension of the tensor is set to `None`
    to indicate that the batch size (the number of records fed to the network each
    time) is currently unknown at definition time; it is specified during run time
    using the `batch_size` parameter. So assuming an as-yet undetermined batch size,
    the shape of the input tensor is `(None, MAX_SENTENCE_LENGTH, 1)`. These tensors
    are fed into an embedding layer of size `EMBEDDING_SIZE` whose weights are initialized
    with small random values and learned during training. This layer will transform
    the tensor to a shape `(None,MAX_SENTENCE_LENGTH, EMBEDDING_SIZE)`. The output
    of the embedding layer is fed into an LSTM with sequence length `MAX_SENTENCE_LENGTH`
    and output layer size `HIDDEN_LAYER_SIZE`, so the output of the LSTM is a tensor
    of shape `(None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH)`. By default, the LSTM
    will output a single tensor of shape `(None, HIDDEN_LAYER_SIZE)` at its last sequence
    (`return_sequences=False`). This is fed to a dense layer with output size of `1`
    with a sigmoid activation function, so it will output either `0` (negative review)
    or `1` (positive review).
  prefs: []
  type: TYPE_NORMAL
- en: 'We compile the model using the binary cross-entropy loss function since it
    predicts a binary value, and the Adam optimizer, a good general purpose optimizer.
    Note that the hyperparameters `EMBEDDING_SIZE`, `HIDDEN_LAYER_SIZE`, `BATCH_SIZE`
    and `NUM_EPOCHS` (set as constants as follows) were tuned experimentally over
    several runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the network for `10` epochs (`NUM_EPOCHS`) and batch size of
    `32` (`BATCH_SIZE`). At each epoch we validate the model using the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this step shows how the loss decreases and accuracy increases
    over multiple epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also plot the loss and accuracy values over time using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/umich-lossplot.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we evaluate our model against the full test set and print the score
    and accuracy. We also pick a few random sentences from our test set and print
    the RNN''s prediction, the label and the actual sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the results, we get back close to 99% accuracy. The predictions
    the model makes for this particular set match exactly with the labels, although
    this is not the case for all predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-3.png)'
  prefs: []
  type: TYPE_IMG
- en: If you would like to run this code locally, you need to get the data from the
    Kaggle website.
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this example is available in the file `umich_sentiment_lstm.py`
    in the code download for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent unit — GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GRU is a variant of the LSTM and was introduced by K. Cho (for more information
    refer to: *Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, by K. Cho, arXiv:1406.1078, 2014). It retains the LSTM''s
    resistance to the vanishing gradient problem, but its internal structure is simpler,
    and therefore is faster to train, since fewer computations are needed to make
    updates to its hidden state. The gates for a GRU cell are illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/gru-cell.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of the input, forget, and output gates in the LSTM cell, the GRU cell
    has two gates,
  prefs: []
  type: TYPE_NORMAL
- en: 'an update gate *z*, and a reset gate r. The update gate defines how much previous
    memory to keep around and the reset gate defines how to combine the new input
    with the previous memory. There is no persistent cell state distinct from the
    hidden state as in LSTM. The following equations define the gating mechanism in
    a GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/gru-eq1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'According to several empirical evaluations (for more information refer to the
    articles: *An Empirical Exploration of Recurrent Network Architectures*, by R.
    Jozefowicz, W. Zaremba, and I. Sutskever, JMLR, 2015 and *Empirical Evaluation
    of Gated Recurrent Neural Networks on Sequence Modeling*, by J. Chung, arXiv:1412.3555\.
    2014), GRU and LSTM have comparable performance and there is no simple way to
    recommend one or the other for a specific task. While GRUs are faster to train
    and need less data to generalize, in situations where there is enough data, an
    LSTM''s greater expressive power may lead to better results. Like LSTMs, GRUs
    are drop-in replacements for the SimpleRNN cell.'
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides built in implementations of both `LSTM` and `GRU`, as well as
    the `SimpleRNN` class we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: GRU with Keras — POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras provides a GRU implementation, that we will use here to build a network
    that does POS tagging. A POS is a grammatical category of words that are used
    in the same way across multiple sentences. Examples of POS are nouns, verbs, adjectives,
    and so on. For example, nouns are typically used to identify things, verbs are
    typically used to identify what they do, and adjectives to describe some attribute
    of these things. POS tagging used to be done manually, but nowadays this is done
    automatically using statistical models. In recent years, deep learning has been
    applied to this problem as well (for more information refer to the article: *Natural
    Language Processing (almost) from Scratch*, by R. Collobert, Journal of Machine
    Learning Research, Pp. 2493-2537, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: For our training data, we will need sentences tagged with part of speech tags.
    The Penn Treebank ([https://catalog.ldc.upenn.edu/ldc99t42](https://catalog.ldc.upenn.edu/ldc99t42))
    is one such dataset, it is a human annotated corpus of about 4.5 million words
    of American English. However, it is a non-free resource. A 10% sample of the Penn
    Treebank is freely available as part of the NLTK ([http://www.nltk.org/](http://www.nltk.org/)),
    which we will use to train our network.
  prefs: []
  type: TYPE_NORMAL
- en: Our model will take in a sequence of words in a sentence and output the corresponding
    POS tags for each word. Thus for an input sequence consisting of the words [*The*,
    *cat*, *sat*, *on*, *the*, *mat*, *.*], the output sequence emitted would be the
    POS symbols [*DT*, *NN*, *VB*, *IN*, *DT*, *NN*].
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We then download the data from NLTK in a format suitable for our downstream
    code. Specifically, the data is available in parsed form as part of the NLTK Treebank
    corpus. We use the following Python code to download this data into two parallel
    files, one for the words in the sentences and one for the POS tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we want to explore the data a little to find out what vocabulary
    size to set. This time, we have to consider two different vocabularies, the source
    vocabulary for the words and the target vocabulary for the POS tags. We need to
    find the number of unique words in each vocabulary. We also need to find the maximum
    number of words in a sentence in our training corpus and the number of records.
    Because of the one-to-one nature of POS tagging, the last two values are identical
    for both vocabularies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code tells us that there are 10,947 unique words and 45 unique
    POS tags. The maximum sentence size is 249, and the number of sentences in the
    10% set is 3,914\. Using this information, we decide to consider only the top
    5,000 words for our source vocabulary. Our target vocabulary has 45 unique POS
    tags, we want to be able to predict all of them, so we will consider all of them
    in our vocabulary. Finally, we set 250 to be our maximum sequence length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like our sentiment analysis example, each row of the input will be represented
    as a sequence of word indices. The corresponding output will be a sequence of
    POS tag indices. So we need to build lookup tables to translate between the words/POS
    tags and their corresponding indices. Here is the code to do that. On the source
    side, we build a vocabulary index with two extra slots to hold the `PAD` and `UNK`
    pseudo-words. On the target side, we don''t drop any words so there is no need
    for the `UNK` pseudo-word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build our datasets to feed into our network. We will use
    these lookup tables to convert our input sentences into a word ID sequence of
    length `MAX_SEQLEN` (`250`). The labels need to be structured as a sequence of
    one-hot vectors of size `T_MAX_FEATURES` + 1 (`46`), also of length `MAX_SEQLEN`
    (`250`). The `build_tensor` function reads the data from the two files and converts
    them to the input and output tensors. Additional default parameters are passed
    in to build the output tensor. This triggers the call to `np_utils.to_categorical()`
    to convert the output sequence of POS tag IDs to one-hot vector representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then split the dataset into a 80-20 train-test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the schematic of our network. It looks complicated,
    so let us deconstruct it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/postag-gru.png)'
  prefs: []
  type: TYPE_IMG
- en: As previously, assuming that the batch size is as yet undetermined, the input
    to the network is a tensor of word IDs of shape `(None, MAX_SEQLEN, 1)`. This
    is sent through an embedding layer, which converts each word into a dense vector
    of shape (`EMBED_SIZE`), so the output tensor from this layer has the shape `(None,
    MAX_SEQLEN, EMBED_SIZE)`. This tensor is fed to the encoder GRU with an output
    size of `HIDDEN_SIZE`. The GRU is set to return a single context vector (`return_sequences=False`)
    after seeing a sequence of size `MAX_SEQLEN`, so the output tensor from the GRU
    layer has shape `(None, HIDDEN_SIZE)`.
  prefs: []
  type: TYPE_NORMAL
- en: This context vector is then replicated using the RepeatVector layer into a tensor
    of shape `(None, MAX_SEQLEN, HIDDEN_SIZE)` and fed into the decoder GRU layer.
    This is then fed into a dense layer which produces an output tensor of shape `(None,
    MAX_SEQLEN, t_vocab_size)`. The activation function on the dense layer is a softmax.
    The argmax of each column of this tensor is the index of the predicted POS tag
    for the word at that position.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model definition is shown as follows: `EMBED_SIZE`, `HIDDEN_SIZE`, `BATCH_SIZE`,
    and `NUM_EPOCHS` are hyperparameters which have been assigned these values after
    experimenting with multiple different values. The model is compiled with the `categorical_crossentropy`
    loss function since we have multiple categories of labels, and the optimizer used
    is the popular `adam` optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We train this model for a single epoch. The model is very rich, with many parameters,
    and begins to overfit after the first epoch of training. When fed the same data
    multiple times in the next epochs, the model begins to overfit to the training
    data and does worse on the validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the training and the evaluation is shown as follows. As you can
    see, the model does quite well after the first epoch of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to actual RNNs, the three recurrent classes in Keras (`SimpleRNN`,
    `LSTM`, and `GRU`) are interchangeable. To demonstrate, we simply replace all
    occurrences of `GRU` in the previous program with `LSTM` and rerun the program.
    The model definition and the import statements are the only things that change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the output, the results of the GRU-based network are quite
    comparable to our previous LSTM-based network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence-to-sequence models are a very powerful class of model. Its most canonical
    application is machine translation, but there are many others such as the previous
    example. Indeed, a lot of NLP tasks further up in the hierarchy, such as named
    entity recognition (for more information refer to the article: *Named Entity Recognition
    with Long Short Term Memory*, by J. Hammerton, Proceedings of the Seventh Conference
    on Natural Language Learning at HLT-NAACL, Association for Computational Linguistics,
    2003) and sentence parsing (for more information refer to the article: *Grammar
    as a Foreign Language*, by O. Vinyals, Advances in Neural Information Processing
    Systems, 2015), as well as more complex networks such as those for image captioning
    (for more information refer to the article: *Deep Visual-Semantic Alignments for
    Generating Image Descriptions*, by A. Karpathy, and F. Li, Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2015.), are examples of
    the sequence-to-sequence compositional model.'
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this example can be found in the file `pos_tagging_gru.py`
    in the the code download for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a given time step *t*, the output of the RNN is dependent on the outputs
    at all previous time steps. However, it is entirely possible that the output is
    also dependent on the future outputs as well. This is especially true for applications
    such as NLP, where the attributes of the word or phrase we are trying to predict
    may be dependent on the context given by the entire enclosing sentence, not just
    the words that came before it. Bidirectional RNNs also help a network architecture
    place equal emphasis on the beginning and end of the sequence, and increase the
    data available for training.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional RNNs are two RNNs stacked on top of each other, reading the input
    in opposite directions. So in our example, one RNN will read the words left to
    right and the other RNN will read the words right to left. The output at each
    time step will be based on the hidden state of both RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides support for bidirectional RNNs through a bidirectional wrapper
    layer. For example, for our POS tagging example, we could make our LSTMs bidirectional
    simply by wrapping them with this Bidirectional wrapper, as shown in the model
    definition code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us performance comparable to the unidirectional LSTM example shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Stateful RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs can be stateful, which means that they can maintain state across batches
    during training. That is, the hidden state computed for a batch of training data
    will be used as the initial hidden state for the next batch of training data.
    However, this needs to be explicitly set, since Keras RNNs are stateless by default
    and resets the state after each batch. Setting an RNN to be stateful means that
    it can build a state across its training sequence and even maintain that state
    when doing predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of using stateful RNNs are smaller network sizes and/or lower training
    times. The disadvantage is that we are now responsible for training the network
    with a batch size that reflects the periodicity of the data, and resetting the
    state after each epoch. In addition, data should not be shuffled while training
    the network, since the order in which the data is presented is relevant for stateful
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful LSTM with Keras — predicting electricity consumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we predict electricity consumption for a consumer using a stateful
    and stateless LSTM network and compare their behaviors. As you will recall, RNNs
    in Keras are stateless by default. In case of stateful models, the internal states
    computed after processing a batch of input is reused as initial states for the
    next batch. In other words, the state computed from element *i* in a batch will
    be used as initial state for for the element *i* in the next batch.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use is the electricity load diagram dataset from the UCI
    Machine Learning Repository ([https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)),
    and contains consumption information about 370 customers, taken at 15 minute intervals
    over a four year period from 2011 to 2014\. We randomly choose customer number
    250 for our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to remember is that most problems can be solved with stateless RNNs,
    so if you do use a stateful RNN, make sure you need it. Typically, you would need
    it when the data has a periodic component. If you think a bit, you will realize
    that electricity consumption is periodic. Consumption tends to be higher during
    the day than at night. Let us extract the consumption data for customer number
    250 and plot the first 10 days of data. Finally we also save it to a binary NumPy
    file for our next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding example is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/econs_plot.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is clearly a daily periodic trend. So the problem is a
    good candidate for a stateful model. Also, based on our observation, a `BATCH_SIZE`
    of `96` (number of 15 minute readings over 24 hours) seems appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: We will show the code for the stateless version of the model simultaneously
    with the one for the stateful version. Most of the code is identical for both
    versions, so we will look at both versions simultaneously. I will point out the
    differences in the code as they arise.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, as usual, we import the necessary libraries and classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we load the data for customer 250 into a long array of size (`140256`)
    from the saved NumPy binary file and rescale it to the range *(0, 1)*. Finally,
    we reshape the input to three dimensions as needed by our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Within each batch, the model will take a sequence of 15 minute readings and
    predict the next one. The length of the input sequence is given by the `NUM_TIMESTEPS`
    variable in the code. Based on some experimentation, we get a value of `NUM_TIMESTEPS`
    as `20`, that is, each input row will be a sequence of length `20`, and the output
    will have length `1`. The next step rearranges the input array into `X` and `Y`
    tensors of shapes `(None, 4)` and `(None, 1)`. Finally, we reshape the input tensor
    `X` to three dimensions as required by the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then split our `X` and `Y` tensors into a 70-30 training test split. Since
    we are working with time series, we just choose a split point and cut the data
    into two parts, rather than using the `train_test_split` function, which also
    shuffles the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'First we define our stateless model. We also set the values of `BATCH_SIZE`
    and `NUM_TIMESTEPS`, as we discussed previously. Our LSTM output size is given
    by `HIDDEN_SIZE`, another hyperparameter that is usually arrived at through experimentation.
    Here, we just set it to `10` since our objective is to compare two networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding definition for the stateful model is very similar, as you
    can see as follows. In the LSTM constructor, you need to set `stateful=True`,
    and instead of `input_shape` where the batch size is determined at runtime, you
    need to set `batch_input_shape` explicitly with the batch size. You also need
    to ensure that your training and test data sizes are perfect multiples of your
    batch size. We will see how to do that later when we look at the training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we compile the model, which is the same for both stateless and stateful
    RNNs. Notice that our metric here is mean squared error instead of our usual accuracy.
    This is because this is really a regression problem; we are interested in knowing
    how far off our predictions are with respect to the labels rather than knowing
    whether our prediction matched the label. You can find a full list of Keras built-in
    metrics on the Keras metrics page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the stateless model, we can use the one liner that we have probably
    become very familiar with by now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding code for the stateful model is shown as follows. There are
    three things to be aware of here.
  prefs: []
  type: TYPE_NORMAL
- en: First, you should select a batch size that reflects the periodicity of your
    data. This is because stateful RNNs align the states from each batch to the next,
    so selecting the right batch size allows the network to learn faster.
  prefs: []
  type: TYPE_NORMAL
- en: Once you set the batch size, the size of your training and test sets needs to
    be exact multiples of your batch size. We have ensured this below by truncating
    the last few records from both our training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: The second thing is that you need to fit the model manually, training the model
    in a loop for the required number of epochs. Each iteration trains the model for
    one epoch, and the state is retained across multiple batches. After each epoch,
    the state of the model needs to be reset manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third thing is that the data should be fed in sequence. By default, Keras
    will shuffle the rows within each batch, which will destroy the alignment we need
    for the stateful RNN to learn effectively. This is done by setting `shuffle=False`
    in the call to `model.fit()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the model against the test data and print out the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the stateless model, run over five epochs, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding output for the stateful model, also run in a loop five times
    for one epoch each time, is as follows. Notice the result of the truncating operation
    in the second line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ss-6-8.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the stateful model produces results that are slightly better
    than the stateless model. In absolute terms, since we have scaled our data to
    the *(0, 1)* range, this means that the stateless model has about 6.2% error rate
    and the stateful model has a 5.9% error rate, or conversely, they are about 93.8%
    and 94.1% accurate respectively. In relative terms, therefore, our stateful model
    outperforms the stateless model by a slight margin.
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this example is provided in the files `econs_data.py` that
    parses the dataset, and `econs_stateful.py` that defines and trains the stateless
    and stateful models, available from the code download for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other RNN variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will round up this chapter by looking at some more variants of the RNN cell.
    RNN is an area of active research and many researchers have suggested variants
    for specific purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'One popular LSTM variant is adding *peephole connections*, which means that
    the gate layers are allowed to peek at the cell state. This was introduced by
    Gers and Schmidhuber (for more information refer to the article: *Learning Precise
    Timing with LSTM Recurrent Networks*, by F. A. Gers, N. N. Schraudolph, and J.
    Schmidhuber, Journal of Machine Learning Research, pp. 115-43) in 2002.'
  prefs: []
  type: TYPE_NORMAL
- en: Another LSTM variant, that ultimately led to the GRU, is to use coupled forget
    and output gates. Decisions about what information to forget and what to acquire
    are made together, and the new information replaces the forgotten information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides only the three basic variants, namely the SimpleRNN, LSTM, and
    GRU layers. However, that isn''t necessarily a problem. Gref conducted an experimental
    survey (for more information refer to the article: *LSTM: A Search Space Odyssey*,
    by K. Greff, arXiv:1503.04069, 2015) of many LSTM variants, and concluded that
    none of the variants improved significantly over the standard LSTM architecture.
    So the components provided in Keras are usually sufficient to solve most problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In case you do need the capability to construct your own layer, you can build
    custom Keras layers. We will look at how to build a custom layer in the next chapter.
    There is also an open source framework called recurrent shop ([https://github.com/datalogai/recurrentshop](https://github.com/datalogai/recurrentshop))
    that allows you to build complex recurrent neural networks with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the basic architecture of recurrent neural networks
    and how they work better than traditional neural networks over sequence data.
    We saw how RNNs can be used to learn an author's writing style and generate text
    using the learned model. We also saw how this example can be extended to predicting
    stock prices or other time series, speech from noisy audio, and so on, as well
    as generate music that was composed by a learned model.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at different ways to compose our RNN units and these topologies can
    be used to model and solve specific problems such as sentiment analysis, machine
    translation, image captioning, and classification, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at one of the biggest drawbacks of the SimpleRNN architecture,
    that of vanishing and exploding gradients. We saw how the vanishing gradient problem
    is handled using the LSTM (and GRU) architectures. We also looked at the LSTM
    and GRU architectures in some detail. We also saw two examples of predicting sentiment
    using an LSTM-based model, and predicting POS tags using a GRU-based sequence-to-sequence
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We then learned about stateful RNNs and how they can be used in Keras. We also
    saw an example of learning a stateful RNN to predict CO levels in the atmosphere.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about some RNN variants that are not available in Keras,
    and briefly explored how to build them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at models that don't quite fit into the basic
    molds we have looked at so far. We will also look at composing these basic models
    larger and more complex ones using the Keras functional API, as well as look at
    some examples of customizing Keras to our needs.
  prefs: []
  type: TYPE_NORMAL
