<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reinforcement Learning and Deep Reinforcement Learning</h1>
                
            
            <article>
                
<p class="calibre2">This chapter provides a concise explanation of the basic terminology and concepts in reinforcement learning. It will give you a good understanding of the basic reinforcement learning framework for developing artificial intelligent agents. This chapter will also introduce deep reinforcement learning and provide you with a flavor of the types of advanced problems the algorithms enable you to solve. You will find mathematical expressions and equations used in quite a few places in this chapter. Although there's enough theory behind reinforcement learning and deep reinforcement learning to fill a whole book, the key concepts that are useful for practical implementation are discussed in this chapter, so that when we actually implement the algorithms in Python to train our agents, you can clearly understand the logic behind them. It is perfectly alright if you are not able to grasp all of it in your first pass. You can always come back to this chapter and revise whenever you need a better understanding. </p>
<p class="calibre2">We will cover the following topics in this chapter:</p>
<ul class="calibre10">
<li class="calibre11">What is reinforcement learning?</li>
<li class="calibre11">The Markov Decision Process</li>
<li class="calibre11">The reinforcement learning framework</li>
<li class="calibre11">What is deep reinforcement learning?</li>
<li class="calibre11">How do deep reinforcement learning agents work in practice?</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">What is reinforcement learning?</h1>
                
            
            <article>
                
<p class="calibre2">If you are new to the field of <strong class="calibre4">Artificial Intelligence</strong> (<strong class="calibre4">AI</strong>) or machine learning, you might be wondering what reinforcement learning is all about. In simple terms, it is learning through reinforcement. <em class="calibre13">Reinforcement</em>, as you know from general English or psychology, is the act of increasing or strengthening the choice to take a particular action in response to something, because of the perceived benefit of receiving higher rewards for taking that action. We humans are good at learning through reinforcement from a very young age. Those who have kids may be utilizing this fact more often to teach good habits to them. Nevertheless, we will all be able to relate to this, because not so long ago we all went through that phase of life! Say parents reward their kid with chocolate if the kid completes their homework on time after school<span class="calibre5"> </span><span class="calibre5">every day</span><span class="calibre5">. The kid</span> <em class="calibre13">learns</em> <span class="calibre5">the fact that he/she will receive chocolate (<em class="calibre13">a </em></span><em class="calibre13">reward</em>) i<span class="calibre5">f he/she completes their homework every day. Therefore, this strengthens their decision to finish their homework every day to receive the chocolate. This process of learning to strengthen a particular choice of action, motivated by the reward they will receive for taking such an action, is called learning by reinforcement or reinforcement learning.</span></p>
<p class="calibre2"><span class="calibre5">You might be thinking, "<em class="calibre13">Oh yeah. That human psychology sounds very familiar to me. But what has that got to do with machine learning or AI?"</em> Good thought. The concept of reinforcement learning was in fact inspired by behavioral psychology. It is at the nexus of several fields of research, the most important being computer science, mathematics, neuroscience, and psychology, as shown in the following diagram:</span></p>
<div class="cdpaligncenter"><img src="../images/00011.jpeg" class="calibre30"/></div>
<p class="calibre2">As we will soon realize, reinforcement learning is one of the most promising methods in machine learning leading towards AI. If all these terms are new to you, do not worry! Starting from the next paragraph, we will go over these terms and understand their relationship with each other to make you feel comfortable. If you already know these terms, it will be a refreshing read with a different perspective.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Understanding what AI means and what's in it in an intuitive way</h1>
                
            
            <article>
                
<p class="calibre2">The intelligence demonstrated by humans and animals is called <em class="calibre13">natural intelligence</em>, but the intelligence demonstrated by machines is called AI, for obvious reasons. We humans develop algorithms and technologies that provide intelligence to machines. Some of the greatest developments on this front are in the fields of machine learning, artificial neural networks, and deep learning. These fields collectively drive the development of AI. There are three main types of machine learning paradigms that have been developed to some reasonable level of maturity so far, and they are the following:</p>
<ul class="calibre10">
<li class="calibre11">Supervised learning</li>
<li class="calibre11">Unsupervised learning</li>
<li class="calibre11">Reinforcement learning</li>
</ul>
<p class="calibre2">In the following diagram, you can get an intuitive picture of the field of AI. You can see that these learning paradigms are subsets of the field of machine learning, and machine learning itself is a subset/branch of AI:</p>
<div class="cdpaligncenter"><img src="../images/00012.jpeg" class="calibre31"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Supervised learning</h1>
                
            
            <article>
                
<p class="calibre2">Supervised learning is similar to how we would teach a kid to recognize someone or some object by name. We provide an input and a name/class label (<em class="calibre13">label</em> for short) associated with that input, and expect the machine to learn that input-to-label mapping. This might sound simple if we just want the machine to learn the input-to-label mapping for a few objects (like in object recognition-type tasks) or persons (like in face/voice/person recognition tasks), but what if we want a machine to learn about several thousand classes where each class may have several different variations in the input? For example, if the task is to recognize a person's face from image inputs, with a thousand other input images with faces to distinguish it from, the task might be complicated even for an adult. There might be several variations in the input images for the same person's face. The person may be wearing glasses in one of the input images, or wearing a hat in another, or sporting a different facial expression altogether. It is a much harder task for a machine to be able to see the input image, identify the face, and recognize it. With recent advancements in the field of deep learning, supervised classification tasks like these are no longer hard for machines. Machines can recognize faces, among several other things, at an unprecedented level of accuracy. For example, the DeepFace system (<a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" class="calibre9">https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf</a>), developed by the Facebook AI research lab, reached an accuracy of 97.45% in face recognition on the Labelled Faces in the Wild dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Unsupervised learning</h1>
                
            
            <article>
                
<p class="calibre2">Unsupervised learning is a form of learning in which there is no label provided to the learning algorithm along with the inputs, unlike the supervised learning paradigm. This class of learning algorithm is typically used to figure out patterns in the input data and cluster similar data together. A recent advancement in the field of deep learning introduced a new form of learning called Generative Adversarial Networks, which have gained massive popularity during the time this book was being written. If you are interested, you can learn a lot more about Generative Adversarial Networks from this video: <a href="https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video" class="calibre9">https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reinforcement learning</h1>
                
            
            <article>
                
<p class="calibre2">Reinforcement learning is kind of a hybrid way of learning compared to supervised and unsupervised learning. As we learned at the start of this section, reinforcement learning is driven by a reward signal. In the case of the <em class="calibre13">kid with their homework</em> problem, the reward signal was the chocolate from their parents. In the machine learning world, a chocolate may not be enticing for a computer (well, we could program a computer to want chocolates, but why would we? Aren't kids enough?!), but a mere scalar value (a number) will do the trick! The reward signals are still human-specified in some way, signifying the intended goal of the task. For example, to train an agent to play Atari games using reinforcement learning, the scores from the games can be the reward signal. This makes reinforcement learning much easier (for humans and not for the machine!) because we don't have to label the button to be pressed at each point in the game to teach the machine how to play the game. Instead, we just ask the machine to learn on its own to maximize their score. Doesn't it sound fascinating that we could make a machine figure out how to play a game, or how to control a car, or how to do its homework all by itself, and all we have to do is just say how it did with a score? That is why we are learning about it in this chapter. You will develop some of those cool machines yourself in the upcoming chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Practical reinforcement learning</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that you have an intuitive understanding of what AI really means and the various classes of algorithm that drive its development, we will now focus on the practical aspects of building a reinforcement learning machine. </span></p>
<p class="calibre2">Here are the core concepts that you need to be aware of to develop reinforcement learning systems:</p>
<ul class="calibre10">
<li class="calibre11">Agent</li>
<li class="calibre11">Rewards</li>
<li class="calibre11">Environment</li>
<li class="calibre11">State</li>
<li class="calibre11">Value function</li>
<li class="calibre11">Policy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Agent</h1>
                
            
            <article>
                
<p class="calibre2">In the reinforcement learning world, a machine is run or instructed by a (software) agent. The agent is the part of the machine that possesses intelligence and makes decisions on what to do next. You will come across the <span class="calibre5">term</span><span class="calibre5"> </span><span class="calibre5">"agent" several times as we dive deeper into reinforcement learning. Reinforcement learning is based on the reward hypothesis, which states that any goal can be described by the maximization of the expected cumulative reward. So, what is this reward exactly? That's what we'll discuss next.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Rewards</h1>
                
            
            <article>
                
<p class="calibre2">A reward, denoted by  <img class="fm-editor-equation" src="../images/00013.jpeg"/>, is usually a scalar quantity that is provided as feedback to the agent to drive its learning. The goal of the agent is to maximize the sum of the reward, and this signal indicates how well the agent is doing at time step <img class="fm-editor-equation1" src="../images/00014.jpeg"/>.  The following examples of reward signals for different tasks may help you get a more intuitive understanding:</p>
<ul class="calibre10">
<li class="calibre11">For the Atari games we discussed before, or any computer games in general, the reward signal can be <kbd class="calibre12">+1</kbd> for every increase in score and <kbd class="calibre12">-1</kbd> for every decrease in score.</li>
<li class="calibre11">For stock trading, the reward signal can be <kbd class="calibre12">+1</kbd> for each dollar gained and <kbd class="calibre12">-1</kbd> for each dollar lost by the agent.</li>
<li class="calibre11">For driving a car in simulation, the reward signal can be <kbd class="calibre12">+1</kbd> for every mile driven and <kbd class="calibre12">-100</kbd> for every collision.</li>
<li class="calibre11">Sometimes, the reward signal can be sparse. For example, for a game of chess or Go, the reward signal could be <kbd class="calibre12">+1</kbd> if the agent wins the game and <kbd class="calibre12">-1</kbd> if the agent loses the game. The reward is sparse because the agent receives the reward signal only after it completes one full game, not knowing how good each move it made was.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Environment</h1>
                
            
            <article>
                
<p class="calibre2">In the first chapter, we looked into the different environments provided by the OpenAI Gym toolkit. You might have been wondering why they were called environments instead of problems, or tasks, or something else. Now that you have progressed to this chapter, does it ring a bell in your head?</p>
<p class="calibre2">The environment is the platform that represents the problem or task that we are interested in, and with which the agent interacts.  The following diagram shows the general reinforcement learning paradigm at the highest level of abstraction:</p>
<div class="cdpaligncenter"><img src="../images/00015.jpeg" class="calibre32"/></div>
<p class="calibre2">At each time step, denoted by <img class="fm-editor-equation2" src="../images/00016.jpeg"/>, the agent receives an observation <img class="fm-editor-equation3" src="../images/00017.jpeg"/> from the environment and then executes an action <img class="fm-editor-equation4" src="../images/00018.jpeg"/>, for which it receives a scalar reward <img class="fm-editor-equation3" src="../images/00019.jpeg"/> back from the environment, along with the next observation <img class="fm-editor-equation5" src="../images/00020.jpeg"/>, and then this process repeats until a terminal state is reached. What is an observation and what is a state? Let's look into that next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">State</h1>
                
            
            <article>
                
<p class="calibre2">As the agent interacts with an environment, the process results in a sequence of observations (<img class="fm-editor-equation6" src="../images/00021.jpeg"/>), actions (<img class="fm-editor-equation7" src="../images/00022.jpeg"/>), and rewards (<img class="fm-editor-equation6" src="../images/00023.jpeg"/>), as described previously.  At some time step <img class="fm-editor-equation2" src="../images/00024.jpeg"/>, what the agent knows so far is the sequence of <img class="fm-editor-equation8" src="../images/00025.jpeg"/>, <img class="fm-editor-equation9" src="../images/00026.jpeg"/>, and <img class="fm-editor-equation8" src="../images/00027.jpeg"/> that it observed until time step <img class="fm-editor-equation2" src="../images/00028.jpeg"/>. It intuitively makes sense to call this the history:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation10" src="../images/00029.jpeg"/></div>
<p class="calibre2">What happens next at time step  <img class="fm-editor-equation11" src="../images/00030.jpeg"/> depends on the history. Formally, the information used to determine what happens next is called the <em class="calibre13">state<strong class="calibre4">. </strong></em>Because it depends on the history up until that time step, it can be denoted as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation12" src="../images/00031.jpeg"/>,</div>
<p class="calibre2">Here, <img class="fm-editor-equation13" src="../images/00032.jpeg"/> denotes some function.</p>
<p class="calibre2">There is one subtle piece of information that is important for you to understand before we proceed. Let's have another look at the general representation of a reinforcement learning system:</p>
<div class="cdpaligncenter"><img src="../images/00033.jpeg" class="calibre33"/></div>
<p class="calibre2">Now, you will notice that the two main entities in the system, the agent and the environment, each has its own representation of the state. The <em class="calibre13">environment state</em>, sometimes denoted by <img class="fm-editor-equation14" src="../images/00034.jpeg"/>, is the environment's own (private) representation, which the environment uses to pick the next observation and reward. This state is not usually visible/available to the agent. Likewise, the agent has its own internal representation of the state, sometimes denoted by <img class="fm-editor-equation15" src="../images/00035.jpeg"/>, which is the information used by the agent to base its actions on. Because this representation is internal to the agent, it is up to the agent to use any function to represent it. Typically, it is some function based on the history<em class="calibre13"> </em>that the agent has observed so far. On a related note, a <em class="calibre13">Markov state<strong class="calibre4"> </strong></em>is a representation of the state using all the useful information from the history. By definition, using the Markov property, a state <img class="fm-editor-equation16" src="../images/00036.jpeg"/> is Markov or Markovian if, and only if, <img class="fm-editor-equation17" src="../images/00037.jpeg"/>, which means that <em class="calibre13">the future is independent of the past given the present</em>. In other words, such a state is a sufficient statistic of the future. Once the state is known, the history can be thrown away. Usually, the environment state, <img class="fm-editor-equation18" src="../images/00038.jpeg"/>, and the history, <img class="fm-editor-equation19" src="../images/00039.jpeg"/>, satisfy the Markov property.</p>
<p class="calibre2">In some cases, the environment may make its internal environmental state directly visible to the agent. Such environments are called <em class="calibre13">fully observable environments. </em>In cases where the agent cannot directly observe the environment state, the agent must construct its own state representation from what it observes. Such environments are called <em class="calibre13">partially observable environments.</em> For example, an agent playing poker can only observe the public cards and not the cards the other players possess. Therefore, it is a partially observed environment. Similarly, an autonomous car with just a camera does not know its absolute location in its environment, which makes the environment only partially observable.</p>
<p class="calibre2">In the next sections, we will learn about some of the key components of an agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Model</h1>
                
            
            <article>
                
<p class="calibre2">A model is an agent's representation of the environment. It is similar to the mental models we have about people and things around us. An agent uses its model of the environment to predict what will happen next. There are two key pieces to it:</p>
<ul class="calibre10">
<li class="calibre11"><img class="fm-editor-equation20" src="../images/00040.jpeg"/>: The state transition model/probability</li>
<li class="calibre11"><img class="fm-editor-equation21" src="../images/00041.jpeg"/>: The reward model</li>
</ul>
<p class="calibre2">The state transition model <img class="fm-editor-equation22" src="../images/00042.jpeg"/> is a probability distribution or a function that predicts the probability of ending up in a state <img class="fm-editor-equation23" src="../images/00043.jpeg"/> in the next time step <img class="fm-editor-equation24" src="../images/00044.jpeg"/> given the state <img class="fm-editor-equation25" src="../images/00045.jpeg"/> and the action <img class="fm-editor-equation25" src="../images/00046.jpeg"/> at time step <img class="fm-editor-equation1" src="../images/00047.jpeg"/>. Mathematically, it is expressed as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation26" src="../images/00048.jpeg"/></div>
<p class="calibre2">The agent uses the reward model <img class="fm-editor-equation27" src="../images/00049.jpeg"/> to predict the immediate next reward that it would get if it were to take action <img class="fm-editor-equation28" src="../images/00050.jpeg"/> while in state <img class="fm-editor-equation25" src="../images/00051.jpeg"/> at time step <img class="fm-editor-equation1" src="../images/00052.jpeg"/>.  This expectation of the reward at the next time step <img class="fm-editor-equation29" src="../images/00053.jpeg"/> can be mathematically expressed as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation30" src="../images/00054.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Value function</h1>
                
            
            <article>
                
<p class="calibre2">A value function represents the agent's prediction of future rewards. There are two types of value function: state-value function and action-value function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">State-value function</h1>
                
            
            <article>
                
<p class="calibre2">A state-value function is a function that represents the agent's estimate of how good it is to be in a state <img class="fm-editor-equation31" src="../images/00055.jpeg"/> at time step <em class="calibre13">t</em>. It is denoted by <img class="fm-editor-equation32" src="../images/00056.jpeg"/> and is usually just called the <em class="calibre13">value function</em>. It represents the agent's prediction of the future reward it would get if it were to end up in state <img class="fm-editor-equation25" src="../images/00057.jpeg"/> at time step <em class="calibre13">t</em>. Mathematically, it can be represented as follows:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation33" src="../images/00058.jpeg"/></div>
<p class="calibre2">What this expression means is that the value of state <img class="fm-editor-equation27" src="../images/00059.jpeg"/> under policy <img class="fm-editor-equation34" src="../images/00060.jpeg"/> is the expected sum of the discounted future rewards, where <img class="fm-editor-equation22" src="../images/00061.jpeg"/> is the discount factor and is a real number in the range [0,1]. Practically, the discount factor is typically set to be in the range of [0.95,0.99]. The other new term is <img class="fm-editor-equation35" src="../images/00062.jpeg"/>, which is the policy of the agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Action-value function</h1>
                
            
            <article>
                
<p class="calibre2">The action-value function is a function that represents the agent's estimate of how good it is to take action <img class="fm-editor-equation4" src="../images/00063.jpeg"/> in state <img class="fm-editor-equation4" src="../images/00064.jpeg"/>. It is denoted by <img class="fm-editor-equation36" src="../images/00065.jpeg"/>. It is related to the state-value function by the following equation:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation37" src="../images/00066.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Policy</h1>
                
            
            <article>
                
<p class="calibre2">The policy denoted by <img class="fm-editor-equation38" src="../images/00067.jpeg"/> prescribes what action is to be taken given the state. It can be seen as a function that maps states to actions. There are two major types of policy: deterministic policies and stochastic policies.</p>
<p class="calibre2">A deterministic policy prescribes one action for a given state, that is, there is only one action, <img class="fm-editor-equation39" src="../images/00068.jpeg"/>, given <em class="calibre13">s</em>. Mathematically, it means <img class="fm-editor-equation40" src="../images/00069.jpeg"/>.</p>
<p class="calibre2">A stochastic policy prescribes an action distribution given a state <img class="fm-editor-equation25" src="../images/00070.jpeg"/> at time step <img class="fm-editor-equation2" src="../images/00071.jpeg"/>, that is, there are multiple actions with a probability value for each action. Mathematically, it means <img class="fm-editor-equation41" src="../images/00072.jpeg"/>.</p>
<p class="calibre2">Agents following different policies may exhibit different behaviors in the same environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Markov Decision Process</h1>
                
            
            <article>
                
<p class="calibre2">A <strong class="calibre4">Markov Decision Process </strong>(<strong class="calibre4">MDP</strong>) provides a formal framework for reinforcement learning. It is used to describe a fully observable environment where the outcomes are partly random and partly dependent on the actions taken by the agent or the decision maker. The following diagram is the progression of a Markov Process into a Markov Decision Process through the Markov Reward Process:</p>
<div class="cdpaligncenter"><img src="../images/00073.jpeg" class="calibre34"/></div>
<p class="calibre2">These stages can be described as follows:</p>
<ul class="calibre10">
<li class="calibre11">A <strong class="calibre1">Markov Process</strong> (or a <em class="calibre25">markov chain) </em>is a sequence of random states s1, s2,...  that obeys the <em class="calibre25">Markov property.<strong class="calibre1"> </strong></em>In simple terms, it is a random process without any memory about its history.</li>
<li class="calibre11">A <strong class="calibre1">Markov Reward Process</strong> (<strong class="calibre1">MRP</strong>) is a <em class="calibre25">Markov Process (</em>also called a M<em class="calibre25">arkov chain) </em>with values.</li>
</ul>
<ul class="calibre10">
<li class="calibre11">A <strong class="calibre1">Markov Decision Process</strong> is a <em class="calibre25">Markov Reward Process</em> with decisions.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Planning with dynamic programming</h1>
                
            
            <article>
                
<p class="calibre2">Dynamic programming is a very general method to efficiently solve problems that can be decomposed into overlapping sub-problems. If you have used any type of recursive function in your code, you might have <span class="calibre5">already</span><span class="calibre5"> </span><span class="calibre5">got some preliminary flavor of dynamic programming. Dynamic programming, in simple terms, tries to cache or store the results of sub-problems so that they can be used later if required, instead of computing the results again.</span></p>
<p class="calibre2">Okay, so how is that relevant here, you may ask. Well, they are pretty useful for solving a fully defined MDP, which means that an agent can find the most optimal way to act in an environment to achieve the highest reward using dynamic programming if it has full knowledge of the MDP! In the following table, you will find a concise summary of what the inputs and outputs are when we are interested in sequential prediction or control:</p>
<table border="1" class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38">Task/objective</td>
<td class="calibre39">Input</td>
<td class="calibre40">Output</td>
</tr>
<tr class="calibre37">
<td class="calibre38"><span>Prediction</span></td>
<td class="calibre39"><span>MDP or MRP and policy </span><img class="fm-editor-equation42" src="../images/00074.jpeg"/></td>
<td class="calibre40"><span>Value function </span><img class="fm-editor-equation43" src="../images/00075.jpeg"/></td>
</tr>
<tr class="calibre37">
<td class="calibre38"><span>Control</span></td>
<td class="calibre39"><span>MDP</span></td>
<td class="calibre40"><span>Optimal value function <img class="fm-editor-equation44" src="../images/00076.jpeg"/> and optimal policy <img class="fm-editor-equation45" src="../images/00077.jpeg"/></span></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Monte Carlo learning and temporal difference learning</h1>
                
            
            <article>
                
<p class="calibre2">At this point, we understand that it is very useful for an agent to learn the state value function <img class="fm-editor-equation46" src="../images/00078.jpeg"/>, which informs the agent about the long-term value of being in state <img class="fm-editor-equation25" src="../images/00079.jpeg"/> so that the agent can decide if it is a good state to be in or not. The <strong class="calibre4">Monte Carlo</strong> (<strong class="calibre4">MC</strong>) and <strong class="calibre4">Temporal Difference</strong> (<strong class="calibre4">TD</strong>) learning methods enable an agent to learn that!</p>
<p class="calibre2">The goal of MC and TD learning is to learn the value functions from the agent's experience as the agent follows its policy <img class="fm-editor-equation35" src="../images/00080.jpeg"/>.</p>
<p class="calibre2">The following table summarizes the value estimate's update equation for the MC and TD learning methods:</p>
<table class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="cdpaligncenter1"><strong class="calibre1">Learning method</strong></td>
<td class="cdpaligncenter2"><strong class="calibre1"><span>State-value function</span></strong></td>
</tr>
<tr class="calibre37">
<td class="cdpaligncenter1"><span>Monte Carlo</span></td>
<td class="cdpaligncenter2"><img class="fm-editor-equation47" src="../images/00081.jpeg"/></td>
</tr>
<tr class="calibre37">
<td class="cdpaligncenter1"><span>Temporal Difference</span></td>
<td class="cdpaligncenter2"><img class="fm-editor-equation48" src="../images/00082.jpeg"/></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2"><span class="calibre5">MC learning updates the value towards the</span> <strong class="calibre4">actual return</strong><span class="calibre5"> </span><img class="fm-editor-equation49" src="../images/00083.jpeg"/><span class="calibre5">, which is the total discounted reward from time step</span> <em class="calibre13">t</em><span class="calibre5">. This means that </span><img class="fm-editor-equation50" src="../images/00084.jpeg"/><span class="calibre5"> until the end. It is important to note that we can calculate this value only after the end of the sequence, whereas TD learning (TD(0) to be precise), updates the value towards the</span> <em class="calibre13">estimated return</em><span class="calibre5"> given by </span><img class="fm-editor-equation51" src="../images/00085.jpeg"/><span class="calibre5">, which can be calculated after every step.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SARSA and Q-learning</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">It is also very useful for an agent to learn the action value function</span><span class="calibre5"> </span><img class="fm-editor-equation52" src="../images/00086.jpeg"/><span class="calibre5">, which informs the agent about the long-term value of taking action </span><img class="fm-editor-equation31" src="../images/00087.jpeg"/><span class="calibre5"> in state </span><img class="fm-editor-equation53" src="../images/00088.jpeg"/><span class="calibre5"> so that the agent can take those actions that will maximize its expected, discounted future reward. The SARSA and Q-learning algorithms enable an agent to learn that! The following table summarizes the update equation for the SARSA algorithm and the Q-learning algorithm:</span></p>
<table class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="cdpaligncenter3"><strong class="calibre1">Learning method</strong></td>
<td class="cdpaligncenter3"><strong class="calibre1">Action-value function</strong></td>
</tr>
<tr class="calibre37">
<td class="cdpaligncenter3">
<p class="calibre2">SARSA</p>
</td>
<td class="cdpaligncenter3"><img class="fm-editor-equation54" src="../images/00089.jpeg"/></td>
</tr>
<tr class="calibre37">
<td class="cdpaligncenter3">
<p class="calibre2">Q-learning</p>
</td>
<td class="cdpaligncenter3"><img class="fm-editor-equation55" src="../images/00090.jpeg"/></td>
</tr>
</tbody>
</table>
<p class="calibre2">SARSA is <span class="calibre5">so</span><span class="calibre5"> </span><span class="calibre5">named because of the sequence State-&gt;Action-&gt;Reward-&gt;State'-&gt;Action' that the algorithm's update step depends on. The description of the sequence goes like this: the agent, in state <em class="calibre13">S</em>, takes an action A and gets a reward R, and ends up in the next state S', after which the agent decides to take an action A' in the new state. Based on this experience, the agent can update its estimate of Q(S,A).</span></p>
<p class="calibre2">Q-learning is a popular off-policy learning algorithm, and it is similar to SARSA, except for one thing. Instead of using the Q value estimate for the new state and the action that the agent took in that new state, it uses the Q value estimate that corresponds to the action that leads to the <em class="calibre13">maximum</em> obtainable Q value from that new state, S'.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep reinforcement learning</h1>
                
            
            <article>
                
<p class="calibre2">With a basic understanding of reinforcement learning, you are now in a better state (hopefully you are not in a strictly Markov state where you have forgotten the history/things you have learned so far) to understand the basics of the cool new suite of algorithms that have been rocking the field of AI in recent times.</p>
<p class="calibre2">Deep reinforcement learning emerged naturally when people made advancements in the deep learning field and applied them to reinforcement learning. We learned about the state-value function, action-value function, and policy. Let's briefly look at how they can be represented mathematically or realized through computer code. The state-value function <img class="fm-editor-equation56" src="../images/00091.jpeg"/> is a real-value function that takes the current state <img class="fm-editor-equation22" src="../images/00092.jpeg"/> as the input and outputs a real-value number (such as 4.57). This number is the agent's prediction of how good it is to be in state <img class="fm-editor-equation25" src="../images/00093.jpeg"/> and the agent keeps updating the value function based on the new experiences it gains. Likewise, the action-value function <img class="fm-editor-equation57" src="../images/00094.jpeg"/> is also a real-value function, which takes action <img class="fm-editor-equation27" src="../images/00095.jpeg"/> as an input in addition to state <img class="fm-editor-equation58" src="../images/00096.jpeg"/>, and outputs a real number. One way to represent these functions is using neural networks because neural networks are universal function approximators, which are capable of representing complex, non-linear functions. For an agent trying to play a game of Atari by just looking at the images on the screen (like we do), state <img class="fm-editor-equation59" src="../images/00097.jpeg"/> could be the pixel values of the image on the screen. In such cases, we could use a deep neural network with convolutional layers to extract the visual features from the state/image, and then a few fully connected layers to finally output <img class="fm-editor-equation60" src="../images/00098.jpeg"/>  or <img class="fm-editor-equation61" src="../images/00099.jpeg"/> , depending on which function we want to approximate. </p>
<div class="packt_infobox"><span class="packt_screen">Recall from the earlier sections of this chapter that </span><img class="fm-editor-equation62" src="../images/00100.jpeg"/><span class="packt_screen"> is the state-value function and provides an estimate of the value of being in state </span><img class="fm-editor-equation63" src="../images/00101.jpeg"/><span class="packt_screen">, and </span><img class="fm-editor-equation64" src="../images/00099.jpeg"/><span class="packt_screen"> is the action-value function, which provides an estimate of</span> the value of each action given the  state.</div>
<p class="calibre2">If we do this, then we are doing deep reinforcement learning! Easy enough to understand? I hope so. Let's look at some other ways in which we can use deep learning in reinforcement learning. </p>
<p class="calibre2">Recall that a policy is represented as <img class="fm-editor-equation65" src="../images/00102.jpeg"/> in the case of deterministic policies, and as <img class="fm-editor-equation66" src="../images/00103.jpeg"/> in the case of stochastic policies, where action <img class="fm-editor-equation67" src="../images/00104.jpeg"/> could be discrete (such as "move left," "move right," or "move straight ahead") or continuous values (such as "0.05" for acceleration, "0.67" for steering, and so on), and they can be single or multi-dimensional. Therefore, a policy can be a complicated function at times! It might have to take in a multi-dimensional state (such as an image) as input and output a multi-dimensional vector of probabilities as output (in the case of stochastic policies). So, this does look like it will be a monster function, doesn't it? Yes it does. That's where deep neural networks come to the rescue! We could approximate an agent's policy using a deep neural network and directly learn to update the policy (by updating the parameters of the deep neural network). This is called policy optimization-based deep reinforcement learning and it has been shown to be quite efficient in solving several challenging control problems, especially in robotics.</p>
<p class="calibre2">So in summary, deep reinforcement learning is the application of deep learning to reinforcement learning and so far, researchers have applied deep learning to reinforcement learning successfully in two ways. One way is using deep neural networks to approximate the value functions, and the other way is to use a deep neural network to represent the policy.</p>
<p class="calibre2">These ideas have been known from the early days, when researchers were trying to use neural networks as value function approximators, even back in 2005. But it rose to stardom only recently because although neural networks or other non-linear value function approximators can better represent the complex values of environment states and actions, they were prone to instability and often led to sub-optimal functions. Only recently have researchers such as Volodymyr Mnih and his colleagues at DeepMind (now part of Google) figured out the trick of stabilizing the learning and trained agents with deep, non-linear function approximators that converged to near-optimal value functions. In the later chapters of this book, we will in fact reproduce some of their then-groundbreaking results, which surpassed human Atari game playing capabilities!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Practical applications of reinforcement and deep reinforcement learning algorithms</h1>
                
            
            <article>
                
<p class="calibre2">Until recently, practical applications of reinforcement learning and deep reinforcement learning were limited, due to sample complexity and instability. But, these algorithms proved to be quite powerful in solving some really hard practical problems. Some of them are listed here to give you an idea:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Learning to play video games better than humans</strong>: This news has probably reached you by now. Researchers at DeepMind and others developed a series of algorithms, starting with DeepMind's Deep-Q-Network, or DQN for short, which reached human-level performance in playing Atari games. We will actually be implementing this algorithm in a later chapter of this book! In essence, it is a deep variant of the Q-learning algorithm we briefly saw in this chapter, with a few changes that increased the speed of learning and the stability. It was able to reach human-level performance in terms of game scores after several games. What is more impressive is that the same algorithm achieved this level of play without any game-specific fine-tuning or changes!</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Mastering the game of Go</strong>: Go is a Chinese game that has challenged AI for several decades. It is played on a full-size 19 x 19 board and is orders of magnitude more complex than chess because of the large number (<img class="fm-editor-equation68" src="../images/00105.jpeg"/>) of possible board positions. Until recently, no AI algorithm or software was able to play anywhere close to the level of humans at this game. AlphaGo—the AI agent from DeepMind that uses deep reinforcement learning and Monte Carlo tree search<span>—</span>changed this all and beat the human world champions Lee Sedol (4-1) and Fan Hui (5-0). DeepMind released more advanced versions of their AI agent, named AlphaGO Zero (which uses zero human knowledge and learned to play all by itself!) and AlphaZero (which could play the games of Go, chess, and Shogi!), all of which used deep reinforcement learning as the core algorithm.</li>
<li class="calibre11"><strong class="calibre1">Helping AI win Jeopardy!</strong>: IBM's Watson<span>—</span>an AI system developed by IBM, which came to fame by beating humans at Jeopardy!<span>—</span>used an extension of TD learning to create its <em class="calibre25">daily-double wagering</em> strategies that helped it to win against human champions.</li>
<li class="calibre11"><strong class="calibre1">Robot locomotion and manipulation:</strong> Both reinforcement learning and deep reinforcement learning have enabled the control of complex robots, both for locomotion and navigation. Several recent works from the researchers at UC Berkeley have shown how, using deep reinforcement, they train policies that offer vision and control for robotic manipulation tasks and generate join actuations for making a complex bipedal humanoid walk and run.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we discussed how an agent interacts with an environment by taking an action based on the observation it receives from the environment, and the environment responds to the agent's action with an (optional) reward and the next observation.</p>
<p class="calibre2">With a concise understanding of the foundations of reinforcement learning, we went deeper to understand what deep reinforcement learning is, and uncovered the fact that we could use deep neural networks to represent value functions and policies. Although this chapter was a little heavy on notation and definitions, hopefully it laid a strong foundation for us to develop some cool agents in the upcoming chapters. In the next chapter, we will consolidate our learning in the first two chapters and put it to use by laying out the groundwork to train an agent to solve some interesting problems.</p>


            </article>

            
        </section>
    </body></html>