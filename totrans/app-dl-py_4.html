<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Neural Networks and Deep Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">The MNIST dataset does not contain numbers on the edges of images. Hence, neither network assigns relevant values to the pixels located in that region. Both networks are much better at classifying numbers correctly if we draw them closer to the center of the designated area. This shows that neural networks can only be as powerful as the data that is used to train them. If the data used for training is very different than what we are trying to predict, the network will most likely produce disappointing results.In this chapter, we will cover the basics of neural networks and how to set up a deep learning programming environment. We will also explore the common components of a neural network and its essential operations. We will conclude this chapter by exploring a trained neural network created using TensorFlow.</p>
<p class="mce-root">This chapter is about understanding what neural networks can do. We will not cover mathematical concepts underlying deep learning algorithms, but will instead describe the essential pieces that make a deep learning system. We will also look at examples where neural networks have been used to solve real-world problems.</p>
<p class="mce-root">This chapter will give you a practical intuition on how to engineer systems that use neural networks to solve problems—including how to determine if a given problem can be solved at all with such algorithms. At its core, this chapter challenges you to think about your problem as a mathematical representation of ideas. By the end of this chapter, you will be able to think about a problem as a collection of these representations and then start to recognize how these representations may be learned by deep learning algorithms.</p>
<p><span>By the end of this chapter, you will be able to:</span></p>
<ul>
<li><span>Cover the basics of neural networks</span></li>
<li><span>Set up a deep learning programming environment</span></li>
<li><span>Explore the common components of a neural network and its essential operations</span></li>
<li><span>Conclude this chapterby exploring a trained neural network created using TensorFlow<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What are Neural Networks?</h1>
                </header>
            
            <article>
                
<p>Neural networks—also known as <strong>Artificial Neural Networks</strong>—were fist proposed in the 40s by MIT professors Warren McCullough and Walter Pitts.</p>
<div class="packt_infobox"><span>For more information refer, Explained: Neural networks. MIT News Office,April 14, 2017. Available at:<br/>
<a href="http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414">http://news.mit.edu/2017/explained-neural-networksdeep-learning-0414</a>.<br/></span></div>
<p>Inspired by advancements in neuroscience, they proposed to create a computer system that reproduced how the brain works (human or otherwise). At its core was the idea of a computer system that worked as an interconnected network. That is, a system that has many simple components. These components both interpret data and influence each other on how to interpret data. This same core idea remains today.</p>
<p>Deep learning is largely considered the contemporary study of neural networks. Think of it as a current name given to neural networks. The main difference is that the neural networks used in deep learning are typically far greater in size—that is, they have many more nodes and layers—than earlier neural networks. Deep learning algorithms and applications typically require resources to achieve success, hence the use of the word <em>deep</em> to emphasize its size and the large number of interconnected components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Successful Applications</h1>
                </header>
            
            <article>
                
<p>Neural networks have been under research since their inception in the 40s in one form or another. It is only recently, however, that deep learning systems have been successfully used in large-scale industry applications.</p>
<p class="mce-root"/>
<p>Contemporary proponents of neural networks have demonstrated great success in speech recognition, language translation, image classification, and other feilds. Its current prominence is backed by a significant increase in available computing power and the emergence of <strong>Graphic Processing Units</strong> (<strong>GPUs</strong>) and <strong>Tensor Processing Units</strong> (<strong>TPUs</strong>)— which are able to perform many more simultaneous mathematical operations than regular CPUs, as well as a much greater availability of data.</p>
<p>Power consumption of different AlphaGo algorithms. AlphaGo is an initiative by DeepMind to develop a series of algorithms to beat the game Go. It is considered a prime example of the power of deep learning. TPUs are a chipset developed by Google for the use in deep learning programs.</p>
<p><span>The graphic depicts the number of GPUs and TPUs used to train different versions of the AlphaGo algorithm. Source: </span><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">https://deepmind.com/blog/alphago-zero-learning-scratch/</a></p>
<div class="packt_infobox">In this book, we will not be using GPUs to fulfil our activities. GPUs are not required to work with neural networks. In a number of simple examples—like the ones provided in this book—all computations can be performed using a simple laptop's CPU. However, when dealing with very large datasets, GPUs can be of great help given that the long time to train a neural network would be unpractical.</div>
<div>Here are a few examples of fields in which neural networks have had great impact:</div>
<ul>
<li><strong>Translating text</strong>: In 2017, Google announced that it was releasing a new algorithm for its translation service called <strong>Transformer</strong>. The algorithm consisted of a recurrent neural network (LSTM) that is trained used bilingual text. Google showed that its algorithm had gained notable accuracy when comparing to industry standards (BLEU) and was also computationally efficient. At the time of writing, Transformer is reportedly used by Google Translate as its main translation algorithm.</li>
</ul>
<div class="packt_infobox">Google Research Blog. Transformer: A Novel Neural Network Architecture for Language Understanding. August 31, 2017. Available at: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://research.googleblog.com/2017/08/transformernovel-neural-network.html</a>.</div>
<p> </p>
<ul>
<li><strong>Self-driving vehicles</strong>: Uber, NVIDIA, and Waymo are believed to be using deep learning models to control different vehicle functions that control driving. Each company is researching a number of possibilities, including training the network using humans, simulating vehicles driving in virtual environments, and even creating a small city-like environment in which vehicles can be trained based on expected and unexpected events.</li>
</ul>
<div class="packt_infobox"><span>Alexis C. Madrigal: Inside Waymo's Secret World for Training SelfDriving Cars. The Atlantic. August 23, 2017. Available at: <a href="https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/">https://</a><br/>
<a href="https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/">www.theatlantic.com/technology/archive/2017/08/ inside-waymos-secret-testing-and-simulationfacilities/537648/</a>.<br/>
NVIDIA: <em>End-to-End Deep Learning for Self-Driving Cars</em>. August 17, 2016. Available at: <a href="https://devblogs.nvidia.com/deep-learning-self-driving-cars/">https://devblogs.nvidia.com/</a><br/>
<a href="https://devblogs.nvidia.com/deep-learning-self-driving-cars/">parallelforall/deep-learning-self-driving-cars/</a>.<br/>
Dave Gershgorn: Uber's new AI team is looking for the shortest route to self-driving cars. Quartz. December 5, 2016. Available at: <a href="https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/">https://qz.com/853236/ubers-new-ai-team-is-looking-for-theshortest-route-to-self-driving-cars/</a><a href="https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/">.<br/></a></span></div>
<ul>
<li><strong>Image recognition</strong>: Facebook and Google use deep learning models to identify entities in images and automatically tag these entities as persons from a set of contacts. In both cases, the networks are trained with previously tagged images as well as with images from the target friend or contact. Both companies report that the models are able to suggest a friend or contact with a high level of accuracy in most cases.</li>
</ul>
<p>While there are many more examples in other industries, the application of deep learning models is still in its infancy. Many more successful applications are yet to come, including the ones that you create.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why Do Neural Networks Work So Well?</h1>
                </header>
            
            <article>
                
<p>Why are neural networks so powerful? Neural networks are powerful because they can be used to predict any given function with reasonable approximation. If one is able to represent a problem as a mathematical function and also has data that represents that function correctly, then a deep learning model can, in principle—and given enough resources—be able to approximate that function. This is typically called the <em>universality principle of neural networks</em>.</p>
<div class="packt_infobox">For more information refer, Michael Nielsen: Neural Networks and Deep Learning: A visual proof that neural nets can compute any function. Available at: <a href="http://neuralnetworksanddeeplearning.com/chap4.html">http://neuralnetworksanddeeplearning.com/chap4.html</a>.</div>
<p>We will not be exploring mathematical proofs of the universality principle in this book. However, two characteristics of neural networks should give you the right intuition on how to understand that principle: representation learning and function approximation.</p>
<div class="packt_infobox"><span>For more information refer, Kai Arulkumaran, Marc Peter Deisenroth,Miles Brundage, and Anil Anthony Bharath. A Brief Survey of Deep Reinforcement Learning. arXiv. September 28, 2017. Available at: </span><a href="https://www.arxiv-vanity.com/papers/1708.05866/">https://www.arxiv-vanity.com/papers/1708.05866/ .</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representation Learning</h1>
                </header>
            
            <article>
                
<p>The data used to train a neural network contains representations (also known as <em>features</em>) that explain the problem you are trying to solve. For instance, if one is interested in recognizing faces from images, the color values of each pixel from a set of images that contain faces will be used as a starting point. The model will then continuously learn higher-level representations by combining pixels together as it goes through its training process.</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c5bc5ed5-3b48-435d-88ab-dbec13f2bed6.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 1: Series of higher-level representations that begin on input data. Derivate image based on original image from: Yann LeCun, Yoshua Bengio &amp; Geoffrey Hinton. "Deep Learning". Nature 521, 436–444 (28 May 2015) doi:10.1038/ nature14539<br/></span></div>
<p>In formal words, neural networks are computation graphs in which each step computes higher abstraction representations from input data.</p>
<p>Each one of these steps represents a progression into a different abstraction layer. Data progresses through these layers, building continuously higher-level representations. The process finishes with the highest representation possible: the one the model is trying to predict.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Function Approximation</h1>
                </header>
            
            <article>
                
<p>When neural networks learn new representations of data, they do so by combining weights and biases with neurons from different layers. They adjust the weights of these connections every time a training cycle takes place using a mathematical technique called back propagation. The weights and biases improve at each round, up to the point that an optimum is achieved.<span>This means that a neural network can measure how wrong it is on every training cycle, adjust the weights and biases of each neuron, and try again. If it determines that a certain modification produces better results than the previous round, it will invest in that modification until an optimal solution is achieved.</span></p>
<p><span>In a nutshell, that procedure is the reason why neural networks can approximate functions. However, there are many reasons why a neural network may not be able to predict a function with perfection, chief among them being that:</span></p>
<ul>
<li><span>Many functions contain stochastic properties (that is, random properties)</span></li>
<li><span>There may be overfitting to peculiarities from the training data</span></li>
<li><span>There may be a lack of training data<br/></span></li>
</ul>
<p>In many practical applications, simple neural networks are able to approximate a function with reasonable precision. These sorts of applications will be our focus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of Deep Learning</h1>
                </header>
            
            <article>
                
<p>Deep learning techniques are best suited to problems that can be defiled with formal mathematical rules (that is, as data representations). If a problem is hard to define this way, then it is likely that deep learning will not provide a useful solution. Moreover, if the data available for a given problem is either biased or only contains partial representations of the underlying functions that generate that problem, then deep learning techniques will only be able to reproduce the problem and not learn to solve it.</p>
<p>Remember that deep learning algorithms are learning different representations of data to approximate a given function. If data does not represent a function appropriately, it is likely that a function will be incorrectly represented by a neural network. Consider the following analogy: you are trying to predict the national prices of gasoline (that is, fuel) and create a deep learning model. You use your credit card statement with your daily expenses on gasoline as an input data for that model. The model may eventually learn the patterns of your gasoline consumption, but it will likely misrepresent price fluctuations of gasoline caused by other factors only represented weekly in your data such as government policies, market competition, international politics, and so on. The model will ultimately yield incorrect results when used in production.</p>
<p>To avoid this problem, make sure that the data used to train a model represents the problem the model is trying to address as accurately as possible.</p>
<div class="packt_infobox">For an in-depth discussion of this topic, refer to François Chollet's upcoming book Deep Learning with Python. François is the creator of Keras, a Python library used in this book. The chapter, The limitations of deep learning, is particularly important for understanding this topic. The working version of that book is available at: <a href="https://blog.keras.io/the-limitations-of-deep-learning.html">https://blog.keras.io/the-limitations-of-deeplearning.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inherent Bias and Ethical Considerations</h1>
                </header>
            
            <article>
                
<p>Researchers have suggested that the use of the deep learning model without considering the inherent bias in the training data can lead not only to poor performing solutions, but also to ethical complications.</p>
<p>For instance, in late 2016, researchers from the Shanghai Jiao Tong University in China created a neural network which correctly classified criminals using only pictures from their faces. The researchers used 1,856 images of Chinese men in which half had been convicted.</p>
<div class="packt_infobox">Their model identifiled inmates with 89.5 percent accuracy. (<a href="https://blog.keras.io/the-limitations-of-deep-learning.html">https:// blog.keras.io/the-limitations-of-deep-learning.html</a>). MIT Technology Review. Neural Network Learns to Identify Criminals by Their Faces. November 22, 2016. Available at: <a href="https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/">https://www.technologyreview.com/s/602955/neuralnetwork-learns-to-identify-criminals-by-their-faces/</a>.</div>
<p>The paper resulted in great furor within the scientific community and popular media. One key issue with the proposed solution is that it fails to properly recognize the bias inherent in the input data. Namely, the data used in this study came from two different sources: one for criminals and one for non-criminals. Some researchers suggest that their algorithm identifies patterns associated with the different data sources used in the study instead of identifying relevant patterns from people's faces. While there are technical considerations one can make about the reliability of the model, the key criticism is on ethical grounds: one ought to clearly recognize the inherent bias in input data used by deep learning algorithms and consider how its application will have an impact on people's lives.</p>
<div class="packt_infobox">Timothy Revell. Concerns as face recognition tech used to 'identify' criminals. New Scientist. December 1, 2016. Available at: <a href="https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/">https://www.newscientist.com/article/2114900-concernsas-face-recognition-tech-used-to-identify-criminals/</a>. For understanding more about the topic of ethics in learning algorithms (including deep learning), refer to the work done by the AI Now Institute (<a href="https://ainowinstitute.org/">https://ainowinstitute.org/</a>), an organization created for the understanding of the social implications of intelligent systems.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Common Components and Operations of Neural Networks</h1>
                </header>
            
            <article>
                
<p>Neural networks have two key components: layers and nodes. Nodes are responsible for specific operations, and layers are groups of nodes used to differentiate different stages of the system.</p>
<p>Typically, neural networks have the following three categories of layers:</p>
<ul>
<li><span><strong>Input</strong>: Where the input data is received and first interpreted</span></li>
<li><span><strong>Hidden</strong>: Where computations take place, modifying data as it goes through</span></li>
<li><span><strong>Output</strong>: Where an output is assembled and evaluated<br/></span></li>
</ul>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/45a615a6-4d0b-4b53-aeec-f523b88145a9.png" style="width:20.33em;height:24.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2: Illustration of the most common layers in a neural network. By Glosser.ca - Own work, Derivative of File: Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461<br/></span></div>
<p>Hidden layers are the most important layers in neural networks. They are referred to as <em>hidden</em> because the representations generated in them are not available in the data, but are learned from it. It is within these layers where the main computations take place in neural networks.</p>
<p>Nodes are where data is represented in the network. There are two values associated with nodes: biases and weights. Both of these values affect how data is represented by the nodes and passed on to other nodes. When a network learns, it effectively adjusts these values to satisfy an optimization function.</p>
<p>Most of the work in neural networks happens in the hidden layers. Unfortunately, there isn't a clear rule for determining how many layers or nodes a network should have. When implementing a neural network, one will probably spend time experimenting with different combinations of layers and nodes. It is advised to always start with a single layer and also with a number of nodes that reflect the number of features the input data has (that is, how many <em>columns</em> are available in a dataset). One will then continue to add layers and nodes until satisfactory performance is achieved—or whenever the network starts over fitting to the training data.</p>
<p>Contemporary neural network practice is generally restricted to the experimentation with the number of nodes and layers (for example, how deep the network is), and the kinds of operations performed at each layer. There are many successful instances in which neural networks outperformed other algorithms simply by adjusting these parameters.</p>
<p>As an intuition, think about data entering a neural network system via the input layer, then moving through the network from node to node. The path that data takes will depend on how interconnected the nodes are, the weights and the biases of each node, the kind of operations that are performed in each layer, and the state of data at the end of such operations. Neural networks often require many "runs" (or epochs) in order to keep tuning the weights and biases of nodes, meaning that data flaws over the different layers of the graph multiple times.</p>
<p>This section offered you an overview of neural networks and deep learning. Additionally, we discussed a starter's intuition to understand the following key concepts:</p>
<ul>
<li>Neural networks can, in principle, approximate most functions, given that it has enough resources and data.</li>
<li>Layers and nodes are the most important structural components of neural networks. One typically spends a lot of time altering those components to find a working architecture.</li>
<li>Weights and biases are the key properties that a network "learns" during its training process.</li>
</ul>
<p>Those concepts will prove useful in our next section as we explore a real-world trained neural network and make modifications to train our own.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a Deep Learning Environment</h1>
                </header>
            
            <article>
                
<p>Before we finish this chapter, we want you to interact with a real neural network. We will start by covering the main software components used throughout this book and make sure that they are properly installed. We will then explore a pretrained neural network and explore a few of the components and operations discussed earlier in the What are Neural Networks? section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software Components for Deep Learning</h1>
                </header>
            
            <article>
                
<p><span>We'll use the following software components for deep learning:</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python 3</h1>
                </header>
            
            <article>
                
<p>We will be using Python 3 . Python is a general-purpose programming language which is very popular with the scientific community—hence its adoption in deep learning. Python 2 is not supported in this book but can be used to train neural networks instead of Python 3. Even if you chose to implement your solutions in Python 2, consider moving to Python 3 as its modern feature set is far more robust than that of its predecessor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a library used for performing mathematical operations in the form of graphs. TensorFlow was originally developed by Google and today it is an open-source project with many contributors. It has been designed with neural networks in mind and is among the most popular choices when creating deep learning algorithms.</p>
<p>TensorFlow is also well-known for its production components. It comes with TensorFlow Serving <a href="https://github.com/tensorflow/serving">(https://github.com/tensorflow/serving),</a> a high-performance system for serving deep learning models. Also, trained TensorFlow models can be consumed in other high-performance programming languages such as Java, Go, and C. This means that one can deploy these models in anything from a micro-computer (that is, a RaspberryPi) to an Android device.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras</h1>
                </header>
            
            <article>
                
<p>In order to interact efficiently with TensorFlow, we will be using Keras (<a href="https://keras.io/">https://keras.io/</a>), a Python package with a high-level API for developing neural networks. While TensorFlow focuses on components that interact with each other in a computational graph, Keras focuses specifically on neural networks. Keras uses TensorFlow as its backend engine and makes developing such applications much easier.</p>
<p>As of November 2017 (TensorFlow version 1.4), Keras is distributed as part of TensorFlow. It is available under the <kbd>tf.keras</kbd> namespace. If you have TensorFlow 1.4 or higher installed, you already have Keras available in your system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorBoard</h1>
                </header>
            
            <article>
                
<p>TensorBoard is a data visualization suite for exploring TensorFlow models and is natively integrated with TensorFlow. TensorBoard works by consuming checkpoint and summary files created by TensorFlow as it trains a neural network. Those can be explored either in near real-time (with a 30 second delay) or after the network has finished training.</p>
<p>TensorBoard makes the process of experimenting and exploring a neural network much easier—plus it's quite exciting to follow the training of your network!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Jupyter Notebooks, Pandas, and NumPy</h1>
                </header>
            
            <article>
                
<p>When working to create deep learning models with Python, it is common to start working interactively, slowly developing a model that eventually turns into more structured software. Three Python packages are used frequently during that process: Jupyter Notebooks, Pandas, and <kbd>NumPy</kbd>:</p>
<ul>
<li>Jupyter Notebooks create interactive Python sessions that use a web browser as its interface</li>
<li>Pandas is a package for data manipulation and analysis</li>
<li>NumPy is frequently used for shaping data and performing numerical computations</li>
</ul>
<p>These packages are used occasionally. They typically do not form part of a production system, but are often used when exploring data and starting to build a model. We focus on the other tools in much more detail.</p>
<div class="packt_infobox">The book <em>Learning Pandas</em> by Michael Heydt (June 2017, Packt Publishing) and <em>Learning Jupyter</em> by Dan Toomey (November 2016, Packt Publishing) offer a comprehensive guide on how to use these technologies. These books are good references for continuing to learn more.</div>
<table style="border-collapse: collapse;width: 89.942%">
<tbody>
<tr>
<td style="width: 14.6491%"><span><strong>Component</strong><br/></span></td>
<td style="width: 52.3509%"><span><strong>Description</strong><br/></span></td>
<td style="width: 18%"><span><strong>Minimum Version</strong><br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>Python<br/></span></td>
<td style="width: 52.3509%"><span>General-purpose programming language. Popular language   used in  the development of deep learning applications.<br/></span></td>
<td style="width: 18%"><span> 3.6<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>TensorFlow<br/></span></td>
<td style="width: 52.3509%"><span>Open-source graph computation Python package typically used for  developing deep learning systems.<br/></span></td>
<td style="width: 18%"><span> 1.4<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>Keras<br/></span></td>
<td style="width: 52.3509%"><span>Python package that provides a high-level interface to TensorFlow.<br/></span></td>
<td style="width: 18%"><span> 2.0.8-tf (distributed<br/>
with TensorFlow)<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>TensorBoard<br/></span></td>
<td style="width: 52.3509%"><span>Browser-based software for visualizing neural network statistics.<br/></span></td>
<td style="width: 18%"><span> 0.4.0<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>Jupyter Notebook<br/></span></td>
<td style="width: 52.3509%"><span>Browser-based software for working interactively  with Python sessions.<br/></span></td>
<td style="width: 18%"><span> 5.2.1<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>Pandas<br/></span></td>
<td style="width: 52.3509%"><span>Python package for analyzing and manipulating data.<br/></span></td>
<td style="width: 18%"><span> 0.21.0<br/></span></td>
</tr>
<tr>
<td style="width: 14.6491%"><span>NumPy<br/></span></td>
<td style="width: 52.3509%"><span>Python package for high-performance numerical computations.<br/></span></td>
<td style="width: 18%"><span> 1.13.3<br/></span></td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Table 1: Software components necessary for creating a deep learning environment<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Verifying Software Components</h1>
                </header>
            
            <article>
                
<p>Before we explore a trained neural network, let's verify that all the software components that we need are available. We have included a script that verifies these components work. Let's take a moment to run the script and deal with any eventual problems we may find.</p>
<p>We will now be testing if the software components required for this book are available in your working environment. First, we suggest the creation of a Python virtual environment using Python's native module <kbd>venv</kbd>. Virtual environments are used for managing project dependencies. We advise each project you create to have its own virtual environments. Let's create one now.</p>
<div class="packt_infobox"><span>If you are more comfortable with <kbd>conda</kbd> environments, feel free to use those instead.<br/></span></div>
<ol>
<li><span>A Python virtual environment can be created by using the following command:<br/></span></li>
</ol>
<pre><span>      $ python3 -m venv venv<br/>      $ source venv/bin/activate</span></pre>
<ol start="2">
<li><span>The latter command will append the string (<kbd>venv</kbd>) to the beginning of your command line. Use the following command to deactivate your virtual environment:<br/></span></li>
</ol>
<pre>      <span>$ deactivate </span></pre>
<div class="packt_infobox">Make sure to always activate your Python virtual environment when working on a project.</div>
<ol start="3">
<li>After activating your virtual environment, make sure that the right components are installed by executing pip over the file <kbd>requirements.txt</kbd>. This will attempt to install the models used in this book in that virtual environment. It will do nothing if they are already available:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d4be57f-1d5b-4db6-8117-43b5b8dde66b.png" style="width:81.17em;height:34.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 3: Image of a terminal running pip to install dependencies from requirements.txt<br/></span></div>
<p style="padding-left: 60px">Install dependencies by running the following command:</p>
<pre>      $ pip install –r requirements.txt </pre>
<p style="padding-left: 60px">This will install all required dependencies for your system. If they are already installed, this command should simply inform you.</p>
<p style="padding-left: 60px">These dependencies are essential for working with all code activities.</p>
<p style="padding-left: 60px">As a final step on this activity, let's execute the script <kbd>test_stack.py</kbd>. That script formally verifies that all the required packages for this book are installed and available in your system.</p>
<ol start="4">
<li>Students, run the script <kbd>Chapter_4/activity_1/test_stack.py</kbd> to check if the dependencies Python 3, TensorFlow, and Keras are available. Use the following command:</li>
</ol>
<pre>      <span>$ python3 chapter_4/activity_1/test_stack.py </span></pre>
<p style="padding-left: 60px">The script returns helpful messages stating what is installed and what needs to be installed.</p>
<ol start="5">
<li><span>Run the following script command in your terminal:<br/></span></li>
</ol>
<pre>      <span>$ tensorboard --help </span></pre>
<p style="padding-left: 60px"><span>You should see a help message that explains what each command does. If you do not see that message – or see an error message instead – please ask for assistance from your instructor:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3898b1a8-36ce-43d2-bb14-5b5b79874129.png" style="width:83.83em;height:30.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4: Image of a terminal running <kbd>python3 test_stack.py</kbd>. The script returns messages informing that all dependencies are installed correctly.</div>
<div class="packt_infobox"><span>If a similar message to the following appears, there is no need to worry:</span><span><br/>
Runtime Warning: compile time version 3.5 of module '<kbd>tensorflow.python.framework.fast_tensor_util</kbd>'does not match runtime version 3.6 return f(*args, **kwds)<br/></span> <span>That message appears if you are running Python 3.6 and the distributed<br/>
TensorFlow wheel was compiled under a different version (in this case, 3.5). You can safely ignore that message.<br/></span></div>
<p>Once we have verified that Python 3, TensorFlow, Keras, TensorBoard, and the packages outlined in <kbd>requirements.txt</kbd> have been installed, we can continue to a demo on how to train a neural network and then go on to explore a trained network using these same tools.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring a Trained Neural Network</h1>
                </header>
            
            <article>
                
<p>In this section, we explore a trained neural network. We do that to understand how a neural network solves a real-world problem (predict handwritten digits) and also to get familiar with the TensorFlow API. When exploring that neural network, we will recognize many components introduced in previous sections such as nodes and layers, but we will also see many that we don't recognize (such as activation functions)—we will explore those in further sections. We will then walk through an exercise on how that neural network was trained and then train that same network on our own.</p>
<p>The network that we will be exploring has been trained to recognize numerical digits (integers) using images of handwritten digits. It uses the MNIST dataset (<a href="http://yann.lecun.com/exdb/mnist/">http:// yann.lecun.com/exdb/mnist/</a>), a classic dataset frequently used for exploring pattern  <span>recognition tasks.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MNIST Dataset</h1>
                </header>
            
            <article>
                
<p>The <strong>Modifiled National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) dataset contains a training set of 60,000 images and a test set of 10,000 images. Each image contains a single handwritten number. This dataset—which is a derivate from one created by the US Government—was originally used to test different approaches to the problem of recognizing handwritten text by computer systems. Being able to do that was important for the purpose of increasing the performance of postal services, taxation systems, and government services. The MNIST dataset is considered too naïve for contemporary methods. Different and more recent datasets are used in contemporary research (for example, CIFAR). However, the MNIST dataset is still very useful for understanding how neural networks work because known models can achieve a high level of accuracy with great efficiency.</p>
<div class="packt_infobox">The CIFAR dataset is a machine learning dataset that contains images organized in different classes. Different than the MNIST dataset, the CIFAR dataset contains classes in many different areas such as animals, activities, and objects. The CIFAR dataset is available at: <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>.</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5bd59dd7-5cc3-4825-8b1e-02a29a48e636.png" style="width:35.33em;height:35.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5: Excerpt from the training set of the MNIST dataset. Each image is a separate 20x20 pixels image with a single handwritten digit. The original dataset is available at: http://yann.lecun.com/exdb/mnist/.<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a Neural Network with TensorFlow</h1>
                </header>
            
            <article>
                
<p>Now, let's train a neural network to recognize new digits using the MNIST dataset.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will be implementing a special-purpose neural network called "Convolutional Neural Network" to solve this problem (we will discuss those in more detail in further sections). Our network contains three hidden layers: two fully connected layers and a convolutional layer. The convolutional layer is defiled by the following TensorFlow snippet of Python code:</p>
<pre><span>    W = tf.Variable(<br/>        tf.truncated_normal([5, 5, size_in, size_out],<br/>        stddev=0.1),<br/>        name="Weights")    <br/>    B = tf.Variable(tf.constant(0.1, shape=[size_out]), <br/>        name="Biases")<br/><br/>    convolution = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1],<br/>    padding="SAME")<br/>    activation = tf.nn.relu(convolution + B)<br/><br/>    tf.nn.max_pool(<br/>    activation,<br/>    ksize=[1, 2, 2, 1],<br/>    strides=[1, 2, 2, 1],<br/>    padding="SAME") </span></pre>
<p>We execute that snippet of code only once during the training of our network.</p>
<p>The variables W and B stand for weights and biases. These are the values used by the nodes within the hidden layers to alter the network's interpretation of the data as it passes through the network. Do not worry about the other variables for now.</p>
<p><span>The</span> <strong>fully connected layers</strong> <span>are defiled by the following snippet of Python code:<br/></span></p>
<pre><span>    W = tf.Variable(<br/>        tf.truncated_normal([size_in, size_out], stddev=0.1),<br/>        name="Weights")<br/>    B = tf.Variable(tf.constant(0.1, shape=[size_out]),<br/>        name="Biases")<br/>        activation = tf.matmul(input, W) + B</span></pre>
<p>Here, we also have the two TensorFlow variables W and B. Notice how simple the initialization of these variables is: W is initialized as a random value from a pruned Gaussian distribution (pruned with <kbd>size_in and size_out</kbd>) with a standard deviation of 0.1, and B (the bias term) is initialized as <kbd>0.1,</kbd> a constant. Both these values will continuously change during each run. That snippet is executed twice, yielding two fully connected networks— one passing data to the other.</p>
<p>Those 11 lines of Python code represent our complete neural network. We will go into a lot more detail in <em>Chapter 5</em>, <em>Model Architecture</em> about each one of those components using Keras. For now, focus on understanding that the network is altering the values of W and B in each layer on every run and how these snippets form different layers. These 11 lines of Python are the culmination of dozens of years of neural network research.</p>
<p><span>Let's now train that network to evaluate how it performs in the MNIST dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a Neural Network</h1>
                </header>
            
            <article>
                
<p><span>Follow the following steps to set up this exercise:<br/></span></p>
<ol>
<li><span>Open two terminal instances.</span></li>
<li><span>In both of them, navigate to the directory <kbd>chapter_4/exercise_a</kbd>.</span></li>
<li><span>In both of them, make sure that your Python 3 virtual environment is active and that the requirements outlined in <kbd>requirements.txt</kbd> are installed.</span></li>
<li><span>In one of them, start a TensorBoard server with the following command:<br/>
<kbd>$ tensorboard --logdir=mnist_example/</kbd></span></li>
<li><span>In the other, run the <kbd>train_mnist.py</kbd> script from within that directory.<br/></span></li>
<li><span>Open your browser in the TensorBoard URL provided when you start the server.<br/></span></li>
</ol>
<p>In the terminal that you ran the script <kbd>train_mnist.py</kbd>, you will see a progress bar with the epochs of the model. When you open the browser page, you will see a couple of graphs. Click on the one that reads <kbd>Accuracy</kbd>, enlarge it and let the page refresh (or click on the <kbd>refresh</kbd> button). You will see the model gaining accuracy as epochs go by.</p>
<p>Use that moment to explain the power of neural networks in reaching a high level of accuracy very early in the training process.</p>
<p>We can see that in about 200 epochs (or steps), the network surpassed 90 percent accuracy. That is, the network is getting 90 percent of the digits in the test set correctly. The network continues to gain accuracy as it trains up to the 2,000th step, reaching a 97 percent accuracy at the end of that period.</p>
<p>Now, let's also test how well those networks perform with unseen data. We will use an open-source web application created by Shafeen Tejani to explore if a trained network correctly predicts handwritten digits that we create.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing Network Performance with Unseen Data</h1>
                </header>
            
            <article>
                
<p>Visit the website <a href="http://mnist-demo.herokuapp.com/">http://mnist-demo.herokuapp.com/</a> in your browser and draw a number between 0 and 9 in the designated white box:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a788e857-da7e-40ac-9b39-b83d78219375.png" style="width:74.08em;height:50.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6: Web application in which we can manually draw digits and test the accuracy of two trained networks<br/></span></div>
<div class="packt_infobox"><span>Source: <a href="https://github.com/ShafeenTejani/mnist-demo">https://github.com/ShafeenTejani/mnist-demo</a> . <br/></span></div>
<p>In the application, you can see the results of two neural networks. The one that we have trained is on the left (called CNN). Does it classify all your handwritten digits correctly? Try drawing numbers at the edge of the designated area. For instance, try drawing the number <strong>1</strong> close to the right edge of that area:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2bf7e60d-4981-42f6-97df-64bbc1d1fe72.png" style="width:70.42em;height:48.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 7: Both networks have a difficult time estimating values drawn on the edges of the area<br/></span></div>
<div class="packt_infobox">In this example, we see the number <strong>1</strong> drawn to the right side of the drawing area. The probability of this number being a <strong>1</strong> is <strong>0</strong> in both networks.</div>
<p>The MNIST dataset does not contain numbers on the edges of images. Hence, neither network assigns relevant values to the pixels located in that region. Both networks are much better at classifying numbers correctly if we draw them closer to the center of the designated area. This shows that neural networks can only be as powerful as the data that is used to train them. If the data used for training is very different than what we are trying to predict, the network will most likely produce disappointing results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity: Exploring a Trained Neural Network</h1>
                </header>
            
            <article>
                
<p>In this section, we will explore the neural network that we have trained during our exercise. We will also train a few other networks by altering hyper parameters from our original one. Let's start by exploring the network trained in our exercise.</p>
<p>We have provided that same trained network as binary files in the directory. Let's open that trained network using TensorBoard and explore its components.</p>
<p>Using your terminal, navigate to the directory <kbd>chapter_4/activity_2</kbd> and execute the following command to start TensorBoard:</p>
<pre>  <span>$ tensorboard --logdir=mnist_example/ </span></pre>
<p><span>Now, open the URL provided by TensorBoard in your browser. You should be able to see the TensorBoard scalars page:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/82521ed5-42e0-4c0b-8c13-3d47a26ca2b2.png" style="width:77.75em;height:39.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 8: Image of a terminal after starting a TensorBoard instance<br/></span></div>
<p><span>After you open the URL provided by the <kbd>tensorboard</kbd> command, you should be able to see the following TensorBoard page:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/52461105-3b8e-47e5-9bef-def7c8431481.png" style="width:72.17em;height:49.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9: Image of the TensorBoard landing page<br/></span></div>
<p>Let's now explore our trained neural network and see how it performed.</p>
<p>On the TensorBoard page, click on the <strong>Scalars</strong> page and enlarge the <strong>Accuracy</strong> graph. Now, move the <strong>Smoothing</strong> slider to <strong>0.9</strong>.</p>
<p>The accuracy graph measures how accurate the network was able to guess the labels of a test set. At fist, the network guesses those labels completely wrong. This happens because we have initialized the weights and biases of our network with random values, so its fist attempts are a guess. The network will then change the weights and biases of its layers on a second run; the network will continue to invest in the nodes that give positive results by altering their weights and biases, and penalize those that don't by gradually reducing their impact on the network (eventually reaching 0). As you can see, this is a really efficient technique that quickly yields great results.</p>
<p>Let's focus our attention on the <strong>Accuracy</strong> graph. See how the algorithm manages to reach great accuracy (&gt; 95 percent) after around 1,000 epochs? What happens between 1,000 and 2,000 epochs?</p>
<p>Would it get more accurate if we continued to train with more epochs? Between 1,000 and 2,000 is when the accuracy of the network continues to improve, but at a decreasing rate. The network may improve slightly if trained with more epochs, but it will not reach 100 percent accuracy with the current architecture.</p>
<p>The script is a modified version of an official Google script that was created to show how TensorFlow works. We have divided the script into functions that are easier to understand and added many comments to guide your learning. Try running that script by modifying the variables at the top of the script:</p>
<pre><span>     LEARNING_RATE = 0.0001<br/>     EPOCHS = 2000</span></pre>
<p>Now, try running that script by modifying the values of those variables. For instance, try modifying the learning rate to <strong>0.1</strong> and the epochs to <strong>100</strong>. Do you think the network can achieve comparable results?</p>
<div class="packt_infobox">There are many other parameters that you can modify in your neural network. For now, experiment with the epochs and the learning rate of your network. You will notice that those two on their own can greatly change the output of your network—but only by so much. Experiment to see if you can train this network faster with the current architecture just by altering those two parameters.</div>
<p>Verify how your network is training using TensorBoard. Alter those parameters a few more times by multiplying the starting values by 10 until you notice that the network is improving. This process of tuning the network and finding improved accuracy is similar to what is used in industry applications today to improve existing neural network models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored a TensorFlow-trained neural network using TensorBoard and trained our own modified version of that network with different epochs and learning rates. This gave you hands-on experiences on how to train a highly performant neural network and also allowed you to explore some of its limitations.</p>
<p>Do you think we can achieve similar accuracy with real Bitcoin data? We will attempt to predict future Bitcoin prices using a common neural network algorithm during C<em>hapter 5</em>, <em>Model Architecture</em>. In C<em>hapter 6</em>, <em>Model Evaluation and Optimization</em>, we will evaluate and improve that model and, finally, in C<em>hapter 7</em>, <em>Productization</em>, we will create a program that serves the prediction of that system via a HTTP API.</p>


            </article>

            
        </section>
    </body></html>