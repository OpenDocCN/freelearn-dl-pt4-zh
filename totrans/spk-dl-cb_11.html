<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating and Visualizing Word Vectors Using Word2Vec</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Acquiring data</li>
<li>Importing the necessary libraries</li>
<li>Preparing the data</li>
<li>Building and training the model</li>
<li>Visualizing further</li>
<li>Analyzing further</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Before training a neural<span> </span>network<span> on text data and generating text using LSTM cells, it is important to understand how text data (such as words, sentences, customer reviews, or stories) is converted to word vectors first before it is fed into a neural network. This chapter will describe how to convert a text into a corpus and generate word vectors from the corpus, which makes it easy to group similar words using techniques such as Euclidean distance calculation or cosine distance calculation between different word vectors.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Acquiring data</h1>
                </header>
            
            <article>
                
<p>The first step is to acquire some data to work with. For this chapter, we will require a lot of text data to convert it into tokens and visualize it to understand how neural networks rank word vectors based on Euclidean and Cosine distances. It is an important step in understanding how different words get associated with each other. This, in turn, can be used to design better, more efficient language and text-processing models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li class="mce-root">The text data for the model needs to be in files of <kbd>.txt</kbd> format, and you must ensure that the files are placed in the current working directory. The text data can be anything from Twitter feeds, news feeds, customer reviews, computer code, or whole books saved in the <kbd>.txt</kbd> format in the working directory. In our case, we have used the <em>Game of Thrones</em> books as the input text to our model. However, any text can be substituted in place of the books, and the same model will work.</li>
<li>Many classical texts are no longer protected under copyright. This means that you can download all of the text for these books for free and use them in experiments, such as creating generative models. The best place to get access to free books that are no longer protected by copyright is<span> Project Gutenberg (<a href="https://www.gutenberg.org/">https://www.gutenberg.org/</a>).</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>Begin by visiting the Project Gutenberg website and browsing for a book that interests you. Click on the book, and then click on <span class="packt_screen">UTF-8</span>, which allows you to download the book in plain-text format. The link is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1608 image-border" src="assets/de2e19ef-378b-4782-b3b1-0393bdb35026.png" style="width:52.42em;height:20.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Project Gutenberg Dataset download page</div>
<ol start="2">
<li><span>After clicking on</span> <span class="packt_screen">Plain Text UTF-8</span><span>, you should see a page that looks like the following screenshot. Right click on the page and click on</span> <span class="packt_screen">Save As..</span><span>. Next, rename the file to whatever you choose and save it in your working directory:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1609 image-border" src="assets/44d32c31-04e8-4ee0-b4e6-2106b0afafc4.png" style="width:33.00em;height:44.33em;"/></div>
<ol start="3">
<li>You should now see a <kbd>.txt</kbd> file with the specified filename in your current working directory.</li>
<li><span>Project Gutenberg adds a standard header and footer to each book; this is not part of the original text. Open the file in a text editor, and delete the header and the footer.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>Check for the current working directory using the following command: <kbd>pwd</kbd>.</li>
<li>The working directory can be changed using the <kbd>cd</kbd> command as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1578 image-border" src="assets/4399df8e-ceb1-4d15-aa8f-abd4ab5b8e26.png" style="width:29.33em;height:9.58em;"/></div>
<ol start="3">
<li>Notice that, in our case, the text files are contained in a folder named <kbd>USF</kbd>, and, therefore, this is set as the working directory. You may similarly store one or more <kbd>.txt</kbd> files in the working directory for use as input to the model.</li>
<li>UTF-8 specifies the type of encoding of the characters in the text file. <strong>UTF-8</strong> stands <span>for <strong>Unicode Transformation Format</strong>. The <strong>8</strong> means it uses <strong>8-bit</strong> blocks to represent a character.</span></li>
<li>UTF-8 is a compromise character encoding that can be as compact as ASCII (if the file is just plain-English text) but can also contain any Unicode characters (with some increase in file size).</li>
<li>It is not necessary for the text file to be in a UTF-8 format, as we will use the codecs library at a later stage to encode all the text into the Latin1 encoding format.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>For more information about UTF-8 and Latin1 encoding formats, visit the following links:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/UTF-8">https://en.wikipedia.org/wiki/UTF-8</a></li>
<li><a href="http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html">http://www.ic.unicamp.br/~stolfi/EXPORT/www/ISO-8859-1-Encoding.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Visit the following link to understand the need for word vectors in neural networks better:<br/>
<a href="https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1">https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1</a><br/></p>
<p>Listed below are some other useful articles related to the topic of converting words to vectors:<br/>
<a href="https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/">https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/</a></p>
<p><a href="https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817">https://towardsdatascience.com/word-to-vectors-natural-language-processing-b253dd0b0817</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing the necessary libraries</h1>
                </header>
            
            <article>
                
<p>Before we begin, we require the following libraries and dependencies, which need to be imported into our Python environment. These libraries will make our tasks a lot easier, as they have readily available functions and models that can be used instead of doing that ourselves. This also makes the code more compact and readable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The following libraries and dependencies will be required to create word vectors and plots and visualize the n-dimensional word vectors in a 2D space:</p>
<ul>
<li><kbd>future</kbd></li>
<li><kbd>codecs</kbd></li>
<li><kbd>glob</kbd></li>
<li><kbd><kbd>multiprocessing</kbd></kbd></li>
<li><kbd>os</kbd></li>
<li><kbd><kbd>pprint</kbd></kbd></li>
<li><kbd>re</kbd></li>
<li><kbd>nltk</kbd></li>
<li><kbd>Word2Vec</kbd></li>
<li><kbd>sklearn</kbd></li>
<li><kbd>numpy</kbd></li>
<li><kbd>matplotlib</kbd></li>
<li><kbd>pandas</kbd></li>
<li><kbd>seaborn</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>Type the following commands into your Jupyter notebook to import all the required libraries:</li>
</ol>
<pre style="padding-left: 60px">from __future__ import absolute_import, division, print_function<br/>import codecs<br/>import glob<br/>import logging<br/>import multiprocessing<br/>import os<br/>import pprint<br/>import re<br/>import nltk<br/>import gensim.models.word2vec as w2v<br/>import sklearn.manifold<br/>import numpy<br/>as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns<br/>%pylab inline</pre>
<ol start="2">
<li>You should see an output that looks like the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1579 image-border" src="assets/cf709ffc-0cbc-419d-8b2c-b3b55f7c4daa.png" style="width:43.83em;height:23.58em;"/></div>
<ol start="3">
<li>Next, import the <kbd>stopwords</kbd> and <kbd>punkt</kbd> libraries using the following commands:</li>
</ol>
<pre style="padding-left: 90px">nltk.download("punkt")<br/>nltk.download("stopwords")</pre>
<ol start="4">
<li>The output you see must look like the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1580 image-border" src="assets/0e52652d-b201-4877-8399-99a018016314.png" style="width:73.75em;height:18.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section will describe the purpose of each library being used for this recipe.</p>
<ol>
<li>The <kbd>future</kbd> library is the missing link between Python 2 and Python 3. It acts as a bridge between the two versions and allows us to use syntax from both versions.</li>
<li>
<p>The <kbd>codecs</kbd> library will be used to perform the encoding of all words present in the text file. This constitutes our dataset.</p>
</li>
<li>Regex is the library used to look up or search for a file really quickly. The <kbd>glob</kbd> function allows quick and efficient searching through a large database for a required file.</li>
<li>
<p>The <kbd>multiprocessing</kbd> library allows us to perform concurrency, which is a way of running multiple threads and having each thread run a different process. It is a way of making programs run faster by parallelization.                                            </p>
</li>
<li>The <kbd>os</kbd> library allows easy interaction with the operating system, such as a Mac, Windows, and so on, and performs functions such as reading a file.</li>
<li>
<p>The <kbd>pprint</kbd> library provides a capability for <span>pretty-printing arbitrary Python data structures in a form that can be used as input to the interpreter.</span></p>
</li>
<li>The <kbd>re</kbd> <span>module provides regular expression matching operations similar to those found in Perl.</span></li>
<li>NLTK is a natural language toolkit capable of tokenizing words in very short code. When fed in a whole sentence, the <kbd>nltk</kbd> function breaks up sentences and outputs tokens for each word. Based on these tokens, the words may be organized into different categories. NLTK does this by comparing each word with a huge database of pre-trained words called a <strong>lexicon</strong>.</li>
<li><kbd>Word2Vec</kbd> is Google's model, trained on a huge dataset of word vectors. It groups semantically similar words close to one another. This will be the most important library for this section.</li>
<li><kbd>sklearn.manifold</kbd> allows the dimensionality reduction of the dataset by employing <strong>t-distributed Stochastic Neighbor Embedding</strong> <span>(<strong>t-SNE</strong></span>) techniques. Since each word vector is multi-dimensional, we require some form of dimensionality reduction techniques to bring the dimensionality of these words down to a lower dimensional space so it can be visualized in a 2D space.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><kbd>Numpy</kbd> is a commonly used <kbd>math</kbd> library. <kbd>Matplotlib</kbd> is the <kbd>plotting</kbd> library we will utilize, and <kbd>pandas</kbd> provide a lot of flexibility in data handling by allowing easy reshaping, slicing, indexing, subsetting, and manipulation of data.</p>
<p>The <kbd>Seaborn</kbd> library is another statistical data visualization library that we require along with <kbd>matplotlib</kbd>. <kbd>Punkt</kbd> and <kbd>Stopwords</kbd> are two data-processing libraries that simplify tasks such as splitting a piece of text from a corpus into tokens (that is, via tokenization) and removing <kbd>stopwords</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>For more information regarding some of the libraries utilized, visit the following links:</p>
<ul>
<li><a href="https://docs.python.org/3/library/codecs.html">https://docs.python.org/3/library/codecs.html</a></li>
<li><a href="https://docs.python.org/2/library/pprint.html">https://docs.python.org/2/library/pprint.html</a></li>
<li><a href="https://docs.python.org/3/library/re.html">https://docs.python.org/3/library/re.html</a></li>
<li><a href="https://www.nltk.org/">https://www.nltk.org/</a></li>
<li><a href="https://www.tensorflow.org/tutorials/word2vec">https://www.tensorflow.org/tutorials/word2vec</a></li>
<li><a href="http://scikit-learn.org/stable/modules/manifold.html">http://scikit-learn.org/stable/modules/manifold.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>A number of data-preprocessing steps are to be performed before the data is fed into the model. This section will describe how to clean the data and prepare it so it can be fed into the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>All the text from the <kbd>.txt</kbd> files is first converted into one big corpus. This is done by reading each sentence from each file and adding it to an empty corpus. A number of preprocessing steps are then executed to remove irregularities such as white spaces, spelling errors, <kbd>stopwords</kbd>, and so on. The cleaned text data has to then be tokenized, and the tokenized sentences are added to an empty array by running them through a loop.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>
<p>Type in the following commands to search for the <kbd>.txt</kbd> files within the working directory and print the names of the files found:</p>
</li>
</ol>
<pre style="padding-left: 60px">book_names = sorted(glob.glob("./*.txt"))<br/>print("Found books:")<br/>book_names</pre>
<p style="padding-left: 60px">In our case, there are five books named <kbd>got1</kbd>, <kbd>got2</kbd>, <kbd>got3</kbd>, <kbd>got4</kbd>, and <kbd>got5</kbd> saved in the working directory. </p>
<ol start="2">
<li>Create a <kbd>corpus</kbd>, read each sentence starting with the first file, encode it, and add the encoded characters to a <kbd>corpus</kbd> using the following commands:</li>
</ol>
<pre style="padding-left: 60px">corpus = u''<br/>for book_name in book_names:<br/>print("Reading '{0}'...".format(book_name))<br/>with codecs.open(book_name,"r","Latin1") as book_file:<br/>corpus += book_file.read()<br/>print("Corpus is now {0} characters long".format(len(corpus)))<br/>print()</pre>
<ol start="3">
<li>Execute the code in the preceding steps, which should result in an output that looks like the following screenshot:
<div class="CDPAlignCenter CDPAlign"/>
</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1581 image-border" src="assets/9dc0f8ab-814f-46ce-9efa-8946d4378b2f.png" style="text-align: center;color: black;font-size: 1em;width:44.50em;height:34.67em;"/></div>
<ol start="4">
<li>Load the English pickle <kbd>tokenizer</kbd> from <kbd>punkt</kbd> using the following command:</li>
</ol>
<pre style="padding-left: 60px">tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')</pre>
<ol start="5">
<li><kbd>Tokenize</kbd> the entire <kbd>corpus</kbd> into sentences using the following command:</li>
</ol>
<pre style="padding-left: 60px">raw_sentences = tokenizer.tokenize(corpus)</pre>
<ol start="6">
<li>Define the function to split sentences into their constituent words as well as remove unnecessary characters in the following manner:</li>
</ol>
<pre style="padding-left: 60px">def sentence_to_wordlist(raw):<br/>     clean = re.sub("[^a-zA-Z]"," ", raw)<br/>     words = clean.split()<br/>     return words</pre>
<ol start="7">
<li>Add all the raw sentences where each word of the sentence is tokenized to a new array of sentences. This is done by using the following code:</li>
</ol>
<pre style="padding-left: 60px">sentences = []<br/>for raw_sentence in raw_sentences:<br/>  if len(raw_sentence) &gt; 0:<br/>  sentences.append(sentence_to_wordlist(raw_sentence))</pre>
<ol start="8">
<li>Print a random sentence from the corpus to visually see how the <kbd>tokenizer</kbd> splits sentences and creates a word list from the result. This is done using the following commands:</li>
</ol>
<pre style="padding-left: 60px">print(raw_sentences[50])<br/>print(sentence_to_wordlist(raw_sentences[50]))</pre>
<ol start="9">
<li>Count the total tokens from the dataset using the following commands:</li>
</ol>
<pre style="padding-left: 60px">token_count = sum([len(sentence) for sentence in sentences])<br/>print("The book corpus contains {0:,} tokens".format(token_count))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Executing the tokenizer and tokenizing all the sentences in the corpus should result in an output that looks like the one in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1582 image-border" src="assets/1257f121-c354-454a-99af-54282e394815.png" style="width:36.67em;height:6.83em;"/></div>
<p>Next, removing unnecessary characters, such as hyphens and special characters, are done in the following manner. Splitting up all the sentences using the user-defined <kbd>sentence_to_wordlist()</kbd> function produces an output as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1583 image-border" src="assets/18472244-abe7-4029-a5aa-3ab539a44964.png" style="width:71.67em;height:12.33em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign CDPAlignCenter">Adding the raw sentences to a new array named <kbd>sentences[]</kbd> produces an output as shown in the following screenshot:<br/>
<img class="alignnone size-full wp-image-1584 image-border" src="assets/c7d44661-9a51-4107-8661-8c09e6610aa4.png" style="width:90.42em;height:19.08em;"/></p>
<p>On printing the total number of tokens in the corpus, we notice that there are 1,110,288 tokens in the entire corpus. This is illustrated in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1585 image-border" src="assets/a535b630-52be-4d91-8b31-bf9626571f4d.png" style="width:46.33em;height:7.17em;"/></div>
<p>The functionality is as follows:</p>
<ol>
<li>The pre-trained <kbd>tokenizer</kbd> from NLTK is used to tokenize the entire corpus by counting each sentence as a token. Every tokenized sentence is added to the variable <kbd>raw_sentences</kbd>, which stores the tokenized sentences.</li>
<li>In the next step, common stopwords are removed, and the text is cleaned by splitting each sentence into its words.</li>
<li>A random sentence along with its wordlist is printed to understand how this works. In our case, we have chosen to print the 50th sentence in the <kbd>raw_sentences</kbd> array.</li>
<li>The total number of tokens (in our case, sentences) in the sentences array are counted and printed. In our case, we see that 1,110,288 tokens are created by the <kbd>tokenizer</kbd>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span><span>More information about tokenizing paragraphs and sentences can be found by visiting the following links:<br/></span></span></p>
<ul>
<li><a href="https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize">https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize</a></li>
<li><a href="https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk">https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk</a></li>
<li><a href="https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/">https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>For more information about how regular expressions work, visit the following link:</p>
<p><a href="https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python">https://stackoverflow.com/questions/13090806/clean-line-of-punctuation-and-split-into-words-python</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and training the model</h1>
                </header>
            
            <article>
                
<p>Once we have the text data in the form of tokens in an array, we are able to input it in the array format to the model. First, we have to define a number of hyperparameters for the model. This section will describe how to do the following:</p>
<ul>
<li>Declare model hyperparameters</li>
<li>Build a model using <kbd>Word2Vec</kbd></li>
<li>Train the model on the prepared dataset</li>
<li>Save and checkpoint the trained model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Some of the model hyperparameters that are to be declared include the following:</p>
<ul>
<li>Dimensionality of resulting word vectors</li>
<li>Minimum word count threshold</li>
<li>Number of parallel threads to run while training the model</li>
<li>Context window length</li>
<li>Downsampling (for frequently occurring words)</li>
<li>Setting a seed</li>
</ul>
<p>Once the previously mentioned hyperparameters are declared, the model can be built using the <kbd>Word2Vec</kbd> function from the <kbd>Gensim</kbd> library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>
<p>Declare the hyperparameters for the model using the following commands:</p>
</li>
</ol>
<pre style="padding-left: 60px">num_features = 300<br/>min_word_count = 3<br/>num_workers = multiprocessing.cpu_count()<br/>context_size = 7<br/>downsampling = 1e-3<br/>seed = 1</pre>
<ol start="2">
<li>Build the model, using the declared hyperparameters, with the following lines of code:</li>
</ol>
<pre style="padding-left: 60px">got2vec = w2v.Word2Vec(<br/>    sg=1,<br/>    seed=seed,<br/>    workers=num_workers,<br/>    size=num_features,<br/>    min_count=min_word_count,<br/>    window=context_size,<br/>    sample=downsampling<br/>)</pre>
<ol start="3">
<li>Build the model's vocabulary using the tokenized sentences and iterating through all the tokens. This is done using the <kbd>build_vocab</kbd> function in the following manner:</li>
</ol>
<pre style="padding-left: 60px">got2vec.build_vocab(sentences,progress_per=10000, keep_raw_vocab=False, trim_rule=None)</pre>
<ol start="4">
<li>Train the model using the following command:</li>
</ol>
<pre style="padding-left: 60px">got2vec.train(sentences, total_examples=got2vec.corpus_count, total_words=None, epochs=got2vec.iter, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False)</pre>
<ol start="5">
<li>Create a directory named trained, if it doesn't already exist. Save and checkpoint the <kbd>trained</kbd> model using the following commands:</li>
</ol>
<pre style="padding-left: 60px">if not os.path.exists("trained"):<br/>     os.makedirs("trained")<br/>got2vec.wv.save(os.path.join("trained", "got2vec.w2v"), ignore=[])</pre>
<ol start="6">
<li>To load the saved model at any point, use the following command:</li>
</ol>
<pre style="padding-left: 60px">got2vec = w2v.KeyedVectors.load(os.path.join("trained", "got2vec.w2v"))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>The declaration of model parameters does not produce any output. It just makes space in the memory to store variables as model parameters. The following screenshot describes this process:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1586 image-border" src="assets/b01b9b42-aaa9-41a6-91dd-550faff73ec6.png" style="width:33.08em;height:19.83em;"/></div>
</li>
</ol>
<ol start="2">
<li>The model is built using the preceding hyperparameters. In our case, we have named the model <kbd>got2vec</kbd> ,but the model may be named as per your liking. The model definition is illustrated in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1589 image-border" src="assets/abe1bb95-6a89-45e2-8060-229f2759473c.png" style="width:25.00em;height:9.25em;"/></div>
</li>
<li>Running the <kbd>build_vocab</kbd> command on the model should produce an output as seen in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1592 image-border" src="assets/54a633e8-d76c-42a8-b40c-aea8e440a79d.png" style="width:87.42em;height:29.67em;"/></div>
</li>
<li>Training the model is done by defining the parameters as seen in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1588 image-border" src="assets/a6116087-5efc-474e-9e00-69289485ade7.png" style="width:61.83em;height:8.42em;"/></div>
</li>
</ol>
<ol start="5">
<li>The above command produces an output as shown in the following screenshot:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1591 image-border" src="assets/9d7dd945-1ed2-497f-9cd1-2e7daa05b4c7.png" style="text-align: center;color: black;font-size: 1em;width:91.17em;height:47.92em;"/></div>
<ol start="6">
<li>The commands to save, checkpoint, and load the model produce the following output, as shown in the screenshot:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1593 image-border" src="assets/12a49260-afaf-484f-925c-cd73e1682646.png" style="text-align: center;color: black;font-size: 1em;width:79.83em;height:22.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li>In our case, we notice the <kbd>build_vocab</kbd> function identifies 23,960 different word types from a list of 1,110,288 words. However, this number will vary for different text corpora.</li>
<li>Each word is represented by a 300-dimensional vector since we have declared the dimensionality to be 300. Increasing this number increases the training time of the model but also makes sure the model generalizes easily to new data.</li>
<li>The downsampling rate of 1e<img class="fm-editor-equation" src="assets/4b993d4b-623a-44ac-8980-51f3d1ea5c11.png" style="width:1.17em;height:1.25em;"/>3 is found to be a good rate. This is specified to let the model know when to downsample frequently occurring words, as they are not of much importance when it comes to analysis. Examples of such words are this, that, those, them, and so on.</li>
<li>A seed is set to make results reproducible. Setting a seed also makes debugging a lot easier.</li>
<li>Training the model takes about 30 seconds using regular CPU computing since the model is not very complex.</li>
<li>The model, when check-pointed, is saved under the <kbd>trained</kbd> folder inside the working directory.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>For more information on <kbd>Word2Vec</kbd> models and the Gensim library, visit the following link:</p>
<p><a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing further </h1>
                </header>
            
            <article>
                
<p>This section will describe how to squash the dimensionality of all the trained words and put it all into one giant matrix for visualization purposes. Since each word is a 300-dimensional vector, it needs to be brought down to a lower dimension for us to visualize it in a 2D space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Once the model is saved and checkpointed after training, begin by loading it into memory, as you did in the previous section. The libraries and modules that will be utilized in this section are: </p>
<ul>
<li><kbd>tSNE</kbd></li>
<li><kbd>pandas</kbd></li>
<li><kbd>Seaborn</kbd></li>
<li><kbd>numpy</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>Squash the dimensionality of the 300-dimensional word vectors by using the following command:</li>
</ol>
<pre style="padding-left: 60px"> tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)</pre>
<ol start="2">
<li>Put all the word vectors into one giant matrix (named <kbd>all_word_vectors_matrix</kbd>), and view it using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> all_word_vectors_matrix = got2vec.wv.syn0<br/> print (all_word_vectors_matrix)</pre>
<ol start="3">
<li>Use the <kbd>tsne</kbd> technique to fit all the learned representations into a two- dimensional space using the following command:</li>
</ol>
<pre style="padding-left: 60px"> all_word_vectors_matrix_2d =  tsne.fit_transform(all_word_vectors_matrix)</pre>
<ol start="4">
<li>Gather all the word vectors, as well as their associated words, using the following code:</li>
</ol>
<pre style="padding-left: 60px"> points = pd.DataFrame(<br/>     [<br/>            (word, coords[0], coords[1])<br/>             for word, coords in [<br/>              (word, all_word_vectors_matrix_2d[got2vec.vocab[word].index])<br/>                for word in got2vec.vocab<br/>         ]<br/>    ],<br/>    columns=["word", "x", "y"]<br/>)</pre>
<ol start="5">
<li>The <kbd>X</kbd> and <kbd>Y</kbd> coordinates and associated words of the first ten points can be obtained using the following command:</li>
</ol>
<pre style="padding-left: 60px">points.head(10)</pre>
<ol start="6">
<li>Plot all the points using the following commands:</li>
</ol>
<pre style="padding-left: 60px">sns.set_context("poster")<br/>points.plot.scatter("x", "y", s=10, figsize=(15, 15))</pre>
<ol start="7">
<li>A selected region of the plotted graph can be zoomed into for a closer inspection. Do this by slicing the original data using the following function:</li>
</ol>
<pre style="padding-left: 60px">def plot_region(x_bounds, y_bounds):<br/>    slice = points[<br/>        (x_bounds[0] &lt;= points.x) &amp;<br/>        (points.x &lt;= x_bounds[1]) &amp;<br/>        (y_bounds[0] &lt;= points.y) &amp;<br/>        (points.y &lt;= y_bounds[1])<br/>        ]<br/>    ax = slice.plot.scatter("x", "y", s=35, figsize=(10, 8))<br/>        for i, point in slice.iterrows():<br/>            ax.text(point.x + 0.005, point.y + 0.005, point.word,                                                  fontsize=11)</pre>
<ol start="8">
<li>Plot the sliced data using the following command. The sliced data can be visualized as a zoomed-in region of the original plot of all data points:</li>
</ol>
<pre style="padding-left: 60px">plot_region(x_bounds=(20.0, 25.0), y_bounds=(15.5, 20.0))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>The t-SNE algorithm is a non-linear dimensionality reduction technique. Computers are easily able to interpret and process many dimensions during their computations. However, humans are only capable of visualizing two or three dimensions at a time. Therefore, these dimensionality reduction techniques come in very handy when trying to draw insights from data.</li>
</ol>
<p> </p>
<ol start="2">
<li>On applying t-SNE to the 300-dimensional vectors, we are able to squash it into just two dimensions to plot it and view it.</li>
<li>By specifying <kbd>n_components</kbd> as 2, we let the algorithm know that it has to squash the data into a two-dimensional space. Once this is done, we add all the squashed vectors into one giant matrix named <kbd>all_word_vectors_matrix</kbd>, which is illustrated in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1594 image-border" src="assets/325c8759-34c8-4a40-ad78-7e050945cfe2.png" style="width:33.58em;height:19.58em;"/></div>
</li>
<li>The t-SNE algorithm needs to be trained on all these word vectors. The training takes about five minutes on a regular CPU.</li>
<li>Once the t-SNE is finished training on all the word vectors, it outputs 2D vectors for each word. These vectors may be plotted as points by converting all of them into a data frame. This is done as shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1595 image-border" src="assets/e3554859-c59a-4361-929c-5aac54dcf654.png" style="width:34.58em;height:12.50em;"/></div>
</li>
</ol>
<ol start="6">
<li>We see that the preceding code produces a number of points where each point represents a <span class="packt_screen">word</span> along with its <span class="packt_screen">X</span> and <span class="packt_screen">Y</span> coordinates. On inspection of the first twenty points of the data frame, we see an output as illustrated in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1596 image-border" src="assets/a3db5c42-0c48-4990-a657-2a8117e6df71.png" style="width:26.83em;height:41.17em;"/></div>
<ol start="7">
<li>On plotting all the points using the <kbd>all_word_vectors_2D</kbd> variable, you should see an output that looks similar to the one in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1597 image-border" src="assets/82c9e244-106f-4c01-825f-dbb00d5bd297.png" style="width:51.17em;height:8.17em;"/></div>
</li>
<li>The above command will produce a plot of all tokens or words generated from the entire text as shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1598 image-border" src="assets/7f14da02-37c8-42f0-a62c-019961507c8f.png" style="width:42.83em;height:35.58em;"/></div>
</li>
</ol>
<ol start="9">
<li>We can use the <kbd>plot_region</kbd> function to zoom into a certain area of the plot so that we are able to actually see the words, along with their coordinates. This step is illustrated in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1599 image-border" src="assets/3c916ae2-d6a1-4d0b-b069-5a2a880562af.png" style="width:39.00em;height:11.08em;"/></div>
</li>
</ol>
<ol start="10">
<li>An enlarged or zoomed in area of the plot can be visualized by setting the <kbd>x_bounds</kbd> and <kbd>y_bounds</kbd>, values as shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1600 image-border" src="assets/a0214205-14a3-4165-ae29-22535d292dbb.png" style="width:62.92em;height:45.42em;"/></div>
</li>
</ol>
<ol start="11">
<li>A different region of the same plot can be visualized by varying the <kbd>x_bounds</kbd> and <kbd>y_bounds</kbd> values as shown in the following two screenshots:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1601 image-border" src="assets/ab6375bb-8bea-4697-8109-3e05c12cee31.png" style="width:64.67em;height:45.42em;"/></div>
</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1602 image-border" src="assets/1041a93a-3c8e-42f2-a001-c0dea26cbc11.png" style="text-align: center;color: black;font-size: 1em;width:63.50em;height:45.33em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The following additional points are of note:</p>
<ul>
<li>For more information on how the t-SNE algorithm works, visit the following link:</li>
<li class="mce-root CDPAlignLeft CDPAlign"><a href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm">https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm</a></li>
<li>More information about cosine distance similarity and ranking can be found by visiting the following link:<br/>
<a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></li>
<li>Use the following link to explore the different functions of the <kbd>Seaborn</kbd> library:<br/>
<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing further</h1>
                </header>
            
            <article>
                
<p>This section will describe further analysis that can be performed on the data after visualization. For example, exploring cosine distance similarity between different word vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The following link is a great blog on how cosine distance similarity works and also discusses some of the math involved:</p>
<p><a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li>Various<span> </span>natural-language processing tasks can be performed using the different functions of<span> </span><kbd>Word2Vec</kbd>. One of them is finding the most semantically similar words given a certain word (that is, word vectors that have a high cosine similarity or a short Euclidean distance between them). This can be done by using the<span> </span><kbd>most_similar</kbd> function form<span> </span><kbd>Word2Vec</kbd>, as shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1603 image-border" src="assets/9ac6fa2d-4380-4100-80be-ec37006e6efd.png" style="width:45.33em;height:15.50em;"/></div>
This screenshots  all the closest words related to the word <kbd>Lannister</kbd>:<br/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1604 image-border" src="assets/30dd7d1c-bbed-4cef-9a87-377ff69d022a.png" style="width:29.00em;height:14.67em;"/></div>
This screenshot shows a list of all the words related to word<span> </span><kbd>Jon</kbd>:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1605 image-border" src="assets/ec9b90eb-4a33-49a9-ac48-75b56b4f65e6.png" style="width:26.58em;height:15.08em;"/></div>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li>There are various methods to measure the semantic similarity between words. The one we are using in this section is based on cosine similarity. We can also explore linear relationships between words by using the following lines of code:</li>
</ul>
<pre style="padding-left: 60px"> def nearest_similarity_cosmul(start1, end1, end2):<br/>    similarities = got2vec.most_similar_cosmul(<br/>        positive=[end2, start1],<br/>        negative=[end1]<br/>)<br/>start2 = similarities[0][0]<br/>print("{start1} is related to {end1}, as {start2} is related to         {end2}".format(**locals()))<br/>return start2</pre>
<ul>
<li>To find the cosine similarity of nearest words to a given set of words, use the following commands:</li>
</ul>
<pre style="padding-left: 60px">nearest_similarity_cosmul("Stark", "Winterfell", "Riverrun")<br/>nearest_similarity_cosmul("Jaime", "sword", "wine")<br/>nearest_similarity_cosmul("Arya", "Nymeria", "dragons")</pre>
<ul>
<li>The preceding process is illustrated in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1606 image-border" src="assets/09338881-485f-4402-945c-c4ff5abc61b5.png" style="width:76.83em;height:15.67em;"/></div>
</li>
<li><span>The results are as follows:</span>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1607 image-border" src="assets/0af641a8-cf30-4c33-8994-300ef137b545.png" style="width:40.83em;height:8.33em;"/></div>
</li>
<li>As seen in this section, word vectors form the basis of all NLP tasks. It is important to understand them and the math that goes into building these models before diving into more complicated NLP models such as<span> </span><strong>recurrent neural networks</strong><span> </span>and<span> </span><strong>Long Short-Term Memory</strong><span> </span>(<strong>LSTM</strong>) cells.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Further reading can be undertaken for a better understanding of the use of cosine distance similarity, clustering and other machine learning techniques used in ranking word vectors. Provided below are a few links to useful published papers on this topic:</p>
<ul>
<li><a href="https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1530163881&amp;Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&amp;response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf">https://s3.amazonaws.com/academia.edu.documents/32952068/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1530163881&amp;Signature=YG6YjvJb2z0JjmfHzaYujA2ioIo%3D&amp;response-content-disposition=inline%3B%20filename%3DSimilarity_Measures_for_Text_Document_Cl.pdf</a></li>
<li><a href="http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf">http://csis.pace.edu/ctappert/dps/d861-12/session4-p2.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>