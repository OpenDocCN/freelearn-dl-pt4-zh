<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer021">
			<h1 id="_idParaDest-180" class="chapter-number"><a id="_idTextAnchor259"/>4</h1>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor260"/>Forecasting with PyTorch Lightning</h1>
			<p>In this chapter, we’ll build forecasting models using PyTorch Lightning. We’ll touch on several aspects of this framework, such as creating a data module to handle data preprocessing or creating a <strong class="source-inline">LightningModel</strong> structure that encapsulates the training process <a id="_idIndexMarker194"/>of neural networks. We’ll also explore <strong class="bold">TensorBoard</strong> to monitor the training process of neural networks<a id="_idTextAnchor261"/><a id="_idTextAnchor262"/>. Then, we’ll describe a <a id="_idIndexMarker195"/>few metrics for evaluating deep <a id="_idIndexMarker196"/>neural networks for forecasting, such as <strong class="bold">Mean Absolute Scaled Error</strong> (<strong class="bold">MASE</strong>) and <strong class="bold">Symmetric Mean Absolute Percentage Error</strong> (<strong class="bold">SMAPE</strong>). In this chapter, we’ll focus on multivariate time series, which contain more than <span class="No-Break">one variable.</span></p>
			<p>This chapter will guide you through the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Preparing a multivariate time series for <span class="No-Break">supervised learning</span></li>
				<li>Training a linear regression model for forecasting with a multivariate <span class="No-Break">time series</span></li>
				<li>Feedforward neural networks for multivariate time <span class="No-Break">series forecasting</span></li>
				<li>LSTM neural networks for multivariate time <span class="No-Break">series forecasting</span></li>
				<li>Evaluating deep neural networks <span class="No-Break">for forecasting</span></li>
				<li>Monitoring the training process <span class="No-Break">using Tensorboard</span></li>
				<li>Using callbacks – <span class="No-Break"><strong class="source-inline">EarlyStopping</strong></span></li>
			</ul>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor263"/>Technical requirements</h1>
			<p>In this chapter, we’ll leverage the following Python libraries, all of which you can install <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li>PyTorch <span class="No-Break">Lightning (2.1.4)</span></li>
				<li>PyTorch <span class="No-Break">Forecasting (1.0.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> (2.2.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">ray</strong></span><span class="No-Break"> (2.9.2)</span></li>
				<li><span class="No-Break"><strong class="source-inline">numpy</strong></span><span class="No-Break"> (1.26.3)</span></li>
				<li><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> (2.1.4)</span></li>
				<li><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break"> (1.4.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">sktime</strong></span><span class="No-Break"> (0.26.0)</span></li>
			</ul>
			<p>The code for this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor264"/>Preparing a multivariate time series for supervised learning</h1>
			<p>The first recipe of this chapter addresses the problem of preparing a multivariate time <a id="_idIndexMarker197"/>series for supervised <a id="_idIndexMarker198"/>learning. We’ll show how the sliding window method we used in the previous chapter can be extended to solve this task. Then, we’ll demonstrate how to prepare a time series using <strong class="source-inline">TimeSeriesDataSet</strong>, a PyTorch Forecasting class that handles the preprocessing steps of <span class="No-Break">time seri<a id="_idTextAnchor265"/>es.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor266"/>Getting ready</h2>
			<p>We’ll use the same time series we analyzed in <a href="B21145_01.xhtml#_idTextAnchor019"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. We’ll need to load the dataset with <strong class="source-inline">pandas</strong> using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import pandas as pd
data = pd.read_csv('assets/daily_multivariate_timeseries.csv',
                   parse_dates=['Datetime'],
                   index_col='Datetime')</pre>			<p>The following figure shows a sample of the time series. Please note that the axes have been transposed for <span class="No-Break">visualization purposes:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B21145_04_001.jpg" alt="Figure 4.1: Sample of a multivariate time series. The variables of the series are shown on the x axis for visualization purposes" width="1650" height="631"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: Sample of a multivariate time series. The variables of the series are shown on the x axis for visualization purposes</p>
			<p>The <a id="_idIndexMarker199"/>preceding dataset <a id="_idIndexMarker200"/>contains nine variables related to meteorological conditions. As we did in <a href="B21145_03.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, the goal is to forecast the next solar radiation values. We’ll use the lags of the extra available variables as input explanatory variables. In the next chapter, you will learn how to prepare a multivariate time series for cases where you want to forecast <span class="No-Break">several varia<a id="_idTextAnchor267"/>bles.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor268"/>How to do it…</h2>
			<p>We’ll transform a multivariate time series for supervised learning. First, we’ll describe how to do this using the sliding window approach that we used in <a href="B21145_03.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Then, we’ll show how this process can be simplified with the <strong class="source-inline">TimeSeriesDataSet</strong> data structure, which is based <span class="No-Break">on PyTorch.</span></p>
			<h3>Using a sliding window</h3>
			<p>In the <a id="_idIndexMarker201"/>previous chapter, we used a sliding window approach to transform a univariate time series from a sequence into a matrix format. Preparing a multivariate time series for supervised learning requires a similar process: we apply the sliding window technique to each variable and then combine the results. This process can be carried out <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
TARGET = 'Incoming Solar'
N_LAGS = 7
HORIZON = 1
input_data = []
output_data = []
for i in range(N_LAGS, data.shape[0]-HORIZON+1):
    input_data.append(data.iloc[i - N_LAGS:i].values)
    output_data.append(data.iloc[i:(i+HORIZON)][TARGET])
input_data, output_data = np.array(input_data), np.array(output_data)</pre>			<p>The <a id="_idIndexMarker202"/>preceding code follows <span class="No-Break">these steps:</span></p>
			<ol>
				<li>First, we define the number of lags and forecasting horizon. We set the number of lags to <strong class="source-inline">7</strong>  (<strong class="source-inline">N_LAGS=7</strong>), the forecasting horizon to <strong class="source-inline">1</strong> (<strong class="source-inline">HORIZON=1</strong>), and the target variable to <span class="No-Break"><strong class="source-inline">Incoming Solar.</strong></span></li>
				<li>Then, we iterate over each time step in the multivariate time series. At each point, we retrieve the previous <strong class="source-inline">N_LAGS</strong>, add these to the <strong class="source-inline">input_data</strong>, and add the next value of solar radiation to the output data. This means we’ll use the past <strong class="source-inline">7</strong> values of each variable to forecast the next value of <span class="No-Break">solar radiation.</span></li>
				<li>Finally, we transform the input and output data from a <strong class="source-inline">Python</strong> list into a <strong class="source-inline">NumPy</strong> <span class="No-Break"><strong class="source-inline">array</strong></span><span class="No-Break"> structure.</span></li>
			</ol>
			<p><strong class="source-inline">output_data</strong> is a one-dimensional vector that represents the future value of solar radiation. <strong class="source-inline">input_data</strong> has 3 dimensions: the first dimension refers to the number of samples, the second is the number of lags, and the third is the number of variables in <span class="No-Break">the series.</span></p>
			<h3>Using the TimeSeriesDataSet class</h3>
			<p>So far, we’ve been using a sliding window method to preprocess time series for supervised learning. This function and other preprocessing tasks that are required for training <a id="_idIndexMarker203"/>a neural network are automated by the <strong class="source-inline">TimeSeriesDataSet</strong> class, which is available in the PyTorch <span class="No-Break">Forecasting library.</span></p>
			<p><strong class="source-inline">TimeSeriesDataSet</strong> provides a simple and useful way of preparing and passing data to models. Let’s see how this structure can be used to handle multivariate time series. First, we need to shape the time series in a pandas DataFrame structure with three main pieces <span class="No-Break">of information:</span></p>
			<ul>
				<li><strong class="source-inline">group_id</strong>: A column that identifies the name of a time series. If the dataset contains a single time series, this column will show a constant value.  Some datasets involve multiple time series that can be distinguished by <span class="No-Break">this variable.</span></li>
				<li><strong class="source-inline">time_index</strong>: This stores the time at which a value is captured by a <span class="No-Break">given series.</span></li>
				<li><strong class="bold">Other variables</strong>:  Extra variables that store the value of the time series. A multivariate time series contains <span class="No-Break">several variables.</span></li>
			</ul>
			<p>Our time series already contains several variables. Now, we need to add information about <strong class="source-inline">time_index</strong> and <strong class="source-inline">group_id</strong>, which can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
mvtseries['group_id'] = 0
mvtseries['time_index'] = np.arange(mvtseries.shape[0])</pre>			<p>The value of <strong class="source-inline">group_id</strong> is constantly <strong class="source-inline">0</strong> since we’re working with a single time series. We use <strong class="source-inline">0</strong> arbitrarily. You can use any name that suits you. We use the <strong class="source-inline">np.arange</strong><strong class="source-inline">()</strong> function to create this time series’ <strong class="source-inline">time_index</strong>. This creates a variable that gives <strong class="source-inline">0</strong> for the first observation, <strong class="source-inline">1</strong> for the second observation, and <span class="No-Break">so on.</span></p>
			<p>Then, we must create an instance of the <strong class="source-inline">TimeSeriesDataSet</strong> class, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
dataset = TimeSeriesDataSet(
    data=mvtseries,
    group_ids=["group_id"],
    target="Incoming Solar",
    time_idx="time_index",
    max_encoder_length=7,
    max_prediction_length=1,
    time_varying_unknown_reals=['Incoming Solar',
                                'Wind Dir',
                                'Snow Depth',
                                'Wind Speed',
                                'Dewpoint',
                                'Precipitation',
                                'Vapor Pressure',
                                'Relative Humidity',
                                'Air Temp'],
)</pre>			<p>We can <a id="_idIndexMarker204"/>transform a <strong class="source-inline">TimeSeriesDataSet</strong> dataset into a <strong class="source-inline">DataLoader</strong> class <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
data_loader = dataset.to_dataloader(batch_size=1, shuffle=False)</pre>			<p><strong class="source-inline">DataLoader</strong> is used to pass observations to a model. Here’s an example of what an observation <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
x, y = next(iter(data_loader))
x['encoder_cont']
y</pre>			<p>We use the <strong class="source-inline">next</strong><strong class="source-inline">()</strong> and <strong class="source-inline">iter</strong><strong class="source-inline">()</strong> methods to get an observation from the data loader. This observation is stored as <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>, which represent the input and output data, respectively. The main input is the <strong class="source-inline">encoder_cont</strong> item, which denotes the <strong class="source-inline">7</strong> lags of each variable. This data is a PyTorch tensor with the shape (<strong class="source-inline">1</strong>, <strong class="source-inline">7</strong>, <strong class="source-inline">9</strong>) representing (batch size, number of lags, number of variables). The batch size is a parameter that represents the number of samples used in each training iteration of a neural network. The output data is a float that represents the next value of the solar <span class="No-Break">radiation variable.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor269"/>How it works…</h2>
			<p>The <strong class="source-inline">TimeSeriesDataSet</strong> constructor <a id="_idIndexMarker205"/>requires a <span class="No-Break">few parameters:</span></p>
			<ul>
				<li><strong class="source-inline">data</strong>: A time series dataset with the three elements <span class="No-Break">described earlier</span></li>
				<li><strong class="source-inline">group_ids</strong>: The column in <strong class="source-inline">data</strong> that identifies each time series in <span class="No-Break">the dataset</span></li>
				<li><strong class="source-inline">target</strong>: The column in <strong class="source-inline">data</strong> that we want to forecast (<span class="No-Break">target variable)</span></li>
				<li><strong class="source-inline">time_idx</strong>: The column in <strong class="source-inline">data</strong> that contains the time information of <span class="No-Break">each observation</span></li>
				<li><strong class="source-inline">max_encoder_length</strong>: The number of lags used to build the <span class="No-Break">auto-regressive model</span></li>
				<li><strong class="source-inline">max_prediction_length</strong>: The forecasting horizon – that is, how many future time steps should <span class="No-Break">be predicted</span></li>
				<li><strong class="source-inline">time_varying_unknown_reals</strong>: A list of columns in <strong class="source-inline">data</strong> that describes which numeric variables vary <span class="No-Break">over time</span></li>
			</ul>
			<p>There are other parameters related to <strong class="source-inline">time_varying_unknown_reals</strong>. This particular input details all numeric observations whose future is unknown to the user, such as the variables we want to forecast. Yet, in some cases, we know the future value of an observation, such as the price of a product. This type of variable should be included in the <strong class="source-inline">time_varying_known_reals</strong> input. There are also the <strong class="source-inline">time_varying_known_categoricals</strong> and <strong class="source-inline">time_varying_unknown_categoricals</strong> inputs, which can be used for categorical variables instead of <span class="No-Break">numeric ones.</span></p>
			<p>Regarding <a id="_idIndexMarker206"/>the forecasting task, the transformation we carried out in this recipe is the basis of a type of modeling called <strong class="bold">Auto-Regressive Distributed Lags</strong> (<strong class="bold">ARDL</strong>). ARDL is an extension of auto-regression that also includes the lags of exogenous variables<a id="_idTextAnchor270"/><a id="_idTextAnchor271"/> <span class="No-Break">as input.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor272"/>Training a linear regression model for forecasting with a multivariate time series</h1>
			<p>In <a id="_idIndexMarker207"/>this recipe, we’ll use PyTorch to train a linear regression model as our first forecasting <a id="_idIndexMarker208"/>model fit on a multivariate time series. We’ll show you how to use <strong class="source-inline">TimeSeriesDataSet</strong> to handle the preprocessing steps for training the model and passi<a id="_idTextAnchor273"/>ng data <span class="No-Break">to it.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor274"/>Getting ready</h2>
			<p>We’ll start this recipe with the <strong class="source-inline">mvtseries</strong> dataset that we used in the <span class="No-Break">previous recipe:</span></p>
			<pre class="source-code">
import pandas as pd
mvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',
            parse_dates=['datetime'],
            index_col='datetime')</pre>			<p>Now, let’s see how we can use this dataset to train a <span class="No-Break">P<a id="_idTextAnchor275"/>yTorch model.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor276"/>How to do it…</h2>
			<p>In the following code, we’ll describe the necessary steps to prepare the time series and build a linear <span class="No-Break">regression model:</span></p>
			<ol>
				<li>We start by preprocessing the time series. This includes creating the group identifier and time <span class="No-Break">index columns:</span><pre class="source-code">
mvtseries["target"] = mvtseries["Incoming Solar"]
mvtseries["time_index"] = np.arange(mvtseries.shape[0])
mvtseries["group_id"] = 0</pre></li>				<li>Then, we <a id="_idIndexMarker209"/>must split the data into different partitions. For this <a id="_idIndexMarker210"/>recipe, we’ll only keep the <span class="No-Break">training indices:</span><pre class="source-code">
time_indices = data["time_index"].values
train_indices, _ = train_test_split(
    time_indices,
    test_size=test_size,
    shuffle=False)
train_indices, _ = train_test_split(train_indices,
                                    test_size=0.1,
                                    shuffle=False)
train_df = data.loc[data["time_index"].isin(train_indices)]
 train_df_mod = train_df.copy()</pre></li>				<li>Then, we must standardize the time series using the <span class="No-Break"><strong class="source-inline">StandardScaler</strong></span><span class="No-Break"> operator:</span><pre class="source-code">
target_scaler = StandardScaler()
target_scaler.fit(train_df_mod[["target"]])
train_df_mod["target"] = target_scaler.transform
    (train_df_mod[["target"]])
train_df_mod = train_df_mod.drop("Incoming Solar", axis=1)
 feature_names = [
    col for col in data.columns
    if col != "target" and col != "Incoming Solar"
]</pre></li>				<li>The <a id="_idIndexMarker211"/>preprocessed <a id="_idIndexMarker212"/>time series is passed onto a <span class="No-Break"><strong class="source-inline">TimeSeriesDataSet</strong></span><span class="No-Break"> instance:</span><pre class="source-code">
training_dataset = TimeSeriesDataSet(
    train_df_mod,
    time_idx="time_index",
    target="target",
    group_ids=["group_id"],
    max_encoder_length=n_lags,
    max_prediction_length=horizon,
    time_varying_unknown_reals=feature_names,
    scalers={name: StandardScaler()
             for name in feature_names},
)
loader = training_dataset.to_dataloader(batch_size=batch_size,
                                        shuffle=False)</pre><p class="list-inset">The <strong class="source-inline">TimeSeriesDataSet</strong> object is transformed into a data loader that can be used to pass batches of samples to a model. This is done using the  <strong class="source-inline">to_dataloader()</strong> method. We encapsulate all these data preparation steps into a single function called <strong class="source-inline">create_training_set</strong>. You can check out the function’s source in this book’s <span class="No-Break">GitHub repository.</span></p></li>				<li>Next, we <a id="_idIndexMarker213"/>call the <strong class="source-inline">create_training_set</strong><strong class="source-inline">()</strong> function to create <a id="_idIndexMarker214"/>the <span class="No-Break">training dataset:</span><pre class="source-code">
N_LAGS = 7
HORIZON = 1
BATCH_SIZE = 10
data_loader = create_training_set(
    data=mvtseries,
    n_lags=N_LAGS,
    horizon=HORIZON,
    batch_size=BATCH_SIZE,
    test_size=0.3
)</pre></li>				<li>Then, we must define the linear regression model using PyTorch, <span class="No-Break">as follows:</span><pre class="source-code">
import torch
from torch import nn
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
    def forward(self, X):
        X = X.view(X.size(0), -1)
        return self.linear(X)</pre><p class="list-inset">Here, we define a class called <strong class="source-inline">LinearRegressionModel</strong> that implements the <a id="_idIndexMarker215"/>multiple linear regression model. It contains a single linear transformation layer (<strong class="source-inline">nn.Linear</strong>). This class takes the input and output sizes <a id="_idIndexMarker216"/>as input, which are the second dimension of the <strong class="source-inline">train_input</strong> and <strong class="source-inline">train_output</strong> objects, respectively. We created the model by calling the class with <span class="No-Break">these parameters.</span></p></li>				<li>Now, we will create an instance of this model, <span class="No-Break">as follows:</span><pre class="source-code">
num_vars = mvtseries.shape[1] + 1
model = LinearRegressionModel(N_LAGS * num_vars, HORIZON)</pre><p class="list-inset"><strong class="source-inline">num_vars</strong> contains the number of variables in the time series. Then, we define the input of the model to <strong class="source-inline">num_vars</strong> times <strong class="source-inline">N_LAGS</strong> and the output to the <span class="No-Break">forecasting horizon.</span></p></li>				<li>We can perform the training process using the <span class="No-Break">following code:</span><pre class="source-code">
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10
for epoch in range(num_epochs):
    for batch in data_loader:
        x, y = batch
        X = x["encoder_cont"].squeeze(-1)
        y_pred = model(X)
        y_pred = y_pred.squeeze(1)
        y_actual = y[0].squeeze(1)
        loss = criterion(y_pred, y_actual)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    print(f"epoch: {epoch + 1}, loss = {loss.item():.4f}")</pre><p class="list-inset">Here, we set the learning rate to <strong class="source-inline">0.001</strong> and the optimizer to Adam. Adam is a popular <a id="_idIndexMarker217"/>alternative to approaches such as SGD that has better <span class="No-Break">converge characteristics.</span></p><p class="list-inset">At each <a id="_idIndexMarker218"/>training epoch, we get the lags of each batch from the data loader and process them with the model. Note that each batch is reshaped into a two-dimensional format that is required by a linear model. This is done in the <strong class="source-inline">forward()</strong> method of the <span class="No-Break"><strong class="source-inline">Linea<a id="_idTextAnchor277"/>rRegressionModel</strong></span><span class="No-Break"> class.</span></p></li>			</ol>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor278"/>How it works…</h2>
			<p>We used the <strong class="source-inline">TimeSeriesDataSet</strong> class to handle the data preparation process for us. Then, we converted the dataset into a <strong class="source-inline">DataLoader</strong> class using the <strong class="source-inline">to_dataloader()</strong> method. This data loader provides batches of data to the model. Although we did not define it explicitly, each batch follows an autoregressive way of modeling. The input is based on the past few observations of the time series, and the output represents the <span class="No-Break">future observations.</span></p>
			<p>We implement the linear regression model as a class so that it follows the same structure as in the previous chapter. We could create the model with <strong class="source-inline">model = nn.Linear(input_size, output_size)</strong> <span class="No-Break">for simplicity.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor279"/>Feedforward neural networks for multivariate time series forecasting</h1>
			<p>In this recipe, we’ll return our attention to deep neural networks. We’ll show you how <a id="_idIndexMarker219"/>to build a forecasting model for multivariate time series using a deep feedforward neural <a id="_idIndexMarker220"/>network. We’ll describe how to couple the <strong class="source-inline">DataModule</strong> class with <strong class="source-inline">TimeSeriesDataSet</strong> to encapsulate the data preprocessing steps. We’ll also place the <strong class="source-inline">PyTorch</strong> models within a <strong class="source-inline">LightningModule</strong> structure, which standardizes the training process of <span class="No-Break">neural networks.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor280"/>Getting ready</h2>
			<p>We’ll continue to use the multivariate time series related to solar <span class="No-Break">radiation forecasting:</span></p>
			<pre class="source-code">
import pandas as pd
mvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',
                        parse_dates=['datetime'],
                        index_col='datetime')
n_vars = mvtseries.shape[1]</pre>			<p>In this recipe, we’ll use a data module from <strong class="source-inline">pytorch_lightning</strong> to handle data preprocessing. Data modules are classes that contain all the steps for preprocessing data and sharing it with models. Here is the basic structure of a <span class="No-Break">data module:</span></p>
			<pre class="source-code">
import lightning.pytorch as pl
class ExampleDataModule(pl.LightningDataModule):
    def __init__(self,
                 data: pd.DataFrame,
                 batch_size: int):
        super().__init__()
        self.data = data
        self.batch_size = batch_size
    def setup(self, stage=None):
        pass
    def train_dataloader(self):
        pass
    def val_dataloader(self):
        pass
    def test_dataloader(self):
        pass
    def predict_dataloader(self):
        pass</pre>			<p>All <a id="_idIndexMarker221"/>data modules inherit from the <strong class="source-inline">LightningDataModule</strong> class. There are a few key methods <a id="_idIndexMarker222"/>that we need <span class="No-Break">to implement:</span></p>
			<ul>
				<li><strong class="source-inline">setup()</strong>: This method includes all the main data <span class="No-Break">preprocessing steps</span></li>
				<li><strong class="source-inline">train_dataloader()</strong>, <strong class="source-inline">val_dataloader()</strong>, <strong class="source-inline">test_dataloader()</strong>, and <strong class="source-inline">predict_dataloader()</strong>: These are a set of methods that get the data loader for the respective dataset (training, validation, testing, <span class="No-Break">and prediction)</span></li>
			</ul>
			<p>Besides a <strong class="source-inline">DataModule</strong> class, we’ll also leverage the <strong class="source-inline">LightningModule</strong> class to encapsulate all the model processes. These modules have the <span class="No-Break">following structure:</span></p>
			<pre class="source-code">
class ExampleModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.network = ...
    def forward(self, x):
        pass
    def training_step(self, batch, batch_idx):
        pass
    def validation_step(self, batch, batch_idx):
        pass
    def test_step(self, batch, batch_idx):
        pass
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        pass
    def configure_optimizers(self):
        pass</pre>			<p>Let’s <a id="_idIndexMarker223"/>take a <a id="_idIndexMarker224"/>closer look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">ExampleModel</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li>We define any necessary neural network elements in the attributes of the class (such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">self.network</strong></span><span class="No-Break">)</span></li>
				<li>The <strong class="source-inline">forward()</strong> method defines how the elements of the network interact and model the <span class="No-Break">time series</span></li>
				<li><strong class="source-inline">training_step</strong>, <strong class="source-inline">validation_step</strong>, and <strong class="source-inline">testing_step</strong> describe the training, validation, and testing processes of the <span class="No-Break">network, respectively</span></li>
				<li><strong class="source-inline">predict_step</strong> details the process of getting the latest observations and making a forecast, mimicking a <span class="No-Break">deployment scenario</span></li>
				<li>Finally, the <strong class="source-inline">configure_optimizers</strong><strong class="source-inline">()</strong> method details the optimization setup for <span class="No-Break">the network</span></li>
			</ul>
			<p>Let’s see how we can create a data module to preprocess a multivariate time series, and how it couples with <strong class="source-inline">TimeSeriesDataSet</strong>. Then, we’ll implement a <strong class="source-inline">LightningModule</strong> structure to handle the training and testing process of a feedforward <span class="No-Break">neural network.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor281"/>How to do it…</h2>
			<p>The <a id="_idIndexMarker225"/>following <a id="_idIndexMarker226"/>code shows how to define the data module to handle the preprocessing steps. First, let’s look at the <span class="No-Break">class constructor:</span></p>
			<pre class="source-code">
from pytorch_forecasting import TimeSeriesDataSet
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
class MultivariateSeriesDataModule(pl.LightningDataModule):
    def __init__(
            self,
            data: pd.DataFrame,
            n_lags: int,
            horizon: int,
            test_size: float,
            batch_size: int
    ):
        super().__init__()
        self.data = data
        self.feature_names = 
            [col for col in data.columns if col != "Incoming Solar"]
        self.batch_size = batch_size
        self.test_size = test_size
        self.n_lags = n_lags
        self.horizon = horizon
        self.target_scaler = StandardScaler()
        self.training = None
        self.validation = None
        self.test = None
        self.predict_set = None</pre>			<p>In <a id="_idIndexMarker227"/>the constructor, we define all the necessary elements for data preparation, such as the number <a id="_idIndexMarker228"/>of lags, forecasting horizon, and datasets. This includes the initialization of the <strong class="source-inline">target_scaler</strong> attribute, which is used to standardize the value of the <span class="No-Break">time series.</span></p>
			<p>Then, we create the <strong class="source-inline">setup()</strong> method, which includes the data <span class="No-Break">preprocessing logic:</span></p>
			<pre class="source-code">
def setup(self, stage=None):
    self.preprocess_data()
    train_indices, val_indices, test_indices = self.split_data()
    train_df = self.data.loc
        [self.data["time_index"].isin(train_indices)]
    val_df = self.data.loc[self.data["time_index"].isin(val_indices)]
    test_df = self.data.loc
        [self.data["time_index"].isin(test_indices)]
     self.target_scaler.fit(train_df[["target"]])
    self.scale_target(train_df, train_df.index)
    self.scale_target(val_df, val_df.index)
    self.scale_target(test_df, test_df.index)
    train_df = train_df.drop("Incoming Solar", axis=1)
    val_df = val_df.drop("Incoming Solar", axis=1)
    test_df = test_df.drop("Incoming Solar", axis=1)
    self.training = TimeSeriesDataSet(
        train_df,
        time_idx="time_index",
        target="target",
        group_ids=["group_id"],
        max_encoder_length=self.n_lags,
        max_prediction_length=self.horizon,
        time_varying_unknown_reals=self.feature_names,
        scalers={name: StandardScaler() for name in 
            self.feature_names},
    )
    self.validation = TimeSeriesDataSet.from_dataset
        (self.training, val_df)
    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)
    self.predict_set = TimeSeriesDataSet.from_dataset(
    self.training, self.data, predict=True)</pre>			<p>Some of the methods, such as <strong class="source-inline">self.preprocess_data()</strong>, have been omitted for conciseness. You can find their source in this book’s <span class="No-Break">GitHub repository.</span></p>
			<p>Finally, we <a id="_idIndexMarker229"/>must build the data loaders that are responsible for passing data <span class="No-Break">to models:</span></p>
			<pre class="source-code">
    def train_dataloader(self):
        return self.training.to_dataloader
            (batch_size=self.batch_size, shuffle=False)
    def val_dataloader(self):
        return self.validation.to_dataloader
            (batch_size=self.batch_size, shuffle=False)
    def test_dataloader(self):
        return self.test.to_dataloader
            (batch_size=self.batch_size, shuffle=False)
    def predict_dataloader(self):
        return self.predict_set.to_dataloader
            (batch_size=1, shuffle=False)</pre>			<p>Let’s <a id="_idIndexMarker230"/>take a closer look at this <span class="No-Break">data module:</span></p>
			<ul>
				<li>The data preprocessing steps are carried out in the <strong class="source-inline">setup()</strong> method. This includes transforming the time series by including the <strong class="source-inline">time_index</strong> and <strong class="source-inline">group_id</strong> variables, as well as training, validation, and testing splits. The datasets are structured with a <strong class="source-inline">TimeSeriesDataSet</strong> class. Note that we only need to define a <strong class="source-inline">TimeSeriesDataSet</strong> instance for one of the datasets. We can use the <strong class="source-inline">from_dataset</strong><strong class="source-inline">()</strong> method to set up an existing <strong class="source-inline">TimeSeriesDataSet</strong> instance for <span class="No-Break">another dataset.</span></li>
				<li>The information for the preprocessing steps can be passed in the constructor of the <strong class="source-inline">DataModule</strong> class, such as the number of lags (<strong class="source-inline">n_lags</strong>) or <span class="No-Break">forecasting </span><span class="No-Break"><strong class="source-inline">horizon</strong></span><span class="No-Break">.</span></li>
				<li>The data loaders can be obtained by using the <strong class="source-inline">to_dataloader()</strong> method on the <span class="No-Break">respective dataset.</span></li>
			</ul>
			<p>Then, we <a id="_idIndexMarker231"/>can design <a id="_idIndexMarker232"/>the neural network architecture. We will create a class named <strong class="source-inline">FeedForwardNet</strong> that implements a feedforward neural network with <span class="No-Break">three layers:</span></p>
			<pre class="source-code">
import torch
from torch import nn
class FeedForwardNet(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, output_size),
        )
    def forward(self, X):
        X = X.view(X.size(0), -1)
        return self.net(X)</pre>			<p>The network architecture is defined in the <strong class="source-inline">self.net</strong> attribute. The layers of the network are stacked on top of each other with the <span class="No-Break"><strong class="source-inline">nn.Sequential</strong></span><span class="No-Break"> container:</span></p>
			<ul>
				<li>The first layer receives the input data with a size of <strong class="source-inline">input_size</strong>. It is a linear transformation (<strong class="source-inline">nn.Linear</strong>) that contains <strong class="source-inline">16</strong> units with a <strong class="source-inline">ReLU()</strong> activation <span class="No-Break">function (</span><span class="No-Break"><strong class="source-inline">nn.ReLU</strong></span><span class="No-Break">).</span></li>
				<li>The results are passed into the second layer of the same linear type and activation function. This layer contains <span class="No-Break"><strong class="source-inline">8</strong></span><span class="No-Break"> units.</span></li>
				<li>The final layer is also a linear transformation of the inputs coming from the previous one. Its size is the same as <strong class="source-inline">output_size</strong>, which in the case of a time series refers to the <span class="No-Break">forecasting horizon.</span></li>
			</ul>
			<p>Then, we <a id="_idIndexMarker233"/>insert this neural network within a <strong class="source-inline">LightningModule</strong> model class. First, let’s <a id="_idIndexMarker234"/>look at the class constructor and the <span class="No-Break"><strong class="source-inline">forward</strong></span><span class="No-Break"><strong class="source-inline">()</strong></span><span class="No-Break"> method:</span></p>
			<pre class="source-code">
from pytorch_forecasting.models import BaseModel
class FeedForwardModel(BaseModel):
    def __init__(self, input_dim: int, output_dim: int):
        self.save_hyperparameters()
        super().__init__()
        self.network = FeedForwardNet(
            input_size=input_dim,
            output_size=output_dim,
        )
        self.train_loss_history = []
        self.val_loss_history = []
        self.train_loss_sum = 0.0
        self.val_loss_sum = 0.0
        self.train_batch_count = 0
        self.val_batch_count = 0
    def forward(self, x):
        network_input = x["encoder_cont"].squeeze(-1)
        prediction = self.network(network_input)
        output = self.to_network_output(prediction=prediction)
        return output</pre>			<p>The <a id="_idIndexMarker235"/>constructor stores the network elements, while the <strong class="source-inline">forward()</strong> method details how these <a id="_idIndexMarker236"/>elements interact in the forward pass of the network. The <strong class="source-inline">forward()</strong> method also transforms the output in the original data scale using the <strong class="source-inline">to_network_output()</strong> method. The training step and network optimizer are defined <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def training_step(self, batch, batch_idx):
    x, y = batch
    y_pred = self(x).prediction
    y_pred = y_pred.squeeze(1)
    y_actual = y[0].squeeze(1)
    loss = F.mse_loss(y_pred, y_actual)
    self.train_loss_sum += loss.item()
    self.train_batch_count += 1
    self.log("train_loss", loss)
    return loss
def configure_optimizers(self):
    return torch.optim.Adam(self.parameters(), lr=0.01)</pre>			<p>The <strong class="source-inline">configure_optimizers()</strong> method is where we set up the optimization process. In the training step, we get a batch of samples, pass the input onto the neural <a id="_idIndexMarker237"/>network, and then compute the mean squared error using the actual data. Then, we store the error information in <span class="No-Break">different attributes.</span></p>
			<p>The <a id="_idIndexMarker238"/>validation and testing steps work similarly to how they do in the <span class="No-Break">training phase:</span></p>
			<pre class="source-code">
def validation_step(self, batch, batch_idx):
    x, y = batch
    y_pred = self(x).prediction
    y_pred = y_pred.squeeze(1)
    y_actual = y[0].squeeze(1)
    loss = F.mse_loss(y_pred, y_actual)
    self.val_loss_sum += loss.item()
    self.val_batch_count += 1
    self.log("val_loss", loss)
    return loss
def test_step(self, batch, batch_idx):
    x, y = batch
    y_pred = self(x).prediction
    y_pred = y_pred.squeeze(1)
    y_actual = y[0].squeeze(1)
    loss = F.mse_loss(y_pred, y_actual)
    self.log("test_loss", loss)</pre>			<p>In <a id="_idIndexMarker239"/>the prediction <a id="_idIndexMarker240"/>step, we simply pass the input data on to the neural network and get its output <span class="No-Break">in response:</span></p>
			<pre class="source-code">
def predict_step(self, batch, batch_idx):
    x, y = batch
    y_pred = self(x).prediction
    y_pred = y_pred.squeeze(1)
    return y_pred</pre>			<p>Let’s take a look at the preceding <span class="No-Break"><strong class="source-inline">FeedForwardModel</strong></span><span class="No-Break"> module:</span></p>
			<ul>
				<li>The neural network based on <strong class="source-inline">PyTorch</strong> is defined in the <span class="No-Break"><strong class="source-inline">self.network</strong></span><span class="No-Break"> attribute</span></li>
				<li>The <strong class="source-inline">forward()</strong> method describes how the neural network processes an instance that it gets from a <span class="No-Break">data loader</span></li>
				<li>The optimizer is set to <strong class="source-inline">Adam</strong> with a learning rate equal <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.01</strong></span></li>
				<li>Finally, we use the <strong class="source-inline">Trainer</strong> class to train <span class="No-Break">the model:</span></li>
			</ul>
			<pre class="source-code">
datamodule = MultivariateSeriesDataModule(data=mvtseries,
                                          n_lags=7,
                                          horizon=1,
                                          batch_size=32,
                                          test_size=0.3)
model = FeedForwardModel(input_dim=N_LAGS * n_vars, output_dim=1)
trainer = pl.Trainer(max_epochs=30)
trainer.fit(model, datamodule)</pre>			<p>The <a id="_idIndexMarker241"/>training process runs for <strong class="source-inline">30</strong> epochs. To test the model, we can use the <strong class="source-inline">test()</strong> method from the <span class="No-Break"><strong class="source-inline">Trainer</strong></span><span class="No-Break"> instance:</span></p>
			<pre class="source-code">
trainer.test(model=model, datamodule=datamodule)
forecasts = trainer.predict(model=model, datamodule=datamodule)</pre>			<p>Future <a id="_idIndexMarker242"/>observations are forecasted by the <strong class="source-inline">predict</strong><strong class="source-inline">()</strong> method. In both cases, we pass both the model and the data module to the <span class="No-Break"><strong class="source-inline">Trainer</strong></span><span class="No-Break"> instance.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor282"/>How it works…</h2>
			<p>The data module encapsulates all the preparation steps. Any specific transformation that you need to perform on the dataset can be included in the <strong class="source-inline">setup()</strong> method. The logic related to the model is handled in the <strong class="source-inline">LightningModule</strong> instance. Using a <strong class="source-inline">DataModule</strong> and <strong class="source-inline">LightningModule</strong> approach provides a modular and tidier way of developing deep <span class="No-Break">learning models.</span></p>
			<p>The <strong class="source-inline">scalers</strong> argument in the <strong class="source-inline">TimeSeriesDataSet</strong> class is used to pass the scaler that should be used to preprocess the explanatory variables of the time series. In this case, we used <span class="No-Break">the following:</span></p>
			<pre class="source-code">
scalers={name: StandardScaler() for name in self.feature_names}</pre>			<p>Here, we used <strong class="source-inline">StandardScaler</strong> to transform all explanatory variables into a common value range. We standardized the target variable of the time series using the <strong class="source-inline">self.target_scaler</strong> attribute, which includes a <strong class="source-inline">StandardScaler</strong> operator. We normalized the target variable outside of <strong class="source-inline">TimeSeriesDataSet</strong> to give us more control over the target variable. This can serve as an example of how to carry out transformations that may not be readily available in <span class="No-Break">software packages.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor283"/>There’s more…</h2>
			<p>We defined the feedforward neural network using the <strong class="source-inline">nn.Sequential</strong> container. Another <a id="_idIndexMarker243"/>possible approach is to define each element as its own class attribute and call them in the <strong class="source-inline">forward </strong><span class="No-Break">method explicitly:</span></p>
			<pre class="source-code">
class FeedForwardNetAlternative(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.l1 = nn.Linear(input_size, 16)
        self.relu_l1 = nn.ReLU()
        self.l2 = nn.Linear(16, 8)
        self.relu_l2 = nn.ReLU()
        self.l3 = nn.Linear(8, output_size)
    def forward(self, x):
        X = X.view(X.size(0), -1)
        l1_output = self.l1(x)
        l1_actf_output = self.relu_l1(l1_output)
        l2_output = self.l2(l1_actf_output)
        l2_actf_output = self.relu_l2(l2_output)
        l3_output = self.l3(l2_actf_output)
        return l3_output</pre>			<p>Both approaches are equivalent. While the first one is tidier, the second one is <span class="No-Break">more versatile.</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor284"/>LSTM neural networks for multivariate time series forecasting</h1>
			<p>In <a id="_idIndexMarker244"/>this recipe, we’ll <a id="_idIndexMarker245"/>continue the process of building a model to predict the next value of solar radiation using multivariate time series. This time, we’ll train an LSTM recurrent neural network to solve <span class="No-Break">this task.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor285"/>Getting ready</h2>
			<p>The data setup is similar to what we did in the previous recipe. So, we’ll use the same data module we defined there. Now, let’s learn how to build an LSTM neural network with a <span class="No-Break"><strong class="source-inline">LightningModule</strong></span><span class="No-Break"> class.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor286"/>How to do it…</h2>
			<p>The workflow for training an LSTM neural network with PyTorch Lightning is similar, with one small but important detail. For LSTM models, we keep the input data in a three-dimensional structure with a shape of (number of samples, number of lags, number of features). Here’s what the module looks like, starting with the constructor and the <span class="No-Break"><strong class="source-inline">forward</strong></span><span class="No-Break"><strong class="source-inline">()</strong></span><span class="No-Break"> method:</span></p>
			<pre class="source-code">
class MultivariateLSTM(pl.LightningModule):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, 
            batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0), 
            self.hidden_dim).to(self.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0), 
            self.hidden_dim).to(self.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre>			<p>This <a id="_idIndexMarker246"/>time, we don’t have to squeeze the inputs for the network into a two-dimensional vector <a id="_idIndexMarker247"/>since the LSTM takes a three-dimensional input. The logic behind the LSTM is implemented in the <strong class="source-inline">forward()</strong> method. The rest of the methods are identical to what we did in the previous recipe. Here’s <strong class="source-inline">training_step</strong> as <span class="No-Break">an example:</span></p>
			<pre class="source-code">
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        y_pred = y_pred.squeeze(1)
        loss = F.mse_loss(y_pred, y[0])
        self.log('train_loss', loss)
        return loss</pre>			<p>You can find the remaining methods in this book’s <span class="No-Break">GitHub repository.</span></p>
			<p>After defining the model, we can use it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
n_vars = mvtseries.shape[1] - 1
model = MultivariateLSTM(input_dim=n_vars,
                         hidden_dim=10,
                         num_layers=1,
                         output_dim=1)
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model, datamodule)
trainer.test(model, datamodule.test_dataloader())
forecasts = trainer.predict(model=model, datamodule=datamodule)</pre>			<p>As detailed in the preceding code, PyTorch Lightning makes the testing and predicting processes identical <span class="No-Break">across models.</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor287"/>How it works…</h2>
			<p>The <a id="_idIndexMarker248"/>LSTM is a recurrent neural network architecture that’s designed to model sequential data such as time series. This type of network contains a few extra elements relative to feedforward neural networks, such as an extra input dimension or <a id="_idIndexMarker249"/>hidden cell states. In this recipe, we stacked two fully connected layers on top of the LSTM layer. LSTM layers are usually passed on to a fully connected layer because the output of the former is an internal state. So, the fully connected layer processes this output in the particular dimension <span class="No-Break">we need.</span></p>
			<p>The class constructor of the LSTM takes four parameters as input – the number of variables in the time series (<strong class="source-inline">input_size</strong>), the forecasting horizon (<strong class="source-inline">output_size</strong>), the number of <strong class="source-inline">LSTM</strong> layers (<strong class="source-inline">num_layers</strong>), and the number of hidden units in each <strong class="source-inline">LSTM</strong> <span class="No-Break">layer (</span><span class="No-Break"><strong class="source-inline">hidden_size</strong></span><span class="No-Break">).</span></p>
			<p>We defined three layers in the <strong class="source-inline">__init__</strong> constructor method. Besides the <strong class="source-inline">LSTM</strong>, we created two fully connected layers, one of which represents the <span class="No-Break">output layer.</span></p>
			<p>The forward pass of the network works <span class="No-Break">like so:</span></p>
			<ol>
				<li>Initialize the hidden state (<strong class="source-inline">h0</strong>) and cell state (<strong class="source-inline">c0</strong>) with zeros. This is done by calling the <span class="No-Break"><strong class="source-inline">init_hidden_state</strong></span><span class="No-Break"><strong class="source-inline">()</strong></span><span class="No-Break"> method.</span></li>
				<li>Pass the input data to the LSTM stack. The LSTM returns its output and the hidden and cell states of each <span class="No-Break">LSTM layer.</span></li>
				<li>Next, we get the hidden state of the last LSTM layer, which is passed onto a <strong class="source-inline">ReLU</strong><strong class="source-inline">()</strong> <span class="No-Break">activation function.</span></li>
				<li>The results from <strong class="source-inline">ReLU</strong> are passed to the first fully connected layer, whose output is, once again, transformed with a <strong class="source-inline">ReLU</strong><strong class="source-inline">()</strong> activation function. Finally, the output is passed to a linear, fully connected output layer, which provides <span class="No-Break">the forecasts.</span></li>
			</ol>
			<p>This logic is coded in the <strong class="source-inline">forward</strong><strong class="source-inline">()</strong> method of the <span class="No-Break"><strong class="source-inline">LightningModule</strong></span><span class="No-Break"> instance.</span></p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor288"/>There’s more…</h2>
			<p>We created <a id="_idIndexMarker250"/>a deep neural network with a single LSTM layer (<strong class="source-inline">num_layers=1</strong>). However, we could increase this value according to our needs. A model with more than<a id="_idTextAnchor289"/><a id="_idTextAnchor290"/> one LSTM layer is referred to as a <strong class="bold">stacked </strong><span class="No-Break"><strong class="bold">LSTM</strong></span><span class="No-Break"> model.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor291"/>Monitoring the training process using Tensorboard</h1>
			<p>Training deep learning models often involves tuning numerous hyperparameters, assessing <a id="_idIndexMarker251"/>different architectures, and more. To facilitate these tasks, visualization and monitoring tools are <a id="_idIndexMarker252"/>essential. <strong class="source-inline">tensorboard</strong> is a powerful tool for tracking and visualizing various metrics during the training process. In this section, we will guide you through integrating <strong class="source-inline">tensorboard</strong> with PyTorch Lightning for monitoring the <span class="No-Break">training process.</span></p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor292"/>Getting ready</h2>
			<p>Before using <strong class="source-inline">tensorboard</strong> with PyTorch Lightning, you’ll need to have <strong class="source-inline">tensorboard</strong> installed. You can install it using the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install -U tensorboard</pre>			<p>Once installed, make sure that you are utilizing PyTorch Lightning’s built-in <strong class="source-inline">tensorboard</strong> <span class="No-Break">logging capabilities.</span></p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor293"/>How to do it…</h2>
			<p>Here’s how to use <strong class="source-inline">tensorboard</strong> to monitor the <span class="No-Break">training process:</span></p>
			<ol>
				<li>First, ensure that <strong class="source-inline">tensorboard</strong> is imported into <span class="No-Break">your script.</span></li>
				<li>Next, you’ll need to create a <strong class="source-inline">tensorboard</strong> logger and pass it to PyTorch <span class="No-Break">Lightning</span><span class="No-Break">’s </span><span class="No-Break"><strong class="source-inline">Trainer</strong></span><span class="No-Break">:</span><pre class="source-code">
from lightning.pytorch.loggers import TensorBoardLogger
import lightning.pytorch as pl
logger = TensorBoardLogger('logs/')
trainer = pl.Trainer(logger=logger)</pre></li>				<li>You can <a id="_idIndexMarker253"/>then start <strong class="source-inline">tensorboard</strong> by running the following command in <span class="No-Break">your terminal:</span><pre class="source-code">
tensorboard --logdir=logs/</pre></li>				<li>Open <strong class="source-inline">tensorboard</strong> in your web browser by navigating to the URL displayed <a id="_idIndexMarker254"/>in your terminal; usually, this is <strong class="source-inline">http://localhost:6006</strong>. You’ll see real-time updates on various metrics, such as the number of epochs, the train, validation, and test loss, <span class="No-Break">and more.</span></li>
			</ol>
			<p>The following figure shows some plots of the LSTM’s performance from the previous recipe. In this case, we can see how the number of epochs, as well as training and validation losses, <span class="No-Break">are evolving:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B21145_04_002.jpg" alt="Figure 4.2: Comparison of epochs, training loss, and validation loss" width="1650" height="506"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: Comparison of epochs, training loss, and validation loss</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor294"/>How it works…</h2>
			<p><strong class="source-inline">tensorboard</strong> provides visualizations for various training metrics, hyperparameter <a id="_idIndexMarker255"/>tuning, model <a id="_idIndexMarker256"/>graphs, and more. When integrated with PyTorch Lightning, the <span class="No-Break">following occurs:</span></p>
			<ul>
				<li>The logger sends the specified metrics to <strong class="source-inline">tensorboard</strong> <span class="No-Break">during training</span></li>
				<li><strong class="source-inline">tensorboard</strong> reads the logs and provides <span class="No-Break">interactive visualizations</span></li>
				<li>Users can monitor various aspects of training in <span class="No-Break">real time</span></li>
			</ul>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor295"/>There’s more…</h2>
			<p>Here are some additional details to keep <span class="No-Break">in mind:</span></p>
			<ul>
				<li>You can log additional information such as images, text, histograms, <span class="No-Break">and more</span></li>
				<li>By exploring different visualizations, you can gain insights into how your model is performing and make <span class="No-Break">necessary adjustments</span></li>
				<li>Tensorboard’s integration with PyTorch Lightning streamlines the monitoring process, enabling more efficient <span class="No-Break">model development</span></li>
			</ul>
			<p>Using <strong class="source-inline">tensorboard</strong> with PyTorch Lightning offers a robust solution for monitoring and visualizing the training process, allowing for more informed decision-making in <span class="No-Break">model development.</span></p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor296"/>Evaluating deep neural networks for forecasting</h1>
			<p>Evaluating <a id="_idIndexMarker257"/>the performance of forecasting <a id="_idIndexMarker258"/>models is essential to understand <a id="_idIndexMarker259"/>how well they generalize to unseen data. Popular metrics include the <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>), <strong class="bold">Mean Absolute Percentage Error</strong> (<strong class="bold">MAPE</strong>), <strong class="bold">Mean Absolute Scaled Error</strong> (<strong class="bold">MASE</strong>), and <strong class="bold">Symmetric Mean Absolute Percentage Error</strong> (<strong class="bold">SMAPE</strong>), among <a id="_idIndexMarker260"/>others. We will implement these metrics in <a id="_idIndexMarker261"/>Python and show you how they can be applied to evaluate our <span class="No-Break">model’s performance.</span></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor297"/>Getting ready</h2>
			<p>We need predictions from our trained model and the corresponding ground truth values to calculate these metrics. Therefore, we must run our model on the test set first to obtain <span class="No-Break">the predictions.</span></p>
			<p>To simplify <a id="_idIndexMarker262"/>the implementation, we will use the <strong class="source-inline">scikit-learn</strong> and <strong class="source-inline">sktime</strong> libraries since they have useful classes and methods to help us with this task. Since we have not installed <strong class="source-inline">sktime</strong> yet, let’s run the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install sktime</pre>			<p>Now, it is time to import the classes and methods for the different <span class="No-Break">evaluation metrics:</span></p>
			<pre class="source-code">
from sklearn.metrics import mean_squared_error
from sktime.performance_metrics.forecasting 
import mean_absolute_scaled_error, MeanAbsolutePercentageError
import numpy as np</pre>			<h2 id="_idParaDest-208"><a id="_idTextAnchor298"/>How to do it…</h2>
			<p>To evaluate the performance of our model, we must calculate the <strong class="bold">RMSE</strong> using the <strong class="source-inline">scikit-learn</strong> library. For <strong class="bold">MASE</strong>, <strong class="bold">MAPE</strong>, and <strong class="bold">SMAPE</strong>, we must leverage the <strong class="source-inline">sktime</strong> library, which offers readily available functions for <span class="No-Break">these metrics.</span></p>
			<p>Here’s the code detailing how to calculate <span class="No-Break">these metrics:</span></p>
			<pre class="source-code">
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
y_pred = model(X_test).detach().numpy()
y_true = y_test.detach().numpy()
rmse_sklearn = np.sqrt(mean_squared_error(y_true, y_pred)) print(f"RMSE (scikit-learn): {rmse_sklearn}")
mape = mean_absolute_percentage_error(y_true, y_pred) 
print(f"MAPE: {mape}")
mase_sktime = mean_absolute_scaled_error(y_true, y_pred) 
print(f"MASE (sktime): {mase_sktime}")
smape_sktime = symmetric_mean_absolute_percentage_error
    (y_true, y_pred)
 print(f"SMAPE (sktime): {smape_sktime}")</pre>			<h2 id="_idParaDest-209"><a id="_idTextAnchor299"/>How it works…</h2>
			<p>These <a id="_idIndexMarker263"/>metrics each evaluate different aspects of the <span class="No-Break">model’s performance:</span></p>
			<ul>
				<li><strong class="bold">RMSE</strong>: This <a id="_idIndexMarker264"/>metric calculates the square root of the average squared differences between the predicted and actual values. It gives a higher penalty for <span class="No-Break">large errors.</span></li>
				<li><strong class="bold">MASE</strong>: This <a id="_idIndexMarker265"/>metric scales the <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>) by the MAE of a naive forecast. This makes it easier <a id="_idIndexMarker266"/>to interpret, with a MASE value of <strong class="source-inline">1</strong> indicating performance equal to the naive forecast and a MASE value less than <strong class="source-inline">1</strong> indicating better performance than the <span class="No-Break">naive forecast.</span></li>
				<li><strong class="bold">MAPE</strong>: This <a id="_idIndexMarker267"/>metric computes the mean of the absolute percentage difference between the actual and predicted values. It expresses the average absolute error in terms of percentage, which can be useful when you want to understand the relative <span class="No-Break">prediction error.</span></li>
				<li><strong class="bold">SMAPE</strong>: This <a id="_idIndexMarker268"/>metric computes the average absolute percentage error, treating under and over-forecasts equally. It expresses the error as a percentage of the actual values, which can be useful for comparing models and predicting <span class="No-Break">different scales.</span></li>
			</ul>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor300"/>There’s more…</h2>
			<p>Remember, the choice of evaluation metric depends on the specific problem and business requirements. For example, if it is more costly to have a model that under-predicts than over-predicts, a metric that treats these two types of errors differently may be more appropriate. Other metrics, such as MAE, can also be used, depending on the problem. Evaluating a model using multiple metrics is always a good idea to gain a more comprehensive understanding of <span class="No-Break">its performance.</span></p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor301"/>Using callbacks – EarlyStopping</h1>
			<p>Callbacks in PyTorch Lightning are reusable components that allow you to inject custom behavior <a id="_idIndexMarker269"/>into various stages of the training, validation, and testing loops. They offer a way to encapsulate functionalities separate from the main training logic, providing a modular and extensible approach to manage auxiliary tasks such as logging metrics, saving checkpoints, early stopping, <span class="No-Break">and more.</span></p>
			<p>By defining a custom class that inherits from PyTorch Lightning’s base <strong class="source-inline">Callback</strong> class, you can override specific methods corresponding to different points in the training process, such as <strong class="source-inline">on_epoch_start</strong> or <strong class="source-inline">on_batch_end</strong>. When a trainer is initialized with one or more of these callback objects, the defined behavior is automatically executed at the corresponding stage of the training process. This makes callbacks powerful tools for organizing the training pipeline, adding flexibility without cluttering the main <span class="No-Break">training code.</span></p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor302"/>Getting ready</h2>
			<p>After defining and training the LSTM model, as described in the previous section, we can further enhance the training process by incorporating a technique called early stopping. This is used to avoid overfitting by halting the training process when a specified metric stops improving. For this purpose, PyTorch Lightning provides an early stopping callback, which we’ll be integrating into our existing <span class="No-Break">training code.</span></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor303"/>How to do it…</h2>
			<p>To apply <a id="_idIndexMarker270"/>early stopping, we’ll need to modify our existing PyTorch Lightning <strong class="source-inline">Trainer</strong> by adding the <strong class="source-inline">EarlyStopping</strong> callback. Here’s the code to <span class="No-Break">do so:</span></p>
			<pre class="source-code">
import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping
early_stop_callback = EarlyStopping(
    monitor="val_loss",
    min_delta=0.00,
    patience=3,
    verbose=False,
    mode="min"
)
trainer = pl.Trainer(max_epochs=100,
                     callbacks=[early_stop_callback]) 
trainer.fit(model, datamodule)</pre>			<p>In this code snippet, <strong class="source-inline">monitor</strong> is set to the validation loss (<strong class="source-inline">val_loss</strong>), and the training process will stop if this value does not decrease by at least <strong class="source-inline">min_delta</strong> for <strong class="source-inline">patience</strong> consecutive <span class="No-Break">validation epochs.</span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor304"/>How it works…</h2>
			<p>Early stopping is a regularization technique that prevents overfitting in neural networks. It monitors a specified metric (in this case, the validation loss) and halts the training process when this metric <span class="No-Break">stops improving.</span></p>
			<p>Here’s how it works in our <span class="No-Break">LSTM model:</span></p>
			<ul>
				<li><strong class="bold">Monitoring</strong>: Early stopping keeps track of the specified metric (<strong class="source-inline">val_loss</strong>) during the <span class="No-Break">validation phase.</span></li>
				<li><strong class="bold">Patience</strong>: If the monitored metric does not improve by at least <strong class="source-inline">min_delta</strong> for <strong class="source-inline">patience</strong> consecutive epochs, the training process <span class="No-Break">is halted.</span></li>
				<li><strong class="bold">Mode</strong>: The <strong class="source-inline">mode</strong> parameter can be set to <strong class="source-inline">min</strong> or <strong class="source-inline">max</strong>, indicating whether the monitored metric should be minimized or maximized. In our case, we want to minimize the <span class="No-Break">validation loss.</span></li>
			</ul>
			<p>By stopping the <a id="_idIndexMarker271"/>training process early, we can save time and resources, and also potentially obtain a model that generalizes better to <span class="No-Break">unseen data.</span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor305"/>There’s more…</h2>
			<p>Let’s look at some <span class="No-Break">further details:</span></p>
			<ul>
				<li>The early stopping callback is highly configurable, allowing you to tailor its behavior to specific requirements – for example, you can change the <strong class="source-inline">patience</strong> parameter to make the stopping criterion more or <span class="No-Break">less strict</span></li>
				<li>Early stopping can be combined with other callbacks and techniques, such as model checkpointing, to create a robust and efficient <span class="No-Break">training pipeline</span></li>
				<li>Utilizing early stopping appropriately can lead to models that perform better on unseen data as it prevents them from fitting too closely to the <span class="No-Break">training data</span></li>
			</ul>
			<p>This <strong class="source-inline">EarlyStopping</strong> callback integrates seamlessly with PyTorch Lightning and our existing LSTM model, demonstrating the extensibility and ease of use of PyTorch Lightning’s <span class="No-Break">callback system.</span></p>
		</div>
	</div>
</div>
</body></html>