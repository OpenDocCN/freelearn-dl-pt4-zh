<html><head></head><body>
        <section id="4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data Representation Using Autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">This chapter will introduce unsupervised applications of deep learning using autoencoders. In this chapter, we will cover the following topics:</p>
<ul class="calibre12">
<li class="calibre13">Setting up autoencoders</li>
<li class="calibre13">Data normalization</li>
<li class="calibre13">Setting up a regularized autoencoder</li>
<li class="calibre13">Fine-tuning the parameters of the autoencoder</li>
<li class="calibre13">Setting up stacked autoencoders</li>
<li class="calibre13">Setting up denoising autoencoders</li>
<li class="calibre13">Building and comparing stochastic encoders and decoders</li>
<li class="calibre13">Learning manifolds from autoencoders</li>
<li class="calibre13">Evaluating the sparse decomposition</li>
</ul>


            </article>

            
        </section>
    

        <section id="4V8DU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
                
            
            <article>
                
<p class="calibre2">Neural networks aim to find a non-linear relationship between input <em class="calibre9">X</em> with output <em class="calibre9">y,</em> as <em class="calibre9">y=f(x)</em><strong class="calibre1">.</strong> An autoencoder is a form of unsupervised neural network which tries to find a relationship between features in space such that <em class="calibre9">h</em>=<em class="calibre9">f(x)</em>, which helps us learn the relationship between input space and can be used for data compression, dimensionality reduction, and feature learning.</p>
<p class="calibre2">Â </p>
<div class="packt_infobox">An autoencoder consists of an encoder and decoder. The encoder helps encode the input <em class="calibre21">x</em> in a latent representation <em class="calibre21">y,</em> whereas a decoder converts back the <em class="calibre21">y</em> to <em class="calibre21">x</em>. Both the encoder and decoder possess a similar representation of form.</div>
<p class="calibre2">Here is a representation of a one layer autoencoder:</p>
<div class="cdpaligncenter"><strong class="calibre1"><img src="../images/00051.jpeg" class="calibre39"/></strong></div>
<p class="calibre2">The coder encodes input <em class="calibre9">X</em> to <em class="calibre9">h</em> under a hidden layer contain, whereas the decoder helps to attain the original data from encoded output <em class="calibre9">h</em>. The matrices <em class="calibre9">W<sub class="calibre30">e</sub></em> and <em class="calibre9">W<sub class="calibre30">d</sub></em> represent the weights of the encoder and decoder layers, respectively. The function <em class="calibre9">f</em> is the activation function.</p>
<p class="calibre2">An illustration of an autoencoder is shown in the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border42" src="../images/00053.jpeg"/></div>
<p class="calibre2">The constraints in the form of nodes allows the autoencoder to discover interesting structures within the data. For example, in the encoder in the preceding diagram, the five-input dataset must pass through three-node compression to get an encoded value <em class="calibre9">h</em><strong class="calibre1"><em class="calibre9">.</em></strong> The <strong class="calibre1">Encoded Output layer</strong> of an encoder can have a dimensionality that is the same, lower, or higher than the <strong class="calibre1">input/output Decoded Output layer</strong>. The <strong class="calibre1">Encoded Output layer</strong> with a fewer number of nodes than the input layer is referred to as an under-complete representation, and can be thought of as data compression transforming data into low-dimensional representation.</p>
<p class="calibre2">An <strong class="calibre1">Encoded Output layer</strong> with a larger number of input layers is referred to as an over-complete representation and is used in a <strong class="calibre1">sparse autoencoder</strong> as a regularization strategy. The objective of an autoencoder is to find <em class="calibre9">y,</em> capturing the main factors along the variation of data, which is similar to <strong class="calibre1">Principal Component Analysis (PCA)</strong>, and thus can be used for compression as well.</p>


            </article>

            
        </section>
    

        <section id="506UG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">There exist a lot of different architectures of autoencoders distinguished by cost functions used to capture data representation. The most basic autoencoder is known as a vanilla autoencoder. It's a two-layer neural network with one hidden layer the same number of nodes at the input and output layers, with an objective to minimize the cost function<em class="calibre9">.</em> The typical choices, but not limited to, for a loss function are <strong class="calibre1">mean square error</strong> (<strong class="calibre1">MSE</strong>) for regression and cross entropy for classification. The current approach can be easily extended to multiple layers, also known as multilayer autoencoder.</p>
<p class="calibre2">The number of nodes plays a very critical role in autoencoders. If the number of nodes in the hidden layer is less than the input layer then an autoencoder is known as an <strong class="calibre1">under-complete</strong> autoencoder. A higher number of nodes in the hidden layer represents an <strong class="calibre1">over-complete</strong> autoencoder or sparse autoencoder.</p>
<p class="calibre2">The sparse autoencoder aims to impose sparsity in the hidden layer. This sparsity can be achieved by introducing a higher number of nodes than the input in the hidden layer or by introducing a penalty in the loss function that will move the weights for the hidden layer toward zero. Some autoencoders attain the sparsity by manually zeroing out the weight for nodes; these are referred to as <strong class="calibre1">K-sparse autoencoders</strong><em class="calibre9">.</em> We will set up an autoencoder on the occupancy dataset discussed in <a href="part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 1</a>, <em class="calibre9">Getting Started</em>. The hidden layer for the current example can be tweaked around.</p>


            </article>

            
        </section>
    

        <section id="515F21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Let's use the <kbd class="calibre10">Occupancy</kbd> dataset to set up an autoencoder:</p>
<ul class="calibre12">
<li class="calibre13">Download the <kbd class="calibre10">Occupancy</kbd> dataset as described in <a href="part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 1</a>, <em class="calibre9">Getting Started</em></li>
<li class="calibre13">TensorFlow installation in R and Python</li>
</ul>


            </article>

            
        </section>
    

        <section id="523VK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The current occupancy dataset as described in <a href="part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 1</a>, <em class="calibre9">Getting Started</em>, is used to demonstrate the autoencoder setup in R using TensorFlow:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Set up the R TensorFlow environment.</li>
<li value="2" class="calibre13">The <kbd class="calibre10">load_occupancy_data</kbd> function can be used to load the data by setting the correct working directory path using <kbd class="calibre10">setwd</kbd>:</li>
</ol>
<pre class="calibre23">
# Function to load Occupancy data<br class="title-page-tagline"/>load_occupancy_data&lt;-function(train){<br class="title-page-tagline"/>xFeatures = c("Temperature", "Humidity", "Light", "CO2",<br class="title-page-tagline"/>  "HumidityRatio")<br class="title-page-tagline"/>yFeatures = "Occupancy"<br class="title-page-tagline"/>  if(train){<br class="title-page-tagline"/>    occupancy_ds &lt;-  as.matrix(read.csv("datatraining.txt",stringsAsFactors = T))<br class="title-page-tagline"/>  } else<br class="title-page-tagline"/>  {<br class="title-page-tagline"/>    occupancy_ds &lt;- as.matrix(read.csv("datatest.txt",stringsAsFactors = T))<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>  occupancy_ds&lt;-apply(occupancy_ds[, c(xFeatures, yFeatures)], 2, FUN=as.numeric) <br class="title-page-tagline"/>  return(occupancy_ds)<br class="title-page-tagline"/>}
</pre>
<ol start="3" class="calibre15">
<li class="calibre13" value="3">The train and test occupancy dataset can be loaded to the R environment with the following script:</li>
</ol>
<pre class="calibre23">
occupancy_train &lt;-load_occupancy_data(train=T)<br class="title-page-tagline"/>occupancy_test &lt;- load_occupancy_data(train = F)
</pre>


            </article>

            
        </section>
    

        <section id="532G61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data normalization</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Data normalization</strong> is a critical step in machine learning to bring data to a similar scale. It is also known as feature scaling and is performed as data preprocessing.</p>
<div class="packt_infobox">The correct normalization is very critical in neural networks, else it will lead to saturation within the hidden layers, which in turn leads to zero gradient and no learning will be possible.</div>


            </article>

            
        </section>
    

        <section id="5410O1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">There are multiple ways to perform normalization:</p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Min-max standardization</strong>: The min-max retains the original distribution and scales the feature values between <em class="calibre9">[0, 1],</em> with <em class="calibre9">0</em> as the minimum value of the feature and <em class="calibre9">1</em> as the maximum value. The standardization is performed as follows:</li>
</ul>
<div class="cdpaligncenter"><img src="../images/00055.jpeg" class="calibre39"/></div>
<p class="calibre24">Here, <em class="calibre9">x</em>' is the normalized value of the feature. The method is sensitive to outliers in the dataset.</p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Decimal scaling</strong>: This form of scaling is used where values of different decimal ranges are present. For example, two features with different bounds can be brought to a similar scale using decimal scaling as follows:</li>
</ul>
<div class="cdpaligncenter"><em class="calibre9">x'=x/10<sup class="calibre40">n</sup></em></div>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">Z-score:</strong> This transformation scales the value toward a normal distribution with a zero mean and unit variance. The Z-score is computed as:</li>
</ul>
<div class="cdpaligncenter"><em class="calibre9">Z</em>=(<em class="calibre9"><em class="calibre9">x-Âµ)/Ï</em></em></div>
<p class="calibre24">Here, <em class="calibre9">Âµ</em> is the mean and Ï is the standard deviation of the feature. These distributions are very efficient for a dataset with a Gaussian distribution.</p>
<div class="packt_infobox">
<p class="calibre31">All the preceding methods are sensitive to outliers; there are other more robust approaches for normalization that you can explore, such as <strong class="calibre36">Median Absolute Deviation</strong> (<strong class="calibre36">MAD</strong>), tanh-estimator, and double sigmoid.</p>
</div>


            </article>

            
        </section>
    

        <section id="54VHA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Visualizing dataset distribution</h1>
                
            
            <article>
                
<p class="calibre2">Let's look at the distribution of features for the occupation data:</p>
<pre class="calibre20">
&gt; ggpairs(occupancy_train$data[, occupancy_train$xFeatures])
</pre>
<div class="cdpaligncenter"><img class="image-border43" src="../images/00059.gif"/></div>
<p class="calibre2">The figure shows that the features have linear correlations and the distributions are non-normal. The non-normality can be further validated using the Shapiro-Wilk test, using the <kbd class="calibre10">shapiro.test</kbd> function from R. Let's use min-max standardization for the occupation data.</p>


            </article>

            
        </section>
    

        <section id="55U1S1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Perform the following operation for data normalization:</li>
</ol>
<pre class="calibre23">
minmax.normalize&lt;-function(ds, scaler=NULL){<br class="title-page-tagline"/>  if(is.null(scaler)){<br class="title-page-tagline"/>    for(f in ds$xFeatures){<br class="title-page-tagline"/>      scaler[[f]]$minval&lt;-min(ds$data[,f])<br class="title-page-tagline"/>      scaler[[f]]$maxval&lt;-max(ds$data[,f])<br class="title-page-tagline"/>      ds$data[,f]&lt;-(ds$data[,f]-scaler[[f]]$minval)/(scaler[[f]]$maxval-scaler[[f]]$minval)<br class="title-page-tagline"/>    }<br class="title-page-tagline"/>    ds$scaler&lt;-scaler<br class="title-page-tagline"/>  } else<br class="title-page-tagline"/>  {<br class="title-page-tagline"/>    for(f in ds$xFeatures){<br class="title-page-tagline"/>      ds$data[,f]&lt;-(ds$data[,f]-scaler[[f]]$minval)/(scaler[[f]]$maxval-scaler[[f]]$minval)<br class="title-page-tagline"/>    }<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>  return(ds)<br class="title-page-tagline"/>}
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The <kbd class="calibre10">minmax.normalize</kbd> function normalizes the data using min-max normalization. When the <kbd class="calibre10">scaler</kbd> variable is <kbd class="calibre10">NULL</kbd>, it performs normalization using the dataset provided, or normalizes using <kbd class="calibre10">scaler</kbd> values. The normalized data pair plot is shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border44" src="../images/00060.gif"/></div>
<p class="normalpackt">This figure shows min-max normalization bringing the values within bounds <em class="calibre9">[0, 1]</em> and it does not change the distribution and correlations between features.</p>


            </article>

            
        </section>
    

        <section id="56SIE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to set up an autoencoder model</h1>
                
            
            <article>
                
<p class="calibre2">The next step is to set up the autoencoder model. Let's set up a vanilla autoencoder using TensorFlow:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Reset the <kbd class="calibre10">graph</kbd> and start <kbd class="calibre10">InteractiveSession</kbd>:</li>
</ol>
<pre class="calibre23">
# Reset the graph and set-up a interactive session<br class="title-page-tagline"/>tf$reset_default_graph()<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Define the input parameter where <kbd class="calibre10">n</kbd> and <kbd class="calibre10">m</kbd> are the number of samples and features, respectively. To build, network <kbd class="calibre10">m</kbd> is used to set up the input parameter:</li>
</ol>
<pre class="calibre23">
# Network Parameters<br class="title-page-tagline"/>n_hidden_1 = 5 # 1st layer num features<br class="title-page-tagline"/>n_input = length(xFeatures) # Number of input features<br class="title-page-tagline"/>nRow&lt;-nrow(occupancy_train)
</pre>
<p class="calibre35">When <kbd class="calibre10">n_hidden_1</kbd> is low, the autoencoder is compressing the data and is referred to as an under-complete autoencoder; whereas, when <kbd class="calibre10">n_hidden_1</kbd> is large, then the autoencoder is sparse and is referred to as an over-complete autoencoder.</p>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Define graph input parameters that include the input tensor and layer definitions for the encoder and decoder:</li>
</ol>
<pre class="calibre23">
# Define input feature<br class="title-page-tagline"/>x &lt;- tf$constant(unlist(occupancy_train[, xFeatures]), shape=c(nRow, n_input), dtype=np$float32) <br class="title-page-tagline"/><br class="title-page-tagline"/># Define hidden and bias layer for encoder and decoders<br class="title-page-tagline"/>hiddenLayerEncoder&lt;-tf$Variable(tf$random_normal(shape(n_input, n_hidden_1)), dtype=np$float32)<br class="title-page-tagline"/>biasEncoder &lt;- tf$Variable(tf$zeros(shape(n_hidden_1)), dtype=np$float32)<br class="title-page-tagline"/>hiddenLayerDecoder&lt;-tf$Variable(tf$random_normal(shape(n_hidden_1, n_input)))<br class="title-page-tagline"/>biasDecoder &lt;- tf$Variable(tf$zeros(shape(n_input)))
</pre>
<p class="calibre35"><span>The preceding script designs a single-layer encoder and decoder.</span></p>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define a function to evaluate the response:</li>
</ol>
<pre class="calibre23">
auto_encoder&lt;-function(x, hiddenLayerEncoder, biasEncoder){<br class="title-page-tagline"/>  x_transform &lt;- tf$nn$sigmoid(tf$add(tf$matmul(x, hiddenLayerEncoder), biasEncoder))<br class="title-page-tagline"/>  x_transform<br class="title-page-tagline"/>}
</pre>
<p class="calibre35">The <kbd class="calibre10">auto_encoder</kbd> function takes the node bias weights and computes the output. The same function can be used for <kbd class="calibre10">encoder</kbd> and <kbd class="calibre10">decoder</kbd> by passing respective weights.</p>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Create <kbd class="calibre10">encoder</kbd> and <kbd class="calibre10">decoder</kbd> objects by passing symbolic TensorFlow variables:</li>
</ol>
<pre class="calibre23">
encoder_obj = auto_encoder(x,hiddenLayerEncoder, biasEncoder)<br class="title-page-tagline"/>y_pred = auto_encoder(encoder_obj, hiddenLayerDecoder, biasDecoder)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">The <kbd class="calibre10">y_pred</kbd> is the outcome from <kbd class="calibre10">decoder</kbd>, which takes the <kbd class="calibre10">encoder</kbd> object as input with nodes and bias weights:</li>
</ol>
<pre class="calibre23">
Define loss function and optimizer module. <br class="title-page-tagline"/>learning_rate = 0.01<br class="title-page-tagline"/>cost = tf$reduce_mean(tf$pow(x - y_pred, 2))<br class="title-page-tagline"/>optimizer = tf$train$RMSPropOptimizer(learning_rate)$minimize(cost)
</pre>
<p class="calibre35">The preceding script defines mean square error as the cost function, and uses <kbd class="calibre10">RMSPropOptimizer</kbd> from TensorFlow with 0.1 learning rate for the optimization of weights. The TensorFlow graph for the preceding model is shown in the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border45" src="../images/00063.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="57R301-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Running optimization</h1>
                
            
            <article>
                
<p class="calibre2">The next step is to run optimizer optimization. Executing this process in TensorFlow consists of two steps:</p>
<ol class="calibre15">
<li value="1" class="calibre13">The first step is parameter initialization of the variables defined in the graph. The initialization is performed by calling the <kbd class="calibre10">global_variables_initializer</kbd> function from TensorFlow:</li>
</ol>
<pre class="calibre23">
# Initializing the variables<br class="title-page-tagline"/>init = tf$global_variables_initializer()<br class="title-page-tagline"/>sess$run(init)
</pre>
<p class="calibre35">Optimization is performed based on optimizing and monitoring the train and test performance:</p>
<pre class="calibre23">
costconvergence&lt;-NULL<br class="title-page-tagline"/>for (step in 1:1000) {<br class="title-page-tagline"/>  sess$run(optimizer)<br class="title-page-tagline"/>  if (step %% 20==0){<br class="title-page-tagline"/>    costconvergence&lt;-rbind(costconvergence, c(step, sess$run(cost), sess$run(costt)))<br class="title-page-tagline"/>    cat(step, "-", "Traing Cost ==&gt;", sess$run(cost), "\n")<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>}
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The cost function from train and test can be observed to understand convergence of the model, as shown in the following figure:</li>
</ol>
<pre class="calibre23">
costconvergence&lt;-data.frame(costconvergence)<br class="title-page-tagline"/>colnames(costconvergence)&lt;-c("iter", "train", "test")<br class="title-page-tagline"/>plot(costconvergence[, "iter"], costconvergence[, "train"], type = "l", col="blue", xlab = "Iteration", ylab = "MSE")<br class="title-page-tagline"/>lines(costconvergence[, "iter"], costconvergence[, "test"], col="red")<br class="title-page-tagline"/>legend(500,0.25, c("Train","Test"), lty=c(1,1), lwd=c(2.5,2.5),col=c("blue","red"))
</pre>
<div class="cdpaligncenter"><img class="image-border46" src="../images/00133.gif"/></div>
<p class="calibre2">This graph shows that the model major <kbd class="calibre10">convergence</kbd> is at around <strong class="calibre1">400</strong> iterations; however, it is still converging at a very slow rate even after <strong class="calibre1">1,000</strong> iterations. The model is stable in both the train and holdout test datasets.</p>


            </article>

            
        </section>
    

        <section id="58PJI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a regularized autoencoder</h1>
                
            
            <article>
                
<p class="calibre2">A regularized autoencoder extends the standard autoencoder by adding a regularization parameter to the <kbd class="calibre10">cost</kbd> function.</p>


            </article>

            
        </section>
    

        <section id="59O441-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The regularized autoencoder is an extension of the standard autoencoder. The set-up will require:</p>
<ol class="calibre15">
<li value="1" class="calibre13">TensorFlow installation in R and Python.</li>
<li value="2" class="calibre13">Implementation of a standard autoencoder.</li>
</ol>


            </article>

            
        </section>
    

        <section id="5AMKM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The code setup for the autoencoder can directly be converted to a regularized autoencoder by replacing the cost definition with the following lines:</p>
<pre class="calibre23">
Lambda=0.01<br class="title-page-tagline"/>cost = tf$reduce_mean(tf$pow(x - y_pred, 2))<br class="title-page-tagline"/>Regularize_weights = tf$nn$l2_loss(weights)<br class="title-page-tagline"/>cost = tf$reduce_mean(cost + lambda * Regularize_weights)
</pre>


            </article>

            
        </section>
    

        <section id="5BL581-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="normalpackt">As mentioned earlier, a regularized autoencoder extends the standard autoencoder by adding a regularization parameter to the cost function, shown as follows:</p>
<div class="cdpaligncenter"><img src="../images/00148.jpeg" class="calibre41"/></div>
<p class="calibre2">Here, <em class="calibre9">Î»</em> is the regularization parameter and <em class="calibre9">i</em> and <em class="calibre9">j</em> are the node indexes with <em class="calibre9">W</em> representing the hidden layer weights for the autoencoder. The regularization autoencoder aims to ensure more robust encoding and prefers a low weight <em class="calibre9">h</em> function. The concept is further utilized to develop a contractive autoencoder, which utilizes the Frobenius norm of the Jacobian matrix on input, represented as follows:</p>
<div class="cdpaligncenter"><img src="../images/00029.jpeg" class="calibre42"/></div>
<p class="calibre2">where <strong class="calibre1">J(x)</strong> is the Jacobian matrix and is evaluated as follows:</p>
<div class="cdpaligncenter"><img src="../images/00146.jpeg" class="calibre43"/></div>
<p class="calibre2">For a linear encoder, a contractive encoder and regularized encoder converge to L2 weight decay. The regularization helps in making the autoencoder less sensitive to the input; however, the minimization of the cost function helps the model to capture the variation and remain sensitive to manifolds of high density. These autoencoders are also referred to as <strong class="calibre1">contractive autoencoders</strong>.</p>


            </article>

            
        </section>
    

        <section id="5CJLQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Fine-tuning the parameters of the autoencoder</h1>
                
            
            <article>
                
<p class="calibre2">The autoencoder involves a couple of parameters to tune, depending on the type of autoencoder we are working on. The major parameters in an autoencoder include the following:</p>
<ul class="calibre12">
<li class="calibre13">Number of nodes in any hidden layer</li>
<li class="calibre13">Number of hidden layers applicable for deep autoencoders</li>
<li class="calibre13">Activation unit such as sigmoid, tanh, softmax, and ReLU activation functions</li>
<li class="calibre13">Regularization parameters or weight decay terms on hidden unit weights</li>
<li class="calibre13">Fraction of the signal to be corrupted in a denoising autoencoder</li>
<li class="calibre13">Sparsity parameters in sparse autoencoders that control the expected activation of neurons in hidden layers</li>
<li class="calibre13">Batch size, if using batch gradient descent learning; learning rate and momentum parameter for stochastic gradient descent</li>
<li class="calibre13">Maximum iterations to be used for the training</li>
<li class="calibre13">Weight initialization</li>
<li class="calibre13">Dropout regularization if dropout is used</li>
</ul>
<p class="calibre2">These hyperparameters can be trained by setting the problem as a grid search problem. However, each hyperparameter combination requires training the neuron weights for the hidden layer(s), which results in increasing computational complexity with an increase in the number of layers and number of nodes within each layer. To deal with these critical parameters and training issues, stacked autoencoder concepts have been proposed that train each layer separately to get pretrained weights, and then the model is fine-tuned using the obtained weights. This approach tremendously improves the training performance over the conventional mode of training.</p>


            </article>

            
        </section>
    

        <section id="5DI6C1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up stacked autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">The stacked autoencoder is an approach to train deep networks consisting of multiple layers trained using the greedy approach. An example of a stacked autoencoder is shown in the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border47" src="../images/00037.jpeg"/></div>
<div class="packt_figref">An example of a stacked autoencoder</div>


            </article>

            
        </section>
    

        <section id="5EGMU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The preceding diagram demonstrates a stacked autoencoder with two layers. A stacked autoencoder can have <em class="calibre9">n</em> layers, where each layer is trained using one layer at a time. For example, the previous layer will be trained as follows:</p>
<div class="cdpaligncenter"><img class="image-border48" src="../images/00039.jpeg"/></div>
<div class="packt_figref">Training of a stacked autoencoder</div>
<p class="calibre2">The initial pre-training of layer 1 is obtained by training it over the actual input <em class="calibre9">x<sub class="calibre30">i</sub></em> . The first step is to optimize the <em class="calibre9">We(1)</em> layer of the encoder with respect to output X. The second step in the preceding example is to optimize the weights <em class="calibre9">We(2)</em> in the second layer, using <em class="calibre9">We(1)</em> as input and output. Once all the layers of <em class="calibre9">We(i)</em> where <em class="calibre9">i</em>=1, 2, ...,<em class="calibre9">n</em> is number of layers are pretrained, model fine-tuning is performed by connecting all the layers together, as shown in step 3 of the preceding diagram. The concept can also be applied to denoising to train multilayer networks, which is known as a stacked denoising autoencoder. The code developed in a denoising autoencoder can be easily tweaked to develop a <strong class="calibre1">stacked denoising autoencoder</strong>, which is an extension of the stacked autoencoder.</p>
<p class="calibre2">The requirements for this recipe are:</p>
<ol class="calibre15">
<li value="1" class="calibre13">R should be installed.</li>
<li value="2" class="calibre13"><kbd class="calibre10">SAENET</kbd> package, The package can be download from Cran using the command <kbd class="calibre10">install.packages("SAENET")</kbd><em class="calibre9">.</em></li>
</ol>


            </article>

            
        </section>
    

        <section id="5FF7G1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">There are other popular libraries in R to develop stacked autoencoders. Let's utilize the <kbd class="calibre10">SAENET</kbd> package from R to set up a stacked autoencoder. The <kbd class="calibre10">SAENET</kbd> is a stacked autoencoder implementation, using a feedforward neural network using the <kbd class="calibre10">neuralnet</kbd> package from CRAN:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Get the <kbd class="calibre10">SAENET</kbd> package from the CRAN repository, if not installed already:</li>
</ol>
<pre class="calibre23">
install.packages("SAENET")
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Load all library dependencies:</li>
</ol>
<pre class="calibre23">
require(SAENET)
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Load the train and test occupancy dataset using <kbd class="calibre10">load_occupancy_data</kbd>:</li>
</ol>
<pre class="calibre23">
occupancy_train &lt;-load_occupancy_data(train=T)<br class="title-page-tagline"/>occupancy_test &lt;- load_occupancy_data(train = F)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Normalize the dataset using the <kbd class="calibre10">minmax.normalize</kbd> function:</li>
</ol>
<pre class="calibre23">
# Normalize dataset<br class="title-page-tagline"/>occupancy_train&lt;-minmax.normalize(occupancy_train, scaler = NULL)<br class="title-page-tagline"/>occupancy_test&lt;-minmax.normalize(occupancy_test, scaler = occupancy_train$scaler)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">The stacked autoencoder model can be built using the <kbd class="calibre10">SAENET.train</kbd> train function from the <kbd class="calibre10">SAENET</kbd> package:</li>
</ol>
<pre class="calibre23">
# Building Stacked Autoencoder<br class="title-page-tagline"/>SAE_obj&lt;-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 2), unit.type ="tanh", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000)
</pre>
<p class="calibre2">The output of the last node can be extracted using the <kbd class="calibre10">SAE_obj[[n]]$X.output</kbd> command.</p>


            </article>

            
        </section>
    

        <section id="5GDO21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up denoising autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">Denoising autoencoders are a special kind of autoencoder with a focus on extracting robust features from the input dataset. Denoising autoencoders are similar to the previous model except with a major difference that the data is corrupted before training the network. Different approaches for corruption can be used such as masking, which induces random error into the data.</p>


            </article>

            
        </section>
    

        <section id="5HC8K1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Let's use the CIFAR-10 image data to set up a denoising dataset:</p>
<ul class="calibre12">
<li class="calibre13">Download the CIFAR-10 dataset using the <kbd class="calibre10">download_cifar_data</kbd> function (covered in <a href="part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 3</a>, <em class="calibre9">Convolution Neural Network</em>)</li>
<li class="calibre13">TensorFlow installation in R and Python</li>
</ul>


            </article>

            
        </section>
    

        <section id="5IAP61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">We first need to read the dataset.</p>


            </article>

            
        </section>
    

        <section id="5J99O1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reading the dataset</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Load the <kbd class="calibre10">CIFAR</kbd> dataset using the steps explained in <a href="part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 3</a>, <em class="calibre9">Convolution Neural Network</em>. The data files <kbd class="calibre10">data_batch_1</kbd> and <kbd class="calibre10">data_batch_2</kbd> are used to train. The <kbd class="calibre10">data_batch_5</kbd> and <kbd class="calibre10">test_batch</kbd> files are used for validation and testing, respectively. The data can be flattened using the <kbd class="calibre10">flat_data</kbd> function:</li>
</ol>
<pre class="calibre23">
train_data &lt;- flat_data(x_listdata = images.rgb.train)<br class="title-page-tagline"/>test_data &lt;- flat_data(x_listdata = images.rgb.test)<br class="title-page-tagline"/>valid_data &lt;- flat_data(x_listdata = images.rgb.valid)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The <kbd class="calibre10">flat_data</kbd> function flattens the dataset as <em class="calibre9">NCOL = (Height * Width * number of channels)</em>, thus the dimension of the dataset is (# of images X NCOL). The images in <kbd class="calibre10">CIFAR</kbd> are 32 x 32 with three RGB channels; thus, we obtain 3,072 columns after data flattening:</li>
</ol>
<pre class="calibre23">
&gt; dim(train_data$images)<br class="title-page-tagline"/>[1] 40000  3072
</pre>


            </article>

            
        </section>
    

        <section id="5K7QA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Corrupting data to train</h1>
                
            
            <article>
                
<ol start="1" class="calibre15">
<li value="1" class="calibre13">The next critical function needed to set up a denoising autoencoder is data corruption:</li>
</ol>
<pre class="calibre23">
# Add noise using masking or salt &amp; pepper noise method<br class="title-page-tagline"/>add_noise&lt;-function(data, frac=0.10, corr_type=c("masking", "saltPepper", "none")){<br class="title-page-tagline"/>  if(length(corr_type)&gt;1) corr_type&lt;-corr_type[1] <br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # Assign a copy of data<br class="title-page-tagline"/>  data_noise = data<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # Evaluate chaining parameters for autoencoder<br class="title-page-tagline"/>  nROW&lt;-nrow(data)<br class="title-page-tagline"/>  nCOL&lt;-ncol(data)<br class="title-page-tagline"/>  nMask&lt;-floor(frac*nCOL)<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  if(corr_type=="masking"){<br class="title-page-tagline"/>    for( i in 1:nROW){<br class="title-page-tagline"/>      maskCol&lt;-sample(nCOL, nMask)<br class="title-page-tagline"/>      data_noise[i,maskCol,,]&lt;-0<br class="title-page-tagline"/>    }<br class="title-page-tagline"/>  } else if(corr_type=="saltPepper"){<br class="title-page-tagline"/>    minval&lt;-min(data[,,1,])<br class="title-page-tagline"/>    maxval&lt;-max(data[,,1,])<br class="title-page-tagline"/>    for( i in 1:nROW){<br class="title-page-tagline"/>      maskCol&lt;-sample(nCOL, nMask)<br class="title-page-tagline"/>      randval&lt;-runif(length(maskCol))<br class="title-page-tagline"/>      ixmin&lt;-randval&lt;0.5<br class="title-page-tagline"/>      ixmax&lt;-randval&gt;=0.5<br class="title-page-tagline"/>      if(sum(ixmin)&gt;0) data_noise[i,maskCol[ixmin],,]&lt;-minval<br class="title-page-tagline"/>      if(sum(ixmax)&gt;0) data_noise[i,maskCol[ixmax],,]&lt;-maxval<br class="title-page-tagline"/>    }<br class="title-page-tagline"/>  } else<br class="title-page-tagline"/>  {<br class="title-page-tagline"/>    data_noise&lt;-data<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>  return(data_noise)<br class="title-page-tagline"/>}
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The CIFAR-10 data can be corrupted using the following script:</li>
</ol>
<pre class="calibre23">
# Corrupting input signal<br class="title-page-tagline"/>xcorr&lt;-add_noise(train_data$images, frac=0.10, corr_type="masking")
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">An example image after corruption is as follows:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border49" src="../images/00002.jpeg"/></div>
<ol start="4" class="calibre15">
<li class="calibre13" value="4">The preceding figure uses the masking approach to add noise. This method adds zero values at random image locations with a defined fraction. Another approach to add noise is by using salt &amp; pepper noise. This method selects random locations in the image and replaces them, adding min or max values to the image using the flip of coin principle. An example of the salt and pepper approach for data corruption is shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border50" src="../images/00045.jpeg"/></div>
<p class="calibre2">The data corruption helps the autoencoder to learn more robust representation.</p>


            </article>

            
        </section>
    

        <section id="5L6AS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a denoising autoencoder</h1>
                
            
            <article>
                
<p class="calibre2">The next step is to set up the autoencoder model:</p>
<ol class="calibre15">
<li value="1" class="calibre13">First, reset the graph and start an interactive session as follows:</li>
</ol>
<pre class="calibre23">
# Reset the graph and set-up an interactive session<br class="title-page-tagline"/>tf$reset_default_graph()<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The next step is to define two placeholders for the input signal and corrupt signal:</li>
</ol>
<pre class="calibre23">
# Define Input as Placeholder variables<br class="title-page-tagline"/>x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x')<br class="title-page-tagline"/>x_corrput&lt;-tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x_corrput')
</pre>
<p class="calibre24">The <kbd class="calibre10">x_corrupt</kbd> will be used as input in the autoencoder, and <em class="calibre9">x</em> is the actual image that will be used as output.</p>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Set up a denoising autoencoder function as shown in the following code:</li>
</ol>
<pre class="calibre23">
# Setting-up denoising autoencoder<br class="title-page-tagline"/>denoisingAutoencoder&lt;-function(x, x_corrput, img_size_flat=3072, hidden_layer=c(1024, 512), out_img_size=256){<br class="title-page-tagline"/><br class="title-page-tagline"/>  # Building Encoder<br class="title-page-tagline"/>  encoder = NULL<br class="title-page-tagline"/>  n_input&lt;-img_size_flat<br class="title-page-tagline"/>  curentInput&lt;-x_corrput<br class="title-page-tagline"/>  layer&lt;-c(hidden_layer, out_img_size)<br class="title-page-tagline"/>  for(i in 1:length(layer)){<br class="title-page-tagline"/>    n_output&lt;-layer[i]<br class="title-page-tagline"/>    W = tf$Variable(tf$random_uniform(shape(n_input, n_output), -1.0 / tf$sqrt(n_input), 1.0 / tf$sqrt(n_input)))<br class="title-page-tagline"/>    b = tf$Variable(tf$zeros(shape(n_output)))<br class="title-page-tagline"/>    encoder&lt;-c(encoder, W)<br class="title-page-tagline"/>    output = tf$nn$tanh(tf$matmul(curentInput, W) + b)<br class="title-page-tagline"/>    curentInput = output<br class="title-page-tagline"/>    n_input&lt;-n_output<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # latent representation<br class="title-page-tagline"/>  z = curentInput<br class="title-page-tagline"/>  encoder&lt;-rev(encoder)<br class="title-page-tagline"/>  layer_rev&lt;-c(rev(hidden_layer), img_size_flat)<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # Build the decoder using the same weights<br class="title-page-tagline"/>  decoder&lt;-NULL<br class="title-page-tagline"/>  for(i in 1:length(layer_rev)){<br class="title-page-tagline"/>    n_output&lt;-layer_rev[i]<br class="title-page-tagline"/>    W = tf$transpose(encoder[[i]])<br class="title-page-tagline"/>    b = tf$Variable(tf$zeros(shape(n_output)))<br class="title-page-tagline"/>    output = tf$nn$tanh(tf$matmul(curentInput, W) + b)<br class="title-page-tagline"/>    curentInput = output<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # now have the reconstruction through the network<br class="title-page-tagline"/>  y = curentInput<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # cost function measures pixel-wise difference<br class="title-page-tagline"/>  cost = tf$sqrt(tf$reduce_mean(tf$square(y - x)))<br class="title-page-tagline"/>  return(list("x"=x, "z"=z, "y"=y, "x_corrput"=x_corrput, "cost"=cost))<br class="title-page-tagline"/>}<br class="title-page-tagline"/><br class="title-page-tagline"/>
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Create the denoising object:</li>
</ol>
<pre class="calibre23">
# Create denoising AE object<br class="title-page-tagline"/>dae_obj&lt;-denoisingAutoencoder(x, x_corrput, img_size_flat=3072, hidden_layer=c(1024, 512), out_img_size=256)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Set the cost function:</li>
</ol>
<pre class="calibre23">
# Learning set-up<br class="title-page-tagline"/>learning_rate = 0.001<br class="title-page-tagline"/>optimizer = tf$train$AdamOptimizer(learning_rate)$minimize(dae_obj$cost)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Run optimization:</li>
</ol>
<pre class="calibre23">
# We create a session to use the graph<br class="title-page-tagline"/>sess$run(tf$global_variables_initializer())<br class="title-page-tagline"/>for(i in 1:500){<br class="title-page-tagline"/>  spls &lt;- sample(1:dim(xcorr)[1],1000L)<br class="title-page-tagline"/>  if (i %% 1 == 0) {<br class="title-page-tagline"/>    x_corrput_ds&lt;-add_noise(train_data$images[spls, ], frac = 0.3, corr_type = "masking")<br class="title-page-tagline"/>    optimizer$run(feed_dict = dict(x=train_data$images[spls, ], x_corrput=x_corrput_ds))<br class="title-page-tagline"/>    trainingCost&lt;-dae_obj$cost$eval((feed_dict = dict(x=train_data$images[spls, ], x_corrput=x_corrput_ds)))<br class="title-page-tagline"/>    cat("Training Cost - ", trainingCost, "\n")<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="5M4RE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The autoencoder keeps learning about the function form for the feature to capture the relationship between input and output. An example of how the computer is visualizing the image after 1,000 iterations is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border51" src="../images/00061.jpeg"/></div>
<p class="calibre2">After 1,000 iterations, the computer can distinguish between a major part of the object and environment. As we run the algorithm further to fine-tune the weights, the computer keeps learning more features about the object itself, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border52" src="../images/00140.gif"/></div>
<p class="calibre2">The preceding graph shows that the model is still learning, but the learning rate has become smaller over the iterations as it starts learning fine features about objects, as shown in the following image. There are instances when the model starts ascending instead of descending, due to batch gradient descent:</p>
<div class="cdpaligncenter"><img class="image-border53" src="../images/00007.jpeg"/></div>
<div class="packt_figref">Illustration of learning using denoising autoencoder</div>


            </article>

            
        </section>
    

        <section id="5N3C01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Building and comparing stochastic encoders and decoders</h1>
                
            
            <article>
                
<p class="calibre2">Stochastic encoders fall into the domain of generative modeling, where the objective is to learn join probability <em class="calibre9">P(X)</em> over given data <em class="calibre9">X</em> transformed into another high-dimensional space. For example, we want to learn about images and produce similar, but not exactly the same, images by learning about pixel dependencies and distribution. One of the popular approaches in generative modeling is <strong class="calibre1">Variational autoencoder</strong> (<strong class="calibre1">VAE</strong>), which combines deep learning with statistical inference by making a strong distribution assumption on <em class="calibre9">h ~ P(h),</em> such as Gaussian or Bernoulli. For a given weight <em class="calibre9">W</em><strong class="calibre1">,</strong> the <em class="calibre9">X</em> can be sampled from the distribution as <em class="calibre9">Pw(X|h)</em><strong class="calibre1">.</strong> An example of VAE architecture is shown in the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border54" src="../images/00052.gif"/></div>
<p class="calibre2">The cost function of VAE is based on log likelihood maximization. The cost function consists of reconstruction and regularization error terms:</p>
<p class="calibre2"><em class="calibre9">Cost = Reconstruction Error + Regularization Error</em></p>
<p class="calibre2">The <strong class="calibre1">reconstruction error</strong> is how well we could map the outcome with the training data and the <strong class="calibre1">regularization error</strong> puts a penalty on the distribution formed at the encoder and decoder.</p>


            </article>

            
        </section>
    

        <section id="5O1SI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">TensorFlow needs to be installed and loaded in the environment:</p>
<pre class="calibre20">
require(tensorflow)
</pre>
<p class="calibre2">Dependencies need to be loaded:</p>
<pre class="calibre20">
require(imager)<br class="title-page-tagline"/>require(caret)
</pre>
<p class="calibre2">The <kbd class="calibre10">MNIST</kbd> dataset needs to be loaded. The dataset is normalized using the following script:</p>
<pre class="calibre20">
# Normalize Dataset<br class="title-page-tagline"/>normalizeObj&lt;-preProcess(trainData, method="range")<br class="title-page-tagline"/>trainData&lt;-predict(normalizeObj, trainData)<br class="title-page-tagline"/>validData&lt;-predict(normalizeObj, validData)
</pre>
<div class="cdpaligncenter"><img class="image-border55" src="../images/00054.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="5P0D41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The <kbd class="calibre10">MNIST</kbd> dataset is used to demonstrate the concept of sparse decomposition. The <kbd class="calibre10">MNIST</kbd> dataset uses handwritten digits. It is downloaded from the <kbd class="calibre10">tensorflow</kbd> dataset library. The dataset consists of handwritten images of <kbd class="calibre10">28 x 28</kbd> pixels. It consists of 55,000 training examples, 10,000 test examples, and 5,000 test examples. The dataset can be downloaded from the <kbd class="calibre10">tensorflow</kbd> library using the following script:</li>
</ol>
<pre class="calibre23">
library(tensorflow)<br class="title-page-tagline"/>datasets &lt;- tf$contrib$learn$datasets<br class="title-page-tagline"/>mnist &lt;- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE) 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">For computational simplicity, the <kbd class="calibre10">MNIST</kbd> image size is reduced from <kbd class="calibre10">28 x 28</kbd> pixels to <kbd class="calibre10">16 x 16</kbd> pixels using the following function:</li>
</ol>
<pre class="calibre23">
# Function to reduce image size<br class="title-page-tagline"/>reduceImage&lt;-function(actds, n.pixel.x=16, n.pixel.y=16){<br class="title-page-tagline"/>  actImage&lt;-matrix(actds, ncol=28, byrow=FALSE)<br class="title-page-tagline"/>  img.col.mat &lt;- imappend(list(as.cimg(actImage)),"c")<br class="title-page-tagline"/>  thmb &lt;- resize(img.col.mat, n.pixel.x, n.pixel.y)<br class="title-page-tagline"/>  outputImage&lt;-matrix(thmb[,,1,1], nrow = 1, byrow = F)<br class="title-page-tagline"/>  return(outputImage)<br class="title-page-tagline"/>} 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">The following script can be used to prepare the <kbd class="calibre10">MNIST</kbd> training data with a 16 x 16 pixel image:</li>
</ol>
<pre class="calibre23">
# Covert train data to 16 x 16  image<br class="title-page-tagline"/>trainData&lt;-t(apply(mnist$train$images, 1, FUN=reduceImage))<br class="title-page-tagline"/>validData&lt;-t(apply(mnist$test$images, 1, FUN=reduceImage))
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">The <kbd class="calibre10">plot_mnist</kbd> function can be used to visualize the selected <kbd class="calibre10">MNIST</kbd> image:</li>
</ol>
<pre class="calibre23">
# Function to plot MNIST dataset<br class="title-page-tagline"/>plot_mnist&lt;-function(imageD, pixel.y=16){<br class="title-page-tagline"/>  actImage&lt;-matrix(imageD, ncol=pixel.y, byrow=FALSE)<br class="title-page-tagline"/>  img.col.mat &lt;- imappend(list(as.cimg(actImage)), "c")<br class="title-page-tagline"/>  plot(img.col.mat)<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section>

                            <header id="5PUTM2-a0a93989f17f4d6cb68b8cfd331bc5ab">
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a VAE model</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Start a new TensorFlow environment:</li>
</ol>
<pre class="calibre23">
tf$reset_default_graph()<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Define network parameters:</li>
</ol>
<pre class="calibre23">
n_input=256<br class="title-page-tagline"/>n.hidden.enc.1&lt;-64
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Start a new TensorFlow environment:</li>
</ol>
<pre class="calibre23">
tf$reset_default_graph()<br class="title-page-tagline"/>sess&lt;-tf$InteractiveSession()
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define network parameters:</li>
</ol>
<pre class="calibre23">
n_input=256<br class="title-page-tagline"/>n.hidden.enc.1&lt;-64
</pre>
<p class="calibre24">The preceding parameter will form a VAE network as follows:</p>
<div class="cdpaligncenter"><img class="image-border56" src="../images/00056.gif"/></div>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Define the model initialization function, defining weights and biases at each layer of <kbd class="calibre10">encoder</kbd> and <kbd class="calibre10">decoder</kbd>:</li>
</ol>
<pre class="calibre23">
model_init&lt;-function(n.hidden.enc.1, n.hidden.enc.2, <br class="title-page-tagline"/>                               n.hidden.dec.1,  n.hidden.dec.2, <br class="title-page-tagline"/>                               n_input, n_h)<br class="title-page-tagline"/>{ weights&lt;-NULL<br class="title-page-tagline"/> ############################<br class="title-page-tagline"/> # Set-up Encoder<br class="title-page-tagline"/> ############################<br class="title-page-tagline"/> # Initialize Layer 1 of encoder<br class="title-page-tagline"/> weights[["encoder_w"]][["h1"]]=tf$Variable(xavier_init(n_input,<br class="title-page-tagline"/> n.hidden.enc.1))<br class="title-page-tagline"/> weights[["encoder_w"]]<br class="title-page-tagline"/>[["h2"]]=tf$Variable(xavier_init(n.hidden.enc.1, n.hidden.enc.2))<br class="title-page-tagline"/>  weights[["encoder_w"]][["out_mean"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))<br class="title-page-tagline"/>  weights[["encoder_w"]][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))<br class="title-page-tagline"/>  weights[["encoder_b"]][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.1), dtype=tf$float32))<br class="title-page-tagline"/>  weights[["encoder_b"]][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.2), dtype=tf$float32))<br class="title-page-tagline"/>  weights[["encoder_b"]][["out_mean"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))<br class="title-page-tagline"/>  weights[["encoder_b"]][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))<br class="title-page-tagline"/>  <br class="title-page-tagline"/> ############################<br class="title-page-tagline"/> # Set-up Decoder<br class="title-page-tagline"/> ############################<br class="title-page-tagline"/> weights[['decoder_w']][["h1"]]=tf$Variable(xavier_init(n_h, n.hidden.dec.1))<br class="title-page-tagline"/> weights[['decoder_w']][["h2"]]=tf$Variable(xavier_init(n.hidden.dec.1, n.hidden.dec.2))<br class="title-page-tagline"/> weights[['decoder_w']][["out_mean"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))<br class="title-page-tagline"/> weights[['decoder_w']][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))<br class="title-page-tagline"/> weights[['decoder_b']][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.1), dtype=tf$float32))<br class="title-page-tagline"/> weights[['decoder_b']][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.2), dtype=tf$float32))<br class="title-page-tagline"/> weights[['decoder_b']][["out_mean"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))<br class="title-page-tagline"/> weights[['decoder_b']][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))<br class="title-page-tagline"/> return(weights)<br class="title-page-tagline"/>} 
</pre>
<p class="calibre24">The <kbd class="calibre10">model_init</kbd> function returns <kbd class="calibre10">weights</kbd>, which is a two-dimensional list. The first dimension captures the weight's association and type. For example, it describes if the <kbd class="calibre10">weights</kbd> variable is assigned to the encoder or decoder and if it stores the weight of the node or bias. The <kbd class="calibre10">xavier_init</kbd> function in <kbd class="calibre10">model_init</kbd> is used to assign initial weights for model training:</p>
<pre class="calibre23">
# Xavier Initialization using Uniform distribution <br class="title-page-tagline"/>xavier_init&lt;-function(n_inputs, n_outputs, constant=1){<br class="title-page-tagline"/>  low = -constant*sqrt(6.0/(n_inputs + n_outputs)) <br class="title-page-tagline"/>  high = constant*sqrt(6.0/(n_inputs + n_outputs))<br class="title-page-tagline"/>  return(tf$random_uniform(shape(n_inputs, n_outputs), minval=low, maxval=high, dtype=tf$float32))<br class="title-page-tagline"/>}
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Set up the encoder evaluation function:</li>
</ol>
<pre class="calibre23">
# Encoder update function<br class="title-page-tagline"/>vae_encoder&lt;-function(x, weights, biases){<br class="title-page-tagline"/>  layer_1 = tf$nn$softplus(tf$add(tf$matmul(x, weights[['h1']]), biases[['b1']])) <br class="title-page-tagline"/>  layer_2 = tf$nn$softplus(tf$add(tf$matmul(layer_1, weights[['h2']]), biases[['b2']])) <br class="title-page-tagline"/>  z_mean = tf$add(tf$matmul(layer_2, weights[['out_mean']]), biases[['out_mean']])<br class="title-page-tagline"/>  z_log_sigma_sq = tf$add(tf$matmul(layer_2, weights[['out_log_sigma']]), biases[['out_log_sigma']])<br class="title-page-tagline"/>  return (list("z_mean"=z_mean, "z_log_sigma_sq"=z_log_sigma_sq))<br class="title-page-tagline"/>} 
</pre>
<p class="calibre24">The <kbd class="calibre10">vae_encoder</kbd> computes the mean and variance to sample the layer, using the weights and bias from the hidden layer.</p>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Set up the decoder evaluation function:</li>
</ol>
<pre class="calibre23">
# Decoder update function<br class="title-page-tagline"/>vae_decoder&lt;-function(z, weights, biases){<br class="title-page-tagline"/>  layer1&lt;-tf$nn$softplus(tf$add(tf$matmul(z, weights[["h1"]]), biases[["b1"]]))<br class="title-page-tagline"/>  layer2&lt;-tf$nn$softplus(tf$add(tf$matmul(layer1, weights[["h2"]]), biases[["b2"]]))<br class="title-page-tagline"/>  x_reconstr_mean&lt;-tf$nn$sigmoid(tf$add(tf$matmul(layer2, weights[['out_mean']]), biases[['out_mean']]))<br class="title-page-tagline"/>  return(x_reconstr_mean)<br class="title-page-tagline"/>}<strong class="calibre1"> </strong>
</pre>
<p class="calibre24">The <kbd class="calibre10">vae_decoder</kbd> function computes the mean and standard deviation associated with the sampling layer at output and average output.</p>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Set up the function for reconstruction estimation:</li>
</ol>
<pre class="calibre23">
# Parameter evaluation<br class="title-page-tagline"/>network_ParEval&lt;-function(x, network_weights, n_h){<br class="title-page-tagline"/><br class="title-page-tagline"/>  distParameter&lt;-vae_encoder(x, network_weights[["encoder_w"]], network_weights[["encoder_b"]])<br class="title-page-tagline"/>  z_mean&lt;-distParameter$z_mean<br class="title-page-tagline"/>  z_log_sigma_sq &lt;-distParameter$z_log_sigma_sq<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # Draw one sample z from Gaussian distribution<br class="title-page-tagline"/>  eps = tf$random_normal(shape(BATCH, n_h), 0, 1, dtype=tf$float32)<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # z = mu + sigma*epsilon<br class="title-page-tagline"/>  z = tf$add(z_mean, tf$multiply(tf$sqrt(tf$exp(z_log_sigma_sq)), eps))<br class="title-page-tagline"/>  <br class="title-page-tagline"/>  # Use generator to determine mean of<br class="title-page-tagline"/>  # Bernoulli distribution of reconstructed input<br class="title-page-tagline"/>  x_reconstr_mean &lt;- vae_decoder(z, network_weights[["decoder_w"]], network_weights[["decoder_b"]])<br class="title-page-tagline"/>  return(list("x_reconstr_mean"=x_reconstr_mean, "z_log_sigma_sq"=z_log_sigma_sq, "z_mean"=z_mean))<br class="title-page-tagline"/>}
</pre>
<ol start="9" class="calibre15">
<li value="9" class="calibre13">Define the cost function for optimization:</li>
</ol>
<pre class="calibre23">
# VAE cost function<br class="title-page-tagline"/>vae_optimizer&lt;-function(x, networkOutput){<br class="title-page-tagline"/>  x_reconstr_mean&lt;-networkOutput$x_reconstr_mean<br class="title-page-tagline"/>  z_log_sigma_sq&lt;-networkOutput$z_log_sigma_sq<br class="title-page-tagline"/>  z_mean&lt;-networkOutput$z_mean<br class="title-page-tagline"/>  loss_reconstruction&lt;--1*tf$reduce_sum(x*tf$log(1e-10 + x_reconstr_mean)+<br class="title-page-tagline"/>                                       (1-x)*tf$log(1e-10 + 1 - x_reconstr_mean), reduction_indices=shape(1))<br class="title-page-tagline"/>  loss_latent&lt;--0.5*tf$reduce_sum(1+z_log_sigma_sq-tf$square(z_mean)-<br class="title-page-tagline"/>                                    tf$exp(z_log_sigma_sq), reduction_indices=shape(1))<br class="title-page-tagline"/>  cost = tf$reduce_mean(loss_reconstruction + loss_latent)<br class="title-page-tagline"/>  return(cost)<br class="title-page-tagline"/>}
</pre>
<ol start="10" class="calibre15">
<li value="10" class="calibre13">Set up the model to train:</li>
</ol>
<pre class="calibre23">
# VAE Initialization<br class="title-page-tagline"/>x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x')<br class="title-page-tagline"/>network_weights&lt;-model_init(n.hidden.enc.1, n.hidden.enc.2, <br class="title-page-tagline"/>                            n.hidden.dec.1,  n.hidden.dec.2, <br class="title-page-tagline"/>                            n_input, n_h)<br class="title-page-tagline"/>networkOutput&lt;-network_ParEval(x, network_weights, n_h)<br class="title-page-tagline"/>cost=vae_optimizer(x, networkOutput)<br class="title-page-tagline"/>optimizer = tf$train$AdamOptimizer(lr)$minimize(cost) 
</pre>
<ol start="11" class="calibre15">
<li value="11" class="calibre13">Run optimization:</li>
</ol>
<pre class="calibre23">
sess$run(tf$global_variables_initializer())<br class="title-page-tagline"/>for(i in 1:ITERATION){<br class="title-page-tagline"/>  spls &lt;- sample(1:dim(trainData)[1],BATCH)<br class="title-page-tagline"/>  out&lt;-optimizer$run(feed_dict = dict(x=trainData[spls,]))<br class="title-page-tagline"/>  if (i %% 100 == 0){<br class="title-page-tagline"/>  cat("Iteration - ", i, "Training Loss - ",  cost$eval(feed_dict = dict(x=trainData[spls,])), "\n")<br class="title-page-tagline"/>  }<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="5QTE81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Output from the VAE autoencoder</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The outcome can be generated using the following script:</li>
</ol>
<pre class="calibre23">
spls &lt;- sample(1:dim(trainData)[1],BATCH)<br class="title-page-tagline"/>networkOutput_run&lt;-sess$run(networkOutput, feed_dict = dict(x=trainData[spls,]))<br class="title-page-tagline"/><br class="title-page-tagline"/># Plot reconstructured Image<br class="title-page-tagline"/>x_sample&lt;-trainData[spls,]<br class="title-page-tagline"/>NROW&lt;-nrow(networkOutput_run$x_reconstr_mean)<br class="title-page-tagline"/>n.plot&lt;-5<br class="title-page-tagline"/>par(mfrow = c(n.plot, 2), mar = c(0.2, 0.2, 0.2, 0.2), oma = c(3, 3, 3, 3))<br class="title-page-tagline"/>pltImages&lt;-sample(1:NROW,n.plot)<br class="title-page-tagline"/>for(i in pltImages){<br class="title-page-tagline"/>  plot_mnist(x_sample[i,])<br class="title-page-tagline"/>  plot_mnist(networkOutput_run$x_reconstr_mean[i,])<br class="title-page-tagline"/>}
</pre>
<p class="calibre24">The outcome obtained after <kbd class="calibre10">20,000</kbd> iterations from the preceding VAE autoencoder is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border57" src="../images/00147.gif"/></div>
<p class="calibre2">Additionally, as VAE is a generative model, the outcome is not an exact replica of the input and would vary with runs, as a representative sample is extracted from the estimated distribution.</p>


            </article>

            
        </section>
    

        <section id="5RRUQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Learning manifolds from autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">Manifold learning is an approach in machine learning that assumes that data lies on a manifold of a much lower dimension. These manifolds can be linear or non-linear. Thus, the area tries to project the data from high-dimension space to a low dimension. For example, <strong class="calibre1">principle component analysis</strong> (<strong class="calibre1">PCA</strong>) is an example of linear manifold learning whereas an autoencoder is a <strong class="calibre1">non-linear dimensionality reduction</strong> (<strong class="calibre1">NDR</strong>) with the ability to learn non-linear manifolds in low dimensions. A comparison of linear and non-linear manifold learning is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border58" src="../images/00062.gif"/></div>
<p class="calibre2">As you can see from graph <strong class="calibre1">a)</strong>, the data is residing at a linear manifold, whereas in graph graph <strong class="calibre1">b)</strong>, the data is residing on a second-order non-linear manifold.</p>


            </article>

            
        </section>
    

        <section id="5SQFC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Let's take an output from the stacked autoencoder section and analyze how manifolds look when transferred into a different dimension.</p>


            </article>

            
        </section>
    

        <section id="5TOVU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up principal component analysis</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Before getting into non-linear manifolds, let's analyze principal component analysis on the occupancy data:</li>
</ol>
<pre class="calibre23">
# Setting-up principal component analysis <br class="title-page-tagline"/>pca_obj &lt;- prcomp(occupancy_train$data,<br class="title-page-tagline"/>                 center = TRUE,<br class="title-page-tagline"/>                 scale. = TRUE)<br class="title-page-tagline"/>                 scale. = TRUE)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The preceding function will transform the data into six orthogonal directions specified as linear combinations of features. The variance explained by each dimension can be viewed using the following script:</li>
</ol>
<pre class="calibre23">
plot(pca_obj, type = "l")
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">The preceding command will plot the variance across principal components, as shown in the following figure:<strong class="calibre1"><br class="title-page-tagline"/></strong></li>
</ol>
<div class="cdpaligncenter"><img class="image-border59" src="../images/00064.gif"/></div>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">For the occupancy dataset, the first two principal components capture the majority of the variation, and when the principal component is plotted, it shows separation between the positive and negative classes for occupancy, as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border60" src="../images/00066.gif"/></div>
<div class="packt_figref">Output from the first two principal components</div>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Let's visualize the manifold in a low dimension learned by the autoencoder. Let's use only one dimension to visualize the outcome, as follows:</li>
</ol>
<pre class="calibre23">
SAE_obj&lt;-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 1), unit.type ="tanh", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000) 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">The encoder architecture for the preceding script is shown as follows:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border61" src="../images/00115.jpeg"/></div>
<p class="calibre24">The hidden layer outcome with one latent node from the stacked autoencoder is shown as follows:</p>
<div class="packt_figref"><img class="image-border62" src="../images/00092.gif"/></div>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">The preceding graph shows that occupancy is true at the peaks of the latent variables. However, the peaks are found at different values. Let's increase the latent variables 2, as captured by PCA. The model can be developed and data can be plotted using the following script:</li>
</ol>
<pre class="calibre23">
SAE_obj&lt;-SAENET.train(X.train= subset(occupancy_train$data, select=-c(Occupancy)), n.nodes=c(4, 3, 2), unit.type ="tanh", lambda = 1e-5, beta = 1e-5, rho = 0.01, epsilon = 0.01, max.iterations=1000)  <br class="title-page-tagline"/><br class="title-page-tagline"/># plotting encoder values<br class="title-page-tagline"/>plot(SAE_obj[[3]]$X.output[,1], SAE_obj[[3]]$X.output[,2], col="blue", xlab = "Node 1 of layer 3", ylab = "Node 2 of layer 3")<br class="title-page-tagline"/>ix&lt;-occupancy_train$data[,6]==1  <br class="title-page-tagline"/>points(SAE_obj[[3]]$X.output[ix,1], SAE_obj[[3]]$X.output[ix,2], col="red")
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">The encoded values with two layers are shown in the following diagram:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border63" src="../images/00123.gif"/></div>


            </article>

            
        </section>
    

        <section id="5UNGG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Evaluating the sparse decomposition</h1>
                
            
            <article>
                
<p class="calibre2">The sparse autoencoder is also known as over-complete representation and has a higher number of nodes in the hidden layer. The sparse autoencoders are usually executed with the sparsity parameter (regularization), which acts as a constraint and restricts the node to being active. The sparsity can also be assumed as nodes dropout caused due to sparsity constraints. The loss function for a sparse autoencoder consists of a reconstruction error, a regularization term to contain the weight decay, and KL divergence for sparsity constraint. The following representation gives a very good illustration of what we are talking about:</p>
<div class="cdpaligncenter"><img src="../images/00095.jpeg" class="calibre39"/></div>


            </article>

            
        </section>
    

        <section id="5VM121-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The dataset is loaded and set up.</li>
<li value="2" class="calibre13">Install and load the <kbd class="calibre10">autoencoder</kbd> package using the following script:</li>
</ol>
<pre class="calibre23">
install.packages("autoencoder")<br class="title-page-tagline"/>require(autoencoder)
</pre>


            </article>

            
        </section>
    

        <section id="60KHK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The standard autoencoder code of TensorFlow can easily be extended to the sparse autoencoder module by updating the cost function. This section will introduce the autoencoder package of R, which comes with built-in functionality to run the sparse autoencoder:</li>
</ol>
<pre class="calibre23">
### Setting-up parameter<br class="title-page-tagline"/>nl&lt;-3 <br class="title-page-tagline"/>N.hidden&lt;-100 <br class="title-page-tagline"/>unit.type&lt;-"logistic" <br class="title-page-tagline"/>lambda&lt;-0.001 <br class="title-page-tagline"/>rho&lt;-0.01 <br class="title-page-tagline"/>beta&lt;-6 <br class="title-page-tagline"/>max.iterations&lt;-2000 <br class="title-page-tagline"/>epsilon&lt;-0.001 <br class="title-page-tagline"/><br class="title-page-tagline"/>### Running sparse autoencoder<br class="title-page-tagline"/>spe_ae_obj &lt;- autoencode(X.train=trainData,  X.test = validData, nl=nl, N.hidden=N.hidden, unit.type=unit.type,lambda=lambda,beta=beta,              epsilon=epsilon,rho=rho,max.iterations=max.iterations, rescale.flag = T)
</pre>
<p class="calibre2">The major parameters in the <kbd class="calibre10">autoencode</kbd> functions are as follows:</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">nl</kbd>: This is the number of layers including the input and output layer (the default is three).</li>
<li class="calibre13"><kbd class="calibre10">N.hidden</kbd>: This is the vector with the number of neurons in each hidden layer.</li>
<li class="calibre13"><kbd class="calibre10">unit.type</kbd>: This is the type of activation function to be used.</li>
<li class="calibre13"><kbd class="calibre10">lambda</kbd>: This is the regularization parameter.</li>
<li class="calibre13"><kbd class="calibre10">rho</kbd>: This is the sparsity parameter.</li>
<li class="calibre13"><kbd class="calibre10">beta</kbd>: This is the penalty for the sparsity term.</li>
<li class="calibre13"><kbd class="calibre10">max.iterations</kbd>: This is the maximum number of iterations.</li>
<li class="calibre13"><kbd class="calibre10">epsilon</kbd>: This is the parameter for weight initialization. The weights are initialized using Gaussian distribution ~N(0, epsilon2).</li>
</ul>


            </article>

            
        </section>
    

        <section id="61J261-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The following figure shows the shapes and orientation of digits from <kbd class="calibre10">MNIST</kbd> captured by the sparse autoencoder:</p>
<div class="cdpaligncenter"><img class="image-border64" src="../images/00138.gif"/></div>
<div class="packt_figref">Filter generated by the sparse autoencoder to get the digit outcome</div>
<p class="calibre2">The filter learned by the sparse autoencoder can be visualized using the <kbd class="calibre10">visualize.hidden.units</kbd> function from the autoencoder package. The package plots the weight of the final layer with respect to the output. In the current scenario, 100 is the number of neurons in the hidden layer and 256 is the number of nodes in the output layer.</p>


            </article>

            
        </section>
    </body></html>