["```py\nfrom utils import *\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\nimport time\n```", "```py\n#Helper function\ndef separate_dataset(trainset,ratio=0.5):\n   datastring = []\n    datatarget = []\n    for i in range(int(len(trainset.data)*ratio)):\n        data_ = trainset.data[i].split('\\n')\n        data_ = list(filter(None, data_))\n        for n in range(len(data_)):\n            data_[n] = clearstring(data_[n])\n        datastring += data_\n        for n in range(len(data_)):\n            datatarget.append(trainset.target[i])\n    return datastring, datatarget\n```", "```py\ntrainset = sklearn.datasets.load_files(container_path = './data', encoding = 'UTF-8')\ntrainset.data, trainset.target = separate_dataset(trainset,1.0)\nprint (trainset.target_names)\nprint ('No of training data' , len(trainset.data))\nprint ('No. of test data' , len(trainset.target))\n\n# Output: ['negative', 'positive']\nNo of training data 10662\nNo of test data 10662\n```", "```py\nONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\nONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\ntrain_X, test_X, train_Y, test_Y, train_onehot, test_onehot = train_test_split(trainset.data, trainset.target, \nONEHOT, test_size = 0.2)\n\nconcat = ' '.join(trainset.data).split()\nvocabulary_size = len(list(set(concat)))\ndata, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\nprint('vocab from size: %d'%(vocabulary_size))\nprint('Most common words', count[4:10])\nprint('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n\n# OUTPUT:vocab from size: 20465\n'Most common words', [(u'the', 10129), (u'a', 7312), (u'and', 6199), (u'of', 6063), (u'to', 4233), (u'is', 3378)]\n\n'Sample data': \n[4, 662, 9, 2543, 8, 22, 4, 3558, 18064, 98] --> \n[u'the', u'rock', u'is', u'destined', u'to', u'be', u'the', u'21st', u'centurys', u'new']\n```", "```py\n# Tag to mark the beginning of the sentence\n'GO' = 0th position\n# Tag to add extra padding in the sentence\n'PAD'= 1st position\n# Tag to mark the end of the sentence\n'EOS'= 2nd position\n# Tag to mark the unknown word\n'UNK'= 3rd position\n```", "```py\nsize_layer = 128\nnum_layers = 2\nembedded_size = 128\ndimension_output = len(trainset.target_names)\nlearning_rate = 1e-3\nmaxlen = 50\nbatch_size = 128\n```", "```py\nclass Model:\n    def __init__(self, size_layer, num_layers, embedded_size,\n                 dict_size, dimension_output, learning_rate):\n\n        def cells(reuse=False):\n            return tf.nn.rnn_cell.BasicRNNCell(size_layer,reuse=reuse)\n\n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n\n        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n\n        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)\n\n        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n\n        self.logits = tf.matmul(outputs[:, -1], W) + b\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n\n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(size_layer,num_layers,embedded_size,vocabulary_size+4,dimension_output,learning_rate)\nsess.run(tf.global_variables_initializer())\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n'%(EPOCH))\n        break\n\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n        train_loss += loss\n        train_acc += acc\n\n    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss = sess.run([model.accuracy, model.cost], \n                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n        test_loss += loss\n        test_acc += acc\n\n    train_loss /= (len(train_X) // batch_size)\n    train_acc /= (len(train_X) // batch_size)\n    test_loss /= (len(test_X) // batch_size)\n    test_acc /= (len(test_X) // batch_size)\n\n    if test_acc > CURRENT_ACC:\n        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n\n    print('time taken:', time.time()-lasttime)\n   print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss, train_acc,test_loss,test_acc))\n    EPOCH += 1\n\n```", "```py\nlogits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})\nprint(metrics.classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))\n```", "```py\nclass Model:\n    def __init__(self, size_layer, num_layers, embedded_size,\n                 dict_size, dimension_output, learning_rate):\n\n        def cells(reuse=False):\n            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n\n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n\n        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n\n        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)\n\n        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n\n        self.logits = tf.matmul(outputs[:, -1], W) + b\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n\n        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n'%(EPOCH))\n        break\n\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n        train_loss += loss\n        train_acc += acc\n\n    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n        acc, loss = sess.run([model.accuracy, model.cost], \n                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n        test_loss += loss\n        test_acc += acc\n\n    train_loss /= (len(train_X) // batch_size)\n    train_acc /= (len(train_X) // batch_size)\n    test_loss /= (len(test_X) // batch_size)\n    test_acc /= (len(test_X) // batch_size)\n\n    if test_acc > CURRENT_ACC:\n        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n\n    print('time taken:', time.time()-lasttime)\n    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n                                                                                          train_acc,test_loss,\n                                                                                          test_acc))\n    EPOCH += 1\n```", "```py\n('time taken:', 18.061596155166626)\nepoch: 10, training loss: 0.015714, training acc: 0.994910, valid loss: 4.252270, valid acc: 0.500000\n\n('time taken:', 17.786305904388428)\nepoch: 11, training loss: 0.011198, training acc: 0.995975, valid loss: 4.644272, valid acc: 0.502441\n\n('time taken:', 19.031064987182617)\nepoch: 12, training loss: 0.009245, training acc: 0.996686, valid loss: 4.575824, valid acc: 0.499512\n\n('time taken:', 16.996762990951538)\nepoch: 13, training loss: 0.006528, training acc: 0.997751, valid loss: 4.449901, valid acc: 0.501953\n\n('time taken:', 17.008245944976807)\nepoch: 14, training loss: 0.011770, training acc: 0.995739, valid loss: 4.282045, valid acc: 0.499023\n\nbreak epoch:15\n```", "```py\nlogits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})\nprint(metrics.classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nimport collections\nfrom utils import *\n\nfile_path = './conversation_data/'\n\nwith open(file_path+'from.txt', 'r') as fopen:\n    text_from = fopen.read().lower().split('\\n')\nwith open(file_path+'to.txt', 'r') as fopen:\n    text_to = fopen.read().lower().split('\\n')\nprint('len from: %d, len to: %d'%(len(text_from), len(text_to)))\n\nconcat_from = ' '.join(text_from).split()\nvocabulary_size_from = len(list(set(concat_from)))\ndata_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n\nconcat_to = ' '.join(text_to).split()\nvocabulary_size_to = len(list(set(concat_to)))\ndata_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n\nGO = dictionary_from['GO']\nPAD = dictionary_from['PAD']\nEOS = dictionary_from['EOS']\nUNK = dictionary_from['UNK']\n```", "```py\nclass Chatbot:\n def __init__(self, size_layer, num_layers, embedded_size,\n from_dict_size, to_dict_size, learning_rate, batch_size):\n\n def cells(reuse=False):\n return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n\n self.X = tf.placeholder(tf.int32, [None, None])\n self.Y = tf.placeholder(tf.int32, [None, None])\n self.X_seq_len = tf.placeholder(tf.int32, [None])\n self.Y_seq_len = tf.placeholder(tf.int32, [None])\n\n with tf.variable_scope(\"encoder_embeddings\"): \n encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n main = tf.strided_slice(self.X, [0, 0], [batch_size, -1], [1, 1])\n\n with tf.variable_scope(\"decoder_embeddings\"): \n decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)\n\n with tf.variable_scope(\"encoder\"):\n rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n _, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded,\n dtype = tf.float32)\n with tf.variable_scope(\"decoder\"):\n rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n outputs, _ = tf.nn.dynamic_rnn(rnn_cells_dec, decoder_embedded, \n initial_state = last_state,\n dtype = tf.float32)\n with tf.variable_scope(\"logits\"): \n self.logits = tf.layers.dense(outputs,to_dict_size)\n print(self.logits)\n masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n with tf.variable_scope(\"cost\"): \n self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.logits,\n targets = self.Y,\n weights = masks)\n with tf.variable_scope(\"optimizer\"): \n self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n```", "```py\nsize_layer = 128\nnum_layers = 2\nembedded_size = 128\nlearning_rate = 0.001\nbatch_size = 32\nepoch = 50\n```", "```py\ndef pad_sentence_batch(sentence_batch, pad_int):\n    padded_seqs = []\n    seq_lens = []\n    max_sentence_len = 50\n    for sentence in sentence_batch:\n        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n        seq_lens.append(50)\n    return padded_seqs, seq_lens\n\ndef check_accuracy(logits, Y):\n    acc = 0\n    for i in range(logits.shape[0]):\n        internal_acc = 0\n        for k in range(len(Y[i])):\n            if Y[i][k] == logits[i][k]:\n                internal_acc += 1\n        acc += (internal_acc / len(Y[i]))\n    return acc / logits.shape[0]\n```", "```py\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Chatbot(size_layer, num_layers, embedded_size, vocabulary_size_from + 4, \n                vocabulary_size_to + 4, learning_rate, batch_size)\nsess.run(tf.global_variables_initializer())\n\nfor i in range(epoch):\n total_loss, total_accuracy = 0, 0\n for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):\n batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)\n batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)\n predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer], \n feed_dict={model.X:batch_x,\n model.Y:batch_y,\n model.X_seq_len:seq_x,\n model.Y_seq_len:seq_y})\n total_loss += loss\n total_accuracy += check_accuracy(predicted,batch_y)\n total_loss /= (len(text_from) // batch_size)\n total_accuracy /= (len(text_from) // batch_size)\n print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))\n\nOUTPUT:\nepoch: 47, avg loss: 0.682934, avg accuracy: 0.000000\nepoch: 48, avg loss: 0.680367, avg accuracy: 0.000000\nepoch: 49, avg loss: 0.677882, avg accuracy: 0.000000\nepoch: 50, avg loss: 0.678484, avg accuracy: 0.000000\n.\n.\n.\nepoch: 1133, avg loss: 0.000464, avg accuracy: 1.000000\nepoch: 1134, avg loss: 0.000462, avg accuracy: 1.000000\nepoch: 1135, avg loss: 0.000460, avg accuracy: 1.000000\nepoch: 1136, avg loss: 0.000457, avg accuracy: 1.000000\n```", "```py\ndef predict(sentence):\n    X_in = []\n    for word in sentence.split():\n        try:\n            X_in.append(dictionary_from[word])\n        except:\n            X_in.append(PAD)\n            pass\n\n    test, seq_x = pad_sentence_batch([X_in], PAD)\n    input_batch = np.zeros([batch_size,seq_x[0]])\n    input_batch[0] =test[0] \n\n    log = sess.run(tf.argmax(model.logits,2), \n                                      feed_dict={\n                                              model.X:input_batch,\n                                              model.X_seq_len:seq_x,\n                                              model.Y_seq_len:seq_x\n                                              }\n                                      )\n\n    result=' '.join(rev_dictionary_to[i] for i in log[0])\n    return result\n```", "```py\n>> predict('where do you live')\n>> i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n\n>> print predict('how are you ?')\n>> i am PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n```", "```py\n>> predict('where do you live')\n>> miami florida PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n\n>> print predict('how are you ?')\n>> i am fine thank you PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n```"]