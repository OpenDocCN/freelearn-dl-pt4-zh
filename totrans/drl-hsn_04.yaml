- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cross-Entropy Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, you learned about PyTorch. In this chapter, we will wrap
    up Part 1 of this book and you will become familiar with one of the reinforcement
    learning (RL) methods: cross-entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that it is much less famous than other tools in the RL practitioner’s
    toolbox, such as deep Q-network (DQN) or advantage actor-critic (A2C), the cross-entropy
    method has its own strengths. Firstly, the cross-entropy method is really simple,
    which makes it an easy method to follow. For example, its implementation on PyTorch
    is less than 100 lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the method has good convergence. In simple environments that don’t
    require you to learn complex, multistep policies and that have short episodes
    with frequent rewards, the cross-entropy method usually works very well. Of course,
    lots of practical problems don’t fall into this category, but sometimes they do.
    In such cases, the cross-entropy method (on its own or as part of a larger system)
    can be the perfect fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The practical side of the cross-entropy method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the cross-entropy method works in two environments in Gym (the familiar
    CartPole and the grid world of FrozenLake)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theoretical background of the cross-entropy method. This section is optional
    and requires a bit of knowledge of probability and statistics, but if you want
    to understand why the method works, then you can delve into it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The taxonomy of RL methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cross-entropy method falls into the model-free, policy-based, and on-policy
    categories of methods. These notions are new, so let’s spend some time exploring
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the methods in RL can be classified into various groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Model-free or model-based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value-based or policy-based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-policy or off-policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other ways that you can taxonomize RL methods, but, for now, we are
    interested in the above three. Let’s define them, as the specifics of your problem
    can influence your choice of a particular method.
  prefs: []
  type: TYPE_NORMAL
- en: The term model-free means that the method doesn’t build a model of the environment
    or reward; it just directly connects observations to actions (or values that are
    related to actions). In other words, the agent takes current observations and
    does some computations on them, and the result is the action that it should take.
    In contrast, model-based methods try to predict what the next observation and/or
    reward will be. Based on this prediction, the agent tries to choose the best possible
    action to take, very often making such predictions multiple times to look more
    and more steps into the future.
  prefs: []
  type: TYPE_NORMAL
- en: Both classes of methods have strong and weak sides, but usually pure model-based
    methods are used in deterministic environments, such as board games with strict
    rules. On the other hand, model-free methods are usually easier to train because
    it’s hard to build good models of complex environments with rich observations.
    All of the methods described in this book are from the model-free category, as
    those methods have been the most active area of research for the past few years.
    Only recently have researchers started to mix the benefits from both worlds (for
    example, in Chapter [20](ch024.xhtml#x1-36400020), we’ll take a look at the AlphaGo
    Zero and MuZero methods, which use a model-based approach to board games and Atari).
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this from another angle, policy-based methods directly approximate
    the policy of the agent, that is, what actions the agent should carry out at every
    step. The policy is usually represented by a probability distribution over the
    available actions. Alternatively, the method could be value-based. In this case,
    instead of the probability of actions, the agent calculates the value of every
    possible action and chooses the action with the best value. These families of
    methods are equally popular, and we will discuss value-based methods in the next
    part of the book. Policy methods will be the topic of Part 3.
  prefs: []
  type: TYPE_NORMAL
- en: The third important classification of methods is on-policy versus off-policy.
    We will discuss this distinction in more depth in Parts 2 and 3 of the book, but,
    for now, it is enough to explain off-policy as the ability of the method to learn
    from historical data (obtained by a previous version of the agent, recorded by
    human demonstration, or just seen by the same agent several episodes ago). On
    the other hand, on-policy methods require fresh data for training, generated from
    the policy we’re currently updating. They cannot be trained on old historical
    data because the result of the training will be wrong. This makes such methods
    much less data-efficient (you need much more communication with the environment),
    but in some cases, this is not a problem (for example, if our environment is very
    lightweight and fast, so we can quickly interact with it).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our cross-entropy method is model-free, policy-based, and on-policy, which
    means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t build a model of the environment; it just says to the agent what
    to do at every step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It approximates the policy of the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires fresh data obtained from the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cross-entropy method in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The explanation of the cross-entropy method can be split into two unequal parts:
    practical and theoretical. The practical part is intuitive in nature, while the
    theoretical explanation of why the cross-entropy method works and what happens,
    is more sophisticated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may remember that the central and trickiest thing in RL is the agent, which
    tries to accumulate as much total reward as possible by communicating with the
    environment. In practice, we follow a common machine learning (ML) approach and
    replace all of the complications of the agent with some kind of nonlinear trainable
    function, which maps the agent’s input (observations from the environment) to
    some output. The details of the output that this function produces may depend
    on a particular method or a family of methods (such as value-based or policy-based
    methods), as described in the previous section. As our cross-entropy method is
    policy-based, our nonlinear function (neural network (NN)) produces the policy,
    which basically says for every observation which action the agent should take.
    In research papers, policy is denoted as π(a|s), where a are actions and s is
    the current state. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SaTmrpalienabalcteion EfOPAnubocvnaslticeiirt∼rcooivynnoπamn(tπae (ai(nN|oatNsn|s)))s
    ](img/B22150_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: A high-level approach to policy-based RL'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the policy is usually represented as a probability distribution
    over actions, which makes it very similar to a classification problem, with the
    number of classes being equal to the number of actions we can carry out.
  prefs: []
  type: TYPE_NORMAL
- en: 'This abstraction makes our agent very simple: it needs to pass an observation
    from the environment to the NN, get a probability distribution over actions, and
    perform random sampling using the probability distribution to get an action to
    carry out. This random sampling adds randomness to our agent, which is a good
    thing because at the beginning of the training, when our weights are random, the
    agent behaves randomly. As soon as the agent gets an action to issue, it fires
    the action to the environment and obtains the next observation and reward for
    the last action. Then the loop continues, as shown in Figure [4.1](#x1-76002r1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the agent’s lifetime, its experience is presented as episodes. Every
    episode is a sequence of observations that the agent has got from the environment,
    actions it has issued, and rewards for these actions. Imagine that our agent has
    played several such episodes. For every episode, we can calculate the total reward
    that the agent has claimed. It can be discounted or not discounted; for simplicity,
    let’s assume a discount factor of γ = 1, which just means an undiscounted sum
    of all local rewards for every episode. This total reward shows how good this
    episode was for the agent. It is illustrated in Figure [4.2](#x1-76004r2), which
    contains four episodes (note that different episodes have different values for
    o[i], a[i], and r[i]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Episode 1: o1,a1,r1 o2,a2,r2 o3,a3,r3 o4,a4,rR4= r1 + r2 + r3 + r4 Episode
    2: o1,a1,r1 o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 Episode 3: o1,a1,r1 o2,a2,r2 R
    = r1 + r2 Episode 4: o1,a1,r1 o2,a2,r2 o3,a3,r3 R = r1 + r2 + r3 ](img/B22150_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Example episodes with their observations, actions, and rewards'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every cell represents the agent’s step in the episode. Due to randomness in
    the environment and the way that the agent selects actions to take, some episodes
    will be better than others. The core of the cross-entropy method is to throw away
    bad episodes and train on better ones. So, the steps of the method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Play N episodes using our current model and environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the total reward for every episode and decide on a reward boundary.
    Usually, we use a percentile of all rewards, such as the 50th or 70th.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Throw away all episodes with a reward below the boundary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the remaining ”elite” episodes (with rewards higher than the boundary)
    using observations as the input and issued actions as the desired output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 1 until we become satisfied with the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, that’s the cross-entropy method’s description. With the preceding procedure,
    our NN learns how to repeat actions, which leads to a larger reward, constantly
    moving the boundary higher and higher. Despite the simplicity of this method,
    it works well in basic environments, it’s easy to implement, and it’s quite robust
    against changing hyperparameters, which makes it an ideal baseline method to try.
    Let’s now apply it to our CartPole environment.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-entropy method on CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole code for this example is in Chapter04/01_cartpole.py. Here, I will
    show only the most important parts. Our model’s core is a one-hidden-layer NN,
    with rectified linear unit (ReLU) and 128 hidden neurons (which is absolutely
    arbitrary; you can try to increase or decrease this constant – we’ve left this
    as an exercise for you). Other hyperparameters are also set almost randomly and
    aren’t tuned, as the method is robust and converges very quickly. We define constants
    at the top of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the constants include the count of neurons in
    the hidden layer, the count of episodes we play on every iteration (16), and the
    percentile of each episode’s total rewards that we use for elite episode filtering.
    We will take the 70th percentile, which means that we will keep the top 30% of
    episodes sorted by reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is nothing special about our NN; it takes a single observation from the
    environment as an input vector and outputs a number for every action we can perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output from the NN is a probability distribution over actions, so a straightforward
    way to proceed would be to include softmax nonlinearity after the last layer.
    However, in the code, we don’t apply softmax to increase the numerical stability
    of the training process. Rather than calculating softmax (which uses exponentiation)
    and then calculating cross-entropy loss (which uses a logarithm of probabilities),
    we will use the nn.CrossEntropyLoss PyTorch class later, which combines softmax
    and cross-entropy into a single, more numerically stable expression. CrossEntropyLoss
    requires raw, unnormalized values from the NN (also called logits). The downside
    of this is that we need to remember to apply softmax every time we need to get
    probabilities from our NN’s output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define two helper dataclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of these dataclasses is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'EpisodeStep: This will be used to represent one single step that our agent
    made in the episode, and it stores the observation from the environment and what
    action the agent performed. We will use episode steps from elite episodes as training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Episode: This is a single episode stored as a total undiscounted reward and
    a collection of EpisodeStep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at a function that generates batches with episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function accepts the environment (the Env class instance from
    the Gym library), our NN, and the count of episodes it should generate on every
    iteration. The batch variable will be used to accumulate our batch (which is a
    list of Episode instances). We also declare a reward counter for the current episode
    and its list of steps (the EpisodeStep objects). Then we reset our environment
    to obtain the first observation and create a softmax layer, which will be used
    to convert the NN’s output to a probability distribution of actions. That’s our
    preparations complete, so we are ready to start the environment loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At every iteration, we convert our current observation to a PyTorch tensor
    and pass it to the NN to obtain action probabilities. There are several things
    to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: All nn.Module instances in PyTorch expect a batch of data items, and the same
    is true for our NN, so we convert our observation (which is a vector of four numbers
    in CartPole) into a tensor of size 1 × 4 (to achieve this, we call the unsqueeze(0)
    function on our tensor, which adds an extra dimension at the zero position of
    the shape).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we haven’t used nonlinearity at the output of our NN, it outputs raw action
    scores, which we need to feed through the softmax function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both our NN and the softmax layer return tensors that track gradients, so we
    need to unpack this by accessing the tensor.data field and then converting the
    tensor into a NumPy array. This array will have the same two-dimensional structure
    as the input, with the batch dimension on axis 0, so we need to get the first
    batch element to obtain a one-dimensional vector of action probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have the probability distribution of actions, we can use it to
    obtain the actual action for the current step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we sample the distribution using NumPy’s function random.choice(). After
    this, we will pass this action to the environment to get our next observation,
    our reward, the indication of the episode ending, and the truncation flag. The
    last value returned by the step() function is extra information from the environment
    and is discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reward is added to the current episode’s total reward, and our list of
    episode steps is also extended with an (observation, action) pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that we save the observation that was used to choose the action, but not
    the observation returned by the environment as a result of the action. These are
    the tiny, but important, details that you need to keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'The continuation of the code handles the situation when the current episode
    is over (in the case of CartPole, the episode ends when the stick has fallen down,
    despite our efforts, or when the time limit of the environment has been reached):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We append the finalized episode to the batch, saving the total reward (as the
    episode has been completed and we have accumulated all the rewards) and steps
    we have taken. Then we reset our total reward accumulator and clean the list of
    steps. After that, we reset our environment to start over.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our batch has reached the desired count of episodes, we return it to the
    caller for processing using yield. Our function is a generator, so every time
    the yield operator is executed, the control is transferred to the outer iteration
    loop and then continues after the yield line. If you are not familiar with Python’s
    generator functions, refer to the Python documentation: [https://wiki.python.org/moin/Generators](https://wiki.python.org/moin/Generators).
    After processing, we will clean up the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last, but very important, step in our loop is to assign an observation
    obtained from the environment to our current observation variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After that, everything repeats infinitely – we pass the observation to the NN,
    sample the action to perform, ask the environment to process the action, and remember
    the result of this processing.
  prefs: []
  type: TYPE_NORMAL
- en: One very important fact to understand in this function’s logic is that the training
    of our NN and the generation of our episodes are performed at the same time. They
    are not completely in parallel, but every time our loop accumulates enough episodes
    (16), it passes control to this function caller, which is supposed to train the
    NN using gradient descent. So, when yield is returned, the NN will have different,
    slightly better (we hope) behavior. As you should remember from the beginning
    of the chapter, the cross-entropy method is from the on-policy class, so using
    fresh training data is important for the method to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since training and data gathering happen in the same thread, proper synchronization
    isn’t necessary. However, you should be aware of the frequent jumps between training
    the NN and using it. Okay; now we need to define yet another function, and then
    we will be ready to switch to the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This function is at the core of the cross-entropy method – from the given batch
    of episodes and percentile value, it calculates a boundary reward, which is used
    to filter elite episodes to train on. To obtain the boundary reward, we will use
    NumPy’s percentile function, which, from the list of values and the desired percentile,
    calculates the percentile’s value. Then, we will calculate the mean reward, which
    is used only for monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will filter off our episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For every episode in the batch, we will check that the episode has a higher
    total reward than our boundary and, if it has, we will populate lists of observations
    and actions that we will train on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the final step of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will convert our observations and actions from elite episodes into
    tensors, and return a tuple of four: observations, actions, the boundary of reward,
    and the mean reward. The last two values are not used in the training; we will
    write them into TensorBoard to check the performance of our agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the final chunk of code that glues everything together, and mostly consists
    of the training loop, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the beginning, we create all the required objects: the environment, our
    NN, the objective function, the optimizer, and the summary writer for TensorBoard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the training loop, we iterate our batches (a list of Episode objects):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We perform filtering of the elite episodes using the filter_batch function.
    The result is tensors of observations and taken actions, the reward boundary used
    for filtering, and the mean reward. After that, we zero the gradients of our NN
    and pass observations to the NN, obtaining its action scores. These scores are
    passed to the objective function, which will calculate the cross-entropy between
    the NN output and the actions that the agent took. The idea of this is to reinforce
    our NN to carry out those elite actions that have led to good rewards. Then, we
    calculate gradients on the loss and ask the optimizer to adjust our NN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the loop is mostly the monitoring of progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: On the console, we show the iteration number, the loss, the mean reward of the
    batch, and the reward boundary. We also write the same values to TensorBoard to
    get a nice chart of the agent’s learning performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last check in the loop is the comparison of the mean rewards of our batch
    episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When the mean reward becomes greater than 475, we stop our training. Why 475?
    In Gym, the CartPole-v1 environment is considered to be solved when the mean reward
    for the last 100 episodes is greater than 475\. However, our method converges
    so quickly that 100 episodes are usually what we need. The properly trained agent
    can balance the stick for an infinitely long period of time (obtaining any amount
    of score), but the length of an episode in CartPole-v1 is limited to 500 steps
    (if you look in [https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/\_\_init\_\_.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/)(gymnasium/envs/__init__.py)
    where all the environments are registered, v1 of Cartpole has max_episode_steps=500).
    With all this in mind, we will stop training after the mean reward in the batch
    is greater than 475, which is a good indication that our agent knows how to balance
    the stick like a pro.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it. So, let’s start our first RL training!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It usually doesn’t take the agent more than 50 batches to solve the problem.
    My experiments show something from 30 to 60 episodes, which is a really good learning
    performance (remember, we need to play only 16 episodes for every batch). TensorBoard
    shows that our agent is consistently making progress, pushing the upper boundary
    at almost every batch (there are some periods of rolling down, but most of the
    time it improves):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Mean reward (left) and loss (right) during the training![PIC](img/B22150_04_04.png)
    Figure 4.4: The reward boundary during the training'
  prefs: []
  type: TYPE_NORMAL
- en: 'To monitor the training process, you can tweak the environment creation by
    setting the rendering mode in the CartPole environment and adding a RecordVideo
    wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'During the training, it will create a video directory with a bunch of MP4 movies
    inside, allowing you to compare the progress of agent training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The MP4 movies might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: CartPole episode movie'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now pause for a bit and think about what’s just happened. Our NN has learned
    how to play the environment purely from observations and rewards, without any
    interpretation of observed values. The environment could easily not be a cart
    with a stick; it could be, say, a warehouse model with product quantities as an
    observation and money earned as the reward. Our implementation doesn’t depend
    on environment-related details. This is the beauty of the RL model and, in the
    next section, we will look at how exactly the same method can be applied to a
    different environment from the Gym collection.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-entropy method on FrozenLake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next environment that we will try to solve using the cross-entropy method
    is FrozenLake. Its world is from the so-called grid world category, when your
    agent lives in a grid of size 4 × 4 and can move in four directions: up, down,
    left, and right. The agent always starts at the top left, and its goal is to reach
    the bottom-right cell of the grid. There are holes in the fixed cells of the grid
    and if you get into those holes, the episode ends and your reward is zero. If
    the agent reaches the destination cell, then it obtains a reward of 1.0 and the
    episode ends.'
  prefs: []
  type: TYPE_NORMAL
- en: To make life more complicated, the world is slippery (it’s a frozen lake after
    all), so the agent’s actions do not always turn out as expected – there is a 33%
    chance that it will slip to the right or to the left. If you want the agent to
    move left, for example, there is a 33% probability that it will, indeed, move
    left, a 33% chance that it will end up in the cell above, and a 33% chance that
    it will end up in the cell below. As you will see at the end of the section, this
    makes progress difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: The FrozenLake environment rendered in human mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how this environment is represented in the Gym API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our observation space is discrete, which means that it’s just a number from
    0 to 15 inclusive. Obviously, this number is our current position in the grid.
    The action space is also discrete, but it can be from zero to three. Although
    the action space is similar to CartPole, the observation space is represented
    in a different way. To minimize the required changes in our implementation, we
    can apply the traditional one-hot encoding of discrete inputs, which means that
    the input to our network will have 16 float numbers and zero everywhere, except
    the index that we will encode (representing our current position on the grid).
  prefs: []
  type: TYPE_NORMAL
- en: 'As this transformation only affects the observation of the environment, it
    could be implemented as an ObservationWrapper, as we discussed in Chapter [2](ch006.xhtml#x1-380002).
    Let’s call it DiscreteOneHotWrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With that wrapper applied to the environment, both the observation space and
    action space are 100% compatible with our CartPole solution (source code Chapter04/02_frozenlake_naive.py).
    However, by launching it, we can see that our training process doesn’t improve
    the score over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Mean reward (left) and loss (right) on the FrozenLake environment'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: The reward boundary during the training (boring 0.0 all the time)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what’s going on, we need to look deeper at the reward structure
    of both environments. In CartPole, every step of the environment gives us the
    reward 1.0, until the moment that the pole falls. So, the longer our agent balanced
    the pole, the more reward it obtained. Due to randomness in our agent’s behavior,
    different episodes were of different lengths, which gave us a pretty normal distribution
    of the episodes’ rewards. After choosing a reward boundary, we rejected less successful
    episodes and learned how to repeat better ones (by training on successful episodes’
    data). This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Distribution of the reward in the CartPole environment'
  prefs: []
  type: TYPE_NORMAL
- en: In the FrozenLake environment, episodes and their rewards look different. We
    get the reward of 1.0 only when we reach the goal, and this reward says nothing
    about how good each episode was. Was it quick and efficient, or did we make four
    rounds on the lake before we randomly stepped into the final cell? We don’t know;
    it’s just a 1.0 reward and that’s it. The distribution of rewards for our episodes
    is also problematic. There are only two kinds of episodes possible, with zero
    reward (failed) and one reward (successful), and failed episodes will obviously
    dominate at the beginning of the training, when the agent acts randomly. So, our
    percentile selection of elite episodes is totally wrong and gives us bad examples
    to train on. This is the reason for our training failure.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Reward distribution of the FrozenLake environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example shows us the limitations of the cross-entropy method:'
  prefs: []
  type: TYPE_NORMAL
- en: For training, our episodes have to be finite (in general, they could be infinite)
    and, preferably, short
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total reward for the episodes should have enough variability to separate
    good episodes from bad ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is beneficial to have an intermediate reward during the episode instead of
    having the reward at the end of the episode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Later in the book, you will become familiar with other methods that address
    these limitations. For now, if you are curious about how FrozenLake can be solved
    using the cross-entropy method, here is a list of tweaks of the code that you
    need to make (the full example is in Chapter04/03_frozenlake_tweaked.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Larger batches of played episodes: In CartPole, it was sufficient to have 16
    episodes on every iteration, but FrozenLake requires at least 100 just to get
    some successful episodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discount factor applied to the reward: To make the total reward for an episode
    depend on its length, and to add variety in episodes, we can use a discounted
    total reward with the discount factor γ = 0.9 or 0.95\. In this case, the reward
    for short episodes will be higher than the reward for long ones. This increases
    variability in reward distribution, which helps to avoid situations like the one
    shown in Figure [4.10](#x1-78034r10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keeping elite episodes for a longer time: In the CartPole training, we sampled
    episodes from the environment, trained on the best ones, and threw them away.
    In FrozenLake, a successful episode is a much rarer animal, so we need to keep
    them for several iterations to train on them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decreasing the learning rate: This will give our NN time to average more training
    samples, as a smaller learning rate decreases the effect of new data on the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Much longer training time: Due to the sparsity of successful episodes and the
    random outcome of our actions, it’s much harder for our NN to get an idea of the
    best behavior to perform in any particular situation. To reach 50% successful
    episodes, about 5,000 training iterations are required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To incorporate all these into our code, we need to change the filter_batch
    function to calculate the discounted reward and return elite episodes for us to
    keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in the training loop, we will store previous elite episodes to pass them
    to the preceding function on the next training iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the code is the same, except that the learning rate decreased 10
    times and the BATCH_SIZE was set to 100\. After a period of patient waiting (the
    new version takes about 50 minutes to finish 10,000 iterations), you can see that
    the training of the model stopped improving at around 55% of solved episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Mean reward (left) and loss (right) of the tweaked version'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: The reward boundary of the tweaked version'
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to address this (by applying entropy loss regularization, for
    example), but those techniques will be discussed in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The final point to note here is the effect of slipperiness in the FrozenLake
    environment. Each of our actions, with 33% probability, is replaced with the 90^∘
    rotated action (the up action, for instance, will succeed with a 0.33 probability,
    and there will be a 0.33 chance that it will be replaced with the left action
    and a 0.33 chance with the right action).
  prefs: []
  type: TYPE_NORMAL
- en: 'The nonslippery version is in Chapter04/04_frozenlake_nonslippery.py, and the
    only difference is in the environment creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect is dramatic! The nonslippery version of the environment can be solved
    in 120-140 batch iterations, which is 100 times faster than the noisy environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This is also evident from the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Mean reward (left) and loss (right) of the nonslippery version'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: The reward boundary of the nonslippery version'
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical background of the cross-entropy method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is optional and is included for readers who want to understand
    why the method works. If you wish, you can refer to the original paper by Kroese,
    titled Cross-entropy method, [[Kro+11](#)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The basis of the cross-entropy method lies in the importance sampling theorem,
    which states this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq4.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our RL case, H(x) is a reward value obtained by some policy x, and p(x)
    is a distribution of all possible policies. We don’t want to maximize our reward
    by searching all possible policies; instead, we want to find a way to approximate
    p(x)H(x) by q(x), iteratively minimizing the distance between them. The distance
    between two probability distributions is calculated by Kullback-Leibler (KL) divergence,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq6.png) ![π (a |s) = P[At = a|St = s]
    ](img/eq7.png)'
  prefs: []
  type: TYPE_IMG
- en: The first term in KL is called entropy and it doesn’t depend on p[2](x), so
    it could be omitted during the minimization. The second term is called cross-entropy,
    which is a very common optimization objective in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining both formulas, we can get an iterative algorithm, which starts with
    q[0](x) = p(x) and on every step improves. This is an approximation of p(x)H(x)
    with an update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a generic cross-entropy method that can be significantly simplified
    in our RL case. We replace our H(x) with an indicator function, which is 1 when
    the reward for the episode is above the threshold and 0 when the reward is below.
    Our policy update will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![π (a |s) = P[At = a|St = s] ](img/eq79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Strictly speaking, the preceding formula misses the normalization term, but
    it still works in practice without it. So, the method is quite clear: we sample
    episodes using our current policy (starting with some random initial policy) and
    minimize the negative log likelihood of the most successful samples and our policy.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, refer to the book written by Reuven Rubinstein and Dirk
    P. Kroese [[RK04](#)] that is dedicated to this method. A shorter description
    can be found in the Cross-entropy method paper ([[Kro+11](#)]).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you became familiar with the cross-entropy method, which is
    simple but quite powerful, despite its limitations. We applied it to a CartPole
    environment (with huge success) and to FrozenLake (with much more modest success).
    In addition, we discussed the taxonomy of RL methods, which will be referenced
    many times during the rest of the book, as different approaches to RL problems
    have different properties, which influences their applicability.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter ends the introductory part of the book. In the next part, we will
    switch to a more systematic study of RL methods and discuss the value-based family
    of methods. In upcoming chapters, we will explore more complex, but more powerful,
    tools of deep RL.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
- en: Part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Value-based methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
