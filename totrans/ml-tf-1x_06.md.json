["```py\n      $ brew install bazel\n```", "```py\n $ sudo apt-get install swig\n```", "```py\n      $ brew install swig\n```", "```py\n      $ pip install -U protobuf==3.3.0\n```", "```py\n      $ pip install asciitree\n```", "```py\n      $ pip install numpy autograd \n```", "```py\n$ git clone --recursive https://github.com/tensorflow/models.git\n$ cd models/research/syntaxnet/tensorflow\n$ ./configure\n```", "```py\n$ cd ..\n$ bazel test ...\n```", "```py\n      $ bazel clean --expunge\n```", "```py\n test --test_output=errors\n```", "```py\n$ echo 'Faaris likes to feed the kittens.' | bash  \n./syntaxnet/demo.sh\n```", "```py\n    I syntaxnet/term_frequency_map.cc:101] Loaded 46 terms from \n syntaxnet/models/parsey_mcparseface/label-map.\n    I syntaxnet/embedding_feature_extractor.cc:35] Features: input.digit \n input.hyphen; input.prefix(length=\"2\") input(1).prefix(length=\"2\") \n input(2).prefix(length=\"2\") input(3).prefix(length=\"2\") input(-\n 1).prefix(length=\"2\")...\n    I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: \n other;prefix2;prefix3;suffix2;suffix3;words\n    I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: \n 8;16;16;16;16;64\n    I syntaxnet/term_frequency_map.cc:101] Loaded 46 terms from \n syntaxnet/models/parsey_mcparseface/label-map.\n    I syntaxnet/embedding_feature_extractor.cc:35] Features: \n stack.child(1).label stack.child(1).sibling(-1).label stack.child(-\n 1)....\n    I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: \n labels;tags;words\n    I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: \n 32;32;64\n    I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from \n syntaxnet/models/parsey_mcparseface/tag-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 64036 terms from \n syntaxnet/models/parsey_mcparseface/word-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 64036 terms from \n syntaxnet/models/parsey_mcparseface/word-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from \n syntaxnet/models/parsey_mcparseface/tag-map.\n    INFO:tensorflow:Building training network with parameters: \n feature_sizes: [12 20 20] domain_sizes: [   49    51 64038]\n    INFO:tensorflow:Building training network with parameters: \n feature_sizes: [2 8 8 8 8 8] domain_sizes: [    5 10665 10665  8970  \n 8970 64038]\n    I syntaxnet/term_frequency_map.cc:101] Loaded 46 terms from \n syntaxnet/models/parsey_mcparseface/label-map.\n    I syntaxnet/embedding_feature_extractor.cc:35] Features: \n stack.child(1).label stack.child(1).sibling(-1).label stack.child(-\n 1)....\n    I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: \n labels;tags;words\n    I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: \n 32;32;64\n    I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from \n syntaxnet/models/parsey_mcparseface/tag-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 64036 terms from \n syntaxnet/models/parsey_mcparseface/word-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 49 terms from \n syntaxnet/models/parsey_mcparseface/tag-map.\n    I syntaxnet/term_frequency_map.cc:101] Loaded 46 terms from \n syntaxnet/models/parsey_mcparseface/label-map.\n    I syntaxnet/embedding_feature_extractor.cc:35] Features: input.digit \n input.hyphen; input.prefix(length=\"2\") input(1).prefix(length=\"2\") \n input(2).prefix(length=\"2\") input(3).prefix(length=\"2\") input(-\n 1).prefix(length=\"2\")...\n    I syntaxnet/embedding_feature_extractor.cc:36] Embedding names: \n other;prefix2;prefix3;suffix2;suffix3;words\n    I syntaxnet/embedding_feature_extractor.cc:37] Embedding dims: \n 8;16;16;16;16;64\n    I syntaxnet/term_frequency_map.cc:101] Loaded 64036 terms from \n syntaxnet/models/parsey_mcparseface/word-map.\n    INFO:tensorflow:Processed 1 documents\n    INFO:tensorflow:Total processed documents: 1\n    INFO:tensorflow:num correct tokens: 0\n    INFO:tensorflow:total tokens: 7\n    INFO:tensorflow:Seconds elapsed in evaluation: 0.12, eval metric: \n 0.00%\n    INFO:tensorflow:Processed 1 documents\n    INFO:tensorflow:Total processed documents: 1\n    INFO:tensorflow:num correct tokens: 1\n    INFO:tensorflow:total tokens: 6\n    INFO:tensorflow:Seconds elapsed in evaluation: 0.47, eval metric: \n 16.67%\n    INFO:tensorflow:Read 1 documents\n    Input: Faaris likes to feed the kittens .\n    Parse:\n    likes VBZ ROOT\n     +-- Faaris NNP nsubj\n     +-- feed VB xcomp\n     |   +-- to TO aux\n     |   +-- kittens NNS dobj\n     |       +-- the DT det\n     +-- . . punct\n```", "```py\nInput: Stop speaking so loudly and be quiet !\nParse:\nStop VB ROOT\n+-- speaking VBG xcomp\n|   +-- loudly RB advmod\n|       +-- so RB advmod\n|       +-- and CC cc\n|       +-- quiet JJ conj\n|           +-- be VB cop\n+-- ! . punct\n```", "```py\n input { \n  name: 'wsj-data' \n  record_format: 'conll-sentence' \n  Part { \n    file_pattern: './wsj.conll' \n   } \n } \n input { \n  name: 'wsj-data-tagged' \n  record_format: 'conll-sentence' \n  Part { \n    file_pattern: './wsj-tagged.conll' \n   } \n } \n```", "```py\n insomnia loses points when it surrenders to a formulaic bang-bang , \n shoot-em-up scene at the conclusion . but the performances of pacino \n , williams , and swank keep the viewer wide-awake all the way through \n .\n    what might have been readily dismissed as the tiresome rant of an \n aging filmmaker still thumbing his nose at convention takes a \n surprising , subtle turn at the midway point .\n    at a time when commercialism has squeezed the life out of whatever \n idealism american moviemaking ever had , godfrey reggio's career \n shines like a lonely beacon .\n    an inuit masterpiece that will give you goosebumps as its uncanny \n tale of love , communal discord , and justice unfolds .\n    this is popcorn movie fun with equal doses of action , cheese , ham \n and cheek ( as well as a serious debt to the road warrior ) , but it \n feels like unrealized potential\n    it's a testament to de niro and director michael caton-jones that by \n movie's end , we accept the characters and the film , flaws and all .\n    performances are potent , and the women's stories are ably intercut \n and involving .\n    an enormously entertaining movie , like nothing we've ever seen \n before , and yet completely familiar .\n    lan yu is a genuine love story , full of traditional layers of \n awakening and ripening and separation and recovery .\n    your children will be occupied for 72 minutes .\n    pull[s] off the rare trick of recreating not only the look of a \n certain era , but also the feel .\n    twohy's a good yarn-spinner , and ultimately the story compels .\n    'tobey maguire is a poster boy for the geek generation . '\n     . . . a sweetly affecting story about four sisters who are coping , \n in one way or another , with life's endgame .\n    passion , melodrama , sorrow , laugther , and tears cascade over the \n screen effortlessly . . .\n    road to perdition does display greatness , and it's worth seeing . \n but it also comes with the laziness and arrogance of a thing that \n already knows it's won .\n```", "```py\n$ ./train.py\n...\n2017-06-15T04:42:08.793884: step 30101, loss 0, acc 1\n2017-06-15T04:42:08.934489: step 30102, loss 1.54599e-07, acc 1\n2017-06-15T04:42:09.082239: step 30103, loss 3.53902e-08, acc 1\n2017-06-15T04:42:09.225435: step 30104, loss 0, acc 1\n2017-06-15T04:42:09.369348: step 30105, loss 2.04891e-08, acc 1\n2017-06-15T04:42:09.520073: step 30106, loss 0.0386909, acc \n0.984375\n2017-06-15T04:42:09.676975: step 30107, loss 8.00917e-07, acc 1\n2017-06-15T04:42:09.821703: step 30108, loss 7.83049e-06, acc 1\n...\n2017-06-15T04:42:23.220202: step 30199, loss 1.49012e-08, acc 1\n2017-06-15T04:42:23.366740: step 30200, loss 5.67226e-05, acc 1\n\nEvaluation:\n2017-06-15T04:42:23.781196: step 30200, loss 9.74802, acc 0.721\n...\nSaved model checkpoint to /Users/saif/Documents/BOOK/cnn-text-\nclassification-tf/runs/1465950150/checkpoints/model-30200\n```", "```py\n$ ./eval.py â€“eval_train --checkpoint_dir==./runs/1465950150/checkpoints/\n\nParameters:\nALLOW_SOFT_PLACEMENT=True\nBATCH_SIZE=64\nCHECKPOINT_DIR=/Users/saif/Documents/BOOK/cnn-text-classification-\ntf/runs/1465950150/checkpoints/\nLOG_DEVICE_PLACEMENT=False\n\nLoading data...\nVocabulary size: 18765\nTest set size 10662\nEvaluating...\nTotal number of test examples: 10662\nAccuracy: 0.973832 \n```"]