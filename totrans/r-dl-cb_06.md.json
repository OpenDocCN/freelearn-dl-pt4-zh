["```py\nlibrary(tensorflow) \n\n```", "```py\n# Load mnist dataset from tensorflow library \ndatasets <- tf$contrib$learn$datasets \nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE) \n\n```", "```py\n# Reset the graph and set-up a interactive session \ntf$reset_default_graph() \nsess<-tf$InteractiveSession() \n\n```", "```py\n# Covert train data to 16 x 16  pixel image \ntrainData<-t(apply(mnist$train$images, 1, FUN=reduceImage)) \nvalidData<-t(apply(mnist$test$images, 1, FUN=reduceImage)) \n\n```", "```py\nlabels <- mnist$train$labels \nlabels_valid <- mnist$test$labels\n\n```", "```py\n# Define Model parameter \nn_input<-16 \nstep_size<-16 \nn.hidden<-64 \nn.class<-10 \n\n```", "```py\nlr<-0.01 \nbatch<-500 \niteration = 100 \n\n```", "```py\n# Set up a most basic RNN \nrnn<-function(x, weight, bias){ \n  # Unstack input into step_size \n  x = tf$unstack(x, step_size, 1) \n\n  # Define a most basic RNN  \n  rnn_cell = tf$contrib$rnn$BasicRNNCell(n.hidden) \n\n  # create a Recurrent Neural Network \n  cell_output = tf$contrib$rnn$static_rnn(rnn_cell, x, dtype=tf$float32) \n\n  # Linear activation, using rnn inner loop  \n  last_vec=tail(cell_output[[1]], n=1)[[1]] \n  return(tf$matmul(last_vec, weights) + bias) \n} \nDefine a function eval_func to evaluate mean accuracy using actual (y) and predicted labels (yhat): \n# Function to evaluate mean accuracy \neval_acc<-function(yhat, y){ \n  # Count correct solution \n  correct_Count = tf$equal(tf$argmax(yhat,1L), tf$argmax(y,1L)) \n\n  # Mean accuracy \n  mean_accuracy = tf$reduce_mean(tf$cast(correct_Count, tf$float32)) \n\n  return(mean_accuracy) \n}\n\n```", "```py\nwith(tf$name_scope('input'), { \n# Define placeholder for input data \nx = tf$placeholder(tf$float32, shape=shape(NULL, step_size, n_input), name='x') \ny <- tf$placeholder(tf$float32, shape(NULL, n.class), name='y') \n\n# Define Weights and bias \nweights <- tf$Variable(tf$random_normal(shape(n.hidden, n.class))) \nbias <- tf$Variable(tf$random_normal(shape(n.class))) \n}) \n\n```", "```py\n# Evaluate rnn cell output \nyhat = rnn(x, weights, bias) \nDefine the loss function and optimizer \ncost = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(logits=yhat, labels=y)) \noptimizer = tf$train$AdamOptimizer(learning_rate=lr)$minimize(cost) \n\n```", "```py\nsess$run(tf$global_variables_initializer()) \nfor(i in 1:iteration){ \n  spls <- sample(1:dim(trainData)[1],batch) \n  sample_data<-trainData[spls,] \n  sample_y<-labels[spls,] \n\n  # Reshape sample into 16 sequence with each of 16 element \n  sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input)) \n  out<-optimizer$run(feed_dict = dict(x=sample_data$eval(), y=sample_y)) \n\n  if (i %% 1 == 0){ \n    cat(\"iteration - \", i, \"Training Loss - \",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), \"\\n\") \n  } \n} \n\n```", "```py\nvalid_data=tf$reshape(validData, shape(-1, step_size, n_input)) \ncost$eval(feed_dict=dict(x=valid_data$eval(), y=labels_valid)) \n\n```", "```py\nlibrary(tensorflow) \n\n```", "```py\n# Reset the graph and set-up a interactive session \ntf$reset_default_graph() \nsess<-tf$InteractiveSession() \n\n```", "```py\n# Covert train data to 16 x 16  pixel image \ntrainData<-t(apply(mnist$train$images, 1, FUN=reduceImage)) \nvalidData<-t(apply(mnist$test$images, 1, FUN=reduceImage)) \n\n```", "```py\nlabels <- mnist$train$labels \nlabels_valid <- mnist$test$labels \n\n```", "```py\n# Define Model parameter \nn_input<-16 \nstep_size<-16 \nn.hidden<-64 \nn.class<-10 \n\n```", "```py\nlr<-0.01 \nbatch<-500 \niteration = 100 \n\n```", "```py\nbidirectionRNN<-function(x, weights, bias){ \n  # Unstack input into step_size \n  x = tf$unstack(x, step_size, 1) \n\n  # Forward lstm cell \n  rnn_cell_forward = tf$contrib$rnn$BasicRNNCell(n.hidden) \n\n  # Backward lstm cell \n  rnn_cell_backward = tf$contrib$rnn$BasicRNNCell(n.hidden) \n\n  # Get lstm cell output \n  cell_output = tf$contrib$rnn$static_bidirectional_rnn(rnn_cell_forward, rnn_cell_backward, x, dtype=tf$float32) \n\n  # Linear activation, using rnn inner loop last output \n  last_vec=tail(cell_output[[1]], n=1)[[1]] \n  return(tf$matmul(last_vec, weights) + bias) \n} \n\n```", "```py\n# Function to evaluate mean accuracy \neval_acc<-function(yhat, y){ \n  # Count correct solution \n  correct_Count = tf$equal(tf$argmax(yhat,1L), tf$argmax(y,1L)) \n\n  # Mean accuracy \n  mean_accuracy = tf$reduce_mean(tf$cast(correct_Count, tf$float32)) \n\n  return(mean_accuracy) \n} \n\n```", "```py\nwith(tf$name_scope('input'), { \n# Define placeholder for input data \nx = tf$placeholder(tf$float32, shape=shape(NULL, step_size, n_input), name='x') \ny <- tf$placeholder(tf$float32, shape(NULL, n.class), name='y') \n\n# Define Weights and bias \nweights <- tf$Variable(tf$random_normal(shape(n.hidden, n.class))) \nbias <- tf$Variable(tf$random_normal(shape(n.class))) \n}) \n\n```", "```py\n# Evaluate rnn cell output \nyhat = bidirectionRNN(x, weights, bias) \n\n```", "```py\ncost = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(logits=yhat, labels=y)) \noptimizer = tf$train$AdamOptimizer(learning_rate=lr)$minimize(cost) \n\n```", "```py\nsess$run(tf$global_variables_initializer()) \n# Running optimization \nfor(i in 1:iteration){ \n  spls <- sample(1:dim(trainData)[1],batch) \n  sample_data<-trainData[spls,] \n  sample_y<-labels[spls,] \n\n  # Reshape sample into 16 sequence with each of 16 element \n  sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input)) \n  out<-optimizer$run(feed_dict = dict(x=sample_data$eval(), y=sample_y)) \n\n  if (i %% 1 == 0){ \n    cat(\"iteration - \", i, \"Training Loss - \",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), \"\\n\") \n  } \n} \n\n```", "```py\nvalid_data=tf$reshape(validData, shape(-1, step_size, n_input)) \ncost$eval(feed_dict=dict(x=valid_data$eval(), y=labels_valid))\n\n```", "```py\nnum_layers <- 3 \n\n```", "```py\nstacked_rnn<-function(x, weight, bias){ \n  # Unstack input into step_size \n  x = tf$unstack(x, step_size, 1) \n\n  # Define a most basic RNN  \n  network = tf$contrib$rnn$GRUCell(n.hidden) \n\n  # Then, assign stacked RNN cells \n  network = tf$contrib$rnn$MultiRNNCell(lapply(1:num_layers,function(k,network){network},network)) \n\n  # create a Recurrent Neural Network \n  cell_output = tf$contrib$rnn$static_rnn(network, x, dtype=tf$float32) \n\n  # Linear activation, using rnn inner loop  \n  last_vec=tail(cell_output[[1]], n=1)[[1]] \n  return(tf$matmul(last_vec, weights) + bias) \n} \n\n```", "```py\n# LSTM implementation \nlstm<-function(x, weight, bias){ \n  # Unstack input into step_size \n  x = tf$unstack(x, step_size, 1) \n\n  # Define a lstm cell \n  lstm_cell = tf$contrib$rnn$BasicLSTMCell(n.hidden, forget_bias=1.0, state_is_tuple=TRUE) \n\n  # Get lstm cell output \n  cell_output = tf$contrib$rnn$static_rnn(lstm_cell, x, dtype=tf$float32) \n\n  # Linear activation, using rnn inner loop last output \n  last_vec=tail(cell_output[[1]], n=1)[[1]] \n  return(tf$matmul(last_vec, weights) + bias) \n} \n\n```"]