- en: Creating a Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dialogue agents and chatbots have been on the rise in recent years. Many businesses
    have resorted to chatbots to answer customer inquiries, and this has been largely
    successful. Chatbots have been growing quickly, at 5.6x in the last year ([https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b](https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b)). Chatbots
    can help organizations to communicate and interact with customers without any
    human intervention, at a very minimal cost. Over 51% of customers have stated
    that they want businesses to be available 24/7, and they expect replies in less
    than one hour. For businesses to achieve this kind of success in an affordable
    manner, especially with a large customer base, they must resort to chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: The background problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many chatbots are created with regular machine learning natural language processing
    algorithms, and these focus on immediate responses. A new concept is to create
    chatbots with the use of deep reinforcement learning. This would mean that the
    future implications of our immediate responses would be considered to maintain
    coherence.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to apply deep reinforcement learning to
    natural language processing. Our reward function will be a future-looking function,
    and you will learn how to think probabilistically through the creation of this
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we will use mainly consists of conversations from selected
    movies. This dataset will help to stimulate and understand conversational methods
    in the chatbot. Also, there are movie lines, which are essentially the same as
    the movie conversations, albeit shorter exchanges between people. Other data sets
    that will be used include some containing movie titles, movie characters, and
    raw scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our solution will use modeling and will focus on the future direction of a
    dialogue agent, so as to generate coherent and interesting dialogue. The model
    will simulate the dialogue between two virtual agents, with the use of policy
    gradient methods. These methods are designed to reward the sequences of interaction
    that display three important properties of conversation: informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function). In our solution, an action will be defined as the dialogue or communication
    utterance that the chatbot generates. Also, a state will be defined as the two
    previous interaction turns. In order to achieve all of this, we will use the scripts
    in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Data parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data parser script is designed to help with the cleaning and preprocessing
    of our datasets. There are a number of dependencies in this script, such as `pickle`,
    `codecs`, `re`, `OS`, `time`, and `numpy`. This script contains three functions.
    The first function helps to filter words, by preprocessing word counts and creating
    vocabulary based on word count thresholds. The second function helps to parse
    all words into this script, and the third function helps to extract only the defined
    vocabulary from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following module cleans and preprocesses the text in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, iterate through the captions and create the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, parse all the words from the movie lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract only the vocabulary part of the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, the utterance dictionary is created and stored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The data is parsed and can be utilized in further steps.
  prefs: []
  type: TYPE_NORMAL
- en: Data reader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data reader script helps to generate trainable batches from the preprocessed
    training text from the data parser script. Let''s start by importing the required
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This helper module helps generate trainable batches from the preprocessed training
    text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code gets the batch number from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shuffles the index from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code generates the batch indices, based on the batch number that
    was obtained earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code generates the training batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The following function generates training batch with the former.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code generates the testing batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the data reading part.
  prefs: []
  type: TYPE_NORMAL
- en: Helper methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script consists of a `Seq2seq` dialogue generator model, which is used
    for the reverse model of the backward entropy loss. This will determine the semantic
    coherence reward for the policy gradients dialogue. Essentially, this script will
    help us to represent our future reward function. The script will achieve this
    via the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating builds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the preceding actions are based on **long short-term memory** (**LSTM**)
    units.
  prefs: []
  type: TYPE_NORMAL
- en: The feature extractor script helps with the extraction of features and characteristics
    from the data, in order to help us train it better. Let us start by importing
    the required modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, define the model inputs. If reinforcement learning is set to True, a scalar
    is computed based on semantic coherence and ease of answering loss caption.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next, define the encoding layers which perform encoding for the sequence to
    sequence network. The input sequence is passed into the encoder and returns the
    output of RNN output and the state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, define the training process for decoder using LSTMS cells with the encoder
    state together with the decoder inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, define an inference decoder similar to the one used for the training.
    Makes use of a greedy helper which feeds the last output of the decoder as the
    next decoder input. The output returned contains the training logits and the sample
    id.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, create a decoding layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, create the bos inclusion which appends the index corresponding to <bos>
    referring to the beginning of a sentence to the first index of the caption tensor
    for every batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, define pad sequences which creates an array of size maxlen from every
    question by padding with zeros or truncating where necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Ignore non-vocabulary parts if the data and take only all alphabets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, create batches to be fed into the network from in word vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next, generate sentences from the predicted indices. Replace <unk>, <pad> with
    the word with the next highest probability whenever predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This concludes all the helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following script contains the policy gradient model, which will be used
    where it combines reinforcement learning rewards with the cross-entropy loss.
    The dependencies include `numpy` and `tensorflow`. Our policy gradient is based
    on an LSTM encoder-decoder. We will use a stochastic demonstration of our policy
    gradient, which will be a probability distribution of actions over specified states.
    The script represents all of these, and specifies the policy gradient loss to
    be minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Run the output of the first cell through the second cell; the input is concatenated
    with zeros. The final state for the responses mostly consists of two components—the
    latent representation of the input by the encoder, and the state of the decoder,
    based on the selected words. The return includes placeholder tensors and other
    tensors, such as losses and training optimization operation. Let's start by importing
    the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will create a chatbot class to create the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Next, create a method that builds the model. If policy gradients are requested,
    then get the input accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Next, get the inference layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Next, get the loss layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the state of the policy gradient, either minimize cross entropy
    loss or policy gradient loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now we have all the methods that are required for training.
  prefs: []
  type: TYPE_NORMAL
- en: Training the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scripts that were written previously were combined with training the dataset. Let''s
    start the training by importing all the modules that are developed in the previous
    sections as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a set of generic responses observed in the original `seq2seq` model
    which the policy gradients are trained to avoid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will define all the constants that are required for the training. Tha
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define the training function. Based on the type, either the forward or
    reverse sequence to sequence model is loaded. The data is also read in reverse
    model based on the model as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create the vocabulary as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The above command print should print the following indicated the vocabulary
    size that is filtered.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `word_to_index` variable is filled with the map of filtered words to an
    integer as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `index_to_word` variable is filled with the map of integer to the filtered
    works which will work as a reverse lookup.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Next, load the word to vector model from `gensim` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Next, instantiate and build the model the Chatbot model with all the constants
    that were defined. Restore a checkpoint, if present from the previous run or initialize
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Next, start the training by iterating through the epochs and start the batches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `batch_input` has the list of words from the training set.  The `batch_target` has
    the list of sentences for the input which will be the target. The list of words
    is converted to vector form using the helper functions. Make the feed dictionary
    for the graph using the transformed inputs, masks and targets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Next, train the model by calling the optimizer by feeding the training data.
    Log the loss value at certain intervals to see the progress of the training. Save
    the model at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The output should appear as shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The model is trained for both forward and reverse and the corresponding models
    are stored. In the next function, the models are restored and trained again to
    create the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Two graphs are created to load the trained models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Next, the data is loaded to train the data in batches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, learn when to say generic texts as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Next, call the functions defined in sequence. First train a forward model, followed
    by reverse model and policy gradient at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the training of the chatbot. The model is trained in forward
    and reverse manner to
  prefs: []
  type: TYPE_NORMAL
- en: Testing and results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training the model, we tested it against our test dataset and obtained
    reasonably coherent dialogue. There is one very important issue: the context of
    the communication. Hence, depending on the dataset that is used, the result will
    be in its context. For our context, the results that were obtained were very reasonable,
    and they satisfied our three measures of performance—informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Next, declare the paths to the various model that are already trained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Next, declare the path of the files consisting of questions and responses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Next, declare the constants required for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the data and the model as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open the responses file and prepare the list of questions as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: By passing the path to the model, we can test the chatbot for various responses.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chatbots are taking the world by storm, and are predicted to become more prevalent
    in the coming years. The coherence of the results obtained from dialogues with
    these chatbots has to constantly improve if they are to gain widespread acceptance.
    One way to achieve this would be via the use of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we implemented reinforcement learning in the creation of a chatbot.
    The learning was based on a policy gradient method that focused on the future
    direction of a dialogue agent, in order to generate coherent and interesting interactions.
    The datasets that we used were from movie conversations. We proceeded to clean
    and preprocess the datasets, obtaining the vocabulary from them. We then formulated
    our policy gradient method. Our reward functions were represented by a sequence
    to sequence model. We then trained and tested our data and obtained very reasonable
    results, proving the viability of using reinforcement learning for dialogue agents.
  prefs: []
  type: TYPE_NORMAL
