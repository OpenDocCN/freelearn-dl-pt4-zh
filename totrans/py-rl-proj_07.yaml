- en: Creating a Chatbot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dialogue agents and chatbots have been on the rise in recent years. Many businesses
    have resorted to chatbots to answer customer inquiries, and this has been largely
    successful. Chatbots have been growing quickly, at 5.6x in the last year ([https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b](https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b)). Chatbots
    can help organizations to communicate and interact with customers without any
    human intervention, at a very minimal cost. Over 51% of customers have stated
    that they want businesses to be available 24/7, and they expect replies in less
    than one hour. For businesses to achieve this kind of success in an affordable
    manner, especially with a large customer base, they must resort to chatbots.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The background problem
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many chatbots are created with regular machine learning natural language processing
    algorithms, and these focus on immediate responses. A new concept is to create
    chatbots with the use of deep reinforcement learning. This would mean that the
    future implications of our immediate responses would be considered to maintain
    coherence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to apply deep reinforcement learning to
    natural language processing. Our reward function will be a future-looking function,
    and you will learn how to think probabilistically through the creation of this
    function.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we will use mainly consists of conversations from selected
    movies. This dataset will help to stimulate and understand conversational methods
    in the chatbot. Also, there are movie lines, which are essentially the same as
    the movie conversations, albeit shorter exchanges between people. Other data sets
    that will be used include some containing movie titles, movie characters, and
    raw scripts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step guide
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our solution will use modeling and will focus on the future direction of a
    dialogue agent, so as to generate coherent and interesting dialogue. The model
    will simulate the dialogue between two virtual agents, with the use of policy
    gradient methods. These methods are designed to reward the sequences of interaction
    that display three important properties of conversation: informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function). In our solution, an action will be defined as the dialogue or communication
    utterance that the chatbot generates. Also, a state will be defined as the two
    previous interaction turns. In order to achieve all of this, we will use the scripts
    in the following sections.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Data parser
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data parser script is designed to help with the cleaning and preprocessing
    of our datasets. There are a number of dependencies in this script, such as `pickle`,
    `codecs`, `re`, `OS`, `time`, and `numpy`. This script contains three functions.
    The first function helps to filter words, by preprocessing word counts and creating
    vocabulary based on word count thresholds. The second function helps to parse
    all words into this script, and the third function helps to extract only the defined
    vocabulary from the data:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following module cleans and preprocesses the text in the training dataset:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, iterate through the captions and create the vocabulary.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, parse all the words from the movie lines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Extract only the vocabulary part of the data, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, the utterance dictionary is created and stored.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data is parsed and can be utilized in further steps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Data reader
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data reader script helps to generate trainable batches from the preprocessed
    training text from the data parser script. Let''s start by importing the required
    methods:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This helper module helps generate trainable batches from the preprocessed training
    text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code gets the batch number from the data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code shuffles the index from the data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code generates the batch indices, based on the batch number that
    was obtained earlier:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following code generates the training batch:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following function generates training batch with the former.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code generates the testing batch:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This concludes the data reading part.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Helper methods
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script consists of a `Seq2seq` dialogue generator model, which is used
    for the reverse model of the backward entropy loss. This will determine the semantic
    coherence reward for the policy gradients dialogue. Essentially, this script will
    help us to represent our future reward function. The script will achieve this
    via the following actions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Encoding
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating builds
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the preceding actions are based on **long short-term memory** (**LSTM**)
    units.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The feature extractor script helps with the extraction of features and characteristics
    from the data, in order to help us train it better. Let us start by importing
    the required modules.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, define the model inputs. If reinforcement learning is set to True, a scalar
    is computed based on semantic coherence and ease of answering loss caption.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, define the encoding layers which perform encoding for the sequence to
    sequence network. The input sequence is passed into the encoder and returns the
    output of RNN output and the state.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, define the training process for decoder using LSTMS cells with the encoder
    state together with the decoder inputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, define an inference decoder similar to the one used for the training.
    Makes use of a greedy helper which feeds the last output of the decoder as the
    next decoder input. The output returned contains the training logits and the sample
    id.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, create a decoding layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, create the bos inclusion which appends the index corresponding to <bos>
    referring to the beginning of a sentence to the first index of the caption tensor
    for every batch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, define pad sequences which creates an array of size maxlen from every
    question by padding with zeros or truncating where necessary.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Ignore non-vocabulary parts if the data and take only all alphabets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, create batches to be fed into the network from in word vector representation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next, generate sentences from the predicted indices. Replace <unk>, <pad> with
    the word with the next highest probability whenever predicted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This concludes all the helper functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot model
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following script contains the policy gradient model, which will be used
    where it combines reinforcement learning rewards with the cross-entropy loss.
    The dependencies include `numpy` and `tensorflow`. Our policy gradient is based
    on an LSTM encoder-decoder. We will use a stochastic demonstration of our policy
    gradient, which will be a probability distribution of actions over specified states.
    The script represents all of these, and specifies the policy gradient loss to
    be minimized.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Run the output of the first cell through the second cell; the input is concatenated
    with zeros. The final state for the responses mostly consists of two components—the
    latent representation of the input by the encoder, and the state of the decoder,
    based on the selected words. The return includes placeholder tensors and other
    tensors, such as losses and training optimization operation. Let's start by importing
    the required libraries.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will create a chatbot class to create the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, create a method that builds the model. If policy gradients are requested,
    then get the input accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next, get the inference layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, get the loss layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Depending on the state of the policy gradient, either minimize cross entropy
    loss or policy gradient loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we have all the methods that are required for training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Training the data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scripts that were written previously were combined with training the dataset. Let''s
    start the training by importing all the modules that are developed in the previous
    sections as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, let''s create a set of generic responses observed in the original `seq2seq` model
    which the policy gradients are trained to avoid:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, we will define all the constants that are required for the training. Tha
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, define the training function. Based on the type, either the forward or
    reverse sequence to sequence model is loaded. The data is also read in reverse
    model based on the model as shown here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, create the vocabulary as shown here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The above command print should print the following indicated the vocabulary
    size that is filtered.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `word_to_index` variable is filled with the map of filtered words to an
    integer as shown here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `index_to_word` variable is filled with the map of integer to the filtered
    works which will work as a reverse lookup.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Next, load the word to vector model from `gensim` library.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Next, instantiate and build the model the Chatbot model with all the constants
    that were defined. Restore a checkpoint, if present from the previous run or initialize
    the graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Next, start the training by iterating through the epochs and start the batches.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `batch_input` has the list of words from the training set.  The `batch_target` has
    the list of sentences for the input which will be the target. The list of words
    is converted to vector form using the helper functions. Make the feed dictionary
    for the graph using the transformed inputs, masks and targets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, train the model by calling the optimizer by feeding the training data.
    Log the loss value at certain intervals to see the progress of the training. Save
    the model at the end.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The output should appear as shown here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The model is trained for both forward and reverse and the corresponding models
    are stored. In the next function, the models are restored and trained again to
    create the chatbot.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Two graphs are created to load the trained models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Next, the data is loaded to train the data in batches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Also, learn when to say generic texts as shown here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next, call the functions defined in sequence. First train a forward model, followed
    by reverse model and policy gradient at the end.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This concludes the training of the chatbot. The model is trained in forward
    and reverse manner to
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Testing and results
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training the model, we tested it against our test dataset and obtained
    reasonably coherent dialogue. There is one very important issue: the context of
    the communication. Hence, depending on the dataset that is used, the result will
    be in its context. For our context, the results that were obtained were very reasonable,
    and they satisfied our three measures of performance—informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, declare the paths to the various model that are already trained.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, declare the path of the files consisting of questions and responses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Next, declare the constants required for the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, load the data and the model as shown here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, open the responses file and prepare the list of questions as shown here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: By passing the path to the model, we can test the chatbot for various responses.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chatbots are taking the world by storm, and are predicted to become more prevalent
    in the coming years. The coherence of the results obtained from dialogues with
    these chatbots has to constantly improve if they are to gain widespread acceptance.
    One way to achieve this would be via the use of reinforcement learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we implemented reinforcement learning in the creation of a chatbot.
    The learning was based on a policy gradient method that focused on the future
    direction of a dialogue agent, in order to generate coherent and interesting interactions.
    The datasets that we used were from movie conversations. We proceeded to clean
    and preprocess the datasets, obtaining the vocabulary from them. We then formulated
    our policy gradient method. Our reward functions were represented by a sequence
    to sequence model. We then trained and tested our data and obtained very reasonable
    results, proving the viability of using reinforcement learning for dialogue agents.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了在创建聊天机器人过程中使用强化学习。该学习方法基于一种政策梯度方法，重点关注对话代理的未来方向，以生成连贯且有趣的互动。我们使用的数据集来自电影对话。我们对数据集进行了清理和预处理，从中获取了词汇表。然后，我们制定了我们的政策梯度方法。我们的奖励函数通过一个序列到序列模型表示。接着，我们训练并测试了我们的数据，获得了非常合理的结果，证明了使用强化学习进行对话代理的可行性。
