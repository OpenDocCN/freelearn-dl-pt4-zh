- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Sut88]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Richard S Sutton. “Learning to predict by the methods of temporal differences”.
    In: Machine learning 3 (1988), pp. 9–44.'
  prefs: []
  type: TYPE_NORMAL
- en: '[HS96]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sepp Hochreiter and Jürgen Schmidhuber. “LSTM can solve hard long time lag
    problems”. In: Advances in neural information processing systems 9 (1996).'
  prefs: []
  type: TYPE_NORMAL
- en: '[RK04]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified
    approach to combinatorial optimization, Monte-Carlo simulation, and machine learning.
    Vol. 133\. Springer, 2004.'
  prefs: []
  type: TYPE_NORMAL
- en: '[SL08]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexander L Strehl and Michael L Littman. “An analysis of model-based interval
    estimation for Markov decision processes”. In: Journal of Computer and System
    Sciences 74.8 (2008), pp. 1309–1331.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kro+11]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dirk P Kroese et al. “Cross-entropy method’”. In: European Journal of Operational
    Research 31 (2011), pp. 276–283.'
  prefs: []
  type: TYPE_NORMAL
- en: '[LS11]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Joel Lehman and Kenneth O Stanley. “Abandoning objectives: Evolution through
    the search for novelty alone”. In: Evolutionary computation 19.2 (2011), pp. 189–223.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mni13]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Volodymyr Mnih. “Playing atari with deep reinforcement learning”. In: arXiv
    preprint arXiv:1312.5602 (2013).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sil+14]'
  prefs: []
  type: TYPE_NORMAL
- en: 'David Silver et al. “Deterministic policy gradient algorithms”. In: International
    conference on machine learning. Pmlr. 2014, pp. 387–395.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Lil15]'
  prefs: []
  type: TYPE_NORMAL
- en: 'TP Lillicrap. “Continuous control with deep reinforcement learning”. In: arXiv
    preprint arXiv:1509.02971 (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: '[MG15]'
  prefs: []
  type: TYPE_NORMAL
- en: 'James Martens and Roger Grosse. “Optimizing neural networks with kronecker-factored
    approximate curvature”. In: International conference on machine learning. PMLR.
    2015, pp. 2408–2417.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mni+15]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Volodymyr Mnih et al. “Human-level control through deep reinforcement learning”.
    In: nature 518.7540 (2015), pp. 529–533.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sch+15]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tom Schaul et al. “Prioritized Experience Replay”. In: (2015). arXiv: [1511.05952
    [cs.LG]](https://arxiv.org/abs/1511.05952). url: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sch15]'
  prefs: []
  type: TYPE_NORMAL
- en: 'John Schulman. “Trust Region Policy Optimization”. In: arXiv preprint arXiv:1502.05477
    (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: '[VGS16]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hado Van Hasselt, Arthur Guez, and David Silver. “Deep reinforcement learning
    with double q-learning”. In: Proceedings of the AAAI conference on artificial
    intelligence. Vol. 30\. 1\. 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Wan+16]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ziyu Wang et al. “Dueling network architectures for deep reinforcement learning”.
    In: International conference on machine learning. PMLR. 2016, pp. 1995–2003.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BDM17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Marc G Bellemare, Will Dabney, and Rémi Munos. “A distributional perspective
    on reinforcement learning”. In: International conference on machine learning.
    PMLR. 2017, pp. 449–458.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chr+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paul Christiano et al. Deep reinforcement learning from human preferences.
    2017\. eprint: [arXiv:1706.03741](#).'
  prefs: []
  type: TYPE_NORMAL
- en: '[For+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Meire Fortunato et al. “Noisy Networks for Exploration”. In: (2017). arXiv:
    [1706.10295 [cs.LG]](https://arxiv.org/abs/1706.10295). url: [https://arxiv.org/abs/1706.10295](https://arxiv.org/abs/1706.10295).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mar+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jarryd Martin et al. “Count-based exploration in feature space for reinforcement
    learning”. In: arXiv preprint arXiv:1706.08090 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ost+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Georg Ostrovski et al. “Count-based exploration with neural density models”.
    In: International conference on machine learning. PMLR. 2017, pp. 2721–2730.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sal+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tim Salimans et al. “Evolution strategies as a scalable alternative to reinforcement
    learning”. In: arXiv preprint arXiv:1703.03864 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sch+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'John Schulman et al. “Proximal policy optimization algorithms”. In: arXiv preprint
    arXiv:1707.06347 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[SSa17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'David Silver, Julian Schrittwieser, and Karen Simonyan et al. Mastering the
    game of Go without human knowledge. 2017\. eprint: [10.1038/nature24270](#).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sil+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'David Silver et al. Mastering Chess and Shogi by Self-Play with a General Reinforcement
    Learning Algorithm. 2017\. arXiv: [1712.01815 [cs.AI]](https://arxiv.org/abs/1712.01815).
    url: [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Suc+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Felipe Petroski Such et al. “Deep neuroevolution: Genetic algorithms are a
    competitive alternative for training deep neural networks for reinforcement learning”.
    In: arXiv preprint arXiv:1712.06567 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Vas17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Vaswani. “Attention is all you need”. In: Advances in Neural Information
    Processing Systems (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Wu+17]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yuhuai Wu et al. “Scalable trust-region method for deep reinforcement learning
    using kronecker-factored approximation”. In: Advances in neural information processing
    systems 30 (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Bar+18]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gabriel Barth-Maron et al. “Distributed distributional deterministic policy
    gradients”. In: arXiv preprint arXiv:1804.08617 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Bur+18]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yuri Burda et al. “Exploration by random network distillation”. In: arXiv preprint
    arXiv:1810.12894 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Haa+18]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuomas Haarnoja et al. “Soft actor-critic: Off-policy maximum entropy deep
    reinforcement learning with a stochastic actor”. In: International conference
    on machine learning. PMLR. 2018, pp. 1861–1870.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Hes+18]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matteo Hessel et al. “Rainbow: Combining improvements in deep reinforcement
    learning”. In: Proceedings of the AAAI conference on artificial intelligence.
    Vol. 32\. 1\. 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '[McA+18]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stephen McAleer et al. “Solving the Rubik’s cube without human knowledge”.
    In: arXiv preprint arXiv:1805.07470 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Bak+20]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bowen Baker et al. Emergent Tool Use From Multi-Agent Autocurricula. 2020\.
    arXiv: [1909.07528 [cs.LG]](https://arxiv.org/abs/1909.07528). url: [https://arxiv.org/abs/1909.07528](https://arxiv.org/abs/1909.07528).'
  prefs: []
  type: TYPE_NORMAL
- en: '[FS20]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexander H Frey Jr and David Singmaster. “Handbook of cubik math”. In: (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sch+20]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Julian Schrittwieser et al. “Mastering Atari, Go, chess and shogi by planning
    with a learned model”. In: Nature 588.7839 (Dec. 2020), pp. 604–609\. issn: 1476-4687\.
    doi: [10.1038/s41586-020-03051-4](https://doi.org/10.1038/s41586-020-03051-4).
    url: [http://dx.doi.org/10.1038/s41586-020-03051-4](http://dx.doi.org/10.1038/s41586-020-03051-4).'
  prefs: []
  type: TYPE_NORMAL
- en: '[BDR23]'
  prefs: []
  type: TYPE_NORMAL
- en: Marc G Bellemare, Will Dabney, and Mark Rowland. Distributional reinforcement
    learning. MIT Press, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[www.packt.com](https://www.packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![PIC](img/file350.png)](https://www.packtpub.com/en-us/product/mastering-pytorch-9781801074308)'
  prefs: []
  type: TYPE_NORMAL
- en: Mastering PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: Ashish Ranjan Jha
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781801074308'
  prefs: []
  type: TYPE_NORMAL
- en: Implement text, vision, and music generation models using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a deep Q-network (DQN) model in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy PyTorch models on mobile devices (Android and iOS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Become well versed in rapid prototyping using PyTorch with fastai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform neural architecture search effectively using AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily interpret machine learning models using Captum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design ResNets, LSTMs, and graph neural networks (GNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create language and vision transformer models using Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![PIC](img/file351.png)](https://www.packtpub.com/en-us/product/python-for-algorithmic-trading-cookbook-9781835084700)'
  prefs: []
  type: TYPE_NORMAL
- en: Python for Algorithmic Trading Cookbook
  prefs: []
  type: TYPE_NORMAL
- en: Jason Strimpel
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781835084700'
  prefs: []
  type: TYPE_NORMAL
- en: Acquire and process freely available market data with the OpenBB Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a research environment and populate it with financial market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use machine learning to identify alpha factors and engineer them into signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use VectorBT to find strategy parameters using walk-forward optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build production-ready backtests with Zipline Reloaded and evaluate factor performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up the code framework to connect and send an order to Interactive Brokers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
