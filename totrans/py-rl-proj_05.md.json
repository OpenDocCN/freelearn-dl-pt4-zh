["```py\npython3 -m pip install gym\npython3 -m pip install pygame\n\ngit clone https://github.com/tambetm/minecraft-py.git\ncd minecraft-py\npython setup.py install\n\ngit clone https://github.com/tambetm/gym-minecraft.git\ncd gym-minecraft\npython setup.py install\n```", "```py\nimport logging\nimport minecraft_py\nlogging.basicConfig(level=logging.DEBUG)\n\nproc, _ = minecraft_py.start()\nminecraft_py.stop(proc)\n```", "```py\nimport gym\nimport gym_minecraft\nimport minecraft_py\n\ndef start_game():\n    env = gym.make('MinecraftBasic-v0')\n    env.init(start_minecraft=True)\n    env.reset()\n\n    done = False\n    while not done:\n        env.render(mode='human')\n        action = env.action_space.sample()\n        obs, reward, done, info = env.step(action)\n    env.close()\n\nif __name__ == \"__main__\":\n    start_game()\n```", "```py\nimport gym\nimport gym_minecraft\nimport minecraft_py\nimport numpy, time\nfrom utils import cv2_resize_image\n\nclass Game:\n\n    def __init__(self, name='MinecraftBasic-v0', discrete_movement=False):\n\n        self.env = gym.make(name)\n        if discrete_movement:\n            self.env.init(start_minecraft=True, allowDiscreteMovement=[\"move\", \"turn\"])\n        else:\n            self.env.init(start_minecraft=True, allowContinuousMovement=[\"move\", \"turn\"])\n        self.actions = list(range(self.env.action_space.n))\n        frame = self.env.reset()\n\n        self.frame_skip = 4\n        self.total_reward = 0\n        self.crop_size = 84\n        self.buffer_size = 8\n        self.buffer_index = 0\n        self.buffer = [self.crop(self.rgb_to_gray(frame)) for _ in range(self.buffer_size)]\n        self.last_frame = frame\n\n    def rgb_to_gray(self, im):\n        return numpy.dot(im, [0.2126, 0.7152, 0.0722])\n\n    def reset(self):\n        frame = self.env.reset()\n        self.total_reward = 0\n        self.buffer_index = 0\n        self.buffer = [self.crop(self.rgb_to_gray(frame)) for _ in range(self.buffer_size)]\n        self.last_frame = frame\n\n    def add_frame_to_buffer(self, frame):\n        self.buffer_index = self.buffer_index % self.buffer_size\n        self.buffer[self.buffer_index] = frame\n        self.buffer_index += 1\n\n    def get_available_actions(self):\n        return list(range(len(self.actions)))\n\n    def get_feedback_size(self):\n        return (self.crop_size, self.crop_size)\n\n    def crop(self, frame):\n        feedback = cv2_resize_image(frame, \n                                    resized_shape=(self.crop_size, self.crop_size), \n                                    method='scale', crop_offset=0)\n        return feedback\n\n    def get_current_feedback(self, num_frames=4):\n        assert num_frames < self.buffer_size, \"Frame buffer is not large enough.\"\n        index = self.buffer_index - 1\n        frames = [numpy.expand_dims(self.buffer[index - k], axis=0) for k in range(num_frames)]\n        if num_frames > 1:\n            return numpy.concatenate(frames, axis=0)\n        else:\n            return frames[0]\n\n    def play_action(self, action, num_frames=4):\n        reward = 0\n        termination = 0\n        for i in range(self.frame_skip):\n            a = self.actions[action]\n            frame, r, done, _ = self.env.step(a)\n            reward += r\n            if i == self.frame_skip - 2: \n                self.last_frame = frame\n            if done: \n                termination = 1\n        self.add_frame_to_buffer(self.crop(numpy.maximum(self.rgb_to_gray(frame), self.rgb_to_gray(self.last_frame))))\n\n        r = numpy.clip(reward, -1, 1)\n        self.total_reward += reward\n        return r, self.get_current_feedback(num_frames), termination\n```", "```py\nInitialize thread step counter ;\nInitialize global shared parameters  and ;\nRepeat for each episode:\n    Reset gradients  and ;\n    Synchronize thread-specific parameters  and ;\n    Set the start time step ;\n    Receive an observation state ;\n    While  is not the terminal state and :\n        Select an action  according to ;\n        Execute action  in the simulator and observe reward  and the next state ;\n        Set ;\n    End While\n    Set  if  is the terminal state or  otherwise; \n    For  do\n        Update ;\n        Accumulate gradients wrt : ;\n        Accumulate gradients wrt : ;\n    End For\n    Perform asynchronous update of  using  and of  using .\n```", "```py\nclass FFPolicy:\n\n    def __init__(self, input_shape=(84, 84, 4), n_outputs=4, network_type='cnn'):\n\n        self.width = input_shape[0]\n        self.height = input_shape[1]\n        self.channel = input_shape[2]\n        self.n_outputs = n_outputs\n        self.network_type = network_type\n        self.entropy_beta = 0.01\n\n        self.x = tf.placeholder(dtype=tf.float32, \n                                shape=(None, self.channel, self.width, self.height))\n        self.build_model()\n```", "```py\n    def build_model(self):\n\n        self.net = {}\n        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))\n\n        if self.network_type == 'cnn':\n            self.net['conv1'] = conv2d(self.net['input'], 16, kernel=(8, 8), stride=(4, 4), name='conv1')\n            self.net['conv2'] = conv2d(self.net['conv1'], 32, kernel=(4, 4), stride=(2, 2), name='conv2')\n            self.net['feature'] = linear(self.net['conv2'], 256, name='fc1')\n        else:\n            self.net['fc1'] = linear(self.net['input'], 50, init_b = tf.constant_initializer(0.0), name='fc1')\n            self.net['feature'] = linear(self.net['fc1'], 50, init_b = tf.constant_initializer(0.0), name='fc2')\n\n        self.net['value'] = tf.reshape(linear(self.net['feature'], 1, activation=None, name='value',\n                                              init_b = tf.constant_initializer(0.0)), \n                                       shape=(-1,))\n\n        self.net['logits'] = linear(self.net['feature'], self.n_outputs, activation=None, name='logits',\n                                    init_b = tf.constant_initializer(0.0))\n\n        self.net['policy'] = tf.nn.softmax(self.net['logits'], name='policy')\n        self.net['log_policy'] = tf.nn.log_softmax(self.net['logits'], name='log_policy')\n\n        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n```", "```py\n    def build_gradient_op(self, clip_grad=None):\n\n        self.action = tf.placeholder(dtype=tf.float32, shape=(None, self.n_outputs), name='action')\n        self.reward = tf.placeholder(dtype=tf.float32, shape=(None,), name='reward')\n        self.advantage = tf.placeholder(dtype=tf.float32, shape=(None,), name='advantage')\n\n        value = self.net['value']\n        policy = self.net['policy']\n        log_policy = self.net['log_policy']\n        entropy = -tf.reduce_sum(policy * log_policy, axis=1)\n        p_loss = -tf.reduce_sum(tf.reduce_sum(log_policy * self.action, axis=1) * self.advantage + self.entropy_beta * entropy)\n        v_loss = 0.5 * tf.reduce_sum((value - self.reward) ** 2)\n        total_loss = p_loss + v_loss\n\n        self.gradients = tf.gradients(total_loss, self.vars)\n        if clip_grad is not None:\n            self.gradients, _ = tf.clip_by_global_norm(self.gradients, clip_grad)\n\n        tf.summary.scalar(\"policy_loss\", p_loss, collections=['policy_network'])\n        tf.summary.scalar(\"value_loss\", v_loss, collections=['policy_network'])\n        tf.summary.scalar(\"entropy\", tf.reduce_mean(entropy), collections=['policy_network'])\n        self.summary_op = tf.summary.merge_all('policy_network')\n\n        return self.gradients\n```", "```py\n    def build_model(self):\n\n        self.net = {}\n        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))\n\n        if self.network_type == 'cnn':\n            self.net['conv1'] = conv2d(self.net['input'], 16, kernel=(8, 8), stride=(4, 4), name='conv1')\n            self.net['conv2'] = conv2d(self.net['conv1'], 32, kernel=(4, 4), stride=(2, 2), name='conv2')\n            self.net['feature'] = linear(self.net['conv2'], 256, name='fc1')\n        else:\n            self.net['fc1'] = linear(self.net['input'], 50, init_b = tf.constant_initializer(0.0), name='fc1')\n            self.net['feature'] = linear(self.net['fc1'], 50, init_b = tf.constant_initializer(0.0), name='fc2')\n\n        num_units = self.net['feature'].get_shape().as_list()[-1]\n        self.lstm = tf.contrib.rnn.BasicLSTMCell(num_units=num_units, forget_bias=0.0, state_is_tuple=True)\n        self.init_state = self.lstm.zero_state(batch_size=1, dtype=tf.float32)\n\n        step_size = tf.shape(self.x)[:1]\n        feature = tf.expand_dims(self.net['feature'], axis=0)\n        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(self.lstm, feature, \n                                                     initial_state=self.init_state, \n                                                     sequence_length=step_size,\n                                                     time_major=False)\n        outputs = tf.reshape(lstm_outputs, shape=(-1, num_units))\n        self.final_state = lstm_state\n\n        self.net['value'] = tf.reshape(linear(outputs, 1, activation=None, name='value',\n                                              init_b = tf.constant_initializer(0.0)), \n                                       shape=(-1,))\n\n        self.net['logits'] = linear(outputs, self.n_outputs, activation=None, name='logits',\n                                    init_b = tf.constant_initializer(0.0))\n\n        self.net['policy'] = tf.nn.softmax(self.net['logits'], name='policy')\n        self.net['log_policy'] = tf.nn.log_softmax(self.net['logits'], name='log_policy')\n\n        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n```", "```py\nclass A3C:\n\n    def __init__(self, system, directory, param, agent_index=0, callback=None):\n\n        self.system = system\n        self.actions = system.get_available_actions()\n        self.directory = directory\n        self.callback = callback\n        self.feedback_size = system.get_feedback_size()\n        self.agent_index = agent_index\n\n        self.set_params(param)\n        self.init_network()\n```", "```py\n    def init_network(self):\n\n        input_shape = self.feedback_size + (self.num_frames,)\n        worker_device = \"/job:worker/task:{}/cpu:0\".format(self.agent_index)\n\n        with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):\n            with tf.variable_scope(\"global\"):\n                if self.use_lstm is False:\n                    self.shared_network = FFPolicy(input_shape, len(self.actions), self.network_type)\n                else:\n                    self.shared_network = LSTMPolicy(input_shape, len(self.actions), self.network_type)\n\n                self.global_step = tf.get_variable(\"global_step\", shape=[], \n                                                   initializer=tf.constant_initializer(0, dtype=tf.int32),\n                                                   trainable=False, dtype=tf.int32)\n                self.best_score = tf.get_variable(\"best_score\", shape=[], \n                                                   initializer=tf.constant_initializer(-1e2, dtype=tf.float32),\n                                                   trainable=False, dtype=tf.float32)\n\n        with tf.device(worker_device):\n            with tf.variable_scope('local'):\n                if self.use_lstm is False:\n                    self.network = FFPolicy(input_shape, len(self.actions), self.network_type)\n                else:\n                    self.network = LSTMPolicy(input_shape, len(self.actions), self.network_type)\n                # Sync params\n                self.update_local_ops = update_target_graph(self.shared_network.vars, self.network.vars)\n                # Learning rate\n                self.lr = tf.get_variable(name='lr', shape=[], \n                                          initializer=tf.constant_initializer(self.learning_rate),\n                                          trainable=False, dtype=tf.float32)\n                self.t_lr = tf.placeholder(dtype=tf.float32, shape=[], name='new_lr')\n                self.assign_lr_op = tf.assign(self.lr, self.t_lr)\n                # Best score\n                self.t_score = tf.placeholder(dtype=tf.float32, shape=[], name='new_score')\n                self.assign_best_score_op = tf.assign(self.best_score, self.t_score)\n                # Build gradient_op\n                self.increase_step = self.global_step.assign_add(1)\n                gradients = self.network.build_gradient_op(clip_grad=40.0)\n                # Additional summaries\n                tf.summary.scalar(\"learning_rate\", self.lr, collections=['a3c'])\n                tf.summary.scalar(\"score\", self.t_score, collections=['a3c'])\n                tf.summary.scalar(\"best_score\", self.best_score, collections=['a3c'])\n                self.summary_op = tf.summary.merge_all('a3c')\n\n        if self.shared_optimizer:\n            with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):\n                with tf.variable_scope(\"global\"):\n                    optimizer = create_optimizer(self.update_method, self.lr, self.rho, self.rmsprop_epsilon)\n                    self.train_op = optimizer.apply_gradients(zip(gradients, self.shared_network.vars))\n        else:\n            with tf.device(worker_device):\n                with tf.variable_scope('local'):\n                    optimizer = create_optimizer(self.update_method, self.lr, self.rho, self.rmsprop_epsilon)\n                    self.train_op = optimizer.apply_gradients(zip(gradients, self.shared_network.vars))\n```", "```py\ndef update_target_graph(from_vars, to_vars):\n    op_holder = []\n    for from_var, to_var in zip(from_vars, to_vars):\n        op_holder.append(to_var.assign(from_var))\n    return op_holder\n```", "```py\ndef create_optimizer(method, learning_rate, rho, epsilon):\n    if method == 'rmsprop':\n        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate, \n                                        decay=rho,\n                                        epsilon=epsilon)\n    elif method == 'adam':\n        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                     beta1=rho)\n    else:\n        raise\n    return opt\n```", "```py\n    def run(self, sess, saver=None):\n\n        num_of_trials = -1\n        for episode in range(self.num_episodes):\n\n            self.system.reset()\n            cell = self.network.run_initial_state(sess)\n            state = self.system.get_current_feedback(self.num_frames)\n            state = numpy.asarray(state / self.input_scale, dtype=numpy.float32)\n            replay_memory = []\n\n            for _ in range(self.T):\n                num_of_trials += 1\n                global_step = sess.run(self.increase_step)\n                if len(replay_memory) == 0:\n                    init_cell = cell\n                    sess.run(self.update_local_ops)\n\n                action, value, cell = self.choose_action(sess, state, cell)\n                r, new_state, termination = self.play(action)\n                new_state = numpy.asarray(new_state / self.input_scale, dtype=numpy.float32)\n                replay = (state, action, r, new_state, value, termination)\n                replay_memory.append(replay)\n                state = new_state\n\n                if len(replay_memory) == self.async_update_interval or termination:\n                    states, actions, rewards, advantages = self.n_step_q_learning(sess, replay_memory, cell)\n                    self.train(sess, states, actions, rewards, advantages, init_cell, num_of_trials)\n                    replay_memory = []\n\n                if global_step % 40000 == 0:\n                    self.save(sess, saver)\n                if self.callback:\n                    self.callback()\n                if termination:\n                    score = self.system.get_total_reward()\n                    summary_str = sess.run(self.summary_op, feed_dict={self.t_score: score})\n                    self.summary_writer.add_summary(summary_str, global_step)\n                    self.summary_writer.flush()\n                    break\n\n            if global_step - self.eval_counter > self.eval_frequency:\n                self.evaluate(sess, n_episode=10, saver=saver)\n                self.eval_counter = global_step\n```", "```py\ndef n_step_q_learning(self, sess, replay_memory, cell):\n\n        batch_size = len(replay_memory)\n        w, h = self.system.get_feedback_size()\n        states = numpy.zeros((batch_size, self.num_frames, w, h), dtype=numpy.float32)\n        rewards = numpy.zeros(batch_size, dtype=numpy.float32)\n        advantages = numpy.zeros(batch_size, dtype=numpy.float32)\n        actions = numpy.zeros((batch_size, len(self.actions)), dtype=numpy.float32)\n\n        for i in reversed(range(batch_size)):\n            state, action, r, new_state, value, termination = replay_memory[i]\n            states[i] = state\n            actions[i][action] = 1\n            if termination != 0:\n                rewards[i] = r\n            else:\n                if i == batch_size - 1:\n                    rewards[i] = r + self.gamma * self.Q_value(sess, new_state, cell)\n                else:\n                    rewards[i] = r + self.gamma * rewards[i+1]\n            advantages[i] = rewards[i] - value\n\n        return states, actions, rewards, advantages\n```", "```py\n    def train(self, sess, states, actions, rewards, advantages, init_cell, iter_num):\n\n        lr = self.anneal_lr(iter_num)\n        feed_dict = self.network.get_feed_dict(states, actions, rewards, advantages, init_cell)\n        sess.run(self.assign_lr_op, feed_dict={self.t_lr: lr})\n\n        step = int((iter_num - self.async_update_interval + 1) / self.async_update_interval)\n        if self.summary_writer and step % 10 == 0:\n            summary_str, _, step = sess.run([self.network.summary_op, self.train_op, self.global_step], \n                                            feed_dict=feed_dict)\n            self.summary_writer.add_summary(summary_str, step)\n            self.summary_writer.flush()\n        else:\n            sess.run(self.train_op, feed_dict=feed_dict)\n```", "```py\nimport numpy, time, random\nimport argparse, os, sys, signal\nimport tensorflow as tf\nfrom a3c import A3C\nfrom cluster import cluster_spec\nfrom environment import new_environment\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    numpy.random.seed(seed)\n\ndef delete_dir(path):\n    if tf.gfile.Exists(path):\n        tf.gfile.DeleteRecursively(path)\n    tf.gfile.MakeDirs(path)\n    return path\n\ndef shutdown(signal, frame):\n    print('Received signal {}: exiting'.format(signal))\n    sys.exit(128 + signal)\n\ndef train(args, server):\n\n    os.environ['OMP_NUM_THREADS'] = '1'\n    set_random_seed(args.task * 17)\n    log_dir = os.path.join(args.log_dir, '{}/train'.format(args.env))\n    if not tf.gfile.Exists(log_dir):\n        tf.gfile.MakeDirs(log_dir)\n\n    game, parameter = new_environment(args.env)\n    a3c = A3C(game, log_dir, parameter.get(), agent_index=args.task, callback=None)\n\n    global_vars = [v for v in tf.global_variables() if not v.name.startswith(\"local\")] \n    ready_op = tf.report_uninitialized_variables(global_vars)\n    config = tf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:{}/cpu:0\".format(args.task)])\n\n    with tf.Session(target=server.target, config=config) as sess:\n        saver = tf.train.Saver()\n        path = os.path.join(log_dir, 'log_%d' % args.task)\n        writer = tf.summary.FileWriter(delete_dir(path), sess.graph_def)\n        a3c.set_summary_writer(writer)\n\n        if args.task == 0:\n            sess.run(tf.global_variables_initializer())\n        else:\n            while len(sess.run(ready_op)) > 0:\n                print(\"Waiting for task 0 initializing the global variables.\")\n                time.sleep(1)\n        a3c.run(sess, saver)\n\ndef main():\n\n    parser = argparse.ArgumentParser(description=None)\n    parser.add_argument('-t', '--task', default=0, type=int, help='Task index')\n    parser.add_argument('-j', '--job_name', default=\"worker\", type=str, help='worker or ps')\n    parser.add_argument('-w', '--num_workers', default=1, type=int, help='Number of workers')\n    parser.add_argument('-l', '--log_dir', default=\"save\", type=str, help='Log directory path')\n    parser.add_argument('-e', '--env', default=\"demo\", type=str, help='Environment')\n\n    args = parser.parse_args()\n    spec = cluster_spec(args.num_workers, 1)\n    cluster = tf.train.ClusterSpec(spec)\n\n    signal.signal(signal.SIGHUP, shutdown)\n    signal.signal(signal.SIGINT, shutdown)\n    signal.signal(signal.SIGTERM, shutdown)\n\n    if args.job_name == \"worker\":\n        server = tf.train.Server(cluster, \n                                 job_name=\"worker\", \n                                 task_index=args.task,\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=0, \n                                                       inter_op_parallelism_threads=0)) # Use default op_parallelism_threads\n        train(args, server)\n    else:\n        server = tf.train.Server(cluster, \n                                 job_name=\"ps\", \n                                 task_index=args.task,\n                                 config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\n        # server.join()\n        while True:\n            time.sleep(1000)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nimport argparse, os, sys, cluster\nfrom six.moves import shlex_quote\n\nparser = argparse.ArgumentParser(description=\"Run commands\")\nparser.add_argument('-w', '--num_workers', default=1, type=int,\n                    help=\"Number of workers\")\nparser.add_argument('-e', '--env', type=str, default=\"demo\",\n                    help=\"Environment\")\nparser.add_argument('-l', '--log_dir', type=str, default=\"save\",\n                    help=\"Log directory path\")\n\ndef new_cmd(session, name, cmd, logdir, shell):\n    if isinstance(cmd, (list, tuple)):\n        cmd = \" \".join(shlex_quote(str(v)) for v in cmd)\n    return name, \"tmux send-keys -t {}:{} {} Enter\".format(session, name, shlex_quote(cmd))\n\ndef create_commands(session, num_workers, logdir, env, shell='bash'):\n\n    base_cmd = ['CUDA_VISIBLE_DEVICES=',\n                sys.executable, \n                'worker.py', \n                '--log_dir', logdir,\n                '--num_workers', str(num_workers),\n                '--env', env]\n\n    cmds_map = [new_cmd(session, \"ps\", base_cmd + [\"--job_name\", \"ps\"], logdir, shell)]\n    for i in range(num_workers):\n        cmd = base_cmd + [\"--job_name\", \"worker\", \"--task\", str(i)]\n        cmds_map.append(new_cmd(session, \"w-%d\" % i, cmd, logdir, shell))\n    cmds_map.append(new_cmd(session, \"htop\", [\"htop\"], logdir, shell))\n\n    windows = [v[0] for v in cmds_map]\n    notes = [\"Use `tmux attach -t {}` to watch process output\".format(session),\n             \"Use `tmux kill-session -t {}` to kill the job\".format(session),\n             \"Use `ssh -L PORT:SERVER_IP:SERVER_PORT username@server_ip` to remote Tensorboard\"]\n\n    cmds = [\"kill $(lsof -i:{}-{} -t) > /dev/null 2>&1\".format(cluster.PORT, num_workers+cluster.PORT),\n            \"tmux kill-session -t {}\".format(session),\n            \"tmux new-session -s {} -n {} -d {}\".format(session, windows[0], shell)]\n\n    for w in windows[1:]:\n        cmds.append(\"tmux new-window -t {} -n {} {}\".format(session, w, shell))\n    cmds.append(\"sleep 1\")\n\n    for _, cmd in cmds_map:\n        cmds.append(cmd)\n    return cmds, notes\n\ndef main():\n\n    args = parser.parse_args()\n    cmds, notes = create_commands(\"a3c\", args.num_workers, args.log_dir, args.env)\n\n    print(\"Executing the following commands:\")\n    print(\"\\n\".join(cmds))\n\n    os.environ[\"TMUX\"] = \"\"\n    os.system(\"\\n\".join(cmds))\n\n    print(\"Notes:\")\n    print('\\n'.join(notes))\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nclass Parameter:\n\n    def __init__(self, lr=7e-4, directory=None):\n        self.directory = directory\n        self.learning_rate = lr\n        self.gamma = 0.99\n        self.num_history_frames = 4\n        self.iteration_num = 100000\n        self.async_update_interval = 5\n        self.rho = 0.99\n        self.rmsprop_epsilon = 1e-1\n        self.update_method = 'rmsprop'\n        self.clip_delta = 0\n        self.max_iter_num = 10 ** 8\n        self.network_type = 'cnn'\n        self.input_scale = 255.0\n```", "```py\npython3 train.py -w 2 -e demo\n```", "```py\npython3 train.py -w 8 -e Breakout\n```", "```py\npython3 train.py -w 8 -e MinecraftBasic-v0\n```", "```py\ntmux attach -t a3c\n```", "```py\ntensorboard --logdir=.\n```"]