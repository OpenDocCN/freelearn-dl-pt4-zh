- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global Forecasting Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explore various time series forecasting scenarios and learn
    how to handle them with deep learning. These scenarios include multi-step and
    multi-output forecasting tasks, and problems involving multiple time series. We’ll
    cover each of these cases, explaining how to prepare your data, train appropriate
    neural network models, and validate them.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be able to build deep learning forecasting
    models for different time series datasets. This includes hyperparameter optimization,
    which is an important stage in model development.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will guide you through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step forecasting with multivariate time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step and multi-output forecasting with multivariate time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing multiple time series for a global model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a global LSTM with multiple time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global forecasting models for seasonal time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization using Ray Tune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy` (1.26.3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` (2.0.3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn` (1.4.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sktime` (0.26.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` (2.2.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch-forecasting` (1.0.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch-lightning` (2.1.4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gluonts` (0.14.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ray` (2.9.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install these libraries in one go using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The recipes in this chapter will follow a design philosophy based on PyTorch
    Lightning that provides a modular and flexible way of building and deploying PyTorch
    models. The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step forecasting with multivariate time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve been working on forecasting the next value of a single variable
    of a time series. Forecasting the value of the next observation is referred to
    as one-step-ahead forecasting. In this recipe, we’ll extend the models we developed
    in the previous chapter for multi-step-ahead forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-step ahead forecasting is the process of forecasting several observations
    in advance. This task is important for reducing the long-term uncertainty of time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that much of the work we did before is also applicable to multi-step
    forecasting settings. The `TimeSeriesDataSet` class makes it extremely simple
    to extend the one-step-ahead problem to the multi-step case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll set the forecasting horizon to `7` and the number of
    lags to `14`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In practice, this means the predictive task is to forecast the next 7 days of
    solar radiation based on the past 14 days of data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For multi-step ahead forecasting problems, two things need to be changed:'
  prefs: []
  type: TYPE_NORMAL
- en: One is the output dimension of the neural network model. Instead of `1` (which
    represents the next value), the output dimension needs to match the number of
    prediction steps. This is done in the `output_dim` variable of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction length of the data module needs to be set to the forecasting
    horizon. This is done in the `max_prediction_length` parameter of the `TimeSeriesDataSet`
    class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two inputs can be passed to the data and model modules as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the training and testing of the model remain the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We trained the model for 20 epochs and then evaluated it in the test set, which
    is retrieved using the data loader defined in the data module.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional supervised machine learning models usually learn from a one-dimensional
    target variable. In forecasting problems, this variable can be, for example, the
    value of the time series in the next period. However, multi-step-ahead forecasting
    problems require the prediction of several values at each time. Deep learning
    models are naturally multi-output algorithms. So, they can handle several target
    variables with a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other approaches for multi-step-ahead forecasting often involve creating several
    models or reusing the same model for different horizons. However, a multi-output
    approach is preferable because it enables the capture of dependencies among different
    horizons. This can lead to better forecasting performance, as has been documented
    in articles such as the following: Taieb, Souhaib Ben, et al., *A review and comparison
    of strategies for multi-step ahead time series forecasting based on the NN5 forecasting
    competition*. Expert systems with applications 39.8 (2012): 7067-7083'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are other ways we could use a deep learning neural network for multi-step-ahead
    forecasting. Three other popular methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recursive`: Training a neural network for one-step-ahead forecasting and using
    it recursively to get multi-step forecasts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Direct`: Training one neural network for each forecasting horizon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DirRec`: Training one neural network for each forecasting horizon and feeding
    the previous forecast as input to the next one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step and multi-output forecasting with multivariate time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll extend the LSTM model to predict multiple steps of several
    variables of a multivariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, in this chapter, we have built several models to forecast the future
    of one particular variable, solar radiation. We used the extra variables in the
    time series to improve the modeling of solar radiation.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, when working with multivariate time series, we’re often interested in forecasting
    several variables, not just one. A common example occurs when dealing with spatiotemporal
    data. A spatiotemporal dataset is a particular case of a multivariate time series
    where a real-world process is observed in different locations. In this type of
    dataset, the goal is to forecast the future values of all these locations. Again,
    we can leverage the fact that neural networks are multi-output algorithms to handle
    multiple target variables in a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll work with the solar radiation dataset, as in previous
    ones. However, our goal is to forecast the future values of three variables—solar
    radiation, vapor pressure, and air temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Regarding data preparation, the process is similar to what we did before. The
    difference is that we set the target variable (`TARGET`) to the preceding list
    of variables instead of just solar radiation. The `TimeSeriesDataSet` class and
    the data module handle all the preprocessing and data sharing for us.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by tweaking the data module to handle multiple target variables. In
    the following code, we make the necessary changes. Let’s start by defining the
    constructor of the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor contains a new argument, `target_variables`, which we use to
    pass the list of target variables. Besides that, we also make a small change to
    the `self.target_scaler` attribute, which is now a dictionary object that contains
    a scaler for each target variable. Then, we build the `setup()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The main differences from the previous recipe are the following. We pass the
    list of target variables to the target input of the `TimeSeriesDataSet` class.
    The scaling process of the target variables is also changed to a `for` loop that
    iterates over each target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also update the model module to process multiple target variables. Let’s
    start with the constructor and `forward()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `forward()` method is the same as in the previous chapter. We store a few
    more elements in the constructor, such as the forecasting horizon (`self.horizon`),
    as they are necessary in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We add an `n_output` parameter to the constructor, which details the number
    of target variables (in this example, `3`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output dimension is set to the number of target variables times the forecasting
    horizon (`self.n_output *` `self.horizon`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When processing the data in the training and testing steps, the predictions
    are reshaped into the appropriate format (batch size, horizon, and number of variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compute the MSE loss for each target variable, and then take the average
    across them using `torch.mean(torch.stack(loss))`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the remaining processes are similar to what we did in previous recipes
    based on PyTorch Lightning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The modeling approach used in this recipe follows the idea of **Vector Auto-Regression**
    (**VAR**). VAR works by modeling the future value of the variables of a multivariate
    time series as a function of the past values of all these variables. Predicting
    multiple variables may be relevant in several scenarios, such as spatiotemporal
    forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we adapted the VAR principle to a deep learning context, specifically
    through the use of LSTM networks. Unlike traditional VAR models that linearly
    project future values based on past observations, our deep learning model captures
    nonlinear relationships and temporal dependencies across multiple time steps and
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the `loss``()` function of our model—essential for training and evaluating
    its performance—we had to perform some changes in the `training_step()` and `test_step()`
    methods. After the network generates predictions, we segment the output by variable.
    This segmentation allows us to calculate the MSE loss for each variable separately.
    These individual losses are then aggregated to form a composite loss measure,
    which guides the optimization process of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing multiple time series for a global model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it is time to move on to the type of time series problems that involve
    multiple time series. In this recipe, we will learn the fundamentals of global
    forecasting models and how they work. We’ll also explore how to prepare a dataset
    that contains multiple time series for forecasting. Again, we leverage the capabilities
    of the `TimeSeriesDataSet` and `DataModule` classes to help us do this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve been working with time series problems involving a single dataset.
    Now, we’ll learn about global forecasting models, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transitioning from local to global models**: Initially, our work with time
    series forecasting focused on single datasets, where models predict future values
    based on historical data of one series. These so-called local models are tailored
    to specific time series, whereas global models involve handling multiple related
    time series and capturing relevant information across them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leveraging neural networks**: Neural networks excel in data-rich environments,
    making them ideal for global forecasting. This is particularly effective in domains
    such as retail, where understanding the relationships across different product
    sales can lead to more accurate forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll learn how to build a global forecasting model using a dataset concerning
    transportation called **NN5**. This dataset was used in a previous forecasting
    competition and includes 111 different time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is available in the `gluonts` Python library and can be loaded as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a sample of five of the time series in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Sample of the NN5 time series dataset](img/B21145_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Sample of the NN5 time series dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original source of this dataset is at the following link: [https://zenodo.org/records/3889750](https://zenodo.org/records/3889750).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s build a `DataModule` class to handle the data preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll build a `LightningDataModule` class that handles a dataset with multiple
    time series and passes them to a model. Here’s what it looks like starting with
    the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, we store the necessary elements for training and using the model.
    This includes a `self.target_scaler` attribute based on a `LocalScaler` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main method of the `LocalScaler` class is `transform()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This method applies two preprocessing operations to the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: A log transformation to stabilize the variance of the time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardization of each time series in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can extend this class to include any transformation that you need to perform
    on your dataset. The complete implementation of the `LocalScaler` class is available
    on the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we preprocess the data in the `setup()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we split the data into training, validation, testing,
    and prediction sets and set up the respective `TimeSeriesDataSet` instances. Finally,
    the data loaders are similar to what we’ve done in previous recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call the data module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using this module, each individual series in the dataset will be processed in
    such a way as to use the last `N_LAGS` values to predict the next `HORIZON` observations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Global methods are trained on multiple time series. The idea is that there are
    common patterns across the different time series. So, a neural network can use
    observations from these series to train better models.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, we retrieved a dataset involving several time series
    from the `gluonts` Python library via the `get_dataset``()` function. The process
    of preparing a dataset that contains multiple time series for supervised learning
    is similar to what we did before. The key input to the `TimeSeriesDataSet` instance
    is the `group_id` variable that details the entity to which each observation belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main work happens in the `setup()` method. First, we transform the dataset
    into a `pandas` DataFrame with a long format. Here’s a sample of this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Sample of the NN5 time series dataset in a long format](img/B21145_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Sample of the NN5 time series dataset in a long format'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the `group_id` column is not constant and details which time series
    the observation refers to. Since each time series is univariate, there’s a single
    numeric variable called `value`.
  prefs: []
  type: TYPE_NORMAL
- en: Training a global LSTM with multiple time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned how to prepare datasets with multiple time
    series for supervised learning with a global forecasting model. In this recipe,
    we continue this topic and describe how to train a global LSTM neural network
    for forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll continue with the same data module we used in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how to create an LSTM module to handle a data module with multiple
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a `LightningModule` class that contains the implementation of the
    LSTM. First, let’s look at the class constructor and the `forward()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic of the neural network is similar to what we’ve done for a dataset
    with a single time series. This is also true for the remaining methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can call the model and train it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Using the PyTorch Lightning design, the training, testing, and prediction steps
    are similar to what we did in other recipes based on this framework.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, the `LightningModule` class that contains the LSTM is identical
    to the one we built for a single multivariate time series. This class only deals
    with the part of the model definition, so no change is necessary. The main work
    is done during the data preprocessing stage. So, we only need to change the `setup()`
    method in the data module to reflect the necessary changes, which were explained
    in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We transitioned from a local LSTM model, designed for forecasting a single time
    series, to a global LSTM model capable of handling multiple time series simultaneously.
    The main difference lies in how the data is prepared and presented to the model
    than changes in the neural network architecture itself. Both local and global
    models utilize the same underlying LSTM structure, characterized by its ability
    to process sequences of data and predict future values.
  prefs: []
  type: TYPE_NORMAL
- en: In a local LSTM setup, the model’s input typically follows the structure [`batch_size`,
    `sequence_length`, `num_features`], with the output shaped to match the forecasting
    horizon, usually [`batch_size`, `horizon`]. This setup is straightforward as it
    deals with data from a single series.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting to a global LSTM model, the approach to input and output configuration
    remains fundamentally the same in terms of dimensionality. However, the input
    now aggregates information across multiple time series. It increases the ability
    of the neural network to learn new patterns and dependencies not just within a
    single series but across several. Consequently, the output of a global LSTM model
    is designed to produce forecasts for multiple time series simultaneously, reflecting
    predictions across the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Global forecasting models for seasonal time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to extend a data module to include extra explanatory variables
    in a `TimeSeriesDataSet` class and a `DataModule` class. We’ll use a particular
    case about seasonal time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We load the dataset that we used in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset contains time series with a daily granularity. Here, we’ll model
    weekly seasonality using the `Fourier` series. Unlike what we did in the previous
    chapter (in the *Handling seasonality: seasonal dummies and Fourier series* recipe),
    we’ll learn how to include these features using the `TimeSeriesDataSet` framework.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the updated `DataModule` that includes the `Fourier` series. We only
    describe part of the `setup()` method for brevity. The remaining methods stay
    the same, and you can check them in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `setup()` method, we compute the `Fourier` terms using the date and
    time information of the dataset. This leads to four deterministic variables: `sin_7_1`,
    `cos_7_1`, `sin_7_2`, and `cos_7_2`. These are `Fourier` series that we use to
    model seasonality. After adding them to the dataset using `pd.concat([tseries_long,
    fourier_features], axis=1)`, we use the `time_varying_known_reals` argument to
    tell that these features vary over time but in a predictable way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the LSTM, we need to update the input dimension to `5` to reflect the number
    of variables in the dataset (the target variable plus four `Fourier` series).
    This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Again, the training and inference stages are similar to the previous recipe
    since the only differences here are in the data preprocessing stage handled by
    the data module.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modeling seasonality with the `Fourier` series involves enriching the dataset
    with extra variables derived from the Fourier transformation. This approach was
    implemented in the `setup()` method of the `DataModule` instance, where these
    variables were incorporated into the `TimeSeriesDataSet` objects.
  prefs: []
  type: TYPE_NORMAL
- en: '`Fourier` series decomposition allows us to capture seasonality by breaking
    down complex periodic patterns into simpler, sinusoidal waves. Each component
    of the `Fourier` series corresponds to a different frequency, capturing different
    seasonal cycles within the time series data. This is particularly beneficial for
    neural networks for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Fourier` series acts as automatic feature engineering, creating informative
    features that directly encode periodic behaviors. This can significantly improve
    the ability of the model to recognize and predict seasonal patterns, even in complex
    or noisy data. Since Fourier features are added to the input data, they can work
    with any neural network algorithm or architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fourier` series can model these multiple seasonality levels simultaneously,
    providing a more nuanced representation of the data that can be difficult to achieve
    with traditional seasonal decomposition methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved generalization**: By providing a clear, mathematical representation
    of seasonality, Fourier features help neural networks to generalize better from
    the observed data to unseen future periods. This reduces the risk of overfitting
    noise and anomalies in the data, focusing the model’s learning on the underlying
    periodic trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can check the following URL to learn how to include extra categorical variables
    in the dataset (such as holidays): [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data).'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization using Ray Tune
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks have hyperparameters that define their structure and learning
    process. Hyperparameters include the learning rate or the number of hidden layers
    and units. Different hyperparameter values can affect the learning process and
    the accuracy of models. Incorrectly chosen values can result in underfitting or
    overfitting, which decreases the model’s performance. So, it’s important to optimize
    the value of hyperparameters to get the most out of deep learning models. In this
    recipe, we’ll explore how to do hyperparameter optimization using Ray Tune, including
    learning rate, regularization parameters, the number of hidden layers, and so
    on. The optimization of these parameters is very important to the performance
    of our models. More often than not, we face poor results in fitting neural network
    models simply due to poor selection of hyperparameters, which can lead to underfitting
    or overfitting unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we begin with hyperparameter optimization, we need to install Ray Tune,
    if it’s not already installed. This can be done using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same data and LSTM model to optimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we also made all the necessary imports for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s discuss how we can implement hyperparameter optimization using Ray Tune:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the search space**: First, define the hyperparameter space you want
    to explore.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Configure Ray Tune**: Initialize the Tune experiment with the desired settings,
    such as the number of trials, resources, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run the optimization**: Execute the experiment by passing the training function
    and the defined search space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analyze the results**: Utilize Ray Tune’s tools to analyze the results and
    identify the best hyperparameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s start by defining the search space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we only optimize two parameters: the number of hidden units
    and the number of layers in the LSTM neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the training cycle within a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the training function, we pass it to a `TorchTrainer` class
    instance, along with the running configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the `ScalingConfig` instance, we configured the computational environment,
    specifying whether the process should run on a GPU or CPU, the number of workers
    allocated, and the resources per worker. Meanwhile, the `RunConfig` instance is
    set to define the optimization process, including the metric that should be monitored
    throughout this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create a `Tuner` instance that combines this information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Tuner` instance requires a scheduler as one of its inputs. For this purpose,
    we utilize `ASHAScheduler`, which employs an **Asynchronous Successive Halving
    Algorithm** (**ASHA**) to efficiently allocate resources across various configurations.
    This method helps identify the most effective configuration by iteratively narrowing
    down the search space based on performance. Ultimately, by running this process,
    we can determine the optimal configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we get the configuration that minimizes the validation
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the best hyperparameters based on the validation loss, we can
    evaluate the model on the test set. Retrieve the model weights from the checkpoint
    and load the best hyperparameters from the tuning process. Then, use these parameters
    to load the model and evaluate it on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we load the model with the best configuration and test
    it in the test set defined in the `DataModule` class.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our hyperparameter optimization process involves defining a search space, configuring
    and executing the optimization, and analyzing the results. The code snippets shared
    in this section provide a step-by-step guide to integrating Ray Tune into any
    machine learning workflow, allowing us to explore and find the best hyperparameters
    for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: The `search_space` dictionary defines the hyperparameter search space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `train_tune()` function encapsulates the training process, including model
    configuration, data preparation, and fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ScalingConfig` class defines the computational environment for the optimization
    process, such as whether to run it on GPU or CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RunConfig` class sets up how the optimization is done, such as the metric
    that should be tracked during this process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ASHAScheduler` class is a scheduler that defines how to select from among
    different possible configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ray Tune efficiently explores the hyperparameter space using various algorithms
    such as Random Search, Grid Search, or more advanced methods such as ASHA. It
    parallelizes trials to utilize available resources effectively, hence speeding
    up the search process.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ray Tune offers several additional features and advantages. It can integrate
    with other libraries, making it compatible with popular machine learning frameworks
    such as PyTorch, TensorFlow, and Scikit-Learn. Moreover, it provides advanced
    search algorithms such as Bayesian Optimization and Population-Based Training,
    giving users the flexibility to experiment with different optimization strategies.
    Lastly, Ray Tune supports visualization tools, allowing users to utilize TensorBoard
    or custom tools provided by Ray to effectively visualize and analyze the hyperparameter
    search process.
  prefs: []
  type: TYPE_NORMAL
