- en: Generating Images Based on Label Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we got the first taste of the potential of GANs to
    learn the connections between latent vectors and generated images and made a vague
    observation that latent vectors somehow manipulate the attributes of images. In
    this chapter, we will officially make use of the label and attribute information
    commonly seen in open datasets to properly establish the bridge between latent
    vectors and image attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use **conditional GANs** (**CGANs**)
    to generate images based on a given label and how to implement adversarial learning
    with autoencoders and age human faces from young to old. Following this, you will
    be shown how to efficiently organize your source code for easy adjustments and
    extensions.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you will have learned both supervised and unsupervised
    approaches to improve the quality of the images generated by GANs with label and
    attribute information. This chapter also introduces the basic source code hierarchy
    throughout this book, which can be very useful for your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: CGANs – how are labels used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating images from labels with CGANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Fashion-MNIST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InfoGAN – unsupervised attribute extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CGANs – how are labels used?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that a relation between the latent vector
    and the generated images can be established by the training process of GANs and
    certain manipulation of the latent vectors is reflected by the changes in the
    generated images. But we have no control over what part or what kinds of latent
    vectors would give us images with the attributes we want. To address this issue,
    we will use a CGAN to add label information in the training process so that we
    can have a say in what kinds of images the model will generate.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of CGANs was proposed by Mehdi Mirza and Simon Osindero in their paper,
    *Conditional Generative Adversarial Nets*. The core idea was to integrate the
    label information into both generator and discriminator networks so that the label
    vector would alter the distribution of latent vectors, which leads to images with
    different attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the vanilla GAN model, CGAN makes a small change to the objective
    function to make it possible to include extra information by replacing the real
    data, ![](img/eaca6776-3f02-440c-9a80-4356f4487348.png), and generated data, ![](img/6326aae7-0bc5-4054-903b-0b2495f591cf.png), with
    ![](img/c4248bfa-8bfe-4af4-ad14-746e0d5d2dd5.png) and ![](img/98915d08-f8bc-4f40-af11-71b0a59b2cb0.png),
    respectively, in which ![](img/a6657c7b-fa05-41b0-a223-a17d531ff638.png) represents auxiliary
    information such as label and attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32861aa6-4c2c-4fcc-ba65-145237f1815b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, ![](img/61b7d781-89a5-497d-8fbc-3a376b09e790.png) borrows
    the form of conditional probability that describes how data ![](img/1f85531f-4c3a-4308-ae51-fa820cb8a117.png) is
    distributed under the condition of ![](img/61e3a5c4-e687-4d28-be72-5bb31c3fc309.png).
    To calculate the new object function, we need the generator network to be able
    to generate data given certain conditions and the discriminator network to tell
    whether the input image obeys the given condition. Therefore, in this section,
    we will talk about how to design the generator and discriminator to achieve this
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: We will create two different models in this chapter and, in order to write reusable
    code, we will put our source codes in separate files instead of putting all the code
    in to one single file as we did in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Combining labels with the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the generator network of the CGAN is illustrated as follows. As
    described in the original paper, all data is generated through an MLP-like network.
    Unlike in the original paper, however, we use a much deeper structure and techniques
    such as batch normalization and LeakyReLU to ensure better-looking results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/360f39cd-eb70-4a81-af55-0f39dec63c23.png)'
  prefs: []
  type: TYPE_IMG
- en: The generator network architecture of the CGAN
  prefs: []
  type: TYPE_NORMAL
- en: The label value is transformed into a vector with a length of 10, which is concatenated
    with the latent vector *z*. All data in the generator network is stored in the
    form of a vector. The length of the output vector equals the multiplication of
    width and height of the generated image, which is ![](img/c674568b-845e-4ceb-8acd-3dc0a733c1da.png) for
    the MNIST dataset. We can, of course, change the size of the output image to other
    values we want (we will set the image size to 64 x 64 later in the source code).
  prefs: []
  type: TYPE_NORMAL
- en: Let's organize the codes differently from previous chapters and create a `cgan.py`
    file for model definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the PyTorch and NumPy modules at the beginning of the source
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the `Generator` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The generator network consists of 5 linear layers, 3 of which are connected
    to batch normalization layers, and the first 4 linear layers have `LeakyReLU`
    activation functions while the last has a `Tanh` activation function. The label
    information is processed by the `nn.Embedding` module, which behaves as a lookup
    table. Say we have 10 labels at hand for training samples. The embedding layer
    transforms the 10 different labels into 10 pre-defined embedding vectors, which
    are initialized based on normal distribution by default. The embedding vector
    of labels is then concatenated with the random latent vector to serve as the input
    vector of the first layer. Finally, we need to reshape the output vector into
    2D images as the final results.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating labels into the discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the discriminator network of the CGAN is illustrated as
    follows. Again, the discriminator architecture is different from the one used
    in the original paper. You are, of course, more than welcome to make adjustments
    to the networks and see whether your models can generate better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65c76439-518e-45bd-b9b8-f83c697cc7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator network architecture of CGAN
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the generator network, the label value is also part of the input
    of the discriminator network of the CGAN. The input image (with a size of 28 x
    28) is transformed into a vector with a length of 784, therefore, the total length
    of the input vector of the discriminator network is 794\. There are 4 hidden layers
    in the discriminator network. Unlike common CNN models for image classification, the
    discriminator network outputs a single value instead of a vector with the length
    as the number of classes. It is because we already include the label information
    in the network input and we only want the discriminator network to tell us how close an
    image is to the real images, given the label condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define the discriminator network in the `cgan.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the labels are passed by another `nn.Embedding` module before being
    concatenated with the image vector. The discriminator network consists of 5 linear
    layers, 2 of which are connected to `Dropout` layers to enhance the generalization
    capability. Since we cannot always guarantee that the output values of the last
    layer lie within a range of [0, 1], we need a `Sigmoid` activation function to
    make sure of that.
  prefs: []
  type: TYPE_NORMAL
- en: A `Dropout` layer with a dropout rate of 0.4 means that, at each iteration during
    training, each neuron has a probability of 0.4 of not participating in the calculation
    of the final results. Therefore, different submodels are trained at different
    training steps, which makes it harder for the whole model to overfit the training
    data compared to the one without `Dropout` layers. `Dropout` layers are often
    deactivated during evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which layer has a `Dropout` or `LeakyReLU` activation function
    is rather subjective. You can try out other combinations and find out which configuration
    yields the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images from labels with the CGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we defined the architecture of both generator and discriminator
    networks of the CGAN. Now, let's write the code for model training. In order to
    make it easy for you to reproduce the results, we will use MNIST as the training
    set to see how the CGAN performs in image generation. What we want to accomplish
    here is that, after the model is trained, it can generate the correct digit image
    we tell it to, with extensive variety.
  prefs: []
  type: TYPE_NORMAL
- en: One-stop model training API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s create a new `Model` class that serves as a wrapper for different
    models and provides the one-stop training API. Create a new file named `build_gan.py`
    and import the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s create the `Model` class. In this class, we will initialize the
    `Generator` and `Discriminator` modules and provide `train` and `eval` methods
    so that users can simply call `Model.train()` (or `Model.eval()`) somewhere else
    to complete the model training (or evaluation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the generator network, `netG`, and the discriminator network, `netD`,
    are initialized based on the class number (`classes`), image channel (`channels`),
    image size (`img_size`), and length of the latent vector (`latent_dim`). These
    arguments will be given later. For now, let''s assume that these values are already
    known. Since we need to initialize all tensors and functions in this class, we
    need to define the `device` our model is running on (`self.device`). The `optim_G`
    and `optim_D` objects are optimizers for the two networks. They are initialized
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument of the `Adam` optimizer, `filter(lambda p: p.requires_grad,
    self.netG.parameters())`, is to grab all `Tensor` whose `requires_grad` flag is
    set to `True`. It is pretty useful when part of the model is untrained (for example,
    fine-tuning the last layer after transferring a trained model to a new dataset),
    even though it''s not necessary in our case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s define a method called `train` for model training. Arguments of
    `train` include the number of training epochs (`epochs`), the iteration interval
    between logging messages (`log_interval`), the output directory for results (`out_dir`),
    and whether to print training messages to the Terminal (`verbose`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In `train`, we first switch the networks to train mode (for example, `self.netG.train()`).
    It mostly affects the behaviors of `Dropout` and  the batch normalization layers.
    Then, we define a set of fixed latent vectors (`viz_noise`) and labels (`viz_label`).
    They are used to occasionally produce images during training so that we can track
    how the model is trained, otherwise, we may only realize the training has gone
    south after the training is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we omitted some parts of the code (including the evaluation API and model
    exporting and loading). You can get the full source code from the code repository
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Argument parsing and model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, the only thing left for us to do is to create and define the main entry
    for the project. In this file, we will need to define the arguments we previously
    have assumed to be known. These hyper-parameters are essential when we create
    any network, and we will elegantly parse these values. Let''s create a new file
    called `main.py` and import the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Have you noticed that the only Python module that's related to our model is `build_gan.Model`?
    We can easily create another project and copy most of the content in this file
    without major revisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let''s define the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have already defined the networks and training schedule in separate
    files, the initialization and training of the model are accomplished with only
    3 lines of codes: `model = Model()`, `model.create_optim()`, and `model.train()`.
    This way, our code is easy to read, modify, and maintain, and we can easily use
    most of the code in other projects.'
  prefs: []
  type: TYPE_NORMAL
- en: The `FLAGS` object stores all the arguments and hyper-parameters needed for
    model definition and training. To make the configuration of the arguments more
    user-friendly, we will use the `argparse` module provided by Python.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you would like to use a different dataset, you can change the definition
    of the `dataset` object in the same way as in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` entry of the source code and the definitions of arguments are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A new argument is created by `parser.add_argument(ARG_NAME, ARG_TYPE, DEFAULT_VALUE,
    HELP_MSG)`, in which `ARG_NAME` is the argument name, `ARG_TYPE` is the value
    type of argument (for example, `int`, `float`, `bool`, or `str`), `DEFAULT_VALUE`
    is the default argument value when none is given, and `HELP_MSG` is the message
    printed when running `python main.py --help` in the Terminal. The argument value
    is specified by `python main.py --ARG_NAME ARG_VALUE`, or you can change the default
    value in the source code and simply run `pythin main.py`. Here, our model is to
    be trained for 200 epochs with a batch size of 128\. The learning rate is set
    to 0.0002,  because a small learning rate value is suitable for the `Adam` method.
    The length of the latent vector is 100 and the size of the generated image is
    set to 64\. We also set the random seed to 1 so that you can produce the exact
    same results as in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean_string` is defined in the `utils.py` file, which is as follows (reference
    visit [https://stackoverflow.com/a/44561739/3829845](https://stackoverflow.com/a/44561739/3829845) for
    more information). Otherwise, passing `--train False` in the Terminal will not
    affect the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We still need to do some preprocessing on the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first make sure that CUDA is indeed available to PyTorch. Then, we
    manually set the random seed to the NumPy, PyTorch, and CUDA backend. We need
    to clear the output directory each time we retrain the model and all output messages
    are redirected to an external file, `log.txt`. Finally, we print all of the arguments
    taken before running the `main` function so that we may have a chance to check
    whether we have configured the model correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, open a Terminal and run the following script. Remember to change `DATA_DIRECTORY`
    to the path of the MNIST dataset on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output message may look like this (the order of the arguments might be
    different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes about 22 minutes to train for 200 epochs on a GTX 1080Ti graphics
    card and costs about 729 MB of GPU memory. The generated images from the MNIST
    dataset are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02600168-55c5-4106-b548-bce59d22737c.png)![](img/2532795f-15e5-4362-acd0-3f62db82621f.png)![](img/9a207fa4-3801-43a6-81ae-105e4e27de51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch;
    right: 200th epoch)'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the digit images are correctly generated for the corresponding
    labels while maintaining realistic variety in appearance. Because we treat the
    images as very long vectors in the model, it is hard to generate smoothness in
    both vertical and horizontal directions and it is easy to spot speckle noise in
    the generated images after only 25 epochs of training. However, the quality of
    the images gets a lot better after 200 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Fashion-MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So you know by now that the MNIST dataset is comprised of a bunch of handwritten
    numbers. It is the defacto standard for the Machine Learning community, and it
    is often used to validate processes. Another group has decided to create another
    dataset that could be a better replacement. This project is named **Fashion-MNIST** and
    is designed to be a simple drop-in replacement. You can get a deeper understanding
    of the project at [https://www.kaggle.com/zalando-research/fashionmnist/data#](https://www.kaggle.com/zalando-research/fashionmnist/data#).
  prefs: []
  type: TYPE_NORMAL
- en: '**Fashion-MNIST** consists of a training set of 60,000 images and labels and
    a test set of 10,000 images and labels. All images are grayscale and set to 28x28
    pixels, and there are 10 classes of images, namely: T-shirt/top, Trouser, Pullover,
    Dress, Coat, Saldal, Shirt, Sneaker, Bag, and Ankle boot. You can already begin
    to see that this replacement dataset should work the algorithms harder.'
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the use of the dataset, we will use the program that we just
    created for the standard MNIST dataset, and make a few changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the main.py file to `fashion-main.py` to keep the original safe. Now in
    the `fashion-main.py` file find the following portion of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It's the fourth line in the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all you need to change is the the `dset.MNIST(` to `dset.FashionMNIST(`
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Luckily, torchvision already has a built-in class for Fashion-MNIST. We'll point
    out a few others in a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Now save your source file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, to make sure that your dataset from the first example is safe, rename the
    Data folder that was used last time. The new dataset will automatically be downloaded
    for you. One other thing you should do is to rename your output folder, again
    to keep that safe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we did with the last program, we''ll start it with a command line entry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output in the Terminal will look pretty much like that of the last program,
    except for the lines showing the download of the new dataset information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of the output images you can expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b484efe-7cf1-4bbb-adcf-9cf7b26bd6f1.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left is the real sample data, in the middle is the result from epoch
    0, and finally on the right is the result from epoch 199.  While not perfect,
    you can see that the output is getting quite good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, I said that we would look at other classes that torchvision supports.
    There are too many to discuss here, but if you go to: [https://pytorch.org/docs/stable/torchvision/datasets.html](https://pytorch.org/docs/stable/torchvision/datasets.html),
    you can see the large list of each supported class and the API parameters. Many
    of them can be used as-is with our code, with the exception of modifying the dataset
    line in your code, and even allow the program to download the dataset for you.'
  prefs: []
  type: TYPE_NORMAL
- en: InfoGAN – unsupervised attribute extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we have learned how to use auxiliary information
    such as the labels of data to improve the image quality generated by GANs. However,
    it is not always possible to prepare accurate labels of training samples beforehand.
    Sometimes, it is even difficult for us to accurately describe the labels of extremely
    complex data. In this section, we will introduce another excellent model from
    the GAN family, **InfoGAN**, which is capable of extracting data attributes during
    training in an unsupervised manner. InfoGAN was proposed by Xi Chen, Yan Duan,
    Rein Houthooft, et. al. in their paper, *InfoGAN: Interpretable Representation
    Learning by Information Maximizing Generative Adversarial Nets*. It showed that
    GANs could not only learn to generate realistic samples but also learn semantic
    features, which are essential to sample generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to CGANs, InfoGAN also replaces the original distribution of data with
    conditional distribution (with auxiliary information as conditions). The main
    difference is that InfoGAN does not need to feed label and attribute information
    into the discriminator network; instead, it uses another classifier, Q, to measure
    how auxiliary features are learned. The objective function of InfoGAN is as follows.
    You may notice that it adds another objective, ![](img/79d47314-c5a5-43d0-aba0-ba44df6de818.png), at
    the end of the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcb9dc84-25a6-41b6-8ebe-bdcfa1679a8f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, ![](img/cbf2563c-2567-41b1-8fb9-d9378d5acbb3.png) is the generated
    sample, ![](img/9cfdd18b-8977-4117-85a1-cc2a0c46c587.png) is the latent vector,
    and ![](img/654bfdde-2fef-4d3e-91a0-4f17c16d495b.png) represent auxiliary information. ![](img/a35772fe-0a4e-4ad1-8b0b-be86133e22e5.png) describes
    the actual distribution of ![](img/509e60c1-6cca-4ace-a755-73b3061c0b63.png), which
    is rather hard to find. Therefore, we use the posterior distribution, ![](img/475265f4-181b-4049-81ec-0b07128db5a0.png), to
    estimate ![](img/6bc1c28f-753d-4dfd-940b-a0228b0ad023.png), and this process is
    done with a neural network classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding formula, ![](img/8136ddf4-79c1-4755-805c-944529966d0b.png) is,
    in fact, an approximation of **mutual information,** ![](img/1d81edb9-a9f6-437e-bddb-d2e0a675c288.png), between
    the auxiliary vector and generated sample. Mutual information, ![](img/ec123496-34f5-4261-bba5-46cc2d607f0c.png), describes
    how much we know about random variable *X* based on knowledge of *Y*—![](img/3afa941c-e215-4e7c-aab4-eb1c3da11316.png),
    in which ![](img/fb07f42f-3d25-44b5-9082-949730ee7887.png) is **entropy** and ![](img/6eae0ac6-7c94-429d-b139-c01f56f40ef6.png) is
    **conditional entropy**. It can also be described by the **Kullback–Leibler divergence**, ![](img/410d5388-e62a-4b72-906d-0d136f5d3614.png),
    which describes the information loss when we use marginal distributions to approximate
    the joint distribution of *X* and *Y*. You can refer to the original InfoGAN paper
    for detailed mathematical derivation. For now, you only need to know that ![](img/ffd1b3f6-526a-4e87-9b42-7a447788c35c.png) tells
    us whether the generation of ![](img/88503b5b-daaa-443d-899c-9b1e282c37b3.png) based
    on ![](img/ad5450c3-c36f-43c1-9a2b-bd936e9b1e73.png) goes as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Network definitions of InfoGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the generator network of InfoGAN is illustrated as follows.
    The reproduction of results from the original paper is rather tricky to handle.
    Therefore, we present a model architecture based on this GitHub repository, [https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96400795-ee88-4aea-8602-ff8f394dcf9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator architecture of InfoGAN
  prefs: []
  type: TYPE_NORMAL
- en: The generator network of InfoGAN consists of 4 hidden layers. The first hidden
    layer transforms the input vector with a length of 74 (*62+10+2*) into a length
    of 8,192 (*128*8*8*), which is then directly turned into tensor with the dimensionality
    of *128*8*8.* The feature maps are then gradually up-scaled to 32*32 images. The
    scaling of feature maps is done with `torch.nn.functional.interpolate`. We need
    to define a derived `Module` class for upsampling so that we can treat it as other
    `torch.nn` layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new source file called `infogan.py` and import the same Python
    modules as in `cgan.py` and define the `Upsample` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We use the `bilinear` method to up-scale the images because it's the best fit
    compared to other choices. Since we derive this class from `torch.nn.Module` and
    only use the functions from `torch` to perform the calculation in the forward
    pass, our custom class will have no trouble performing the gradient back-propagation
    in training.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch 1.0, calling `nn.Upsample` will give a deprecated warning and it
    is now, in fact, implemented with `torch.nn.functional.interpolate`. Therefore,
    our custom `Upsample` layer is the same as `nn.Upsample`, but without warning
    message.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this class, we use a helper function, `_create_deconv_layer`, to create the
    convolutional hidden layers. Since we will use the custom `Upsample` layer to
    increase the size of feature maps, we only need to use `nn.Conv2d`, whose input
    size equals, output size, rather than `nn.ConvTranspose2d` as in the DCGAN in
    the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In our configuration of InfoGAN, `torch.nn.functional.interpolate` combined
    with `nn.Conv2d` performs better than `nn.ConvTranspose2d` with stride. Although
    you are welcome to try out different configurations and see whether they produce
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the discriminator network of InfoGAN is illustrated as
    follows. Again, we use a different structure than in the original paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/780dbcbf-9c2c-443f-b87e-1c14b91e065e.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator architecture of InfoGAN
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator network consists of 4 hidden layers. As explained before,
    InfoGAN uses an additional classifier network to measure the validity of the auxiliary
    vector. In fact, this additional classifier shares most of its weights (the first
    4 hidden layers) with the discriminator. Therefore, the quality measure on the
    image is represented by a 1 x 1 tensor, which is a result of a linear layer at
    the end of the 4 hidden layers. The measurement of auxiliary information, which
    includes class fidelity and style fidelity, is obtained from two different groups
    of linear layers, in which the *128*2*2* feature maps are first mapped to 128-length
    vectors then mapped to output vectors with lengths of 10 and 2, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of the discriminator in PyTorch code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here, we treat the quality fidelity (`self.adv_loss`) as in an ordinary GAN
    model, the class fidelity (`self.class_loss`) as a classification problem (because
    label values are hard-coded, often in one-hot codes) and the style fidelity (`self.style_loss`)
    as an expectation maximization problem (because we want style vectors to obey
    certain random distribution). Therefore, cross-entropy (`torch.nn.CrossEntropyLoss`)
    and mean squared (`torch.nn.MSELoss`) loss functions are used for them.
  prefs: []
  type: TYPE_NORMAL
- en: We'd like explain why mean squared error is used for expectation maximization.
    We assume that the style vectors obey a normal distribution with a mean of 0 and
    a standard deviation of 1\. In the calculation of entropy, the logarithm of the
    probability of random variable is taken. The logarithm of **probability density
    function** (**pdf**) of the normal distribution ![](img/47a22187-f8e0-4124-a3e9-9f5ef9993390.png) is
    deduced to ![](img/1d1d3b79-700b-48b4-9935-48c9ffb365ed.png). Therefore, mean
    squared error is suited for such a purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation of InfoGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to make some adjustments to the training API so that we can make use
    of the class and style vectors for attribute extraction and image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add several imported modules in the `build_gan.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The default weight initialization provided by PyTorch easily leads to saturation,
    so we need a custom `weight` initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add the following lines in the definition of the `Model` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And we need to change the definitions of `self.netG` and `self.netD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add an optimizer for mutual information at the end of the `create_optim`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to make some adjustments to `train` method, in which we first
    train the generator and discriminator networks and update the two networks again
    based on auxiliary information. Here, we omit all of the `if self.infogan` statements
    and only show the training procedure for InfoGAN. Full source code can be referred
    to in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the fixed latent vectors for result visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `self._to_onehot` method is responsible for transforming the label
    values to one-hot coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'A training iteration for InfoGAN includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Training the generator with fake data and letting the discriminator see them
    as real ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the discriminator with both real and fake data to increase its ability
    to distinguish them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training both generator and discriminator so that the generator can produce
    samples with good quality based on given auxiliary information and the discriminator
    can tell whether the generated samples obey the distribution of given auxiliary
    information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We don''t need to change anything in the `main.py` file at all, and we can simply
    run the following script in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes about 2 hours to finish 200 epochs of training and costs about 833
    MB GPU memory on a GTX 1080Ti graphics card. The results produced during training
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fa92731-6e17-4dfe-83a9-b2aa5fd986bb.png)![](img/d9b0754d-2e98-42c8-b71e-f8e6c754a753.png)![](img/dae5ea62-3aff-4a3e-8c19-b32c49227292.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch;
    right: 200th epoch)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training is done, run the following script to perform the model evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `model.eval()` with `mode=0` or `mode=1` will tell us what the two
    values in the style vector are responsible for, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d1c962e-7cd5-43de-bf24-47691f80c5c6.png)![](img/25924586-1b46-4b84-b7b2-d4439f2f37a5.png)'
  prefs: []
  type: TYPE_IMG
- en: The first style bit (mode=0) controls the digit angle, and the second style
    bit (mode=1) controls the width of strokes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the style vector values is responsible for the angle of digits, and the
    other is responsible for the width of strokes, just as the original InfoGAN paper
    proclaims. Imagine what this technique is capable of on complex datasets and an
    elaborate training configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We can do a lot more with CGANs and similar. For example, the labels can be
    more than for images. An individual pixel in the image can certainly have its
    own label. In the next chapter, we will look into how GANs perform on pixel-wise
    labels and we can do interesting things, more than hand-written digits and human
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mirza M and Osindero S. (2014). *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hui J. (Jun 3, 2018). *GAN — CGAN & InfoGAN (using labels to improve GAN)*.
    Retrieved from [https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d](https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhang Z and Song Y and Qi H. (2017). *Age Progression/Regression by Conditional
    Adversarial Autoencoder*. CVPR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chen X, Duan Y, Houthooft R. (2016). *InfoGAN: Interpretable Representation
    Learning by Information Maximizing Generative Adversarial Nets*. arXiv preprint
    arXiv:1606.03657.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discovered **Conditional Generative Adversarial Networks**
    (**CGANs**), which worked with MNIST and Fashion-MNIST, and we learned about using
    the InfoGAN model, which again, worked with MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will learn about image-to-image translation, which I
    truly believe you will find exciting and very relevant in today's world.
  prefs: []
  type: TYPE_NORMAL
