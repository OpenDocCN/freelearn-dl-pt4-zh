- en: Simulating Control Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw the notable success of **deep Q-learning** (**DQN**)
    in training an AI agent to play Atari games. One limitation of DQN is that the
    action space must be discrete, namely, only a finite number of actions are available
    for the agent to select and the total number of actions cannot be too large. However,
    many practical tasks require continuous actions, which makes DQN difficult to
    apply. A naive remedy for DQN in this case is discretizing the continuous action
    space. But this remedy doesn't work due to the curse of dimensionality, meaning
    that DQN quickly becomes infeasible and does not generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will discuss deep reinforcement learning algorithms for control
    tasks with a continuous action space. Several classic control tasks, such as CartPole,
    Pendulum, and Acrobot, will be introduced first. You will learn how to simulate
    these tasks using Gym and understand the goal and the reward for each task. Then,
    a basic actor-critic algorithm, called the **deterministic policy gradient** (**DPG**),
    will be represented. You will learn what the actor-critic architecture is, and
    why these kinds of algorithms can address continuous control tasks. Besides this,
    you will also learn how to implement DPG via Gym and TensorFlow. Finally, a more
    advanced algorithm, called the **trust region policy optimization** (**TRPO**),
    will be introduced. You will understand why TRPO works much better than DPG and
    how to learn a policy by applying the conjugate gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter requires some background knowledge of mathematical programming
    and convex/non-convex optimization. Don't be afraid-we will discuss these algorithms
    step by step to make sure that you fully understand the mechanism behind them.
    Understanding why they work, when they cannot work, and what their advantages
    and disadvantages are is much more important than simply knowing how to implement
    them with Gym and TensorFlow. After finishing this chapter, you will understand
    that the magic show of deep reinforcement learning is directed by mathematics
    and deep learning together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to classic control tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic policy gradient methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trust region policy optimization for complex control tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to control tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI Gym offers classic control tasks from the classic reinforcement learning
    literature. These tasks include CartPole, MountainCar, Acrobot, and Pendulum.
    To find out more, visit the OpenAI Gym website at: [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control).
    Besides this, Gym also provides more complex continuous control tasks running
    in the popular physics simulator MuJoCo. Here is the homepage for MuJoCo: [http://www.mujoco.org/](http://www.mujoco.org/).
    MuJoCo stands for Multi-Joint Dynamics with Contact, which is a physics engine
    for research and development in robotics, graphics, and animation. The tasks provided
    by Gym are Ant, HalfCheetah, Hopper, Humanoid, InvertedPendulum, Reacher, Swimmer,
    and Walker2d. These names are very tricky, aren''t they? For more details about
    these tasks, please visit the following link: [https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)[.](http://www.mujoco.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you don''t have a full installation of OpenAI Gym, you can install the `classic_control`
    and `mujoco` environment dependencies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'MuJoCo is not open source, so you''ll have to follow the instructions in `mujoco-py` (available
    at [https://github.com/openai/mujoco-py#obtaining-the-binaries-and-license-key](https://github.com/openai/mujoco-py#obtaining-the-binaries-and-license-key))
    to set it up. After the classic control environment is installed, try the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If it runs successfully, a small window will pop up, showing the screen of
    the Acrobot task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f7814e7-22ab-410e-901e-c217a06af6ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Besides Acrobot, you can replace the `Acrobot-v1` task name with `CartPole-v0`,
    `MountainCarContinuous-v0`, and `Pendulum-v0` to check out the other control tasks.
    You can run the following code to simulate these tasks and try to get a high-level
    understanding of their physical properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Gym uses the same interface for all the tasks, including Atari games, classic
    control tasks, and MuJoCo control tasks. At each step, an action is randomly drawn
    from the action space by calling `task.env.action_space.sample()` and then this
    action is submitted to the simulator via `task.step(action)`, which tells the
    simulator to execute it. The `step` function returns the observation and the reward
    corresponding to this action.
  prefs: []
  type: TYPE_NORMAL
- en: The classic control tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now go through the details of each control task and answer the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the control inputs and the corresponding feedbacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the reward function defined?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the action space continuous or discrete?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the details of these control tasks is quite important for designing
    proper reinforcement learning algorithms because their specifications, such as
    the dimension of the action space and the reward function, can affect the performance
    a lot.
  prefs: []
  type: TYPE_NORMAL
- en: 'CartPole is quite a famous control task in both the control and reinforcement
    learning communities. Gym implements the CartPole system described by *Barto,
    Sutton, and Anderson* in their paper *Neuronlike Adaptive Elements That Can Solve
    Difficult Learning Control Problem*, 1983\. In CartPole, a pole is attached by
    an un-actuated joint to a cart, which moves along a frictionless track, as illustrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e679e07-1c03-4306-9a0e-e182bd2e43f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the specifications of CartPole:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Goal** | The goal is to prevent the pole from falling over. |'
  prefs: []
  type: TYPE_TB
- en: '| **Action** | The action space is discrete, namely, the system is controlled
    by applying a force of +1 (right direction) and -1 (left direction) to the cart.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Observation** | The observation is a vector with four elements, for example,
    [ 0.0316304, -0.1893631, -0.0058115, 0.27025422], which describe the positions
    of the pole and the cart. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reward** | A reward of +1 is provided for every timestep that the pole
    remains upright. |'
  prefs: []
  type: TYPE_TB
- en: '| **Termination** | The episode ends when the pole is more than 15 degrees
    from vertical, or the cart moves more than 2.4 units from the center. |'
  prefs: []
  type: TYPE_TB
- en: Because this chapter talks about solving continuous control tasks, we will later
    design a wrapper for CartPole to convert its discrete action space into a continuous
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'MountainCar was first described by Andrew Moore in his PhD thesis *A. Moore,
    Efficient Memory-Based Learning for Robot Control*, 1990, which is widely applied
    as the benchmark for control, **Markov decision process** (**MDP**), and reinforcement
    learning algorithms. In MountainCar, a small car is on a one-dimensional track,
    moving between two mountains and trying to reach the yellow flag, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05ae469-0ba1-4cef-811e-584dc93ba924.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following table provides its specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Goal** | The goal is to reach the top of the right mountain. However, the
    car''s engine is not strong enough to scale the mountain in a single pass. Therefore,
    the only way to succeed is to drive back and forth to build up momentum. |'
  prefs: []
  type: TYPE_TB
- en: '| **Action** | The action space is continuous. The input action is the engine
    force applied to the car. |'
  prefs: []
  type: TYPE_TB
- en: '| **Observation** | The observation is a vector with two elements, for example,
    [-0.46786288, -0.00619457], which describe the velocity and the position of the
    car. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reward** | The reward is greater if you spend less energy to reach the
    goal. |'
  prefs: []
  type: TYPE_TB
- en: '| **Termination** | The episode ends when the car reaches the goal flag or
    the maximum number of steps is reached. |'
  prefs: []
  type: TYPE_TB
- en: 'The Pendulum swing-up problem is a classic problem in the control literature
    and is used as a benchmark for testing control algorithms. In Pendulum, a pole
    is attached to a pivot point, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b3f9b21-e5d5-4f08-bb4c-fc5edac9557b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the specifications of Pendulum:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Goal** | The goal is to swing the pole up so it stays upright and to prevent
    it from falling over. |'
  prefs: []
  type: TYPE_TB
- en: '| **Action** | The action space is continuous. The input action is the torque
    applied to the pole. |'
  prefs: []
  type: TYPE_TB
- en: '| **Observation** | The observation is a vector with three elements, for example,
    [-0.19092327, 0.98160496, 3.36590881], which indicate the angle and angular velocity
    of the pole. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reward** | The reward is computed by a function with the angle, angular
    velocity, and the torque as the inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| **Termination** | The episode ends when the maximum number of steps is reached.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Acrobot was first described by Sutton in the paper *Generalization in Reinforcement
    Learning: Successful Examples Using Sparse Coarse Coding*, 1996\. The Acrobot
    system includes two joints and two links, where the joint between the two links
    is actuated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0528244b-5626-4c4e-841b-2ece663eb886.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the settings of Acrobot:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Goal** | The goal is to swing the end of the lower link up to a given height.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Action** | The action space is discrete, namely, the system is controlled
    by applying a torque of 0, +1 and -1 to the links. |'
  prefs: []
  type: TYPE_TB
- en: '| **Observation** | The observation is a vector with six elements, for example,
    [0.9926474, 0.12104186, 0.99736744, -0.07251337, 0.47965018, -0.31494488], which
    describe the positions of the two links. |'
  prefs: []
  type: TYPE_TB
- en: '| **Reward** | A reward of +1 is provided for every timestep where the lower
    link is at the given height or, otherwise, -1. |'
  prefs: []
  type: TYPE_TB
- en: '| **Termination** | The episode ends when the end of the lower link is at the
    given height, or the maximum number of steps is reached. |'
  prefs: []
  type: TYPE_TB
- en: 'Note that, in Gym, both CartPole and Acrobot have discrete action spaces, which
    means these two tasks can be solved by applying the deep Q-learning algorithm.
    Well, because this chapter considers continuous control tasks, we need to convert
    their action spaces into continuous ones. The following class provides a wrapper
    for Gym classic control tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For CartPole and Acrobot, the input action should be a probability vector indicating
    the probability of selecting each action. In the `play_action` function, an action
    is randomly sampled based on this probability vector and submitted to the system.
    The `get_total_reward` function returns the total reward in one episode. The `get_action_dim` and
    `get_state_dim` functions return the dimension of the action space and the observation,
    respectively. The `get_activation_fn` function is used for the output layer in
    the actor network, which we will discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in the previous chapter, DQN uses the Q-network to estimate the
    `state-action value` function, which has a separate output for each available
    action. Therefore, the Q-network cannot be applied, due to the continuous action
    space. A careful reader may remember that there is another architecture of the
    Q-network that takes both the state and the action as its inputs, and outputs
    the estimate of the corresponding Q-value. This architecture doesn''t require
    the number of available actions to be finite, and has the capability to deal with
    continuous input actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4a36225-b99e-4587-941e-2149251fecc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we use this kind of network to estimate the `state-action value` function,
    there must be another network that defines the behavior policy of the agent, namely
    outputting a proper action given the observed state. In fact, this is the intuition
    behind actor-critic reinforcement learning algorithms. The actor-critic architecture
    contains two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor**: The actor defines the behavior policy of the agent. In control tasks,
    it outputs the control signal given the current state of the system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Critic**: The critic estimates the Q-value of the current policy. It can
    judge whether the policy is good or not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, if both the actor and the critic can be trained with the feedbacks
    (state, reward, next state, termination signal) received from the system, as in
    training the Q-network in DQN, then the classic control tasks will be solved.
    But how do we train them?
  prefs: []
  type: TYPE_NORMAL
- en: The theory behind policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One solution is the **deep deterministic policy gradient** (**DDPG**) algorithm,
    which combines the actor-critic approach with insights from the success of DQN.
    This is discussed in the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra and M. Riedmiller. *Deterministic
    policy gradient algorithms*. In ICML, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver
    and D. Wierstra. *Continuous control with deep reinforcement learning*. In ICLR,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason why DDPG is introduced first is that it is quite similar to DQN,
    so you can understand the mechanism behind it much more easily after finishing
    the previous chapter. Recall that DQN is able to train the Q-network in a stable
    and robust way for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q-network is trained with the samples randomly drawn from the replay memory
    to minimize the correlations between samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A target network is used to estimate the target Q-value, reducing the probability
    that oscillation or divergence of the policy occurs. DDPG applies the same strategy,
    which means that DDPG is also a model-free and off-policy method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use the same notations as in the previous chapter for the reinforcement
    learning setting. At each timestep ![](img/0213555b-9970-4ebb-a299-93cecc221371.png),
    the agent observes state ![](img/37a22f30-aa50-4a09-bb47-2e17e53160d9.png), takes
    action ![](img/3c05b5fd-7a4c-4908-bf33-124ab777c762.png) ,and then receives the
    corresponding reward ![](img/fc6a21d6-8165-4005-a792-6d83e1f304db.png) generated
    from a function ![](img/87145533-c582-40ee-8cda-be95c2ccc08c.png). Instead of
    using ![](img/f25863dd-e76e-4b76-9500-1e1d5e13c773.png) to represent the set of
    all the available actions at state ![](img/dd850911-cd3e-4b3a-be3d-0c9f40a99565.png),
    here, we use ![](img/77bd2c0f-79ee-4a83-b663-17c921f87f38.png) to denote the policy
    of the agent, which maps states to a probability distribution over the actions.
    Many approaches in reinforcement learning, such as DQN, use the Bellman equation
    as the backbone:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d33ff51d-b88d-4860-a676-3bed39d88850.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference between this formulation and the one in DQN is that the
    policy ![](img/2735e862-bbe8-4a3b-a3cf-4e13d9f6316a.png) here is stochastic, so
    that the expectation of ![](img/9b74ca4d-4795-42e5-9963-782190843add.png) is taken
    over ![](img/63b79d7f-ff2c-4b13-b7d9-67134f921b91.png). If the target policy ![](img/df3852c6-b7d4-48f2-906c-85e20d45b6d5.png)
    is deterministic, which can be described as a function ![](img/03d19098-5a51-4ecf-9349-97d235f0572c.png),
    then this inner expectation can be avoided:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e5b2b35-ed62-474b-9d78-9ab73c8cd725.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation depends only on the environment. This means that it is possible
    to learn the `state-action value` function ![](img/0e72e393-18cf-49e8-987b-1665d38289b7.png)
    off-policy, using transitions that are generated from other policies, as we did
    in DQN. The function ![](img/11aa1617-ea61-409b-a303-bf28c1aa4cb0.png), the critic,
    can be approximated by a neural network parameterized by ![](img/d3a5c035-972e-4dda-8af5-497e2bde1689.png)
    and the policy ![](img/bf619c4b-02a7-4134-a83e-c7c5f7740445.png), the actor, can
    also be represented by another neural network parameterized by ![](img/0beff831-f282-44be-a4c1-fa3abdc824d9.png)
    (in DQN, ![](img/afb1eec9-f424-4f1b-a510-4e582f3dfa11.png) is just ![](img/b7c37d7d-ea25-475a-9b07-4e839a6024fa.png)).
    Then, the critic ![](img/e84ee47b-b0f0-4ab4-9c06-807b68db4548.png) can be be trained
    by minimizing the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8885b2f4-a2b1-4a7f-93d1-06c763aa57f4.png),'
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/373f36de-a6b8-4fdd-9f4c-199efec86702.png). As in DQN, ![](img/db4d322c-2054-423a-8834-466d98288b33.png)
    can be estimated via the target network and the samples for approximating ![](img/761e8806-ef76-48da-ac45-a8f533f2277e.png)
    can be randomly drawn from the replay memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the actor ![](img/65c68316-35e0-4352-b0bd-4512ce079d3a.png), we fix
    the critic ![](img/baeaa310-3fb3-4cfd-9786-5b9595570c8b.png), learned by minimizing
    the loss function ![](img/51ce9801-6891-4d3b-aa28-5fcd61d91376.png), and try to
    maximize ![](img/8a50c452-1c5f-41b1-8dc7-8f01a0a8686d.png) over ![](img/f20dd0ca-a41a-4750-95d3-86434c885672.png),
    since a larger Q-value means a better policy. This can be done by following the
    applying the chain rule to the expected return with respect to the actor parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b67bb9db-a7c5-40cf-8fcb-1daf7adb257e.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the high-level architecture of DDPG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4a8706-e5a0-400f-bf01-eeac828b66c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compared to DQN, there is a small difference in updating the target network.
    Instead of directly copying the weights of ![](img/83eb027c-4020-42e0-8222-63ccf5903e5b.png)
    to the target network after several iterations, a soft update is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74e5cb42-fed1-4443-a51e-0b0a56c6ad40.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/4aa85430-4001-4131-b53c-e9d49369d0ee.png) represents the weights
    of the target network. This update means that the target values are constrained
    to change slowly, greatly improving the stability of learning. This simple change
    moves the relatively unstable problem of learning the value function closer to
    the case of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to DQN, DDPG also needs to balance exploration and exploitation during
    the training. Since the action generated by the policy ![](img/9df779a4-c193-41f5-8bd6-380ca55498d2.png)
    is continuous, the ![](img/9a1c3ea8-ddcc-4e58-ad65-ec7cae3f99d7.png)-greedy method
    cannot be applied. Instead, we can construct an exploration policy ![](img/d75015e2-a728-46e8-9712-98640b2121d9.png)
    by adding noise sampled from a distribution ![](img/bf171446-b357-49f3-a646-c1cdb3b58a70.png)
    to the actor policy ![](img/f0bfd658-9b98-4c21-a175-efd17e3715b3.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dde2dd51-dbd4-48c5-98d5-8cfab67139c5.png) where ![](img/3d8a98da-c78f-43e5-b978-ca2258dddb19.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4d4c9de1-140a-446c-a4c9-77d2d42b8185.png) can be chosen as ![](img/90caf88c-219a-40e7-8fe4-12dc270d38ce.png),
    where ![](img/c8166a78-9468-4ac9-b5d1-b82e4e5e753c.png) is the standard Gaussian
    distribution and ![](img/a1072a0b-8629-45ef-a059-7c0de83c8a87.png) decreases during
    each training step. Another choice is to apply an Ornstein-Uhlenbeck process to
    generate the exploration noise ![](img/d90e6e65-59e3-447f-9a31-42a692177ee9.png).'
  prefs: []
  type: TYPE_NORMAL
- en: DPG algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following pseudo code shows the DDPG algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: There is a natural extension of DDPG by replacing the feedforward neural networks
    used for approximating the actor and the critic with recurrent neural networks.
    This extension is called the **recurrent deterministic policy gradient** algorithm
    (**RDPG**) and is discussed in the f paper N. Heess, J. J. Hunt, T. P. Lillicrap
    and D. Silver. *Memory-based control with recurrent neural networks*. 2015.
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent critic and actor are trained using **backpropagation through time**
    (**BPTT**). For readers who are interested in it, the paper can be downloaded
    from [https://arxiv.org/abs/1512.04455](https://arxiv.org/abs/1512.04455).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will show you how to implement the actor-critic architecture using
    TensorFlow. The code structure is almost the same as the DQN implementation that
    was shown in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ActorNetwork` is a simple MLP that takes the observation state as its
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor requires four arguments: `input_state`, `output_dim`, `hidden_layers`,
    and `activation`. `input_state` is a tensor for the observation state. `output_dim` is
    the dimension of the action space. `hidden_layers` specifies the number of the
    hidden layers and the number of units for each layer. `activation` indicates the
    activation function for the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CriticNetwork` is also a MLP, which is enough for the classic control
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The network takes the state and the action as its inputs. It first maps the
    state into a hidden feature representation and then concatenates this representation
    with the action, followed by several hidden layers. The output layer generates
    the Q-value that corresponds to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor-critic network combines the actor network and the critic network
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor requires six arguments, as follows: `input_dim` and `action_dim`
    are the dimensions of the state space and the action space, respectively. `critic_layers`
    and `actor_layers` specify the hidden layers of the critic network and the actor
    network.  `actor_activation` indicates the activation function for the output
    layer of the actor network. `scope` is the scope name used for the `scope` TensorFlow
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: The constructor first creates an instance of the `self.actor_network` actor
    network with an input of `self.x,` where `self.x` represents the current state.
    It then creates an instance of the critic network using the following as the inputs: `self.actor_network.get_output_layer()` as
    the output of the actor network and `self.x` as the current state. Given these
    two networks, the constructor calls `self._build()` to build the loss functions
    for the actor and critic that we discussed previously. The actor loss is `-tf.reduce_mean(value)`,
    where `value` is the Q-value computed by the critic network. The critic loss is
    `0.5 * tf.reduce_mean(tf.square((value - self.y)))`, where `self.y` is a tensor
    for the predicted target value computed by the target network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class `ActorCriticNet` provides the functions for calculating the action
    and the Q-value given the current state, that is, `get_action` and `get_value`.
    It also provides `get_action_value`, which computes the `state-action value` function
    given the current state and the action taken by the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Because DPG has almost the same architecture as DQN, the implementations of
    the replay memory and the optimizer are not shown in this chapter. For more details,
    you can refer to the previous chapter or visit our GitHub repository ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    By combining these modules together, we can implement the `DPG` class for the
    deterministic policy gradient algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `config` includes all the parameters of DPG, for example, batch size
    and learning rate for training. The `task` is an instance of a certain classic
    control task. In the constructor, the replay memory, Q-network, target network,
    and optimizer are initialized by calling the `_init_modules` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `choose_action` function selects an action based on the current estimate
    of the actor-critic network and the observed state.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a Gaussian noise controlled by `epsilon` is added for exploration.
  prefs: []
  type: TYPE_NORMAL
- en: The `play` function submits an action into the simulator and returns the feedback
    from the simulator. The `update_target_network` function updates the target network
    from the current actor-critic network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin the training process, the following function can be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In each episode, it calls `replay_memory.phi` to get the current state and calls
    the `choose_action` function to select an action based on the current state. This
    action is submitted into the simulator by calling the `play` function, which returns
    the corresponding reward, next state, and termination signal. Then, the `(current
    state, action, reward, termination)` transition is stored into the replay memory.
    For every `update_interval` step (`update_interval = 1` ,by default), the actor-critic
    network is trained with a batch of transitions that are randomly sampled from
    the replay memory. For every `time_between_two_copies` step, the target network
    is updated and the weights of the Q-network are saved to the hard disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training step, the following function can be called for evaluating
    the performance of our trained agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full implementation of DPG can be downloaded from our GitHub ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    To train an agent for CartPole, run the following command under the `src` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: There are two arguments in `train.py`. One is `-t`, or `--task`, indicating
    the name of the classic control task you want to test. The other one is `-d`,
    or `--device`, which specifies the device (CPU or GPU) that you want to use to
    train the actor-critic network. Since the dimensions of the state spaces of these
    classic control tasks are relatively low compared to the Atari environment, using
    the CPU to train the agent is fast enough. It should only take several minutes
    to finish.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training, you can open a new Terminal and type the following command
    to visualize both the architecture of the actor-critic network and the training
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `logdir` points to the folder where the `CartPole-v0` log file is stored.
    Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86309e48-df49-4dc5-91d1-53a3b33e5f6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Tensorboard view
  prefs: []
  type: TYPE_NORMAL
- en: The top two graphs show the changes of the actor loss and the critic loss against
    the training step. For classical control tasks, the actor loss usually decreases
    consistently, while the critic loss has a large fluctuation. After 60,000 training
    steps, the score becomes stable, achieving 200, the highest score that can be
    reached in the CartPole simulator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a similar command, you can also train an agent for the `Pendulum` task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, check the training procedure via `Tensorboard`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the changes of the score during  training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af8e21c-55b9-405d-ac5a-1fc545f87f55.png)'
  prefs: []
  type: TYPE_IMG
- en: Changes in score during training
  prefs: []
  type: TYPE_NORMAL
- en: 'A careful reader may notice that the score of Pendulum fluctuates widely compared
    to the score of CartPole. There are two reasons that are causing this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: In Pendulum, the starting position of the pole is not deterministic, namely,
    it may be different for two episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The train procedure in DPG may not be always stable, especially for complicated
    tasks, such as MuJoCo control tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MuJoCo control tasks, for example, Ant, HalfCheetah, Hopper, Humanoid,
    InvertedPendulum, Reacher, Swimmer, and Walker2d provided by Gym, have high-dimensional
    state and action space, which makes DPG fail. If you are curious about what happens
    when running DPG with the `Hopper-v0` task, you can try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: After several minutes, you will see that DPG cannot teach Hopper how to walk.
    The main reason why DPG fails in this case is that the simple actor and critic
    updates discussed here become unstable with high-dimensional inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Trust region policy optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **trust region policy optimization** (**TRPO**) algorithm was proposed
    to solve complex continuous control tasks in the following paper: Schulman, S.
    Levine, P. Moritz, M. Jordan and P. Abbeel. *Trust Region Policy Optimization*.
    In ICML, 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand why TRPO works requires some mathematical background. The main
    idea is that it is better to guarantee that the new policy, ![](img/31f8d3cd-e898-4f0e-8ba2-b373935baa9e.png),
    optimized by one training step, not only monotonically decreases the optimization
    loss function (and thus improves the policy), but also does not deviate from the
    previous policy ![](img/312b08f4-832f-453a-8d90-e4d0f2d9c0bb.png) much, which
    means that there should be a constraint on the difference between ![](img/8fe518c7-f75b-44df-9f5b-e0299bf3ee1a.png)
    and ![](img/d8d38231-bc91-43c4-a75f-8f3d39b6d8ca.png), for example, ![](img/dba88e62-aaac-48df-8696-ab24ba291e01.png)
    for a certain constraint function ![](img/87c66a83-5938-4481-8b63-92a08ddd9b81.png)
    constant ![](img/5b9d38d9-0b1e-443b-b93f-0b124e2af135.png).
  prefs: []
  type: TYPE_NORMAL
- en: Theory behind TRPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the mechanism behind TRPO. If you feel that this part is hard to
    understand, you can skip it and go directly to how to run TRPO to solve MuJoCo
    control tasks. Consider an infinite-horizon discounted Markov decision process
    denoted by ![](img/4382afa1-271c-4016-bcf0-682471689d03.png), where ![](img/fa7e1576-fa63-4912-9f28-766c74f39662.png)
    is a finite set of states, ![](img/b8bf6678-7e34-4238-bfae-95ca5d706259.png) is
    a finite set of actions, ![](img/ee138461-338b-4461-9c8f-f852cea0201b.png) is
    the transition probability distribution, ![](img/4e72cb1f-a0b0-45c3-96df-eb65d2815c6a.png)
    is the cost function, ![](img/8bcde0ac-2124-40cb-ae6b-fbacc977fa36.png) is the
    distribution of the initial state, and ![](img/61ab4a6b-a890-4de7-a592-f11acc0c5e6c.png)
    is the discount factor. Let ![](img/779c9c94-3227-4173-bc9b-47a496df8645.png)
    be a stochastic policy that we want to learn by minimizing the following expected
    discounted cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b2b2d3b-82ce-4be8-9268-804c004a1c75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, this is ![](img/c6326a20-968a-4c4b-8edc-ef2a78164482.png), ![](img/4b3f52e1-26c7-469c-84ed-afc5dde95507.png) and ![](img/687aa323-eee8-4230-8c65-e74bae743a2e.png). The
    definitions of the `state-action value` function ![](img/ec709246-272b-4d6e-a633-8650958a88d8.png),
    the value function ![](img/fd5bb2d1-02ca-4612-b937-a28495e2122b.png) ,and the
    advantage function ![](img/e2731530-62ec-47a3-ad8f-0e4f0fa96b4d.png) under policy ![](img/f0c57c4c-00ae-41c7-ae5b-504777148477.png)
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ecdecae-c636-4f18-8a0f-d9f1852b3f55.png)![](img/dbe7950e-c726-4be1-ae1d-571772cd0ec3.png)![](img/695cfd42-af47-4f7a-a1df-2c19428aabb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, this is ![](img/8ad044e8-e6bc-40d4-93c0-37581d8858a6.png) and ![](img/e487be73-6cde-4217-8024-e4f7cd401537.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to improve policy ![](img/19d0debc-2372-4819-a9b2-ac5e04248f11.png)
    (by reducing the expected discounted cost) during each training step. In order
    to design an algorithm monotonically improving ![](img/68b1c497-e041-4391-8a5b-36ea39ab6685.png),
    let''s consider the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f20b954f-9a2d-4cbf-94c3-27e99e78982c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, this is ![](img/4878a2f8-03b8-4c93-8ca6-ebc08fc5382b.png), ![](img/311043e8-58ca-4b6b-89e5-f82e712a6f04.png) and ![](img/324cf7f3-59a2-460b-84a2-efef176891c5.png).
    This equation holds for any policy ![](img/81d9e91f-d378-4c87-a354-414d1ad84942.png). For
    the readers who are interested in the proof of this equation, refer to the appendix
    in the TRPO paper or the paper *Approximately optimal approximate reinforcement
    learning*, written by Kakade and Langford. To simplify this equation, let ![](img/10619e12-c8bf-4cf2-84a7-1c4e1cb07ec1.png)
    be the discounted visitation frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c544a5f-60c8-4d3f-80a6-c3296d59e3fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By rearranging the preceding equation to sum over states instead of timesteps,
    it becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bceac54-47bb-4518-acb3-7b256a13febc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this equation, we can see that any policy update ![](img/fda3a1ca-854b-44d8-a0c6-197d8cc0cb8c.png)
    that has a non-positive expected advantage at every state ![](img/a6e419f2-c2d2-46fe-afef-48570caec744.png),
    that is, ![](img/baf76ede-f391-4ed9-b6e9-605cb166bcdf.png), is guaranteed to reduce
    the cost ![](img/0ffe18a6-a81d-4139-8539-c77c0badf09c.png). Therefore, for discrete
    action space such as the Atari environment, the deterministic policy ![](img/88b0ef26-401f-4033-8576-82d6fca72fab.png), selected
    in DQN, guarantees to improves the policy if there is at least one state-action
    pair with a negative advantage value and nonzero state visitation probability.
    However, in practical problems, especially when the policy is approximated by
    a neural network, there will be some state for which the expected advantage is
    positive, due to approximation errors. Besides this, the dependency of ![](img/5642fe43-339d-46fb-be5d-7b6bd14b1df0.png) on ![](img/0fc4904e-49e5-447a-a56a-efecaa1ff4a3.png) makes
    this equation hard to optimize, so TRPO considers optimizing the following function
    by replacing ![](img/d7fce747-17fc-4da3-97d6-20db3c3c9ed9.png) with ![](img/3f64995a-2da0-4ea4-9508-dd5d750197b6.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78aa2307-eb0c-43b9-bd95-7d12f26f0530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Kakade and Langford showed that if we have a parameterized policy, ![](img/8fd694a5-34cc-4604-8330-309b3cae3d44.png),
    which is a differentiable function of the parameter ![](img/1fe4211a-0762-4c8e-b964-f82205c58113.png),
    then for any parameter ![](img/aed1c2ad-56e3-4a6c-b9cc-667860302101.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b26d3705-7acd-46e3-95d9-7960a539a17d.png)![](img/2c4d94aa-c595-4312-b77b-4f5b5aa2268d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that improving ![](img/7e29d171-632b-47f7-8fde-aa755abac2ad.png)
    will also improve ![](img/eb9f53b5-15ee-4804-99a3-d05ae7b70007.png) with a sufficient
    small update on ![](img/a5a215c0-8b48-4d4c-af73-b0cb2855f812.png). Based on this
    idea, Kakade and Langford proposed a policy updating scheme called the conservative
    policy iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b34ce87-7dad-4891-bfc4-d06ac637bf61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/8e97e940-f4a8-4aca-a389-196e3672e47b.png) is the current policy, ![](img/0690ebf6-d1d9-4a5d-9975-88e070099654.png)
    is the new policy, and ![](img/656c223d-a53d-466f-9049-cfb3ea1aa43e.png) is obtained
    by solving ![](img/bb69abc4-4efa-431d-bde8-8cb81333e9d9.png). They proved the
    following bound for this update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2202bc30-b02d-4b4f-ac00-ccd006344629.png) where ![](img/4e8d8a89-687b-4c09-a75b-fd05efbb42bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that this bound only applies to mixture policies generated by the preceding
    update. In TRPO, the authors extended this bound to general stochastic policies,
    rather than just mixture policies. The main idea is to replace mixture weight ![](img/433ef82d-2ae0-4059-a0db-d0a9b5d60936.png)
    with a distance measure between ![](img/94e130cc-71b0-40ca-b6ad-e962705670f2.png)
    and ![](img/a5b4a9a7-64dd-420f-8422-255798c4721b.png). An interesting pick of
    the distance measure is the total variation divergence. Taking two discrete distributions ![](img/3234f41f-86a5-4366-9189-5ca28c7c9043.png)
    and ![](img/6c78aec7-e723-4044-9371-d99781c5b6ce.png) as an example, the total
    variation divergence is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b39bd755-aac8-44f3-b148-40e7e6c75d22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For policies ![](img/e1a2e0b2-a80f-4c94-a238-eb5c6a72884b.png) and ![](img/a5952e1b-fdf3-4d52-952d-08efb4aab567.png),
    let ![](img/962c116d-a5af-45c3-9868-2f3ff9c7c69c.png) be the maximum total variation
    divergence over all the states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8cdaa06-5dd1-42b5-964c-709d9194dd14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With ![](img/f5ad7c02-2977-4428-92c9-e1d932054ab3.png) and ![](img/4e930847-34f4-4fd3-bbc6-65f4ec91fd6a.png),
    it can be shown that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb185217-9538-492f-bffe-32d1307b6d08.png), where ![](img/00448aeb-93d9-4f75-bb9c-32de71911cab.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the total variation divergence can be upper bounded by the KL divergence,
    namely, ![](img/78c2353d-c2a3-4fe1-9e1b-407ac9f8101a.png), which means that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2de74f64-0e3f-4130-9b12-48ec49724f87.png), where ![](img/055f7bdb-5fcd-4f75-be54-a5033373ef64.png).'
  prefs: []
  type: TYPE_NORMAL
- en: TRPO algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the preceding policy improvement bound, the following algorithm is
    developed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In each step, this algorithm minimizes the upper bound of ![](img/af7c9656-8e07-4215-9d38-0f867bd07473.png),
    so that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22c9cc5b-58c6-4996-bb21-280af47bc7db.png)'
  prefs: []
  type: TYPE_IMG
- en: The last equation follows from that ![](img/8acb0164-88a5-4075-a1ad-798769022863.png)
    for any policy ![](img/72c0ceb5-4b1f-4a7b-a34d-74cf20f5ca6e.png). This implies
    that this algorithm is guaranteed to generate a sequence of monotonically improving
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, since the exact value of ![](img/08cd6ec4-d66e-4134-b835-13a665021708.png) in ![](img/6d906ae5-9485-497d-90be-9e3205312eb6.png)
    is hard to calculate, and it is difficult to control the step size of each update
    using the penalty term, TRPO replaces the penalty term with the constraint that
    KL divergence is bounded by a constant ![](img/e604b012-bcac-4dca-b9bf-c5d4ac226e43.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e0c56b3-1599-4ad7-8933-d3fcf07b1c67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But this problem is still impractical to solve due to the large number of constraints.
    Therefore, TRPO uses a heuristic approximation that considers the average KL divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d508e2a-acf2-4345-aebb-e232439dcfbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This leads to the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e098bb38-864f-4fbd-ab5c-2a22a7902534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, by expanding ![](img/fff01813-299b-4fa6-910c-c6bdb438fbaf.png),
    we need to solve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/305da5d4-b490-4301-9e4d-9b949f277cda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the question is: how do we optimize this problem? A straightforward idea
    is to sample several trajectories by simulating the policy ![](img/e6fe0c4f-2dd5-4f93-a676-47a4d0599d9d.png) for
    some number of steps and then approximate the objective function of this problem
    using these trajectories. Since the advantage function ![](img/d0f93482-d0f8-411f-b9e3-e0ae54566ede.png),
    we replace ![](img/bf05b41e-458f-4cbf-ac10-dff5f4e25f56.png) with by the Q-value ![](img/b9edf522-8800-4336-8ac7-1c14d0aa0bd8.png)
    in the objective function, which only changes the objective by a constant. Besides,
    note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34f59a5f-7d84-44a2-b76d-f483147a3157.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, given a trajectory ![](img/c40ea99f-c792-44cb-bef0-1944188cfac8.png) generated
    under policy ![](img/17240a16-8143-425d-a94b-3f3029041e9b.png), we will optimize
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27da421f-eb3d-4630-b1e7-dec03aab4f25.png)'
  prefs: []
  type: TYPE_IMG
- en: For the MuJoCo control tasks, both the policy ![](img/17f26470-538d-4521-be3f-f3e4ea1ed2b6.png)
    and the `state-action value` function ![](img/21d3f97e-547f-4d2e-903b-9120ab4f820d.png)
    are approximated by neural networks. In order to optimize this problem, the KL
    divergence constraint can be approximated by the Fisher information matrix. This
    problem can then be solved via the conjugate gradient algorithm. For more details,
    you can download the source code of TRPO from GitHub and check `optimizer.py`,
    which implements the conjugate gradient algorithm using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments on MuJoCo tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Swimmer` task is a good example to test TRPO. This task involves a 3-link
    swimming robot in a viscous fluid, where the goal is to make it swim forward as
    fast as possible by actuating the two joints ([http://gym.openai.com/envs/Swimmer-v2/](http://gym.openai.com/envs/Swimmer-v2/)).
    The following screenshot shows how `Swimmer` looks in the MuJoCo simulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4e13121-31e1-4e6b-b34a-3f3d6577829c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To train an agent for `Swimmer`, run the following command under the `src` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There are two arguments in `train.py`. One is `-t`, or `--task`, indicating
    the name of the MuJoCo or classic control task you want to test. Since the state
    spaces of these control tasks have relatively low dimensions compared to the Atari
    environment, it is enough to use CPU alone to train the agent by setting `CUDA_VISIBLE_DEVICES` to
    empty, which will take between 30 minutes and two hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training, you can open a new Terminal and type the following command
    to visualize the training procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `logdir` points to the folder where the `Swimmer` log file is stored.
    Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61d02d5f-0705-499b-80e3-b4334d0e13af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, after 200 episodes, the total reward achieved in each episode becomes
    stable, namely, around 366\. To check how `Swimmer` moves after the training,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You will see a funny-looking `Swimmer` object walking on the floor.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the classical control tasks and the MuJoCo control tasks
    provided by Gym. You have learned the goals and specifications of these tasks
    and how to implement a simulator for them. The most important parts of this chapter
    were the deterministic DPG and the TRPO for continuous control tasks. You learned
    the theory behind them, which explains why they work well in these tasks. You
    also learned how to implement DPG and TRPO using TensorFlow, and how to visualize
    the training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about how to apply reinforcement learning
    algorithms to more complex tasks, for example, playing Minecraft. We will introduce
    the **Asynchronous Actor-Critic** (**A3C**) algorithm, which is much faster than
    DQN at complex tasks, and has been widely applied as a framework in many deep
    reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
