- en: Training Your GANs to Break Different Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been a clear trend that people enjoy using deep learning methods to
    solve problems in the computer vision field. Has one of your classmates or colleagues
    ever shown off their latest image classifier to you? Now, with GANs, you may actually
    get the chance to show them what you can do by generating adversarial examples
    to break their previous models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We will be looking into the fundamentals of adversarial examples and how to
    attack and confuse a CNN model with **FGSM** (**Fast Gradient Sign Method**).
    We will also learn how to train an ensemble classifier with several pre-trained
    CNN models via transfer learning on Kaggle's Cats vs. Dogs dataset, following
    which, we will learn how to use an accimage library to speed up your image loading
    even more and train a GAN model to generate adversarial examples and fool the
    image classifier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial examples – attacking deep learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial examples – attacking deep learning models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is known that with deep learning methods that have huge numbers of parameters,
    sometimes more than tens of millions, it becomes more difficult for humans to comprehend
    what exactly they have learned, except the fact that they perform unexpectedly
    well in CV and NLP fields. If someone around you feels exceptionally comfortable
    using deep learning to solve each and every practical problem without a second
    thought, what we are about to learn in this chapter will help them to realize
    the potential risks their models are exposed to.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: What are adversarial examples and how are they created?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial examples are a kind of sample (often modified based on real data)
    that are easily mistakenly classified by a machine learning system (and sometimes
    look normal to the human eye). Modifications to image data could be a small amount
    of added noise ([https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research))
    or a small image patch (Tom B. Brown, et al, 2017). Sometimes, printing them on
    paper and taking pictures of adversarial examples also fools neural networks.
    It is even possible to 3D-print an object that fools neural networks from almost
    all perspectives (Anish Athalye, et al, 2018). Although you can create some random
    samples that look like nothing natural and still cause neural networks to make
    mistakes, it is far more interesting to study the adversarial examples that look
    normal to humans but are misclassified by neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Be assured that we are not going off-topic discussing adversarial examples here.
    For starters, Ian Goodfellow, known as the father of GANs, has spent a decent
    amount of time studying adversarial examples. Adversarial examples and GANs might
    be siblings! Joking aside, GANs are good for generating convincing and realistic
    samples, as well as generating samples that fool other classifier models. In this
    chapter, we will first walk through how to construct adversarial examples and
    attack a small model in PyTorch. Then, we will show you how to use GANs to generate
    adversarial examples to attack a large model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacking with PyTorch
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is an excellent toolbox for adversarial attacks, defense, and benchmarks
    for TensorFlow called CleverHans ([https://github.com/tensorflow/cleverhans](https://github.com/tensorflow/cleverhans)).
    Currently, the developers are making plans to support PyTorch ([https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py](https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py)).
    In this section, we will need to implement an adversarial example in PyTorch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet is based on the official tutorial by PyTorch: [https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html).
    We will slightly modify the model and the creation of adversarial examples will
    be performed in batchs. Start with a blank file named `advAttackGAN.py`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the modules:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the device and the perturbation factors:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the CNN model, which is known as the LeNet-5 model:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the data loader for both training and testing. Here, we''ll use the
    MNIST dataset:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that, to make the defined perturbation factors work for our model, we are
    not normalizing (whitening) the data by subtracting the mean value and dividing
    by the standard deviation value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `model`, `optimizer`, and `loss` functions:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `train` and `test` functions:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s train the model and see what this small model is capable of:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output messages may look like these:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see that our small CNN model achieves 99.31% test accuracy after only
    5 epochs of training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, implement FGSM to create an adversarial example from the read sample and
    its derivatives:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use `fgsm_attack` to perturbate test images and see what happens:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we save the first five test images to `adv_examples` to show the predicted
    labels before and after perturbation. You can always replace the `indices = torch.arange(5)` line with
    the following line to only show adversarial examples that cause the model to fail:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output messages in the Terminal may look like these:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that, as `epsilon` increases, more samples are mistakenly classified
    by the model. The test accuracy of the model drops to 16.7% at worst.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, illustrate the perturbed images with `matplotlib`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here are the first five test images and their predicted labels before and after
    perturbation with different factor values:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c40dba63-d0ee-4476-9f3b-b8d4a4f906f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Adversarial examples created from MNIST
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial examples
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have been using GANs to generate various types of images in the previous
    chapters. Now, it's time to try generating adversarial examples with GANs and
    break some models!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Preparing an ensemble classifier for Kaggle's Cats vs. Dogs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make our demonstration more similar to practical scenarios, we will train
    a decent model on Kaggle's Cats vs. Dogs dataset ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)),
    then break the model with adversarial examples generated by GAN. This dataset
    contains 25,000 training images and 12,500 testing images of either dogs or cats.
    Here, we will only use the 25,000 training images in our experiment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, after downloading the dataset, put images of cats and dogs
    in separate folders, so that the file structure looks like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model we are training on this dataset is formed of several pre-trained
    models provided by PyTorch Hub ([https://github.com/pytorch/hub](https://github.com/pytorch/hub)).
    We will also need to perform transfer training on the pre-trained models to fit
    our dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76921976-ed7d-47c0-8a37-cbf386539d27.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Ensemble model for Kaggle's Cats vs. Dogs
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to load and preprocess the data, create an ensemble classifier,
    and train this model. Here are the detailed steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Python file named `cats_dogs.py` and import the Python modules:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the custom module files, `advGAN`, `data_utils`,and `model_ensemble`,
    will be created later.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the main entry point in `cats_dogs.py`, in which argument values are
    parsed and the image decoding backend is defined. Here, only some of the lines
    are shown due to the length. The full source code is available in the `cats_dogs`
    folder, under the code repository for this chapter:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we use `accimage` as an image decoding backend for `torchvision`. **Accimage**
    ([https://github.com/pytorch/accimage](https://github.com/pytorch/accimage)) is
    an image decoding and preprocessing library designed for `torchvision`, which
    uses Intel IPP ([https://software.intel.com/en-us/intel-ipp](https://software.intel.com/en-us/intel-ipp))
    to improve processing speed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Above the main entry point, define the `main` function, in which we first load
    and split the training images into a training set and a validation set:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We split the 25,000 training images into 2 collections, in which 80% of the
    images are randomly selected to form the training set and the remaining 20% form
    the validation set. Here, `_transforms_catsdogs` is defined in `data_utils.py`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, we are not whitening the images. However, if you are interested in how
    to efficiently calculate the mean and standard deviation values of a dataset,
    the code snippet is provided in the `mean_std.py` file.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Be comfortable with using `multiprocessing.Pool` to process your data, which
    is demonstrated in `mean_std.py`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 熟练使用 `multiprocessing.Pool` 来处理数据，这在 `mean_std.py` 中有演示。
- en: 'Get the pre-trained model files from PyTorch Hub and start transfer learning:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 PyTorch Hub 获取预训练模型文件并开始迁移学习：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The code for evaluation is omitted due to the length. Here, `transfer_init`
    is defined in `model_ensemble.py` and is responsible for replacing the second
    to the last layer in each model so that we can train it for any number of classes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码过长，评估代码已省略。这里，`transfer_init` 在 `model_ensemble.py` 中定义，负责替换每个模型倒数第二层，以便我们可以根据需要训练任意数量的类别：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here's why we can transfer the knowledge learned from one domain (trained on
    ImageNet) to another domain (Cats vs. Dogs) by simply replacing the last layer
    (usually a fully connected layer). All of the convolution layers in a CNN are
    responsible for extracting features from the image and intermediate feature maps.
    The fully connected layer can be seen as recombining the highest-level features
    to form the final abstraction of the raw data. It is obvious that good models
    trained on ImageNet are good at extracting features. Therefore, recombining those
    features differently is highly likely to be capable for an easier dataset such
    as Cats vs. Dogs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为什么我们可以通过简单地替换最后一层（通常是全连接层），将从一个领域（在 ImageNet 上训练）学到的知识迁移到另一个领域（猫狗分类）。CNN
    中的所有卷积层负责从图像及中间特征图中提取特征。全连接层可以看作是重新组合最高层次的特征，形成原始数据的最终抽象。显然，在 ImageNet 上训练的好模型非常擅长提取特征。因此，以不同方式重新组合这些特征，很可能能够处理像猫狗分类这样较简单的数据集。
- en: 'Also, `data_prefetcher` is used to speed up the training process. It''s defined
    in `data_utils.py`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`data_prefetcher` 用于加速训练过程。它在 `data_utils.py` 中定义：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The training of these individual models can be really fast. Here is the GPU
    memory consumption and validation accuracy after 25 epochs of transfer learning:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单独模型的训练速度可以非常快。以下是经过 25 个 epoch 的迁移学习后的 GPU 内存消耗和验证准确率：
- en: '| **Model** | **Memory** | **Accuracy** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **内存** | **准确率** |'
- en: '| MobileNet V2 | 1665MB | 98.14% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet V2 | 1665MB | 98.14% |'
- en: '| ResNet-18 | 1185MB | 98.24% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 | 1185MB | 98.24% |'
- en: '| DenseNet | 1943MB | 98.76% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet | 1943MB | 98.76% |'
- en: '| GoogleNet | 1447MB | 98.06% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GoogleNet | 1447MB | 98.06% |'
- en: '| ResNeXt-50 | 1621MB | 98.98% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ResNeXt-50 | 1621MB | 98.98% |'
- en: ResNet-34, ShuffleNet V2, SqueezeNet, and VGG-11 are not selected due to either
    low performance or high memory consumption (over 2 GB).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-34、ShuffleNet V2、SqueezeNet 和 VGG-11 未被选择，原因是它们的性能较低或内存消耗过高（超过 2 GB）。
- en: Saving your model to the hard drive with `torch.save(model.state_dict(), PATH)`
    will only export the parameter values and you need to explicitly define `model`
    before loading it in another script. However, `torch.save(model, PATH)` will save
    everything, including the model definition, to the file.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.save(model.state_dict(), PATH)` 将模型保存到硬盘时，只会导出参数值，你需要在加载到另一个脚本时明确地定义
    `model`。然而，`torch.save(model, PATH)` 会保存所有内容，包括模型定义。
- en: 'Put together the ensemble classifier in `model_ensemble.py`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `model_ensemble.py` 中组合集成分类器：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, the prediction results from all models are combined together to give the
    final prediction in `vote_layer`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，将所有模型的预测结果组合在一起，在 `vote_layer` 中给出最终预测。
- en: Alternatively, you can always directly put together the feature maps from the
    last convolution layers in the pretrained models and train one single fully connected
    layer to predict the image label.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以直接将预训练模型中最后一层卷积层的特征图组合起来，训练一个单独的全连接层来预测图像标签。
- en: 'Get back to the `cats_dogs.py` file and start training the ensemble classifier:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到 `cats_dogs.py` 文件并开始训练集成分类器：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Again, the code for evaluation is omitted due to the length. Validation accuracy
    of the ensemble classifier reaches 99.32% after only 2 epochs of training. The
    training of the ensemble classifier only takes 2775 MB of the GPU memory and the
    exported model file size is no more than 200 MB.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 再次由于代码过长，评估代码已省略。集成分类器经过仅仅 2 个 epoch 的训练后，验证准确率达到了 99.32%。集成分类器的训练仅占用 2775 MB
    的 GPU 内存，导出的模型文件大小不超过 200 MB。
- en: Breaking the classifier with advGAN
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 advGAN 打破分类器
- en: 'The GAN model we''ll use for generating adversarial examples is largely borrowed
    from [https://github.com/mathcbc/advGAN_pytorch](https://github.com/mathcbc/advGAN_pytorch).
    Let''s create two files named `advGAN.py` and `models.py` and put the following
    code in these files:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于生成对抗样本的GAN模型主要借鉴自[https://github.com/mathcbc/advGAN_pytorch](https://github.com/mathcbc/advGAN_pytorch)。让我们创建两个文件，分别命名为`advGAN.py`和`models.py`，并将以下代码放入这些文件中：
- en: '`advGAN.py`: Within this file, you will see the following:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`advGAN.py`：在这个文件中，你将看到以下内容：'
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Part of the code is omitted due to the length. We can see that this GAN model
    is only responsible for generating the noise part in the adversarial example,
    which is clamped to [-0.3, 0.3] before being added to the original image. During
    training, MSE loss is used to measure the discriminator loss. L1-loss is used
    to calculate the adversarial loss for the generator. The L2-norm of the generated
    perturbation noise is also included in the generator loss. However, the performance
    of the GAN is highly related to the classifier (`self.model`) we are aiming to
    break, which means that the GAN model needs to be retrained each time a new classifier
    is introduced.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅原因，部分代码被省略。我们可以看到，这个GAN模型只负责生成对抗示例中的噪声部分，生成的噪声会被限制在[-0.3, 0.3]范围内，然后加到原始图像中。在训练过程中，使用MSE损失来衡量判别器损失，使用L1损失来计算生成器的对抗损失。生成的扰动噪声的L2范数也包含在生成器损失中。然而，GAN的性能与我们试图突破的分类器（`self.model`）密切相关，这意味着每次引入新的分类器时，都需要重新训练GAN模型。
- en: The code in `models.py` is omitted here but is available in the code repository
    for this chapter, since you can basically design the discriminator and generator
    any way you like. Here, we use a 4-layer CNN as the discriminator network and
    a 14-layer ResNet-like CNN as the generator network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`models.py`中的代码在这里省略，但可以在本章的代码库中找到，因为你基本上可以根据自己的需求设计判别器和生成器。这里，我们使用一个4层的CNN作为判别器网络，以及一个14层类似ResNet的CNN作为生成器网络。'
- en: Back to `cats_dogs.py`, we need to train the GAN model to learn how to break
    the ensemble classifier.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`cats_dogs.py`，我们需要训练GAN模型来学习如何突破集成分类器。
- en: 'Redefine the data loader because we need a smaller batch size to fit in the
    11 GB GPU memory:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义数据加载器，因为我们需要一个更小的批量大小来适应11 GB的GPU内存：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Start training the GAN model:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始训练GAN模型：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Attack the ensemble classifier with the GAN:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GAN攻击集成分类器：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It takes about 6 hours to finish 60 epochs of training on the GAN model. The
    attack result may look like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN模型60个epoch大约需要6个小时。攻击结果可能如下所示：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the validation accuracy drops from 99.32% to 10.3% as a result
    of the adversarial attack by the GAN.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，验证准确率从99.32%降到10.3%，这是由于GAN对抗攻击的结果。
- en: 'Display some of the misclassified images with `matplotlib`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`显示一些被错误分类的图像：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that everything in the code is finished, it's time to finally actually run
    our program.  We need to do this multiple times, once for each model. Create an
    empty folder named models in your code folder to hold the saved models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代码中的一切都已经完成，最终我们可以真正运行程序了。我们需要多次运行程序，每次针对不同的模型。请在你的代码文件夹中创建一个名为models的空文件夹，用于保存模型。
- en: 'We''ll start the program from the command line:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从命令行启动程序：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once all the models have run, we can finally test our ensemble code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有模型都运行完毕，我们就可以最终测试我们的集成代码：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here are some of the pertubated images generated by the GAN that fooled our
    ensemble classifier:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些由GAN生成的扰动图像，这些图像成功欺骗了我们的集成分类器：
- en: '![](img/0156142b-1ebb-4e7d-b7b9-67837d9e8eab.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0156142b-1ebb-4e7d-b7b9-67837d9e8eab.png)'
- en: Adversarial examples generated by the GAN
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GAN生成的对抗样本
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: A lot that has gone on in this chapter. You've learned the basics of Fast Gradient
    Sign Methods, how to train a  classifier with pre-trained models, how to deal
    with transfer learning, and much more.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容涵盖了很多内容。你学到了快速梯度符号法（Fast Gradient Sign Methods）的基础知识，如何使用预训练模型训练分类器，如何处理迁移学习，等等。
- en: In the next chapter, we will show how to combine **NLP** (**Natural Language
    Processing**) with GANs and generate images from the description text.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将展示如何将**NLP**（**自然语言处理**）与GAN结合，并从描述文本中生成图像。
- en: References and further reading list
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和进一步阅读列表
- en: Goodfellow I, Papernot N, Huang S, et. al. (Feb 24, 2017). *Attacking machine
    learning with adversarial examples*. Retrieved from [https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow I, Papernot N, Huang S, 等（2017年2月24日）。*通过对抗性样本攻击机器学习*。来源：[https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research)。
- en: Brown T, Mané D, Roy A, et al (2017). *Adversarial Patch*. NIPS.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown T, Mané D, Roy A, 等（2017）。*对抗性补丁*。NIPS。
- en: Athalye A, Engstrom L, Ilyas A. (2018). *Synthesizing Robust Adversarial Examples*.
    ICML.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Athalye A, Engstrom L, Ilyas A.（2018）。*合成稳健的对抗性样本*。ICML。
