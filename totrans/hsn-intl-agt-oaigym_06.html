<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing an Intelligent Agent for Optimal Control using Deep Q-Learning</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter, we implemented an intelligent agent that used Q-learning to solve the Mountain Car problem from scratch in about seven minutes on a dual-core laptop CPU. In this chapter, we will implement an advanced version of Q-learning called deep Q-learning, which can be used to solve several discrete control problems that are much more complex than the Mountain Car problem. Discrete control problems are (sequential) decision-making problems in which the action space is discretized into a finite number of values. In the previous chapter, the learning agent used a 2-dimensional state-space vector as the input, which contained the information about the position and velocity of the cart to take optimal control actions. In this chapter, we will see how we can implement a learning agent that takes (the on-screen) visual image as input and learns to take optimal control actions. This is close to how we would approach the problem, isn't it? We humans do not calculate the location and velocity of an object to decide what to do next. We simply observe what is going on and then learn to take actions that improve over time, eventually solving the problem completely. </p>
<p class="calibre2">This chapter will guide you in how to progressively build a better agent by improving upon our Q-learning agent implementation step-by-step<span class="calibre5"> using recently published methods for stable Q-learning with deep neural network function approximation</span>. <span class="calibre5">By the end of this chapter, you will have learnt how to implement and train a deep Q-learning agent that observes the pixels on the screen and plays Atari games using the Atari Gym environment and gets pretty good scores!</span><span class="calibre5"> We will also discuss how you can visualize and compare the performance of the agent as the learning progresses. You will see how the same agent algorithm can be trained on several different Atari games and that the agent is still able to learn to play the games well. If you cannot wait to see something in action or if you like to see and get a glimpse of what you will be developing before diving in, you can check out the code for this chapter under the</span> <kbd class="calibre12">ch6</kbd> <span class="calibre5">folder from the book's code repository and try out the pre-trained agents on several Atari games! Instructions on how to run the pre-trained agents are available in the</span> <kbd class="calibre12">ch6/README.md</kbd><span class="calibre5"> file.</span></p>
<p class="calibre2">This chapter has a lot of technical details to equip you with enough background and knowledge for you to understand the step-by-step process of improving the basic Q-learning algorithm and building a much more capable and intelligent agent based on deep Q-learning, along with several modules and tools needed to train and test the agent in a systematic manner. The following is an outline of the higher-level topics that will be covered in this chapter:</p>
<ul class="calibre10">
<li class="calibre11">Various methods to improve the Q-learning agent, including the following:
<ul class="calibre68">
<li class="calibre11">Neural network approximation of action-value functions</li>
<li class="calibre11">Experience replay</li>
<li class="calibre11">Exploration schedules</li>
</ul>
</li>
<li class="calibre11">Implementing deep convolutional neural networks using PyTorch for action-value function approximation</li>
<li class="calibre11">Stabilizing deep Q-networks using target networks</li>
<li class="calibre11">Logging and monitoring learning performance of PyTorch agents using TensorBoard</li>
<li class="calibre11">Managing parameters and configurations</li>
<li class="calibre11">Atari Gym environment</li>
<li class="calibre11">Training deep Q-learners to play Atari games</li>
</ul>
<p class="calibre2">Let's get started with the first topic and see how we can start from where we left off in the previous chapter and continue making progress toward a more capable and intelligent agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Improving the Q-learning agent</h1>
                
            
            <article>
                
<p class="calibre2">In the last chapter, we revisited the Q-learning algorithm and implemented the <kbd class="calibre12">Q_Learner</kbd> class. For the Mountain car environment, we used a multi-dimensional array of shape 51x51x3 to represent the action-value function,<img src="../images/00130.jpeg" class="calibre69"/>. Note that we had discretized the state space to a fixed number of bins given by the <kbd class="calibre12">NUM_DISCRETE_BINS</kbd> configuration parameter <span class="calibre5">(we used 50) </span>. We essentially quantized or approximated the observation with a low-dimensional, discrete representation to reduce the number of possible elements in the n-dimensional array. With such a discretization of the observation/state space, we restricted the possible location of the car to a fixed set of 50 locations and the possible velocity of the car to a fixed set of 50 values. Any other location or velocity value would be approximated to one of those fixed set of values. Therefore, it is possible that the agent receives the same value for the position when the car was actually at different positions. For some environments, that can be an issue. The agent may not learn enough to distinguish between falling off a cliff versus standing just on the edge so as to leap forward. In the next section, we will look into how we can use a more powerful function approximator to represent the action-value function instead of a simple n-dimensional array/table that has its limitations. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using neural networks to approximate Q-functions</h1>
                
            
            <article>
                
<p class="calibre2">Neural networks are shown to be effective as universal function approximators. In fact, there is a universal approximation theorem that states that a single hidden layer feed-forward neural network can approximate any continuous function that is closed and bounded in <img class="fm-editor-equation72" src="../images/00131.jpeg"/>. It basically means that even simple (shallow) neural networks can approximate several functions. Doesn't it feel too good to be true that you can use a simple neural network with a fixed number of weights/parameters to approximate practically any function? It is actually true, except for one note that prevents it from being used practically anywhere and everywhere. Though a single hidden layer neural network can approximate any function with a finite set of parameters, we do not have a universally guaranteed way of <em class="calibre13">learning</em> those parameters that can best represent any function. You will see that researchers have been able to use neural networks to approximate several sophisticated and useful functions. Today, most of the intelligence that is built into the ubiquitous smartphones are powered by (heavily optimized) neural networks. Several best-performing systems that organize your photos into albums automatically based on people, places, and the context in the photos, systems that recognize your face and voice, or systems that automatically compose email replies for you are all powered by neural networks. Even the state-of-the-art techniques that generate human-like <span class="calibre5">realistic voices that you hear from voice assistants such as Google Assistant, are powered by neural networks. </span></p>
<div class="packt_infobox">Google Assistant currently uses WaveNet and WaveNet2 developed by Deepmind for <strong class="calibre27">Text-To-Speech</strong> (<strong class="calibre27">TTS</strong>) synthesis, which is shown to be much more realistic than any other TTS system that has been developed so far.</div>
<p class="calibre2">I hope that motivates you enough to use a neural network to approximate the Q-function! In this section, we will start by approximating the Q-function with a shallow (not deep) single-hidden layer neural network and use it to solve the famous Cart Pole problem. Though neural networks are powerful function approximators, we will see that it is not trivial to train even a single layer neural network to approximate Q-functions for reinforcement learning problems. We will look at some ways to improve Q-learning with neural network approximation and, in the later sections of this chapter, we will look at how we can use deep neural networks with much more representation power to approximate the Q-function.</p>
<p class="calibre2"><span class="calibre5">Let's get started with the neural network approximation by first revisiting the </span><kbd class="calibre12">Q_Learner</kbd> <span class="calibre5">class's </span><kbd class="calibre12">__init__(...)</kbd> <span class="calibre5">method that we implemented in the previous chapter:</span></p>
<pre class="calibre17">class Q_Learner(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        <strong class="calibre1">self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,</strong><br class="title-page-name"/><strong class="calibre1">                           self.action_shape))  # (51 x 51 x 3)</strong><br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0</pre>
<p class="calibre2"><span class="calibre5">In the preceding code, the line in bold font is where we initialize the Q-function as a multi-dimensional NumPy array. In the following sections, we will see how we can replace the NumPy array representation with a more powerful neural network representation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing a shallow Q-network using PyTorch </h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will start implementing a simple neural network using PyTorch's neural network module and then look at how we can use that to replace the multi-dimensional array-based Q action-value table-like function.</p>
<p class="calibre2">Let's start with the neural network implementation. The following code illustrates how you can implement a <span class="calibre5"><strong class="calibre4">Single Layer Perceptron</strong> (<strong class="calibre4">SLP</strong>) using PyTorch</span><span class="calibre5">:</span></p>
<pre class="calibre17">import torch<br class="title-page-name"/><br class="title-page-name"/>class SLP(torch.nn.Module):<br class="title-page-name"/>    """<br class="title-page-name"/>    A Single Layer Perceptron (SLP) class to approximate functions<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, input_shape, output_shape, device=torch.device("cpu")):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param input_shape: Shape/dimension of the input<br class="title-page-name"/>        :param output_shape: Shape/dimension of the output<br class="title-page-name"/>        :param device: The device (cpu or cuda) that the SLP should use to store the inputs for the forward pass<br class="title-page-name"/>        """<br class="title-page-name"/>        super(SLP, self).__init__()<br class="title-page-name"/>        self.device = device<br class="title-page-name"/>        self.input_shape = input_shape[0]<br class="title-page-name"/>        self.hidden_shape = 40<br class="title-page-name"/>        self.linear1 = torch.nn.Linear(self.input_shape, self.hidden_shape)<br class="title-page-name"/>        self.out = torch.nn.Linear(self.hidden_shape, output_shape)<br class="title-page-name"/><br class="title-page-name"/>    def forward(self, x):<br class="title-page-name"/>        x = torch.from_numpy(x).float().to(self.device)<br class="title-page-name"/>        x = torch.nn.functional.relu(self.linear1(x))<br class="title-page-name"/>        x = self.out(x)<br class="title-page-name"/>        return x</pre>
<p class="calibre2">The SLP class implements a single layer neural network with 40 hidden units between the input and the output layer using the <kbd class="calibre12">torch.nn.Linear</kbd>class, and uses the <strong class="calibre4">Rectified Linear Unit</strong> (<strong class="calibre4">ReLU</strong> or <strong class="calibre4">relu</strong>) as the activation function. This code is available as <kbd class="calibre12">ch6/function_approximator/perceptron.py</kbd> in this book's code repository. The number 40 is nothing special, so fe<span class="calibre5">el free to vary the number of hidden units in the neural network.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the Shallow_Q_Learner</h1>
                
            
            <article>
                
<p class="calibre2">We can then modify the <kbd class="calibre12">Q_Learner</kbd> class to use this SLP to represent the Q-function. Note that we will have to modify the <kbd class="calibre12">Q_Learner</kbd> class <kbd class="calibre12">learn(...)</kbd> method as well to calculate the gradients of loss with respect to the weights of the SLP and backpropagate them so as to update and optimize the neural network's weights to improve its Q-value representation to be close to the actual values. We'll also slightly modify the <kbd class="calibre12">get_action(...)</kbd> method to get the Q-values with a forward pass through the neural network. The following code for the <kbd class="calibre12">Shallow_Q_Learner</kbd> class with the changes from the <kbd class="calibre12">Q_Learner</kbd> class implementation are shown in bold to make it easy for you to see the differences at a glance:</p>
<pre class="calibre17"><strong class="calibre1">import torch<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/><br class="title-page-name"/></strong>EPSILON_MIN = 0.005<br class="title-page-name"/>max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE<br class="title-page-name"/>EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps<br class="title-page-name"/>ALPHA = 0.05  # Learning rate<br class="title-page-name"/>GAMMA = 0.98  # Discount factor<br class="title-page-name"/>NUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim<br class="title-page-name"/><br class="title-page-name"/>class <strong class="calibre1">Shallow_Q_Learner</strong>(object):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        self.obs_shape = env.observation_space.shape<br class="title-page-name"/>        self.obs_high = env.observation_space.high<br class="title-page-name"/>        self.obs_low = env.observation_space.low<br class="title-page-name"/>        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim<br class="title-page-name"/>        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins<br class="title-page-name"/>        self.action_shape = env.action_space.n<br class="title-page-name"/>        # Create a multi-dimensional array (aka. Table) to represent the<br class="title-page-name"/>        # Q-values<br class="title-page-name"/>        <strong class="calibre1">self.Q = SLP(self.obs_shape, self.action_shape)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-5)</strong><br class="title-page-name"/>        self.alpha = ALPHA  # Learning rate<br class="title-page-name"/>        self.gamma = GAMMA  # Discount factor<br class="title-page-name"/>        self.epsilon = 1.0<br class="title-page-name"/><br class="title-page-name"/>    def discretize(self, obs):<br class="title-page-name"/>        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, obs):<br class="title-page-name"/>        discretized_obs = self.discretize(obs)<br class="title-page-name"/>        # Epsilon-Greedy action selection<br class="title-page-name"/>        if self.epsilon &gt; EPSILON_MIN:<br class="title-page-name"/>            self.epsilon -= EPSILON_DECAY<br class="title-page-name"/>        if np.random.random() &gt; self.epsilon:<br class="title-page-name"/>            <strong class="calibre1">return np.argmax(self.Q(discretized_obs).data.to(torch.device('cpu')).numpy())</strong><br class="title-page-name"/>        else:  # Choose a random action<br class="title-page-name"/>            return np.random.choice([a for a in range(self.action_shape)])<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, obs, action, reward, next_obs):<br class="title-page-name"/>        <strong class="calibre1">#discretized_obs = self.discretize(obs)</strong><br class="title-page-name"/>        <strong class="calibre1">#discretized_next_obs = self.discretize(next_obs)</strong><br class="title-page-name"/>       <strong class="calibre1"> td_target = reward + self.gamma * torch.max(self.Q(next_obs))</strong><br class="title-page-name"/>        <strong class="calibre1">td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)</strong><br class="title-page-name"/>        <strong class="calibre1">#self.Q[discretized_obs][action] += self.alpha * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/></strong></pre>
<div class="packt_infobox">The <kbd class="calibre28">Shallow_Q_Learner</kbd> class implementation is discussed here just to make it easy for you to understand how a neural network-based Q-function approximation can be implemented to replace the traditional tabular Q-learning implementations.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Solving the Cart Pole problem using a Shallow Q-Network</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will implement a full training script to solve the Cart Pole problem using the Shallow <kbd class="calibre12">Q_Learner</kbd> class that we developed in the previous section:</p>
<pre class="calibre17">#!/usr/bin/env python import gym import random import torch from torch.autograd import Variable import numpy as np from utils.decay_schedule import LinearDecaySchedule from function_approximator.perceptron import SLP<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>env = gym.make("CartPole-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 100000<br class="title-page-name"/>MAX_STEPS_PER_EPISODE = 300<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Shallow_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, learning_rate=0.005,<br class="title-page-name"/>                 gamma=0.98):<br class="title-page-name"/>        self.state_shape = state_shape<br class="title-page-name"/>        self.action_shape = action_shape<br class="title-page-name"/>        self.gamma = gamma # Agent's discount factor<br class="title-page-name"/>        self.learning_rate = learning_rate # Agent's Q-learning rate<br class="title-page-name"/>        # self.Q is the Action-Value function. This agent represents Q using a<br class="title-page-name"/>        # Neural Network.<br class="title-page-name"/>        self.Q = SLP(state_shape, action_shape)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-3)<br class="title-page-name"/>        # self.policy is the policy followed by the agent. This agents follows<br class="title-page-name"/>        # an epsilon-greedy policy w.r.t it's Q estimate.<br class="title-page-name"/>        self.policy = self.epsilon_greedy_Q<br class="title-page-name"/>        self.epsilon_max = 1.0<br class="title-page-name"/>        self.epsilon_min = 0.05<br class="title-page-name"/>        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,<br class="title-page-name"/>                                    final_value=self.epsilon_min,<br class="title-page-name"/>                                    max_steps= 0.5 * MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)<br class="title-page-name"/>        self.step_num = 0<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        return self.policy(observation)<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        # Decay Epsilion/exploratin as per schedule<br class="title-page-name"/>        if random.random() &lt; self.epsilon_decay(self.step_num):<br class="title-page-name"/>            action = random.choice([i for i in range(self.action_shape)])<br class="title-page-name"/>        else:<br class="title-page-name"/>            action = np.argmax(self.Q(observation).data.numpy())<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>        return action<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next):<br class="title-page-name"/>        td_target = r + self.gamma * torch.max(self.Q(s_next))<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss(self.Q(s)[a], td_target)<br class="title-page-name"/>        # Update Q estimate<br class="title-page-name"/>        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    observation_shape = env.observation_space.shape<br class="title-page-name"/>    action_shape = env.action_space.n<br class="title-page-name"/>    agent = Shallow_Q_Learner(observation_shape, action_shape)<br class="title-page-name"/>    first_episode = True<br class="title-page-name"/>    episode_rewards = list()<br class="title-page-name"/>    for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        for step in range(MAX_STEPS_PER_EPISODE):<br class="title-page-name"/>            # env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            agent.learn(obs, action, reward, next_obs)<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                if first_episode: # Initialize max_reward at the end of first episode<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    first_episode = False<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; max_reward:<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))<br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()</pre>
<p class="calibre2">Create a script named <kbd class="calibre12">shallow_Q_Learner.py</kbd> with the preceding code in the <kbd class="calibre12">ch6</kbd> folder and run it like so:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch6$ python shallow_Q_Learner.py</strong></pre>
<p class="calibre2">You will see the agent learning to balance the Pole on the Cart in the Gym's <kbd class="calibre12">CartPole-v0</kbd> environment. You should see the episode number, the number of steps the agent took before the episode ended, the episode reward the agent received, the mean episode reward that the agent has received, and also the best episode reward that the agent has received so far printed on the console. You can uncomment the <kbd class="calibre12">env.render()</kbd> line if you want to visually see the Cart Pole environment and how the agent is trying to learn and balance it.</p>
<div class="packt_infobox">The <kbd class="calibre28">Shallow_Q_Learner</kbd> class implementation and the full training script shows how you can use a simple neural network to approximate the Q-function. It is not a good implementation to solve complex games like Atari. In the following subsequent sections, we will systematically improve their performance using new techniques. We will also implement a Deep Convolutional Q-Network that can take the raw screen image as the input and predict the Q-values that the agent can use to play various Atari games. </div>
<p class="calibre2"><span class="calibre5">You may notice that it takes a very long time for the agent to improve and finally be able to solve the problem. In the next section, we will implement the concept of experience replay to improve the performance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Experience replay </h1>
                
            
            <article>
                
<p class="calibre2">In most environments, the information received by the agent is not <strong class="calibre4">independent and identically distributed</strong> (<strong class="calibre4">i.i.d</strong>). What this means is that the observation that the agent receives is strongly correlated with the previous observation it had received and the next observation it will receive. This is understandable because typically, the problems that the agent solves in typical reinforcement learning environments are sequential. Neural networks are shown to converge better if the samples are i.i.d.</p>
<p class="calibre2">Experience replay also enables the reuse of the past experience of the agent. Neural network updates, especially with lower learning rates, require several back-propagation and optimization steps to converge to good values. Reusing the past experience data, especially in mini-batches to update the neural network, greatly helps with the convergence of the Q-network which is close to the true action values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the experience memory</h1>
                
            
            <article>
                
<p class="calibre2">Let's implement an experience memory class to store the experiences collected by the agent. Before that, let's cement our understanding of what we mean by <em class="calibre13">experience</em>. In reinforcement learning where the problems are represented using <strong class="calibre4">Markov Decision Processes</strong> (<strong class="calibre4">MDP</strong>), which we discussed in <a href="part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 2</a>, <em class="calibre13">Reinforcement Learning and Deep Reinforcement Learning</em>, it is efficient to represent one experience as a data structure that consists of the observation at time step <em class="calibre13">t</em>, the action taken following that observation, the reward received for that action, and the next observation (or state) that the environment transitioned to due to the agent's action. It is useful to also include the "done" Boolean value that signifies whether this particular next observation marked the end of the episode or not. Let's use Python's <kbd class="calibre12">namedtuple</kbd> from the collections library to represent such a data structure, as shown in the following code snippet:</p>
<pre class="calibre17">from collections import namedtuple<br class="title-page-name"/>Experience = namedtuple("Experience", ['obs', 'action', 'reward', 'next_obs',<br class="title-page-name"/>                                       'done'])</pre>
<p class="calibre2">The <kbd class="calibre12">namedtuple</kbd> data structure makes it convenient to access the elements using a name attribute (like 'obs', 'action', and so on) instead of a numerical index (like 0, 1 and so on).</p>
<p class="calibre2">We can now move on to implement the experience memory class using the experience data structure we just created. To figure out what methods we need to implement in the experience memory class, let's think about how we will be using it later. </p>
<p class="calibre2">First, we want to be able to store new experiences in the experience memory that the agent collects. Then, we want to sample or retrieve experiences in batches from the experience memory when we want to replay to update the Q-function. So, essentially, we will need a method that can store new experiences and a method that can sample a single or a batch of experiences. </p>
<p class="calibre2">Let's dive into the experience memory implementation, starting with the initialization method where we initialize the memory with the desired capacity, as follows:</p>
<pre class="calibre17">class ExperienceMemory(object):<br class="title-page-name"/>    """<br class="title-page-name"/>    A cyclic/ring buffer based Experience Memory implementation<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, capacity=int(1e6)):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param capacity: Total capacity (Max number of Experiences)<br class="title-page-name"/>        :return:<br class="title-page-name"/>        """<br class="title-page-name"/>        self.capacity = capacity<br class="title-page-name"/>        self.mem_idx = 0 # Index of the current experience<br class="title-page-name"/>        self.memory = []</pre>
<p class="calibre2">The <kbd class="calibre12">mem_idx</kbd> member variable will be used to point to the current writing head or the index location where we will be storing new experiences when they arrive.</p>
<div class="packt_tip">A "cyclic buffer" is also known by other names that you may have heard of: "circular buffer", "ring buffer", and "circular queue". They all represent the same underlying data structure that uses a ring-like fixed-size data representation.</div>
<p class="calibre2">Next, we'll look at the <kbd class="calibre12">store</kbd> method's implementation: </p>
<pre class="calibre17">def store(self, experience):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param experience: The Experience object to be stored into the memory<br class="title-page-name"/>        :return:<br class="title-page-name"/>        """<br class="title-page-name"/>        self.memory.insert(self.mem_idx % self.capacity, experience)<br class="title-page-name"/>        self.mem_idx += 1</pre>
<p class="calibre2">Simple enough, right? We are storing the experience at <kbd class="calibre12">mem_idx</kbd>, like we discussed.</p>
<p class="calibre2">The next code is our <kbd class="calibre12">sample</kbd> method implementation:</p>
<pre class="calibre17">import random<br class="title-page-name"/>    def sample(self, batch_size):<br class="title-page-name"/>        """<br class="title-page-name"/><br class="title-page-name"/>        :param batch_size: Sample batch_size<br class="title-page-name"/>        :return: A list of batch_size number of Experiences sampled at random from mem<br class="title-page-name"/>        """<br class="title-page-name"/>        assert batch_size &lt;= len(self.memory), "Sample batch_size is more than available exp in mem"<br class="title-page-name"/>        return random.sample(self.memory, batch_size)<br class="title-page-name"/><br class="title-page-name"/>    </pre>
<p class="calibre2">In the preceding code, we make use of Python's random library to uniformly sample experiences from the experience memory at random. We will also implement a simple <kbd class="calibre12">get_size</kbd> helper method, which we will use to find out how many experiences are already stored in the experience memory:</p>
<pre class="calibre17">def get_size(self):<br class="title-page-name"/>        """<br class="title-page-name"/><br class="title-page-name"/>        :return: Number of Experiences stored in the memory<br class="title-page-name"/>        """<br class="title-page-name"/>        return len(self.memory)</pre>
<p class="calibre2">The full implementation of the experience memory class is available at <kbd class="calibre12">ch6/utils/experience_memory.py</kbd>, in this book's code repository.</p>
<p class="calibre2">Next, we'll look at how we can replay experiences sampled from the experience memory to update the agent's Q-function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing the replay experience method for the Q-learner class</h1>
                
            
            <article>
                
<p class="calibre2">So, we have implemented a memory system for the agent to store its past experience using a neat cyclic buffer. In this section, we will look at how we can use the experience memory to replay experience in the Q-learner class.</p>
<p class="calibre2">The following code snippet implements the <kbd class="calibre12">replay_experience</kbd> method that shows how we sample from the experience memory and call a soon-to-be-implemented method that lets the agent learn from the sampled batch of experiences:</p>
<pre class="calibre17">def replay_experience(self, batch_size=REPLAY_BATCH_SIZE):<br class="title-page-name"/>        """<br class="title-page-name"/>        Replays a mini-batch of experience sampled from the Experience Memory<br class="title-page-name"/>        :param batch_size: mini-batch size to sample from the Experience Memory<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        experience_batch = self.memory.sample(batch_size)<br class="title-page-name"/>        self.learn_from_batch_experience(experience_batch)</pre>
<p class="calibre2">In the case of online learning methods like SARSA, the action value estimate was updated after every step of interaction between the agent and the environment. This way, the updates propagated information that the agent just experienced. If the agent does not experience something quite often, such updates may let the agent forget about those experiences and may result in bad performance when the agent encounters a similar situation in the future. This is undesirable, especially with neural networks which have many parameters (or weights) that needs to be adjusted to the right set of values. That is one of the main motivations behind using an experience memory and replaying the past experiences during updates to the Q action-value estimates. We will now implement the <kbd class="calibre12">learn_from_batch_experience</kbd> method that extends the <kbd class="calibre12">learn</kbd> method we implemented in the previous chapter to learn from a batch of experiences rather than from a single experience. The following is the method's implementation:</p>
<pre class="calibre17">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br class="title-page-name"/>def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        """<br class="title-page-name"/>        Updated the DQN based on the learning from a mini-batch of experience.<br class="title-page-name"/>        :param experiences: A mini-batch of experience<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss(<br class="title-page-name"/>            self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>            td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        self.Q_optimizer.step()</pre>
<p class="calibre2">The method receives a batch (or a mini-batch) of experience and first extracts the observation batches, action batches, reward batches, and the next observation batches separately in order to use them individually in the subsequent steps. The <kbd class="calibre12">done_batch</kbd> signifies for each experience whether or not the next observation is the end of an episode. We then calculate the <strong class="calibre4">Temporal Difference</strong> (<strong class="calibre4">TD</strong>) error with a max over action, which is the Q-learning target. Note that we multiply the second term in the <kbd class="calibre12">td_target</kbd> calculation with <kbd class="calibre12">~done_batch</kbd>.</p>
<p class="calibre2">This takes care of specifying a zero value for terminal states. So, if a particular <kbd class="calibre12">next_obs</kbd> in the <kbd class="calibre12">next_obs_batch</kbd> was terminal, the second term would become 0, resulting in just <kbd class="calibre12">td_target = rewards_batch</kbd>.</p>
<p class="calibre2">We then calculate a mean squared error between the <kbd class="calibre12">td_target</kbd> (target Q-value) and the Q-value predicted by the Q-network. We use this error as the guiding signal and back-propagate it to all the nodes in the neural network before making an optimization step to update the parameters/weights to minimize the error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Revisiting the epsilon-greedy action policy</h1>
                
            
            <article>
                
<p class="calibre2">In the previous chapter, we discussed the <img class="fm-editor-equation59" src="../images/00132.jpeg"/>-greedy action selection policy <span class="calibre5">which takes the best action as per the agent's action-value estimate with a probability of 1-</span><img class="fm-editor-equation73" src="../images/00133.jpeg"/><span class="calibre5"> and takes a random action with a probability given by epsilon, </span><img class="fm-editor-equation73" src="../images/00134.jpeg"/><span class="calibre5">. Epsilon is a hyperparameter that can be tuned based on the experiments to a good value. A higher value of <img class="fm-editor-equation59" src="../images/00135.jpeg"/> means that the agent's actions will be random and a lower value of <img class="fm-editor-equation59" src="../images/00136.jpeg"/> means that the agent's action will more likely exploit what it already knows about the environment and will not try to explore. Should I explore more by taking never/less tried actions? Or should I exploit what I already know and take the best action to my knowledge which may be limited? This is the exploration-exploitation dilemma that a reinforcement learning agent suffers from.</span></p>
<p class="calibre2"><span class="calibre5">Intuitively, it is helpful to have a very high value (the maximum is 1.0) for <img class="fm-editor-equation59" src="../images/00137.jpeg"/> during the initial stages of the agent's learning process so that the agent can explore the state space by taking mostly random actions. Once it has got enough experience and has gained a better understanding of the environment, lowering the <img class="fm-editor-equation59" src="../images/00138.jpeg"/> value will let the agent take actions based on what it believes to be the best actions more often. It will be useful to have a utility function that takes care of varying the <img class="fm-editor-equation59" src="../images/00139.jpeg"/> value, right? Let's implement such a function in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing an epsilon decay schedule</h1>
                
            
            <article>
                
<p class="calibre2">We can decay (or decrease) the <img class="fm-editor-equation59" src="../images/00140.jpeg"/> value linearly (in the following left-hand side graph), exponentially (in the following right-hand side graph) or using some other decay schedule. <span class="calibre5">Linear and exponential schedules are the most commonly used decay schedules for the exploration parameter </span><img class="fm-editor-equation59" src="../images/00141.jpeg"/>:</p>
<div class="cdpaligncenter"><img src="../images/00142.jpeg" class="calibre70"/></div>
<p class="calibre2">In the preceding graphs, you can see how the epsilon (exploration) value varies with the different schedule schemes (linear on the left graph, exponential on the right graph). The decay schedule shown in the preceding graphs use an epsilon_max (start) value of 1, epsilon_min (final) value of 0.01 in the linear case, and exp(-10000/2000) in the exponential case, with both of them maintaining a constant value of epsilon_min after 10,000 episodes.</p>
<p class="calibre2">The following code implements the <kbd class="calibre12">LinearDecaySchedule</kbd>, which we will use for our<span class="calibre5"> </span><kbd class="calibre12">Deep_Q_Learning</kbd><span class="calibre5"> </span>agent implementation to play Atari games:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>class LinearDecaySchedule(object):<br class="title-page-name"/>    def __init__(self, initial_value, final_value, max_steps):<br class="title-page-name"/>        assert initial_value &gt; final_value, "initial_value should be &lt; final_value"<br class="title-page-name"/>        self.initial_value = initial_value<br class="title-page-name"/>        self.final_value = final_value<br class="title-page-name"/>        self.decay_factor = (initial_value - final_value) / max_steps<br class="title-page-name"/><br class="title-page-name"/>    def __call__(self, step_num):<br class="title-page-name"/>        current_value = self.initial_value - self.decay_factor * step_num<br class="title-page-name"/>        if current_value &lt; self.final_value:<br class="title-page-name"/>            current_value = self.final_value<br class="title-page-name"/>        return current_value<br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    import matplotlib.pyplot as plt<br class="title-page-name"/>    epsilon_initial = 1.0<br class="title-page-name"/>    epsilon_final = 0.05<br class="title-page-name"/>    MAX_NUM_EPISODES = 10000<br class="title-page-name"/>    MAX_STEPS_PER_EPISODE = 300<br class="title-page-name"/>    linear_sched = LinearDecaySchedule(initial_value = epsilon_initial,<br class="title-page-name"/>                                    final_value = epsilon_final,<br class="title-page-name"/>                                    max_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)<br class="title-page-name"/>    epsilon = [linear_sched(step) for step in range(MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)]<br class="title-page-name"/>    plt.plot(epsilon)<br class="title-page-name"/>    plt.show()</pre>
<p class="calibre2">The preceding script is available at <kbd class="calibre12">ch6/utils/decay_schedule.py</kbd> in this book's code repository. If you run the script, you will see that the <kbd class="calibre12">main </kbd>function creates a linear decay schedule for epsilon and plots the value. You can experiment with different values of <kbd class="calibre12">MAX_NUM_EPISODES</kbd>, <kbd class="calibre12">MAX_STEPS_PER_EPISODE</kbd>, <kbd class="calibre12">epsilon_initial</kbd>, and <kbd class="calibre12">epsilon_final</kbd> to visually see how the epsilon values vary with the number of steps. In the next section, we will implement the <kbd class="calibre12">get_action(...)</kbd><span class="calibre5"> </span>method which implements the <img class="fm-editor-equation59" src="../images/00143.jpeg"/>-greedy action selection policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing a deep Q-learning agent</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will discuss how we can scale up our shallow Q-learner to a more sophisticated and powerful deep Q-learner-based agent that can learn to act based on raw visual image inputs, which we will use towards the end of this chapter to train agents that play Atari games well. Note that you can train this deep Q-learning agent in any learning environments with a discrete action space. The Atari game environments are one such interesting class of environments that we will use in this book.</p>
<p class="calibre2">We will start with a deep convolutional Q-network implementation and incorporate it into our Q-learner. Then, we will see how we can use the technique of target Q-networks to improve the stability of the deep Q-learner. We will then combine all the techniques we have discussed so far to put together the full implementation of our deep Q learning-based agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing a deep convolutional Q-network in PyTorch</h1>
                
            
            <article>
                
<p class="calibre2">Let's implement a 3-layer deep <strong class="calibre4">Convolutional Neural Network</strong> (<strong class="calibre4">CNN</strong>) that takes the Atari game screen pixels as the input and outputs the action-values for each of the possible actions for that particular game, which is defined in the OpenAI Gym environment. The following code is for the CNN class:</p>
<pre class="calibre17">import torch<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class CNN(torch.nn.Module):<br class="title-page-name"/>    """<br class="title-page-name"/>    A Convolution Neural Network (CNN) class to approximate functions with visual/image inputs<br class="title-page-name"/>    """<br class="title-page-name"/>    def __init__(self, input_shape, output_shape, device="cpu"):<br class="title-page-name"/>        """<br class="title-page-name"/>        :param input_shape: Shape/dimension of the input image. Assumed to be resized to C x 84 x 84<br class="title-page-name"/>        :param output_shape: Shape/dimension of the output.<br class="title-page-name"/>        :param device: The device (cpu or cuda) that the CNN should use to store the inputs for the forward pass<br class="title-page-name"/>        """<br class="title-page-name"/>        # input_shape: C x 84 x 84<br class="title-page-name"/>        super(CNN, self).__init__()<br class="title-page-name"/>        self.device = device<br class="title-page-name"/>        self.layer1 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2, padding=1),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.layer2 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=0),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.layer3 = torch.nn.Sequential(<br class="title-page-name"/>            torch.nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),<br class="title-page-name"/>            torch.nn.ReLU()<br class="title-page-name"/>        )<br class="title-page-name"/>        self.out = torch.nn.Linear(18 * 18 * 32, output_shape)<br class="title-page-name"/><br class="title-page-name"/>    def forward(self, x):<br class="title-page-name"/>        x = torch.from_numpy(x).float().to(self.device)<br class="title-page-name"/>        x = self.layer1(x)<br class="title-page-name"/>        x = self.layer2(x)<br class="title-page-name"/>        x = self.layer3(x)<br class="title-page-name"/>        x = x.view(x.shape[0], -1)<br class="title-page-name"/>        x = self.out(x)<br class="title-page-name"/>        return x</pre>
<div class="packt_infobox">As you can see, it is easy to add more layers to the neural network. We could use a deeper network that has more than three layers, but it will come at the cost of requiring more compute power and time. In deep reinforcement learning, and especially in Q learning with function approximation, there are no proven convergence guarantees. We should therefore make sure that our agent's implementation is good enough to learn and make progress well before increasing the capacity of the Q /value-function representation by using a much deeper neural network.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using the target Q-network to stabilize an agent's learning</h1>
                
            
            <article>
                
<p class="calibre2">A simple technique of freezing the Q-network for a fixed number of steps and then using that to generate the Q learning targets to update the parameters of the deep Q-network was shown to be considerably effective in reducing the oscillations and stabilize Q learning with neural network approximation. This technique is a relatively simpler one, but it turns out to be very helpful for stable learning.</p>
<p class="calibre2">The implementation is going to be straightforward and simple. We have to make two changes or updates to our existing deep Q-learner class:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Create a target Q-network and sync/update it with the original Q-network periodically</li>
<li value="2" class="calibre11">Use the target Q-network to generate the Q-learning targets</li>
</ol>
<p class="calibre2">To compare how the agent performs with and without the target Q-network, you can use the parameter manager and the logging and visualization tools that we developed in the earlier sections of this chapter to visually verify the performance gain with the target Q-network enabled.</p>
<p class="calibre2">We will first add a new class member called <kbd class="calibre12">Q_target</kbd> that we can add to the <kbd class="calibre12">__init__ </kbd>method of our deep Q-learner class. The following code snippet shows the lines of code where we add the new member right after our previous declaration of <kbd class="calibre12">self.DQN</kbd> in the <kbd class="calibre12">deep_Q_learner.py</kbd> script:</p>
<pre class="calibre17">self.Q = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)<br class="title-page-name"/><strong class="calibre1">if self.params['use_target_network']:</strong><br class="title-page-name"/><strong class="calibre1">    self.Q_target = self.DQN(state_shape, action_shape, device).to(device)</strong></pre>
<p class="calibre2">We can then modify the <kbd class="calibre12">learn_from_batch_experience</kbd> method that we implemented earlier to use the target Q-network to create the Q-learning target. The following code snippet shows the changes in bold font from our first implementation:</p>
<pre class="calibre17">def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        <strong class="calibre1">if self.params['use_target_network']:</strong><br class="title-page-name"/><strong class="calibre1">            if self.step_num % self.params['target_network_update_freq'] == 0:</strong><br class="title-page-name"/><strong class="calibre1">                # The *update_freq is the Num steps after which target net is updated.</strong><br class="title-page-name"/><strong class="calibre1">                # A schedule can be used instead to vary the update freq.</strong><br class="title-page-name"/><strong class="calibre1">                self.Q_target.load_state_dict(self.Q.state_dict())</strong><br class="title-page-name"/><strong class="calibre1">            td_target = reward_batch + ~done_batch * \</strong><br class="title-page-name"/><strong class="calibre1">                np.tile(self.gamma, len(next_obs_batch)) * \</strong><br class="title-page-name"/><strong class="calibre1">                self.Q_target(next_obs_batch).max(1)[0].data</strong><br class="title-page-name"/>       <strong class="calibre1"> else:</strong><br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>                                                       td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        writer.add_scalar("DQL/td_error", td_error.mean(), self.step_num)<br class="title-page-name"/>        self.Q_optimizer.step()</pre>
<p class="calibre2">This completes our target Q-network implementation.</p>
<p class="calibre2">How do we know if the agent is benefiting from the target Q-network and other improvements we discussed in the previous sections? In the next section, we will look at ways to log and visualize the agent's performance so that we can monitor and figure out whether or not the improvements we discussed actually lead to better performances. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Logging and visualizing an agent's learning process</h1>
                
            
            <article>
                
<p class="calibre2">We now have a learning agent that uses a neural network to learn Q-values and update itself to perform better at the task. The agent takes a while to learn before it starts acting wisely. How do we know what is going on with the agent at a given time? How do we know if the agent is making progress or simply acting dumb? How do we see and measure the progress of the agent with time? Should we just sit and wait for the training to end? No. There should be some better way, don't you think?</p>
<p class="calibre2">Yes, and there is! It is actually important for us, the developers of the agents, to be able to observe how the agent is performing in order to figure out if there is an issue with the implementation or if some of the hyperparameters are too bad for the agent to learn anything. We have had the preliminary version of logging and seen how the agent's learning was progressing with the console outputs generated using the print statements. That gave us a first-hand look into the episode number, episode reward, the maximum reward, and so on, but it was more like a single snapshot at a given time. We want to be able to see the history of the progress to see if the agent's learning is converging with the learning error decreasing or not, and so on. This will enable us to think in the right direction to update our implementation or tweak the parameters to improve the learning performance of the agent.</p>
<p class="calibre2">The TensorFlow deep learning library offers a tool called TensorBoard. It is a powerful tool to visualize the neural network graphs, plot quantitative metrics like learning errors, rewards, and so on as the training progresses. It can even be used to visualize images and a few other useful data types. It makes it easier to understand, identify, and debug our deep learning algorithm implementations. In the next section, we will see how we can use TensorBoard to log and visualize our agent's progress.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using TensorBoard for logging and visualizing a PyTorch RL agent's progress</h1>
                
            
            <article>
                
<p class="calibre2">Though TensorBoard is a tool that was released for the TensorFlow deep learning library, it is a flexible tool in itself, which can be used with other deep learning libraries like PyTorch. Basically, the TensorBoard tool reads the TensorFlow events summary data from log files and updates the visualizations and plots periodically. Fortunately, we have a library called <kbd class="calibre12">tensorboardX</kbd> that provides a convenient interface to create the events that TensorBoard can work with. This way, we can easily generate the appropriate events from our agent training code to log and visualize how our agent's learning process is progressing. The use of this library is pretty straightforward and simple. We import <kbd class="calibre12">tensorboardX</kbd> and create a<span class="calibre5"> </span><kbd class="calibre12">SummaryWriter</kbd> object with the desired log file name. We can then add new scalars (and also other supported data) using the <kbd class="calibre12">SummaryWriter</kbd> object to add new data points to the plot which will be updated periodically. The following screenshot is an example of what the TensorBoard's output will look like with the kind of information we will be logging in our agent training script to visualize its progress:</p>
<p class="cdpaligncenter4"><img src="../images/00144.jpeg" class="calibre71"/></p>
<p class="calibre2">In the preceding screenshot, the bottom-right most plot titled <strong class="calibre4">main/mean_ep_reward</strong> shows how the agent has been learning to progressively get higher and higher rewards over time steps. In all the plots in the preceding screenshot, the <em class="calibre13">x</em>-axis shows the number of training steps and the <em class="calibre13">y</em>-axis has the value of the data that was logged, as signified by the titles of each of the plots. </p>
<p class="calibre2">Now, we know how to log and visualize the performance of the agent as it is training. But still, a question remains as to how we can compare the agent with and without one or more of the improvements we discussed in the earlier sections in this chapter. We discussed several improvements, and each adds new hyperparameters. In order to manage the various hyperparameters and to easily turn on and off the improvements and configurations, in the next section, we will discuss a way to achieve this by building a simple parameter management class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Managing hyperparameters and configuration parameters</h1>
                
            
            <article>
                
<p class="calibre2">As you may have noticed, our agent has several hyperparameters like the learning rate, gamma, epsilon start/minimum value, and so on. There are also several configuration parameters for both the agent and the environment that we would want to be able to modify easily and run instead of searching through the code to find where that parameter was defined. Having a simple and good way to manage these parameters also helps when we want to automate the training process or run parameter sweeps or other methods to tune and find the best set of parameters that work for the agent.</p>
<p class="calibre2">In the following two subsections, we will look at how we can use a JSON file to specify the parameters and hyperparameters in an easy to use way and implement a parameter manager class to handle these externally configurable parameters to update the agent and the environment configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using a JSON file to easily configure parameters</h1>
                
            
            <article>
                
<p class="calibre2">Before we implement our parameter manager, let's get an idea of what our parameter configuration JSON file will look like. The following is a snippet of the <kbd class="calibre12">parameters.json</kbd> file that we will use to configure the parameters of the agent and the environment. The <strong class="calibre4">JavaScript Object Notation</strong> (<strong class="calibre4">JSON</strong>) file is a convenient and human-readable format for such data representation. We will discuss what each of these parameters mean in the later sections of this chapter. For now, we will concentrate on how we can use such a file to specify or change the parameters used by the agent and the environment:</p>
<pre class="calibre17">{<br class="title-page-name"/>  "agent": {<br class="title-page-name"/>    "max_num_episodes": 70000,<br class="title-page-name"/>    "max_steps_per_episode": 300,<br class="title-page-name"/>    "replay_batch_size": 2000,<br class="title-page-name"/>    "use_target_network": true,<br class="title-page-name"/>    "target_network_update_freq": 2000,<br class="title-page-name"/>    "lr": 5e-3,<br class="title-page-name"/>    "gamma": 0.98,<br class="title-page-name"/>    "epsilon_max": 1.0,<br class="title-page-name"/>    "epsilon_min": 0.05,<br class="title-page-name"/>    "seed": 555,<br class="title-page-name"/>    "use_cuda": true,<br class="title-page-name"/>    "summary_filename_prefix": "logs/DQL_"<br class="title-page-name"/>  },<br class="title-page-name"/>  "env": {<br class="title-page-name"/>    "type": "Atari",<br class="title-page-name"/>    "episodic_life": "True",<br class="title-page-name"/>    "clip_reward": "True",<br class="title-page-name"/>    "useful_region": {<br class="title-page-name"/>        "Default":{<br class="title-page-name"/>                "crop1": 34,<br class="title-page-name"/>                "crop2": 34,<br class="title-page-name"/>                "dimension2": 80<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The parameters manager</h1>
                
            
            <article>
                
<p class="calibre2">Did you like the parameter configuration file example that you just saw? I hope you did. In this section, we will implement a parameter manger that will help us load, get, and set these parameters as necessary. </p>
<p class="calibre2">We will start by creating a Python class named <kbd class="calibre12">ParamsManger</kbd><span class="calibre5"> </span>that initializes the <kbd class="calibre12">params</kbd><span class="calibre5"> </span>member variable with the dictionary of parameters read from the <kbd class="calibre12">params_file</kbd><span class="calibre5"> </span>using the JSON Python library, as follows:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import JSON<br class="title-page-name"/><br class="title-page-name"/>class ParamsManager(object):<br class="title-page-name"/>    def __init__(self, params_file):<br class="title-page-name"/>        """<br class="title-page-name"/>        A class to manage the Parameters. Parameters include configuration parameters and Hyper-parameters<br class="title-page-name"/>        :param params_file: Path to the parameters JSON file<br class="title-page-name"/>        """<br class="title-page-name"/>        self.params = JSON.load(open(params_file, 'r'))</pre>
<p class="calibre2">We will then implement a few methods that will be convenient for us. We will start with the <kbd class="calibre12">get_params</kbd><span class="calibre5"> </span>method that returns the whole dictionary of parameters that we read from the JSON file:</p>
<pre class="calibre17">    def get_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns all the parameters<br class="title-page-name"/>        :return: The whole parameter dictionary<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params</pre>
<p class="calibre2">Sometimes, we may just want to get the parameters that correspond to the agent or those that correspond to the environment which we can pass in while we initialize the agent or the environment. Since we had neatly separated the agent and the environment parameters in the <kbd class="calibre12">parameters.json</kbd><span class="calibre5"> </span>file that we saw in the previous section, the implementation is straightforward, as follows:</p>
<pre class="calibre17"> def get_env_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns the environment configuration parameters<br class="title-page-name"/>        :return: A dictionary of configuration parameters used for the environment<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params['env']<br class="title-page-name"/>    def get_agent_params(self):<br class="title-page-name"/>        """<br class="title-page-name"/>        Returns the hyper-parameters and configuration parameters used by the agent<br class="title-page-name"/>        :return: A dictionary of parameters used by the agent<br class="title-page-name"/>        """<br class="title-page-name"/>        return self.params['agent']</pre>
<p class="calibre2">We will also implement another simple method to update the agent parameters so that we can also supply/read the agent parameters from the command line when we launch our training script:</p>
<pre class="calibre17">   <br class="title-page-name"/>    def update_agent_params(self, **kwargs):<br class="title-page-name"/>        """<br class="title-page-name"/>        Update the hyper-parameters (and configuration parameters) used by the agent<br class="title-page-name"/>        :param kwargs: Comma-separated, hyper-parameter-key=value pairs. Eg.: lr=0.005, gamma=0.98<br class="title-page-name"/>        :return: None<br class="title-page-name"/>        """<br class="title-page-name"/>        for key, value in kwargs.items():<br class="title-page-name"/>            if key in self.params['agent'].keys():<br class="title-page-name"/>                self.params['agent'][key] = value</pre>
<p class="calibre2">The preceding parameter manager implementation, along with a simple test procedure, is available at <kbd class="calibre12">ch6/utils/params_manager.py</kbd><span class="calibre5">, </span>in this book's code repository. <span class="calibre5">In the next section, we will consolidate all the techniques we have discussed and implemented so far to put together a complete deep Q learning-based agent.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">A complete deep Q-learner to solve complex problems with raw pixel input</h1>
                
            
            <article>
                
<p class="calibre2">From the beginning of this chapter, we have implemented several additional techniques and utility tools to improve the agent. In this section, we will consolidate all the improvements and the utility tools we have discussed so far into a unified <kbd class="calibre12">deep_Q_Learner.py</kbd> script. We will be using this unified agent script to train on the Atari Gym environment in the next section and watch the agent improving its performance and fetching more and more scores over time.</p>
<p class="calibre2">The following code is the unified version that utilizes the following features that we developed in the previous sections of this chapter:</p>
<ul class="calibre10">
<li class="calibre11">Experience memory</li>
<li class="calibre11">Experience replay to learn from (mini) batches of experience</li>
<li class="calibre11">Linear epsilon decay schedule</li>
<li class="calibre11">Target network for stable learning</li>
</ul>
<ul class="calibre10">
<li class="calibre11"><span>Parameter management using JSON files</span></li>
<li class="calibre11">Performance visualization and logging using TensorBoard:</li>
</ul>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>import gym<br class="title-page-name"/>import torch<br class="title-page-name"/>import random<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>import environment.atari as Atari<br class="title-page-name"/>from utils.params_manager import ParamsManager<br class="title-page-name"/>from utils.decay_schedule import LinearDecaySchedule<br class="title-page-name"/>from utils.experience_memory import Experience, ExperienceMemory<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/>from function_approximator.cnn import CNN<br class="title-page-name"/>from tensorboardX import SummaryWriter<br class="title-page-name"/>from datetime import datetime<br class="title-page-name"/>from argparse import ArgumentParser<br class="title-page-name"/><br class="title-page-name"/>args = ArgumentParser("deep_Q_learner")<br class="title-page-name"/>args.add_argument("--params-file",<br class="title-page-name"/>                  help="Path to the parameters JSON file. Default is parameters.JSON",<br class="title-page-name"/>                  default="parameters.JSON",<br class="title-page-name"/>                  type=str,<br class="title-page-name"/>                  metavar="PFILE")<br class="title-page-name"/>args.add_argument("--env-name",<br class="title-page-name"/>                  help="ID of the Atari environment available in OpenAI Gym. Default is Pong-v0",<br class="title-page-name"/>                  default="Pong-v0",<br class="title-page-name"/>                  type=str,<br class="title-page-name"/>                  metavar="ENV")<br class="title-page-name"/>args = args.parse_args()<br class="title-page-name"/><br class="title-page-name"/>params_manager= ParamsManager(args.params_file)<br class="title-page-name"/>seed = params_manager.get_agent_params()['seed']<br class="title-page-name"/>summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']<br class="title-page-name"/>summary_file_name = summary_file_path_prefix + args.env_name + "_" + datetime.now().strftime("%y-%m-%d-%H-%M")<br class="title-page-name"/>writer = SummaryWriter(summary_file_name)<br class="title-page-name"/>global_step_num = 0<br class="title-page-name"/>use_cuda = params_manager.get_agent_params()['use_cuda']<br class="title-page-name"/># new in PyTorch 0.4<br class="title-page-name"/>device = torch.device("cuda" if torch.cuda.is_available() and use_cuda else "cpu")<br class="title-page-name"/>torch.manual_seed(seed)<br class="title-page-name"/>np.random.seed(seed)<br class="title-page-name"/>if torch.cuda.is_available() and use_cuda:<br class="title-page-name"/>    torch.cuda.manual_seed_all(seed)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Deep_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, params):<br class="title-page-name"/>        """<br class="title-page-name"/>        self.Q is the Action-Value function. This agent represents Q using a Neural Network<br class="title-page-name"/>        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional<br class="title-page-name"/>        image, use a Convolutional-Neural-Network<br class="title-page-name"/><br class="title-page-name"/>        :param state_shape: Shape (tuple) of the observation/state<br class="title-page-name"/>        :param action_shape: Shape (number) of the discrete action space<br class="title-page-name"/>        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters<br class="title-page-name"/>        """<br class="title-page-name"/>        self.state_shape = state_shape<br class="title-page-name"/>        self.action_shape = action_shape<br class="title-page-name"/>        self.params = params<br class="title-page-name"/>        self.gamma = self.params['gamma'] # Agent's discount factor<br class="title-page-name"/>        self.learning_rate = self.params['lr'] # Agent's Q-learning rate<br class="title-page-name"/><br class="title-page-name"/>        if len(self.state_shape) == 1: # Single dimensional observation/state space<br class="title-page-name"/>            self.DQN = SLP<br class="title-page-name"/>        elif len(self.state_shape) == 3: # 3D/image observation/state<br class="title-page-name"/>            self.DQN = CNN<br class="title-page-name"/><br class="title-page-name"/>        self.Q = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)<br class="title-page-name"/>        if self.params['use_target_network']:<br class="title-page-name"/>            self.Q_target = self.DQN(state_shape, action_shape, device).to(device)<br class="title-page-name"/>        # self.policy is the policy followed by the agent. This agents follows<br class="title-page-name"/>        # an epsilon-greedy policy w.r.t it's Q estimate.<br class="title-page-name"/>        self.policy = self.epsilon_greedy_Q<br class="title-page-name"/>        self.epsilon_max = 1.0<br class="title-page-name"/>        self.epsilon_min = 0.05<br class="title-page-name"/>        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,<br class="title-page-name"/>                                    final_value=self.epsilon_min,<br class="title-page-name"/>                                    max_steps= self.params['epsilon_decay_final_step'])<br class="title-page-name"/>        self.step_num = 0<br class="title-page-name"/>                <br class="title-page-name"/>        self.memory = ExperienceMemory(capacity=int(self.params['experience_memory_capacity'])) # Initialize an Experience memory with 1M capacity<br class="title-page-name"/><br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        if len(observation.shape) == 3: # Single image (not a batch)<br class="title-page-name"/>            if observation.shape[2] &lt; observation.shape[0]: # Probably observation is in W x H x C format<br class="title-page-name"/>                # Reshape to C x H x W format as per PyTorch's convention<br class="title-page-name"/>                observation = observation.reshape(observation.shape[2], observation.shape[1], observation.shape[0])<br class="title-page-name"/>            observation = np.expand_dims(observation, 0) # Create a batch dimension<br class="title-page-name"/>        return self.policy(observation)<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        # Decay Epsilon/exploration as per schedule<br class="title-page-name"/>        writer.add_scalar("DQL/epsilon", self.epsilon_decay(self.step_num), self.step_num)<br class="title-page-name"/>        self.step_num +=1<br class="title-page-name"/>        if random.random() &lt; self.epsilon_decay(self.step_num):<br class="title-page-name"/>            action = random.choice([i for i in range(self.action_shape)])<br class="title-page-name"/>        else:<br class="title-page-name"/>            action = np.argmax(self.Q(observation).data.to(torch.device('cpu')).numpy())<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>        return action<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next, done):<br class="title-page-name"/>        # TD(0) Q-learning<br class="title-page-name"/>        if done: # End of episode<br class="title-page-name"/>            td_target = reward + 0.0 # Set the value of terminal state to zero<br class="title-page-name"/>        else:<br class="title-page-name"/>            td_target = r + self.gamma * torch.max(self.Q(s_next))<br class="title-page-name"/>        td_error = td_target - self.Q(s)[a]<br class="title-page-name"/>        # Update Q estimate<br class="title-page-name"/>        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error<br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.backward()<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/>    def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        batch_xp = Experience(*zip(*experiences))<br class="title-page-name"/>        obs_batch = np.array(batch_xp.obs)<br class="title-page-name"/>        action_batch = np.array(batch_xp.action)<br class="title-page-name"/>        reward_batch = np.array(batch_xp.reward)<br class="title-page-name"/>        next_obs_batch = np.array(batch_xp.next_obs)<br class="title-page-name"/>        done_batch = np.array(batch_xp.done)<br class="title-page-name"/><br class="title-page-name"/>        if self.params['use_target_network']:<br class="title-page-name"/>            if self.step_num % self.params['target_network_update_freq'] == 0:<br class="title-page-name"/>                # The *update_freq is the Num steps after which target net is updated.<br class="title-page-name"/>                # A schedule can be used instead to vary the update freq.<br class="title-page-name"/>                self.Q_target.load_state_dict(self.Q.state_dict())<br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q_target(next_obs_batch).max(1)[0].data<br class="title-page-name"/>        else:<br class="title-page-name"/>            td_target = reward_batch + ~done_batch * \<br class="title-page-name"/>                np.tile(self.gamma, len(next_obs_batch)) * \<br class="title-page-name"/>                self.Q(next_obs_batch).detach().max(1)[0].data<br class="title-page-name"/><br class="title-page-name"/>        td_target = td_target.to(device)<br class="title-page-name"/>        action_idx = torch.from_numpy(action_batch).to(device)<br class="title-page-name"/>        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),<br class="title-page-name"/>                                                       td_target.float().unsqueeze(1))<br class="title-page-name"/><br class="title-page-name"/>        self.Q_optimizer.zero_grad()<br class="title-page-name"/>        td_error.mean().backward()<br class="title-page-name"/>        writer.add_scalar("DQL/td_error", td_error.mean(), self.step_num)<br class="title-page-name"/>        self.Q_optimizer.step()<br class="title-page-name"/><br class="title-page-name"/>    def replay_experience(self, batch_size = None):<br class="title-page-name"/>        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']<br class="title-page-name"/>        experience_batch = self.memory.sample(batch_size)<br class="title-page-name"/>        self.learn_from_batch_experience(experience_batch)<br class="title-page-name"/><br class="title-page-name"/>    def save(self, env_name):<br class="title-page-name"/>        file_name = self.params['save_dir'] + "DQL_" + env_name + ".ptm"<br class="title-page-name"/>        torch.save(self.Q.state_dict(), file_name)<br class="title-page-name"/>        print("Agent's Q model state saved to ", file_name)<br class="title-page-name"/><br class="title-page-name"/>    def load(self, env_name):<br class="title-page-name"/>        file_name = self.params['load_dir'] + "DQL_" + env_name + ".ptm"<br class="title-page-name"/>        self.Q.load_state_dict(torch.load(file_name))<br class="title-page-name"/>        print("Loaded Q model state from", file_name)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env_conf = params_manager.get_env_params()<br class="title-page-name"/>    env_conf["env_name"] = args.env_name<br class="title-page-name"/>    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default<br class="title-page-name"/>    custom_region_available = False<br class="title-page-name"/>    for key, value in env_conf['useful_region'].items():<br class="title-page-name"/>        if key in args.env_name:<br class="title-page-name"/>            env_conf['useful_region'] = value<br class="title-page-name"/>            custom_region_available = True<br class="title-page-name"/>            break<br class="title-page-name"/>    if custom_region_available is not True:<br class="title-page-name"/>        env_conf['useful_region'] = env_conf['useful_region']['Default']<br class="title-page-name"/>    print("Using env_conf:", env_conf)<br class="title-page-name"/>    env = Atari.make_env(args.env_name, env_conf)<br class="title-page-name"/>    observation_shape = env.observation_space.shape<br class="title-page-name"/>    action_shape = env.action_space.n<br class="title-page-name"/>    agent_params = params_manager.get_agent_params()<br class="title-page-name"/>    agent = Deep_Q_Learner(observation_shape, action_shape, agent_params)<br class="title-page-name"/>    if agent_params['load_trained_model']:<br class="title-page-name"/>        try:<br class="title-page-name"/>            agent.load(env_conf["env_name"])<br class="title-page-name"/>        except FileNotFoundError:<br class="title-page-name"/>            print("WARNING: No trained model found for this environment. Training from scratch.")<br class="title-page-name"/>    first_episode = True<br class="title-page-name"/>    episode_rewards = list()<br class="title-page-name"/>    for episode in range(agent_params['max_num_episodes']):<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        done = False<br class="title-page-name"/>        step = 0<br class="title-page-name"/>        #for step in range(agent_params['max_steps_per_episode']):<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            if env_conf['render']:<br class="title-page-name"/>                env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            #agent.learn(obs, action, reward, next_obs, done)<br class="title-page-name"/>            agent.memory.store(Experience(obs, action, reward, next_obs, done))<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/>            step += 1<br class="title-page-name"/>            global_step_num +=1<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                if first_episode: # Initialize max_reward at the end of first episode<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    first_episode = False<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; max_reward:<br class="title-page-name"/>                    max_reward = cum_reward<br class="title-page-name"/>                    agent.save(env_conf['env_name'])<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))<br class="title-page-name"/>                writer.add_scalar("main/ep_reward", cum_reward, global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/mean_ep_reward", np.mean(episode_rewards), global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/max_ep_rew", max_reward, global_step_num)<br class="title-page-name"/>                if agent.memory.get_size() &gt;= 2 * agent_params['replay_batch_size']:<br class="title-page-name"/>                    agent.replay_experience()<br class="title-page-name"/><br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()<br class="title-page-name"/>    writer.close()</pre>
<p class="calibre2">The preceding code, along with some additional changes required for using the Atari wrappers that we will be discussing in the next section, are available at <kbd class="calibre12">ch6/deep_Q_Learner.py</kbd>, in this book's code repository. After we complete the next section on <em class="calibre13">The Atari Gym environment</em>, we will use the agent implementation in <kbd class="calibre12">deep_Q_Learner.py</kbd> to train the agents on the Atari games and see their performance in the end.</p>
<div class="packt_tip">This book's code repository will have the latest and up-to-date code implementations with improvements and bug fixes that will be committed after this book is printed. So, it is a good idea to star and watch the book's code repository on GitHub to get automatic updates about these changes and improvements.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The Atari Gym environment</h1>
                
            
            <article>
                
<p class="calibre2">In <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 4</a>, <em class="calibre13">Exploring the Gym and its Features</em>, we looked at the various list of environments available in the Gym, including the Atari games category, and used a script to list all the Gym environments available on your computer. We also looked at the nomenclature of the environment names, especially for the Atari games. In this section, we will use the Atari environments and see how we can customize the environments with Gym environment wrappers. The following is a collage of 9 screenshots from 9 different Atari environments:</p>
<div class="cdpaligncenter"><img src="../images/00145.jpeg" class="calibre72"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Customizing the Atari Gym environment</h1>
                
            
            <article>
                
<p class="calibre2">Sometimes, we may want to change the way the observations are sent back by the environment or change the scale of the rewards so that our agents can learn better or filter out some information before the agent receives them or change the way the environment is rendered on the screen. So far, we have been developing and customizing our agent to make it act well in the environment. Wouldn't it be nice to have some flexibility around how and what the environment sends back to the agent so that we can customize how the agent learns to act? Fortunately, the Gym library makes it easy to extend or customize the information sent by the environment with the help of Gym environment wrappers. The wrapper interface allows us to subclass and add routines as layers on top of the previous routines. We can add custom processing statements to one or more of the following methods of the Gym environment class:<span class="calibre5"> </span></p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">__init__(self, env)__</kbd></li>
<li class="calibre11"><kbd class="calibre12"><span>_seed</span></kbd></li>
<li class="calibre11"><kbd class="calibre12">_reset</kbd></li>
<li class="calibre11"><kbd class="calibre12">_step</kbd></li>
<li class="calibre11"><kbd class="calibre12">_render</kbd></li>
<li class="calibre11"><kbd class="calibre12">_close</kbd></li>
</ul>
<p class="calibre2">Depending on the customization we would like to do to the environment, we can decide which methods we want to extend. For example, if we want to change the shape/size of the observation, we can extend the <kbd class="calibre12">_step</kbd> and <kbd class="calibre12">_reset</kbd> methods. <span class="calibre5">In the next subsection, we will see how we can make use of the wrapper interface to customize the Atari Gym environments.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing custom Gym environment wrappers</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will look at a few Gym environment wrappers that are especially very useful for the Gym Atari environments. Most of the wrappers we will implement in this section can be used with other environments as well to improve the learning performance of the agents.</p>
<p class="calibre2">The following table mentions a list of the wrappers will be implementing in the following section with a brief description for each of the wrappers to give you an overview:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">Wrapper</strong></td>
<td class="calibre48"><strong class="calibre1">Brief description of the purpose</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2"><kbd class="calibre12"><span>ClipRewardEnv</span></kbd></p>
</td>
<td class="calibre48"><span> To implement reward clipping</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>AtariRescale</span></kbd></td>
<td class="calibre48"><span> To rescale the screen pixels to a 84x84x1 gray scale image</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>NormalizedEnv</span></kbd></td>
<td class="calibre48"><span>To normalize the images based on the mean and variance observed in the environment</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>NoopResetEnv</span></kbd></td>
<td class="calibre48"><span>To perform a random number of <kbd class="calibre12">noop</kbd> (empty) actions on reset to sample different initial states</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>FireResetEnv</span></kbd></td>
<td class="calibre48"><span>To perform a fire action on reset</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>EpisodicLifeEnv</span></kbd></td>
<td class="calibre48"><span>To mark end of life as end of episode and reset when game is over</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>MaxAndSkipEnv</span></kbd></td>
<td class="calibre48">Repeats the action for a fixed number (specified using the <kbd class="calibre12">skip</kbd> argument; the default is 4) of steps</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reward clipping</h1>
                
            
            <article>
                
<p class="calibre2">Different problems or environments provide different ranges of reward values. For example, we saw in the previous chapter that in the <kbd class="calibre12">Mountain Car v0</kbd> environment, the agent receives a reward of -1 for every time step until episode termination, no matter which way the agent moves the car. In the <kbd class="calibre12">Cart Pole v0</kbd> environment, the agent receives a reward of +1 for every time step until episode termination. <span class="calibre5">In Atari game environments like MS Pac-Man, if the agent eats a single ghost, it will receive a reward of up to +1,600. We can start to see how the magnitudes of the reward as well as the occasion of the reward varies widely across different environments and learning problems. If our deep Q-learner agent algorithm has to solve this variety of problems without us trying to fine-tune the hyperparameters to work well for each of the environments independently, we have to do something about the varying scales of reward. This is exactly the intuition behind </span>reward clipping, i<span class="calibre5">n which we clip the reward to be either -1, 0, or +1, depending on the sign of the actual reward received from the environment. This way, we limit the magnitude of the reward which can vary widely across the different environments. We can implement this simple reward clipping technique and apply it to our environments by inheriting from the <kbd class="calibre12">gym.RewardWrapper</kbd></span> class and modifying the <kbd class="calibre12">reward(...) </kbd>function, as shown in the following code snippet:</p>
<pre class="calibre17">class ClipRewardEnv(gym.RewardWrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        gym.RewardWrapper.__init__(self, env)<br class="title-page-name"/><br class="title-page-name"/>    def reward(self, reward):<br class="title-page-name"/>        """ Clip rewards to be either -1, 0 or +1 based on the sign"""<br class="title-page-name"/>        return np.sign(reward)</pre>
<div class="packt_infobox">The technique of clipping the reward to (-1, 0, 1) works well for Atari games. But, it is good to know that this may not be the the best technique to universally handle environments with varying reward magnitudes and frequency. Clipping the reward value modifies the learning objective of the agent and may sometimes lead to qualitatively different policies being learned by the agent than what is desired.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Preprocessing Atari screen image frames</h1>
                
            
            <article>
                
<p class="calibre2">The Atari Gym environment produces observations which typically have a shape of 210x160x3, which represents a RGB (color) image of a width of 210 pixels and a height of 160 pixels. While the color image at the original resolution of 210x160x3 has more pixels and therefore more information, it turns out that often, better performance is possible with reduced resolution. Lower resolution means less data to be processed by the agent at every step, which translates to faster training time, especially on consumer grade computing hardware that you and I own.</p>
<p class="calibre2">Let's create a preprocessing pipeline that would take the original observation image (of the Atari screen) and perform the following operations:</p>
<div class="cdpaligncenter"><img src="../images/00146.jpeg" class="calibre73"/></div>
<p class="calibre2">We can crop out the region on the screen that does not have any useful information regarding the environment for the agent. </p>
<p class="calibre2">Finally, we resize the image to a dimension of 84x84. We can choose a different number, other than 84, as long as it contains a reasonable amount of pixels. However, it is efficient to have a square matrix (like 84x84 or 80x80) as the convolution operations (for example, with CUDA) are optimized for such square input:</p>
<pre class="calibre17">def process_frame_84(frame, conf):<br class="title-page-name"/>    frame = frame[conf["crop1"]:conf["crop2"] + 160, :160]<br class="title-page-name"/>    frame = frame.mean(2)<br class="title-page-name"/>    frame = frame.astype(np.float32)<br class="title-page-name"/>    frame *= (1.0 / 255.0)<br class="title-page-name"/>    frame = cv2.resize(frame, (84, conf["dimension2"]))<br class="title-page-name"/>    frame = cv2.resize(frame, (84, 84))<br class="title-page-name"/>    frame = np.reshape(frame, [1, 84, 84])<br class="title-page-name"/>    return frame<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class AtariRescale(gym.ObservationWrapper):<br class="title-page-name"/>    def __init__(self, env, env_conf):<br class="title-page-name"/>        gym.ObservationWrapper.__init__(self, env)<br class="title-page-name"/>        self.observation_space = Box(0.0, 1.0, [1, 84, 84])<br class="title-page-name"/>        self.conf = env_conf<br class="title-page-name"/><br class="title-page-name"/>    def observation(self, observation):<br class="title-page-name"/>        return process_frame_84(observation, self.conf)</pre>
<div class="packt_infobox">Note that with a resolution of 84x84 pixels for one observation frame with a data type of <kbd class="calibre28">numpy.float32</kbd> which takes 4 bytes, we need about 4x84x84 = 28,224 bytes. As you may recall from the <em class="calibre46">Experience memory</em> section, one experience object contains two frames (one for the observation and the other for the next observation), which means we'll need 2x 28,224 = 56,448 bytes (+ 2 bytes for <em class="calibre46">action</em> + 4 bytes for <em class="calibre46">reward). </em>The 56,448 bytes (or 0.056448 MB) may not seem much, but if you consider the fact that it is typical to be using an experience memory capacity in the order of 1e6 (million), you may realize that we need about 1e6 x 0.056448 MB = 56,448 MB or 56.448 GB! This means that we will need 56.448 GB of RAM just for the experience memory with a capacity of 1 million experiences!</div>
<div class="packt_tip">You can do a couple of memory optimizations to reduce the required RAM for training the agent. Using a smaller experience memory is a straightforward way to reduce the memory footprint in some games. In some environments, having a larger experience memory will help the agent to learn faster. One way to reduce the memory footprint it by not scaling the frames (by dividing by 255) while storing, which requires a floating point representation (<kbd class="calibre28">numpy.float32</kbd>) and rather storing the frames as numpy.uint8 so that we only need 1 byte instead of 4 bytes per pixels, which will help in reducing the memory requirement by a factor of 4. Then, when we want to use the stored experiences in our forward pass to the network to the deep Q-network to get the Q-value predictions, we can scale the images to be in the range 0.0 to 1.0.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Normalizing observations</h1>
                
            
            <article>
                
<p class="calibre2">In some cases, normalizing the observations can help with convergence speed. The most commonly used normalization process involves two steps:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Zero-centering using mean subtraction</li>
<li value="2" class="calibre11">Scaling using the standard deviation</li>
</ol>
<p class="calibre2">In essence, the following is the normalization process:</p>
<pre class="calibre17">(x - numpy.mean(x)) / numpy.std(x)</pre>
<p class="calibre2">In the previous process, x is the observation. Note that other normalization processes are also used, depending on the range of the normalized value that is desired. For example, if we wanted the values after the normalization to lie between 0 and 1, we could use the following:</p>
<pre class="calibre17">(x - numpy.min(x)) / (numpy.max(x) - numpy.min(x))</pre>
<p class="calibre2">In the previous process, instead of subtracting the mean, we subtract the minimum value and divide by the difference between the maximum and the minimum value. This way, the minimum value in the observation/x gets normalized to 0 and the maximum value gets normalized to a value of 1. </p>
<p class="calibre2">Alternatively, if we wanted the values after the normalization to lie between -1 and +1, then the following can be used:</p>
<pre class="calibre17">2 * (x - numpy.min(x)) / (numpy.max(x) - numpy.min(x)) - 1</pre>
<p class="calibre2">In our environment normalization wrapper implementation, we will use the first method where we zero-center the observation data using mean subtraction and scale using the standard deviation of the data in the observation. In fact, we will go one step further and calculate the running mean and standard deviation of all the observations we have received so far to normalize the observations based on the distribution of the observation data the agent has observed so far. This is more appropriate as there can be high variance between different observations from the same environment. The following is the implementation code for the normalization wrapper that we discussed:</p>
<pre class="calibre17">class NormalizedEnv(gym.ObservationWrapper):<br class="title-page-name"/>    def __init__(self, env=None):<br class="title-page-name"/>        gym.ObservationWrapper.__init__(self, env)<br class="title-page-name"/>        self.state_mean = 0<br class="title-page-name"/>        self.state_std = 0<br class="title-page-name"/>        self.alpha = 0.9999<br class="title-page-name"/>        self.num_steps = 0<br class="title-page-name"/><br class="title-page-name"/>    def observation(self, observation):<br class="title-page-name"/>        self.num_steps += 1<br class="title-page-name"/>        self.state_mean = self.state_mean * self.alpha + \<br class="title-page-name"/>            observation.mean() * (1 - self.alpha)<br class="title-page-name"/>        self.state_std = self.state_std * self.alpha + \<br class="title-page-name"/>            observation.std() * (1 - self.alpha)<br class="title-page-name"/><br class="title-page-name"/>        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))<br class="title-page-name"/>        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))<br class="title-page-name"/><br class="title-page-name"/>        return (observation - unbiased_mean) / (unbiased_std + 1e-8)</pre>
<div class="packt_infobox">The image frames that we get as observation from the environment (even after our preprocessing wrapper) is already on the same scale (0-255 or 0.0 to 1.0). The scaling step in the normalization procedure may not be very helpful in this case. This wrapper in general could be useful for other environment types and was also not observed to be detrimental to the performance for already scaled image observations from Gym environments like Atari.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Random no-ops on reset</h1>
                
            
            <article>
                
<p class="calibre2">When the environment is reset, the agent usually starts from the same initial state and therefore receives the same observation on reset. The agent may memorize or get used to the starting state in one game level so much that they might start performing poorly they start in a slightly different position or game level. Sometimes, it was found to be helpful to randomize the initial state, such as sampling different initial states from which the agent starts the episode. To make that happen, we can add a Gym wrapper that performs a random number of "no-ops" before sending out the first observation after the reset. The Arcade Learning Environment for the Atari 2600 that the Gym library uses for the Atari environment supports a "NOOP" or no-operation action, which in the Gym library is coded as an action with a value of 0. So, we will step the environment with a random number of <em class="calibre13">action</em>=0<em class="calibre13"> </em>before returning the observation to the agent, as shown in the following code snippet:</p>
<pre class="calibre17">class NoopResetEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env, noop_max=30):<br class="title-page-name"/>        """Sample initial states by taking random number of no-ops on reset.<br class="title-page-name"/>        No-op is assumed to be action 0.<br class="title-page-name"/>        """<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        self.noop_max = noop_max<br class="title-page-name"/>        self.noop_action = 0<br class="title-page-name"/>        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """ Do no-op action for a number of steps in [1, noop_max]."""<br class="title-page-name"/>        self.env.reset()<br class="title-page-name"/>        noops = random.randrange(1, self.noop_max + 1) # pylint: disable=E1101<br class="title-page-name"/>        assert noops &gt; 0<br class="title-page-name"/>        obs = None<br class="title-page-name"/>        for _ in range(noops):<br class="title-page-name"/>            obs, _, done, _ = self.env.step(self.noop_action)<br class="title-page-name"/>        return obs<br class="title-page-name"/><br class="title-page-name"/>    def step(self, ac):<br class="title-page-name"/>        return self.env.step(ac)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Fire on reset</h1>
                
            
            <article>
                
<p class="calibre2">Some Atari games require the player to press the <span class="calibre5">Fire</span> button to start the game. Some games require the <span class="calibre5">Fire</span> button to be pressed after every life is lost. More often that not, this is the only use for the <span class="calibre5">Fire</span> button! Although it might look trivial for us to realize that, it may be difficult for the reinforcement learning agents to figure that out on their own sometimes. It is not the case that they are incapable of learning that. In fact, they are capable of figuring out lots of hidden glitches or modes in the game that no human has ever figured out! For example, in the game of Qbert, an agent trained using Evolutionary Strategies (which is a black-box type learning strategy inspired by genetic algorithms) figured out a peculiar way with which it can keep receiving scores and never let the game end! You know how much the agent was able to score? ~1,000,000! They could only get that much because the game was reset artificially due to a time limit. Can you try scoring that much in the game of Qbert? You can see that agent scoring in action here: <a href="https://youtu.be/meE5aaRJ0Zs" class="calibre9">https://www.youtube.com/watch?v=meE5aaRJ0Zs</a>.</p>
<p class="calibre2">The point is not that the agents are so smart to figure all these things out. They definitely can, but most of the time, this harms the progress the agent can make in a reasonable amount of time. This is especially true when we want a single agent to tackle several different varieties of games (one at a time). We are better off starting with simpler assumptions and making them more complicated after we have been able to train the agents to play well using the simpler assumptions.</p>
<p class="calibre2">Therefore, we will implement a <kbd class="calibre12">FireResetEnv</kbd> Gym wrapper that will press the Fire button on every reset and get the environment started for the agent. The code's implementation is as follows:</p>
<pre class="calibre17">class FireResetEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        """Take action on reset for environments that are fixed until firing."""<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'<br class="title-page-name"/>        assert len(env.unwrapped.get_action_meanings()) &gt;= 3<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        self.env.reset()<br class="title-page-name"/>        obs, _, done, _ = self.env.step(1)<br class="title-page-name"/>        if done:<br class="title-page-name"/>            self.env.reset()<br class="title-page-name"/>        obs, _, done, _ = self.env.step(2)<br class="title-page-name"/>        if done:<br class="title-page-name"/>            self.env.reset()<br class="title-page-name"/>        return obs<br class="title-page-name"/><br class="title-page-name"/>    def step(self, ac):<br class="title-page-name"/>        return self.env.step(ac)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Episodic life</h1>
                
            
            <article>
                
<p class="calibre2">In many games, including Atari games, the player gets to play with more than one life. </p>
<p class="calibre2">It was observed, used, and reported by Deepmind that terminating an episode when a life is lost helps the agent learn better. It has to be noted that the intention is to signify to the agent that losing a life is a bad thing to do. In this case, when the episode terminates, we will not reset the environment and rather continue until the game is actually over, after which we reset the environment. If we reset the game after every loss of life, we would be limiting the agent's exposure to observations and experiences that can be collected with just one life, which is usually bad for the agent's learning performance. </p>
<p class="calibre2">To implement what we just discussed, we will use the <kbd class="calibre12">EpisodicLifeEnv</kbd> class that marks the end of an episode when a life is lost, and reset the environment when the game is over, as shown in the following code snippet:</p>
<pre class="calibre17">class EpisodicLifeEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env):<br class="title-page-name"/>        """Make end-of-life == end-of-episode, but only reset on true game over.<br class="title-page-name"/>        Done by DeepMind for the DQN and co. since it helps value estimation.<br class="title-page-name"/>        """<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        self.lives = 0<br class="title-page-name"/>        self.was_real_done = True<br class="title-page-name"/><br class="title-page-name"/>    def step(self, action):<br class="title-page-name"/>        obs, reward, done, info = self.env.step(action)<br class="title-page-name"/>        self.was_real_done = True<br class="title-page-name"/>        # check current lives, make loss of life terminal,<br class="title-page-name"/>        # then update lives to handle bonus lives<br class="title-page-name"/>        lives = info['ale.lives']<br class="title-page-name"/>        if lives &lt; self.lives and lives &gt; 0:<br class="title-page-name"/>            # for Qbert sometimes we stay in lives == 0 condition for a few frames<br class="title-page-name"/>            # so its important to keep lives &gt; 0, so that we only reset once<br class="title-page-name"/>            # the environment advertises done.<br class="title-page-name"/>            done = True<br class="title-page-name"/>            self.was_real_done = False<br class="title-page-name"/>        self.lives = lives<br class="title-page-name"/>        return obs, reward, done, info<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """Reset only when lives are exhausted.<br class="title-page-name"/>        This way all states are still reachable even though lives are episodic,<br class="title-page-name"/>        and the learner need not know about any of this behind-the-scenes.<br class="title-page-name"/>        """<br class="title-page-name"/>        if self.was_real_done:<br class="title-page-name"/>            obs = self.env.reset()<br class="title-page-name"/>            self.lives = 0<br class="title-page-name"/>        else:<br class="title-page-name"/>            # no-op step to advance from terminal/lost life state<br class="title-page-name"/>            obs, _, _, info = self.env.step(0)<br class="title-page-name"/>            self.lives = info['ale.lives']<br class="title-page-name"/>        return obs</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Max and skip-frame</h1>
                
            
            <article>
                
<p class="calibre2">The <kbd class="calibre12">Gym</kbd> library provides environments that have <kbd class="calibre12">NoFrameskip</kbd> in their ID, which we discussed in <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 4</a>, <em class="calibre13">Exploring the Gym and its Features</em>, where we discussed the nomenclature of the Gym environments. As you may recall from our discussion in <a href="part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 4</a>, <em class="calibre13">Exploring the Gym and its Features</em>, b<span class="calibre5">y default, if there is no presence o</span>f <kbd class="calibre12">Deterministic</kbd> or <kbd class="calibre12">NoFrameskip</kbd> in t<span class="calibre5">he environment name,</span><span class="calibre5"> the action sent to the environment is repeatedly performed for a duration of </span><em class="calibre13">n </em><span class="calibre5">frames, where </span><em class="calibre13">n</em><span class="calibre5"> is uniformly sampled from (2, 3, 4).</span><span class="calibre5"> If we want to step through the environment at a specific rate, we can use the Gym</span> Atari environments with <kbd class="calibre12">NoFrameskip</kbd> in their ID, which will step through the underlying environment without any alteration to the step duration. The step rate, in this case, is of<img class="fm-editor-equation74" src="../images/00147.jpeg"/> a second, which is 60 frames per second. We can then customize the environment to skip at our choice to skip the rate (<em class="calibre13">k</em>) to step at a specific rate. The implementation for such a custom step/skip rate is as follows:</p>
<pre class="calibre17">class MaxAndSkipEnv(gym.Wrapper):<br class="title-page-name"/>    def __init__(self, env=None, skip=4):<br class="title-page-name"/>        """Return only every `skip`-th frame"""<br class="title-page-name"/>        gym.Wrapper.__init__(self, env)<br class="title-page-name"/>        # most recent raw observations (for max pooling across time steps)<br class="title-page-name"/>        self._obs_buffer = deque(maxlen=2)<br class="title-page-name"/>        self._skip = skip<br class="title-page-name"/><br class="title-page-name"/>    def step(self, action):<br class="title-page-name"/>        total_reward = 0.0<br class="title-page-name"/>        done = None<br class="title-page-name"/>        for _ in range(self._skip):<br class="title-page-name"/>            obs, reward, done, info = self.env.step(action)<br class="title-page-name"/>            self._obs_buffer.append(obs)<br class="title-page-name"/>            total_reward += reward<br class="title-page-name"/>            if done:<br class="title-page-name"/>                break<br class="title-page-name"/><br class="title-page-name"/>        max_frame = np.max(np.stack(self._obs_buffer), axis=0)<br class="title-page-name"/>        return max_frame, total_reward, done, info<br class="title-page-name"/><br class="title-page-name"/>    def reset(self):<br class="title-page-name"/>        """Clear past frame buffer and init. to first obs. from inner env."""<br class="title-page-name"/>        self._obs_buffer.clear()<br class="title-page-name"/>        obs = self.env.reset()<br class="title-page-name"/>        self._obs_buffer.append(obs)<br class="title-page-name"/>        return obs</pre>
<p class="calibre2">Notice that we are also taking the maximum of the pixel values over the frames that were skipped and sending that as the observation instead of totally ignoring all the intermediate image frames that were skipped. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Wrapping the Gym environment</h1>
                
            
            <article>
                
<p class="calibre2">Finally, we will apply the preceding wrappers that we developed based on the environment configuration we specify using the <kbd class="calibre12">parameters.JSON</kbd> file:</p>
<pre class="calibre17">def make_env(env_id, env_conf):<br class="title-page-name"/>    env = gym.make(env_id)<br class="title-page-name"/>    if 'NoFrameskip' in env_id:<br class="title-page-name"/>        assert 'NoFrameskip' in env.spec.id<br class="title-page-name"/>        env = NoopResetEnv(env, noop_max=30)<br class="title-page-name"/>        env = MaxAndSkipEnv(env, skip=env_conf['skip_rate'])<br class="title-page-name"/><br class="title-page-name"/>    if env_conf['episodic_life']:<br class="title-page-name"/>        env = EpisodicLifeEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    if 'FIRE' in env.unwrapped.get_action_meanings():<br class="title-page-name"/>        env = FireResetEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    env = AtariRescale(env, env_conf['useful_region'])<br class="title-page-name"/>    env = NormalizedEnv(env)<br class="title-page-name"/><br class="title-page-name"/>    if env_conf['clip_reward']:<br class="title-page-name"/>        env = ClipRewardEnv(env)<br class="title-page-name"/>    return env<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">All of the environment wrappers that we discussed previously are implemented and available in the <kbd class="calibre12">ch6/environment/atari.py</kbd> in this book's code repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training the deep Q-learner to play Atari games</h1>
                
            
            <article>
                
<p class="calibre2">We have gone through several new techniques in this chapter. You deserve a pat on your back for making it this far! Now starts the fun part where you can let your agents train by themselves to play several Atari games and see how they are progressing. What is great about our deep Q-learner is the fact that we can use the same agent to train and play any of the Atari games!</p>
<p class="calibre2">By the end of this section, you should be able to use our deep Q learning agent to observe the pixels on the screen and take actions by sending the joystick commands to the Atari Gym environment, just like what is shown in the following screenshot:</p>
<p class="calibre2"/>
<div class="cdpaligncenter"><img src="../images/00148.jpeg" class="calibre74"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Putting together a comprehensive deep Q-learner</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">It is time to combine all the techniques we have discussed into a comprehensive implementation that makes use of all of those techniques to get maximum performance. </span>We will use the <kbd class="calibre12">environment.atari</kbd> module that we created in the previous section with several useful Gym environment wrappers. Let's <span class="calibre5">look at the code outline to understand the code's structure:</span></p>
<div class="packt_infobox"><span class="packt_screen">You will notice that some sections of the code are removed for brevity and replaced with<kbd class="calibre28">...</kbd>, signifying that the code in that section has been folded/hidden. You</span><span class="packt_screen"> can find the latest</span><span class="packt_screen"> version of the complete code in this book's code repository at </span><kbd class="calibre28">ch6/deep_Q_Learner.py</kbd>.</div>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>import gym<br class="title-page-name"/>import torch<br class="title-page-name"/>import random<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>import environment.atari as Atari<br class="title-page-name"/>import environment.utils as env_utils<br class="title-page-name"/>from utils.params_manager import ParamsManager<br class="title-page-name"/>from utils.decay_schedule import LinearDecaySchedule<br class="title-page-name"/>from utils.experience_memory import Experience, ExperienceMemory<br class="title-page-name"/>from function_approximator.perceptron import SLP<br class="title-page-name"/>from function_approximator.cnn import CNN<br class="title-page-name"/>from tensorboardX import SummaryWriter<br class="title-page-name"/>from datetime import datetime<br class="title-page-name"/>from argparse import ArgumentParser<br class="title-page-name"/><br class="title-page-name"/>args = ArgumentParser("deep_Q_learner")<br class="title-page-name"/>args.add_argument("--params-file", help="Path to the parameters json file. Default is parameters.json",<br class="title-page-name"/>                 default="parameters.json", metavar="PFILE")<br class="title-page-name"/>args.add_argument("--env-name", help="ID of the Atari environment available in OpenAI Gym. Default is Seaquest-v0",<br class="title-page-name"/>                  default="Seaquest-v0", metavar="ENV")<br class="title-page-name"/>args.add_argument("--gpu-id", help="GPU device ID to use. Default=0", default=0, type=int, metavar="GPU_ID")<br class="title-page-name"/>args.add_argument("--render", help="Render environment to Screen. Off by default", action="store_true", default=False)<br class="title-page-name"/>args.add_argument("--test", help="Test mode. Used for playing without learning. Off by default", action="store_true",<br class="title-page-name"/>                  default=False)<br class="title-page-name"/>args = args.parse_args()<br class="title-page-name"/><br class="title-page-name"/>params_manager= ParamsManager(args.params_file)<br class="title-page-name"/>seed = params_manager.get_agent_params()['seed']<br class="title-page-name"/>summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']<br class="title-page-name"/>summary_file_path= summary_file_path_prefix + args.env_name + "_" + datetime.now().strftime("%y-%m-%d-%H-%M")<br class="title-page-name"/>writer = SummaryWriter(summary_file_path)<br class="title-page-name"/># Export the parameters as json files to the log directory to keep track of the parameters used in each experiment<br class="title-page-name"/>params_manager.export_env_params(summary_file_path + "/" + "env_params.json")<br class="title-page-name"/>params_manager.export_agent_params(summary_file_path + "/" + "agent_params.json")<br class="title-page-name"/>global_step_num = 0<br class="title-page-name"/>use_cuda = params_manager.get_agent_params()['use_cuda']<br class="title-page-name"/># new in PyTorch 0.4<br class="title-page-name"/>device = torch.device("cuda:" + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else "cpu")<br class="title-page-name"/>torch.manual_seed(seed)<br class="title-page-name"/>np.random.seed(seed)<br class="title-page-name"/>if torch.cuda.is_available() and use_cuda:<br class="title-page-name"/>    torch.cuda.manual_seed_all(seed)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>class Deep_Q_Learner(object):<br class="title-page-name"/>    def __init__(self, state_shape, action_shape, params):<br class="title-page-name"/>        ...<br class="title-page-name"/>        <br class="title-page-name"/>    def get_action(self, observation):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def epsilon_greedy_Q(self, observation):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def learn(self, s, a, r, s_next, done):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def learn_from_batch_experience(self, experiences):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def replay_experience(self, batch_size = None):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/>    def load(self, env_name):<br class="title-page-name"/>        ...<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env_conf = params_manager.get_env_params()<br class="title-page-name"/>    env_conf["env_name"] = args.env_name<br class="title-page-name"/>    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default<br class="title-page-name"/>    ...<br class="title-page-name"/>    # If a saved (pre-trained) agent's brain model is available load it as per the configuration<br class="title-page-name"/>    if agent_params['load_trained_model']:<br class="title-page-name"/>    ...<br class="title-page-name"/><br class="title-page-name"/>    # Start the training process<br class="title-page-name"/>    episode = 0<br class="title-page-name"/>    while global_step_num &lt;= agent_params['max_training_steps']:<br class="title-page-name"/>        obs = env.reset()<br class="title-page-name"/>        cum_reward = 0.0 # Cumulative reward<br class="title-page-name"/>        done = False<br class="title-page-name"/>        step = 0<br class="title-page-name"/>        #for step in range(agent_params['max_steps_per_episode']):<br class="title-page-name"/>        while not done:<br class="title-page-name"/>            if env_conf['render'] or args.render:<br class="title-page-name"/>                env.render()<br class="title-page-name"/>            action = agent.get_action(obs)<br class="title-page-name"/>            next_obs, reward, done, info = env.step(action)<br class="title-page-name"/>            #agent.learn(obs, action, reward, next_obs, done)<br class="title-page-name"/>            agent.memory.store(Experience(obs, action, reward, next_obs, done))<br class="title-page-name"/><br class="title-page-name"/>            obs = next_obs<br class="title-page-name"/>            cum_reward += reward<br class="title-page-name"/>            step += 1<br class="title-page-name"/>            global_step_num +=1<br class="title-page-name"/><br class="title-page-name"/>            if done is True:<br class="title-page-name"/>                episode += 1<br class="title-page-name"/>                episode_rewards.append(cum_reward)<br class="title-page-name"/>                if cum_reward &gt; agent.best_reward:<br class="title-page-name"/>                    agent.best_reward = cum_reward<br class="title-page-name"/>                if np.mean(episode_rewards) &gt; prev_checkpoint_mean_ep_rew:<br class="title-page-name"/>                    num_improved_episodes_before_checkpoint += 1<br class="title-page-name"/>                if num_improved_episodes_before_checkpoint &gt;= agent_params["save_freq_when_perf_improves"]:<br class="title-page-name"/>                    prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)<br class="title-page-name"/>                    agent.best_mean_reward = np.mean(episode_rewards)<br class="title-page-name"/>                    agent.save(env_conf['env_name'])<br class="title-page-name"/>                    num_improved_episodes_before_checkpoint = 0<br class="title-page-name"/>                print("\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}".<br class="title-page-name"/>                      format(episode, step+1, cum_reward, np.mean(episode_rewards), agent.best_reward))<br class="title-page-name"/>                writer.add_scalar("main/ep_reward", cum_reward, global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/mean_ep_reward", np.mean(episode_rewards), global_step_num)<br class="title-page-name"/>                writer.add_scalar("main/max_ep_rew", agent.best_reward, global_step_num)<br class="title-page-name"/>                # Learn from batches of experience once a certain amount of xp is available unless in test only mode<br class="title-page-name"/>                if agent.memory.get_size() &gt;= 2 * agent_params['replay_start_size'] and not args.test:<br class="title-page-name"/>                    agent.replay_experience()<br class="title-page-name"/><br class="title-page-name"/>                break<br class="title-page-name"/>    env.close()<br class="title-page-name"/>    writer.close()</pre>
<p class="calibre2"> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hyperparameters</h1>
                
            
            <article>
                
<p class="calibre2">The following is a list of hyperparameters that our deep Q-learner uses, with a brief description of what they are and the types of values they take:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">Hyperparameter</strong></td>
<td class="calibre48"><strong class="calibre1">Brief description</strong></td>
<td class="calibre48"><strong class="calibre1">Value type</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">max_num_episodes</kbd></td>
<td class="calibre48">Maximum number of episodes to run the agent.</td>
<td class="calibre48">Integer (for example, 100,000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">replay_memory_capacity</kbd></td>
<td class="calibre48">Total capacity of the experience memory capacity.</td>
<td class="calibre48">Integer or exponential notation (for example, 1e6)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">replay_batch_size</kbd></td>
<td class="calibre48">Number of transitions used in a (mini) batch to update the Q-function in each update iteration during experience replay.</td>
<td class="calibre48">Integer (for example, 2,000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">use_target_network</kbd></td>
<td class="calibre48">Whether a target Q-network is to be used or not.</td>
<td class="calibre48">Boolean (true/false)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">target_network_update_freq</kbd></td>
<td class="calibre48">The number of steps after which the target Q-network is updated using the main Q-network.</td>
<td class="calibre48">Integer (for example, 1,000)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">lr</kbd></td>
<td class="calibre48">The learning rate for the deep Q-network.</td>
<td class="calibre48">float (for example, 1e-4)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">gamma</kbd></td>
<td class="calibre48">The discount factor for the MDP.</td>
<td class="calibre48">
<p class="calibre2">float (for example, 0.98)</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">epsilon_max</kbd></td>
<td class="calibre48">The maximum value of the epsilon from which the decay starts.</td>
<td class="calibre48">float (for example, 1.0)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">epsilon_min</kbd></td>
<td class="calibre48">The minimum value for epsilon to which the decay will finally settle to.</td>
<td class="calibre48">float (for example, :0.05)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">seed</kbd></td>
<td class="calibre48">The seed used to seed numpy and torch (and <kbd class="calibre12">torch.cuda</kbd>) to be able to reproduce (to some extent) the randomness introduced by those libraries.</td>
<td class="calibre48">Integer (for example, :555)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">use_cuda</kbd></td>
<td class="calibre48">Whether or not to use CUDA based GPU if a GPU is available.</td>
<td class="calibre48">Boolean (for example, : true)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">load_trained_model</kbd></td>
<td class="calibre48">Whether or not to load a trained model if one exists for this environment/problem. If <span>this parameter is set to true but </span>no trained model is available, the model will be trained from scratch.</td>
<td class="calibre48">Boolean (for example, : true)</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">load_dir</kbd></td>
<td class="calibre48">The path to the directory (including the forward slash) from where the trained model should be loaded from to resume training.</td>
<td class="calibre48">String (for example, : "trained_models/")</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">save_dir</kbd></td>
<td class="calibre48">The path to a directory where the models should be saved. New models are saved every time the agent achieves a new best score/reward.</td>
<td class="calibre48">string (for example, : trained_models/")</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">Please refer to the <kbd class="calibre12">ch6/parameters.JSON</kbd> file in this book's code repository for the updated list of parameters used by the agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Launching the training process</h1>
                
            
            <article>
                
<p class="calibre2">We have now put together all the pieces for the deep Q-learner and are ready to train the agent! Be sure to check out/pull/download the latest code from this book's code repository.</p>
<p class="calibre2">You can pick any environment from the list of Atari environments and train the agent we developed using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "ENV_ID"</strong></pre>
<p class="calibre2">In the previous command, <kbd class="calibre12">ENV_ID</kbd> is the name/ID of the Atari Gym environment. For example, if you want to train the agent on the <kbd class="calibre12">pong</kbd> environment with no frame skip, you would run the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "PongNoFrameskip-v4"</strong></pre>
<p class="calibre2">By default, the training logs will be saved to <kbd class="calibre12">./logs/DQL_{ENV}_{T}</kbd>, where <kbd class="calibre12">{ENV}</kbd> is the name of the environment and <span class="calibre5"><span class="calibre5"><kbd class="calibre12">{T}</kbd></span></span> is the time stamp obtained when you run the agent. If you start a TensorBoard instance using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ tensorboard --logdir=logs/</strong></pre>
<p class="calibre2">By default, our <kbd class="calibre12">deep_Q_learner.py</kbd> script will use the <kbd class="calibre12">parameters.JSON</kbd> file located in the same directory as the script for reading the configurable parameter values. You can override with a different parameter configuration file using the command-line <kbd class="calibre12">--params-file</kbd> argument.</p>
<p class="calibre2">If the <kbd class="calibre12">load_trained_model</kbd> parameter is set to <kbd class="calibre12">true</kbd> in the <kbd class="calibre12">parameters.JSON</kbd> file and if a saved model for the chosen environment is available, our script will try to initialize the agent with the model that it learned previously so that it can resume from where it left off rather than train from scratch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Testing performance of your deep Q-learner in Atari games</h1>
                
            
            <article>
                
<p class="calibre2">It feels great, doesn't it? You have now developed an agent that can learn to play any Atari game and get better at it by itself! Once you have your agent trained on any Atari game, you can use the test mode of the script to test the agent's performance based on its learning so far. You can enable the test mode by using the <kbd class="calibre12">--test</kbd> argument in the <kbd class="calibre12">deep_q_learner.py</kbd> script. It is useful to enable rendering of the environment, too, so that you can visually see (apart from the rewards printed on the console) how the agent is performing. As an example, you can test the agent in the <kbd class="calibre12">Seaquest</kbd> Atari game using the following command:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env "Seaquest-v0" --test --render</strong></pre>
<p class="calibre2">You will see the Seaquest game window come up and the agent showing of its skills!</p>
<p class="calibre2">A couple of points to note regarding the <kbd class="calibre12">test</kbd> mode are the following:</p>
<ul class="calibre10">
<li class="calibre11">The test mode will turn off the agent's learning routine. Therefore, the agent will not learn or update itself in the test mode. This mode is only used to test how a trained agent is performing. If you want to see how the agent is performing while it is learning, you can just use the <kbd class="calibre12">--render</kbd> option without the <kbd class="calibre12">--test</kbd> option.</li>
<li class="calibre11">The test mode assumes that a trained model for the environment you choose exists in the <kbd class="calibre12">trained_models</kbd> folder. Otherwise, a newborn agent, without any prior knowledge, will start playing the game from scratch. Also, since learning is disabled, you will not see the agent improving!</li>
</ul>
<p class="calibre2">Now, it's your turn to go out, experiment, review, and compare the performance of the agent we implemented in different Atari Gym environments and see how much the agent can score! If you train an agent to play well in a game, you can show and share it to other fellow readers by opening a pull request on this book's code repository from your fork. You will be featured on the page!</p>
<p class="calibre2">Once you get comfortable using the code base we developed, you can do several experiments with it. For example, you can turn off the target Q-network or increase/decrease the experience memory/replay batch size by simply changing the <kbd class="calibre12">parameters.JSON</kbd> file and comparing the performance using the very convenient TensorBoard dashboard. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">We started this chapter with the grand goal of developing intelligent learning agents that can achieve great scores in Atari games. We made incremental progress towards it by implementing several techniques to improve upon the Q-learner that we developed in the previous chapter. We first started with learning how we can use a neural network to approximate the Q action-value function and made our learning concrete by practically implementing a shallow neural network to solve the famous Cart Pole problem. We then implemented experience memory and experience replay that enables the agent to learn from (mini) randomly sampled batches of experiences that helped in improving the performance by breaking the correlations between the agent's interactions and increasing the sample efficiency with the batch replay of the agent's prior experience. We then revisited the epsilon-greedy action selection policy and implemented a decay schedule to decrease the exploration based on a schedule to let the agent rely more on its learning.</p>
<p class="calibre2">We then looked at how to use TensorBoard's logging and visualization capabilities with our PyTorch-based learning agent so that we can watch the agent's training progress in a simple and intuitive way. We also implemented a neat little parameter manager class that enabled us to configure the hyperparameters of the agent and other configuration parameters using an external easy-to-read JSON file.</p>
<p class="calibre2">After we got a good baseline and the helpful utility tools implemented, we started our implementation of the deep Q-learner. We started that section by implementing a deep convolutional neural network in PyTorch which we then used to represent our agent's Q (action-value) function. We then saw how easy it was to implement the idea of using a target Q-network which is known to stabilize the agent's Q learning process. We then put together our deep Q learning-based agent that can learn to act based on just the raw pixel observations from a Gym environment.</p>
<p class="calibre2">We then laid our eyes and hands on the Atari Gym environments and looked at several ways to customize the Gym environments using Gym environment wrappers. We also discussed several useful wrappers for the Atari environment and specifically implemented wrappers to clip the reward, preprocess the observation image frames, normalize the observations over all the entire sampled observation distribution, send random noop actions on reset to sample different start states, press the Fire button on resets, and to step at a custom rate by frame skipping. We finally saw how we can consolidate this all together into a comprehensive agent training code base and train the agent on any Atari game and see the progress summary on TensorBoard. We also looked at how we could save the state and resume the training of the agent from a previous saved state instead of rerunning the training from scratch. Towards the end, we saw the improving performance of the agent we implemented and trained.</p>
<p class="calibre2">We hope that you had a lot of fun throughout this whole chapter. We will be looking at and implementing a different algorithm in the next chapter, which can be used for taking much more complex actions rather than a discrete set of button presses and how we can use it to train an agent to autonomously control a car in simulation!</p>


            </article>

            
        </section>
    </body></html>