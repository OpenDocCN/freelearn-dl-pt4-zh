<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-3-deep-learning-with-pytorch">
<h1 class="chapterNumber">3</h1>
<h1 class="chapterTitle" id="sigil_toc_id_406">
<span id="x1-530003"/>Deep Learning with PyTorch
    </h1>
<p>In the previous chapter, you became familiar with open source libraries, which provided you with a collection of <span class="cmbx-10x-x-109">reinforcement learning </span>(<span class="cmbx-10x-x-109">RL</span>) environments. However, recent developments in RL, and especially its combination with <span class="cmbx-10x-x-109">deep</span> <span class="cmbx-10x-x-109">learning </span>(<span class="cmbx-10x-x-109">DL</span>), now make it possible to solve much more challenging problems than ever before. This is partly due to the development of DL methods and tools. This chapter is dedicated to one such tool, PyTorch, which enables us to implement complex DL models with just a bunch of lines of Python code.</p>
<p>The chapter doesn’t pretend to be a complete DL manual, as the field is very wide and dynamic; however, we will cover:</p>
<ul>
<li>
<p>The PyTorch library specifics and implementation details (assuming that you are already familiar with DL fundamentals)</p>
</li>
<li>
<p>Higher-level libraries on top of PyTorch, with the aim of simplifying common DL problems</p>
</li>
<li>
<p>The PyTorch Ignite library, which will be used in some examples</p>
</li>
</ul>
<div class="tcolorbox infobox" id="tcolobox-25">
<div class="tcolorbox-content">
<p>All of the examples in this chapter were updated for the latest(at the time of writing) PyTorch 2.3.1, which has changes in comparison to version 1.3.0, which was used in the second edition of this book. If you are using the old PyTorch, consider upgrading. Throughout this chapter, we will discuss the differences that are present in the latest version.</p>
</div>
</div>
<section class="level3 sectionHead" id="tensors">
<h1 class="heading-1" id="sigil_toc_id_46"> <span id="x1-540003.1"/>Tensors</h1>
<p>A <span class="cmbx-10x-x-109">tensor </span>is the<span id="dx1-54001"/> fundamental building block of all DL toolkits. The name sounds rather mystical, but the underlying idea is that a tensor is just a multi-dimensional array. Using the analogy of school math, one single number is like a point, which is zero-dimensional, while a vector is one-dimensional like a line segment, and a matrix is a two-dimensional object. Three-dimensional number collections can be represented by a cuboid of numbers, but they don’t have a separate name in the same way as a <span class="cmti-10x-x-109">matrix</span>. We can keep the term “tensor” for collections of higher dimensions.</p>
<div class="minipage">
<p><img alt="19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx aaa i,ij,,jk,k,...ii,j " height="300" src="../Images/B22150_03_01.png" width="600"/> <span id="x1-54002r1"/></p>
<span class="id">Figure 3.1: Going from a single number to an n-dimensional tensor </span>
</div>
<p>Another thing to note about tensors used in DL is that they are only partially related to tensors used in <span class="cmti-10x-x-109">tensor calculus </span>or <span class="cmti-10x-x-109">tensor algebra</span>. In DL, a tensor is any multi-dimensional array, but in mathematics, a tensor is a mapping between vector spaces, which might be represented as a multi-dimensional array in some cases, but <span id="dx1-54003"/>has much more semantical payload behind it. Mathematicians usually frown at anybody who uses well-established mathematical terms to name different things, so be warned!</p>
</section>
<section class="level3 sectionHead" id="the-creation-of-tensors">
<h1 class="heading-1" id="sigil_toc_id_47"> <span id="x1-550003.2"/>The creation of tensors</h1>
<p>As we’ll deal with <span id="dx1-55001"/>tensors everywhere in this book, we need to be familiar with basic operations on them, and the most basic is how to create one. There are several ways to do this, and your choice might influence code readability and performance.</p>
<p>If you are familiar with the NumPy library (and you should be), then you already know that its central purpose is the handling of multi-dimensional arrays in a generic way. Even though in NumPy, such arrays aren’t called tensors, they are, in fact, tensors. Tensors are used very widely in scientific computations as generic storage for data. For example, a color image could be encoded as a 3D tensor with the dimensions of width, height, and color plane. Apart from dimensions, a tensor is characterized by the type of its elements. There are 13 types supported by PyTorch:</p>
<ul>
<li>
<p>Four float types: 16-bit, 32-bit, and 64-bit. 16-bit float has two variants: <span class="cmtt-10x-x-109">float16 </span>has more bits for precision while <span class="cmtt-10x-x-109">bfloat16 </span>has larger exponent part</p>
</li>
<li>
<p>Three complex types: 32-bit, 64-bit, and 128-bit</p>
</li>
<li>
<p>Five integer types: 8-bit signed, 8-bit unsigned, 16-bit signed, 32-bit signed, and 64-bit signed</p>
</li>
<li>
<p>Boolean type</p>
</li>
</ul>
<p>There are also four ”quantized number” types, but they are using the preceding types, just with different bit representation and interpretation.</p>
<p>Tensors of different types are represented by different classes, with the most commonly used being <span class="cmtt-10x-x-109">torch.FloatTensor </span>(corresponding to a 32-bit float), <span class="cmtt-10x-x-109">torch.ByteTensor </span>(an 8-bit unsigned integer), and <span class="cmtt-10x-x-109">torch.LongTensor </span>(a 64-bit signed integer). You can find names of other tensor types in the documentation.</p>
<p>There are three ways<span id="dx1-55002"/> to create a tensor in PyTorch:</p>
<ul>
<li>
<p>By calling a constructor of the required type.</p>
</li>
<li>
<p>By asking PyTorch to create a tensor with specific data for you. For example, you can use the <span class="cmtt-10x-x-109">torch.zeros() </span>function to create a tensor filled with zero values.</p>
</li>
<li>
<p>By converting a NumPy array or a Python list into a tensor. In this case, the type will be taken from the array’s type.</p>
</li>
</ul>
<p>To give you examples of these methods, let’s look at a simple session:</p>
<pre class="lstlisting" id="listing-30"><code>$ python 
&gt;&gt;&gt; import torch 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; a = torch.FloatTensor(3, 2) 
&gt;&gt;&gt; a 
tensor([[0., 0.], 
       [0., 0.], 
       [0., 0.]])</code></pre>
<p>Here, we imported both PyTorch and NumPy and created a new float tensor tensor of size 3 <span class="cmsy-10x-x-109">× </span>2. As you can see, PyTorch initializes memory with zeros, which is a different behaviour from previous versions. Before, it just allocated memory and kept it uninitialized, which is slightly faster but less safe (as it might introduce tricky bugs and security issues). But you shouldn’t rely on this behaviour, as it might change again (or behave differently on different hardware backends) and always initialize the contents of the tensor. To do so, you can either use one of the tensor construct operators:</p>
<pre class="lstlisting" id="listing-31"><code>&gt;&gt;&gt; torch.zeros(3, 4) 
tensor([[0., 0., 0., 0.], 
       [0., 0., 0., 0.], 
       [0., 0., 0., 0.]])</code></pre>
<p>Or you can call the tensor modification method:</p>
<pre class="lstlisting" id="listing-32"><code>&gt;&gt;&gt; a.zero_() 
tensor([[0., 0.], 
       [0., 0.], 
       [0., 0.]])</code></pre>
<p>There<span id="dx1-55019"/> are <span id="dx1-55020"/>two types of<span id="dx1-55021"/> operation for tensors: <span class="cmbx-10x-x-109">inplace </span>and <span class="cmbx-10x-x-109">functional</span>. Inplace operations have an underscore appended to their name and operate on the tensor’s content. After this, the object itself is returned. The functional equivalent creates a copy of the tensor with the performed modification, leaving the original tensor untouched. Inplace operations are usually more efficient from a performance and memory point of view, but modification of an existing tensor (especially if it is shared in different pieces of code) might lead to hidden bugs.</p>
<p>Another way to create a tensor by its constructor is to provide a Python iterable (for example, a list or tuple), which will be used as the contents of the newly created tensor:</p>
<pre class="lstlisting" id="listing-33"><code>&gt;&gt;&gt; torch.FloatTensor([[1,2,3],[3,2,1]]) 
tensor([[1., 2., 3.], 
       [3., 2., 1.]])</code></pre>
<p>Here, we are creating the same tensor with zeros from the NumPy array:</p>
<pre class="lstlisting" id="listing-34"><code>&gt;&gt;&gt; n = np.zeros(shape=(3, 2)) 
&gt;&gt;&gt; n 
array([[0., 0.], 
      [0., 0.], 
      [0., 0.]]) 
&gt;&gt;&gt; b = torch.tensor(n) 
&gt;&gt;&gt; b 
tensor([[0., 0.], 
       [0., 0.], 
       [0., 0.]], dtype=torch.float64)</code></pre>
<p>The <span class="cmtt-10x-x-109">torch.tensor </span>method accepts the NumPy array as an argument and creates a tensor of appropriate shape from it. In the preceding example, we created a NumPy array initialized by zeros, which created a double (64-bit float) array by default. So, the resulting tensor has the <span class="cmtt-10x-x-109">DoubleTensor </span>type (which is shown in the example with the <span class="cmtt-10x-x-109">dtype </span>value). Usually, in DL, double precision is not required and it adds an extra memory and performance overhead. Common practice is to use the 32-bit float type, or even the 16-bit float type, which is more than enough. To create such a tensor, you need to specify explicitly the type of NumPy array:</p>
<pre class="lstlisting" id="listing-35"><code>&gt;&gt;&gt; n = np.zeros(shape=(3, 2), dtype=np.float32) 
&gt;&gt;&gt; torch.tensor(n) 
tensor([[0., 0.], 
       [0., 0.], 
       [0., 0.]])</code></pre>
<p>As an option, the type of the desired tensor could be provided to the <span class="cmtt-10x-x-109">torch.tensor </span>function in the <span class="cmtt-10x-x-109">dtype </span>argument. However, be careful, since this argument expects to get a PyTorch type<span id="dx1-55040"/> specification and not the NumPy one. PyTorch types are kept in the <span class="cmtt-10x-x-109">torch </span>package, for example, <span class="cmtt-10x-x-109">torch.float32</span>, <span class="cmtt-10x-x-109">torch.uint8</span>, and so on.</p>
<pre class="lstlisting" id="listing-36"><code>&gt;&gt;&gt; n = np.zeros(shape=(3,2)) 
&gt;&gt;&gt; torch.tensor(n, dtype=torch.float32) 
tensor([[0., 0.], 
       [0., 0.], 
       [0., 0.]])</code></pre>
<div class="tcolorbox infobox" id="tcolobox-26">
<div class="tcolorbox-content">
<p><span class="cmbx-10x-x-109">A note on compatibility</span><br/> The <span class="cmtt-10x-x-109">torch.tensor() </span>method and explicit PyTorch type specification were added in the 0.4.0 release, and this is a step toward the simplification of tensor creation. In previous versions, the <span class="cmtt-10x-x-109">torch.from</span><span class="cmtt-10x-x-109">_numpy() </span>function was a recommended way to convert NumPy arrays, but it had issues with handling the combination of the Python list and NumPy arrays. This <span class="cmtt-10x-x-109">from</span><span class="cmtt-10x-x-109">_numpy() </span>function is still present for backward compatibility, but it is deprecated in favor of the more flexible <span class="cmtt-10x-x-109">torch.tensor()</span> method.</p>
</div>
</div>
<section class="level4 subsectionHead" id="scalar-tensors">
<h2 class="heading-2" id="sigil_toc_id_48"> <span id="x1-560003.2.1"/>Scalar tensors</h2>
<p>Since the 0.4.0 release, PyTorch<span id="dx1-56001"/> has supported <span id="dx1-56002"/>zero-dimensional tensors that correspond to scalar values (on the left of <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-54002r1"><span class="cmti-10x-x-109">3.1</span></a>). Such tensors can be the result of some operations, such as summing all values in a tensor. Previously, such cases were handled by the creation of a one-dimensional tensor (also known as vector) with a single dimension equal to one.</p>
<p>This solution worked, but it wasn’t very simple, as extra indexation was needed to access the value. Now, zero-dimensional tensors are natively supported and returned by the appropriate functions, and they can be <span id="dx1-56003"/>created <span id="dx1-56004"/>by the <span class="cmtt-10x-x-109">torch.tensor() </span>function. To access the actual Python value of such a tensor, we can use the special <span class="cmtt-10x-x-109">item() </span>method:</p>
<pre class="lstlisting" id="listing-37"><code>&gt;&gt;&gt; a = torch.tensor([1,2,3]) 
&gt;&gt;&gt; a 
tensor([1, 2, 3]) 
&gt;&gt;&gt; s = a.sum() 
&gt;&gt;&gt; s 
tensor(6) 
&gt;&gt;&gt; s.item() 
6 
&gt;&gt;&gt; torch.tensor(1) 
tensor(1)</code></pre>
</section>
<section class="level4 subsectionHead" id="tensor-operations">
<h2 class="heading-2" id="sigil_toc_id_49"> <span id="x1-570003.2.2"/>Tensor operations</h2>
<p>There are lots of <span id="dx1-57001"/>operations that you can perform on tensors, and there are too many to list them all. Usually, it’s enough to search in the PyTorch documentation at <a class="url" href="http://pytorch.org/docs/"><span class="cmtt-10x-x-109">http://pytorch.org/docs/</span></a>. I need to mention that there are two places to look for operations:</p>
<ul>
<li>
<p>The <span class="cmtt-10x-x-109">torch </span>package: The function usually accepts the tensor as an argument.</p>
</li>
<li>
<p>The tensor class: The function operates on the called tensor.</p>
</li>
</ul>
<p>Most of the time, tensor operations in PyTorch are trying to correspond to their NumPy equivalent, so if there is some not-very-specialized function in NumPy, then there is a good chance that PyTorch will also have it. Examples are <span class="cmtt-10x-x-109">torch.stack()</span>, <span class="cmtt-10x-x-109">torch.transpose()</span>, and <span class="cmtt-10x-x-109">torch.cat()</span>. This is very convenient, as NumPy is a very widely used library (especially in the scientific community), so your PyTorch code becomes readable by anyone familiar with NumPy without looking into the documentation.</p>
</section>
<section class="level4 subsectionHead" id="gpu-tensors">
<h2 class="heading-2" id="sigil_toc_id_50"> <span id="x1-580003.2.3"/>GPU tensors</h2>
<p>PyTorch<span id="dx1-58001"/> transparently<span id="dx1-58002"/> supports CUDA GPUs, which means that all operations have two versions — CPU and GPU — that are automatically selected. The decision is made based on the type of tensors that you are operating on.</p>
<p>Every tensor type that I mentioned is for CPU and has its GPU equivalent. The only difference is that GPU tensors reside in the <span class="cmtt-10x-x-109">torch.cuda </span>package, instead of just <span class="cmtt-10x-x-109">torch</span>. For example, <span class="cmtt-10x-x-109">torch.FloatTensor </span>is a 32-bit float tensor that resides in CPU memory, but <span class="cmtt-10x-x-109">torch.cuda.FloatTensor </span>is its GPU counterpart.</p>
<div class="tcolorbox infobox" id="tcolobox-27">
<div class="tcolorbox-content">
<p>In fact, under the hood, PyTorch supports not just CPU and CUDA; it has a notion of <span class="cmti-10x-x-109">backend</span>, which is an abstract computation device with memory. Tensors could be allocated in the backend’s memory and computations could be performed on them. For example, on Apple hardware, PyTorch supports <span class="cmbx-10x-x-109">Metal</span> <span class="cmbx-10x-x-109">Performance Shaders </span>(<span class="cmbx-10x-x-109">MPS</span>) as a backend called <span class="cmtt-10x-x-109">mps</span>. In this chapter, we focus on CPU and GPU as the mostly used backends, but your PyTorch code could be executed on much more fancier <span id="dx1-58003"/>hardware without major modifications.</p>
</div>
</div>
<p>To convert from CPU to GPU, there is a tensor method, <span class="cmtt-10x-x-109">to(device)</span>, that creates a copy of the tensor to a specified device (this could be CPU or GPU). If the tensor is already on the device, nothing happens and the original tensor will be returned. The device type can be specified in different ways. First of all, you can just pass a string name of the device, which is <span class="cmtt-10x-x-109">"cpu" </span>for CPU memory or <span class="cmtt-10x-x-109">"cuda" </span>for GPU. A GPU device could have an optional device index specified after the colon; for example, the second GPU card in the system could be addressed by <span class="cmtt-10x-x-109">"cuda:1" </span>(the index is zero-based).</p>
<p>Another slightly <span id="dx1-58004"/>more efficient way to specify a device in the <span class="cmtt-10x-x-109">to() </span>method is by using the <span class="cmtt-10x-x-109">torch.device </span>class, which accepts the device name and optional index. To access the device that your tensor is currently residing in, it has a <span class="cmtt-10x-x-109">device</span> property:</p>
<pre class="lstlisting" id="listing-38"><code>&gt;&gt;&gt; a = torch.FloatTensor([2,3]) 
&gt;&gt;&gt; a 
tensor([2., 3.]) 
&gt;&gt;&gt; ca = a.to(’cuda’) 
&gt;&gt;&gt; ca 
tensor([2., 3.], device=’cuda:0’)</code></pre>
<p>Here, we created a tensor on the CPU, then copied it to GPU memory. Both copies can be used in computations, and all GPU-specific machinery is transparent to the user:</p>
<pre class="lstlisting" id="listing-39"><code>&gt;&gt;&gt; a+1 
tensor([3., 4.]) 
&gt;&gt;&gt; ca + 1 
tensor([3., 4.], device=’cuda:0’) 
&gt;&gt;&gt; ca.device 
device(type=’cuda’, index=0)</code></pre>
<div class="tcolorbox infobox" id="tcolobox-28">
<div class="tcolorbox-content">
<p>The <span class="cmtt-10x-x-109">to() </span>method and <span class="cmtt-10x-x-109">torch.device </span>class were introduced in 0.4.0. In previous versions, copying between CPU and GPU was performed by separate tensor methods, <span class="cmtt-10x-x-109">cpu() </span>and <span class="cmtt-10x-x-109">cuda()</span>, respectively, which required adding the extra lines of code to explicitly convert tensors into their CUDA versions. In newer PyTorch versions, you can create a desired <span class="cmtt-10x-x-109">torch.device </span>object at the beginning of the program and use <span class="cmtt-10x-x-109">to(device) </span>on every tensor that you’re creating. The old methods in the tensor, <span class="cmtt-10x-x-109">cpu() </span>and <span class="cmtt-10x-x-109">cuda()</span>, are still present and might be handy if you want to<span id="dx1-58017"/> ensure that a tensor is in CPU or GPU memory regardless of its original<span id="dx1-58018"/> location.</p>
</div>
</div>
</section>
</section>
<section class="level3 sectionHead" id="gradients">
<h1 class="heading-1" id="sigil_toc_id_51"> <span id="x1-590003.3"/>Gradients</h1>
<p>Even with<span id="dx1-59001"/> transparent GPU support, all of this dancing with tensors isn’t worth bothering without one “killer feature” — the automatic computation of gradients. This functionality was originally implemented in the Caffe toolkit and then became the de facto standard in DL libraries.</p>
<p>Earlier, computing gradients manually was extremely painful to implement and debug, even for the simplest <span class="cmbx-10x-x-109">neural network </span>(<span class="cmbx-10x-x-109">NN</span>). You had to calculate derivatives for all your functions, apply the chain rule, and then implement the result of the calculations, praying that everything was done right. This could be a very useful exercise for understanding the nuts and bolts of DL, but it wasn’t something that you wanted to repeat over and over again by experimenting with different NN architectures.</p>
<p>Luckily, those days have gone now, much like programming your hardware using a soldering iron and vacuum tubes! Now, defining an NN of hundreds of layers requires nothing more than assembling it from predefined building blocks or, in the extreme case of you doing something fancy, defining the transformation expression manually.</p>
<p>All gradients will be carefully calculated for you, backpropagated, and applied to the network. To be able to achieve this, you need to define your network architecture using DL library primitives. In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-59002r2"><span class="cmti-10x-x-109">3.2</span></a>, I have outlined the direction of the data and gradients flow during the optimization process:</p>
<div class="minipage">
<p><img alt="IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts " height="300" src="../Images/B22150_03_02.png" width="600"/> <span id="x1-59002r2"/></p>
<span class="id">Figure 3.2: Data and gradients flowing through the NN </span>
</div>
<p>What can make a fundamental difference is how your gradients are calculated. There are <span id="dx1-59003"/>two approaches:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Static graph</span>: In this method, you need to define your calculations in advance and it won’t be possible to change them later. The graph will be processed and optimized by the DL library before any computation is made. This model is implemented in TensorFlow (versions before 2.0), Theano, and many other DL toolkits.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Dynamic graph</span>: You don’t need to define your graph in advance exactly as it will be executed; you just need to execute operations that you want to use for data transformation on your actual data. During this, the library will record the order of the operations performed, and when you ask it to calculate gradients, it will unroll its history of operations, accumulating the gradients of the network parameters. This method is also called notebook gradients and it is implemented in PyTorch, Chainer, and some others.</p>
</li>
</ul>
<p>Both methods have their strengths and weaknesses. For example, a static graph is usually faster, as all computations can be moved to the GPU, minimizing the data transfer overhead. Additionally, in a static graph, the library has <span id="dx1-59004"/>much more freedom in optimizing the order that computations are performed in or even removing parts of the graph.</p>
<p>On the other hand, although a dynamic graph has a higher computation overhead, it gives a developer much more freedom. For example, they can say, “For this piece of data, I can apply this network two times, and for this piece of data, I’ll use a completely different model with gradients clipped by the batch mean”. Another very appealing strength of the dynamic graph model is that it allows you to express your transformation more naturally and in a more “Pythonic” way. In the end, it’s just a Python library with a bunch of functions, so just call them <span id="dx1-59005"/>and let the library do the magic.</p>
<div class="tcolorbox infobox" id="tcolobox-29">
<div class="tcolorbox-content">
<p>Since version 2.0, PyTorch introduced the <span class="cmtt-10x-x-109">torch.compile </span>function, which speeds up PyTorch code by JIT-compiling the code into optimized kernels. This is an evolution of the <span class="cmti-10x-x-109">TorchScript </span>and <span class="cmti-10x-x-109">FX</span> <span class="cmti-10x-x-109">Tracing </span>compiling methods from earlier versions.</p>
<p>From a historical perspective, this is highly amusing how originally radically different approaches of TensorFlow (static graph) and PyTorch (dynamic graph) are fusing into each other over time. Nowadays, PyTorch supports <span class="cmtt-10x-x-109">compile() </span>and TensorFlow has “eager execution mode”.</p>
</div>
</div>
<section class="level4 subsectionHead" id="tensors-and-gradients">
<h2 class="heading-2" id="sigil_toc_id_52"> <span id="x1-600003.3.1"/>Tensors and gradients</h2>
<p>PyTorch tensors have a built-in gradient calculation<span id="dx1-60001"/> and<span id="dx1-60002"/> tracking machinery, so all you need to do is convert the data into tensors and perform computations using the tensor methods and functions provided by <span class="cmtt-10x-x-109">torch</span>. Of course, if you need to access underlying low-level details, you always can, but most of the time, PyTorch does what you’re expecting.</p>
<p>There are several attributes related to gradients that every tensor has:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">grad</span>: A property that holds a tensor of the same shape containing computed gradients.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">is</span><span class="cmtt-10x-x-109">_leaf</span>: Equals <span class="cmtt-10x-x-109">True </span>if this tensor was constructed by the user and <span class="cmtt-10x-x-109">False </span>if the object is a result of function transformation (in other words, have a parent in the computation graph).</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad</span>: Equals <span class="cmtt-10x-x-109">True </span>if this tensor requires gradients to be calculated. This property is inherited from leaf tensors, which get this value from the tensor construction step (<span class="cmtt-10x-x-109">torch.zeros()</span> or <span class="cmtt-10x-x-109">torch.tensor() </span>and so on). By default, the constructor has <span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad=False</span>, so if you want gradients to be calculated for your tensor, then you need to explicitly say so.</p>
</li>
</ul>
<p>To make all of this gradient-leaf machinery clearer, let’s consider this session:</p>
<pre class="lstlisting" id="listing-40"><code>&gt;&gt;&gt; v1 = torch.tensor([1.0, 1.0], requires_grad=True) 
&gt;&gt;&gt; v2 = torch.tensor([2.0, 2.0])</code></pre>
<p>Here, we created two tensors. The first requires gradients to be calculated and the second doesn’t.</p>
<p>Next, we have added both vectors element-wise (which is vector <span class="cmtt-10x-x-109">[3, 3]</span>), doubled every element, and summed them together:</p>
<pre class="lstlisting" id="listing-41"><code>&gt;&gt;&gt; v_sum = v1 + v2 
&gt;&gt;&gt; v_sum 
tensor([3., 3.], grad_fn=&lt;AddBackward0&gt;) 
&gt;&gt;&gt; v_res = (v_sum*2).sum() 
&gt;&gt;&gt; v_res 
tensor(12., grad_fn=&lt;SumBackward0&gt;)</code></pre>
<p>The result is a zero-dimensional tensor with the value <span class="cmtt-10x-x-109">12</span>. Okay, so far this is just a simple math. Now let’s look at the underlying graph that our expressions created:</p>
<div class="minipage">
<p><img alt="vv+v×Σv2 12sruesm " height="300" src="../Images/B22150_03_03.png" width="500"/> <span id="x1-60011r3"/></p>
<span class="id">Figure 3.3: Graph representation of the expression </span>
</div>
<p>If we check <span id="dx1-60012"/>the attributes<span id="dx1-60013"/> of our tensors, then we will find that <span class="cmbx-10x-x-109">v1 </span>and <span class="cmbx-10x-x-109">v2 </span>are the only leaf nodes and every variable, except <span class="cmbx-10x-x-109">v2</span>, requires gradients to be calculated:</p>
<pre class="lstlisting" id="listing-42"><code>&gt;&gt;&gt; v1.is_leaf, v2.is_leaf 
(True, True) 
&gt;&gt;&gt; v_sum.is_leaf, v_res.is_leaf 
(False, False) 
&gt;&gt;&gt; v1.requires_grad 
True 
&gt;&gt;&gt; v2.requires_grad 
False 
&gt;&gt;&gt; v_sum.requires_grad 
True 
&gt;&gt;&gt; v_res.requires_grad 
True</code></pre>
<p>As you can see, the property <span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad </span>is sort of “sticky”: if one of the variables involved in computations has it set to <span class="cmtt-10x-x-109">True</span>, all subsequent nodes also have it. This is logical behaviour, as we normally need gradients to be calculated for all intermediate steps in our computation. But “calculation” doesn’t mean they will be preserved in the <span class="cmtt-10x-x-109">.grad </span>field. For memory efficiency, gradients are stored only for leaf nodes with <span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad=True</span>. If you want to keep gradients in the non-leaf nodes, you need to call their <span class="cmtt-10x-x-109">retain</span><span class="cmtt-10x-x-109">_grad() </span>method, which tells PyTorch to keep the gradients for non-leaf node.</p>
<p>Now, let’s tell PyTorch to calculate the gradients of our graph:</p>
<pre class="lstlisting" id="listing-43"><code>&gt;&gt;&gt; v_res.backward() 
&gt;&gt;&gt; v1.grad 
tensor([2., 2.])</code></pre>
<p>By calling the backward function, we asked PyTorch to calculate the numerical derivative of the <span class="cmtt-10x-x-109">v</span><span class="cmtt-10x-x-109">_res </span>variable with respect to any variable that our graph has. In other words, what influence do small changes to the <span class="cmtt-10x-x-109">v</span><span class="cmtt-10x-x-109">_res </span>variable have on the rest of the graph? In our particular example, the value of 2 in the gradients of <span class="cmtt-10x-x-109">v1 </span>means that by increasing any element of <span class="cmtt-10x-x-109">v1 </span>by one, the resulting value of <span class="cmtt-10x-x-109">v</span><span class="cmtt-10x-x-109">_res </span>will grow by two.</p>
<p>As mentioned, PyTorch calculates gradients only for leaf tensors with <span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad=True</span>. Indeed, if we try to check the gradients of <span class="cmtt-10x-x-109">v2</span>, we get nothing:</p>
<pre class="lstlisting" id="listing-44"><code>&gt;&gt;&gt; v2.grad</code></pre>
<p>The reason<span id="dx1-60030"/> for that is efficiency in <span id="dx1-60031"/>terms of computations and memory. In real life, our network can have millions of optimized parameters, with hundreds of intermediate operations performed on them. During gradient descent optimization, we are not interested in gradients of any intermediate matrix multiplication; the only things we want to adjust in the model are gradients of loss with respect to model parameters (weights). Of course, if you want to calculate the gradients of input data (it could be useful if you want to generate some adversarial examples to fool the existing NN or adjust pretrained word embeddings), then you can easily do so by passing <span class="cmtt-10x-x-109">requires</span><span class="cmtt-10x-x-109">_grad=True </span>on tensor creation.</p>
<p>Basically, you now have everything needed to implement your own NN optimizer. The rest of this chapter is about extra, convenient functions, which will provide you with higher-level building blocks of NN architectures, popular optimization algorithms, and common loss functions. However, don’t forget that you can easily reimplement all of these bells and whistles in any way that you like. This is why PyTorch is so popular among DL researchers — for its elegance and flexibility.</p>
<div class="tcolorbox infobox" id="tcolobox-30">
<div class="tcolorbox-content">
<span class="cmbx-10x-x-109">Compatibility</span>
<p>Support of gradients calculation in tensors is one of the major changes in PyTorch 0.4.0. In previous versions, graph tracking and gradients accumulation were done in a separate, very thin class, <span class="cmtt-10x-x-109">Variable</span>. This worked as a wrapper around the tensor and automatically saved the history of computations in order to be able to backpropagate. This class is still present in 2.2.0 (available in <span class="cmtt-10x-x-109">torch.autograd</span>), but it is deprecated and will go away soon, so new <span id="dx1-60032"/>code should <span id="dx1-60033"/>avoid using it. From my perspective, this change is great, as the <span class="cmtt-10x-x-109">Variable </span>logic was really thin, but it still required extra code and the developer’s attention to wrap and unwrap tensors. Now, gradients are a built-in tensor property, which makes the API much cleaner.</p>
</div>
</div>
</section>
</section>
<section class="level3 sectionHead" id="nn-building-blocks">
<h1 class="heading-1" id="sigil_toc_id_53"> <span id="x1-610003.4"/>NN building blocks</h1>
<p>In the <span class="cmtt-10x-x-109">torch.nn </span>package, you will find tons <span id="dx1-61001"/>of predefined classes providing you with the basic functionality blocks. All of them are designed with practice in mind (for example, they support mini-batches, they have sane default values, and the weights are properly initialized). All modules follow the convention of <span class="cmti-10x-x-109">callable</span>, which means that the instance of any class can act as a function when applied to its arguments. For example, the <span class="cmtt-10x-x-109">Linear </span>class implements a feed-forward layer with optional bias:</p>
<pre class="lstlisting" id="listing-45"><code>&gt;&gt;&gt; l = nn.Linear(2, 5) 
&gt;&gt;&gt; v = torch.FloatTensor([1, 2]) 
&gt;&gt;&gt; l(v) 
tensor([-0.1039, -1.1386,  1.1376, -0.3679, -1.1161], grad_fn=&lt;ViewBackward0&gt;)</code></pre>
<p>Here, we created a randomly initialized feed-forward layer, with two inputs and five outputs, and applied it to our float tensor. All classes in the <span class="cmtt-10x-x-109">torch.nn</span> packages inherit from the <span class="cmtt-10x-x-109">nn.Module </span>base class, which you can use to implement your own higher-level NN blocks. You will see how you can do this in the next section, but, for now, let’s look at useful methods that all <span class="cmtt-10x-x-109">nn.Module </span>children provide. They are as follows:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">parameters()</span>: This function returns an iterator of all variables that require gradient computation (that is, module weights).</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">zero</span><span class="cmtt-10x-x-109">_grad()</span>: This function initializes all gradients of all parameters to zero.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">to(device)</span>: This function moves all module parameters to a given device (CPU or GPU).</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">state</span><span class="cmtt-10x-x-109">_dict()</span>: This function returns the dictionary with all module parameters and is useful for model serialization.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">load</span><span class="cmtt-10x-x-109">_state</span><span class="cmtt-10x-x-109">_dict()</span>: This function initializes the module with the state dictionary.</p>
</li>
</ul>
<p>The whole list of available classes can be found in the documentation at <a class="url" href="http://pytorch.org/docs"><span class="cmtt-10x-x-109">http://pytorch.org/docs</span></a>.</p>
<p>Now, I should mention one very convenient class that allows you to combine other layers into the pipe: <span class="cmtt-10x-x-109">Sequential</span>. The best way to demonstrate <span class="cmtt-10x-x-109">Sequential </span>is through an example:</p>
<pre class="lstlisting" id="listing-46"><code>&gt;&gt;&gt; s = nn.Sequential( 
... nn.Linear(2, 5), 
... nn.ReLU(), 
... nn.Linear(5, 20), 
... nn.ReLU(), 
... nn.Linear(20, 10), 
... nn.Dropout(p=0.3), 
... nn.Softmax(dim=1)) 
&gt;&gt;&gt; s 
Sequential( 
  (0): Linear(in_features=2, out_features=5, bias=True) 
  (1): ReLU() 
  (2): Linear(in_features=5, out_features=20, bias=True) 
  (3): ReLU() 
  (4): Linear(in_features=20, out_features=10, bias=True) 
  (5): Dropout(p=0.3, inplace=False) 
  (6): Softmax(dim=1) 
)</code></pre>
<p>Here, we defined a <span id="dx1-61024"/>three-layer NN with softmax on output, applied along dimension 1 (dimension 0 is batch samples), <span class="cmbx-10x-x-109">rectified linear unit </span>(<span class="cmbx-10x-x-109">ReLU</span>) nonlinearities, and dropout. Let’s push something through it:</p>
<pre class="lstlisting" id="listing-47"><code>&gt;&gt;&gt; s(torch.FloatTensor([[1,2]])) 
tensor([[0.0847, 0.1145, 0.1063, 0.1458, 0.0873, 0.1063, 0.0864, 0.0821, 0.0894, 
        0.0971]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
<p>So, our mini-batch of one vector successfully <span id="dx1-61028"/>traversed through the network!</p>
</section>
<section class="level3 sectionHead" id="custom-layers">
<h1 class="heading-1" id="sigil_toc_id_54"> <span id="x1-620003.5"/>Custom layers</h1>
<p>In the previous <span id="dx1-62001"/>section, I briefly mentioned the <span class="cmtt-10x-x-109">nn.Module </span>class as a base parent for all NN building blocks exposed by PyTorch. It’s not just a unifying parent for the existing layers — it’s much more than that. By subclassing the <span class="cmtt-10x-x-109">nn.Module </span>class, you can create your own building blocks, which can be stacked together, reused later, and integrated into the PyTorch framework flawlessly.</p>
<p>At its core, the <span class="cmtt-10x-x-109">nn.Module </span>provides quite rich functionality to its children:</p>
<ul>
<li>
<p>It tracks all submodules that the current module includes. For example, your building block can have two feed-forward layers used somehow to perform the block’s transformation. To keep track of (register) the submodule, you just need to assign it to the class’s field.</p>
</li>
<li>
<p>It provides functions to deal with all parameters of the registered submodules. You can obtain a full list of the module’s parameters (<span class="cmtt-10x-x-109">parameters() </span>method), zero its gradients (<span class="cmtt-10x-x-109">zero</span><span class="cmtt-10x-x-109">_grads() </span>method), move to CPU or GPU (<span class="cmtt-10x-x-109">to(device) </span>method), serialize and deserialize the module (<span class="cmtt-10x-x-109">state</span><span class="cmtt-10x-x-109">_dict() </span>and <span class="cmtt-10x-x-109">load</span><span class="cmtt-10x-x-109">_state</span><span class="cmtt-10x-x-109">_dict()</span>), and even perform generic transformations using your own callable (<span class="cmtt-10x-x-109">apply()</span> method).</p>
</li>
<li>
<p>It establishes the convention of <span class="cmtt-10x-x-109">Module </span>application to data. Every module needs to perform its data transformation in the <span class="cmtt-10x-x-109">forward()</span> method by overriding it.</p>
</li>
<li>
<p>There are some more functions, such as the ability to register a hook function to tweak module transformation or gradients flow, but they are more for advanced use cases.</p>
</li>
</ul>
<p>These functionalities allow us to nest our submodels into higher-level models in a unified way, which is extremely useful when dealing with complexity. It could be a simple one-layer linear <span id="dx1-62002"/>transformation or a 1001-layer <span class="cmbx-10x-x-109">residual NN </span>(<span class="cmbx-10x-x-109">ResNet</span>) monster, but if they follow the conventions of <span class="cmtt-10x-x-109">nn.Module</span>, then both of them could be handled in the same way. This is very handy for code reusability and simplification (by hiding non-relevant implementation details).</p>
<p>To make our <span id="dx1-62003"/>life simpler, when following the above conventions, PyTorch authors simplified the creation of modules through careful design and a good dose of Python magic. So, to create a custom module, we usually have to do only two things — register submodules and implement the <span class="cmtt-10x-x-109">forward()</span> method.</p>
<p>Let’s look at how this can be done for our <span class="cmtt-10x-x-109">Sequential </span>example from the previous section, but in a more generic and reusable way (the full sample is <span class="cmtt-10x-x-109">Chapter03/01</span><span class="cmtt-10x-x-109">_modules.py</span>). The following is our module class that inherits <span class="cmtt-10x-x-109">nn.Module</span>:</p>
<div class="tcolorbox" id="tcolobox-31">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-48"><code>class OurModule(nn.Module): 
    def __init__(self, num_inputs, num_classes, dropout_prob=0.3): 
        super(OurModule, self).__init__() 
        self.pipe = nn.Sequential( 
            nn.Linear(num_inputs, 5), 
            nn.ReLU(), 
            nn.Linear(5, 20), 
            nn.ReLU(), 
            nn.Linear(20, num_classes), 
            nn.Dropout(p=dropout_prob), 
            nn.Softmax(dim=1) 
        )</code></pre>
</div>
</div>
<p>In the constructor, we pass three parameters: the input size, the output size, and the optional dropout probability. The first thing we need to do is call the parent’s constructor to let it initialize itself.</p>
<p>In the second step in the preceding code, we create an already familiar <span class="cmtt-10x-x-109">nn.Sequential </span>with a bunch of layers and assign it to our class field named pipe. By assigning a <span class="cmtt-10x-x-109">Sequential </span>instance to our object’s field, we will automatically register this module (<span class="cmtt-10x-x-109">nn.Sequential </span>inherits from <span class="cmtt-10x-x-109">nn.Module</span>, as does everything in the <span class="cmtt-10x-x-109">nn </span>package). To register it, we don’t need to call anything, we just need to assign our submodules to fields. After the constructor finishes, all those fields will be registered automatically. If you really want to, there is a function in <span class="cmtt-10x-x-109">nn.Module</span> to register submodules called <span class="cmtt-10x-x-109">add</span><span class="cmtt-10x-x-109">_module()</span>. It might be useful if your module can have variable number of layers and they need to be created programmatically.</p>
<p>Next, we must override the <span class="cmtt-10x-x-109">forward </span>function with our implementation of data transformation:</p>
<div class="tcolorbox" id="tcolobox-32">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-49"><code>    def forward(self, x): 
        return self.pipe(x)</code></pre>
</div>
</div>
<p>As our module is a very simple wrapper around the <span class="cmtt-10x-x-109">Sequential </span>class, we just need to ask <span class="cmtt-10x-x-109">self.pipe </span>to transform the data. Note that to apply a module to the data, we need to call the module as a callable (that is, pretend that the module instance is a function and call it with the arguments) and not use the <span class="cmtt-10x-x-109">forward() </span>function of the <span class="cmtt-10x-x-109">nn.Module </span>class. This is because <span class="cmtt-10x-x-109">nn.Module </span>overrides the <span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_call</span><span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_() </span>method, which is being used when we treat an instance as callable. This <span id="dx1-62018"/>method does some <span class="cmtt-10x-x-109">nn.Module </span>magic and calls our <span class="cmtt-10x-x-109">forward() </span>method. If we call <span class="cmtt-10x-x-109">forward()</span> directly, we will intervene with the <span class="cmtt-10x-x-109">nn.Module </span>duty, which can give wrong results.</p>
<p>So, that’s what we need to do to define our own module. Now, let’s use it:</p>
<div class="tcolorbox" id="tcolobox-33">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-50"><code>if __name__ == "__main__": 
    net = OurModule(num_inputs=2, num_classes=3) 
    print(net) 
    v = torch.FloatTensor([[2, 3]]) 
    out = net(v) 
    print(out) 
    print("Cuda’s availability is %s" % torch.cuda.is_available()) 
    if torch.cuda.is_available(): 
        print("Data from cuda: %s" % out.to(’cuda’))</code></pre>
</div>
</div>
<p>We create our module, providing it with the desired number of inputs and outputs, then we create a tensor and ask our module to transform it, following the same convention of using it as callable. After that, we print our network’s structure (<span class="cmtt-10x-x-109">nn.Module </span>overrides <span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_str</span><span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_() </span>and <span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_repr</span><span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_()</span>) to represent the inner structure in a nice way. The last thing we show is the result of the network’s transformation. The output of our code should look like this:</p>
<pre class="lstlisting" id="listing-51"><code>Chapter03$ python 01_modules.py 
OurModule( 
  (pipe): Sequential( 
   (0): Linear(in_features=2, out_features=5, bias=True) 
   (1): ReLU() 
   (2): Linear(in_features=5, out_features=20, bias=True) 
   (3): ReLU() 
   (4): Linear(in_features=20, out_features=3, bias=True) 
   (5): Dropout(p=0.3, inplace=False) 
   (6): Softmax(dim=1) 
  ) 
) 
tensor([[0.3297, 0.3854, 0.2849]], grad_fn=&lt;SoftmaxBackward0&gt;) 
Cuda’s availability is False</code></pre>
<p>Of course, everything that was said about the dynamic nature of PyTorch is still true. The <span class="cmtt-10x-x-109">forward() </span>method is called for every batch of data, so if you want to do some complex transformations based on the data you need to process, like hierarchical softmax or a random choice of network to apply, then nothing can stop you from doing so. The count of arguments to your module is also not limited by one parameter. So, if you want, you can write a module with multiple required parameters and dozens of optional arguments, and it will be fine.</p>
<p>Next, we need to <span id="dx1-62042"/>get familiar with two important pieces of the PyTorch library that will simplify our lives: loss functions and optimizers.</p>
</section>
<section class="level3 sectionHead" id="loss-functions-and-optimizers">
<h1 class="heading-1" id="sigil_toc_id_55"> <span id="x1-630003.6"/>Loss functions and optimizers</h1>
<p>The network that transforms input data into output is not the only thing we need for training. We also define our learning objective, which has to be a function that accepts two arguments — the network’s output and the desired output. Its responsibility is to return to us a single number — how close the network’s prediction is from the desired result. This function is called the <span class="cmbx-10x-x-109">loss</span> <span class="cmbx-10x-x-109">function</span>, and its output is the <span class="cmbx-10x-x-109">loss value</span>. Using the loss value, we calculate gradients of network parameters and adjust them to decrease this loss value, which pushes our model to better results in the future. Both the loss function and the method of tweaking a network’s parameters by gradient are so common and exist in so many forms that both of them form a significant part of the PyTorch library. Let’s start with loss functions.</p>
<section class="level4 subsectionHead" id="loss-functions">
<h2 class="heading-2" id="sigil_toc_id_56"> <span id="x1-640003.6.1"/>Loss functions</h2>
<p>Loss functions <span id="dx1-64001"/>reside in the <span class="cmtt-10x-x-109">nn </span>package and are implemented as an <span class="cmtt-10x-x-109">nn.Module</span> subclass. Usually, they accept two arguments: output from the network (prediction) and desired output (ground-truth data, which is also called the label of the data sample). At the time of writing, PyTorch 2.3.1 contains over 20 different loss functions and, of course, nothing stops you from writing any custom function you want to optimize.</p>
<p>The most commonly used standard loss functions are:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">nn.MSELoss</span>: The mean square error between arguments, which is the standard loss for regression problems.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">nn.BCELoss </span>and <span class="cmtt-10x-x-109">nn.BCEWithLogits</span>: Binary cross-entropy loss. The first version expects a single probability value (usually it’s the output of the <span class="cmtt-10x-x-109">Sigmoid </span>layer), while the second version assumes raw scores as input and applies <span class="cmtt-10x-x-109">Sigmoid </span>itself. The second way is usually more numerically stable and efficient. These losses (as their names suggest) are frequently used in binary classification problems.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">nn.CrossEntropyLoss </span>and <span class="cmtt-10x-x-109">nn.NLLLoss</span>: Famous “maximum likelihood” criteria that are used in multi-class classification problems. The first version expects raw scores for each class and applies <span class="cmtt-10x-x-109">LogSoftmax </span>internally, while the second expects to have log probabilities as the input.</p>
</li>
</ul>
<p>There are other loss functions available and you are always free to write your own <span class="cmtt-10x-x-109">Module </span>subclass to compare the output and target. Now, let’s look at the second piece of the optimization process.</p>
</section>
<section class="level4 subsectionHead" id="optimizers">
<h2 class="heading-2" id="sigil_toc_id_57"> <span id="x1-650003.6.2"/>Optimizers</h2>
<p>The responsibility <span id="dx1-65001"/>of the basic optimizer is to take the gradients of model parameters and change these parameters in order to decrease the loss value. By decreasing the loss value, we are pushing our model toward the desired output, which can give us hope for better model performance in the future. Changing parameters may sound simple, but there are lots of details here and the optimizer procedure is still a hot research topic. In the <span class="cmtt-10x-x-109">torch.optim </span>package, PyTorch provides lots of popular optimizer implementations, and the most widely known are as follows:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">SGD</span>: A vanilla stochastic gradient descent algorithm with an optional momentum extension</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">RMSprop</span>: An optimizer proposed by Geoffrey Hinton</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Adagrad</span>: An adaptive gradients optimizer</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Adam</span>: A quite successful and popular combination of both <span class="cmtt-10x-x-109">RMSprop</span> and <span class="cmtt-10x-x-109">Adagrad</span></p>
</li>
</ul>
<p>All optimizers expose the unified interface, which makes it easy to experiment with different optimization methods (sometimes the optimization method can really make a difference in convergence dynamics and the final result). On construction, you need to pass an iterable of tensors, which will be modified during the optimization process. The usual practice is to pass the result of the <span class="cmtt-10x-x-109">params() </span>call of the upper-level <span class="cmtt-10x-x-109">nn.Module </span>instance, which will return an iterable of all leaf tensors with gradients.</p>
<p>Now, let’s discuss the common blueprint of a training loop:</p>
<div class="tcolorbox" id="tcolobox-34">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-52"><code>for batch_x, batch_y in iterate_batches(data, batch_size=N): 
    batch_x_t = torch.tensor(batch_x) 
    batch_y_t = torch.tensor(batch_y) 
    out_t = net(batch_x_t) 
    loss_t = loss_function(out_t, batch_y_t). 
    loss_t.backward() 
    optimizer.step() 
    optimizer.zero_grad()</code></pre>
</div>
</div>
<p>Usually, you iterate over your data over and over again (one iteration over a full set of examples is called an <span class="cmti-10x-x-109">epoch</span>). Data is usually too large to fit into CPU or GPU memory at once, so it is split into batches of equal size. Every batch includes data samples and target labels, and both of them have to be tensors (lines 2 and 3).</p>
<p>You pass data samples to your network (line 4) and feed its output and target labels to the loss function (line 5). The result of the loss function shows the “badness” of the network result relative to the target labels. As input to the network and the network’s weights are tensors, all transformations of your network are nothing more than a graph of operations with intermediate tensor instances. The same is true for the loss function — its result is also a tensor of one single loss value.</p>
<p>Every tensor in <span id="dx1-65010"/>this computation graph remembers its parent, so to calculate gradients for the whole network, all you need to do is call the <span class="cmtt-10x-x-109">backward()</span> function on a loss function result (line 6). The result of this call will be the unrolling of the graph of the performed computations and the calculating of gradients for every leaf tensor with <span class="cmtt-10x-x-109">require</span><span class="cmtt-10x-x-109">_grad=True</span>. Usually, such tensors are our model’s parameters, such as the weights and biases of feed-forward networks, and convolution filters. Every time a gradient is calculated, it is accumulated in the <span class="cmtt-10x-x-109">tensor.grad </span>field, so one tensor can participate in a transformation multiple times and its gradients will be properly <span id="dx1-65011"/>summed together. For example, one single <span class="cmbx-10x-x-109">recurrent neural network (RNN) </span>cell could be applied to multiple input items.</p>
<p>After the <span class="cmtt-10x-x-109">loss.backward() </span>call is finished, we have the gradients accumulated, and now it’s time for the optimizer to do its job — it takes all gradients from the parameters we have passed to it on construction and applies them. All this is done with the method <span class="cmtt-10x-x-109">step() </span>(line 7).</p>
<p>The last, but not least, piece of the training loop is our responsibility to zero gradients of parameters. This can be done by calling <span class="cmtt-10x-x-109">zero</span><span class="cmtt-10x-x-109">_grad()</span> on our network, but, for our convenience, the optimizer also exposes such a call, which does the same thing (line 8). Sometimes, <span class="cmtt-10x-x-109">zero</span><span class="cmtt-10x-x-109">_grad()</span> is placed at the beginning of the training loop, but it doesn’t matter much.</p>
<p>The preceding scheme is a very flexible way to perform optimization and it can fulfill the requirements even in sophisticated research. For example, you can have two optimizers tweaking the options of different models on the same data (and this is a real-life scenario<span id="dx1-65012"/> from <span class="cmbx-10x-x-109">generative adversarial network (GAN)</span> training).</p>
<p>So, we are done with the essential functionality <span id="dx1-65013"/>of PyTorch required to train NNs. This chapter ends with a practical medium-size example to demonstrate all the concepts covered, but before we get to it, we need to discuss one important topic that is essential for an NN practitioner — monitoring the learning process.</p>
</section>
</section>
<section class="level3 sectionHead" id="monitoring-with-tensorboard">
<h1 class="heading-1" id="sigil_toc_id_58"> <span id="x1-660003.7"/>Monitoring with TensorBoard</h1>
<p>If you have ever<span id="dx1-66001"/> tried to train an NN on your own, then you will know how painful and uncertain it can be. I’m not talking about following the existing tutorials and demos, when all the hyperparameters are already tuned for you, but about taking some data and creating something from scratch. Even with modern DL high-level toolkits, where all best practices, such as proper weights initialization; optimizers’ betas, gammas, and other options set to sane defaults; and tons of other stuff hidden under the hood, there are still lots of decisions that you have to make, hence lots of things that could go wrong. As a result, your code almost never works from the first run, and this is something that you should get used to.</p>
<p>Of course, with practice and experience, you will develop a strong understanding of the possible causes of problems, but this needs input data about what’s going on inside your network. So, you need to be able to peek inside your training process somehow and observe its dynamics. Even small networks (such as tiny MNIST tutorial networks) could have hundreds of thousands of parameters with quite nonlinear training dynamics.</p>
<p>DL practitioners have developed a list of things that you should observe during your training, which usually includes the following:</p>
<ul>
<li>
<p>Loss value, which normally consists of several components like base loss and regularization losses. You should monitor both the total loss and the individual components over time.</p>
</li>
<li>
<p>Results of validation on training and test datasets.</p>
</li>
<li>
<p>Statistics about gradients and weights.</p>
</li>
<li>
<p>Values produced by the network. For example, if you are solving a classification problem, you definitely want to measure the entropy of predicted class probabilities. In the case of a regression problem, raw predicted values can give tons of data about the training.</p>
</li>
<li>
<p>Learning rates and other hyperparameters, if they are adjusted over time.</p>
</li>
</ul>
<p>The list could be much longer and include domain-specific metrics, such as word embedding projections, audio samples, and images generated by GANs. You also may want to monitor values related to training speed, like how long an epoch takes, to see the effect of your optimizations or problems with hardware.</p>
<p>To cut a long story short, you need a generic solution to track lots of values over time and represent them for analysis, preferably developed <span id="dx1-66002"/>especially for DL (just imagine looking at such statistics using an Excel spreadsheet). Luckily, such tools exist, and we will explore them next.</p>
<section class="level4 subsectionHead" id="tensorboard-101">
<h2 class="heading-2" id="sigil_toc_id_59"> <span id="x1-670003.7.1"/>TensorBoard 101</h2>
<p>When the first<span id="dx1-67001"/> edition of this book was written, there wasn’t too much choice for NN monitoring. As time has passed by and new people and companies have become involved with the pursuit of ML and DL, more new tools have appeared, for example, MLflow <a class="url" href="https://mlflow.org/"><span class="cmtt-10x-x-109">https://mlflow.org/</span></a>. In this book, we will still focus on the TensorBoard utility from TensorFlow, but you might consider trying other alternatives.</p>
<p>From the first public version, TensorFlow included a special tool called TensorBoard, which was developed to solve the problem we are talking about — how to observe and analyze various NN characteristics during and after the training. TensorBoard is a powerful, generic solution with a large community and it looks quite pretty:</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file9.png" width="360"/> <span id="x1-67002r4"/></p>
<span class="id">Figure 3.4: The TensorBoard web interface (for better visualization, refer to https://packt.link/gbp/9781835882702 ) </span>
</div>
<p>From the architecture point of view, TensorBoard is a Python web service that you can start on your computer, passing it the directory where your training process will save values to be analyzed. Then, you point your browser to TensorBoard’s port (usually <span class="cmtt-10x-x-109">6006</span>), and it shows you an interactive web interface with values updated in real time, as shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-67002r4"><span class="cmti-10x-x-109">3.4</span></a>. It’s nice and convenient, especially when your training is performed on a remote machine somewhere in the cloud.</p>
<p>Originally, TensorBoard was deployed as a part of TensorFlow, but after some time, it has been moved to a separate project (it’s still being maintained by Google) and it has its own package name. However, TensorBoard still uses the TensorFlow data format, so we will need to write this data from our PyTorch program. Several years ago, it required third-party libraries to be installed, but <span id="dx1-67003"/>nowadays, PyTorch already comes with support of this data format (available in the <span class="cmtt-10x-x-109">torch.utils.tensorboard</span> package).</p>
</section>
<section class="level4 subsectionHead" id="plotting-metrics">
<h2 class="heading-2" id="sigil_toc_id_60"> <span id="x1-680003.7.2"/>Plotting metrics</h2>
<p>To give you an <span id="dx1-68001"/>impression of how simple is to use TensorBoard, let’s consider a small example that is not related to NNs, but is just about writing values into TensorBoard (the full example code is in <span class="cmtt-10x-x-109">Chapter03/02</span><span class="cmtt-10x-x-109">_tensorboard.py</span>).</p>
<p>In the following code, we import the required packages, create a writer of data, and define functions that we are going to visualize:</p>
<div class="tcolorbox" id="tcolobox-35">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-53"><code>import math 
from torch.utils.tensorboard.writer import SummaryWriter 
 
if __name__ == "__main__": 
    writer = SummaryWriter() 
    funcs = {"sin": math.sin, "cos": math.cos, "tan": math.tan}</code></pre>
</div>
</div>
<p>By default, <span class="cmtt-10x-x-109">SummaryWriter </span>will create a unique directory in the <span class="cmtt-10x-x-109">runs </span>directory for every launch, to be able to compare different rounds of training. The name of the new directory includes the current date and time, and the hostname. To override this, you can pass the <span class="cmtt-10x-x-109">log</span><span class="cmtt-10x-x-109">_dir </span>argument to <span class="cmtt-10x-x-109">SummaryWriter</span>. You can also add a suffix to the name of the directory by passing a <span class="cmtt-10x-x-109">comment </span>argument, for example, to capture different experiments’ semantics, such as <span class="cmtt-10x-x-109">dropout=0.3 </span>or <span class="cmtt-10x-x-109">strong</span><span class="cmtt-10x-x-109">_regularisation</span>.</p>
<p>Next, we loop over angle ranges in degrees:</p>
<div class="tcolorbox" id="tcolobox-36">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-54"><code>    for angle in range(-360, 360): 
        angle_rad = angle * math.pi / 180 
        for name, fun in funcs.items(): 
            val = fun(angle_rad) 
            writer.add_scalar(name, val, angle) 
 
    writer.close()</code></pre>
</div>
</div>
<p>Here, we convert the angle ranges into radians and calculate our functions’ values. Every value is added to the writer using the <span class="cmtt-10x-x-109">add</span><span class="cmtt-10x-x-109">_scalar </span>function, which takes three arguments: the name of the parameter, its value, and the current iteration (which has to be an integer). The last thing we need to do after the loop is close the writer. Note that the writer does a periodical flush (by default, every two minutes), so even in the case of a lengthy optimization process, you will still see your values. If you need to flush <span class="cmtt-10x-x-109">SummaryWriter </span>data explicitly, it has the <span class="cmtt-10x-x-109">flush()</span> method.</p>
<p>The result of running this will be zero output on the console, but you will see a new directory created inside the <span class="cmtt-10x-x-109">runs </span>directory with a single file. To look at the result, we need to start TensorBoard:</p>
<pre class="lstlisting" id="listing-55"><code>Chapter03$ tensorboard --logdir runs 
TensorFlow installation not found - running with reduced feature set. 
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all 
TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)</code></pre>
<p>If you are running TensorBoard on a remote server, you will need to add<span id="dx1-68019"/> the <span class="cmtt-10x-x-109">--bind</span><span class="cmtt-10x-x-109">_all </span>command-line option to make it accessible from other machines. Now you can open <span class="cmtt-10x-x-109">http://localhost:6006 </span>in your browser to see something like this:</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file10.png" width="360"/> <span id="x1-68020r5"/></p>
<span class="id">Figure 3.5: Plots produced by the example (for better visualization, refer to https://packt.link/gbp/9781835882702 ) </span>
</div>
<p>The graphs are interactive, so you can hover over them with your mouse to see the actual values and select regions to zoom in and look at details. To zoom out, double-click inside the graph. If you run your program several times, then you will see several items in the <span class="cmbx-10x-x-109">Runs </span>list on the left, which can be enabled and disabled in any combination, allowing you to compare the dynamics of several optimizations. TensorBoard allows you to analyze not only scalar values but also images, audio, text data, and embeddings, and it can even show you the structure of your network. Refer to the documentation of TensorBoard for all those features. Now, it’s time to combine everything you learned in <span id="dx1-68021"/>this chapter and look at a real NN optimization problem using PyTorch.</p>
</section>
</section>
<section class="level3 sectionHead" id="gan-on-atari-images">
<h1 class="heading-1" id="sigil_toc_id_61"> <span id="x1-690003.8"/>GAN on Atari images</h1>
<p>Almost every <span id="dx1-69001"/>book <span id="dx1-69002"/>about DL uses the MNIST dataset to show you the power of DL, which, over the years, has made this dataset extremely boring, like a fruit fly for genetic researchers. To break this tradition, and add a bit more fun to the book, I’ve tried to avoid well-beaten paths and illustrate PyTorch using something different. I briefly referred to <span class="cmbx-10x-x-109">generative adversarial networks </span>(<span class="cmbx-10x-x-109">GANs</span>) earlier in the chapter. In this example, we will train a GAN to generate screenshots of various Atari games.</p>
<p>The simplest GAN architecture is this: we have two NNs where the first works <span id="dx1-69003"/>as a ”cheater” (it is also called the <span class="cmti-10x-x-109">generator</span>), and the other as a ”detective” (another name is the <span class="cmti-10x-x-109">discriminator</span>). Both networks compete with each other — the generator tries to generate fake data, which will be hard for the discriminator to distinguish from your dataset, and the discriminator tries to detect the generated data samples. Over time, both networks improve their skills — the generator produces more and more realistic data samples, and the discriminator invents more sophisticated ways to distinguish the fake items.</p>
<p>Practical usage of GANs includes image quality improvement, realistic image generation, and feature learning. In our example, practical usefulness is almost zero, but it will be a good showcase about everything we learned about PyTorch so far.</p>
<p>So, let’s get started. The whole example code is in the file <span class="cmtt-10x-x-109">Chapter03/03</span><span class="cmtt-10x-x-109">_atari</span><span class="cmtt-10x-x-109">_gan.py</span>. Here, we will look at only the most significant pieces of code, without the import section and constants declaration. The following class is a wrapper around a Gym game:</p>
<div class="tcolorbox" id="tcolobox-37">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-56"><code>class InputWrapper(gym.ObservationWrapper): 
    """ 
    Preprocessing of input numpy array: 
    1. resize image into predefined size 
    2. move color channel axis to a first place 
    """ 
    def __init__(self, *args): 
        super(InputWrapper, self).__init__(*args) 
        old_space = self.observation_space 
        assert isinstance(old_space, spaces.Box) 
        self.observation_space = spaces.Box( 
            self.observation(old_space.low), self.observation(old_space.high), 
            dtype=np.float32 
        ) 
 
    def observation(self, observation: gym.core.ObsType) -&gt; gym.core.ObsType: 
        # resize image 
        new_obs = cv2.resize( 
            observation, (IMAGE_SIZE, IMAGE_SIZE)) 
        # transform (w, h, c) -&gt; (c, w, h) 
        new_obs = np.moveaxis(new_obs, 2, 0) 
        return new_obs.astype(np.float32)</code></pre>
</div>
</div>
<p>The preceding class includes several transformations:</p>
<ul>
<li>
<p>Resize the input image from 210<span class="cmsy-10x-x-109">×</span>160 (the standard Atari resolution) to a square size of 64 <span class="cmsy-10x-x-109">× </span>64</p>
</li>
<li>
<p>Move the color plane of the image from the last position to the first, to meet the PyTorch convention of convolution layers that input a tensor with the shape of the channels, height, and width</p>
</li>
<li>
<p>Cast the image from <span class="cmtt-10x-x-109">bytes </span>to <span class="cmtt-10x-x-109">float</span></p>
</li>
</ul>
<p>Then, we define two <span class="cmtt-10x-x-109">nn.Module </span>classes: <span class="cmtt-10x-x-109">Discriminator </span>and <span class="cmtt-10x-x-109">Generator</span>. The first takes our scaled color image as input and, by applying five <span id="dx1-69026"/>layers<span id="dx1-69027"/> of convolutions, converts it into a single number passed through a <span class="cmtt-10x-x-109">Sigmoid </span>nonlinearity. The output from <span class="cmtt-10x-x-109">Sigmoid </span>is interpreted as the probability that <span class="cmtt-10x-x-109">Discriminator </span>thinks our input image is from the real dataset.</p>
<p><span class="cmtt-10x-x-109">Generator </span>takes as input a vector of random numbers (latent vector) and, by using the “transposed convolution” operation (it is also known<span id="dx1-69028"/> as deconvolution), converts this vector into a color image of the original resolution. We will not look at those classes here as they are lengthy and not very relevant to our example; you can find them in the complete example file.</p>
<p>As input, we will use screenshots from several Atari games played simultaneously by a random agent. <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-69029r6"><span class="cmti-10x-x-109">3.6</span></a> is an example of what the input data looks like.</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file11.png" width="360"/> <span id="x1-69029r6"/></p>
<span class="id">Figure 3.6: Sample screenshots from three Atari games </span>
</div>
<p>Images are combined in batches that are generated by the following function:</p>
<div class="tcolorbox" id="tcolobox-38">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-57"><code>def iterate_batches(envs: tt.List[gym.Env], 
                    batch_size: int = BATCH_SIZE) -&gt; tt.Generator[torch.Tensor, None, None]: 
    batch = [e.reset()[0] for e in envs] 
    env_gen = iter(lambda: random.choice(envs), None) 
 
    while True: 
        e = next(env_gen) 
        action = e.action_space.sample() 
        obs, reward, is_done, is_trunc, _ = e.step(action) 
        if np.mean(obs) &gt; 0.01: 
            batch.append(obs) 
        if len(batch) == batch_size: 
            batch_np = np.array(batch, dtype=np.float32) 
            # Normalising input to [-1..1] 
            yield torch.tensor(batch_np * 2.0 / 255.0 - 1.0) 
            batch.clear() 
        if is_done or is_trunc: 
            e.reset()</code></pre>
</div>
</div>
<p>This function infinitely samples the environment from the provided list, issues random actions, and remembers observations in the batch list. When the batch becomes of the required size, we normalize the image, convert <span id="dx1-69048"/>it to a tensor, and<span id="dx1-69049"/> yield from the generator. The check for the non-zero mean of the observation is required due to a bug in one of the games to prevent the flickering of an image.</p>
<p>Now, let’s look at our <span class="cmtt-10x-x-109">main </span>function, which prepares models and runs the training loop:</p>
<div class="tcolorbox" id="tcolobox-39">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-58"><code>if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("--dev", default="cpu", help="Device name, default=cpu") 
    args = parser.parse_args() 
 
    device = torch.device(args.dev) 
    envs = [ 
        InputWrapper(gym.make(name)) 
        for name in (’Breakout-v4’, ’AirRaid-v4’, ’Pong-v4’) 
    ] 
    shape = envs[0].observation_space.shape</code></pre>
</div>
</div>
<p>Here, we process the command-line arguments (which could be only one optional argument, <span class="cmtt-10x-x-109">--dev</span>, which specifies the device to use for computations) and create our environment pool with a wrapper applied. This environment array will be passed to the <span class="cmtt-10x-x-109">iterate</span><span class="cmtt-10x-x-109">_batches </span>function later to generate training data.</p>
<p>In the following piece, we create our classes — a summary writer, both networks, a loss function, and two optimizers:</p>
<div class="tcolorbox" id="tcolobox-40">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-59"><code>    net_discr = Discriminator(input_shape=shape).to(device) 
    net_gener = Generator(output_shape=shape).to(device) 
 
    objective = nn.BCELoss() 
    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, 
                               betas=(0.5, 0.999)) 
    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, 
                               betas=(0.5, 0.999)) 
    writer = SummaryWriter()</code></pre>
</div>
</div>
<p>Why do we need two optimizers? It’s because that’s the way that GANs get trained: to train the discriminator, we need to show it both real and fake data samples with appropriate labels (1 for real and 0 for fake). During this pass, we update only the discriminator’s parameters.</p>
<p>After that, we pass both real and fake samples through the discriminator<span id="dx1-69070"/> again, but this time, the<span id="dx1-69071"/> labels are 1s for all samples and we update only the generator’s weights. The second pass teaches the generator how to fool the discriminator and confuse real samples with the generated ones.</p>
<p>We then define arrays, which will be used to accumulate losses, iterator counters, and variables with the true and fake labels. We also store the current timestamp to report the time elapsed after 100 iterations of training:</p>
<div class="tcolorbox" id="tcolobox-41">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-60"><code>    gen_losses = [] 
    dis_losses = [] 
    iter_no = 0 
 
    true_labels_v = torch.ones(BATCH_SIZE, device=device) 
    fake_labels_v = torch.zeros(BATCH_SIZE, device=device) 
    ts_start = time.time()</code></pre>
</div>
</div>
<p>At the beginning of the following training loop, we generate a random vector and pass it to the <span class="cmtt-10x-x-109">Generator </span>network:</p>
<div class="tcolorbox" id="tcolobox-42">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-61"><code>    for batch_v in iterate_batches(envs): 
        # fake samples, input is 4D: batch, filters, x, y 
        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1) 
        gen_input_v.normal_(0, 1) 
        gen_input_v = gen_input_v.to(device) 
        batch_v = batch_v.to(device) 
        gen_output_v = net_gener(gen_input_v)</code></pre>
</div>
</div>
<p>We then train the discriminator by applying it two times, once to the true data samples in our batch and once to the generated ones:</p>
<div class="tcolorbox" id="tcolobox-43">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-62"><code>        dis_optimizer.zero_grad() 
        dis_output_true_v = net_discr(batch_v) 
        dis_output_fake_v = net_discr(gen_output_v.detach()) 
        dis_loss = objective(dis_output_true_v, true_labels_v) + \ 
                   objective(dis_output_fake_v, fake_labels_v) 
        dis_loss.backward() 
        dis_optimizer.step() 
        dis_losses.append(dis_loss.item())</code></pre>
</div>
</div>
<p>In the preceding code, we need to call the <span class="cmtt-10x-x-109">detach() </span>function on the generator’s output to prevent <span id="dx1-69094"/>gradients of this training <span id="dx1-69095"/>pass from flowing into the generator (<span class="cmtt-10x-x-109">detach() </span>is a method of tensor, which makes a copy of it without connection to the parent’s operation, i.e., detaching the tensor from the parent’s graph).</p>
<p>Now it’s the generator’s training time:</p>
<div class="tcolorbox" id="tcolobox-44">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-63"><code>        gen_optimizer.zero_grad() 
        dis_output_v = net_discr(gen_output_v) 
        gen_loss_v = objective(dis_output_v, true_labels_v) 
        gen_loss_v.backward() 
        gen_optimizer.step() 
        gen_losses.append(gen_loss_v.item())</code></pre>
</div>
</div>
<p>We pass the generator’s output to the discriminator, but now we don’t stop the gradients. Instead, we apply the objective function with <span class="cmtt-10x-x-109">True </span>labels. It will push our generator in the direction where the samples that it generates make the discriminator confuse them with the real data. That was the code related to training, and the next couple of lines report losses and feed image samples to TensorBoard:</p>
<div class="tcolorbox" id="tcolobox-45">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-64"><code>        iter_no += 1 
        if iter_no % REPORT_EVERY_ITER == 0: 
            dt = time.time() - ts_start 
            log.info("Iter %d in %.2fs: gen_loss=%.3e, dis_loss=%.3e", 
                     iter_no, dt, np.mean(gen_losses), np.mean(dis_losses)) 
            ts_start = time.time() 
            writer.add_scalar("gen_loss", np.mean(gen_losses), iter_no) 
            writer.add_scalar("dis_loss", np.mean(dis_losses), iter_no) 
            gen_losses = [] 
            dis_losses = [] 
        if iter_no % SAVE_IMAGE_EVERY_ITER == 0: 
            img = vutils.make_grid(gen_output_v.data[:64], normalize=True) 
            writer.add_image("fake", img, iter_no) 
            img = vutils.make_grid(batch_v.data[:64], normalize=True) 
            writer.add_image("real", img, iter_no)</code></pre>
</div>
</div>
<p>The training of this example is quite a lengthy<span id="dx1-69117"/> process. On <span id="dx1-69118"/>a GTX 1080Ti GPU, 100 iterations take about 2.7 seconds. At the beginning, the generated images are completely random noise, but after 10k–20k iterations, the generator becomes more and more proficient at its job and the generated images become more and more similar to the real game screenshots.</p>
<div class="tcolorbox infobox" id="tcolobox-46">
<div class="tcolorbox-content">
<p>It also worth noting the performance improvement in software libraries. In the first and second editions of the book, exactly the same example ran much slower on the same hardware I have. On GTX 1080Ti, 100 iterations took around 40 seconds. Now, with PyTorch 2.2.0 on exactly the same GPU, 100 iterations take 2.7 seconds. So, instead of 3–4 hours, it now takes about 30 minutes to get good generated images.</p>
</div>
</div>
<p>My experiments gave the following images after 40k–50k of training iterations (about half an hour on a 1080 GPU):</p>
<div class="minipage">
<p><img alt="PIC" height="288" src="../Images/file12.png" width="288"/> <span id="x1-69119r7"/></p>
<span class="id">Figure 3.7: Sample images produced by the generator network </span>
</div>
<p>As you can see, our network was able to reproduce the <span id="dx1-69120"/>Atari screenshots<span id="dx1-69121"/> quite well. In the next section, we’ll look at how we can simplify our code by using the add-on PyTorch library, Ignite.</p>
</section>
<section class="level3 sectionHead" id="pytorch-ignite">
<h1 class="heading-1" id="sigil_toc_id_62"> <span id="x1-700003.9"/>PyTorch Ignite</h1>
<p>PyTorch is an <span id="dx1-70001"/>elegant and flexible library, which makes it a favorite choice for thousands of researchers, DL enthusiasts, industry developers, and others. But flexibility has its own price: too much code to be written to solve your problem. Sometimes, this is very beneficial, such as when implementing some new optimization method or DL trick that hasn’t been included in the standard library yet. Then you just implement the formulas using Python and PyTorch magic will do all the gradient and backpropagation machinery for you. Another example is in situations when you have to work on a very low level, fiddling with gradients, optimizer details, and the way your data is transformed by the NN.</p>
<p>However, sometimes you don’t need this flexibility, which happens when you work on routine tasks, like the simple supervised training of an image classifier. For such tasks, standard PyTorch might be at too low a level when you need to deal with the same code over and over again. The following is a non-exhaustive list of topics that are an essential part of any DL training procedure, but require some code to be written:</p>
<ul>
<li>
<p>Data preparation and transformation, and the generation of batches</p>
</li>
<li>
<p>Calculation of training metrics, like loss values, accuracy, and F1-scores</p>
</li>
<li>
<p>Periodical testing of the model being trained on the test and validation datasets</p>
</li>
<li>
<p>Model checkpointing after some number of iterations or when a new best metric is achieved</p>
</li>
<li>
<p>Sending metrics into a monitoring tool like TensorBoard</p>
</li>
<li>
<p>Hyperparameters change over time, like a learning rate decrease/increase schedule</p>
</li>
<li>
<p>Writing training progress messages on the console</p>
</li>
</ul>
<p>They are all doable using only PyTorch, of course, but it might require you to write a significant amount of code. As those tasks occur in any DL project, it quickly becomes cumbersome to write the same code over and over again. The normal approach to solving the issue is to write the functionality once, wrap it into a library, and reuse it later. If the library is open source and of good quality (easy to use, provides a good level of flexibility, written properly, and so on), it will become popular as more and more people use it in their projects. This process is not DL-specific; it happens everywhere in the software industry.</p>
<p>There are several libraries for PyTorch that simplify the solving of common tasks: <span class="cmtt-10x-x-109">ptlearn</span>, <span class="cmtt-10x-x-109">fastai</span>, <span class="cmtt-10x-x-109">ignite</span>, and some others. The current list of “PyTorch ecosystem projects” can be found here: <a class="url" href="https://pytorch.org/ecosystem"><span class="cmtt-10x-x-109">https://pytorch.org/ecosystem</span></a>.</p>
<p>It might be appealing to start using those high-level libraries from the beginning, as they allow you to solve common problems with just a couple of lines of code, but there is some danger here. If you only know how to use high-level libraries without understanding low-level details, you might get stuck on problems that can’t be solved solely by standard methods. In the very dynamic field of ML, this happens very often.</p>
<p>The main focus of this book is to ensure that you understand RL methods, their implementation, and their applicability, so we will use an incremental approach. In the beginning, we will implement methods using only PyTorch code, but with more progress, examples will be implemented using high-level libraries. For RL, this will be the small library written by me: PTAN (<a class="url" href="https://github.com/Shmuma/ptan/"><span class="cmtt-10x-x-109">https://github.com/Shmuma/ptan/</span></a>), and it will be introduced in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch011.xhtml#x1-1070007"><span class="cmti-10x-x-109">7</span></a>.</p>
<p>To reduce the amount of DL boilerplate code, we will use a library called PyTorch Ignite: <a class="url" href="https://pytorch-ignite.ai"><span class="cmtt-10x-x-109">https://pytorch-ignite.ai</span></a>. In this section, a small<span id="dx1-70002"/> overview of Ignite will be given, then we will check the Atari GAN example once it has been rewritten using Ignite.</p>
<section class="level4 subsectionHead" id="ignite-concepts">
<h2 class="heading-2" id="sigil_toc_id_63"> <span id="x1-710003.9.1"/>Ignite concepts</h2>
<p>At a high <span id="dx1-71001"/>level, Ignite simplifies the writing of the training loop in PyTorch DL. Earlier in this chapter (in the <span class="cmti-10x-x-109">Loss functions and optimizers </span>section, you saw that the minimal training loop consists of:</p>
<ul>
<li>
<p>Sampling a batch of training data</p>
</li>
<li>
<p>Applying an NN to this batch to calculate the loss function—the single value we want to minimize</p>
</li>
<li>
<p>Running backpropagation of the loss to get gradients on the network’s parameters in respect to the loss</p>
</li>
<li>
<p>Asking the optimizer to apply the gradients to the network</p>
</li>
<li>
<p>Repeating until we are happy or bored of waiting</p>
</li>
</ul>
<p>The central piece of Ignite is the <span class="cmtt-10x-x-109">Engine </span>class, which loops over the data source, applying the processing function to the data batch. In addition to that, Ignite offers the ability to provide functions to be called at specific conditions of the training loop. Those conditions are called <span class="cmtt-10x-x-109">Events </span>and could be at the:</p>
<ul>
<li>
<p>Beginning/end of the whole training process</p>
</li>
<li>
<p>Beginning/end of a training epoch (iteration over the data)</p>
</li>
<li>
<p>Beginning/end of a single batch processing</p>
</li>
</ul>
<p>In addition to that, custom events exist and allow you to specify your function to be called every <span class="cmmi-10x-x-109">N </span>events, for example, if you want to do some calculations every 100 batches or every second epoch.</p>
<p>A very simplistic example of Ignite in action is shown in the following code block:</p>
<div class="tcolorbox" id="tcolobox-47">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-65"><code>from ignite.engine import Engine, Events 
 
def training(engine, batch): 
    optimizer.zero_grad() 
    x, y = prepare_batch() 
    y_out = model(x) 
    loss = loss_fn(y_out, y) 
    loss.backward() 
    optimizer.step() 
    return loss.item() 
 
engine = Engine(training) 
engine.run(data)</code></pre>
</div>
</div>
<p>This code is not runnable as it misses lots of details, like the data source, model, and optimizer creation, but it shows the basic idea of Ignite usage. The main benefit of Ignite is in the ability it provides to extend the training loop with existing functionality. You want the loss value to be smoothed and written in TensorBoard every 100 batches? No problem! Add two lines and it will be done. You want to run model validation every 10 epochs? Okay, write a <span id="dx1-71015"/>function to run a test and attach it to the <span class="cmtt-10x-x-109">Engine </span>instance, and it will be called.</p>
<p>A description of the full Ignite functionality is beyond the scope of the book, but you can read the documentation on the official website: <a class="url" href="https://pytorch-ignite.ai"><span class="cmtt-10x-x-109">https://pytorch-ignite.ai</span></a>.</p>
</section>
<section class="level4 subsectionHead" id="gan-training-on-atari-using-ignite">
<h2 class="heading-2" id="sigil_toc_id_64"> <span id="x1-720003.9.2"/>GAN training on Atari using Ignite</h2>
<p>To give you <span id="dx1-72001"/>an<span id="dx1-72002"/> illustration <span id="dx1-72003"/>of Ignite, let’s change the example of GAN training on Atari images. The full example code is available in <span class="cmtt-10x-x-109">Chapter03/04</span><span class="cmtt-10x-x-109">_atari</span><span class="cmtt-10x-x-109">_gan</span><span class="cmtt-10x-x-109">_ignite.py</span>; here, I will just show code that differs from the previous section.</p>
<p>First, we import several Ignite classes:</p>
<div class="tcolorbox" id="tcolobox-48">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-66"><code>from ignite.engine import Engine, Events 
from ignite.handlers import Timer 
from ignite.metrics import RunningAverage 
from ignite.contrib.handlers import tensorboard_logger as tb_logger</code></pre>
</div>
</div>
<p>The <span class="cmtt-10x-x-109">Engine </span>and <span class="cmtt-10x-x-109">Events </span>classes have already been outlined. The package <span class="cmtt-10x-x-109">ignite.metrics </span>contains classes related to working with the performance metrics of the training process, such as confusion matrices, precision, and recall. In our example, we will use the class <span class="cmtt-10x-x-109">RunningAverage</span>, which provides a way to smooth time series values. In the previous example, we did this by calling <span class="cmtt-10x-x-109">np.mean() </span>on an array of losses, but <span class="cmtt-10x-x-109">RunningAverage </span>provides a more convenient (and mathematically more correct) way of doing this. In addition, we import the TensorBoard logger from the Ignite <span class="cmtt-10x-x-109">contrib </span>package (the functionality of which is contributed by others). We’ll also use the <span class="cmtt-10x-x-109">Timer </span>handler, which<span id="dx1-72008"/> provides a <span id="dx1-72009"/>simple way to <span id="dx1-72010"/>calculate time elapsed between certain events.</p>
<p>As a next step, we need to define our processing function:</p>
<div class="tcolorbox" id="tcolobox-49">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-67"><code>    def process_batch(trainer, batch): 
        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1) 
        gen_input_v.normal_(0, 1) 
        gen_input_v = gen_input_v.to(device) 
        batch_v = batch.to(device) 
        gen_output_v = net_gener(gen_input_v) 
 
        # train discriminator 
        dis_optimizer.zero_grad() 
        dis_output_true_v = net_discr(batch_v) 
        dis_output_fake_v = net_discr(gen_output_v.detach()) 
        dis_loss = objective(dis_output_true_v, true_labels_v) + \ 
                   objective(dis_output_fake_v, fake_labels_v) 
        dis_loss.backward() 
        dis_optimizer.step() 
 
        # train generator 
        gen_optimizer.zero_grad() 
        dis_output_v = net_discr(gen_output_v) 
        gen_loss = objective(dis_output_v, true_labels_v) 
        gen_loss.backward() 
        gen_optimizer.step() 
 
        if trainer.state.iteration % SAVE_IMAGE_EVERY_ITER == 0: 
            fake_img = vutils.make_grid(gen_output_v.data[:64], normalize=True) 
            trainer.tb.writer.add_image("fake", fake_img, trainer.state.iteration) 
            real_img = vutils.make_grid(batch_v.data[:64], normalize=True) 
            trainer.tb.writer.add_image("real", real_img, trainer.state.iteration) 
            trainer.tb.writer.flush() 
        return dis_loss.item(), gen_loss.item()</code></pre>
</div>
</div>
<p>This function takes the data batch and does an update of both the discriminator and generator models on this batch. This function can return any data to be tracked during the training process; in our case, it will be two loss values for both models. In this function, we can also save images to be displayed in TensorBoard.</p>
<p>After this is done, all we need to do is create an <span class="cmtt-10x-x-109">Engine </span>instance, attach the required handlers, and run the training process:</p>
<div class="tcolorbox" id="tcolobox-50">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-68"><code>    engine = Engine(process_batch) 
    tb = tb_logger.TensorboardLogger(log_dir=None) 
    engine.tb = tb 
    RunningAverage(output_transform=lambda out: out[1]).\ 
        attach(engine, "avg_loss_gen") 
    RunningAverage(output_transform=lambda out: out[0]).\ 
        attach(engine, "avg_loss_dis") 
 
    handler = tb_logger.OutputHandler(tag="train", metric_names=[’avg_loss_gen’, ’avg_loss_dis’]) 
    tb.attach(engine, log_handler=handler, event_name=Events.ITERATION_COMPLETED) 
 
    timer = Timer() 
    timer.attach(engine)</code></pre>
</div>
</div>
<p>In the preceding code, we<span id="dx1-72054"/> create our engine, passing <span id="dx1-72055"/>our <span id="dx1-72056"/>processing function and attaching two <span class="cmtt-10x-x-109">RunningAverage </span>transformations for our two loss values. Being attached, every <span class="cmtt-10x-x-109">RunningAverage </span>produces a so-called “metric” — a derived value kept around during the training process. The names of our smoothed metrics are <span class="cmtt-10x-x-109">avg</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_gen </span>for smoothed loss from the generator, and <span class="cmtt-10x-x-109">avg</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dis </span>for smoothed loss from the discriminator. Those two values will be written in TensorBoard after every iteration.</p>
<p>We also attach the timer, which, being created without any constructor arguments, acts as a simple manually-controlled timer (we call its <span class="cmtt-10x-x-109">reset()</span> method manually), but can work in a more flexible way with different configuration options.</p>
<p>The last piece of code attaches another event handler, which will be our function, and is called by the <span class="cmtt-10x-x-109">Engine </span>on every iteration completion:</p>
<div class="tcolorbox" id="tcolobox-51">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-69"><code>    @engine.on(Events.ITERATION_COMPLETED) 
    def log_losses(trainer): 
        if trainer.state.iteration % REPORT_EVERY_ITER == 0: 
            log.info("%d in %.2fs: gen_loss=%f, dis_loss=%f", 
                     trainer.state.iteration, timer.value(), 
                     trainer.state.metrics[’avg_loss_gen’], 
                     trainer.state.metrics[’avg_loss_dis’]) 
            timer.reset() 
 
    engine.run(data=iterate_batches(envs))</code></pre>
</div>
</div>
<p>It will write a log line with an iteration index, time taken and values of smoothed metrics. The final line starts our engine, passing the already defined function as the data source (the <span class="cmtt-10x-x-109">iterate</span><span class="cmtt-10x-x-109">_batches </span>function is a generator, returning the normal iterator over batches, so, it will be perfectly fine to pass its output as a data argument). And that’s it. If you run the <span class="cmtt-10x-x-109">Chapter03/04</span><span class="cmtt-10x-x-109">_atari</span><span class="cmtt-10x-x-109">_gan</span><span class="cmtt-10x-x-109">_ignite.py </span>example, it will work the same way as our previous example, which might not be very impressive for such a<span id="dx1-72067"/> small example, but in real projects, Ignite <span id="dx1-72068"/>usage<span id="dx1-72069"/> normally pays off by making your code cleaner and more extensible.</p>
</section>
</section>
<section class="level3 sectionHead" id="summary-2">
<h1 class="heading-1" id="sigil_toc_id_65"> <span id="x1-730003.10"/>Summary</h1>
<p>In this chapter, you saw a quick overview of PyTorch’s functionality and features. We talked about basic fundamental pieces, such as tensors and gradients, and you saw how an NN can be made from the basic building blocks, before learning how to implement those blocks yourself.</p>
<p>We discussed loss functions and optimizers, as well as the monitoring of training dynamics. Finally, you were introduced to PyTorch Ignite, a library used to provide a higher-level interface for training loops. The goal of the chapter was to give a very quick introduction to PyTorch, which will be used later in the book.</p>
<p>In the next chapter, we are ready to start dealing with the main subject of this book: RL methods.</p>
</section>
</section>
</div></body></html>