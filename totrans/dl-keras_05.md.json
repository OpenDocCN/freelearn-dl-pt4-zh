["```py\nfrom keras.layers import Merge\nfrom keras.layers.core import Dense, Reshape\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Sequential\n\nvocab_size = 5000\nembed_size = 300\n\n```", "```py\nword_model = Sequential()\nword_model.add(Embedding(vocab_size, embed_size,\n                         embeddings_initializer=\"glorot_uniform\",\n                         input_length=1))\nword_model.add(Reshape((embed_size, )))\n\n```", "```py\ncontext_model = Sequential()\ncontext_model.add(Embedding(vocab_size, embed_size,\n                  embeddings_initializer=\"glorot_uniform\",\n                  input_length=1))\ncontext_model.add(Reshape((embed_size,)))\n\n```", "```py\nmodel = Sequential()\nmodel.add(Merge([word_model, context_model], mode=\"dot\"))\nmodel.add(Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\"))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n\n```", "```py\nfrom keras.preprocessing.text import *\nfrom keras.preprocessing.sequence import skipgrams\n\ntext = \"I love green eggs and ham .\"\n\n```", "```py\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\n\n```", "```py\nword2id = tokenizer.word_index\nid2word = {v:k for k, v in word2id.items()}\n\n```", "```py\nwids = [word2id[w] for w in text_to_word_sequence(text)]\npairs, labels = skipgrams(wids, len(word2id))\nprint(len(pairs), len(labels))\nfor i in range(10):\n    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n          id2word[pairs[i][0]], pairs[i][0], \n          id2word[pairs[i][1]], pairs[i][1], \n          labels[i]))\n\n```", "```py\n(and (1), ham (3)) -> 0\n(green (6), i (4)) -> 0\n(love (2), i (4)) -> 1\n(and (1), love (2)) -> 0\n(love (2), eggs (5)) -> 0\n(ham (3), ham (3)) -> 0\n(green (6), and (1)) -> 1\n(eggs (5), love (2)) -> 1\n(i (4), ham (3)) -> 0\n(and (1), green (6)) -> 1 \n\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.embeddings import Embedding\nimport keras.backend as K\n\nvocab_size = 5000\nembed_size = 300\nwindow_size = 1\n\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embed_size, \n                    embeddings_initializer='glorot_uniform',\n                    input_length=window_size*2))\nmodel.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=  (embed_size,)))\nmodel.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n\n```", "```py\nmerge_layer = model.layers[0]\nword_model = merge_layer.layers[0]\nword_embed_layer = word_model.layers[0]\nweights = word_embed_layer.get_weights()[0]\n\n```", "```py\nweights = model.layers[0].get_weights()[0]\n\n```", "```py\nfrom gensim.models import KeyedVectors\nimport logging\nimport os\n\n```", "```py\nclass Text8Sentences(object):\n  def __init__(self, fname, maxlen):\n    self.fname = fname\n    self.maxlen = maxlen\n\n  def __iter__(self):\n    with open(os.path.join(DATA_DIR, \"text8\"), \"rb\") as ftext:\n      text = ftext.read().split(\" \")\n      sentences, words = [], []\n      for word in text:\n        if len(words) >= self.maxlen:\n          yield words\n          words = []\n          words.append(word)\n          yield words\n\n```", "```py\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nDATA_DIR = \"../data/\"\nsentences = Text8Sentences(os.path.join(DATA_DIR, \"text8\"), 50)\nmodel = word2vec.Word2Vec(sentences, size=300, min_count=30)\n\n```", "```py\nmodel.init_sims(replace=True)\nmodel.save(\"word2vec_gensim.bin\")\n\n```", "```py\nmodel = Word2Vec.load(\"word2vec_gensim.bin\")\n\n```", "```py\n>>> model.vocab.keys()[0:10]\n['homomorphism',\n'woods',\n'spiders',\n'hanging',\n'woody',\n'localized',\n'sprague',\n'originality',\n'alphabetic',\n'hermann']\n\n```", "```py\n>>> model[\"woman\"]\n array([ -3.13099056e-01, -1.85702944e+00, 1.18816841e+00,\n -1.86561719e-01, -2.23673001e-01, 1.06527400e+00,\n &mldr;\n 4.31755871e-01, -2.90115297e-01, 1.00955181e-01,\n -5.17173052e-01, 7.22485244e-01, -1.30940580e+00], dtype=”float32”)\n\n```", "```py\n>>> model.most_similar(\"woman\")\n [('child', 0.7057571411132812),\n ('girl', 0.702182412147522),\n ('man', 0.6846336126327515),\n ('herself', 0.6292711496353149),\n ('lady', 0.6229539513587952),\n ('person', 0.6190367937088013),\n ('lover', 0.6062309741973877),\n ('baby', 0.5993420481681824),\n ('mother', 0.5954475402832031),\n ('daughter', 0.5871444940567017)]\n\n```", "```py\n>>> model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n [('queen', 0.6237582564353943),\n ('prince', 0.5638638734817505),\n ('elizabeth', 0.5557916164398193),\n ('princess', 0.5456407070159912),\n ('throne', 0.5439794063568115),\n ('daughter', 0.5364126563072205),\n ('empress', 0.5354889631271362),\n ('isabella', 0.5233952403068542),\n ('regent', 0.520746111869812),\n ('matilda', 0.5167444944381714)]\n\n```", "```py\n>>> model.similarity(\"girl\", \"woman\")\n 0.702182479574\n >>> model.similarity(\"girl\", \"man\")\n 0.574259909834\n >>> model.similarity(\"girl\", \"car\")\n 0.289332921793\n >>> model.similarity(\"bus\", \"car\")\n 0.483853497748\n\n```", "```py\nfrom keras.layers.core import Dense, Dropout, SpatialDropout1D\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.pooling import GlobalMaxPooling1D\nfrom kera\ns.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\n\nnp.random.seed(42)\n\n```", "```py\nINPUT_FILE = \"../data/umich-sentiment-train.txt\"\nVOCAB_SIZE = 5000\nEMBED_SIZE = 100\nNUM_FILTERS = 256\nNUM_WORDS = 3\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\n\n```", "```py\ncounter = collections.Counter()\nfin = open(INPUT_FILE, \"rb\")\nmaxlen = 0\nfor line in fin:\n    _, sent = line.strip().split(\"t\")\n    words = [x.lower() for x in   nltk.word_tokenize(sent)]\n    if len(words) > maxlen:\n        maxlen = len(words)\n    for word in words:\n        counter[word] += 1\nfin.close()\n\nword2index = collections.defaultdict(int)\nfor wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n    word2index[word[0]] = wid + 1\nvocab_size = len(word2index) + 1\nindex2word = {v:k for k, v in word2index.items()}\n\n```", "```py\nxs, ys = [], []\nfin = open(INPUT_FILE, \"rb\")\nfor line in fin:\n    label, sent = line.strip().split(\"t\")\n    ys.append(int(label))\n    words = [x.lower() for x in nltk.word_tokenize(sent)]\n    wids = [word2index[word] for word in words]\n    xs.append(wids)\nfin.close()\nX = pad_sequences(xs, maxlen=maxlen)\nY = np_utils.to_categorical(ys)\n\n```", "```py\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, EMBED_SIZE, input_length=maxlen)\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,\nactivation=\"relu\"))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(2, activation=\"softmax\"))\n\n```", "```py\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n              metrics=[\"accuracy\"])\nhistory = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n                    epochs=NUM_EPOCHS,\n                    validation_data=(Xtest, Ytest))\n\n```", "```py\nfrom gensim.models import KeyedVectors\nfrom keras.layers.core import Dense, Dropout, SpatialDropout1D\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.pooling import GlobalMaxPooling1D\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\n\nnp.random.seed(42)\n\n```", "```py\nINPUT_FILE = \"../data/umich-sentiment-train.txt\"\nWORD2VEC_MODEL = \"../data/GoogleNews-vectors-negative300.bin.gz\"\nVOCAB_SIZE = 5000\nEMBED_SIZE = 300\nNUM_FILTERS = 256\nNUM_WORDS = 3\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\n\n```", "```py\ncounter = collections.Counter()\nfin = open(INPUT_FILE, \"rb\")\nmaxlen = 0\nfor line in fin:\n   _, sent = line.strip().split(\"t\")\n   words = [x.lower() for x in nltk.word_tokenize(sent)]\n   if len(words) > maxlen:\n       maxlen = len(words)\n   for word in words:\n       counter[word] += 1\nfin.close()\n\nword2index = collections.defaultdict(int)\nfor wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n    word2index[word[0]] = wid + 1\nvocab_sz = len(word2index) + 1\nindex2word = {v:k for k, v in word2index.items()}\n\nxs, ys = [], []\nfin = open(INPUT_FILE, \"rb\")\nfor line in fin:\n    label, sent = line.strip().split(\"t\")\n    ys.append(int(label))\n    words = [x.lower() for x in nltk.word_tokenize(sent)]\n    wids = [word2index[word] for word in words]\n    xs.append(wids)\nfin.close()\nX = pad_sequences(xs, maxlen=maxlen)\nY = np_utils.to_categorical(ys)\n\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3,\n     random_state=42)\n\n```", "```py\n# load word2vec model\nword2vec = Word2Vec.load_word2vec_format(WORD2VEC_MODEL, binary=True)\nembedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\nfor word, index in word2index.items():\n    try:\n        embedding_weights[index, :] = word2vec[word]\n    except KeyError:\n        pass\n\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n          weights=[embedding_weights]))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\nmodel.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,\n                        activation=\"relu\"))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(2, activation=\"softmax\"))\n\n```", "```py\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n                    epochs=NUM_EPOCHS,\n                    validation_data=(Xtest, Ytest))\n\nscore = model.evaluate(Xtest, Ytest, verbose=1)\nprint(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n\n```", "```py\n((4960, 42), (2126, 42), (4960, 2), (2126, 2))\n Train on 4960 samples, validate on 2126 samples\n Epoch 1/10\n 4960/4960 [==============================] - 7s - loss: 0.1766 - acc: 0.9369 - val_loss: 0.0397 - val_acc: 0.9854\n Epoch 2/10\n 4960/4960 [==============================] - 7s - loss: 0.0725 - acc: 0.9706 - val_loss: 0.0346 - val_acc: 0.9887\n Epoch 3/10\n 4960/4960 [==============================] - 7s - loss: 0.0553 - acc: 0.9784 - val_loss: 0.0210 - val_acc: 0.9915\n Epoch 4/10\n 4960/4960 [==============================] - 7s - loss: 0.0519 - acc: 0.9790 - val_loss: 0.0241 - val_acc: 0.9934\n Epoch 5/10\n 4960/4960 [==============================] - 7s - loss: 0.0576 - acc: 0.9746 - val_loss: 0.0219 - val_acc: 0.9929\n Epoch 6/10\n 4960/4960 [==============================] - 7s - loss: 0.0515 - acc: 0.9764 - val_loss: 0.0185 - val_acc: 0.9929\n Epoch 7/10\n 4960/4960 [==============================] - 7s - loss: 0.0528 - acc: 0.9790 - val_loss: 0.0204 - val_acc: 0.9920\n Epoch 8/10\n 4960/4960 [==============================] - 7s - loss: 0.0373 - acc: 0.9849 - val_loss: 0.0221 - val_acc: 0.9934\n Epoch 9/10\n 4960/4960 [==============================] - 7s - loss: 0.0360 - acc: 0.9845 - val_loss: 0.0194 - val_acc: 0.9929\n Epoch 10/10\n 4960/4960 [==============================] - 7s - loss: 0.0389 - acc: 0.9853 - val_loss: 0.0254 - val_acc: 0.9915\n 2126/2126 [==============================] - 1s\n Test score: 0.025, accuracy: 0.993\n\n```", "```py\nGLOVE_MODEL = \"../data/glove.6B.300d.txt\"\nword2emb = {}\nfglove = open(GLOVE_MODEL, \"rb\")\nfor line in fglove:\n    cols = line.strip().split()\n    word = cols[0]\n    embedding = np.array(cols[1:], dtype=\"float32\")\n    word2emb[word] = embedding\nfglove.close()\n\n```", "```py\nembedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\nfor word, index in word2index.items():\n    try:\n        embedding_weights[index, :] = word2emb[word]\n    except KeyError:\n        pass\n\n```", "```py\nmodel.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n                     weights=[embedding_weights],\n                     trainable=False))\nmodel.add(SpatialDropout1D(Dropout(0.2)))\n\n```", "```py\nfrom keras.layers.core import Dense, Dropout, SpatialDropout1D\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport collections\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\n\nnp.random.seed(42)\n\nINPUT_FILE = \"../data/umich-sentiment-train.txt\"\nGLOVE_MODEL = \"../data/glove.6B.100d.txt\"\nVOCAB_SIZE = 5000\nEMBED_SIZE = 100\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\n\n```", "```py\ncounter = collections.Counter()\nfin = open(INPUT_FILE, \"rb\")\nmaxlen = 0\nfor line in fin:\n    _, sent = line.strip().split(\"t\")\n    words = [x.lower() for x in nltk.word_tokenize(sent)]\n    if len(words) > maxlen:\n        maxlen = len(words)\n    for word in words:\n        counter[word] += 1\nfin.close()\n\nword2index = collections.defaultdict(int)\nfor wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n     word2index[word[0]] = wid + 1\nvocab_sz = len(word2index) + 1\nindex2word = {v:k for k, v in word2index.items()}\nindex2word[0] = \"_UNK_\"\n\nws, ys = [], []\nfin = open(INPUT_FILE, \"rb\")\nfor line in fin:\n    label, sent = line.strip().split(\"t\")\n    ys.append(int(label))\n    words = [x.lower() for x in nltk.word_tokenize(sent)]\n    wids = [word2index[word] for word in words]\n    ws.append(wids)\nfin.close()\nW = pad_sequences(ws, maxlen=maxlen)\nY = np_utils.to_categorical(ys)\n\n```", "```py\nword2emb = collections.defaultdict(int)\nfglove = open(GLOVE_MODEL, \"rb\")\nfor line in fglove:\n    cols = line.strip().split()\n    word = cols[0]\n    embedding = np.array(cols[1:], dtype=\"float32\")\n    word2emb[word] = embedding\nfglove.close()\n\n```", "```py\nX = np.zeros((W.shape[0], EMBED_SIZE))\nfor i in range(W.shape[0]):\n    E = np.zeros((EMBED_SIZE, maxlen))\n    words = [index2word[wid] for wid in W[i].tolist()]\n    for j in range(maxlen):\n         E[:, j] = word2emb[words[j]]\n    X[i, :] = np.sum(E, axis=1)\n\n```", "```py\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=100, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n                    epochs=NUM_EPOCHS,\n                    validation_data=(Xtest, Ytest))\n\nscore = model.evaluate(Xtest, Ytest, verbose=1)\nprint(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n\n```"]