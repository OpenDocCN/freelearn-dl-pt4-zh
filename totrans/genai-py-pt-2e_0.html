<html><head></head><body>
<div id="_idContainer663">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">15</span></h1>
<h1 class="chapterTitle" id="_idParaDest-280"><span class="koboSpan" id="kobo.2.1">Diffusion Models and AI Art</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In prior chapters, we’ve looked at examples of how generative models can be used to create novel images; we’ve also seen how language models can be used to author answers to questions or create entirely new creative text like poems. </span><span class="koboSpan" id="kobo.3.2">In this chapter, we bring together these two concepts by showing how user prompts can be translated into images, allowing you to author “AI art” using natural language. </span><span class="koboSpan" id="kobo.3.3">In addition to creating novel images, we can perform some useful functions like extending an image beyond its current boundaries (“outfilling”) and defining features for safety screening in our results. </span><span class="koboSpan" id="kobo.3.4">We’ll also look at one of the foundational ideas underlying this image generation methodology, the </span><em class="italic"><span class="koboSpan" id="kobo.4.1">diffusion model</span></em><span class="koboSpan" id="kobo.5.1">, which uses the concept of heat transfer to represent how an input of random numbers is “decoded” into an image. </span><span class="koboSpan" id="kobo.5.2">To illustrate these ideas, we’ll primarily work with </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Stable Diffusion</span></em><span class="koboSpan" id="kobo.7.1">, an open-source generative model, but similar concepts apply to closed-source models such as </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Midjourney </span></em><span class="koboSpan" id="kobo.9.1">and </span><em class="italic"><span class="koboSpan" id="kobo.10.1">DALL-E. </span></em><span class="koboSpan" id="kobo.11.1">Topics we’ll cover include:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.12.1">How diffusion models relate to other kinds of image-generating models</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.13.1">How the Stable Diffusion </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.14.1">model combines </span><strong class="keyWord"><span class="koboSpan" id="kobo.15.1">Variational Autoencoders </span></strong><span class="koboSpan" id="kobo.16.1">(</span><strong class="keyWord"><span class="koboSpan" id="kobo.17.1">VAEs</span></strong><span class="koboSpan" id="kobo.18.1">) and diffusion models to create extremely efficient image sampling</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.19.1">Some examples of using the Stable Diffusion model in the Hugging Face pipelines library, where we:</span><ul>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.20.1">Evaluate key parameters that impact the output of the image generation task</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.21.1">Walk through how the pieces of the Hugging Face pipeline implement each step of the image generation task to create a picture from a user prompt:</span><ul>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.22.1">Tokenizing the user prompt as a byte string</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.23.1">Encoding the byte string prompt as a vector</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.24.1">Generating random number vectors</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.25.1">Using the encoded prompt and random input to run multiple denoising steps to generate a compressed form of the new image</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.26.1">Uncompressing the new image using the decoder arm of a VAE</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 class="heading-1" id="_idParaDest-281"><span class="koboSpan" id="kobo.27.1">A walk through image generation: Why we need diffusion models</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.28.1">Diffusion models are among the latest and most popular methods for image generation, particularly based on user-provided </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.29.1">natural language prompts. </span><span class="koboSpan" id="kobo.29.2">The conceptual challenge of this class of image generation model is to create a method that is:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.30.1">Scalable to train and execute</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.31.1">Able to generate a diversity of images, including with user-guided prompts</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.32.1">Able to generate natural-looking images</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.33.1">Has stable training behavior that is possible to replicate easily</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.34.1">One approach to this problem is “autoregressive” models, where the image is generated pixel by pixel, using the prior-generated pixels as successive inputs1. </span><span class="koboSpan" id="kobo.34.2">The inputs to these models could be both a set of image pixels and natural language instructions from the user that are encoded into an embedding vector. </span><span class="koboSpan" id="kobo.34.3">This </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.35.1">approach is slow, as it makes each pixel dependent upon prior steps in the model output. </span><span class="koboSpan" id="kobo.35.2">As we’ve seen in prior chapters, </span><strong class="keyWord"><span class="koboSpan" id="kobo.36.1">Generative Adversarial Networks </span></strong><span class="koboSpan" id="kobo.37.1">(</span><strong class="keyWord"><span class="koboSpan" id="kobo.38.1">GANs</span></strong><span class="koboSpan" id="kobo.39.1">) can also be used to synthesize images, but they have unstable training behavior that is tricky to replicate and have a tendency to get stuck in local “modes,” rather than generating a broader distribution of natural images</span><sup class="superscript"><span class="koboSpan" id="kobo.40.1">2</span></sup><span class="koboSpan" id="kobo.41.1">. </span><span class="koboSpan" id="kobo.41.2">As we saw with VAEs in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.42.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.43.1">, the objective function based on pixel-wise approximation may not create the most realistic images. </span><span class="koboSpan" id="kobo.43.2">Recently, </span><em class="italic"><span class="koboSpan" id="kobo.44.1">diffusion models </span></em><span class="koboSpan" id="kobo.45.1">have arisen as a promising alternative. </span><span class="koboSpan" id="kobo.45.2">What are they, and how do they solve some of the challenges we’ve mentioned?</span></p>
<h2 class="heading-2" id="_idParaDest-282"><span class="koboSpan" id="kobo.46.1">Pictures from noise: Using diffusion to model natural image variability</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.47.1">The core idea of diffusion models is that we can represent images as a set of pixels, which are like a cloud in high-dimensional </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.48.1">space. </span><span class="koboSpan" id="kobo.48.2">That cloud is highly structured, representing colored patches and objects. </span><span class="koboSpan" id="kobo.48.3">If we add noise – such as random normal noise – to that structure, it becomes a spherical cloud. </span><span class="koboSpan" id="kobo.48.4">However, if we had a recipe for how to reverse that “blurring” of the image, we could create new images from a set of random points.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.49.1">Let’s look at how to write this out mathematically. </span><span class="koboSpan" id="kobo.49.2">We start with our “forward process,” which takes input data, such as an image, </span><span class="koboSpan" id="kobo.50.1"><img alt="" src="../Images/B22333_15_001.png"/></span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.51.1">, and applies stepwise noise to turn it into a vector of random normals. </span><span class="koboSpan" id="kobo.51.2">We will label this forward “blurring” process </span><span class="koboSpan" id="kobo.52.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1084"/><em class="italic"><span class="koboSpan" id="kobo.53.1">,</span></em><span class="koboSpan" id="kobo.54.1"> and we can represent it as a </span><em class="italic"><span class="koboSpan" id="kobo.55.1">Markov </span></em><span class="koboSpan" id="kobo.56.1">process where each step depends only on the prior step:</span></p>
<p class="center"><span class="koboSpan" id="kobo.57.1"><img alt="" src="../Images/B22333_15_003.png"/></span><a id="_idIndexMarker1085"/></p>
<p class="normal"><span class="koboSpan" id="kobo.58.1">In other words, the image at the end composed of random pixels is created by repetitively applying a function </span><span class="koboSpan" id="kobo.59.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1086"/><em class="italic"> </em><span class="koboSpan" id="kobo.60.1">to step </span><span class="koboSpan" id="kobo.61.1"><img alt="" src="../Images/B22333_15_005.png"/></span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.62.1">, dependent on the prior value of </span><span class="koboSpan" id="kobo.63.1"><img alt="" src="../Images/B22333_11_001.png"/></span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">This function </span><span class="koboSpan" id="kobo.65.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.66.1"> defines a transition process that follows a Gaussian distribution parameterized by </span><span class="koboSpan" id="kobo.67.1"><img alt="" src="../Images/B22333_15_008.png"/></span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.68.1">, which controls the variance</span><sup class="superscript"><span class="koboSpan" id="kobo.69.1">3</span></sup><span class="koboSpan" id="kobo.70.1">. </span><span class="koboSpan" id="kobo.70.2">The value of </span><span class="koboSpan" id="kobo.71.1"><img alt="" src="../Images/B22333_15_008.png"/></span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.72.1"> determines the level of noise applied at each step – smaller values (low </span><span class="koboSpan" id="kobo.73.1"><img alt="" src="../Images/B22333_15_008.png"/></span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.74.1">) result in a gradual increase in noise, while larger values (high </span><span class="koboSpan" id="kobo.75.1"><img alt="" src="../Images/B22333_15_008.png"/></span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.76.1">) accelerate the transition, causing the image to degrade into </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.77.1">a noisy set of random pixels more quickly. </span><span class="koboSpan" id="kobo.77.2">Once we’ve applied this “blurring” transformation enough times, the data will be in a distribution such as a random normal.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.78.1">What if we now want to recover an image from this blurred cloud? </span><span class="koboSpan" id="kobo.78.2">We just apply a “reverse” transformation, </span><span class="koboSpan" id="kobo.79.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.80.1">, using a similar formula:</span></p>
<p class="center"><span class="koboSpan" id="kobo.81.1"><img alt="" src="../Images/B22333_15_013.png"/></span><a id="_idIndexMarker1096"/></p>
<p class="normal"><span class="koboSpan" id="kobo.82.1">We can see that </span><span class="koboSpan" id="kobo.83.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.84.1"> and </span><span class="koboSpan" id="kobo.85.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.86.1"> are reverses of each other, but that </span><span class="koboSpan" id="kobo.87.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.88.1"> also represents a recipe for taking random data and generating images from it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.89.1">This process is illustrated below:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.90.1"><img alt="Figure 15.1: The diffusion process for noising and denoising images4" src="../Images/B22333_15_01.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.91.1">Figure 15</span></span><span class="koboSpan" id="kobo.92.1">.1: The diffusion process for noising and denoising images</span><sup class="superscript"><span class="koboSpan" id="kobo.93.1">4</span></sup></p>
<p class="normal"><span class="koboSpan" id="kobo.94.1">This design seems promising conceptually, but it’s not clear how we would guarantee that </span><span class="koboSpan" id="kobo.95.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.96.1"> and </span><span class="koboSpan" id="kobo.97.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.98.1"> are sufficiently close that they would result in high-quality samples when we apply them. </span><span class="koboSpan" id="kobo.98.2">In other words, we need a method to optimize the parameters of </span><span class="koboSpan" id="kobo.99.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.100.1"> and </span><span class="koboSpan" id="kobo.101.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.102.1"> so that they are tuned to generate high-quality reconstructions of an input image once it has been blurred and recovered through </span><span class="koboSpan" id="kobo.103.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.104.1">. </span><span class="koboSpan" id="kobo.104.2">It’s perhaps not surprising, given the familiar </span><span class="koboSpan" id="kobo.105.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.106.1"> and </span><span class="koboSpan" id="kobo.107.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.108.1"> distributions from our discussion of VAEs in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.109.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.110.1">, that this problem can be solved through variational inference</span><sup class="superscript"><span class="koboSpan" id="kobo.111.1">4</span></sup><span class="koboSpan" id="kobo.112.1">. </span><span class="koboSpan" id="kobo.112.2">Let’s see how.</span></p>
<h2 class="heading-2" id="_idParaDest-283"><span class="koboSpan" id="kobo.113.1">Using variational inference to generate high-quality diffusion models</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.114.1">Recall that the </span><strong class="keyWord"><span class="koboSpan" id="kobo.115.1">Evidence Lower Bound</span></strong><span class="koboSpan" id="kobo.116.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.117.1">ELBO</span></strong><span class="koboSpan" id="kobo.118.1">) gives an expression for the log-likelihood of a difficult-to-calculate </span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.119.1">distribution p in terms of an approximating, easy-to-calculate distribution </span><span class="koboSpan" id="kobo.120.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.121.1">:</span></p>
<p class="center"><span class="koboSpan" id="kobo.122.1"><img alt="" src="../Images/B22333_15_025.png"/></span><a id="_idIndexMarker1109"/></p>
<p class="normal"><span class="koboSpan" id="kobo.123.1">Instead of directly maximizing the likelihood of </span><span class="koboSpan" id="kobo.124.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.125.1">, we can maximize the right-hand side, which is a lower bound on the likelihood of </span><span class="koboSpan" id="kobo.126.1"><img alt="" src="../Images/B22333_11_021.png"/></span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.127.1">, in terms of the divergence with an approximating distribution </span><span class="koboSpan" id="kobo.128.1"><img alt="" src="../Images/B22333_11_022.png"/></span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.129.1">. </span><span class="koboSpan" id="kobo.129.2">For convenience purposes, we often minimize the negative log-likelihood (as many computational packages take the minima of a function), which gives the following equation for the diffusion model:</span></p>
<p class="center"><span class="koboSpan" id="kobo.130.1"><img alt="" src="../Images/B22333_15_029.png"/></span><a id="_idIndexMarker1113"/></p>
<p class="normal"><span class="koboSpan" id="kobo.131.1">Note that this equation can be evaluated at multiple steps t in the noising/denoising process. </span><span class="koboSpan" id="kobo.131.2">We can write this out more explicitly as a loss function with beginning, intermediate, and final values.</span></p>
<p class="center"><span class="koboSpan" id="kobo.132.1"><img alt="" src="../Images/B22333_15_030.png"/></span><a id="_idIndexMarker1114"/></p>
<p class="normal"><span class="koboSpan" id="kobo.133.1">Here, DKL is the Kullback–Leibler divergence, as we saw in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.135.1">. </span><span class="koboSpan" id="kobo.135.2">Recall that the forward noising process </span><em class="italic"><span class="koboSpan" id="kobo.136.1">q </span></em><span class="koboSpan" id="kobo.137.1">has a variance </span><span class="koboSpan" id="kobo.138.1"><img alt="" src="../Images/B22333_15_031.png"/></span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.139.1">. </span><span class="koboSpan" id="kobo.139.2">We could try to learn this value, but as it’s often small, we can treat it as fixed. </span><span class="koboSpan" id="kobo.139.3">Thus, the last term in this equation, at time </span><em class="italic"><span class="koboSpan" id="kobo.140.1">T</span></em><span class="koboSpan" id="kobo.141.1">, drops out since it is a constant. </span><span class="koboSpan" id="kobo.141.2">What about the values from </span><em class="italic"><span class="koboSpan" id="kobo.142.1">t</span></em><span class="koboSpan" id="kobo.143.1">=1 to </span><em class="italic"><span class="koboSpan" id="kobo.144.1">T-</span></em><span class="koboSpan" id="kobo.145.1">1? </span><span class="koboSpan" id="kobo.145.2">We already described that </span><em class="italic"><span class="koboSpan" id="kobo.146.1">q</span></em><span class="koboSpan" id="kobo.147.1"> doesn’t have learnable parameters, so we are interested in learning the parameters of </span><em class="italic"><span class="koboSpan" id="kobo.148.1">p</span></em><span class="koboSpan" id="kobo.149.1">, the reverse process that converts random noise into an image. </span><span class="koboSpan" id="kobo.149.2">In the expression:</span></p>
<p class="center"><span class="koboSpan" id="kobo.150.1"><img alt="" src="../Images/B22333_15_032.png"/></span><a id="_idIndexMarker1116"/></p>
<p class="normal"><span class="koboSpan" id="kobo.151.1">We will typically keep the variance as a fixed value, so we just need to learn a function to predict the mean – and that function could be a neural network that takes the input pixels at a given step and outputs a slightly less noisy image. </span><span class="koboSpan" id="kobo.151.2">However, we can reparameterize this equation to make it easier to optimize. </span><span class="koboSpan" id="kobo.151.3">Using the normal distribution, we can write this intermediate likelihood </span><span class="koboSpan" id="kobo.152.1"><img alt="" src="../Images/B22333_15_033.png"/></span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.153.1"> as:</span></p>
<p class="center"><span class="koboSpan" id="kobo.154.1"><img alt="" src="../Images/B22333_15_034.png"/></span><a id="_idIndexMarker1118"/></p>
<p class="normal"><span class="koboSpan" id="kobo.155.1">C is a constant and falls out of the minimization. </span><span class="koboSpan" id="kobo.155.2">We can calculate the value of the mean at a given point in time using the average variance per timestep. </span><span class="koboSpan" id="kobo.155.3">Let:</span></p>
<p class="center"><span class="koboSpan" id="kobo.156.1"><img alt="" src="../Images/B22333_15_035.png"/></span><a id="_idIndexMarker1119"/></p>
<p class="normal"><span class="koboSpan" id="kobo.157.1">And:</span></p>
<p class="center"><span class="koboSpan" id="kobo.158.1"><img alt="" src="../Images/B22333_15_036.png"/></span><a id="_idIndexMarker1120"/></p>
<p class="normal"><span class="koboSpan" id="kobo.159.1">Then, at each timestep, </span><em class="italic"><span class="koboSpan" id="kobo.160.1">x</span></em><span class="koboSpan" id="kobo.161.1"> can be represented as:</span></p>
<p class="center"><span class="koboSpan" id="kobo.162.1"><img alt="" src="../Images/B22333_15_037.png"/></span><a id="_idIndexMarker1121"/></p>
<p class="normal"><span class="koboSpan" id="kobo.163.1">And we’ll optimize:</span></p>
<p class="center"><span class="koboSpan" id="kobo.164.1"><img alt="" src="../Images/B22333_15_038.png"/></span><a id="_idIndexMarker1122"/></p>
<p class="normal"><span class="koboSpan" id="kobo.165.1">This expression</span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.166.1"> shows how the function predicting the mean of </span><em class="italic"><span class="koboSpan" id="kobo.167.1">x</span></em><span class="koboSpan" id="kobo.168.1"> can be represented as an equation in which the unknown is a function predicting the noise </span><em class="italic"><span class="koboSpan" id="kobo.169.1">e </span></em><span class="koboSpan" id="kobo.170.1">as a function of </span><em class="italic"><span class="koboSpan" id="kobo.171.1">t</span></em><span class="koboSpan" id="kobo.172.1">:</span></p>
<p class="center"><span class="koboSpan" id="kobo.173.1"><img alt="" src="../Images/B22333_15_039.png"/></span><a id="_idIndexMarker1124"/></p>
<p class="normal"><span class="koboSpan" id="kobo.174.1">This finally leads us to the following expression:</span></p>
<p class="center"><span class="koboSpan" id="kobo.175.1"><img alt="" src="../Images/B22333_15_040.png"/></span><a id="_idIndexMarker1125"/></p>
<p class="normal"><span class="koboSpan" id="kobo.176.1">Given fixed values for </span><span class="koboSpan" id="kobo.177.1"><img alt="" src="../Images/B22333_15_041.png"/></span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.178.1">, </span><span class="koboSpan" id="kobo.179.1"><img alt="" src="../Images/B22333_15_042.png"/></span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.180.1">, and </span><span class="koboSpan" id="kobo.181.1"><img alt="" src="../Images/B22333_15_043.png"/></span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.182.1">, and input data </span><span class="koboSpan" id="kobo.183.1"><img alt="" src="../Images/B22333_11_037.png"/></span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.184.1">, we are optimizing a function to predict the noise we should subtract at each step of the reverse process </span><span class="koboSpan" id="kobo.185.1"><img alt="" src="../Images/B22333_11_032.png"/></span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.186.1"> to obtain an image </span><span class="koboSpan" id="kobo.187.1"><img alt="" src="../Images/B22333_11_032.png"/></span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.188.1"> from a sample of random noise. </span><span class="koboSpan" id="kobo.188.2">Like </span><span class="koboSpan" id="kobo.189.1"><img alt="" src="../Images/B22333_15_047.png"/></span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.190.1">, that </span><span class="koboSpan" id="kobo.191.1"><img alt="" src="../Images/B22333_15_048.png"/></span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.192.1"> will be a neural network, and that is what we will see implemented in the Stable Diffusion model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.193.1">For the term L</span><sub class="subscript"><span class="koboSpan" id="kobo.194.1">o</span></sub><span class="koboSpan" id="kobo.195.1"> in the diffusion equation on the previous page (i.e., </span><span class="koboSpan" id="kobo.196.1"><img alt="" src="../Images/B22333_15_049.png"/></span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.197.1">), in practice, it has not been found to be needed to train a probabilistic diffusion function, so it is dropped. </span><span class="koboSpan" id="kobo.197.2">We can make one more improvement; if the sample already has low noise (after we’ve run the reverse process for many steps), we can down-weight subsequent samples when we subtract the model-predicted noise. </span><span class="koboSpan" id="kobo.197.3">We do this by incorporating the simulation step t explicitly as a term in our noise-predicting neural network </span><em class="italic"><span class="koboSpan" id="kobo.198.1">e</span></em><span class="koboSpan" id="kobo.199.1">, and drop the multipliers:</span></p>
<p class="center"><span class="koboSpan" id="kobo.200.1"><img alt="" src="../Images/B22333_15_050.png"/></span><a id="_idIndexMarker1135"/></p>
<p class="normal"><span class="koboSpan" id="kobo.201.1">As we’ll see later, how we execute e at each step of the simulation to remove noise successively from a random image is an important design choice in diffusion models, known as the scheduler.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.202.1">However, we have one last challenge to resolve; we can optimize the likelihood function above efficiently, but the actual generation step will be costly since we could be working with large images, and the size</span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.203.1"> of </span><em class="italic"><span class="koboSpan" id="kobo.204.1">x</span></em><span class="koboSpan" id="kobo.205.1"> remains fixed throughout the simulation steps. </span><span class="koboSpan" id="kobo.205.2">This is where Stable Diffusion comes in: it leverages the VAE models we saw in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.206.1">Chapter 11</span></em></span></a><em class="italic"> </em><span class="koboSpan" id="kobo.207.1">to perform the forward and reverse processes we describe above in a latent space that is much smaller than the original image, meaning it is considerably faster for training and inference. </span><span class="koboSpan" id="kobo.207.2">Let’s take a look.</span></p>
<h2 class="heading-2" id="_idParaDest-284"><span class="koboSpan" id="kobo.208.1">Stable Diffusion: Generating images in latent space</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.209.1">As we described, the major insight for the </span><a id="_idIndexMarker1137"/><span class="koboSpan" id="kobo.210.1">Stable Diffusion model was that instead of performing the forward process </span><em class="italic"><span class="koboSpan" id="kobo.211.1">q</span></em><span class="koboSpan" id="kobo.212.1"> and reverse process </span><em class="italic"><span class="koboSpan" id="kobo.213.1">p</span></em><span class="koboSpan" id="kobo.214.1"> that we’ve trained through variation inference in the image space, we do so using a VAE to compress the images, making the calculation much faster than the slower diffusion calculation that can be executed in the original pixel space; this process is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.215.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.216.1">.2</span></em><span class="koboSpan" id="kobo.217.1">.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.218.1"><img alt="Figure 15.2: The Stable Diffusion model5" src="../Images/B22333_15_02.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.219.1">Figure 15</span></span><span class="koboSpan" id="kobo.220.1">.2: The Stable </span><span class="Comment-Reference"><span class="koboSpan" id="kobo.221.1">D</span></span><span class="koboSpan" id="kobo.222.1">iffusion model</span><sup class="superscript"><span class="koboSpan" id="kobo.223.1">5</span></sup></p>
<p class="normal"><span class="koboSpan" id="kobo.224.1">Let’s walk through some of the elements of this workflow. </span><span class="koboSpan" id="kobo.224.2">On the far right, the input image </span><em class="italic"><span class="koboSpan" id="kobo.225.1">x</span></em><span class="koboSpan" id="kobo.226.1"> is “blurred” using a VAE into a latent vector </span><em class="italic"><span class="koboSpan" id="kobo.227.1">z</span></em><span class="koboSpan" id="kobo.228.1">. </span><span class="koboSpan" id="kobo.228.2">Thus, the forward step </span><em class="italic"><span class="koboSpan" id="kobo.229.1">q</span></em><span class="koboSpan" id="kobo.230.1"> is executed through one pass through the VAE! </span><span class="koboSpan" id="kobo.230.2">Then, we can incorporate “conditioning information” (such as a textual prompt from the user) using an embedding method on the far right. </span><span class="koboSpan" id="kobo.230.3">Now, to run the “reverse process,” </span><em class="italic"><span class="koboSpan" id="kobo.231.1">p</span></em><span class="koboSpan" id="kobo.232.1">, we execute a time-varying U-Net</span><sup class="superscript"><span class="koboSpan" id="kobo.233.1">6</span></sup><span class="koboSpan" id="kobo.234.1"> to predict the noise, e, that we should remove from a random image at each time step. </span><span class="koboSpan" id="kobo.234.2">The U-Net </span><span class="Comment-Reference"><span class="koboSpan" id="kobo.235.1">(</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.236.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.237.1">.2</span></em><span class="Comment-Reference"><span class="koboSpan" id="kobo.238.1">) </span></span><span class="koboSpan" id="kobo.239.1">is made up of a number of transformer layers, which compress the latent vector z generated by the VAE (or sampled randomly) into a smaller length before expanding it, in order to enhance the salient features of the latent vector. </span><span class="koboSpan" id="kobo.239.2">The “U” in the name comes from the fact that if the layers are arranged visually with the largest, outermost layers at the top and the innermost, narrowest layers at the bottom of a graph of the network, it resembles the letter U. </span><span class="koboSpan" id="kobo.239.3">Due to this architecture, the U-Net is well suited to extract features/details in images (through the forward, encoding path) that are then labeled/highlighted at a pixel level (through the reverse, decoding path that expands the image to its original dimensions). </span><span class="koboSpan" id="kobo.239.4">In our example, where we use latent vectors instead of the original image, each pass of the latent vector through this U-Net represents one “step” of the denoising process </span><span class="koboSpan" id="kobo.240.1"><img alt="" src="../Images/B22333_11_021.png"/></span><span class="koboSpan" id="kobo.241.1">. </span><span class="koboSpan" id="kobo.241.2">You’ll also notice we’ve added residual connections in this U-Net to enable the efficient flow of information through the network. </span><span class="koboSpan" id="kobo.241.3">We then decode the “denoised” latent vector with the VAE in reverse.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.242.1">In the training phase </span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.243.1">of this model, we would take pairs of images and prompts describing them, embed them in the model, and optimize the lower bound given above. </span><span class="koboSpan" id="kobo.243.2">If we are not training the model, we don’t even need to run the VAE forward to create random vectors; we just sample them. </span><span class="koboSpan" id="kobo.243.3">Now that we’ve seen how Stable Diffusion is set up, and the details of how it evolved from earlier ideas for image generation, let’s see how to put it into practice.</span></p>
<h1 class="heading-1" id="_idParaDest-285"><span class="koboSpan" id="kobo.244.1">Running Stable Diffusion in the cloud</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.245.1">To start, let’s quickly set up our own instance of the Stable Diffusion model in Python code and run an example. </span><span class="koboSpan" id="kobo.245.2">For this purpose, we’ll be</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.246.1"> using Google Colab (</span><a href="https://colab.research.google.com/"><span class="url"><span class="koboSpan" id="kobo.247.1">https://colab.research.google.com/</span></span></a><span class="koboSpan" id="kobo.248.1">), a cloud </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.249.1">environment that allows you to utilize</span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.250.1"> high-performance </span><strong class="keyWord"><span class="koboSpan" id="kobo.251.1">Graphics Processing Unit</span></strong><span class="koboSpan" id="kobo.252.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.253.1">GPU</span></strong><span class="koboSpan" id="kobo.254.1">) computing and large memory resources from your laptop. </span><span class="koboSpan" id="kobo.254.2">Colab is free, but you can also pay for higher-availability resources if you desire. </span><span class="koboSpan" id="kobo.254.3">The interface resembles the Python Jupyter notebooks (</span><a href="https://jupyter.org/"><span class="url"><span class="koboSpan" id="kobo.255.1">https://jupyter.org/</span></span></a><span class="koboSpan" id="kobo.256.1">) that you’ve likely used in the past.</span></p>
<h2 class="heading-2" id="_idParaDest-286"><span class="koboSpan" id="kobo.257.1">Installing dependencies and running an example</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.258.1">Once you’ve set up your Colab</span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.259.1"> account, you just need to install the diffusers package and a few dependencies. </span><span class="koboSpan" id="kobo.259.2">Diffusers is a library created by the company Hugging Face (</span><a href="https://huggingface.co/docs/diffusers/index"><span class="url"><span class="koboSpan" id="kobo.260.1">https://huggingface.co/docs/diffusers/index</span></span></a><span class="koboSpan" id="kobo.261.1">) that provides easy access</span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.262.1"> to a set of state-of-the-art diffusion models (including Stable Diffusion). </span><span class="koboSpan" id="kobo.262.2">It utilizes the pipelines</span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.263.1"> API, also developed by Hugging Face, which abstracts many of the complexities of these models to a simple interface. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.265.1">.3 </span></em><span class="koboSpan" id="kobo.266.1">demonstrates the commands you would provide in a Colab notebook to install diffusers and its dependencies.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.267.1"><img alt="Figure 15.3: Dependencies for diffusers" src="../Images/B22333_15_03.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.268.1">Figure 15</span></span><span class="koboSpan" id="kobo.269.1">.3: Dependencies for diffusers</span></p>
<p class="normal"><span class="koboSpan" id="kobo.270.1">For this example, you’ll want to make sure you have a GPU-enabled runtime, which you can choose by going to </span><em class="italic"><span class="koboSpan" id="kobo.271.1">Runtime</span></em><span class="koboSpan" id="kobo.272.1"> and then </span><em class="italic"><span class="koboSpan" id="kobo.273.1">Change runtime type</span></em><span class="koboSpan" id="kobo.274.1"> on the top ribbon on the notebook.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.275.1"><img alt="Figure 15.4: Runtime for the diffusers example" src="../Images/B22333_15_04.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.276.1">Figure 15</span></span><span class="koboSpan" id="kobo.277.1">.4: Runtime for the diffusers example</span></p>
<p class="normal"><span class="koboSpan" id="kobo.278.1">From there, we’ll initialize the Stable Diffusion 1.4 model using a series of simple commands. </span><span class="koboSpan" id="kobo.278.2">First, we’ll load the model, then initialize a pipeline on the GPU on our runtime. </span><span class="koboSpan" id="kobo.278.3">Then we merely need to supply a text prompt to the pipeline; the model will be run interactively and we can display the result directly in the notebook.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.279.1">To start with, we’ll use</span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.280.1"> an example from the Stable Diffusion paper</span><sup class="superscript"><span class="koboSpan" id="kobo.281.1">5</span></sup><span class="koboSpan" id="kobo.282.1">. </span><span class="koboSpan" id="kobo.282.2">The user prompt is “a zombie in the style of Picasso,” and the Stable Diffusion model translates this prompt into an image representing an undead monster in the abstract, cubist style of the famous 20th-century artist Pablo Picasso:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.283.1"><img alt="Figure 15.5: An example output using the Picasso zombie prompt" src="../Images/B22333_15_05.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.284.1">Figure 15</span></span><span class="koboSpan" id="kobo.285.1">.5: An example output using the Picasso zombie prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.286.1">However, remember that this is not a deterministic output like a typical machine learning prediction, where we get the same output for a given input. </span><span class="koboSpan" id="kobo.286.2">Rather, we’re sampling from possible model outputs from</span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.287.1"> a distribution, so we’re not limited to generating a single output. </span><span class="koboSpan" id="kobo.287.2">Indeed, if we modify the </span><code class="inlineCode"><span class="koboSpan" id="kobo.288.1">num_images_per_prompt</span></code><span class="koboSpan" id="kobo.289.1"> parameter, we can generate a set of images all from the same prompt by printing each element of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.290.1">images</span></code><span class="koboSpan" id="kobo.291.1"> list.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.292.1"><img alt="Figure 15.6: Generating alternative images from the zombie prompt" src="../Images/B22333_15_06.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.293.1">Figure 15</span></span><span class="koboSpan" id="kobo.294.1">.6: Generating alternative images from the zombie prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.295.1">Now that we’ve looked at a basic example, let’s modify some of the parameters to see how they impact the output.</span></p>
<h2 class="heading-2" id="_idParaDest-287"><span class="koboSpan" id="kobo.296.1">Key parameters for Stable Diffusion text-to-image generation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.297.1">Besides generating</span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.298.1"> multiple images, what other parameters could we modify in this example? </span><span class="koboSpan" id="kobo.298.2">One would be to remove the prompt (provide a blank prompt) and see what output we would get:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.299.1"><img alt="Figure 15.7: Running Stable Diffusion with a blank prompt" src="../Images/B22333_15_07.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.300.1">Figure 15</span></span><span class="koboSpan" id="kobo.301.1">.7: Running Stable Diffusion with a blank prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.302.1">Interestingly, as you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.303.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.304.1">.7, </span></em><span class="koboSpan" id="kobo.305.1">the result is a set of seemingly random images, but not blank images or completely random noise. </span><span class="koboSpan" id="kobo.305.2">The reason for this can be explained by one of the components of the pipeline, the VAE we covered in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.306.1">Chapter 11</span></em></span></a><em class="italic"> </em><span class="koboSpan" id="kobo.307.1">and the data used to develop it, as we’ll see later in this chapter.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.308.1">We can also modify how much importance we give to the prompt in generating images, using the </span><em class="italic"><span class="koboSpan" id="kobo.309.1">guidance scale </span></em><span class="koboSpan" id="kobo.310.1">parameter. </span><span class="koboSpan" id="kobo.310.2">As we saw in our overview of the Stable Diffusion model, we can think of the image generation step as modeling the pixels in the image as particles that drift in a multi-dimensional space. </span><span class="koboSpan" id="kobo.310.3">The motion of those particles can either be pushed in a particular direction in correlation with the</span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.311.1"> input prompt from the user or move randomly according to their initial configuration. </span><span class="koboSpan" id="kobo.311.2">The default for the guidance scale is 7.5 – let’s see what happens if we change it to alternative values between 0 and 10:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.312.1"><img alt="Figu﻿re 15.8: Modifying the guidance scale from 0 to 10" src="../Images/B22333_15_08.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.313.1">Figure 15</span></span><span class="koboSpan" id="kobo.314.1">.8: Modifying the guidance scale from 0 to 10</span></p>
<p class="normal"><span class="koboSpan" id="kobo.315.1">You can see that as the guidance scale increases from 0 to 10, the generated image more clearly resembles the prompt. </span><span class="koboSpan" id="kobo.315.2">The image at 0 looks very much like the output from the blank prompt examples in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.316.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.317.1">.6</span></em><span class="koboSpan" id="kobo.318.1"> – indeed, under this setting, the model is using a blank input. </span><span class="koboSpan" id="kobo.318.2">At 0, the model will pay no attention to the prompt, as we’ll see later in this chapter.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.319.1">The impact of this parameter is perhaps more notable when using a more complex prompt, such as the one we used in the last chapter:</span></p>
<blockquote class="packt_quote">
<p class="quote"><span class="koboSpan" id="kobo.320.1">A zombie in the style of Monet. </span><span class="koboSpan" id="kobo.320.2">The zombie is dressed in a farmer’s outfit and holds a paintbrush and canvas. </span><span class="koboSpan" id="kobo.320.3">The sun is setting, and there are mountains in the distance. </span><span class="koboSpan" id="kobo.320.4">The hay in the field in which the zombie is standing comes up to its waist. </span><span class="koboSpan" id="kobo.320.5">There are red flowers in the field</span></p>
</blockquote>
<p class="normal"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.321.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.322.1">.9 </span></em><span class="koboSpan" id="kobo.323.1">shows a comparison of applying a guidance scale of 0 to 10; you can see in the final image that the zombie figure begins to appear.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.324.1"><img alt="Figure 15.9: Image generated with guidance scales 0, 7.5, and 10 using a complex prompt" src="../Images/B22333_15_09.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.325.1">Figure 15</span></span><span class="koboSpan" id="kobo.326.1">.9: Image generated with guidance scales 0, 7.5, and 10 using a complex prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.327.1">In addition to the guidance scale, the number of diffusion steps in the image generation process also impacts how crisp the output is. </span><span class="koboSpan" id="kobo.327.2">As we’ve seen, the image generation by the model can be represented by pixels </span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.328.1">behaving as particles moving in space. </span><span class="koboSpan" id="kobo.328.2">The longer they are able to move, the farther they can transition from an initial, random arrangement to a new configuration that resembles an image. </span><span class="koboSpan" id="kobo.328.3">The default in this pipeline is 50 steps: let’s see what happens if we modify that to </span><code class="inlineCode"><span class="koboSpan" id="kobo.329.1">1</span></code><span class="koboSpan" id="kobo.330.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.331.1">3</span></code><span class="koboSpan" id="kobo.332.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.333.1">10</span></code><span class="koboSpan" id="kobo.334.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.335.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.336.1">.10</span></em><span class="koboSpan" id="kobo.337.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.338.1"><img alt="Figure 15.﻿10: Images generated with 1, 3, and 10 simulation steps" src="../Images/B22333_15_10.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.339.1">Figure 15</span></span><span class="koboSpan" id="kobo.340.1">.10: Images generated with 1, 3, and 10 simulation steps</span></p>
<p class="normal"><span class="koboSpan" id="kobo.341.1">As the number of simulation steps increases, the generated image goes from blurry to resembling our initial examples – at 3 steps, we see output that resembles our prompt but without the simplified cubist lines that become clearer with more simulation steps. </span><span class="koboSpan" id="kobo.341.2">We’ll see later in this chapter how each simulation attempts to subtract “noise” from the image, and thus makes it more clear as more steps are run.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.342.1">Another way we can modify the input is by introducing “negative” prompts, which cancel out part of the initial prompt. </span><span class="koboSpan" id="kobo.342.2">Let’s see how this works by providing </span><code class="inlineCode"><span class="koboSpan" id="kobo.343.1">zombie</span></code><span class="koboSpan" id="kobo.344.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.345.1">Picasso</span></code><span class="koboSpan" id="kobo.346.1">, or </span><code class="inlineCode"><span class="koboSpan" id="kobo.347.1">Cubist</span></code><span class="koboSpan" id="kobo.348.1"> as the negative prompt in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.349.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.350.1">.11</span></em><span class="koboSpan" id="kobo.351.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.352.1"><img alt="Figure 15.11: ﻿Image generated with negative prompts" src="../Images/B22333_15_11.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.353.1">Figure 15</span></span><span class="koboSpan" id="kobo.354.1">.11: Image generated with negative prompts</span></p>
<p class="normal"><span class="koboSpan" id="kobo.355.1">You can see that if we provide elements of the prompt (</span><code class="inlineCode"><span class="koboSpan" id="kobo.356.1">zombie</span></code><span class="koboSpan" id="kobo.357.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.358.1">Picasso</span></code><span class="koboSpan" id="kobo.359.1">), we can cancel out either the subject matter or the style of the image. </span><span class="koboSpan" id="kobo.359.2">We don’t even need to use the exact words; as you can see using </span><code class="inlineCode"><span class="koboSpan" id="kobo.360.1">Cubist</span></code><span class="koboSpan" id="kobo.361.1"> (a term closely associated with the art style of Picasso) produces a similar output to</span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.362.1"> a negative prompt using the artist’s name explicitly. </span><span class="koboSpan" id="kobo.362.2">This is because of how the prompts are encoded as numerical vectors when they are passed to the model, which allows the model to compare the similarity of terms, as we’ll see later when we discuss the embedding step.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.363.1">In addition to modifying the content of the image, we can also easily change its size, as you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.365.1">.12.</span></em></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.366.1"><img alt="Figure 15.12: Imag﻿e generated with varying dimensions" src="../Images/B22333_15_12.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.367.1">Figure 15</span></span><span class="koboSpan" id="kobo.368.1">.12: Image generated with varying dimensions</span></p>
<p class="normal"><span class="koboSpan" id="kobo.369.1">The size of the resulting image is easily changed by modifying the size of the ultimate decoder layer in the last step of the pipeline, as we’ll see later.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.370.1">One of the risks of</span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.371.1"> generating images in an application is that the output could be offensive; fortunately, the pipeline in this example has a built-in feature, a safety checker, to screen such potentially inappropriate content. </span><span class="koboSpan" id="kobo.371.2">We can see the effect of this by modifying the prompt (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.372.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.373.1">.13</span></em><span class="koboSpan" id="kobo.374.1">):</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.375.1"><img alt="Figure 15.13: Image﻿ generated with a toxic/offensive prompt" src="../Images/B22333_15_13.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.376.1">Figure 15</span></span><span class="koboSpan" id="kobo.377.1">.13: Image generated with a toxic/offensive prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.378.1">The safety checker is a model that classifies features of the produced image as </span><strong class="keyWord"><span class="koboSpan" id="kobo.379.1">Not Safe for Work </span></strong><span class="koboSpan" id="kobo.380.1">(</span><strong class="keyWord"><span class="koboSpan" id="kobo.381.1">NSFW</span></strong><span class="koboSpan" id="kobo.382.1">) and blocks </span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.383.1">them. </span><span class="koboSpan" id="kobo.383.2">The features it uses to produce this classification are quite similar to the embeddings used to feed the prompt into the model to generate the image.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.384.1">Now that we’ve seen</span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.385.1"> numerous ways that we can </span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.386.1">tweak the output of the model through various parameters, let’s explore how each of these parameters appears step by step as we walk through each of the components underlying the pipeline.</span></p>
<h1 class="heading-1" id="_idParaDest-288"><span class="koboSpan" id="kobo.387.1">Deep dive into the text-to-image pipeline</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.388.1">In the previous section, we produced all the examples by providing the prompt and various arguments to the pipeline </span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.389.1">directly. </span><span class="koboSpan" id="kobo.389.2">The pipeline consists of several components that act in sequence to produce images from your prompt. </span><span class="koboSpan" id="kobo.389.3">These components are contained in a Python dictionary that is part of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.390.1">Pipeline</span></code><span class="koboSpan" id="kobo.391.1"> class, and so, like any Python dictionary, you can print the key names of the fields to inspect the components (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.392.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.393.1">.14</span></em><span class="koboSpan" id="kobo.394.1">).</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.395.1"><img alt="Figure 15.14: Co﻿mponents of the Stable Diffusion pipeline" src="../Images/B22333_15_14.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.396.1">Figure 15</span></span><span class="koboSpan" id="kobo.397.1">.14: Components of the Stable Diffusion pipeline</span></p>
<p class="normal"><span class="koboSpan" id="kobo.398.1">We’ve seen each of these in action in the prior examples, as will become clearer as we walk through the execution of each:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.399.1">The tokenizer takes our prompt and turns it into a byte representation</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.400.1">The text encoder takes that byte representation and turns it into a numerical vector</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.401.1">The U-Net, which takes a vector of random numbers and the encoded prompt and merges them</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.402.1">The scheduler, which runs diffusion steps to “denoise” this merged vector</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.403.1">The VAE, which converts the merged vector into one or more generated images</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.404.1">The feature extractor, which extracts elements from the generated image that might be labeled as offensive</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.405.1">The safety checker, which scores those extracted elements to see whether the image might be censored</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.406.1">Let’s step through each </span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.407.1">component and see how the parameters we looked at earlier come into play in the execution.</span></p>
<h2 class="heading-2" id="_idParaDest-289"><span class="koboSpan" id="kobo.408.1">The tokenizer</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.409.1">The first step in this pipeline is to convert the prompt into a set of </span><em class="italic"><span class="koboSpan" id="kobo.410.1">tokens</span></em><span class="koboSpan" id="kobo.411.1">, or individual elements to be passed into the </span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.412.1">textual embedding step. </span><span class="koboSpan" id="kobo.412.2">You can access a lot of information about the tokenizer by printing this pipeline component to the notebook:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.413.1"><img alt="Figure 15.15: T﻿he tokenizer properties" src="../Images/B22333_15_15.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.414.1">Figure 15</span></span><span class="koboSpan" id="kobo.415.1">.15: The tokenizer properties</span></p>
<p class="normal"><span class="koboSpan" id="kobo.416.1">Stable Diffusion uses a </span><strong class="keyWord"><span class="koboSpan" id="kobo.417.1">Contrastive Language Image Processing</span></strong><span class="koboSpan" id="kobo.418.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.419.1">CLIP</span></strong><span class="koboSpan" id="kobo.420.1">) model to compute embeddings, which is trained on a</span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.421.1"> joint dataset of images and their captions</span><sup class="superscript"><span class="koboSpan" id="kobo.422.1">2</span></sup><span class="koboSpan" id="kobo.423.1">. </span><span class="koboSpan" id="kobo.423.2">The tokenizer provides the raw input to compute the textual vectors used in the image generation process. </span><span class="koboSpan" id="kobo.423.3">You may have encountered tokenization in the past in one-hot encoding for natural language processing, in which a word (or character) is indexed by a number (for example, each letter in the English alphabet can be indexed with the number 0 to 25). </span><span class="koboSpan" id="kobo.423.4">Stable Diffusion and similar state-of-the-art models use a more efficient embedding than simply mapping each word to an index – instead, they map the text to bytes (using an encoding such as UTF-8) and represent commonly occurring byte pairs as a single</span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.424.1"> byte, a technique called </span><strong class="keyWord"><span class="koboSpan" id="kobo.425.1">Byte Pair Encoding</span></strong><span class="koboSpan" id="kobo.426.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.427.1">BPE</span></strong><span class="koboSpan" id="kobo.428.1">)</span><sup class="superscript"><span class="koboSpan" id="kobo.429.1">8</span></sup><span class="koboSpan" id="kobo.430.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.431.1">BPE is based on the idea that we can compress strings by looking for common recurring patterns. </span><span class="koboSpan" id="kobo.431.2">Let’s take an example:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.432.1">abcabcabde</span></p>
<p class="normal"><span class="koboSpan" id="kobo.433.1">In the first pass, we notice that the most commonly occurring pair of characters is </span><em class="italic"><span class="koboSpan" id="kobo.434.1">ab</span></em><span class="koboSpan" id="kobo.435.1">; we can convert this to a new character, </span><em class="italic"><span class="koboSpan" id="kobo.436.1">f</span></em><span class="koboSpan" id="kobo.437.1">:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.438.1">fcfcfde</span></p>
<p class="normal"><span class="koboSpan" id="kobo.439.1">Now, </span><em class="italic"><span class="koboSpan" id="kobo.440.1">fc</span></em><span class="koboSpan" id="kobo.441.1"> is the most commonly occurring pair. </span><span class="koboSpan" id="kobo.441.2">Convert this to </span><em class="italic"><span class="koboSpan" id="kobo.442.1">g</span></em><span class="koboSpan" id="kobo.443.1">:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.444.1">ggfde</span></p>
<p class="normal"><span class="koboSpan" id="kobo.445.1">Finally, convert </span><em class="italic"><span class="koboSpan" id="kobo.446.1">gg</span></em><span class="koboSpan" id="kobo.447.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.448.1">h</span></em><span class="koboSpan" id="kobo.449.1">:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.450.1">hfde</span></p>
<p class="normal"><span class="koboSpan" id="kobo.451.1">We’ve now compressed the input string from 10 characters to 4, which is much more efficient to work with computationally. </span><span class="koboSpan" id="kobo.451.2">If we need to recover the original string, we just to store a lookup table of the pairs and their corresponding character to reverse this operation, which we can run recursively.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.452.1">One additional detail is that while this example used characters, in practice we use bytes. </span><span class="koboSpan" id="kobo.452.2">This is because special </span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.453.1">characters like emojis would break a fixed-vocabulary character pair compressor since the special characters might not be in the lookup table, but all text can be represented uniformly as bytes, making it more robust.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.454.1">So, to summarize, the tokenizer converts the words in the prompt into bytes and uses a pre-computed lookup table of frequently occurring byte pairs to index those bytes with a set of IDs. </span><span class="koboSpan" id="kobo.454.2">You can see this in action by running just the </span><code class="inlineCode"><span class="koboSpan" id="kobo.455.1">tokenizer</span></code><span class="koboSpan" id="kobo.456.1"> on the input prompt, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.457.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.458.1">.16:</span></em></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.459.1"><img alt="Figure 15.16:﻿ Converting the prompt to byte token IDs" src="../Images/B22333_15_16.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.460.1">Figure 15</span></span><span class="koboSpan" id="kobo.461.1">.16: Converting the prompt to byte token IDs</span></p>
<p class="normal"><span class="koboSpan" id="kobo.462.1">You can access the encoding map that Stable Diffusion’s encoder uses through the </span><code class="inlineCode"><span class="koboSpan" id="kobo.463.1">encoder</span></code><span class="koboSpan" id="kobo.464.1"> property and verify that “320” corresponds to the pair of bytes for the letter “a” and whitespace. </span><span class="koboSpan" id="kobo.464.2">Similarly, “49406” is a placeholder character representing the start of a sentence.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.465.1"><img alt="Figure 15.17: T﻿he tokenizer encoding map" src="../Images/B22333_15_17_A.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.466.1">Figure 15</span></span><span class="koboSpan" id="kobo.467.1">.17: The tokenizer encoding map</span></p>
<h2 class="heading-2" id="_idParaDest-290"><span class="koboSpan" id="kobo.468.1">Generating text embedding</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.469.1">The next step in</span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.470.1"> the pipeline is to transfer the byte-indexed prompt into numerical vectors that can be used as inputs to the image generation step of the model. </span><span class="koboSpan" id="kobo.470.2">This embedding is performed by the CLIP neural network, whose properties you can examine in the notebook, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.471.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.472.1">.18</span></em><span class="koboSpan" id="kobo.473.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.474.1"><img alt="Figure 15.18: T﻿he embedding model" src="../Images/B22333_15_18.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.475.1">Figure 15</span></span><span class="koboSpan" id="kobo.476.1">.18: The embedding model</span></p>
<p class="normal"><span class="koboSpan" id="kobo.477.1">Unlike the tokenizer, which was a lookup table, this component is a neural network that produces embedding vectors of size 768. </span><span class="koboSpan" id="kobo.477.2">You can see that the layers in this network are a stack of 12 transformer</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.478.1"> modules, followed by a final layer of normalization.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.479.1">If we execute this model on the output from our prior step (cast as a tensor, the input type needed for the embedding model, and sent to the GPU with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.480.1">to</span></code><span class="koboSpan" id="kobo.481.1"> command), we’ll get an output of size 768 (for each token) representing the embedded prompt:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.482.1"><img alt="Figure 15.19: G﻿enerating the embedding from the prompt" src="../Images/B22333_15_19.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.483.1">Figure 15</span></span><span class="koboSpan" id="kobo.484.1">.19: Generating the embedding from the prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.485.1">Let’s dissect what is happening in the code block in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.486.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.487.1">.19.</span></em><span class="koboSpan" id="kobo.488.1"> The prompt (</span><code class="inlineCode"><span class="koboSpan" id="kobo.489.1">"a zombie in the style of Picasso"</span></code><span class="koboSpan" id="kobo.490.1">) is first passed to the tokenizer in the pipeline, with a maximum length of 77 (the maximum number of embeddable tokens). </span><span class="koboSpan" id="kobo.490.2">As we saw above, this function will return a byte-pair-encoded representation of the prompt. </span><span class="koboSpan" id="kobo.490.3">These tokens are then mapped to a numerical vector of length 768 each, which you can verify by examining the shape of the model output.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.491.1">In addition to encoding the prompt itself as a numerical vector, we also encode a blank prompt ( ““). </span><span class="koboSpan" id="kobo.491.2">This is because when </span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.492.1">we later pass the embedded prompt to the image generation step, we want to control how much importance we assign to the prompt in generating the image (using the </span><em class="italic"><span class="koboSpan" id="kobo.493.1">guidance scale </span></em><span class="koboSpan" id="kobo.494.1">parameter we’ll see later). </span><span class="koboSpan" id="kobo.494.2">To provide a reference, we need to also provide the embedding using no prompt at all, and the difference between the two will provide information to the image generation model on how to modify the generated image at each step of the process.</span></p>
<h2 class="heading-2" id="_idParaDest-291"><span class="koboSpan" id="kobo.495.1">Generating the latent image using the VAE decoder</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.496.1">To create an image</span><a id="_idIndexMarker1164"/><span class="koboSpan" id="kobo.497.1"> based on your prompt, Stable Diffusion starts with a matrix of normally distributed random numbers. </span><span class="koboSpan" id="kobo.497.2">This is because, as we</span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.498.1"> mentioned earlier, the model was developed using the random vectors (</span><em class="italic"><span class="koboSpan" id="kobo.499.1">latent </span></em><span class="koboSpan" id="kobo.500.1">vectors) generated by VAE that we saw in </span><a href="Chapter_11.xhtml"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.501.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.502.1">, which consists of an </span><em class="italic"><span class="koboSpan" id="kobo.503.1">encoder </span></em><span class="koboSpan" id="kobo.504.1">and a </span><em class="italic"><span class="koboSpan" id="kobo.505.1">decoder</span></em><span class="koboSpan" id="kobo.506.1">. </span><span class="koboSpan" id="kobo.506.2">As a reminder, the encoder is a neural network that takes as input an image and as output generates a (usually lower dimensional) vector or matrix of random numbers. </span><span class="koboSpan" id="kobo.506.3">This random number matrix is a kind of “barcode” for the image, which allows the important information to be compressed into a lower-dimensional space that takes up less memory on your computer – the fact that these vectors are smaller than the original image is one of the key optimizations that make the Stable Diffusion algorithm work so well. </span><span class="koboSpan" id="kobo.506.4">The decoder is a second neural network that is used to reverse this compression, turning a set of random numbers into an image.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.507.1">To see how this works, you can input an image into the </span><code class="inlineCode"><span class="koboSpan" id="kobo.508.1">vae</span></code><span class="koboSpan" id="kobo.509.1"> component of the Stable Diffusion pipeline, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.510.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.511.1">.20. </span></em><span class="koboSpan" id="kobo.512.1">First, you need to convert an input image into a tensor using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.513.1">torchvision to_tensor</span></code><span class="koboSpan" id="kobo.514.1"> function, then pass it through the encoder to create a 4 x 64 x 64 output – the </span><code class="inlineCode"><span class="koboSpan" id="kobo.515.1">half()</span></code><span class="koboSpan" id="kobo.516.1"> command is to</span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.517.1"> convert the input to float16. </span><span class="koboSpan" id="kobo.517.2">In this example, you can see we have compressed a 512-by-512 RGB image into a 4-by-64-by-64 vector.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.518.1"><img alt="Figure 15.20: Generating the latent vector using the VAE" src="../Images/B22333_15_20.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.519.1">Figure 15</span></span><span class="koboSpan" id="kobo.520.1">.20: Generating the latent vector using the VAE</span></p>
<p class="normal"><span class="koboSpan" id="kobo.521.1">Now you can</span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.522.1"> run the decoder to verify that you can turn this latent vector back into an image (which is the final step of the Stable Diffusion algorithm you’ll see in a bit), as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.523.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.524.1">.21.</span></em></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.525.1"><img alt="Figure 15.21: Decoding the latent vector" src="../Images/B22333_15_21.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.526.1">Figure 15</span></span><span class="koboSpan" id="kobo.527.1">.21: Decoding the latent vector</span></p>
<p class="normal"><span class="koboSpan" id="kobo.528.1">Now that we are able to generate samples from a latent vector and encode our prompt, we’re ready to generate images </span><a id="_idIndexMarker1168"/><span class="koboSpan" id="kobo.529.1">using the U-Net, the final network in the Stable Diffusion pipeline.</span></p>
<h2 class="heading-2" id="_idParaDest-292"><span class="koboSpan" id="kobo.530.1">The U-Net</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.531.1">The last element of the Stable </span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.532.1">Diffusion pipeline is U-Net, which takes the encoded prompt and a vector of random noise that is the same shape as an encoded image from the VAE (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.533.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.534.1">.2)</span></em><span class="koboSpan" id="kobo.535.1">. </span><span class="koboSpan" id="kobo.535.2">The U-Net, similar to the VAE, performs an encoding operation through a set of neural network layers and then decodes that </span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.536.1">output into a vector the same size as the random input. </span><span class="koboSpan" id="kobo.536.2">Each time we pass the latent vector through the U-Net, we are predicting how much noise, </span><em class="italic"><span class="koboSpan" id="kobo.537.1">e</span></em><span class="koboSpan" id="kobo.538.1">, to subtract from the latent vector in the last step. </span><span class="koboSpan" id="kobo.538.2">Running this operation multiple times constitutes the “reverse” process for the Stable Diffusion model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.539.1">Since there was no original image – we supplied a random vector – the encoded prompt provides the model with the context of what image to generate.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.540.1"><img alt="Figure 15.﻿22: The U-Net image generation process" src="../Images/B22333_15_22.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.541.1">Figure 15</span></span><span class="koboSpan" id="kobo.542.1">.22: The U-Net image generation process</span></p>
<p class="normal"><span class="koboSpan" id="kobo.543.1">Let’s walk through the steps of generating an image. </span><span class="koboSpan" id="kobo.543.2">Our first step is to generate a random input of the same dimension </span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.544.1">as the VAE output, using </span><code class="inlineCode"><span class="koboSpan" id="kobo.545.1">torch.randn</span></code><span class="koboSpan" id="kobo.546.1">. </span><span class="koboSpan" id="kobo.546.2">We set a fixed seed (manual seed) so that we can make this process repeatable by generating the same random vector each time we call the code – this will make it easy to debug.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.547.1">The component of the pipeline that will run the diffusion process – moving a random vector to a generated image – is called the </span><em class="italic"><span class="koboSpan" id="kobo.548.1">scheduler</span></em><span class="koboSpan" id="kobo.549.1">. </span><span class="koboSpan" id="kobo.549.2">It specifies </span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.550.1">a number of timesteps to run this diffusion process and what properties each of those timesteps has. </span><span class="koboSpan" id="kobo.550.2">For the Stable Diffusion pipeline we are using, the default </span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.551.1">scheduler is the </span><em class="italic"><span class="koboSpan" id="kobo.552.1">PNDMScheduler</span></em><sup class="superscript"><span class="koboSpan" id="kobo.553.1">9</span></sup><span class="koboSpan" id="kobo.554.1">. </span><span class="koboSpan" id="kobo.554.2">It specifies a set of differential equations to use to update the noise prediction at each step of the simulation; the amount of noise is determined by a parameter (</span><code class="inlineCode"><span class="koboSpan" id="kobo.555.1">init_noise_sigma</span></code><span class="koboSpan" id="kobo.556.1">) to scale our simple random input. </span><span class="koboSpan" id="kobo.556.2">Some schedulers apply different scaling/noise at each step of the simulation, but the PNDM scheduler does not, so we do not have to call the </span><code class="inlineCode"><span class="koboSpan" id="kobo.557.1">scale_model_input</span></code><span class="koboSpan" id="kobo.558.1"> function of the scheduler at each step.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.559.1">You’ll notice we also concatenate the blank embedding and prompt; this is more efficient than processing them sequentially and comparing the output and allows us to perform those calculations in parallel. </span><span class="koboSpan" id="kobo.559.2">Finally, we set the </span><em class="italic"><span class="koboSpan" id="kobo.560.1">guidance scale </span></em><span class="koboSpan" id="kobo.561.1">parameter, which defaults to 7.5. </span><span class="koboSpan" id="kobo.561.2">Lower values assign less importance to the input prompt, and will lead to an image that less resembles the prompt. </span><span class="koboSpan" id="kobo.561.3">Greater values will place more importance on the prompt.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.562.1">At each step of the diffusion process, we duplicate the latent vector so that it can be compared with the blank embedding </span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.563.1">and the prompt. </span><span class="koboSpan" id="kobo.563.2">We then pass the textual embedding and the latent image vector to the U-Net, which returns a prediction of what the latent vector would be without noise. </span><span class="koboSpan" id="kobo.563.3">We split this output into two parts; one where that output has been conditioned using the embedded prompt and one that receives the blank embedding.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.564.1">We then create the final U-Net output, </span><code class="inlineCode"><span class="koboSpan" id="kobo.565.1">noise_pred</span></code><span class="koboSpan" id="kobo.566.1">, at each step of the diffusion process by adding in a weighted difference between the prompt-conditioned and unconditional outputs, with the importance of that difference provided by the </span><code class="inlineCode"><span class="koboSpan" id="kobo.567.1">guidance_scale</span></code><span class="koboSpan" id="kobo.568.1">. </span><span class="koboSpan" id="kobo.568.2">Then we run the scheduler diffusion equation to generate </span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.569.1">the input for the next pass.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.570.1">After several rounds (here, 50) of passing the random vector through the U-Net, we decode it with the VAE to get the final output. </span><span class="koboSpan" id="kobo.570.2">The code in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.571.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.572.1">.23 </span></em><span class="koboSpan" id="kobo.573.1">shows how this happens.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.574.1"><img alt="Figure﻿ 15.23: Decoding the U-Net output with the VAE" src="../Images/B22333_15_23.png"/></span></figure>
<p class="packt_figref"><span class="No-Break"><span class="koboSpan" id="kobo.575.1">Figure 15</span></span><span class="koboSpan" id="kobo.576.1">.23: Decoding the U-Net output with the VAE</span></p>
<p class="normal"><span class="koboSpan" id="kobo.577.1">We need to undo the noise scaling we applied at the beginning of the scheduler (</span><code class="inlineCode"><span class="koboSpan" id="kobo.578.1">init_sigma_noise</span></code><span class="koboSpan" id="kobo.579.1">) by dividing by the </span><em class="italic"><span class="koboSpan" id="kobo.580.1">random</span></em><span class="koboSpan" id="kobo.581.1"> variable we had used as a multiplier earlier when we began the</span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.582.1"> diffusion process, then use the decoder arm of the VAE to obtain the image</span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.583.1"> from the latent vector. </span><span class="koboSpan" id="kobo.583.2">We recenter the output and then bind it between 0 and 1 so that the colors will show up correctly in the notebook.</span></p>
<h1 class="heading-1" id="_idParaDest-293"><span class="koboSpan" id="kobo.584.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.585.1">In this chapter, we looked at how the Stable Diffusion algorithm was developed and how it is implemented through the Hugging Face pipeline API. </span><span class="koboSpan" id="kobo.585.2">In the process, we saw how a diffusion model addresses conceptual problems with autoregressive transformer and GAN models by modeling the distribution of natural pixels. </span><span class="koboSpan" id="kobo.585.3">We also saw how this generative diffusion process can be represented as a reversible Markov process, and how we can train the parameters of a diffusion model using a variational bound, similar to a VAE.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.586.1">Furthermore, we saw how the efficiency of a diffusion model is improved by executing the forward and reverse process in latent space in the Stable Diffusion model. </span><span class="koboSpan" id="kobo.586.2">We also illustrated how natural language user prompts are represented as byte encodings and transformed into numerical vectors. </span><span class="koboSpan" id="kobo.586.3">Finally, we looked at the role of the VAE in generating compressed image vectors, and how the U-Net of Stable Diffusion uses the embedded user prompt and a vector of random numbers to generate images by predicting the amount of noise that should be removed in each step of the reverse process.</span></p>
<h1 class="heading-1" id="_idParaDest-294"><span class="koboSpan" id="kobo.587.1">References</span></h1>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.588.1">Ramesh, Aditya et al. </span><span class="koboSpan" id="kobo.588.2">“</span><em class="italic"><span class="koboSpan" id="kobo.589.1">Zero-Shot Text-to-Image Generation.</span></em><span class="koboSpan" id="kobo.590.1">” </span><em class="italic"><span class="koboSpan" id="kobo.591.1">ArXiv</span></em><span class="koboSpan" id="kobo.592.1"> abs/2102.12092 (2021).</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.593.1">Brock, Andrew; Donahue, Jeff; and Simonyan, Karen. </span><span class="koboSpan" id="kobo.593.2">“</span><em class="italic"><span class="koboSpan" id="kobo.594.1">Large scale GAN training for high fidelity natural image synthesis.</span></em><span class="koboSpan" id="kobo.595.1">”</span><em class="italic"><span class="koboSpan" id="kobo.596.1"> arXiv preprint arXiv:1809.11096</span></em><span class="koboSpan" id="kobo.597.1"> (2018).</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.598.1">Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; and Ganguli, Surya (2015-06-01). </span><code class="inlineCode"><span class="koboSpan" id="kobo.599.1">"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"</span></code><span class="koboSpan" id="kobo.600.1"> (PDF). </span><em class="italic"><span class="koboSpan" id="kobo.601.1">Proceedings of the 32nd International Conference on Machine Learning</span></em><span class="koboSpan" id="kobo.602.1">. </span><span class="koboSpan" id="kobo.602.2">37. </span><span class="koboSpan" id="kobo.602.3">PMLR: 2256–2265.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.603.1">Ho, Jonathan; Jain, Ajay; and Abbeel, Pieter. </span><span class="koboSpan" id="kobo.603.2">“</span><em class="italic"><span class="koboSpan" id="kobo.604.1">Denoising diffusion probabilistic models.</span></em><span class="koboSpan" id="kobo.605.1">” </span><em class="italic"><span class="koboSpan" id="kobo.606.1">Advances in neural information processing systems</span></em><span class="koboSpan" id="kobo.607.1"> 33 (2020): 6840-6851.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.608.1">Rombach, Robin et al. </span><span class="koboSpan" id="kobo.608.2">“</span><em class="italic"><span class="koboSpan" id="kobo.609.1">High-Resolution Image Synthesis with Latent Diffusion Models.</span></em><span class="koboSpan" id="kobo.610.1">” </span><em class="italic"><span class="koboSpan" id="kobo.611.1">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span></em><span class="koboSpan" id="kobo.612.1"> (2021): 10674-10685.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.613.1">Ronneberger, Olaf; Fischer, Philipp; and Brox, Thomas. </span><span class="koboSpan" id="kobo.613.2">Unet: Convolutional networks for biomedical image segmentation. </span><span class="koboSpan" id="kobo.613.3">In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234–241. </span><span class="koboSpan" id="kobo.613.4">Springer, 2015.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.614.1">Radford, Alec et al. </span><span class="koboSpan" id="kobo.614.2">“</span><em class="italic"><span class="koboSpan" id="kobo.615.1">Learning transferable visual models from natural language supervision.</span></em><span class="koboSpan" id="kobo.616.1">” </span><em class="italic"><span class="koboSpan" id="kobo.617.1">International conference on machine learning</span></em><span class="koboSpan" id="kobo.618.1">. </span><span class="koboSpan" id="kobo.618.2">PmLR, 2021.</span></li>
<li class="numberedList"><a href="http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM"><span class="url"><span class="koboSpan" id="kobo.619.1">http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM</span></span></a></li>
<li class="numberedList"><a href="https://arxiv.org/pdf/2202.09778.pdf"><span class="url"><span class="koboSpan" id="kobo.620.1">https://arxiv.org/pdf/2202.09778.pdf</span></span></a><span class="koboSpan" id="kobo.621.1"> Liu, Luping et al. </span><span class="koboSpan" id="kobo.621.2">“</span><em class="italic"><span class="koboSpan" id="kobo.622.1">Pseudo numerical methods for diffusion models on manifolds.</span></em><span class="koboSpan" id="kobo.623.1">” </span><em class="italic"><span class="koboSpan" id="kobo.624.1">arXiv preprint arXiv:2202.09778</span></em><span class="koboSpan" id="kobo.625.1"> (2022).</span></li>
</ol>
<h1 class="heading-1" id="_idParaDest-295"><span class="koboSpan" id="kobo.626.1">Join our communities on Discord and Reddit</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.627.1">Have questions about the book or want to contribute to discussions on Generative AI and LLMs? </span><span class="koboSpan" id="kobo.627.2">Join our Discord server at </span><a href="https://packt.link/I1tSU"><span class="url"><span class="koboSpan" id="kobo.628.1">https://packt.link/I1tSU</span></span></a><span class="koboSpan" id="kobo.629.1"> and our Reddit channel at </span><a href="https://packt.link/rmYYs"><span class="url"><span class="koboSpan" id="kobo.630.1">https://packt.link/rmYYs</span></span></a><span class="koboSpan" id="kobo.631.1"> to connect, share, and collaborate with like-minded AI professionals.</span></p>
<table class="table-container" id="table001-10">
<tbody>
<tr>
<td class="table-cell">
<p class="center"><strong class="keyWord"><span class="koboSpan" id="kobo.632.1">Discord QR</span></strong></p>
</td>
<td class="table-cell">
<p class="center"><strong class="keyWord"><span class="koboSpan" id="kobo.633.1">Reddit QR</span></strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="center"><span class="koboSpan" id="kobo.634.1"><img alt="" src="../Images/Discord_Babcock_1.png" style="width:10em"/></span></p>
</td>
<td class="table-cell">
<p class="center"><span class="koboSpan" id="kobo.635.1"><img alt="" src="../Images/Reddit_Babcock.png" style="width:10em"/></span></p>
</td>
</tr>
</tbody>
</table>
</div>


<div id="_idContainer667">
<p class="BM-packtLogo"><span class="koboSpan" id="kobo.1.1"><img alt="" src="../Images/New_Packt_Logo1.png"/></span></p>
<p class="FM-copyright-text"><a href="https://www.packtpub.com"><span class="url"><span class="koboSpan" id="kobo.2.1">www.packtpub.com</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.3.1">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. </span><span class="koboSpan" id="kobo.3.2">For more information, please visit our website.</span></p>
<h1 class="heading-1" id="_idParaDest-296"><span class="koboSpan" id="kobo.4.1">Why subscribe?</span></h1>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.5.1">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.6.1">Improve your learning with Skill Plans built especially for you</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.7.1">Get a free eBook or video every month</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">Fully searchable for easy access to vital information</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.9.1">Copy and paste, print, and bookmark content</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.10.1">At </span><a href="https://www.packt.com"><span class="url"><span class="koboSpan" id="kobo.11.1">www.packt.com</span></span></a><span class="koboSpan" id="kobo.12.1">, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</span></p>
<p class="eop"/>
<h1 class="mainHeading" id="_idParaDest-297"><span class="koboSpan" id="kobo.13.1">Other Books You May Enjoy</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.14.1">If you enjoyed this book, you may be interested in these other books by Packt:</span></p>
<p class="normal"><a href="https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062"><span class="koboSpan" id="kobo.15.1"><img alt="" src="../Images/9781836200079.jpg"/></span></a></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.16.1">LLM Engineer’s Handbook</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.17.1">Paul Iusztin, Maxime Labonne</span></p>
<p class="normal"><span class="koboSpan" id="kobo.18.1">ISBN: 978-1-83620-007-9</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.19.1">Implement robust data pipelines and manage LLM training cycles</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.20.1">Create your own LLM and refine it with the help of hands-on examples</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.21.1">Get started with LLMOps by diving into core MLOps principles such as orchestrators and prompt monitoring</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.22.1">Perform supervised fine-tuning and LLM evaluation</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.23.1">Deploy end-to-end LLM solutions using AWS and other tools</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.24.1">Design scalable and modularLLM systems</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.25.1">Learn about RAG applications by building a feature and inference pipeline</span></li>
</ul>
<p class="normal"><a href="https://www.packtpub.com/en-us/product/generative-ai-with-amazon-bedrock-9781804618585"><span class="koboSpan" id="kobo.26.1"><img alt="" src="../Images/9781803247281.jpg"/></span></a></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.27.1">Generative AI with Amazon Bedrock</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.28.1">Shikhar Kwatra, Bunny Kaushik</span></p>
<p class="normal"><span class="koboSpan" id="kobo.29.1">ISBN: 978-1-80324-728-1</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.30.1">Explore the generative AI landscape and foundation models in Amazon Bedrock</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.31.1">Fine-tune generative models to improve their performance</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.32.1">Explore several architecture patterns for different business use cases</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.33.1">Gain insights into ethical AI practices, model governance, and risk mitigation strategies</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.34.1">Enhance your skills in employing agents to develop intelligence and orchestrate tasks</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.35.1">Monitor and understand metrics and Amazon Bedrock model response</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.36.1">Explore various industrial use cases and architectures to solve real-world business problems using RAG</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.37.1">Stay on top of architectural best practices and industry standards</span></li>
</ul>
<p class="eop"/>
<h1 class="heading-1" id="_idParaDest-298"><span class="koboSpan" id="kobo.38.1">Packt is searching for authors like you</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.39.1">If you’re interested in becoming an author for Packt, please visit </span><a href="https://authors.packtpub.com"><span class="url"><span class="koboSpan" id="kobo.40.1">authors.packtpub.com</span></span></a><span class="koboSpan" id="kobo.41.1"> and apply today. </span><span class="koboSpan" id="kobo.41.2">We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. </span><span class="koboSpan" id="kobo.41.3">You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</span></p>
</div>
<div class="Basic-Text-Frame" id="_idContainer668">
<p class="eop"/>
<h1 class="heading-1" id="_idParaDest-299"><span class="koboSpan" id="kobo.42.1">Share your thoughts</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.43.1">Now you’ve finished </span><em class="italic"><span class="koboSpan" id="kobo.44.1">Generative AI with Python and PyTorch</span></em><span class="koboSpan" id="kobo.45.1">,</span><em class="italic"><span class="koboSpan" id="kobo.46.1"> Second Edition</span></em><span class="koboSpan" id="kobo.47.1">, we’d love to hear your thoughts! </span><span class="koboSpan" id="kobo.47.2">If you purchased the book from Amazon, please </span><a href="https://packt.link/r/1835884458"><span class="url"><span class="koboSpan" id="kobo.48.1">click here to go straight to the Amazon review page</span></span></a><span class="koboSpan" id="kobo.49.1"> for this book and share your feedback or leave a review on the site that you purchased it from.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.50.1">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</span></p>
</div>
</body></html>