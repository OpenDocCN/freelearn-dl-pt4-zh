<html><head></head><body>
        <section id="2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Convolution Neural Network</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre12">
<li class="calibre13">Downloading and configuring an image dataset</li>
<li class="calibre13">Learning the architecture of a CNN classifier</li>
<li class="calibre13">Using functions to initialize weights and biases</li>
<li class="calibre13">Using functions to create a new convolution layer</li>
<li class="calibre13">Using functions to flatten the densely connected layer</li>
<li class="calibre13">Defining placeholder variables</li>
<li class="calibre13">Creating the first convolution layer</li>
<li class="calibre13">Creating the second convolution layer</li>
<li class="calibre13">Flattening the second convolution layer</li>
<li class="calibre13">Creating the first fully connected layer</li>
<li class="calibre13">Applying dropout to the first fully connected layer</li>
<li class="calibre13">Creating the second fully connected layer with dropout</li>
<li class="calibre13">Applying softmax activation to obtain a predicted class</li>
<li class="calibre13">Defining the cost function used for optimization</li>
<li class="calibre13">Performing gradient descent cost optimization</li>
<li class="calibre13">Executing the graph in a TensorFlow session</li>
<li class="calibre13">Evaluating the performance on test data</li>
</ul>


            </article>

            
        </section>
    

        <section id="2PKKS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Convolution neural networks</strong> (<strong class="calibre1">CNN</strong>) are a category of deep learning neural networks with a prominent role in building image recognition- and natural language processing-based classification models.</p>
<div class="packt_infobox">The CNN follows a similar architecture to LeNet, which was primarily designed to recognize characters such as numbers, zip codes, and so on. As against artificial neural networks, CNN have layers of neurons arranged in three-dimensional space (width, depth, and height). Each layer transforms a two-dimensional image into a three-dimensional input volume, which is then transformed into a three-dimensional output volume using neuron activation.</div>
<p class="calibre2">Primarily, CNNs are built using three main types of activation layers: convolution layer ReLU, pooling layer, and fully connected layer. The convolution layer is used to extract features (spatial relationship between pixels) from the input vector (of images) and stores them for further processing after computing a dot product with weights (and biases).</p>
<p class="calibre2"><strong class="calibre1">Rectified Linear Unit</strong> (<strong class="calibre1">ReLU</strong>) is then applied after convolution to introduce non-linearity in the operation.</p>
<p class="calibre2">This is an element-wise operation (such as a threshold function, sigmoid, and tanh) applied to each convolved feature map. Then, the pooling layer (operations such as max, mean, and sum) is used to downsize the dimensionality of each feature map ensuring minimum information loss. This operation of spatial size reduction is used to control overfitting and increase the robustness of the network to small distortions or transformations. The output of the pooling layer is then connected to a traditional multilayer perceptron (also called the fully connected layer). This perceptron uses activation functions such as softmax or SVM to build classifier-based CNN models.</p>
<p class="calibre2">The recipes in this chapter will focus on building a convolution neural network for image classification using Tensorflow in R. While the recipes will provide you with an overview of a typical CNN, we encourage you to adapt and modify the parameters according to your needs.</p>


            </article>

            
        </section>
    

        <section id="2QJ5E1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Downloading and configuring an image dataset</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will use the CIFAR-10 dataset to build a convolution neural network for image classification. The CIFAR-10 dataset consists of 60,000 32 x 32 color images of 10 classes, with 6,000 images per class. These are further divided into five training batches and one test batch, each with 10,000 images.</p>
<p class="calibre2">The test batch contains exactly 1,000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5,000 images from each class. The ten outcome classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The classes are completely mutually exclusive. In addition, the format of the dataset is as follows:</p>
<ul class="calibre12">
<li class="calibre13">The first column: The label with 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck</li>
<li class="calibre13">The next 1,024 columns: Red pixels in the range of 0 to 255</li>
<li class="calibre13">The next 1,024 columns: Green pixels in the range of 0 to 255</li>
<li class="calibre13">The next 1,024 columns: Blue pixels in the range of 0 to 255</li>
</ul>


            </article>

            
        </section>
    

        <section id="2RHM01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">For this recipe, you will require R with some packages installed such as <kbd class="calibre10">data.table</kbd> and <kbd class="calibre10">imager</kbd>.</p>


            </article>

            
        </section>
    

        <section id="2SG6I1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Start R (using Rstudio or Docker) and load the required packages.</li>
<li value="2" class="calibre13">Download the dataset (binary version) from <a href="http://www.cs.toronto.edu/~kriz/cifar.html" class="calibre4">http://www.cs.toronto.edu/~kriz/cifar.html</a> manually or use the following function to download the data in the R environment. The function takes the working directory or the downloaded dataset's location path as an input parameter (<kbd class="calibre10">data_dir</kbd>):</li>
</ol>
<pre class="calibre23">
# Function to download the binary file<br class="title-page-tagline"/>download.cifar.data &lt;- function(data_dir) {<br class="title-page-tagline"/>dir.create(data_dir, showWarnings = FALSE)<br class="title-page-tagline"/>setwd(data_dir)<br class="title-page-tagline"/>if (!file.exists('cifar-10-binary.tar.gz')){<br class="title-page-tagline"/>download.file(url='http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz', destfile='cifar-10-binary.tar.gz', method='wget')<br class="title-page-tagline"/>untar("cifar-10-binary.tar.gz") # Unzip files<br class="title-page-tagline"/>file.remove("cifar-10-binary.tar.gz") # remove zip file<br class="title-page-tagline"/>}<br class="title-page-tagline"/>setwd("..")<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Download the data<br class="title-page-tagline"/>download.cifar.data(data_dir="Cifar_10/")
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Once the dataset is downloaded and untarred (or unzipped), read it in the R environment as train and test datasets. The function takes the filenames of the train and test batch datasets (<kbd class="calibre10">filenames</kbd>) and the number of images to retrieve per batch file (<kbd class="calibre10">num.images</kbd>) as input parameters:</li>
</ol>
<pre class="calibre23">
# Function to read cifar data<br class="title-page-tagline"/>read.cifar.data &lt;- function(filenames,num.images){<br class="title-page-tagline"/>images.rgb &lt;- list()<br class="title-page-tagline"/>images.lab &lt;- list()<br class="title-page-tagline"/>for (f in 1:length(filenames)) {<br class="title-page-tagline"/>to.read &lt;- file(paste("Cifar_10/",filenames[f], sep=""), "rb")<br class="title-page-tagline"/>for(i in 1:num.images) {<br class="title-page-tagline"/>l &lt;- readBin(to.read, integer(), size=1, n=1, endian="big")<br class="title-page-tagline"/>r &lt;- as.integer(readBin(to.read, raw(), size=1, n=1024, endian="big"))<br class="title-page-tagline"/>g &lt;- as.integer(readBin(to.read, raw(), size=1, n=1024, endian="big"))<br class="title-page-tagline"/>b &lt;- as.integer(readBin(to.read, raw(), size=1, n=1024, endian="big"))<br class="title-page-tagline"/>index &lt;- num.images * (f-1) + i<br class="title-page-tagline"/>images.rgb[[index]] = data.frame(r, g, b)<br class="title-page-tagline"/>images.lab[[index]] = l+1<br class="title-page-tagline"/>}<br class="title-page-tagline"/>close(to.read)<br class="title-page-tagline"/>cat("completed :", filenames[f], "\n")<br class="title-page-tagline"/>remove(l,r,g,b,f,i,index, to.read)<br class="title-page-tagline"/>}<br class="title-page-tagline"/>return(list("images.rgb"=images.rgb,"images.lab"=images.lab))<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Train dataset<br class="title-page-tagline"/>cifar_train &lt;- read.cifar.data(filenames = c("data_batch_1.bin","data_batch_2.bin","data_batch_3.bin","data_batch_4.bin", "data_batch_5.bin"))<br class="title-page-tagline"/>images.rgb.train &lt;- cifar_train$images.rgb<br class="title-page-tagline"/>images.lab.train &lt;- cifar_train$images.lab<br class="title-page-tagline"/>rm(cifar_train)<br class="title-page-tagline"/># Test dataset<br class="title-page-tagline"/>cifar_test &lt;- read.cifar.data(filenames = c("test_batch.bin"))<br class="title-page-tagline"/>images.rgb.test &lt;- cifar_test$images.rgb<br class="title-page-tagline"/>images.lab.test &lt;- cifar_test$images.lab<br class="title-page-tagline"/>rm(cifar_test)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">The outcome of the earlier function is a list of red, green, and blue pixel dataframes for each image along with their labels. Then, flatten the data into a list of two dataframes (one for input and the other for output) using the following function, which takes two parameters--a list of input variables (<kbd class="calibre10">x_listdata</kbd>) and a list of output variables (<kbd class="calibre10">y_listdata</kbd>):</li>
</ol>
<pre class="calibre23">
# Function to flatten the data<br class="title-page-tagline"/>flat_data &lt;- function(x_listdata,y_listdata){<br class="title-page-tagline"/># Flatten input x variables<br class="title-page-tagline"/>x_listdata &lt;- lapply(x_listdata,function(x){unlist(x)})<br class="title-page-tagline"/>x_listdata &lt;- do.call(rbind,x_listdata)<br class="title-page-tagline"/># Flatten outcome y variables<br class="title-page-tagline"/>y_listdata &lt;- lapply(y_listdata,function(x){a=c(rep(0,10)); a[x]=1; return(a)})<br class="title-page-tagline"/>y_listdata &lt;- do.call(rbind,y_listdata)<br class="title-page-tagline"/># Return flattened x and y variables<br class="title-page-tagline"/>return(list("images"=x_listdata, "labels"=y_listdata))<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Generate flattened train and test datasets<br class="title-page-tagline"/>train_data &lt;- flat_data(x_listdata = images.rgb.train, y_listdata = images.lab.train)<br class="title-page-tagline"/>test_data &lt;- flat_data(x_listdata = images.rgb.test, y_listdata = images.lab.test)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Once the list of input and output train and test dataframes is ready, perform sanity checks by plotting the images along with their labels. The function requires two mandatory parameters (<kbd class="calibre10">index</kbd>: image row number and <kbd class="calibre10">images.rgb</kbd>: flattened input dataset) and one optional parameter (<kbd class="calibre10">images.lab</kbd>: flattened output dataset):</li>
</ol>
<pre class="calibre23">
labels &lt;- read.table("Cifar_10/batches.meta.txt")<br class="title-page-tagline"/># function to run sanity check on photos &amp; labels import<br class="title-page-tagline"/>drawImage &lt;- function(index, images.rgb, images.lab=NULL) {<br class="title-page-tagline"/>require(imager)<br class="title-page-tagline"/># Testing the parsing: Convert each color layer into a matrix,<br class="title-page-tagline"/># combine into an rgb object, and display as a plot<br class="title-page-tagline"/>img &lt;- images.rgb[[index]]<br class="title-page-tagline"/>img.r.mat &lt;- as.cimg(matrix(img$r, ncol=32, byrow = FALSE))<br class="title-page-tagline"/>img.g.mat &lt;- as.cimg(matrix(img$g, ncol=32, byrow = FALSE)<br class="title-page-tagline"/>img.b.mat &lt;- as.cimg(matrix(img$b, ncol=32, byrow = FALSE))<br class="title-page-tagline"/>img.col.mat &lt;- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c") #Bind the three channels into one image<br class="title-page-tagline"/># Extract the label<br class="title-page-tagline"/>if(!is.null(images.lab)){<br class="title-page-tagline"/>lab = labels[[1]][images.lab[[index]]]<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Plot and output label<br class="title-page-tagline"/>plot(img.col.mat,main=paste0(lab,":32x32 size",sep=" "),xaxt="n")<br class="title-page-tagline"/>axis(side=1, xaxp=c(10, 50, 4), las=1)<br class="title-page-tagline"/>return(list("Image label" =lab,"Image description" =img.col.mat))<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Draw a random image along with its label and description from train dataset<br class="title-page-tagline"/>drawImage(sample(1:50000, size=1), images.rgb.train, images.lab.train)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Now transform the input data using the min-max standardization technique. The <kbd class="calibre10">preProcess</kbd> function from the package can be used for normalization. The <kbd class="calibre10">"range"</kbd> option of the method performs min-max normalization as follows:</li>
</ol>
<pre class="calibre23">
# Function to normalize data<br class="title-page-tagline"/>Require(caret) <br class="title-page-tagline"/>normalizeObj&lt;-preProcess(train_data$images, method="range") <br class="title-page-tagline"/>train_data$images&lt;-predict(normalizeObj, train_data$images) <br class="title-page-tagline"/>test_data$images &lt;- predict(normalizeObj, test_data$images) 
</pre>


            </article>

            
        </section>
    

        <section id="2TEN41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Let's take a look at what we did in the earlier recipe. In step 2, we downloaded the CIFAR-10 dataset from the link mentioned in case it is not present in the given link or working directory. In step 3, the unzipped files are loaded in the R environment as train and test datasets. The train dataset has a list of 50,000 images and the test dataset has a list of 10,000 images along with their labels. Then, in step 4, the train and test datasets are flattened into a list of two dataframes: one with input variables (or images) of length 3,072 (1,024 of red, 1,024 of green, and 1,024 of blue) and the other with output variables (or labels) of length 10 (binary for each class). In step 5, we perform sanity checks for the created train and test datasets by generating plots. The <span>following</span> figure shows a set of six train images along with their labels. Finally, in step 6, the input data is transformed using the min-max standardization technique. An example of categories from the CIFAR-10 dataset is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border33" src="../images/00030.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="2UD7M1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">See also</h1>
                
            
            <article>
                
<p class="calibre2"><em class="calibre9">Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009</em> (<a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank" class="calibre4">http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>). This is also the reference for this section.</p>


            </article>

            
        </section>
    

        <section id="2VBO81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Learning the architecture of a CNN classifier</h1>
                
            
            <article>
                
<p class="calibre2">The CNN classifier covered in this chapter has two convolution layers followed by two fully connected layers in the end, in which the last layer acts as a classifier using the softmax <kbd class="calibre10">activation</kbd> function.</p>


            </article>

            
        </section>
    

        <section id="30A8Q1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The recipe requires the CIFAR-10 dataset. Thus, the CIFAR-10 dataset should be downloaded and loaded into the R environment. Also, images are of size 32 x 32 pixels.</p>


            </article>

            
        </section>
    

        <section id="318PC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Let's define the configuration of the CNN classifier as follows:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Each input image (CIFAR-10) is of size 32 x 32 pixels and can be labeled one among 10 classes:</li>
</ol>
<pre class="calibre23">
# CIFAR images are 32 x 32 pixels.<br class="title-page-tagline"/>img_width  = 32L<br class="title-page-tagline"/>img_height = 32L<br class="title-page-tagline"/><br class="title-page-tagline"/># Tuple with height and width of images used to reshape arrays.<br class="title-page-tagline"/>img_shape = c(img_width, img_height)<br class="title-page-tagline"/># Number of classes, one class for each of 10 images<br class="title-page-tagline"/>num_classes = 10L
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The images of the CIFAR-10 dataset have three channels (red, green, and blue):</li>
</ol>
<pre class="calibre23">
# Number of color channels for the images: 3 channel for red, blue, green scales.<br class="title-page-tagline"/>num_channels = 3L
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">The images are stored in one-dimensional arrays of the following length (<kbd class="calibre10">img_size_flat</kbd>):</li>
</ol>
<pre class="calibre23">
# Images are stored in one-dimensional arrays of length.<br class="title-page-tagline"/>img_size_flat = img_width * img_height * num_channels
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">In the first convolution layer, the size (width x height) of the convolution filter is 5 x 5 pixels (<kbd class="calibre10">filter_size1</kbd>) and the depth (or number) of convolution filter is <kbd class="calibre10">64</kbd> (<kbd class="calibre10">num_filters1</kbd>):</li>
</ol>
<pre class="calibre23">
# Convolutional Layer 1.<br class="title-page-tagline"/>filter_size1 = 5L<br class="title-page-tagline"/>num_filters1 = 64L
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">In the second convolution layer, the size and depth of the convolution filter is the same as the first convolution layer:</li>
</ol>
<pre class="calibre23">
# Convolutional Layer 2.<br class="title-page-tagline"/>filter_size2 = 5L<br class="title-page-tagline"/>num_filters2 = 64L
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Similarly, the output of the first fully connected layer is the same as the input of the second fully connected layer:</li>
</ol>
<pre class="calibre23">
# Fully-connected layer.<br class="title-page-tagline"/>fc_size = 1024L
</pre>


            </article>

            
        </section>
    

        <section id="3279U1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The dimensions and characteristics of an input image are shown in steps 1 and 2, respectively. Every input image is further processed in a convolution layer using a set of filters as defined in steps 4 and 5. The first convolution layer results in a set of 64 images (one for each set filter). In addition, the resolution of these images are also reduced to half (because of 2 x 2 max pooling); namely, from 32 x 32 pixels to 16 x 16 pixels.</p>
<p class="calibre2">The second convolution layer will input these 64 images and provide an output of new 64 images with further reduced resolutions. The updated resolution is now 8 x 8 pixels (again due to 2 x 2 max pooling). In the second convolution layer, a total of 64 x 64 = 4,096 filters are created, which are then further convoluted into 64 output images (or channels). Remember that these 64 images of 8 x 8 resolution correspond to a single input image.</p>
<p class="calibre2">Further, these 64 output images of 8 x 8 pixels are flattened into a single vector of length 4,096 (8 x 8 x 64), as defined in step 3, and are used as an input to a fully connected layer of a given set of neurons, as defined in step 6. The vector of 4,096 elements is then fed into the first fully connected layer of 1,024 neurons. The output neurons are again fed into a second fully connected layer of 10 neurons (equal to <kbd class="calibre10">num_classes</kbd>). These 10 neurons represent each of the class labels, which are then used to determine the (final) class of the image.</p>
<p class="calibre2">First, the weights of the convolution and fully connected layers are randomly initialized till the classification stage (the end of CNN graph). Here, the classification error is computed based on the true class and the predicted class (also called cross entropy).</p>
<p class="calibre2">Then, the optimizer backpropagates the error through the convolution network using the chain rule of differentiation, post which the weights of the layers (or filters) are updated such that the error is minimized. This entire cycle of one forward and backward propagation is called one iteration. Thousands of such iterations are performed till the classification error is reduced to a sufficiently low value.</p>
<div class="packt_infobox">Generally, these iterations are performed using a batch of images instead of a single image to increase the efficiency of computation.</div>
<p class="calibre2">The following image represents the convolution network designed in this chapter:</p>
<div class="cdpaligncenter"><img class="image-border34" src="../images/00032.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="335QG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using functions to initialize weights and biases</h1>
                
            
            <article>
                
<p class="calibre2">Weights and biases form an integral part of any deep neural network optimization and here we define a couple of functions to automate these initializations. It is a good practice to initialize weights with small noise to break symmetry and prevent zero gradients. Additionally, a small positive initial bias would avoid inactivated neurons, suitable for ReLU activation neurons.</p>


            </article>

            
        </section>
    

        <section id="344B21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Weights and biases are model coefficients which need to be initialized before model compilation. This steps require the <kbd class="calibre10">shape</kbd> parameter to be determined based on input dataset.</p>


            </article>

            
        </section>
    

        <section id="352RK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">The following function is used to return randomly initialized weights:</li>
</ol>
<pre class="calibre23">
# Weight Initialization<br class="title-page-tagline"/>weight_variable &lt;- function(shape) {<br class="title-page-tagline"/>initial &lt;- tf$truncated_normal(shape, stddev=0.1)<br class="title-page-tagline"/>tf$Variable(initial)<br class="title-page-tagline"/>}
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">The following function is used to return constant biases:</li>
</ol>
<pre class="calibre23">
bias_variable &lt;- function(shape) {<br class="title-page-tagline"/>initial &lt;- tf$constant(0.1, shape=shape)<br class="title-page-tagline"/>tf$Variable(initial)<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="361C61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">These functions return TensorFlow variables that are later used as part of a Tensorflow graph. The <kbd class="calibre10">shape</kbd> is defined as a list of attributes defining a filter in the convolution layer, which is covered in the next recipe. The weights are randomly initialized with a standard deviation equal to <kbd class="calibre10">0.1</kbd> and biases are initialized with a constant value of <kbd class="calibre10">0.1</kbd>.</p>


            </article>

            
        </section>
    

        <section id="36VSO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using functions to create a new convolution layer</h1>
                
            
            <article>
                
<p class="calibre2">Creating a convolution layer is the primary step in a CNN TensorFlow computational graph. This function is primarily used to define the mathematical formulas in the TensorFlow graph, which is later used in actual computation during optimization.</p>


            </article>

            
        </section>
    

        <section id="37UDA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The input dataset is defined and loaded. The <kbd class="calibre10">create_conv_layer</kbd> function presented in the recipe takes the following five input parameters and needs to be defined while setting-up a convolution layer:</p>
<ol class="calibre15">
<li value="1" class="calibre13"><kbd class="calibre10">Input</kbd>: This is a four-dimensional tensor (or a list) that comprises a number of (input) images, the height of each image (here 32L), the width of each image (here 32L), and the number of channels of each image (here 3L : red, blue, and green).</li>
<li value="2" class="calibre13"><kbd class="calibre10">Num_input_channels</kbd>: This is defined as the number of color channels in the case of the first convolution layer or the number of filter channels in the case of subsequent convolution layers.</li>
<li value="3" class="calibre13"><kbd class="calibre10">Filter_size</kbd>: This is defined as the width and height of each filter in the convolution layer. Here, the filter is assumed to be a square.</li>
<li value="4" class="calibre13"><kbd class="calibre10">Num_filters</kbd>: This is defined as the number of filters in a given convolution layer.</li>
<li value="5" class="calibre13"><kbd class="calibre10">Use_pooling</kbd>: This is a binary variable that is used perform 2 x 2 max pooling.</li>
</ol>


            </article>

            
        </section>
    

        <section id="38STS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the following function to create a new convolution layer:</li>
</ol>
<pre class="calibre23">
# Create a new convolution layer<br class="title-page-tagline"/>create_conv_layer &lt;- function(input,<br class="title-page-tagline"/>num_input_channels,<br class="title-page-tagline"/>filter_size,<br class="title-page-tagline"/>num_filters,<br class="title-page-tagline"/>use_pooling=True)<br class="title-page-tagline"/>{<br class="title-page-tagline"/># Shape of the filter-weights for the convolution.<br class="title-page-tagline"/>shape1 = shape(filter_size, filter_size, num_input_channels, num_filters)<br class="title-page-tagline"/># Create new weights<br class="title-page-tagline"/>weights = weight_variable(shape=shape1)<br class="title-page-tagline"/># Create new biases<br class="title-page-tagline"/>biases = bias_variable(shape=shape(num_filters))<br class="title-page-tagline"/># Create the TensorFlow operation for convolution.<br class="title-page-tagline"/>layer = tf$nn$conv2d(input=input,<br class="title-page-tagline"/>filter=weights,<br class="title-page-tagline"/>strides=shape(1L, 1L, 1L ,1L),<br class="title-page-tagline"/>padding="SAME")<br class="title-page-tagline"/># Add the biases to the results of the convolution.<br class="title-page-tagline"/>layer = layer + biases<br class="title-page-tagline"/># Use pooling (binary flag) to reduce the image resolution<br class="title-page-tagline"/>if(use_pooling){<br class="title-page-tagline"/>layer = tf$nn$max_pool(value=layer,<br class="title-page-tagline"/>ksize=shape(1L, 2L, 2L, 1L),<br class="title-page-tagline"/>strides=shape(1L, 2L, 2L, 1L),<br class="title-page-tagline"/>padding='SAME')<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Add non-linearity using Rectified Linear Unit (ReLU).<br class="title-page-tagline"/>layer = tf$nn$relu(layer)<br class="title-page-tagline"/># Retrun resulting layer and updated weights<br class="title-page-tagline"/>return(list("layer" = layer, "weights" = weights))<br class="title-page-tagline"/>}
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Run the following function to generate plots of convolution layers:</li>
</ol>
<pre class="calibre23">
drawImage_conv &lt;- function(index, images.bw, images.lab=NULL,par_imgs=8) {<br class="title-page-tagline"/>require(imager)<br class="title-page-tagline"/>img &lt;- images.bw[index,,,]<br class="title-page-tagline"/>n_images &lt;- dim(img)[3]<br class="title-page-tagline"/>par(mfrow=c(par_imgs,par_imgs), oma=c(0,0,0,0),<br class="title-page-tagline"/>mai=c(0.05,0.05,0.05,0.05),ann=FALSE,ask=FALSE)<br class="title-page-tagline"/>for(i in 1:n_images){<br class="title-page-tagline"/>img.bwmat &lt;- as.cimg(img[,,i])<br class="title-page-tagline"/># Extract the label<br class="title-page-tagline"/>if(!is.null(images.lab)){<br class="title-page-tagline"/>lab = labels[[1]][images.lab[[index]]]<br class="title-page-tagline"/>}<br class="title-page-tagline"/># Plot and output label<br class="title-page-tagline"/>plot(img.bwmat,axes=FALSE,ann=FALSE)<br class="title-page-tagline"/>}<br class="title-page-tagline"/>par(mfrow=c(1,1))<br class="title-page-tagline"/>}
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Run the following function to generate plots of convolution layer weights:</li>
</ol>
<pre class="calibre23">
drawImage_conv_weights &lt;- function(weights_conv, par_imgs=8) {<br class="title-page-tagline"/>require(imager)<br class="title-page-tagline"/>n_images &lt;- dim(weights_conv)[4]<br class="title-page-tagline"/>par(mfrow=c(par_imgs,par_imgs), oma=c(0,0,0,0),<br class="title-page-tagline"/>mai=c(0.05,0.05,0.05,0.05),ann=FALSE,ask=FALSE)<br class="title-page-tagline"/>for(i in 1:n_images){<br class="title-page-tagline"/>img.r.mat &lt;- as.cimg(weights_conv[,,1,i])<br class="title-page-tagline"/>img.g.mat &lt;- as.cimg(weights_conv[,,2,i])<br class="title-page-tagline"/>img.b.mat &lt;- as.cimg(weights_conv[,,3,i])<br class="title-page-tagline"/>img.col.mat &lt;- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c") <br class="title-page-tagline"/>#Bind the three channels into one image<br class="title-page-tagline"/># Plot and output label<br class="title-page-tagline"/>plot(img.col.mat,axes=FALSE,ann=FALSE)<br class="title-page-tagline"/>}<br class="title-page-tagline"/>par(mfrow=c(1,1))<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="39REE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The function begins with creating a shape tensor; namely, the list of four integers that are the width of a filter, the height of a filter, the number of input channels, and the number of given filters. Using this shape tensor, initialize a tensor of new weights with the defined shape and create a tensor a new (constant) biases, one for each filter.</p>
<p class="calibre2">Once the necessary weights and biases are initialized, create a TensorFlow operation for convolution using the <kbd class="calibre10">tf$nn$conv2d</kbd> function. In our current setup, the strides are set to 1 in all four dimensions and padding is set to <kbd class="calibre10">SAME</kbd>. The first and last are set to 1 by default, but the middle two can factor in higher strides. A stride is the number of pixels by which we allow the filter matrix to slide over the input (image) matrix.</p>
<p class="calibre2">A stride of 3 would mean three pixel jumps across the <em class="calibre9">x</em> or <em class="calibre9">y</em> axis for each filter slide. Smaller strides would produce larger feature maps, thereby requiring higher computation for convergence. As the padding is set to <kbd class="calibre10">SAME</kbd>, the input (image) matrix is padded with zeros around the border so that we can apply the filter to border elements of the input matrix. Using this feature, we can control the size of the output matrix (or feature maps) to be the same as the input matrix.</p>
<p class="calibre2">On convolution, the bias values are added to each filter channel followed by pooling to prevent overfitting. In the current setup, 2 x 2 max-pooling (using <kbd class="calibre10">tf$nn$max_pool</kbd>) is performed to downsize the image resolution. Here, we consider 2 x 2 (<kbd class="calibre10">ksize</kbd><em class="calibre9">)-</em>sized windows and select the largest value in each window. These windows stride by two pixels (<kbd class="calibre10">strides</kbd>) either in the x or y direction.</p>
<p class="calibre2">On pooling, we add non-linearity to the layer using the ReLU activation function (<kbd class="calibre10">tf$nn$relu</kbd>). In ReLU, each pixel is triggered in the filter and all negative pixel values are replaced with zero using the <kbd class="calibre10">max(x,0)</kbd> function, where <em class="calibre9">x</em> is a pixel value. Generally, ReLU activation is performed before pooling. However, as we are using max-pooling, it doesn't necessarily impact the outcome as such because <kbd class="calibre10">relu(max_pool(x))</kbd> is equivalent to <kbd class="calibre10">max_pool(relu(x))</kbd>. Thus, by applying ReLU post pooling, we can save a lot of ReLU operations (~75%).</p>
<p class="calibre2">Finally, the function returns a list of convoluted layers and their corresponding weights. The convoluted layer is a four-dimensional tensor with the following attributes:</p>
<ul class="calibre12">
<li class="calibre13">Number of (input) images, the same as <kbd class="calibre10">input</kbd></li>
<li class="calibre13">Height of each image (reduced to half in the case of 2 x 2 max-pooling)</li>
<li class="calibre13">Width of each image (reduced to half in the case of 2 x 2 max-pooling)</li>
<li class="calibre13">Number of channels produced, one for each convolution filter</li>
</ul>


            </article>

            
        </section>
    

        <section id="3APV01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using functions to create a new convolution layer</h1>
                
            
            <article>
                
<p class="calibre2">The four-dimensional outcome of a newly created convolution layer is flattened to a two-dimensional layer such that it can be used as an input to a fully connected multilayered perceptron.</p>


            </article>

            
        </section>
    

        <section id="3BOFI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The recipe explains how to flatten a convolution layer before building the deep learning model. The input to the given function ( <kbd class="calibre10">flatten_conv_layer</kbd>) is a four-dimensional convolution layer that is defined based on previous layer.</p>


            </article>

            
        </section>
    

        <section id="3CN041-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the following function to flatten the convolution layer:</li>
</ol>
<pre class="calibre23">
flatten_conv_layer &lt;- function(layer){<br class="title-page-tagline"/># Extract the shape of the input layer<br class="title-page-tagline"/>layer_shape = layer$get_shape()<br class="title-page-tagline"/># Calculate the number of features as img_height * img_width * num_channels<br class="title-page-tagline"/>num_features = prod(c(layer_shape$as_list()[[2]],layer_shape$as_list()[[3]],layer_shape$as_list()[[4]]))<br class="title-page-tagline"/># Reshape the layer to [num_images, num_features].<br class="title-page-tagline"/>layer_flat = tf$reshape(layer, shape(-1, num_features))<br class="title-page-tagline"/># Return both the flattened layer and the number of features.<br class="title-page-tagline"/>return(list("layer_flat"=layer_flat, "num_features"=num_features))<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="3DLGM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The function begins with extracting the shape of the given input layer. As stated in previous recipes, the shape of the input layer comprises four integers: image number, image height, image width, and the number of color channels in the image. The number of features (<kbd class="calibre10">num_features</kbd>) is then evaluated using a dot-product of image height, image weight, and number of color channels.</p>
<p class="calibre2">Then, the layer is flattened or reshaped into a two-dimensional tensor (using <kbd class="calibre10">tf$reshape</kbd>). The first dimension is set to -1 (which is equal to the total number of images) and the second dimension is the number of features.</p>
<p class="calibre2">Finally, the function returns a list of flattened layers along with the total number of (input) features.</p>


            </article>

            
        </section>
    

        <section id="3EK181-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using functions to flatten the densely connected layer</h1>
                
            
            <article>
                
<p class="calibre2">The CNN generally ends with a fully connected multilayered perceptron using softmax activation in the output layer. Here, each neuron in the previous convoluted-flattened layer is connected to every neuron in the next (fully connected) layer.</p>
<div class="packt_infobox">The key purpose of the fully convoluted layer is to use the features generated in the convolution and pooling stage to classify the given input image into various outcome classes (here, 10L). It also helps in learning the non-linear combinations of these features to define the outcome classes.</div>
<p class="calibre2">In this chapter, we use two fully connected layers for optimization. This function is primarily used to define the mathematical formulas in the TensorFlow graph, which is later used in actual computation during optimization.</p>


            </article>

            
        </section>
    

        <section id="3FIHQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The (<kbd class="calibre10">create_fc_layer</kbd>) function takes four input parameters, which are as follows:</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Input</kbd>: This is similar to the input of the new convolution layer function</li>
<li class="calibre13"><kbd class="calibre10">Num_inputs</kbd>: This is the number of input features generated post flattening the convoluted layer</li>
<li class="calibre13"><kbd class="calibre10">Num_outputs</kbd>: This is the number of output neurons fully connected with the input neurons</li>
<li class="calibre13"><kbd class="calibre10">Use_relu</kbd>: This takes the binary flag that is set to <kbd class="calibre10">FALSE</kbd> only in the case of the final fully connected layer</li>
</ul>


            </article>

            
        </section>
    

        <section id="3GH2C1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the following function to create a new fully connected layer:</li>
</ol>
<pre class="calibre23">
# Create a new fully connected layer<br class="title-page-tagline"/>create_fc_layer &lt;- function(input,<br class="title-page-tagline"/>num_inputs,<br class="title-page-tagline"/>num_outputs,<br class="title-page-tagline"/>use_relu=True)<br class="title-page-tagline"/>{<br class="title-page-tagline"/># Create new weights and biases.<br class="title-page-tagline"/>weights = weight_variable(shape=shape(num_inputs, num_outputs))<br class="title-page-tagline"/>biases = bias_variable(shape=shape(num_outputs))<br class="title-page-tagline"/># Perform matrix multiplication of input layer with weights and then add biases<br class="title-page-tagline"/>layer = tf$matmul(input, weights) + biases<br class="title-page-tagline"/># Use ReLU?<br class="title-page-tagline"/>if(use_relu){<br class="title-page-tagline"/>layer = tf$nn$relu(layer)<br class="title-page-tagline"/>}<br class="title-page-tagline"/>return(layer)<br class="title-page-tagline"/>}
</pre>


            </article>

            
        </section>
    

        <section id="3HFIU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">The function begins with initialing new weights and biases. Then, perform matrix multiplication of the input layer with initialized weights and add relevant biases.</p>
<p class="calibre2">If, the fully connected layer is not the final layer of the CNN TensorFlow graph, ReLU non-linear activation can be performed. Finally, the fully connected layer is returned.</p>


            </article>

            
        </section>
    

        <section id="3IE3G1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Defining placeholder variables</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's define the placeholder variables that serve as input to the modules in a TensorFlow computational graph. These are typically multidimensional arrays or matrices in the form of tensors.</p>


            </article>

            
        </section>
    

        <section id="3JCK21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The data type of placeholder variables is set to float32 (<kbd class="calibre10">tf$float32</kbd>) and the shape is set to a two-dimensional tensor.</p>


            </article>

            
        </section>
    

        <section id="3KB4K1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Create an input placeholder variable:</li>
</ol>
<pre class="calibre23">
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat), name='x')
</pre>
<p class="calibre24">The NULL value in the placeholder allows us to pass non-deterministic arrays size.</p>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Reshape the input placeholder <kbd class="calibre10">x</kbd> into a four-dimensional tensor:</li>
</ol>
<pre class="calibre23">
x_image = tf$reshape(x, shape(-1L, img_size, img_size, num_channels))
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Create an output placeholder variable:</li>
</ol>
<pre class="calibre23">
y_true = tf$placeholder(tf$float32, shape=shape(NULL, num_classes), name='y_true')
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Get the (<kbd class="calibre10">true</kbd>) classes of the output using argmax:</li>
</ol>
<pre class="calibre23">
y_true_cls = tf$argmax(y_true, dimension=1L)
</pre>


            </article>

            
        </section>
    

        <section id="3L9L61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In step 1, we define an input placeholder variable. The dimensions of the shape tensor are <kbd class="calibre10">NULL</kbd> and <kbd class="calibre10">img_size_flat</kbd>. The former is set to hold any number of images (as rows) and the latter defines the length of input features for each image (as columns). In step 2, the input two-dimensional tensor is reshaped into a four-dimensional tensor, which can be served as input convolution layers. The four dimensions are as follows:</p>
<ul class="calibre12">
<li class="calibre13">The first defines the number of input images (currently set to -1)</li>
<li class="calibre13">The second defines the height of each image (equivalent to image size 32L)</li>
<li class="calibre13">The third defines the width of each image (equivalent to image size, again 32L)</li>
<li class="calibre13">The fourth defines the number of color channels in each image (here 3L)</li>
</ul>
<p class="calibre2">In step 3, we define an output placeholder variable to hold true classes or labels of the images in <kbd class="calibre10">x</kbd>. The dimensions of the shape tensor are <kbd class="calibre10">NULL</kbd> and <kbd class="calibre10">num_classes</kbd>. The former is set to hold any number of images (as rows) and the latter defines the true class of each image as a binary vector of length <kbd class="calibre10">num_classes</kbd> (as columns). In our scenario, we have 10 classes. In step 4, we compress the two-dimensional output placeholder into a one-dimensional tensor of class numbers ranging from 1 to 10.</p>


            </article>

            
        </section>
    

        <section id="3M85O1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating the first convolution layer</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's create the first convolution layer.</p>


            </article>

            
        </section>
    

        <section id="3N6MA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The following are the inputs to the function <kbd class="calibre10">create_conv_layer</kbd> defined in the recipe <em class="calibre9">Using functions to create a new convolution layer</em>.</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Input</kbd>: This is a four-dimensional reshaped input placeholder variable: <kbd class="calibre10">x_image</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_input_channels</kbd>: This is the number of color channels, namely <kbd class="calibre10">num_channels</kbd></li>
<li class="calibre13"><kbd class="calibre10">Filter_size</kbd>: This is the height and width of the filter layer <kbd class="calibre10">filter_size1</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_filters</kbd>: This is the depth of the filter layer, namely <kbd class="calibre10">num_filters1</kbd></li>
<li class="calibre13"><kbd class="calibre10">Use_pooling</kbd>: This is the binary flag set to <kbd class="calibre10">TRUE</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section id="3O56S1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">create_conv_layer</kbd> function with the preceding input parameters:</li>
</ol>
<pre class="calibre23">
# Convolutional Layer 1<br class="title-page-tagline"/>conv1 &lt;- create_conv_layer(input=x_image,<br class="title-page-tagline"/>num_input_channels=num_channels,<br class="title-page-tagline"/>filter_size=filter_size1,<br class="title-page-tagline"/>num_filters=num_filters1,<br class="title-page-tagline"/>use_pooling=TRUE)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Extract the <kbd class="calibre10">layers</kbd> of the first convolution layer:</li>
</ol>
<pre class="calibre23">
layer_conv1 &lt;- conv1$layer<br class="title-page-tagline"/>conv1_images &lt;- conv1$layer$eval(feed_dict = dict(x = train_data$images, y_true = train_data$labels))
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Extract the final <kbd class="calibre10">weights</kbd> of the first convolution layer:</li>
</ol>
<pre class="calibre23">
weights_conv1 &lt;- conv1$weights<br class="title-page-tagline"/>weights_conv1 &lt;- weights_conv1$eval(session=sess)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Generate the first convolution layer plots:</li>
</ol>
<pre class="calibre23">
drawImage_conv(sample(1:50000, size=1), images.bw = conv1_images, images.lab=images.lab.train)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Generate the first convolution layer weight plots:</li>
</ol>
<pre class="calibre23">
drawImage_conv_weights(weights_conv1)
</pre>


            </article>

            
        </section>
    

        <section id="3P3NE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In steps 1 and 2, we create a first convolution layer of four-dimensions. The first dimension (?) represents any number of input images, the second and third dimensions represent the height (16 pixels) and width (16 pixels) of each convoluted image, and the fourth dimension represents the number of channels (64) produced--one for each convoluted filter. In steps 3 and 5, we extract the final weights of the convolution layer, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border35" src="../images/00035.gif"/></div>
<p class="calibre2">In step 4, we plot the output of the first convolution layer, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border36" src="../images/00038.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="3Q2801-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating the second convolution layer</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's create the second convolution layer.</p>


            </article>

            
        </section>
    

        <section id="3R0OI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The following are the inputs to the function <kbd class="calibre10">create_conv_layer</kbd> defined in the recipe <em class="calibre9">Using functions to create a new convolution layer</em>.</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Input</kbd>: This is the four-dimensional output of the first convoluted layer; that is, <kbd class="calibre10">layer_conv1</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_input_channels</kbd>: This is the number of filters (or depth) in the first convoluted layer, <kbd class="calibre10">num_filters1</kbd></li>
<li class="calibre13"><kbd class="calibre10">Filter_size</kbd>: This is the height and width of the filter layer; namely, <kbd class="calibre10">filter_size2</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_filters</kbd>: This is the depth of the filter layer, <kbd class="calibre10">num_filters2</kbd></li>
<li class="calibre13"><kbd class="calibre10">Use_pooling</kbd>: This is the binary flag set to <kbd class="calibre10">TRUE</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section id="3RV941-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">create_conv_layer</kbd> function with the preceding input parameters:</li>
</ol>
<pre class="calibre23">
# Convolutional Layer 2<br class="title-page-tagline"/>conv2 &lt;- create_conv_layer(input=layer_conv1,<br class="title-page-tagline"/>num_input_channels=num_filters1,<br class="title-page-tagline"/>filter_size=filter_size2,<br class="title-page-tagline"/>num_filters=num_filters2,<br class="title-page-tagline"/>use_pooling=TRUE)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Extract the layers of the second convolution layer:</li>
</ol>
<pre class="calibre23">
layer_conv2 &lt;- conv2$layer<br class="title-page-tagline"/>conv2_images &lt;- conv2$layer$eval(feed_dict = dict(x = train_data$images, y_true = train_data$labels))
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Extract the final weights of the second convolution layer:</li>
</ol>
<pre class="calibre23">
weights_conv2 &lt;- conv2$weights<br class="title-page-tagline"/>weights_conv2 &lt;- weights_conv2$eval(session=sess)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Generate the <span>second</span> convolution layer plots:</li>
</ol>
<pre class="calibre23">
drawImage_conv(sample(1:50000, size=1), images.bw = conv2_images, images.lab=images.lab.train)
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Generate the <span>second</span> convolution layer weight plots:</li>
</ol>
<pre class="calibre23">
drawImage_conv_weights(weights_conv2)
</pre>


            </article>

            
        </section>
    

        <section id="3STPM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In steps 1 and 2, we create a second convolution layer of four dimensions. The first dimension (?) represents any number of input images, the second and third dimensions represent the height (8 pixels) and width (8 pixels) of each convoluted image, and the fourth dimension represents the number of channels (64) produced, one for each convoluted filter.</p>
<p class="calibre2">In steps 3 and 5, we extract the final weights of the convolution layer, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border37" src="../images/00040.gif"/></div>
<p class="calibre2">In step 4, we plot the output of the <span>second</span> convolution layer, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border38" src="../images/00042.gif"/></div>


            </article>

            
        </section>
    

        <section id="3TSA81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Flattening the second convolution layer</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's flatten the second convolution layer that we created.</p>


            </article>

            
        </section>
    

        <section id="3UQQQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The following is the input to the function defined in the recipe Creating the second convolution layer, <kbd class="calibre10">flatten_conv_layer</kbd>:</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Layer</kbd>: This is the output of the second convolution layer, <kbd class="calibre10">layer_conv2</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section id="3VPBC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">flatten_conv_layer</kbd> function with the preceding input parameter:</li>
</ol>
<pre class="calibre23">
flatten_lay &lt;- flatten_conv_layer(layer_conv2)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Extract the flattened layer:</li>
</ol>
<pre class="calibre23">
layer_flat &lt;- flatten_lay$layer_flat
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Extract the number of (input) features generated for each image:</li>
</ol>
<pre class="calibre23">
num_features &lt;- flatten_lay$num_features
</pre>


            </article>

            
        </section>
    

        <section id="40NRU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Prior to connecting the output of the (second) convolution layer with a fully connected network, in step 1, we reshape the four-dimensional convolution layer into a two-dimensional tensor. The first dimension (?) represents any number of input images (as rows) and the second dimension represents the flattened vector of features generated for each image of length 4,096; that is, 8 x 8 x 64 (as columns). Steps 2 and 3 validate the dimensions of the reshaped layers and input features.</p>


            </article>

            
        </section>
    

        <section id="41MCG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating the first fully connected layer</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's create the first fully connected layer.</p>


            </article>

            
        </section>
    

        <section id="42KT21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The following are the inputs to the function defined in the recipe <em class="calibre9">Using functions to flatten the densely connected layer</em>, <kbd class="calibre10">create_fc_layer</kbd>:</p>
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Input</kbd>: This is the flattened convolution layer; that is, <kbd class="calibre10">layer_flat</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_inputs</kbd>: This is the number of features created post flattening, <kbd class="calibre10">num_features</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_outputs</kbd>: This is the number of fully connected neurons output, <kbd class="calibre10">fc_size</kbd></li>
<li class="calibre13"><kbd class="calibre10">Use_relu</kbd>: This is the binary flag set to <kbd class="calibre10">TRUE</kbd> to incorporate non-linearity in the tensor</li>
</ul>


            </article>

            
        </section>
    

        <section id="43JDK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">create_fc_layer</kbd> function with the preceding input parameters:</li>
</ol>
<pre class="calibre23">
layer_fc1 = create_fc_layer(input=layer_flat,<br class="title-page-tagline"/>num_inputs=num_features,<br class="title-page-tagline"/>num_outputs=fc_size,<br class="title-page-tagline"/>use_relu=TRUE)
</pre>


            </article>

            
        </section>
    

        <section id="44HU61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Here, we create a fully connected layer that returns a two-dimensional tensor. The first dimension (?) represents any number of (input) images and the second dimension represents the number of output neurons (here, 1,024).</p>


            </article>

            
        </section>
    

        <section id="45GEO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Applying dropout to the first fully connected layer</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's apply dropout to the output of the fully connected layer to reduce the chance of overfitting. The dropout step involves removing some neurons randomly during the learning process.</p>


            </article>

            
        </section>
    

        <section id="46EVA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The dropout is connected to the output of the layer. Thus, model initial structure is set up and loaded. For example, in dropout current layer <kbd class="calibre10">layer_fc1</kbd> is defined, on which dropout is applied.</p>


            </article>

            
        </section>
    

        <section id="47DFS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Create a placeholder for dropout that can take probability as an input:</li>
</ol>
<pre class="calibre23">
keep_prob &lt;- tf$placeholder(tf$float32)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Use TensorFlow's dropout function to handle the scaling and masking of neuron outputs:</li>
</ol>
<pre class="calibre23">
layer_fc1_drop &lt;- tf$nn$dropout(layer_fc1, keep_prob)
</pre>


            </article>

            
        </section>
    

        <section id="48C0E1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In steps 1 and 2, we can drop (or mask) out the output neurons based on the input probability (or percentage). The dropout is generally allowed during training and can be turned off (by assigning probability as <kbd class="calibre10">1</kbd> or <kbd class="calibre10">NULL</kbd>) during testing.</p>


            </article>

            
        </section>
    

        <section id="49AH01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating the second fully connected layer with dropout</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's create the second fully connected layer along with dropout.</p>


            </article>

            
        </section>
    

        <section id="4A91I1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The following are the inputs to the function defined in the recipe <em class="calibre9">Using functions to flatten the densely connected layer</em>, <kbd class="calibre10">create_fc_layer</kbd>:</p>
<div class="calibre38">
<ul class="calibre12">
<li class="calibre13"><kbd class="calibre10">Input</kbd>: This is the output of the first fully connected layer; that is, <kbd class="calibre10">layer_fc1</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_inputs</kbd>: This is the number of features in the output of the first fully connected layer, <kbd class="calibre10">fc_size</kbd></li>
<li class="calibre13"><kbd class="calibre10">Num_outputs</kbd>: This is the number of the fully connected neurons output (equal to the number of labels, <kbd class="calibre10">num_classes</kbd> )</li>
<li class="calibre13"><kbd class="calibre10">Use_relu</kbd>: This is the binary flag set to <kbd class="calibre10">FALSE</kbd></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section id="4B7I41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">create_fc_layer</kbd> function with the preceding input parameters:</li>
</ol>
<pre class="calibre23">
layer_fc2 = create_fc_layer(input=layer_fc1_drop,<br class="title-page-tagline"/>num_inputs=fc_size,<br class="title-page-tagline"/>num_outputs=num_classes,<br class="title-page-tagline"/>use_relu=FALSE)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Use TensorFlow's dropout function to handle the scaling and masking of neuron outputs:</li>
</ol>
<pre class="calibre23">
layer_fc2_drop &lt;- tf$nn$dropout(layer_fc2, keep_prob)
</pre>


            </article>

            
        </section>
    

        <section id="4C62M1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In step 1, we create a fully connected layer that returns a two-dimensional tensor. The first dimension (?) represents any number of (input) images and the second dimension represents the number of output neurons (here, 10 class labels). In step 2, we provide the option for dropout primarily used during the training of the network.</p>


            </article>

            
        </section>
    

        <section id="4D4J81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Applying softmax activation to obtain a predicted class</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, we will normalize the outputs of the second fully connected layer using softmax activation such that each class has a (probability) value restricted between 0 and 1, and all the values across 10 classes add up to 1.</p>


            </article>

            
        </section>
    

        <section id="4E33Q1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The activation function is applied at the end of the pipeline on predictions generated by the deep learning model. Before executing this step, all steps in the pipeline need to be executed. The recipe requires the TensorFlow library.</p>


            </article>

            
        </section>
    

        <section id="4F1KC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run the <kbd class="calibre10">softmax</kbd> activation function on the output of the second fully connected layer:</li>
</ol>
<pre class="calibre23">
y_pred = tf$nn$softmax(layer_fc2_drop)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Use the <kbd class="calibre10">argmax</kbd> function to determine the class number of the label. It is the index of the class with the largest (probability) value:</li>
</ol>
<pre class="calibre23">
y_pred_cls = tf$argmax(y_pred, dimension=1L)
</pre>


            </article>

            
        </section>
    

        <section id="4G04U1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Defining the cost function used for optimization</h1>
                
            
            <article>
                
<p class="calibre2">The cost function is primarily used to evaluate the current performance of the model by comparing the true class labels (<kbd class="calibre10">y_true_cls</kbd>) with the predicted class labels (<kbd class="calibre10">y_pred_cls</kbd>). Based on the current performance, the optimizer then fine-tunes the network parameters, such as weights and biases, to further improve its performance.</p>


            </article>

            
        </section>
    

        <section id="4GULG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The cost function definition is critical as it will decide optimization criteria. The cost function definition will require true classes and predicted classes to do comparison. The objective function used in this recipe is cross entropy, used in multi-classification problems.</p>


            </article>

            
        </section>
    

        <section id="4HT621-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Evaluate the current performance of each image using the cross entropy function in TensorFlow. As the cross entropy function in TensorFlow internally applies softmax normalization, we provide the output of the fully connected layer post dropout (<kbd class="calibre10">layer_fc2_drop</kbd>) as an input along with true labels (<kbd class="calibre10">y_true</kbd>):</li>
</ol>
<pre class="calibre23">
cross_entropy = tf$nn$softmax_cross_entropy_with_logits(logits=layer_fc2_drop, labels=y_true)
</pre>
<p class="calibre24">In the current cost function, softmax activation function is embedded thus the activation function is not required to be defined separately.</p>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Calculate the average of the cross entropy, which needs to be minimized using an optimizer:</li>
</ol>
<pre class="calibre23">
cost = tf$reduce_mean(cross_entropy)
</pre>


            </article>

            
        </section>
    

        <section id="4IRMK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In step 1, we define a cross entropy to evaluate the performance of classification. Based on the exact match between the true and predicted labels, the cross entropy function returns a value that is positive and follows a continuous distribution. As zero cross entropy ensures a full match, optimizers tend to minimize the cross entropy toward the value zero by updating the network parameters such as weights and biases. The cross entropy function returns a value for each individual image that needs to be further compressed into a single scalar value, which can be used in an optimizer. Hence, in step 2, we calculate a simple average of the cross entropy output and store it as <em class="calibre9">cost</em>.</p>


            </article>

            
        </section>
    

        <section id="4JQ761-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing gradient descent cost optimization</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, let's define an optimizer that can minimize the cost. Post optimization, check for CNN performance.</p>


            </article>

            
        </section>
    

        <section id="4KONO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The optimizer definition will require the <kbd class="calibre10">cost</kbd> recipe to be defined as it goes as input to the optimizer.</p>


            </article>

            
        </section>
    

        <section id="4LN8A1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Run an Adam optimizer with the objective of minimizing the cost for a given <kbd class="calibre10">learning_rate</kbd>:</li>
</ol>
<pre class="calibre23">
optimizer = tf$train$AdamOptimizer(learning_rate=1e-4)$minimize(cost)
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Extract the number of <kbd class="calibre10">correct_predictions</kbd> and calculate the mean percentage accuracy:</li>
</ol>
<pre class="calibre23">
correct_prediction = tf$equal(y_pred_cls, y_true_cls)<br class="title-page-tagline"/>accuracy = tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
</pre>


            </article>

            
        </section>
    

        <section id="4MLOS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Executing the graph in a TensorFlow session</h1>
                
            
            <article>
                
<p class="calibre2">Until now, we have only created tensor objects and added them to a TensorFlow graph for later execution. In this recipe, we will learn how to create a TensorFlow session that can be used to execute (or run) the TensorFlow graph.</p>


            </article>

            
        </section>
    

        <section id="4NK9E1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">Before we run the graph, we should have TensorFlow installed and loaded in R. The installation details can be found in <a href="part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 1</a>, <em class="calibre9">Getting Started</em>.</p>


            </article>

            
        </section>
    

        <section id="4OIQ01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Load the <kbd class="calibre10">tensorflow</kbd> library and import the <kbd class="calibre10">numpy</kbd> package:</li>
</ol>
<pre class="calibre23">
library(tensorflow)<br class="title-page-tagline"/>np &lt;- import("numpy")
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Reset or remove any existing <kbd class="calibre10">default_graph</kbd>:</li>
</ol>
<pre class="calibre23">
tf$reset_default_graph()
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Start an <kbd class="calibre10">InteractiveSession</kbd>:</li>
</ol>
<pre class="calibre23">
sess &lt;- tf$InteractiveSession()
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Initialize the <kbd class="calibre10">global_variables</kbd>:</li>
</ol>
<pre class="calibre23">
sess$run(tf$global_variables_initializer())
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Run iterations to perform optimization (<kbd class="calibre10">training</kbd>):</li>
</ol>
<pre class="calibre23">
# Train the model<br class="title-page-tagline"/>train_batch_size = 128L<br class="title-page-tagline"/>for (i in 1:100) {<br class="title-page-tagline"/>spls &lt;- sample(1:dim(train_data$images)[1],train_batch_size)<br class="title-page-tagline"/>if (i %% 10 == 0) {<br class="title-page-tagline"/>train_accuracy &lt;- accuracy$eval(feed_dict = dict(<br class="title-page-tagline"/>x = train_data$images[spls,], y_true = train_data$labels[spls,], keep_prob = 1.0))<br class="title-page-tagline"/>cat(sprintf("step %d, training accuracy %g\n", i, train_accuracy))<br class="title-page-tagline"/>}<br class="title-page-tagline"/>optimizer$run(feed_dict = dict(<br class="title-page-tagline"/>x = train_data$images[spls,], y_true = train_data$labels[spls,], keep_prob = 0.5))<br class="title-page-tagline"/>}
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Evaluate the performance of the trained model on test data:</li>
</ol>
<pre class="calibre23">
# Test the model<br class="title-page-tagline"/>test_accuracy &lt;- accuracy$eval(feed_dict = dict(<br class="title-page-tagline"/>x = test_data$images, y_true = test_data$labels, keep_prob = 1.0))<br class="title-page-tagline"/>cat(sprintf("test accuracy %g", test_accuracy))
</pre>


            </article>

            
        </section>
    

        <section id="4PHAI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Steps 1 through 4 are, in a way, the default way to launch a new TensorFlow session. In step 4, the variables of weights and biases are initialized, which is mandatory before their optimization. Step 5 is primarily to execute the TensorFlow session for optimization. As we have a large number of training images, it becomes highly difficult (computationally) to calculate the optimum gradient taking all the images at once into the optimizer.</p>
<p class="calibre2">Hence, a small random sample of 128 images is selected to train the activation layer (weights and biases) in each iteration. In the current setup, we run 100 iterations and report training accuracy for every tenth iteration.</p>
<p class="calibre2">However, these can be increased based on the cluster configuration or computational power (CPU or GPU) to obtain higher model accuracy. In addition, a 50% dropout rate is used to train the CNN in each iteration. In step 6, we can evaluate the performance of the trained model on a test data of 10,000 images.</p>


            </article>

            
        </section>
    

        <section id="4QFR41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Evaluating the performance on test data</h1>
                
            
            <article>
                
<p class="calibre2">In this recipe, we will look into the performance of the trained CNN on test images using a confusion matrix and plots.</p>


            </article>

            
        </section>
    

        <section id="4REBM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The prerequisite packages for plots are <kbd class="calibre10">imager</kbd> and <kbd class="calibre10">ggplot2</kbd>.</p>


            </article>

            
        </section>
    

        <section id="4SCS81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Get the <kbd class="calibre10">actual</kbd> or <kbd class="calibre10">true</kbd> class labels of test images:</li>
</ol>
<pre class="calibre23">
test_true_class &lt;- c(unlist(images.lab.test))
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Get the predicted class labels of test images. Remember to add <kbd class="calibre10">1</kbd> to each class label, as the starting index of TensorFlow (the same as Python) is 0 and that of R is <kbd class="calibre10">1</kbd>:</li>
</ol>
<pre class="calibre23">
test_pred_class &lt;- y_pred_cls$eval(feed_dict = dict(<br class="title-page-tagline"/>x = test_data$images, y_true = test_data$labels, keep_prob = 1.0))<br class="title-page-tagline"/>test_pred_class &lt;- test_pred_class + 1
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Generate the confusion matrix with rows as true labels and columns as predicted labels:</li>
</ol>
<pre class="calibre23">
table(actual = test_true_class, predicted = test_pred_class)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Generate a plot of the <kbd class="calibre10">confusion</kbd> matrix:</li>
</ol>
<pre class="calibre23">
confusion &lt;- as.data.frame(table(actual = test_true_class, predicted = test_pred_class))<br class="title-page-tagline"/>plot &lt;- ggplot(confusion)<br class="title-page-tagline"/>plot + geom_tile(aes(x=actual, y=predicted, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Normalized\nFrequency")
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Run a helper function to plot images:</li>
</ol>
<pre class="calibre23">
check.image &lt;- function(images.rgb,index,true_lab, pred_lab) {<br class="title-page-tagline"/>require(imager)<br class="title-page-tagline"/># Testing the parsing: Convert each color layer into a matrix,<br class="title-page-tagline"/># combine into an rgb object, and display as a plot<br class="title-page-tagline"/>img &lt;- images.rgb[[index]]<br class="title-page-tagline"/>img.r.mat &lt;- as.cimg(matrix(img$r, ncol=32, byrow = FALSE))<br class="title-page-tagline"/>img.g.mat &lt;- as.cimg(matrix(img$g, ncol=32, byrow = FALSE))<br class="title-page-tagline"/>img.b.mat &lt;- as.cimg(matrix(img$b, ncol=32, byrow = FALSE))<br class="title-page-tagline"/>img.col.mat &lt;- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")<br class="title-page-tagline"/># Plot with actual and predicted label<br class="title-page-tagline"/>plot(img.col.mat,main=paste0("True: ", true_lab,":: Pred: ",<br class="title-page-tagline"/>pred_lab),xaxt="n")<br class="title-page-tagline"/>axis(side=1, xaxp=c(10, 50, 4), las=1)<br class="title-page-tagline"/>}
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Plot random misclassified test images:</li>
</ol>
<pre class="calibre23">
labels &lt;- c("airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck")<br class="title-page-tagline"/># Plot misclassified test images<br class="title-page-tagline"/>plot.misclass.images &lt;- function(images.rgb, y_actual, y_predicted,labels){<br class="title-page-tagline"/># Get indices of misclassified<br class="title-page-tagline"/>indices &lt;- which(!(y_actual == y_predicted))<br class="title-page-tagline"/>id &lt;- sample(indices,1)<br class="title-page-tagline"/># plot the image with true and predicted class<br class="title-page-tagline"/>true_lab &lt;- labels[y_actual[id]]<br class="title-page-tagline"/>pred_lab &lt;- labels[y_predicted[id]]<br class="title-page-tagline"/>check.image(images.rgb,index=id, true_lab=true_lab,pred_lab=pred_lab)<br class="title-page-tagline"/>}<br class="title-page-tagline"/>plot.misclass.images(images.rgb=images.rgb.test,y_actual=test_true_class,y_predicted=test_pred_class,labels=labels)
</pre>


            </article>

            
        </section>
    

        <section id="4TBCQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In steps 1 through 3, we extract the true and predicted test class labels and create a confusion matrix. The following image shows the confusion matrix of the current test predictions:</p>
<div class="cdpaligncenter"><img class="image-border39" src="../images/00046.gif"/></div>
<p class="calibre2">The test accuracy post 700 training iterations is only ~51% and can be further improved by increasing the number of iterations, increasing the batch size, configuring layer parameters such as the number of convolution layers (used 2), types of activation functions (used ReLU), number of fully connected layers (used two), optimization objective function (used accuracy), pooling (used max 2 x 2), dropout probability, and many others.</p>
<p class="calibre2">Step 4 is used to build a facet plot of the test confusion matrix, as shown in the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border40" src="../images/00048.jpeg"/></div>
<p class="calibre2">In step 5, we define a helper function to plot the image along with a header containing both true and predicted classes. The input parameters of the <kbd class="calibre10">check.image</kbd> function are (<kbd class="calibre10">test</kbd>) flattened input dataset (<kbd class="calibre10">images.rgb</kbd>), image number (<kbd class="calibre10">index</kbd>), true label (<kbd class="calibre10">true_lab</kbd><em class="calibre9">),</em> and predicted label (<kbd class="calibre10">pred_lab</kbd>). Here, the red, green, and blue pixels are initially parsed out, converted into a matrix, appended as a list, and displayed as an image using the <em class="calibre9">plot</em> function.</p>
<p class="calibre2">In step 6, we plot misclassified test images using the helper function of step 5. The input parameters of the <kbd class="calibre10">plot.misclass.images</kbd> function are (<kbd class="calibre10">test</kbd>) flattened input dataset (<kbd class="calibre10">images.rgb</kbd>), a vector of true labels (<kbd class="calibre10">y_actual</kbd>), a vector of predicted labels (<kbd class="calibre10">y_predicted</kbd>), and a vector of unique ordered character labels (<kbd class="calibre10">labels</kbd>). Here, the indices of the misclassified images are obtained and an index is randomly selected to generate the plot. The following screenshot shows a set of six misclassified images with true and predicted labels:</p>
<div class="cdpaligncenter"><img class="image-border41" src="../images/00050.jpeg"/></div>


            </article>

            
        </section>
    </body></html>