- en: Building Deep Learning Environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the applied AI deep-learning team, and to our first project—*Building
    a Common Deep Learning Environment*! We're excited about the projects we've assembled
    in this book. The foundation of a common working environment will help us work
    together and learn very cool and powerful **deep learning** (**DL**) technologies,
    such as **computer vision** (**CV**) and **natural language processing** (**NLP**),
    that you will be able to use in your professional career as a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Components in building a common DL environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a local DL environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a DL environment in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the cloud for deployment for DL applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the setup process to reduce errors and get started quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a common DL environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our main goal to achieve by the end of this chapter is to standardize the toolsets
    to work together and achieve consistently accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: In the process of building applications using DL algorithms that can also scale
    for production, it's very important to have the right kind of setup, whether local
    or on the cloud, to make things work end to end. So, in this chapter, we will
    learn how to set up a DL environment that we will be using to run all the experiments
    and finally take the AI models into production.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will discuss the major components required to code, build, and deploy
    the DL models, then various ways to do this, and finally, look at a few code snippets
    that will help to automate the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of required components that we need to build DL applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu 16.04 or greater
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 2.x/3.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow/Keras DL packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA for GPU support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunicorn for deployment at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get focused and into the code!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by setting up your local DL environment. Much of the work that you'll
    do can be done on local machines. But with large datasets and complex model architectures,
    processing time slows down dramatically. This is why we are also setting up a
    DL environment in the cloud, because the processing time for these complex and
    repetitive calculations just becomes too long to be able to efficiently get things
    done otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: We will work straight through the preceding list, and by the end (and with the
    help of a bit of automated script), you'll have everything set up!
  prefs: []
  type: TYPE_NORMAL
- en: DL environment setup locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we will be using Ubuntu OS to run all the experiments,
    because there is great community support for Linux and mostly any DL application
    can be set up easily on Linux. For any assistance on installation and setup related
    to Ubuntu, please refer to the tutorials at [https://tutorials.ubuntu.com/](https://tutorials.ubuntu.com/).
    On top of that, this book will use the Anaconda package with Python 2.7+ to write
    our code, train, and test. Anaconda comes with a huge list of pre-installed Python
    packages, such as `numpy`, `pandas`, `sklearn`, and so on, which are commonly
    used in all kinds of data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need Anaconda? Can't we use Vanilla Python?
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda is a generic bundle that contains iPython Notebook, editor, and lots
    of Python libraries preinstalled, which saves a lot of time on setting up everything.
    With Anaconda, we can quickly get started on solving the data science problem,
    instead of configuring the environment.
  prefs: []
  type: TYPE_NORMAL
- en: But, yes, you can use the default Python—it's totally the reader's choice, and
    we will learn at the end of this chapter how to configure `python env` using script<q>.</q>
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and installing Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Anaconda is a very popular data science platform for people using Python to
    build machine learning and DL models, and deployable applications. The Anaconda
    marketing team put it best on their *What is Anaconda?* page, available at [https://www.anaconda.com/what-is-anaconda/](https://www.anaconda.com/what-is-anaconda/).
    To install Anaconda, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click Anaconda on the menu, then click Downloads to go to the download page
    at [https://www.anaconda.com/download/#linux](https://www.anaconda.com/download/#linux)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose the download suitable for your platform (Linux, OS X, or Windows):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose Python 3.6 version*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the Graphical Installer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions on the wizard, and in 10 to 20 minutes, your Anaconda
    environment (Python) setup will be ready
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the installation process is completed, you can use following command to
    check the Python version on your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If the command does not work, or returns an error, please check the documentation
    for help for your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Installing DL libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's install the Python libraries used for DL, specifically, TensorFlow
    and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is a Python library developed and maintained by Google. You can implement
    many powerful machine learning and DL architectures in custom models and applications
    using TensorFlow. To find out more, visit [https://www.tensorflow.org/](https://www.tensorflow.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the TensorFlow DL library (for all OS except Windows) by typing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you may choose to install using `pip` and a specific version
    of TensorFlow for your platform, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can find the installation instructions for TensorFlow at [https://www.tensorflow.org/get_started/os_setup#anaconda_installation](https://www.tensorflow.org/get_started/os_setup#anaconda_installation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will install `keras` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the environment and the version of the packages, let''s write the
    following script, which will print the version numbers of each library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the script as `dl_versions.py`. Run the script by typing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Voila! Now our Python development environment is ready for us to write some
    awesome DL applications in our local.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a DL environment in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the steps we performed up to now remain the same for the cloud as well,
    but there are a few additional modules required to configure the cloud virtual
    machines to make your DL applications servable and scalable. So, before setting
    up your server, follow the instructions from the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy your DL applications in the cloud, you will need a server good enough
    to train your models and serve at the same time. With huge development in the
    sphere of DL, the need for cloud servers to practice and deploy projects has increased
    drastically, and so have the options on the market. The following is a list of
    some of the best options on offer:'
  prefs: []
  type: TYPE_NORMAL
- en: Paperspace ([https://www.paperspace.com/](https://www.paperspace.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FloydHub ([https://www.floydhub.com](https://www.floydhub.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services ([https://aws.amazon.com/](https://aws.amazon.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Platform ([https://cloud.google.com/](https://cloud.google.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DigitalOcean ([https://cloud.digitalocean.com/](https://cloud.digitalocean.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these options have their own pro and cons, and the final choice totally
    depends on your use case and preferences, so feel free to explore more. In this
    book, we will build and deploy our models mostly on **Google Compute Engine**
    (**GCE**), which is a part of **Google Cloud Platform** (**GCP**). Follow the
    steps mentioned in this chapter to spin up a VM server and get started.
  prefs: []
  type: TYPE_NORMAL
- en: Google has released an internal notebook platform, **Google Colab **([https://colab.research.google.com/](https://colab.research.google.com/)),
    which is pre-installed with all the DL packages and other Python libraries. You
    can write all of your ML/DL applications on the Google Cloud, leveraging free
    GPUs for 10 hours.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud platforms for deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main idea behind this book is to empower you to build and deploy DL applications.
    In this section, we will discuss some critical components required to make your
    applications accessible to millions of users.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to make your application accessible is to expose it as a web service,
    using REST or SOAP APIs. To do so, we have many Python web frameworks to choose
    from, such as `web.py`, Flask, Bottle, and many more. These frameworks allow us
    to easily build web services and deploy them.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have a Google Cloud ([https://cloud.google.com/](https://cloud.google.com/)) account.
    Google is promoting the usage of its platform right now, and is giving away $300
    dollars of credit and 12 months as a free tier user.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these instructions to set up your GCP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating a new project**: Click on the three dots, as shown in the following
    screenshot, and then click on the + sign to create a new project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7425a6cf-6f2f-4d79-b59b-b1e947ce3c68.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Spinning a VM instance**:Click on the three lines on the upper-left corner
    of the screen, select the compute option, and click on Compute Engine. Now choose
    Create new instance. Name the VM instance, and select your zone as us-west2b.
    Choose the machine type size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose your boot disk as Ubuntu 16.04 LTS. In firewall options, choose both
    the http and https option (it's important to make it accessible from the outer
    world). To opt for GPU options, you can click on customize button, and find the
    GPU options. You can choose between two NVIDIA GPUs. Check both Allow HTTP traffic and
    Allow HTTPS traffic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now click on Create. Boom! your new VM is getting ready.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Modify the firewall settings**: Now click on the Firewall rules setting under
    Networking. Under Protocols and Ports, we need to select the port that we will
    use to export our APIs. We have chosen `tcp:8080` as our port number. Now click
    on the Save button. This will assign a rule in the firewall of your VM to access
    the applications from the external world.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Boot your VM**: Now start your VM instance. When you see the green tick,
    click on SSH—this will open a command window, and you are now inside the VM. You
    can also use `gcloud cli` to log in and access your VMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then follow the same steps as we performed to set up the local environment,
    or read further to learn how to create an automation script that will perform
    all the setup automatically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we need a web framework to write our DL applications as web services—again,
    there are lots of options, but to make it simple, we will be using a combination
    of `web.py` and Gunicorn.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know which web framework to choose based on memory consumption,
    CPU utilization, and so on, you can have a look at the comprehensive list of benchmarks
    at [http://klen.github.io/py-frameworks-bench](http://klen.github.io/py-frameworks-bench).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install them using following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to deploy our DL solution as a web service, and scale it to
    production level.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the setup process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing of Python packages and DL libraries can be a tedious process, requiring
    lots of time and repetitive effort. So, to ease the job, we will create a bash
    script that can be used to install everything using a single command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of components that will get installed and configured:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bazel for building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python and associated dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unzip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies for all of the aforementioned services (see the script for exact
    details)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can simply download the automation script to your server or locally, execute
    it, and you''re done. Here are the steps to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the script to your home directory, by cloning the code from the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have the copy of the complete repository, move to the `Chapter01`
    folder, which will contain a script file named `setupDeepLearning.sh`. This is
    the script that we will execute to start the setup process, but, before execution,
    we will have to make it executable using the `chmod` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, we are ready to execute it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow any instructions that appear (basically, say `yes` to everything and
    accept Java''s license). It should take about 10 to 15 minutes to install everything. Once
    it has finished, you will see the list of Python packages being installed, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79783458-7d6a-48fa-b306-16eaadbc7fe0.png)'
  prefs: []
  type: TYPE_IMG
- en: Listed packages with TensorFlow and other Python dependencies
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of other options, too, such as getting Docker images from
    TensorFlow and other DL packages, which can set up fully functional DL machines
    for large-scale and production-ready environments. You can find out more about
    Docker at [https://www.docker.com/what-docker](https://www.docker.com/what-docker). Also,
    for a quick-start guide, follow the instructions on this repository for an all-in-one
    Docker image for DL at [https://github.com/floydhub/dl-docker](https://github.com/floydhub/dl-docker).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked to get the team set up in a common environment with
    a standardized toolset. We are looking to deploy our project applications by utilizing
    Gunicorn and CUDA. Those projects will rely on highly advanced and effective DL
    libraries, such as TensorFlow and Keras running in Python 2.x/3.x. We'll write
    our code using the resources in the Anaconda package, and all of this will be
    running on Ubuntu 16.04 or greater.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are all set to perform experiments and deploy our DL models in production!
  prefs: []
  type: TYPE_NORMAL
