["```py\nclass ReLUConvBN(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super(ReLUConvBN, self).__init__()\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.op(x)\n```", "```py\nclass SepConv(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super(SepConv, self).__init__()\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\n            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_in, affine=affine),\n            nn.ReLU(inplace=False),\n            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),\n            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n    def forward(self, x):\n        return self.op(x)\n```", "```py\nclass DilConv(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n        super(DilConv, self).__init__()\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),\n            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n            )\n\n    def forward(self, x):\n        return self.op(x)\n```", "```py\nclass FactorizedReduce(nn.Module):\n    def __init__(self, C_in, C_out, affine=True):\n        super(FactorizedReduce, self).__init__()\n        assert C_out % 2 == 0\n        self.relu = nn.ReLU(inplace=False)\n        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n        out = self.bn(out)\n        return out\n```", "```py\nimport numpy as np\nfrom scipy import stats\n\ndef initializer_conv(shape,\n                     init='he',\n                     dist='truncnorm',\n                     dist_scale=1.0):\n    w_width = shape[3]\n    w_height = shape[2]\n    size_in = shape[1]\n    size_out = shape[0]\n\n    limit = 0.\n    if init == 'xavier':\n        limit = math.sqrt(2\\. / (w_width * w_height * (size_in + size_out))) * dist_scale\n    elif init == 'he':\n        limit = math.sqrt(2\\. / (w_width * w_height * size_in)) * dist_scale\n    else:\n        raise Exception('Arg `init` not recognized.')\n    if dist == 'norm':\n        var = np.array(stats.norm(loc=0, scale=limit).rvs(shape)).astype(np.float32)\n    elif dist == 'truncnorm':\n        var = np.array(stats.truncnorm(a=-2, b=2, scale=limit).rvs(shape)).astype(np.float32)\n    elif dist == 'uniform':\n        var = np.array(stats.uniform(loc=-limit, scale=2*limit).rvs(shape)).astype(np.float32)\n    else:\n        raise Exception('Arg `dist` not recognized.')\n    return var\n\nclass Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, groups=1, bias=True,\n                 init='he', dist='truncnorm', dist_scale=1.0):\n        super(Conv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride,\n            padding, dilation, groups, bias)\n        self.weight = nn.Parameter(torch.Tensor(\n            initializer_conv([out_channels, in_channels // groups, kernel_size, kernel_size],\n            init=init, dist=dist, dist_scale=dist_scale)))\n```", "```py\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\nfor epoch in range(epochs):\n    ...\n    scheduler.step()\n```", "```py\nclass LRScheduleCosine(object):\n    def __init__(self, optimizer, epoch=0, epoch_start=0, lr_max=0.05, lr_min=0.001, t_mul=10):\n        self.optimizer = optimizer\n        self.epoch = epoch\n        self.lr_min = lr_min\n        self.lr_max = lr_max\n        self.t_start = epoch_start\n        self.t_mul = t_mul\n        self.lr = lr_max\n\n    def step(self):\n        self.epoch += 1\n        self.lr = self.lr_min + 0.5*(self.lr_max-self.lr_min)*(1.+math.cos(math.pi*(self.epoch-self.t_start)/self.t_mul))\n        if self.optimizer is not None:\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = self.lr\n        if self.epoch == self.t_start + self.t_mul:\n            self.t_start += self.t_mul\n            self.t_mul *= 2\n        return self.lr\n```", "```py\nscheduler = LRScheduleCosine(optimizer,\n                                       lr_max=0.025,\n                                       lr_min=0.001,\n                                       t_mul=10)\nfor epoch in range(epochs):\n    lr = scheduler.step()\n    ...\n```"]