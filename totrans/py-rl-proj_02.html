<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Balancing CartPole</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, you will learn about the CartPole balancing problem. The CartPole is an inverted pendulum, where the pole is </span><span>balanced against gravity. Traditionally, this problem is solved by control theory, using analytical equations. </span><span>However, in this chapter, we will solve the problem with machine learning.</span></p>
<p class="mce-root"><span>The following topics will be covered in this chapter:<br/></span></p>
<ul>
<li>Installing OpenAI Gym</li>
<li>The different environments of Gym</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym</h1>
                </header>
            
            <article>
                
<p>OpenAI is a non-profit organization dedicated to researching artificial intelligence. Visit <a href="https://openai.com" target="_blank">https://openai.com</a> for more information about the mission of OpenAI. The technologies developed by OpenAI are free for anyone to use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gym</h1>
                </header>
            
            <article>
                
<p>Gym provides a toolkit to benchmark AI-based tasks. The interface is easy to use. The goal is to enable reproducible research. Visit <a href="https://gym.openai.com" target="_blank">https://gym.openai.com</a> for more information about Gym. An agent can be taught inside of the <kbd>gym</kbd>, and learn activities such as playing games or walking. An environment is a library of problems.</p>
<p>The standard set of problems presented in<span> </span>the gym<span> </span>are as follows:</p>
<ul>
<li>CartPole</li>
<li>Pendulum</li>
<li>Space Invaders</li>
<li>Lunar Lander</li>
<li>Ant</li>
<li>Mountain Car</li>
<li>Acrobot</li>
<li>Car Racing</li>
<li>Bipedal Walker</li>
</ul>
<p>Any algorithm can work out in the gym by training for these activities. All of the problems have the same interface. Therefore, any general reinforcement learning algorithm can be used through the interface.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation </h1>
                </header>
            
            <article>
                
<p>The primary interface of the gym is used through Python. Once you have Python3 in an environment with the <kbd>pip</kbd> installer, the gym can be installed as follows:</p>
<pre><strong>sudo pip install gym</strong> </pre>
<p>Advanced users that want to modify the source can compile from the source by using the following commands:</p>
<pre><strong><span>git </span><span class="hljs-built_in">clone</span><span> https://github.com/openai/gym <br/></span><span class="hljs-built_in">cd</span><span> gym <br/></span><span>pip install </span><span class="hljs-_">-e</span></strong><span><strong> .</strong></span><span><br/></span></pre>
<p>A new environment can be added to the <kbd>gym</kbd> with the source code. There are several environments that need more dependencies. For macOS, install the dependencies by using the following command:</p>
<pre><strong>brew install cmake boost boost-python sdl2 swig wget</strong></pre>
<p>For Ubuntu, use the following commands:</p>
<div class="highlight highlight-source-shell">
<pre><strong>apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig</strong></pre>
<p>Once the dependencies are there, install the complete <kbd>gym</kbd> as follows:</p>
<pre><strong><span>pip install 'gym[all]'</span></strong></pre></div>
<p>This will install most of the environments that are required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running an environment</h1>
                </header>
            
            <article>
                
<p>Any gym environment can be initialized and run by using a simple interface. Let's start by importing the <kbd>gym</kbd> library, as follows:</p>
<ol>
<li>First we import the<span> </span><kbd>gym</kbd><span> </span>library:</li>
</ol>
<pre><span class="hljs-keyword">import</span><span> gym </span></pre>
<ol start="2">
<li>Next, create an environment by passing an argument to <kbd>gym.make</kbd>. In the following code, CartPole is used as an example:</li>
</ol>
<pre><span>environment = gym.make(</span><span class="hljs-string">'CartPole-v0'</span><span>) </span></pre>
<ol start="3">
<li>Next, reset the environment:</li>
</ol>
<pre><span>environment.reset() </span></pre>
<ol start="4">
<li>Then, start an iteration and render the environment, as follows:</li>
</ol>
<pre><span class="hljs-keyword">for</span><span> dummy </span><span class="hljs-keyword">in</span><span> range(</span><span class="hljs-number">100</span><span>):<br/>    environment.render() <br/>    environment.step(environment.action_space.sample())<br/></span></pre>
<p>Also, change the action space at every step, to see CartPole moving. Running the preceding program should produce a visualization. The scene should start with a visualization, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6348db98-2f96-49a2-8b80-e54de2553228.png" style="width:25.08em;height:16.58em;"/></div>
<p>The preceding image is called a <strong>CartPole</strong>. <span>The CartPole is made up of a cart that can move horizontally and a pole that can move rotationally, with respect to the center of the cart</span><span>.</span></p>
<p><span>The pole is pivoted to the cart. After some time, you will notice that the pole is falling to one side, as shown in the following image:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a9316fb1-dc22-4c01-9f74-98455f95c0e8.png" style="width:30.67em;height:20.25em;"/></div>
<p>After a few more iterations, the pole will swing back, as shown in the following image. All of the movements are constrained by the laws of physics. The steps are taken randomly:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/b65a135f-4938-4510-9724-8fa192ed97e6.png" style="width:32.17em;height:21.17em;"/></div>
<p>Other environments can be seen in a similar way, by replacing the argument of the gym environment, such as <kbd>MsPacman-v0</kbd> or <kbd>MountrainCar-v0</kbd>. For other environments, other licenses may be required. Next, we will go through the rest of the environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atari</h1>
                </header>
            
            <article>
                
<p>To play Atari games, any environment can be invoked. The following code refers to the game Space Invaders: </p>
<pre>environment = gym.make(<span>'SpaceInvaders-v0'</span>)</pre>
<p>Once the preceding command has executed, you will see the following screen:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bbaefd4e-bd15-4985-828f-cac8de6259f6.png" style="width:23.25em;height:28.42em;"/></p>
<p class="mce-root">An Atari game can be played in this environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Algorithmic tasks</h1>
                </header>
            
            <article>
                
<p>There are algorithmic tasks that can be learned through reinforcement learning. A copy environment can be invoked, as follows:</p>
<pre>environment = gym.make(<span>'Copy-v0'</span>)</pre>
<p class="mce-root"/>
<p>The process of copying a string is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53e1c62d-adea-4977-a658-cb82c75f4986.png" style="width:21.83em;height:5.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MuJoCo</h1>
                </header>
            
            <article>
                
<p><strong>MuJoCo</strong> stands for <strong>multi-joint dynamics with contact</strong>. It's a simulation environment for robots and multi-body dynamics:</p>
<pre>environment = gym.make(<span>'Humanoid-v2'</span>)</pre>
<p>The following is a visualization for the simulation of a humanoid:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-751 image-border" src="assets/ac513818-fed0-4683-bb2d-902ebd991ccd.png" style="width:49.58em;height:27.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Simulation of a humanoid</div>
<p>There are robots and other objects that can be simulated in this environment. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Robotics</h1>
                </header>
            
            <article>
                
<p>A robotics environment can also be created, as follows:</p>
<pre>environment = gym.make(<span>'HandManipulateBlock-v0'</span>)</pre>
<p>The following is a visualization of a robot hand:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-752 image-border" src="assets/6285ba5e-91b1-484f-b034-83195a7bc30b.png" style="width:54.58em;height:45.42em;"/></p>
<p>There are several environments in which OpenAI Gym can be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov models</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>The problem is set up as a reinforcement learning problem, with a trial and error method. The environment is </span>described using <kbd>state_values</kbd> <kbd>state_values (?)</kbd>, and the <kbd>state_values</kbd> are changed by actions. The actions are determined by an algorithm, based on the current <span><kbd>state_value</kbd>, in order to achieve a particular <kbd>state_value</kbd> that is termed a <strong>Markov model</strong>. </span><span>In an ideal case, the past <kbd>state_values</kbd> does have an influence on future <kbd>state_values</kbd>, but here, we assume that the current <kbd>state_value</kbd> </span><span>has all of the previous <kbd>state_values</kbd> encoded. </span><span>There are two types of <kbd>state_values</kbd>; one is observable, and the other is non-observable. The model has to take non-observable </span><span><kbd>state_values</kbd> into account, as well. That is called a <strong>Hidden Markov model</strong>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CartPole</h1>
                </header>
            
            <article>
                
<p class="mce-root">At each step of the cart and pole, several variables can be observed, such as the position, velocity, angle, and angular <span>velocity. The possible <kbd>state_values</kbd> of the cart are moved right and left:</span></p>
<ol>
<li class="mce-root"><span><kbd>state_values</kbd>: Four dimensions of continuous values.</span></li>
<li class="mce-root"><span><kbd>Actions</kbd>: Two discrete values.</span></li>
<li class="mce-root"><span>The dimensions, or space, can be referred to as the <kbd>state_value</kbd> space and the action space. Let's start by importing the required libraries, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span>import </span>gym<br/><span>import </span>numpy <span>as </span>np<br/><span>import </span>random<br/><span>import </span>math</pre>
<ol start="4">
<li>Next, make the environment for playing CartPole, as follows:</li>
</ol>
<pre style="padding-left: 60px">environment = gym.make(<span>'CartPole-v0'</span>)</pre>
<ol start="5">
<li>Next, define the number of buckets and the number of actions, as follows:</li>
</ol>
<pre style="padding-left: 60px">no_buckets = (<span>1</span><span>, </span><span>1</span><span>, </span><span>6</span><span>, </span><span>3</span>)<br/>no_actions = environment.action_space.n</pre>
<ol start="6">
<li>Next, define the <kbd>state_value_bounds</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">state_value_bounds = <span>list</span>(<span>zip</span>(environment.observation_space.low<span>, </span>environment.observation_space.high))<br/>state_value_bounds[<span>1</span>] = [-<span>0.5</span><span>, </span><span>0.5</span>]<br/>state_value_bounds[<span>3</span>] = [-math.radians(<span>50</span>)<span>, </span>math.radians(<span>50</span>)]</pre>
<ol start="7">
<li>Next, define the <kbd>action_index</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">action_index = <span>len</span>(no_buckets)</pre>
<ol start="8">
<li>Next define the <kbd>q_value_table</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">q_value_table = np.zeros(no_buckets + (no_actions<span>,</span>))</pre>
<ol start="9">
<li>Next, define the minimum exploration rate and the minimum learning rate:</li>
</ol>
<pre style="padding-left: 60px">min_explore_rate = <span>0.01<br/></span>min_learning_rate = <span>0.1</span></pre>
<ol start="10">
<li>Next, define the maximum episodes, the maximum time steps, the streak to the end, the solving time, the discount, and the number of streaks, as constants:</li>
</ol>
<pre style="padding-left: 60px">max_episodes = <span>1000<br/></span>max_time_steps = <span>250<br/></span>streak_to_end = <span>120<br/></span>solved_time = <span>199<br/></span>discount = <span>0.99<br/></span>no_streaks = <span>0<br/></span></pre>
<ol start="11">
<li>Next, define the <kbd>select</kbd> action that can decide the action, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>select_action</span>(state_value<span>, </span>explore_rate):<br/>    <span>if </span>random.random() &lt; explore_rate:<br/>        action = environment.action_space.sample()<br/>    <span>else</span>:<br/>        action = np.argmax(q_value_table[state_value])<br/>    <span>return </span>action</pre>
<ol start="12">
<li>Next, select the explore state, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>select_explore_rate</span>(x):<br/>    <span>return </span><span>max</span>(min_explore_rate<span>, </span><span>min</span>(<span>1</span><span>, </span><span>1.0 </span>- math.log10((x+<span>1</span>)/<span>25</span>)))</pre>
<ol start="13">
<li>Next, select the learning rate, as follows:</li>
</ol>
<pre style="padding-left: 60px"><br/><span>def </span><span>select_learning_rate</span>(x):<br/>    <span>return </span><span>max</span>(min_learning_rate<span>, </span><span>min</span>(<span>0.5</span><span>, </span><span>1.0 </span>- math.log10((x+<span>1</span>)/<span>25</span>)))</pre>
<ol start="14">
<li>Next, <kbd>bucketize</kbd> the <kbd>state_value</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>bucketize_state_value</span>(state_value):<br/>    bucket_indexes = []<br/>    <span>for </span>i <span>in </span><span>range</span>(<span>len</span>(state_value)):<br/>        <span>if </span>state_value[i] &lt;= state_value_bounds[i][<span>0</span>]:<br/>           bucket_index = <span>0<br/></span><span>        </span><span>elif </span>state_value[i] &gt;= state_value_bounds[i][<span>1</span>]:<br/>            bucket_index = no_buckets[i] - <span>1<br/></span><span>        </span><span>else</span>:<br/>            bound_width = state_value_bounds[i][<span>1</span>] - state_value_bounds[i][<span>0</span>]<br/>            offset = (no_buckets[i]-<span>1</span>)*state_value_bounds[i][<span>0</span>]/bound_width<br/>            scaling = (no_buckets[i]-<span>1</span>)/bound_width<br/>            bucket_index = <span>int</span>(<span>round</span>(scaling*state_value[i] - offset))<br/>        bucket_indexes.append(bucket_index)<br/>    <span>return </span><span>tuple</span>(bucket_indexes)<br/><br/></pre>
<ol start="15">
<li>Next, train the episodes, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>for </span>episode_no <span>in </span><span>range</span>(max_episodes):<br/>    explore_rate = select_explore_rate(episode_no)<br/>    learning_rate = select_learning_rate(episode_no)<br/><br/>    observation = environment.reset()<br/><br/>    start_state_value = bucketize_state_value(observation)<br/>    previous_state_value = start_state_value<br/><br/>    <span>for </span>time_step <span>in </span><span>range</span>(max_time_steps):<br/>        environment.render()<br/>        selected_action = select_action(previous_state_value<span>, </span>explore_rate)<br/>        observation<span>, </span>reward_gain<span>, </span>completed<span>, </span>_ = environment.step(selected_action)<br/>        state_value = bucketize_state_value(observation)<br/>        best_q_value = np.amax(q_value_table[state_value])<br/>        q_value_table[previous_state_value + (selected_action<span>,</span>)] += learning_rate * (<br/>                reward_gain + discount * (best_q_value) - q_value_table[previous_state_value + (selected_action<span>,</span>)])</pre>
<ol start="16">
<li>Next, print all of the relevant metrics for the training process, as follows:</li>
</ol>
<pre style="padding-left: 60px"><br/>        <span>print</span>(<span>'Episode number : %d' </span>% episode_no)<br/>        <span>print</span>(<span>'Time step : %d' </span>% time_step)<br/>        <span>print</span>(<span>'Selection action : %d' </span>% selected_action)<br/>        <span>print</span>(<span>'Current state : %s' </span>% <span>str</span>(state_value))<br/>        <span>print</span>(<span>'Reward obtained : %f' </span>% reward_gain)<br/>        <span>print</span>(<span>'Best Q value : %f' </span>% best_q_value)<br/>        <span>print</span>(<span>'Learning rate : %f' </span>% learning_rate)<br/>        <span>print</span>(<span>'Explore rate : %f' </span>% explore_rate)<br/>        <span>print</span>(<span>'Streak number : %d' </span>% no_streaks)<br/><br/>        <span>if </span>completed:<br/>            <span>print</span>(<span>'Episode %d finished after %f time steps' </span>% (episode_no<span>, </span>time_step))<br/>            <span>if </span>time_step &gt;= solved_time:<br/>                no_streaks += <span>1<br/></span><span>            </span><span>else</span>:<br/>                no_streaks = <span>0<br/></span><span>            </span><span>break<br/></span><span><br/></span><span>        </span>previous_state_value = state_value<br/><br/>    <span>if </span>no_streaks &gt; streak_to_end:<br/>        <span>break<br/></span></pre>
<ol start="17">
<li>After training for a period of time, the CartPole will be able to balance itself, as shown in the following image:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eaf363d5-6e10-4327-885b-b5c3211dfbcd.png" style="width:27.00em;height:17.50em;"/></div>
<p>You have learned a program that will stabilize the CartPole.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned about the OpenAI Gym, used in reinforcement learning projects. You saw several examples of the training platform provided out of the box. Then, we formulated the problem of the CartPole, and made the CartPole balance by using a trial and error approach.</p>
<p>In the next chapter, you will learn how to play Atari games by using the Gym and a reinforcement learning approach. </p>


            </article>

            
        </section>
    </body></html>