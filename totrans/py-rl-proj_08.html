<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generating a Deep Learning Image Classifier</h1>
                </header>
            
            <article>
                
<p class="mce-root">Over the past decade, deep learning has made a name for itself by producing state-of-the-heart results across computer vision, natural language processing, speech recognition, and many more such applications. Some of the models that human researchers have designed and engineered have also gained popularity, including AlexNet, Inception, VGGNet, ResNet, and DenseNet; some of them are now the go-to standard for their respective tasks. However, it seems that the better the model gets, the more complex the architecture becomes, especially with the introduction of residual connections between convolutional layers. The task of designing a high-performance neural network has thus become a very arduous one. Hence the question arises: is it possible for an algorithm to learn how to generate neural network architectures?</p>
<p class="mce-root">As the title of this chapter suggests, it is indeed possible to train a neural network to generate neural networks that perform well on a given task. In this chapter, we will examine <strong>Neural Architecture Search</strong> (referred to as <strong>NAS</strong> henceforth), a novel framework developed by Barret Zoph and Quoc V. Le from the Google Brain team that uses deep reinforcement learning to train a Controller to produce child networks that learn to accomplish tasks. We will learn how policy gradient methods (REINFORCE in particular) can train such a Controller. We will then implement a Controller that uses NAS to generate child networks that train on <kbd>CIFAR-10</kbd> data.</p>
<p class="mce-root">In this chapter, we will cover the following:</p>
<ul>
<li>Understanding NAS and how it learns to generate other neural networks</li>
<li>Implementing a simple NAS framework that generates neural networks for training on <kbd>CIFAR-10</kbd> data</li>
</ul>
<p>You can find the original sources of the ensuing topics from the following sources:</p>
<ol>
<li><span>Zoph, B., and Le, Q. V. (2016). <em>Neural Architecture Search with reinforcement learning</em>. </span>arXiv preprint arXiv:1611.01578<span>.</span></li>
<li><span>Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., and Dean, J. (2018).<span> </span><em>Efficient Neural Architecture Search via Parameter Sharing</em>. </span>arXiv preprint arXiv:1802.03268<span>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural Architecture Search</h1>
                </header>
            
            <article>
                
<p>The next few sections will describe the NAS framework. You will learn about how the framework learns to generate other neural networks to complete tasks using a popular reinforcement learning scheme called <strong>REINFORCE</strong>, which is a type of policy gradient algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating and training child networks</h1>
                </header>
            
            <article>
                
<p>Research on algorithms that generate neural architectures has been around since the 1970's. What sets NAS apart from previous works is its ability to cater to large-scale deep learning algorithms and its formulation of the task as a reinforcement learning problem. More specifically, the agent, which we will refer to as the Controller, is a recurrent neural network that generates a sequence of values. You can think of these values as a sort of genetic code of the child network that defines its architecture; it sets the sizes of each convolutional kernel, the length of each kernel, the number of filters in each layer, and so on. In more advanced frameworks, the values also determine the connections between layers to generate residual layers:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4274f1d4-9a06-4333-a570-03141a930279.png" style="width:45.42em;height:23.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 1: Overview of the NAS framework</div>
<p>Moreover, each value of the genetic code that the Controller outputs counts as an action, <em>a</em>, that is sampled with probability, <em>p</em>. Because the Controller is a recurrent neural network, we can represent the <em>t</em><sup>th</sup> action as <img class="fm-editor-equation" src="assets/faa90a7b-db39-49c1-a47e-2c17ba041851.png" style="width:4.75em;height:1.33em;"/>. Once we have a list of actions, <img class="fm-editor-equation" src="assets/7239d03a-caa1-49ca-b868-d135bc3cc4e6.png" style="font-size: 1em;width:9.00em;height:1.17em;"/>—<span>where </span><em>T</em> <span>is some predefined parameter that sets the maximum size of the genetic code</span><span>—we can generate the child network with the specified architecture, </span><em>A</em><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-344 image-border" src="assets/09c0b341-1338-4af2-86dd-6e5bda00cb79.png" style="width:149.50em;height:70.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2: The architecture of the Controller</div>
<p class="mce-root">Once the Controller generates a child network, we train it on a given task until either some termination criteria is met (for example, after a specified number of epochs). We then evaluate the child network on the validation set to produce some validation accuracy, <em>R</em>. The validation accuracy acts as the reward signal for the Controller. So, the objective of the Controller is to maximize the expected reward:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d92c3151-95a1-42bc-902f-49620b4c81ac.png" style="width:7.08em;height:1.33em;"/></div>
<p class="mce-root"/>
<p>Here, <em>J</em> is the reward function (also referred to as the fit function), <img class="fm-editor-equation" src="assets/16d467a8-67e3-48f5-a55b-930031763723.png" style="width:1.08em;height:1.25em;"/> is the parameters of the Controller, and the right-hand side of the equation is the expectation of the reward given a child network architecture, <em>A</em>. In practice, this expectation is calculated by averaging the rewards over <em>m</em> child network models that the Controller produces in one batch:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/52c89b3d-40e1-46ad-93c5-e76d80ccf368.png" style="width:8.08em;height:3.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the Controller</h1>
                </header>
            
            <article>
                
<p>How do we use this reward signal to update the Controller? Remember, this reward signal is not differentiable like a loss function in supervised learning; we may not backpropagate this through the Controller on its own. Instead, we employ a policy gradient method called <strong>REINFORCE</strong> to iteratively update the Controller parameters, <img class="fm-editor-equation" src="assets/ea43e406-8138-401c-bdba-9a1432dd38eb.png" style="width:0.92em;height:1.08em;"/>. In REINFORCE, the gradient of the reward function, <em>J</em>, with respect to the parameters of the Controller, <img class="fm-editor-equation" src="assets/3ad033be-2d52-421a-9c2d-b6da56710c63.png" style="width:1.08em;height:1.25em;"/>, is defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6f285d1b-2866-481f-a3c8-fb2b5f803772.png" style="width:24.75em;height:3.67em;"/></div>
<p>You may recall seeing a similar expression in <a href="857fa187-2524-4682-ae75-ab6b15e00a50.xhtml" target="_blank">Chapter 6</a>, <em>Learning to Play Go</em>. Indeed, this is the policy gradient method that AlphaGo and AlphaGo Zero use to update the weights of their reinforcement learning policy networks. We briefly introduced the method then, but we will go a bit more in-depth here.</p>
<p>Let's break the preceding equation down. On the right-hand side, we would like to represent the probability of choosing some architecture, <em>A</em>. In particular, <br/>
<img class="fm-editor-equation" src="assets/394dfe7f-478c-47e2-a4d5-e51996fe112a.png" style="width:7.08em;height:1.33em;"/> represents the probability that the Controller takes action <img class="fm-editor-equation" src="assets/5ed6092d-17db-4ea7-a4b5-1bd2fa971fa4.png" style="width:1.33em;height:1.17em;"/> given all the previous actions, <img class="fm-editor-equation" src="assets/af6f66a3-0c6d-448d-a13d-0126e02d4ec3.png" style="width:3.42em;height:1.08em;"/>, and the parameters of the Controller, <img class="fm-editor-equation" src="assets/8a4e3cb6-38fe-423c-a588-3c6fb67dd95b.png" style="width:1.08em;height:1.25em;"/>. Again, action <img class="fm-editor-equation" src="assets/78832ef5-12c9-48db-9d8d-82b8853fc29a.png" style="width:1.08em;height:0.92em;"/> corresponds to the <em>t</em><sup>th</sup> value in the genetic sequence that represents the child network's architecture. The joint probability of choosing all actions, <img class="fm-editor-equation" src="assets/50724bbb-cc31-45ee-9c44-2157a323b752.png" style="width:7.67em;height:1.00em;"/>, can be formulated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c15a89ea-f4d1-4c47-a9cf-316fdb643460.png" style="width:18.83em;height:3.67em;"/></div>
<p>By transforming this joint probability to the log space, we can turn the product into a sum of probabilities:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/457a0dfc-e99d-4030-a2cb-e5a9c7253058.png" style="width:17.42em;height:3.58em;"/></div>
<p>In general, we want to maximize this log conditional probability for taking some action. In other words, we want to increase the likelihood of the Controller generating a particular sequence of genetic codes. Hence we perform gradient ascent on this objective with respect to the Controller's parameters by taking the derivative of the log probability of sampling architecture <em>A</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7ca47ff5-b4b3-493d-8ece-9c37fb92dfb8.png" style="width:21.25em;height:3.58em;"/></div>
<p>But how do we update the Controller parameters so that better architectures are generated? This is where we make use of the reward signal, <em>R</em>. By multiplying the preceding with the reward signal, we can control the size of the policy gradient. In other words, if a particular architecture achieved high validation accuracy (with the highest possible being 1.0), the gradients for that policy will be relatively strong and the Controller will learn to produce similar architectures. On the other hand, smaller validation accuracies will mean smaller gradients, which helps the Controller ignore those architectures.</p>
<p>One problem with the REINFORCE algorithm is that the reward signal <em>R</em> can have high variance, which can lead to unstable training curves. To reduce the variance, it is common to subtract the reward with some value, <em>b</em>, which we refer to as the baseline function. In Zoph et al., the baseline function is defined as the exponential moving average of the past rewards. Hence our REINFORCE policy gradient is now defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1246a433-5328-47f6-9b1d-bce44939749d.png" style="width:27.58em;height:3.67em;"/></div>
<p>Once we have this gradient, we apply the usual backpropagation algorithm to update the Controller parameters, <img class="fm-editor-equation" src="assets/c0938788-435b-4258-aed5-1c8beaf53289.png" style="width:1.08em;height:1.25em;"/>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training algorithm</h1>
                </header>
            
            <article>
                
<p>The training steps for the Controller is as follows:</p>
<ul>
<li>For each episode, do the following:
<ol>
<li>Generate <em>m</em> child network architectures</li>
<li>Train child networks on given task and obtain <em>m</em> validation accuracies</li>
<li>Calculate <img class="fm-editor-equation" src="assets/3a57d7b6-c858-459b-9343-a2ce6a8734c7.png" style="width:4.08em;height:1.25em;"/></li>
<li>Update <img class="fm-editor-equation" src="assets/e842e22d-55cd-44ef-97fe-4ab452664d09.png" style="width:0.92em;height:1.08em;"/></li>
</ol>
</li>
</ul>
<p>In Zoph et al., the training procedure is done with several copies of the Controller. Each Controller is parameterized by <img class="fm-editor-equation" src="assets/e842e22d-55cd-44ef-97fe-4ab452664d09.png" style="width:1.08em;height:1.25em;"/>, which itself is stored in a distributed manner among multiple servers, which we call parameter servers.</p>
<p>In each episode of training, the Controller creates several child architectures and trains them independently. The policy gradient calculated as a result is then sent to the parameter servers to update the Controller's parameters:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-345 image-border" src="assets/a144e3eb-57ca-4d76-a1da-e99221b0a29e.png" style="width:162.50em;height:56.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 3: The training architecture </div>
<p class="mce-root">The Controller's parameters are shared among a number of parameter servers. Moreover, multiple copies of the Controller are trained in parallel, each one calculating rewards and gradients for its respective batches of child network architectures.</p>
<p class="mce-root">This architecture allows the Controller to be trained quickly given enough resources. For our purposes, however, we will stick to one Controller that generates <em>m</em> child network architectures. Once we have trained the Controller for a specified number of episodes, we calculate the test accuracy by choosing the child network architecture that had the best validation accuracy and measuring its performance on the test set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing NAS</h1>
                </header>
            
            <article>
                
<p>In this section, we will implement NAS. In particular, our Controller is tasked with generating child network architectures that learn to classify images from the <kbd>CIFAR-10</kbd> dataset. The architecture of the child network will be represented by a list of numbers. Every four values in this list represent a convolutional layer in the child network, each describing the kernel size, stride length, number of filters, and the pooling window size in the subsequent pooling layer. Moreover, we specify the number of layers in a child network as a hyper-parameters. For example, if our child network has three layers, its architecture is represented as a vector of length 12. If we have an architecture represented as <kbd>[3, 1, 12, 2, 5, 1, 24, 2]</kbd>, then the child network is a two-layer network where the first layer has kernel size of 3, stride length of 1, 12 filters, and a max-pooling window size of 2, and the second layer has kernel size of 5, stride length of 1, 24 filters, and max-pooling window size of 2. We set the activation function between each layer as ReLU. The final layer involves flattening the last convolutional layer output and applying a linear layer with the number of classes as its width, followed by a Softmax activation. The following sections will walk you through the implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">child_network.py</h1>
                </header>
            
            <article>
                
<p>We will first implement our child network module. This module contains a class called <kbd>ChildCNN</kbd>, which constructs a child network given some architecture configuration, which we call <kbd>cnn_dna</kbd>. As mentioned previously, <kbd>cnn_dna</kbd> is simply a list of numbers, with each value representing a parameter of its respective convolutional layer. In our <kbd>config.py</kbd>, we specify the max number of layers a child network can have. For our implementation, each convolutional layer is represented by four parameters, where each corresponds to the kernel size, stride length, number of filters, and subsequent max-pooling window size.</p>
<p>Our <kbd>ChildCNN</kbd> is a class that takes the following parameters in its constructor:</p>
<ul>
<li><kbd>cnn_dna</kbd>: The network architecture</li>
<li><kbd>child_id</kbd>: A string that simply identifies the child network architecture</li>
<li><kbd>beta</kbd>: Weight parameter for L2 regularization</li>
<li><kbd>drop_rate</kbd>: Dropout rate</li>
</ul>
<pre><span>import </span>logging<br/><br/><span>import </span>tensorflow <span>as </span>tf<br/><br/>logger = logging.getLogger(__name__)<br/><br/><span>class </span><span>ChildCNN</span>(<span>object</span>):<br/><br/>    <span>def </span>__init__(<span>self</span>, <span>cnn_dna</span>, <span>child_id</span>, <span>beta</span>=<span>1e-4</span>, <span>drop_rate</span>=<span>0.2</span>, <span>**kwargs</span>):<br/>        <span>self</span>.cnn_dna = <span>self</span>.process_raw_controller_output(<span>cnn_dna</span>)<br/>        <span>self</span>.child_id = <span>child_id<br/></span><span>        </span><span>self</span>.beta = <span>beta<br/></span><span>        </span><span>self</span>.drop_rate = <span>drop_rate<br/></span><span>        </span><span>self</span>.is_training = tf.placeholder_with_default(<span>True</span>, <span>shape</span>=<span>None</span>, <span>name</span>=<span>"is_training_{}"</span>.format(<span>self</span>.child_id))<br/>        <span>self</span>.num_classes = <span>10</span></pre>
<p>We also implement a helper function called <kbd>proces_raw_controller_output()</kbd>, which parses <kbd>cnn_dna</kbd> that the Controller outputs:</p>
<pre><span>def </span><span>process_raw_controller_output</span>(<span>self</span>, <span>output</span>):<br/>    <span>"""<br/></span><span>    A helper function for preprocessing the output of the NASCell<br/></span><span>    Args:<br/></span><span>        output (numpy.ndarray) The output of the NASCell<br/></span><span><br/></span><span>    Returns:<br/></span><span>        (list) The child network's architecture<br/></span><span>    """<br/></span><span>    </span>output = <span>output</span>.ravel()<br/>    cnn_dna = [<span>list</span>(<span>output</span>[x:x+<span>4</span>]) <span>for </span>x <span>in </span><span>range</span>(<span>0</span>, <span>len</span>(<span>output</span>), <span>4</span>)]<br/>    <span>return </span>cnn_dna</pre>
<p>Finally, we include the <kbd>build</kbd> method, which builds our child network using the given <kbd>cnn_dna</kbd>. You will notice that, although we are letting the Controller decide the architecture of our child network, we are still hardcoding several things, such as the activation function, <kbd>tf.nn.relu</kbd>, and the way we initialize the kernels. The fact that we are adding a max-pooling layer after each convolutional layer is also hardcoded. A more sophisticated NAS framework would also let the Controller decide these components of the architecture as well, with the trade off being longer training time:</p>
<pre><span>def </span><span>build</span>(<span>self</span>, <span>input_tensor</span>):<br/>    <span>"""<br/></span><span>    Method for creating the child neural network<br/></span><span>    Args:<br/></span><span>        input_tensor: The tensor which represents the input<br/></span><span><br/></span><span>    Returns:<br/></span><span>        The tensor which represents the output logit (pre-softmax activation)<br/></span><span><br/></span><span>    """<br/></span><span>    </span>logger.info(<span>"DNA is: {}"</span>.format(<span>self</span>.cnn_dna))<br/>    output = <span>input_tensor<br/></span><span>    </span><span>for </span>idx <span>in </span><span>range</span>(<span>len</span>(<span>self</span>.cnn_dna)):<br/>        <span># Get the configuration for the layer<br/></span><span>        </span>kernel_size, stride, num_filters, max_pool_size = <span>self</span>.cnn_dna[idx]<br/>        <span>with </span>tf.name_scope(<span>"child_{}_conv_layer_{}"</span>.format(<span>self</span>.child_id, idx)):<br/>            output = tf.layers.conv2d(output,<br/>                    <span># Specify the number of filters the convolutional layer will output<br/></span><span>                    </span><span>filters</span>=num_filters,<br/>                    <span># This specifies the size (height, width) of the convolutional kernel<br/></span><span>                    </span><span>kernel_size</span>=(kernel_size, kernel_size),<br/>                    <span># The size of the stride of the kernel<br/></span><span>                    </span><span>strides</span>=(stride, stride),<br/>                    <span># We add padding to the image<br/></span><span>                    </span><span>padding</span>=<span>"SAME"</span>,<br/>                    <span># It is good practice to name your layers<br/></span><span>                    </span><span>name</span>=<span>"conv_layer_{}"</span>.format(idx),<br/>                    <span>activation</span>=tf.nn.relu,<br/>                    <span>kernel_initializer</span>=tf.contrib.layers.xavier_initializer(),<br/>                    <span>bias_initializer</span>=tf.zeros_initializer(),<br/>                    <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta))</pre>
<p>Each convolutional layer is followed by a max-pooling layer and a dropout layer:</p>
<pre>            <span># We apply 2D max pooling on the output of the conv layer<br/></span><span>            </span>output = tf.layers.max_pooling2d(<br/>                output, <span>pool_size</span>=(max_pool_size, max_pool_size), <span>strides</span>=<span>1</span>,<br/>                <span>padding</span>=<span>"SAME"</span>, <span>name</span>=<span>"pool_out_{}"</span>.format(idx)<br/>            )<br/>            <span># Dropout to regularize the network further<br/></span><span>            </span>output = tf.layers.dropout(output, <span>rate</span>=<span>self</span>.drop_rate, <span>training</span>=<span>self</span>.is_training)</pre>
<p>Finally, after several blocks of convolutional, pooling, and dropout layers, we flatten the output volume and a fully connected layer:</p>
<pre>    <span># Lastly, we flatten the outputs and add a fully-connected layer<br/></span><span>    </span><span>with </span>tf.name_scope(<span>"child_{}_fully_connected"</span>.format(<span>self</span>.child_id)):<br/>        output = tf.layers.flatten(output, <span>name</span>=<span>"flatten"</span>)<br/>        logits = tf.layers.dense(output, <span>self</span>.num_classes)<br/><br/>    <span>return </span>logits</pre>
<p class="mce-root"/>
<p>The argument to our <kbd>build</kbd> method is an input tensor, which has by default a shape of (32, 32, 3), which is the <kbd>CIFAR-10</kbd> data shape. The reader is free to tweak the architecture of this network, including adding a few more fully connected layers or inserting batch normalization layers in between convolutions.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">cifar10_processor.py</h1>
                </header>
            
            <article>
                
<p>This module contains code for processing <kbd>CIFAR-10</kbd> data, which we use to train our child networks. In particular, we construct an input data pipeline using TensorFlow's native <kbd>tf.data.Dataset</kbd> API. Those who have used TensorFlow for some time may be more familiar with creating <kbd>tf.placeholder</kbd> tensors and feeding data via <kbd>sess.run(..., feed_dict={...})</kbd>. However, this is no longer the preferred way of feeding data into the network; in fact, it is the slowest way to train a network, for the repetitive conversions from data in <kbd>numpy</kbd> format to a native TensorFlow format cause significant computational overhead. <kbd>tf.data.Dataset</kbd> alleviates this problem by turning the input pipeline into TensorFlow operations that are part of the symbolic graph. In other words, the data is converted into tensors right from the get-go. This allows for a much smoother input pipeline that can speed up training.</p>
<div class="packt_infobox">Refer to this official tutorial (<a href="https://www.tensorflow.org/guide/datasets_for_estimators">https://www.tensorflow.org/guide/datasets_for_estimators</a>) for more information on the <kbd>tf.data.Dataset</kbd> API.</div>
<p>The <kbd>cifar10_processor.py</kbd> contains a single method to create <kbd>CIFAR-10</kbd> data into tensors. We first implement a helper function for creating a <kbd>tf.data.Dataset</kbd> object:</p>
<pre><span>import </span>logging<br/><br/><span>import </span>numpy <span>as </span>np<br/><span>import </span>tensorflow <span>as </span>tf<br/><span>from </span>keras.datasets <span>import </span>cifar10<br/><span>from </span>keras.utils <span>import </span>np_utils<br/><br/>logger = logging.getLogger(__name__)<br/><br/><span>def </span><span>_create_tf_dataset</span>(<span>x</span>, <span>y</span>, <span>batch_size</span>):<br/>    <span>return </span>tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(<span>x</span>),<br/>                                tf.data.Dataset.from_tensor_slices(<span>y</span>))).shuffle(<span>500</span>).repeat().batch(<span>batch_size</span>)</pre>
<p>In the main data processor function, we first load <kbd>CIFAR-10</kbd> data. We use the <kbd>keras.datasets</kbd> API to do this (run <kbd>pip install keras</kbd> in your Terminal if you don't have Keras):</p>
<pre><span>def </span><span>get_tf_datasets_from_numpy</span>(<span>batch_size</span>, <span>validation_split</span>=<span>0.1</span>):<br/>    <span>"""<br/></span><span>    Main function getting tf.Data.datasets for training, validation, and testing<br/></span><span><br/></span><span>    Args:<br/></span><span>        batch_size (int): Batch size<br/></span><span>        validation_split (float): Split for partitioning training and validation sets. Between 0.0 and 1.0.<br/></span><span>    """<br/></span><span>    </span><span># Load data from keras datasets api<br/></span><span>    </span>(X, y), (X_test, y_test) = cifar10.load_data()<br/><br/>    logger.info(<span>"Dividing pixels by 255"</span>)<br/>    X = X / <span>255.<br/></span><span>    </span>X_test = X_test / <span>255.<br/></span><span><br/></span><span>    </span>X = X.astype(np.float32)<br/>    X_test = X_test.astype(np.float32)<br/>    y = y.astype(np.float32)<br/>    y_test = y_test.astype(np.float32)<br/><br/>    <span># Turn labels into onehot encodings<br/></span><span>    </span><span>if </span>y.shape[<span>1</span>] != <span>10</span>:<br/>        y = np_utils.to_categorical(y, <span>num_classes</span>=<span>10</span>)<br/>        y_test = np_utils.to_categorical(y_test, <span>num_classes</span>=<span>10</span>)<br/><br/>    logger.info(<span>"Loaded data from keras"</span>)<br/><br/>    split_idx = <span>int</span>((<span>1.0 </span>- <span>validation_split</span>) * <span>len</span>(X))<br/>    X_train, y_train = X[:split_idx], y[:split_idx]<br/>    X_valid, y_valid = X[split_idx:], y[split_idx:]</pre>
<p>We then turn these NumPy arrays into TensorFlow tensors, which we can feed directly to our network. What actually happens in our <kbd>_create_tf_dataset </kbd>helper function? We use the <kbd>tf.dataset.Dataset.from_tensor_slices()</kbd> function to turn the data and the labels, both of which are NumPy arrays, into TensorFlow tensors. We then create the native dataset by zipping these tensors. The <kbd>shuffle</kbd>, <kbd>repeat</kbd>, and <kbd>batch</kbd> functions after zipping the data and labels define how we want the input pipeline to work. In our case, we are shuffling the input data, repeating the dataset once we reach the end, and batching the data with a given batch size. We also calculate the number of batches that each dataset has and return them:</p>
<pre>train_dataset = _create_tf_dataset(X_train, y_train, <span>batch_size</span>)<br/>valid_dataset = _create_tf_dataset(X_valid, y_valid, <span>batch_size</span>)<br/>test_dataset = _create_tf_dataset(X_test, y_test, <span>batch_size</span>)<br/><br/><span># Get the batch sizes for the train, valid, and test datasets<br/></span>num_train_batches = <span>int</span>(X_train.shape[<span>0</span>] // <span>batch_size</span>)<br/>num_valid_batches = <span>int</span>(X_valid.shape[<span>0</span>] // <span>batch_size</span>)<br/>num_test_batches = <span>int</span>(X_test.shape[<span>0</span>] // <span>batch_size</span>)<br/><br/><span>return </span>train_dataset, valid_dataset, test_dataset, num_train_batches, num_valid_batches, num_test_batches</pre>
<p>And with that, we have an optimized input data pipeline that is much faster than using <kbd>feed_dict</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">controller.py</h1>
                </header>
            
            <article>
                
<p>The <kbd>controller.py</kbd> module is where everything comes together. We will implement the Controller, which handles training each child network as well as its own parameter updates. We first implement a helper function that calculates an exponential moving average of a list of numbers. We use this as the baseline function for our REINFORCE gradient calculation, as mentioned previously, to calculate the exponential moving average of the past rewards:</p>
<pre><span>import </span>logging<br/><br/><span>import </span>numpy <span>as </span>np<br/><span>import </span>tensorflow <span>as </span>tf<br/><br/><span>from </span>child_network <span>import </span>ChildCNN<br/><span>from </span>cifar10_processor <span>import </span>get_tf_datasets_from_numpy<br/><span>from </span>config <span>import </span>child_network_params, controller_params<br/><br/>logger = logging.getLogger(__name__)<br/><br/><span>def </span><span>ema</span>(<span>values</span>):<br/>    <span>"""<br/></span><span>    Helper function for keeping track of an exponential moving average of a list of values.<br/></span><span>    For this module, we use it to maintain an exponential moving average of rewards<br/></span><span>    <br/></span><span>    Args:<br/></span><span>        values (list): A list of rewards <br/></span><span><br/></span><span>    Returns:<br/></span><span>        (float) The last value of the exponential moving average<br/></span><span>    """<br/></span><span>    </span>weights = np.exp(np.linspace(-<span>1.</span>, <span>0.</span>, <span>len</span>(<span>values</span>)))<br/>    weights /= weights.sum()<br/>    a = np.convolve(<span>values</span>, weights, <span>mode</span>=<span>"full"</span>)[:<span>len</span>(<span>values</span>)]<br/>    <span>return </span>a[-<span>1</span>]</pre>
<p>Next, we define our <kbd>Controller</kbd> class:</p>
<pre><span>class </span><span>Controller</span>(<span>object</span>):<br/><br/>    <span>def </span>__init__(<span>self</span>):<br/>        <span>self</span>.graph = tf.Graph()<br/>        <span>self</span>.sess = tf.Session(<span>graph</span>=<span>self</span>.graph)<br/>        <span>self</span>.num_cell_outputs = controller_params[<span>'components_per_layer'</span>] * controller_params[<span>'max_layers'</span>]<br/>        <span>self</span>.reward_history = []<br/>        <span>self</span>.architecture_history = []<br/>        <span>self</span>.divison_rate = <span>100<br/></span><span>        </span><span>with </span><span>self</span>.graph.as_default():<br/>            <span>self</span>.build_controller()</pre>
<p>There are several attributes to note: <kbd>self.num_cell_outputs</kbd> refers to the number of values that our <span><strong>recurrent neural network</strong> (</span><strong>RNN</strong>) should output and corresponds to the length of the child network architecture configuration. <kbd>self.reward_history</kbd> and <kbd>self.ar chitecture_history</kbd> are simply buffers that allow us to keep track of rewards and child network architectures that the RNN generated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Method for generating the Controller</h1>
                </header>
            
            <article>
                
<p>We next implement a method for generating the Controller, which we call <kbd>build_controller</kbd>. The first step in constructing our Controller is defining the input placeholders. We create two of these—one is for the child network DNA, which is fed as input to the RNN for generating a new child network DNA, and the second is a list for storing discounted rewards when calculating the gradients for REINFORCE:</p>
<pre><span>def </span><span>build_controller</span>(<span>self</span>):<br/>    logger.info(<span>'Building controller network'</span>)<br/>    <span># Build inputs and placeholders<br/></span><span>    </span><span>with </span>tf.name_scope(<span>'controller_inputs'</span>):<br/>        <span># Input to the NASCell<br/></span><span>        </span><span>self</span>.child_network_architectures = tf.placeholder(tf.float32, [<span>None</span>, <span>self</span>.num_cell_outputs], <br/>                                                          <span>name</span>=<span>'controller_input'</span>)<br/>        <span># Discounted rewards<br/></span><span>        </span><span>self</span>.discounted_rewards = tf.placeholder(tf.float32, (<span>None</span>, ), <span>name</span>=<span>'discounted_rewards'</span>)</pre>
<p>We then define the output tensors of our RNN (to be implemented here). Note that the outputs of the RNN are small, in the range of (-1, 1). So, we multiply the output by 10 in order to create the child network DNA:</p>
<pre><span># Build controller<br/></span><span>with </span>tf.name_scope(<span>'network_generation'</span>):<br/>    <span>with </span>tf.variable_scope(<span>'controller'</span>):<br/>        <span>self</span>.controller_output = tf.identity(<span>self</span>.network_generator(<span>self</span>.child_network_architectures), <br/>                                             <span>name</span>=<span>'policy_scores'</span>)<br/>        <span>self</span>.cnn_dna_output = tf.cast(tf.scalar_mul(<span>self</span>.divison_rate, <span>self</span>.controller_output), tf.int32,<br/>                                      <span>name</span>=<span>'controller_prediction'</span>)</pre>
<p>We then define the loss function and optimizer. We use <kbd>RMSPropOptimizer</kbd> as our backpropagation algorithm, where the learning rate decays exponentially. Rather than calling <kbd>optimizer.minimize(loss)</kbd> as is usually done with other neural network models, we call the <kbd>compute_gradients</kbd> method to obtain gradients for calculating REINFORCE gradients:</p>
<pre><span># Set up optimizer<br/></span><span>self</span>.global_step = tf.Variable(<span>0</span>, <span>trainable</span>=<span>False</span>)<br/><span>self</span>.learning_rate = tf.train.exponential_decay(<span>0.99</span>, <span>self</span>.global_step, <span>500</span>, <span>0.96</span>, <span>staircase</span>=<span>True</span>)<br/><span>self</span>.optimizer = tf.train.RMSPropOptimizer(<span>learning_rate</span>=<span>self</span>.learning_rate)<br/><br/><span># Gradient and loss computation<br/></span><span>with </span>tf.name_scope(<span>'gradient_and_loss'</span>):<br/>    <span># Define policy gradient loss for the controller<br/></span><span>    </span><span>self</span>.policy_gradient_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<br/>        <span>logits</span>=<span>self</span>.controller_output[:, -<span>1</span>, :],<br/>        <span>labels</span>=<span>self</span>.child_network_architectures))<br/>    <span># L2 weight decay for Controller weights<br/></span><span>    </span><span>self</span>.l2_loss = tf.reduce_sum(tf.add_n([tf.nn.l2_loss(v) <span>for </span>v <span>in<br/></span><span>                                           </span>tf.trainable_variables(<span>scope</span>=<span>"controller"</span>)]))<br/>    <span># Add the above two losses to define total loss<br/></span><span>    </span><span>self</span>.total_loss = <span>self</span>.policy_gradient_loss + <span>self</span>.l2_loss * controller_params[<span>"beta"</span>]<br/>    <span># Compute the gradients<br/></span><span>    </span><span>self</span>.gradients = <span>self</span>.optimizer.compute_gradients(<span>self</span>.total_loss)<br/><br/>    <span># Gradients calculated using REINFORCE<br/></span><span>    </span><span>for </span>i, (grad, var) <span>in </span><span>enumerate</span>(<span>self</span>.gradients):<br/>        <span>if </span>grad <span>is not None</span>:<br/>            <span>self</span>.gradients[i] = (grad * <span>self</span>.discounted_rewards, var)</pre>
<p>Finally, we apply the REINFORCE gradients on the Controller parameters:</p>
<pre><span>with </span>tf.name_scope(<span>'train_controller'</span>):<br/>    <span># The main training operation. This applies REINFORCE on the weights of the Controller<br/></span><span>    </span><span>self</span>.train_op = <span>self</span>.optimizer.apply_gradients(<span>self</span>.gradients, <span>global_step</span>=<span>self</span>.global_step)<br/><br/>logger.info(<span>'Successfully built controller'</span>)</pre>
<p>The actual Controller network is created via the <kbd>network_generator</kbd> function. As mentioned, the Controller is a recurrent neural network with a special kind of cell. However, we don't have to implement this from scratch, as the developers behind TensorFlow have already implemented a custom <kbd>tf.contrib.rnn.NASCell</kbd>. We simply need to use this to construct our recurrent neural network and obtain the outputs:</p>
<pre><span>def </span><span>network_generator</span>(<span>self</span>, <span>nas_cell_hidden_state</span>):<br/>    <span># number of output units we expect from a NAS cell<br/></span><span>    </span><span>with </span>tf.name_scope(<span>'network_generator'</span>):<br/>        nas = tf.contrib.rnn.NASCell(<span>self</span>.num_cell_outputs)<br/>        network_architecture, nas_cell_hidden_state = tf.nn.dynamic_rnn(nas, tf.expand_dims(<br/>            <span>nas_cell_hidden_state</span>, -<span>1</span>), <span>dtype</span>=tf.float32)<br/>        bias_variable = tf.Variable([<span>0.01</span>] * <span>self</span>.num_cell_outputs)<br/>        network_architecture = tf.nn.bias_add(network_architecture, bias_variable)<br/>        <span>return </span>network_architecture[:, -<span>1</span>:, :]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a child network using the Controller</h1>
                </header>
            
            <article>
                
<p>Now, we implement a method that generates a child network using the Controller:</p>
<pre><span>def </span><span>generate_child_network</span>(<span>self</span>, <span>child_network_architecture</span>):<br/>    <span>with </span><span>self</span>.graph.as_default():<br/>        <span>return </span><span>self</span>.sess.run(<span>self</span>.cnn_dna_output, {<span>self</span>.child_network_architectures: <span>child_network_architecture</span>})</pre>
<p>Once we generate our child network, we call the <kbd>train_child_network</kbd> function to train it. This function takes <kbd>child_dna</kbd> and <kbd>child_id</kbd> and returns the validation accuracy that the child network achieves. First, we instantiate a new <kbd>tf.Graph()</kbd> and a new <kbd>tf.Session()</kbd> so that the child network is separated from the Controller's graph:</p>
<pre><span>def </span><span>train_child_network</span>(<span>self</span>, <span>cnn_dna</span>, <span>child_id</span>):<br/>    <span>"""<br/></span><span>    Trains a child network and returns reward, or the validation accuracy<br/></span><span>    Args:<br/></span><span>        cnn_dna (list): List of tuples representing the child network's DNA<br/></span><span>        child_id (str): Name of child network<br/></span><span><br/></span><span>    Returns:<br/></span><span>        (float) validation accuracy<br/></span><span>    """<br/></span><span>    </span>logger.info(<span>"Training with dna: {}"</span>.format(<span>cnn_dna</span>))<br/>    child_graph = tf.Graph()<br/>    <span>with </span>child_graph.as_default():<br/>        sess = tf.Session()<br/><br/>        child_network = ChildCNN(<span>cnn_dna</span>=<span>cnn_dna</span>, <span>child_id</span>=<span>child_id</span>, **child_network_params)</pre>
<p>We then define the input data pipeline, which uses the <kbd>tf.data.Dataset</kbd> creator we implemented here. In particular, we use <kbd>tf.data.Iterator</kbd> to create a generator that yields a batch of input tensors every time we call <kbd>iterator.get_next()</kbd>. We initialize an iterator for the training and validation datasets respectively. The batch of input tensors contains the <kbd>CIFAR-10</kbd> images and the corresponding labels, which we unpack at the end:</p>
<pre><span># Create input pipeline<br/></span>train_dataset, valid_dataset, test_dataset, num_train_batches, num_valid_batches, num_test_batches = \<br/>    get_tf_datasets_from_numpy(<span>batch_size</span>=child_network_params[<span>"batch_size"</span>])<br/><br/><span># Generic iterator<br/></span>iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)<br/>next_tensor_batch = iterator.get_next()<br/><br/><span># Separate train and validation set init ops<br/></span>train_init_ops = iterator.make_initializer(train_dataset)<br/>valid_init_ops = iterator.make_initializer(valid_dataset)<br/><br/><span># Build the graph<br/></span>input_tensor, labels = next_tensor_batch</pre>
<p>The <kbd>input_tensor</kbd> becomes the argument to the child network's <kbd>build</kbd> method. We then define all the TensorFlow operations needed for training, including the prediction, loss, optimizer, and accuracy operations:</p>
<pre><span># Build the child network, which returns the pre-softmax logits of the child network<br/></span>logits = child_network.build(input_tensor)<br/><br/><span># Define the loss function for the child network<br/></span>loss_ops = tf.nn.softmax_cross_entropy_with_logits_v2(<span>labels</span>=labels, <span>logits</span>=logits, <span>name</span>=<span>"loss"</span>)<br/><br/><span># Define the training operation for the child network<br/></span>train_ops = tf.train.AdamOptimizer(<span>learning_rate</span>=child_network_params[<span>"learning_rate"</span>]).minimize(loss_ops)<br/><br/><span># The following operations are for calculating the accuracy of the child network<br/></span>pred_ops = tf.nn.softmax(logits, <span>name</span>=<span>"preds"</span>)<br/>correct = tf.equal(tf.argmax(pred_ops, <span>1</span>), tf.argmax(labels, <span>1</span>), <span>name</span>=<span>"correct"</span>)<br/>accuracy_ops = tf.reduce_mean(tf.cast(correct, tf.float32), <span>name</span>=<span>"accuracy"</span>)<br/><br/>initializer = tf.global_variables_initializer()</pre>
<p>We then train the child network. Notice that when calling <kbd>sess.run(...)</kbd>, we are no longer passing an argument for the <kbd>feed_dict</kbd> parameter. Instead, we are simply calling the operations we want to run (<kbd>loss_ops</kbd>, <kbd>train_ops</kbd>, and <kbd>accuracy_ops</kbd>). This is because the inputs are already represented as tensors in the child network's graph:</p>
<pre><span># Training<br/></span>sess.run(initializer)<br/>sess.run(train_init_ops)<br/><br/>logger.info(<span>"Training child CNN {} for {} epochs"</span>.format(<span>child_id</span>, child_network_params[<span>"max_epochs"</span>]))<br/><span>for </span>epoch_idx <span>in </span><span>range</span>(child_network_params[<span>"max_epochs"</span>]):<br/>    avg_loss, avg_acc = [], []<br/><br/>    <span>for </span>batch_idx <span>in </span><span>range</span>(num_train_batches):<br/>        loss, _, accuracy = sess.run([loss_ops, train_ops, accuracy_ops])<br/>        avg_loss.append(loss)<br/>        avg_acc.append(accuracy)<br/><br/>    logger.info(<span>"</span><span>\t</span><span>Epoch {}:</span><span>\t</span><span>loss - {:.6f}</span><span>\t</span><span>accuracy - {:.3f}"</span>.format(epoch_idx,<br/>                                                                       np.mean(avg_loss), np.mean(avg_acc)))</pre>
<p>Once training finishes, we calculate the validation accuracy and return it:</p>
<pre>    <span># Validate and return reward<br/></span><span>    </span>logger.info(<span>"Finished training, now calculating validation accuracy"</span>)<br/>    sess.run(valid_init_ops)<br/>    avg_val_loss, avg_val_acc = [], []<br/>    <span>for </span>batch_idx <span>in </span><span>range</span>(num_valid_batches):<br/>        valid_loss, valid_accuracy = sess.run([loss_ops, accuracy_ops])<br/>        avg_val_loss.append(valid_loss)<br/>        avg_val_acc.append(valid_accuracy)<br/>    logger.info(<span>"Valid loss - {:.6f}</span><span>\t</span><span>Valid accuracy - {:.3f}"</span>.format(np.mean(avg_val_loss),<br/>                                                                      np.mean(avg_val_acc)))<br/><br/><span>return </span>np.mean(avg_val_acc)</pre>
<p>Finally, we implement a method for training the Controller. Due to computational resource constraints, we will not parallelize the training procedure (that is, <em>m</em> child networks trained in parallel per Controller epoch). Instead, we will sequentially generate these child networks and keep track of the mean validation accuracy among them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">train_controller method</h1>
                </header>
            
            <article>
                
<p>The <kbd>train_controller</kbd> method is called after we build the Controller. The first step is thus to initialize all the variables and the first state:</p>
<pre><span>def </span><span>train_controller</span>(<span>self</span>):<br/>    <span>with </span><span>self</span>.graph.as_default():<br/>        <span>self</span>.sess.run(tf.global_variables_initializer())<br/><br/>    step = <span>0<br/></span><span>    </span>total_rewards = <span>0<br/></span><span>    </span>child_network_architecture = np.array([[<span>10.0</span>, <span>128.0</span>, <span>1.0</span>, <span>1.0</span>] *<br/>                                           controller_params[<span>'max_layers'</span>]], <span>dtype</span>=np.float32)</pre>
<p>The first <kbd>child_network_architecture</kbd> is a list that resembles an architecture configuration and will be the argument to <kbd>NASCell</kbd>, which would output the first child DNA.</p>
<p>The training procedure consists of two <kbd>for</kbd> loops: one for the number of epochs for the Controller, and another for each child network the Controller generates per epoch. In the inner <kbd>for</kbd> loop, we generate a new <kbd>child_network_architecture</kbd> using <kbd>NASCell</kbd> and train a child network based on it to obtain a validation accuracy:</p>
<pre><span>for </span>episode <span>in </span><span>range</span>(controller_params[<span>'max_episodes'</span>]):<br/>    logger.info(<span>'=============&gt; Episode {} for Controller'</span>.format(episode))<br/>    step += <span>1<br/></span><span>    </span>episode_reward_buffer = []<br/><br/>    <span>for </span>sub_child <span>in </span><span>range</span>(controller_params[<span>"num_children_per_episode"</span>]):<br/>        <span># Generate a child network architecture<br/></span><span>        </span>child_network_architecture = <span>self</span>.generate_child_network(child_network_architecture)[<span>0</span>]<br/><br/>        <span>if </span>np.any(np.less_equal(child_network_architecture, <span>0.0</span>)):<br/>            reward = -<span>1.0<br/></span><span>        </span><span>else</span>:<br/>            reward = <span>self</span>.train_child_network(<span>cnn_dna</span>=child_network_architecture,<br/>                                              <span>child_id</span>=<span>'child/{}'</span>.format(<span>"{}_{}"</span>.format(episode, sub_child)))<br/>        episode_reward_buffer.append(reward)</pre>
<p>After we obtain <em>m</em> validation accuracies, we update our Controller using the mean reward and the gradients computed with respect to the last child network's DNA. We also keep track of past mean rewards. Using the <kbd>ema</kbd> method implemented previously, we calculate the baseline, which we then subtract from the latest mean reward. We then call <kbd>self.sess.run([self.train_op, self.total_loss]...)</kbd> to update the Controller and calculate the Controller's loss:</p>
<pre>mean_reward = np.mean(episode_reward_buffer)<br/><br/><span>self</span>.reward_history.append(mean_reward)<br/><span>self</span>.architecture_history.append(child_network_architecture)<br/>total_rewards += mean_reward<br/><br/>child_network_architecture = np.array(<span>self</span>.architecture_history[-step:]).ravel() / <span>self</span>.divison_rate<br/>child_network_architecture = child_network_architecture.reshape((-<span>1</span>, <span>self</span>.num_cell_outputs))<br/>baseline = ema(<span>self</span>.reward_history)<br/>last_reward = <span>self</span>.reward_history[-<span>1</span>]<br/>rewards = [last_reward - baseline]<br/>logger.info(<span>"Buffers before loss calculation"</span>)<br/>logger.info(<span>"States: {}"</span>.format(child_network_architecture))<br/>logger.info(<span>"Rewards: {}"</span>.format(rewards))<br/><br/><span>with </span><span>self</span>.graph.as_default():<br/>    _, loss = <span>self</span>.sess.run([<span>self</span>.train_op, <span>self</span>.total_loss],<br/>                            {<span>self</span>.child_network_architectures: child_network_architecture,<br/>                             <span>self</span>.discounted_rewards: rewards})<br/><br/>logger.info(<span>'Episode: {} | Loss: {} | DNA: {} | Reward : {}'</span>.format(<br/>    episode, loss, child_network_architecture.ravel(), mean_reward))</pre>
<p>And that's it! You can find the full implementation of <kbd>controller.py</kbd> in the main GitHub repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing ChildCNN</h1>
                </header>
            
            <article>
                
<p>Now that we have implemented both <kbd>child_network</kbd> and <kbd>controller</kbd>, it would be great to test the training of <kbd>ChildCNN</kbd> via our <kbd>Controller</kbd> with custom child network configurations. We would like to make sure that, with a sensible architecture, <kbd>ChildCNN</kbd> can learn sufficiently.</p>
<p>To do this, first open up your favorite Terminal and start a Jupyter console:</p>
<pre>$ ipython<br/>Python 3.6.4 (default, Jan 6 2018, 11:49:38)<br/>Type 'copyright', 'credits' or 'license' for more information<br/>IPython 6.4.0 -- An enhanced Interactive Python. Type '?' for help.</pre>
<p>We first configure our logger so we can see the outputs on the Terminal:</p>
<pre class="mce-root">In [1]: import sys<br/><br/>In [2]: import logging<br/><br/>In [3]: logging.basicConfig(stream=sys.stdout,<br/>   ...: level=logging.DEBUG,<br/>   ...: format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s')<br/>   ...:<br/><br/>In [4]:</pre>
<p class="mce-root"/>
<p>Next, we import the <kbd>Controller</kbd> class from <kbd>controller.py</kbd>:</p>
<pre>In [4]: import numpy as np<br/><br/>In [5]: from controller import Controller<br/><br/>In [6]:</pre>
<p>We then handcraft some child network architecture to be passed to the Controller's <kbd>train_child_network</kbd> function:</p>
<pre>In [7]: dna = np.array([[3, 1, 30, 2], [3, 1, 30, 2], [3, 1, 40, 2]])</pre>
<p>Finally, we instantiate our <kbd>Controller</kbd> and call the <kbd>train_child_network</kbd> method:</p>
<pre>In [8]: controller = Controller()<br/><br/>...<br/><br/>2018-09-16 01:58:54,978 controller INFO Successfully built controller<br/><br/>In [9]: controller.train_child_network(dna, "test")<br/><br/>2018-09-16 01:58:59,208 controller INFO Training with dna: [[ 3 1 30 2]<br/> [ 3 1 30 2]<br/> [ 3 1 40 2]]<br/><strong>2018-09-16 01:58:59,605 cifar10_processor INFO Dividing pixels by 255</strong><br/><strong>2018-09-16 01:59:01,289 cifar10_processor INFO Loaded data from keras</strong><br/><strong>2018-09-16 01:59:03,150 child_network INFO DNA is: [[3, 1, 30, 2], [3, 1, 30, 2], [3, 1, 40, 2]]</strong><br/><strong>2018-09-16 01:59:14,270 controller INFO Training child CNN first for 1000 epochs</strong></pre>
<p>If successful, you should be seeing decent accuracy scores after several epochs of training:</p>
<pre><strong>2018-09-16 06:25:01,927 controller INFO Epoch 436: loss - 1.119608 accuracy - 0.663</strong><br/><strong>2018-09-16 06:25:19,310 controller INFO Epoch 437: loss - 0.634937 accuracy - 0.724</strong><br/><strong>2018-09-16 06:25:36,438 controller INFO Epoch 438: loss - 0.769766 accuracy - 0.702</strong><br/><strong>2018-09-16 06:25:53,413 controller INFO Epoch 439: loss - 0.760520 accuracy - 0.711</strong><br/><strong>2018-09-16 06:26:10,530 controller INFO Epoch 440: loss - 0.606741 accuracy - 0.812</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">config.py</h1>
                </header>
            
            <article>
                
<p>The <kbd>config.py</kbd> module includes configurations used by the Controller and the child networks. Here, you can adjust several training parameters, such as the number of episodes, the learning rate, and the number of child networks generated by the Controller per epoch. You can also experiment with child network sizes, but do note that the larger the child network, the longer training takes for both the Controller and the child network:</p>
<pre>child_network_params = {<br/>    <span>"learning_rate"</span>: <span>3e-5</span>,<br/>    <span>"max_epochs"</span>: <span>100</span>,<br/>    <span>"beta"</span>: <span>1e-3</span>,<br/>    <span>"batch_size"</span>: <span>20<br/></span>}<br/><br/>controller_params = {<br/>    <span>"max_layers"</span>: <span>3</span>,<br/>    <span>"components_per_layer"</span>: <span>4</span>,<br/>    <span>'beta'</span>: <span>1e-4</span>,<br/>    <span>'max_episodes'</span>: <span>2000</span>,<br/>    <span>"num_children_per_episode"</span>: <span>10<br/></span>}</pre>
<p>Some of these numbers (such as <kbd>max_episodes</kbd>) are arbitrarily chosen. We encourage the reader to tweak these numbers to understand how they affect the training of both the Controller and the child networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">train.py</h1>
                </header>
            
            <article>
                
<p>This <kbd>train.py</kbd> module acts as our top-level entry to training the Controller:</p>
<pre><span>import </span>logging<br/><span>import </span>sys<br/><br/><span>from </span>.controller <span>import </span>Controller<br/><br/><span>if </span>__name__ == <span>'__main__'</span>:<br/>    <span># Configure the logger<br/></span><span>    </span>logging.basicConfig(<span>stream</span>=sys.stdout,<br/>                        <span>level</span>=logging.DEBUG,<br/>                        <span>format</span>=<span>'%(asctime)s %(name)-12s %(levelname)-8s %(message)s'</span>)<br/>    controller = Controller()<br/>    controller.train_controller()</pre>
<p class="mce-root"/>
<p>And there we have it; a neural network that generates other neural networks! Make sure your implementation has the following directory structure:</p>
<pre>src<br/>|-- __init__.py<br/>|-- child_network.py<br/>|-- cifar10_processor.py<br/>|-- config.py<br/>|-- constants.py<br/>|-- controller.py<br/>`-- train.py</pre>
<p>To execute training, simply run the following command:</p>
<pre><strong>$ python train.py</strong></pre>
<p>If all works well, you should be seeing output like the following:</p>
<pre><span><strong>2018-09-16 04:13:45,484 src.controller INFO Successfully built controller </strong><br/><strong>2018-09-16 04:13:45,542 src.controller INFO =============&gt; Episode 0 for Controller </strong><br/><strong>2018-09-16 04:13:45,952 src.controller INFO Training with dna: [[ 2 10 2 4 1 1 12 14 7 1 1 1]] 2018-09-16 04:13:45.953482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0 </strong><br/><strong>2018-09-16 04:13:45.953530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix: </strong><br/><strong>2018-09-16 04:13:45.953543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0 </strong><br/><strong>2018-09-16 04:13:45.953558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0: N </strong><br/><strong>2018-09-16 04:13:45.953840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wi th 21618 MB memory) -&gt; physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:03:00.0, compute capability: 5.2) </strong><br/><strong>2018-09-16 04:13:47,143 src.cifar10_processor INFO Dividing pixels by 255 </strong><br/><strong>2018-09-16 04:13:55,119 src.cifar10_processor INFO Loaded data from keras </strong><br/><strong>2018-09-16 04:14:09,050 src.child_network INFO DNA is: [[2, 10, 2, 4], [1, 1, 12, 14], [7, 1, 1, 1]] </strong><br/><strong>2018-09-16 04:14:21,326 src.controller INFO Training child CNN child/0_0 for 100 epochs </strong><br/><strong>2018-09-16 04:14:32,830 src.controller INFO Epoch 0: loss - 2.351300 accuracy - 0.100</strong><br/><strong>2018-09-16 04:14:43,976 src.controller INFO Epoch 1: loss - 2.202928 accuracy - 0.180 </strong><br/><strong>2018-09-16 04:14:53,412 src.controller INFO Epoch 2: loss - 2.102713 accuracy - 0.220 </strong><br/><strong>2018-09-16 04:15:03,704 src.controller INFO Epoch 3: loss - 2.092676 accuracy - 0.232 </strong><br/><strong>2018-09-16 04:15:14,349 src.controller INFO Epoch 4: loss - 2.092633 accuracy - 0.240</strong><br/></span></pre>
<p>You should see logging statements for each child network architecture its CIFAR-10 training logs. During CIFAR-10 training, we print the loss and accuracy for each epoch as well as the validation accuracy which we return to the Controller.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Additional exercises</h1>
                </header>
            
            <article>
                
<p>In this section, we have implemented the NAS framework for <kbd>CIFAR-10</kbd> data. While this is a great start, there are additional features one can implement, which we will leave to the reader as exercises:</p>
<ul>
<li>How can we make the Controller create child networks that solve problems in other domains, such as text and speech recognition?</li>
<li>How can we make the Controller train multiple child networks in parallel in order to speed up the training process?</li>
<li>How can we visualize the training process using TensorBoard?</li>
<li>How can we make the Controller design child networks that include residual connections?</li>
</ul>
<p>Some of these exercises may require significant changes in the code base but are beneficial for deepening your understanding of NAS. We definitely recommend giving these a try!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advantages of NAS</h1>
                </header>
            
            <article>
                
<p>The biggest advantage of NAS is that one does not need to spend copious amounts of time designing a neural network for a particular problem. This also means that those who are not data scientists can also create machine learning agents as long as they can prepare data. In fact, Google has already productized this framework as Cloud AutoML, which allows anyone to train customized machine learning models with minimum effort. According to Google, Cloud AutoML provides the following benefits:</p>
<ul>
<li>Users only need to interact with a simple GUI to create machine learning models.</li>
<li>Users can have Cloud AutoML annotate their own datasets if they are not labeled already. This is similar to Amazon's Mechanical Turk service.</li>
</ul>
<ul>
<li>Models generated by Cloud AutoML are guaranteed to have high accuracy and fast performance.</li>
<li>Easy end-to-end pipeline for uploading data, training and validating the model, deploying the model, and creating a REST endpoint for fetching predictions.</li>
</ul>
<p>Currently, Cloud AutoML can be used for image classification/detection, natural language processing (text classification), and translation.</p>
<div class="packt_infobox">For more information on Cloud AutoML, check out their official page here: <a href="https://cloud.google.com/automl/">https://cloud.google.com/automl/</a></div>
<p>Another advantage that NAS provides is the ability to generate more compact models than those designed by humans. According to <em>Efficient Neural Architecture Search via Parameter Sharing</em> by Hieu Pham et. al., whereas the most recent state-of-the-art neural network for <kbd>CIFAR-10</kbd> classification had 26.2 million parameters, a NAS-generated neural network that achieved comparable test accuracy (97.44% for human-designed network versus 97.35% for the NAS-generated network) only had 3.3 million parameters. Note that older, less-accurate models such as VGG16, ResNet50, and InceptionV3 have 138 million, 25 million, and 23 million parameters respectively. The vast reduction in parameter size allows for more efficient inference time and model storage, both of which are important aspects when deploying models into production.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have implemented NAS, a framework where a reinforcement learning agent (the Controller) generates child neural networks to complete a certain task. We studied the theory behind how the Controller learns to generate better child network architectures via policy gradient methods. We then implemented a simplified version of NAS that generates child networks that learn to classify <kbd>CIFAR-10</kbd> images.</p>
<p>For more information on related topics, refer to the following list of links:</p>
<ul>
<li><span>NAS with reinforcement learning: <a href="https://arxiv.org/abs/1611.01578">https://arxiv.org/abs/1611.01578</a></span></li>
<li><span>Efficient NAS via parameter sharing: <a href="https://arxiv.org/pdf/1802.03268">https://arxiv.org/pdf/1802.03268</a></span></li>
<li><span>Google Cloud AutoML: <a href="https://cloud.google.com/automl/">https://cloud.google.com/automl/</a></span></li>
<li><span>Awesome Architecture Search—a curated list of papers related to generating neural networks: <a href="https://github.com/markdtw/awesome-architecture-search">https://github.com/markdtw/awesome-architecture-search</a></span></li>
</ul>
<p>The NAS framework marks an exciting development in the deep learning field, for we have figured out how to automatically design neural network architectures, a decision previously made by humans. There are now improved versions of NAS and other kinds of algorithms that generate neural networks automatically, which we encourage the reader to look into as well.</p>


            </article>

            
        </section>
    </body></html>