<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-6-deep-q-networks">
<h1 class="chapterNumber">6</h1>
<h1 class="chapterTitle" id="sigil_toc_id_409">
<span id="x1-900006"/>Deep Q-Networks
    </h1>
<p>In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch009.xhtml#x1-820005"><span class="cmti-10x-x-109">5</span></a>, you became familiar with the Bellman equation and the practical method of its application called <span class="cmti-10x-x-109">value iteration</span>. This approach allowed us to significantly improve our speed and convergence in the FrozenLake environment, which is promising, but can we go further? In this chapter, we will apply the same approach to problems of much greater complexity: arcade games from the Atari 2600 platform, which are the de facto benchmark of the <span class="cmbx-10x-x-109">reinforcement learning </span>(<span class="cmbx-10x-x-109">RL</span>) research community.</p>
<p>To deal with this new and more challenging goal, in this chapter, we will:</p>
<ul>
<li>
<p>Talk about <span id="dx1-90001"/>problems with the value iteration method and consider its variation, called <span class="cmti-10x-x-109">Q-learning</span>.</p>
</li>
<li>
<p>Apply Q-learning to so-called grid world environments, which is called <span class="cmbx-10x-x-109">tabular Q-learning</span>.</p>
</li>
<li>
<p>Discuss Q-learning in conjunction with <span class="cmbx-10x-x-109">neural networks </span>(<span class="cmbx-10x-x-109">NNs</span>). This combination has the name <span class="cmbx-10x-x-109">deep Q-network (DQN)</span>.</p>
</li>
</ul>
<p>At the end of the chapter, we will reimplement a DQN algorithm from the famous paper <span class="cmti-10x-x-109">Playing Atari with deep reinforcement learning </span>[<span id="x1-90002"/><a href="#">Mni13</a>], which was published in 2013 and started a new era in RL development. Although it is too early to discuss the practical applicability of these basic methods, this will become clearer to you as you progress with the book.</p>
<section class="level3 sectionHead" id="real-life-value-iteration">
<h1 class="heading-1" id="sigil_toc_id_80"> <span id="x1-910006.1"/>Real-life value iteration</h1>
<p>The improvements that we got in the FrozenLake environment by switching from the<span id="dx1-91001"/> cross-entropy method to the value iteration method are quite encouraging, so it‚Äôs tempting to apply the value iteration method to more challenging problems. However, it is important to look at the assumptions and limitations that our value iteration method has. But let‚Äôs start with a quick recap of the method. On every step, the value iteration method does a loop on all states, and for every state, it performs an update of its value with a Bellman approximation. The variation of the same method for Q-values (values for actions) is almost the same, but we approximate and store values for every state and action. So what‚Äôs wrong with this process?</p>
<p>The first obvious problem is the count of environment states and our ability to iterate over them. In value iteration, we assume that we know all states in our environment in advance, can iterate over them, and can store their value approximations. It‚Äôs easy to do for the simple grid world environment of FrozenLake, but what about other tasks?</p>
<p>To understand this, let‚Äôs first look at how scalable the value iteration approach is, or, in other words, how many states we can easily iterate over in every loop. Even a moderate-sized computer can keep several billion float values in memory (8.5 billion in 32 GB of RAM), so the memory required for value tables doesn‚Äôt look like a huge constraint. Iteration over billions of states and actions will be more <span class="cmbx-10x-x-109">central processing unit </span>(<span class="cmbx-10x-x-109">CPU</span>)-demanding but is not an insurmountable problem.</p>
<p>Nowadays, we have multicore systems that are mostly idle, so by using parallelism, we <span id="dx1-91002"/>can iterate over billions of values in a reasonable amount of time. The real problem is the number of samples required to get good approximations for state transition dynamics. Imagine that you have some environment with, say, a billion states (which corresponds approximately to a FrozenLake of size 31600 <span class="cmsy-10x-x-109">√ó </span>31600). To calculate even a rough approximation for every state of this environment, we would need hundreds of billions of transitions evenly distributed over our states, which is not practical.</p>
<p>To give you an example of an environment with an even larger number of potential states, let‚Äôs consider the Atari 2600 game console again. This was very popular in the 1980s, and many arcade-style games were available for it. The Atari console is archaic by today‚Äôs gaming standards, but its games provide an excellent set of RL problems that humans can master fairly quickly, yet are still challenging for computers. Not surprisingly, this platform (using an emulator, of course) is a very popular benchmark within RL research, as I mentioned.</p>
<p>Let‚Äôs calculate the state space for the Atari platform. The resolution of the screen is 210 <span class="cmsy-10x-x-109">√ó </span>160 pixels, and every pixel has one of 128 colors. So every frame of the screen has 210 <span class="cmsy-10x-x-109">‚ãÖ </span>160 = 33600 pixels and the total number of different screens possible is 128<sup><span class="cmr-8">33600</span></sup>, which is slightly more than 10<sup><span class="cmr-8">70802</span></sup>. If we decide to just enumerate all possible states of the Atari once, it will take billions of billions of years even for the fastest supercomputer. Also, 99(.9)% of this job will be a waste of time, as most of the combinations will never be shown during even long gameplay, so we will never have samples of those states. However, the value iteration method wants to iterate over them just in case.</p>
<p>The second main problem with the value iteration approach is that it limits us to discrete action spaces. Indeed, both <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) and <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) approximations assume that our actions are a mutually exclusive discrete set, which is not true for continuous control problems where actions can represent continuous variables, such as the angle of a steering wheel, the force on an actuator, or the temperature of a heater. This issue is much more challenging than the first, and we will talk about it in the last part of the book, in chapters dedicated to continuous action space problems. For now, let‚Äôs assume that we have a discrete count of actions and that this count is not very large (i.e., orders of 10s). How should we handle the state space size issue?</p>
</section>
<section class="level3 sectionHead" id="tabular-q-learning">
<h1 class="heading-1" id="sigil_toc_id_81"> <span id="x1-920006.2"/>Tabular Q-learning</h1>
<p>The key question to focus on when trying to handle the state space issue is, do we really <span id="dx1-92001"/>need to iterate over every state in the state space? We have an environment that can be used as a source of real-life samples of states. If some state in the state space is not shown to us by the environment, why should we care about its value? We can only use states obtained from the environment to update the values of states, which can save us a lot of work.</p>
<p>This modification of the value iteration method is known as Q-learning, as mentioned earlier, and for cases with explicit state-to-value mappings, it entails the following steps:</p>
<ol>
<li>
<div id="x1-92003x1">
<p>Start with an empty table, mapping states to values of actions.</p>
</div>
</li>
<li>
<div id="x1-92005x2">
<p>By interacting with the environment, obtain the tuple <span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤ </span>(state, action, reward, and the new state). In this step, you need to decide which action to take, and there is no single proper way to make this decision. We discussed this problem as exploration versus exploitation in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a> and will talk a lot about it in this chapter.</p>
</div>
</li>
<li>
<div id="x1-92007x3">
<p>Update the <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) value using the Bellman approximation:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="37" src="../Images/eq18.png" width="287"/>
</div>
</div>
</li>
<li>
<div id="x1-92009x4">
<p>Repeat from step 2.</p>
</div>
</li>
</ol>
<p>As in value iteration, the end condition could be some threshold of the update, or we could perform test episodes to estimate the expected reward from the policy. Another thing to note here is how to update the Q-values. As we take samples from the environment, it‚Äôs generally a bad idea to just assign new values on top of existing values, as training can become unstable.</p>
<p>What is usually done in practice is updating the <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) with approximations using a ‚Äúblending‚Äù technique, which is just averaging between old and new values of Q using learning rate <span class="cmmi-10x-x-109">Œ± </span>with a value from 0 to 1:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="37" src="../Images/eq19.png" width="489"/>
</div>
<p>This allows values of Q to converge smoothly, even if our environment is noisy. The final version of the algorithm is as follows:</p>
<ol>
<li>
<div id="x1-92011x1">
<p>Start with an empty table for <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>).</p>
</div>
</li>
<li>
<div id="x1-92013x2">
<p>Obtain (<span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span>) from the environment.</p>
</div>
</li>
<li>
<div id="x1-92015x3">
<p>Make a Bellman update:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="37" src="../Images/eq19.png" width="489"/>
</div>
</div>
</li>
<li>
<div id="x1-92017x4">
<p>Check convergence conditions. If not met, repeat from step 2.</p>
</div>
</li>
</ol>
<p>As mentioned earlier, this method is called tabular Q-learning, as we keep a table of states <span id="dx1-92018"/>with their Q-values. Let‚Äôs try it on our FrozenLake environment. The whole example code is in <span class="cmtt-10x-x-109">Chapter06/01</span><span class="cmtt-10x-x-109">_frozenlake</span><span class="cmtt-10x-x-109">_q</span><span class="cmtt-10x-x-109">_learning.py</span>:</p>
<p>First, we import packages and define constants and used types:</p>
<div class="tcolorbox" id="tcolobox-88">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-111"><code>import typing as tt 
import gymnasium as gym 
from collections import defaultdict 
from torch.utils.tensorboard.writer import SummaryWriter 
 
ENV_NAME = "FrozenLake-v1" 
GAMMA = 0.9 
ALPHA = 0.2 
TEST_EPISODES = 20 
 
State = int 
Action = int 
ValuesKey = tt.Tuple[State, Action] 
 
class Agent: 
    def __init__(self): 
        self.env = gym.make(ENV_NAME) 
        self.state, _ = self.env.reset() 
        self.values: tt.Dict[ValuesKey] = defaultdict(float)</code></pre>
</div>
</div>
<p>The new thing here is the value of <span class="cmmi-10x-x-109">Œ±</span>, which will be used as the learning rate in the value update. The initialization of our <span class="cmtt-10x-x-109">Agent </span>class is simpler now, as we don‚Äôt need to track the history of rewards and transition counters, just our value table. This will make our memory footprint smaller, which is not a big issue for FrozenLake but can be critical for larger environments.</p>
<p>The method <span class="cmtt-10x-x-109">sample</span><span class="cmtt-10x-x-109">_env </span>is used to obtain the next transition from the environment:</p>
<div class="tcolorbox" id="tcolobox-89">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-112"><code>    def sample_env(self) -&gt; tt.Tuple[State, Action, float, State]: 
        action = self.env.action_space.sample() 
        old_state = self.state 
        new_state, reward, is_done, is_tr, _ = self.env.step(action) 
        if is_done or is_tr: 
            self.state, _ = self.env.reset() 
        else: 
            self.state = new_state 
        return old_state, action, float(reward), new_state</code></pre>
</div>
</div>
<p>We sample a random action from the action space and return the tuple of the old state, the action taken, the reward obtained, and the new state. The tuple will be used in the training loop later.</p>
<p>The next method receives the state of the environment:</p>
<div class="tcolorbox" id="tcolobox-90">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-113"><code>    def best_value_and_action(self, state: State) -&gt; tt.Tuple[float, Action]: 
        best_value, best_action = None, None 
        for action in range(self.env.action_space.n): 
            action_value = self.values[(state, action)] 
            if best_value is None or best_value &lt; action_value: 
                best_value = action_value 
                best_action = action 
        return best_value, best_action</code></pre>
</div>
</div>
<p>This method finds the best <span id="dx1-92055"/>action to take from the given state of the environment by taking the action with the largest value that we have in the table. If we don‚Äôt have the value associated with the state and action pair, then we take it as zero. This method will be used two times: first, in the test method that plays one episode using our current values table (to evaluate our policy‚Äôs quality), and second, in the method that performs the value update to get the value of the next state.</p>
<p>Next, we update our values table using one step from the environment:</p>
<div class="tcolorbox" id="tcolobox-91">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-114"><code>    def value_update(self, state: State, action: Action, reward: float, next_state: State): 
        best_val, _ = self.best_value_and_action(next_state) 
        new_val = reward + GAMMA * best_val 
        old_val = self.values[(state, action)] 
        key = (state, action) 
        self.values[key] = old_val * (1-ALPHA) + new_val * ALPHA</code></pre>
</div>
</div>
<p>Here, we first calculate the Bellman approximation for our state, <span class="cmmi-10x-x-109">s</span>, and action, <span class="cmmi-10x-x-109">a</span>, by summing the immediate reward with the discounted value of the next state. Then, we obtain the previous value of the state and action pair and blend these values together using the learning rate. The result is the new approximation for the value of state <span class="cmmi-10x-x-109">s </span>and action <span class="cmmi-10x-x-109">a</span>, which is stored in our table.</p>
<p>The last method in our <span class="cmtt-10x-x-109">Agent </span>class plays one full episode using the provided test environment:</p>
<div class="tcolorbox" id="tcolobox-92">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-115"><code>    def play_episode(self, env: gym.Env) -&gt; float: 
        total_reward = 0.0 
        state, _ = env.reset() 
        while True: 
            _, action = self.best_value_and_action(state) 
            new_state, reward, is_done, is_tr, _ = env.step(action) 
            total_reward += reward 
            if is_done or is_tr: 
                break 
            state = new_state 
        return total_reward</code></pre>
</div>
</div>
<p>The action on every step is taken using our current value table of Q-values. This method is used to evaluate our current policy to check the progress of learning. Note that this method doesn‚Äôt alter our value table; it only uses it to find the best action to take.</p>
<p>The rest <span id="dx1-92073"/>of the example is the training loop, which is very similar to examples from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch009.xhtml#x1-820005"><span class="cmti-10x-x-109">5</span></a>: we create a test environment, agent, and summary writer, and then, in the loop, we do one step in the environment and perform a value update using the obtained data. Next, we test our current policy by playing several test episodes. If a good reward is obtained, then we stop training:</p>
<div class="tcolorbox" id="tcolobox-93">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-116"><code>if __name__ == "__main__": 
    test_env = gym.make(ENV_NAME) 
    agent = Agent() 
    writer = SummaryWriter(comment="-q-learning") 
 
    iter_no = 0 
    best_reward = 0.0 
    while True: 
        iter_no += 1 
        state, action, reward, next_state = agent.sample_env() 
        agent.value_update(state, action, reward, next_state) 
 
        test_reward = 0.0 
        for _ in range(TEST_EPISODES): 
            test_reward += agent.play_episode(test_env) 
        test_reward /= TEST_EPISODES 
        writer.add_scalar("reward", test_reward, iter_no) 
        if test_reward &gt; best_reward: 
            print("%d: Best test reward updated %.3f -&gt; %.3f" % (iter_no, best_reward, test_reward)) 
            best_reward = test_reward 
        if test_reward &gt; 0.80: 
            print("Solved in %d iterations!" % iter_no) 
            break 
    writer.close()</code></pre>
</div>
</div>
<p>The result of the example is shown here:</p>
<pre class="lstlisting" id="listing-117"><code>Chapter06$ ./01_frozenlake_q_learning.py 
1149: Best test reward updated 0.000 -&gt; 0.500 
1150: Best test reward updated 0.500 -&gt; 0.550 
1164: Best test reward updated 0.550 -&gt; 0.600 
1242: Best test reward updated 0.600 -&gt; 0.650 
2685: Best test reward updated 0.650 -&gt; 0.700 
2988: Best test reward updated 0.700 -&gt; 0.750 
3025: Best test reward updated 0.750 -&gt; 0.850 
Solved in 3025 iterations!</code></pre>
<p>You may <span id="dx1-92107"/>have noticed that this version used more iterations (but your experiment might have a different count of steps) to solve the problem compared to the value iteration method from the previous chapter. The reason for that is that we are no longer using the experience obtained during testing. In the example <span class="cmtt-10x-x-109">Chapter05/02</span><span class="cmtt-10x-x-109">_frozenlake</span><span class="cmtt-10x-x-109">_q</span><span class="cmtt-10x-x-109">_iteration.py</span>, periodical tests caused an update of Q-table statistics. Here, we don‚Äôt touch Q-values during the test, which causes more iterations before the environment gets solved.</p>
<p>Overall, the <span id="dx1-92108"/>total number of samples required from the environment is almost the same. The reward chart in TensorBoard also shows good training dynamics, which is very similar to the value iteration method (the reward plot for value iteration is shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="ch009.xhtml#x1-87114r9"><span class="cmti-10x-x-109">5.9</span></a>):</p>
<div class="minipage">
<p><img alt="PIC" height="200" src="../Images/B22150_06_01.png" width="300"/> <span id="x1-92109r1"/></p>
<span class="id">Figure¬†6.1: Reward dynamics of FrozenLake </span>
</div>
<p>In the next section, we will extend the Q-learning method with NNs‚Äô preprocessing environment states. This will greatly extend the flexibility and applicability of the method we discussed.</p>
</section>
<section class="level3 sectionHead" id="deep-q-learning">
<h1 class="heading-1" id="sigil_toc_id_82"> <span id="x1-930006.3"/>Deep Q-learning</h1>
<p>The Q-learning method <span id="dx1-93001"/>that we have just covered solves the issue of iteration over the full set of states, but it can still struggle with situations when the count of the observable set of states is very large. For example, Atari games can have a large variety of different screens, so if we decide to use raw pixels as individual states, we will quickly realize that we have too many states to track and approximate values for.</p>
<p>In some environments, the count of different observable states could be almost infinite. For example, in CartPole, the environment gives us a state that is four floating point numbers. The number of value combinations is finite (they‚Äôre represented as bits), but this number is extremely large. With just bit values, it is around 2<sup><span class="cmr-8">4</span><span class="cmsy-8">‚ãÖ</span><span class="cmr-8">32</span></sup> <span class="cmsy-10x-x-109">‚âà </span>3<span class="cmmi-10x-x-109">.</span>4 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmr-8">38</span></sup>. In reality, it is less, as state values of the environment are bounded, so not all bit combinations of 4 <span class="cmtt-10x-x-109">float32 </span>values are possible, but the resulting state space is still too large. We could create some bins to discretize those values, but this often creates more problems than it solves; we would need to decide what ranges of parameters are important to distinguish as different states and what ranges could be clustered together. As we‚Äôre trying to implement RL methods in a general way (without looking inside the environment‚Äôs internals), this is not a very promising direction.</p>
<p>In the case of Atari, one single pixel change doesn‚Äôt make much difference, so we might want to treat similar images as one state. However, we still need to distinguish some of the states.</p>
<p>The following image shows two different situations in a game of Pong. We‚Äôre playing against the <span class="cmbx-10x-x-109">artificial intelligence </span>(<span class="cmbx-10x-x-109">AI</span>) opponent by controlling a paddle (our paddle is on the right, whereas our opponent‚Äôs is on the left). The objective of the game is to get the bouncing ball past our opponent‚Äôs paddle, while preventing the ball from getting past our paddle. We can consider the two situations to be completely different. In the situation shown on the right, the ball is close to the <span id="dx1-93002"/>opponent, so we can relax and watch. However, the situation on the left is more demanding; assuming that the ball is moving from left to right, the ball is moving toward our side, so we need to move our paddle quickly to avoid losing a point. The situations in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-93003r2"><span class="cmti-10x-x-109">6.2</span></a> are just two from the 10<sup><span class="cmr-8">70802</span></sup> possible situations, but we want our agent to act on them differently.</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/file32.png" width="251"/> <span id="x1-93003r2"/></p>
<span class="id">Figure¬†6.2: The ambiguity of observations in Pong. In the left image, the ball is moving to the right, toward our paddle, and on the right, its direction is opposite </span>
</div>
<p>As a solution to this problem, we can use a nonlinear representation that maps both the state and action onto a value. In machine learning, this is called a ‚Äúregression problem.‚Äù The concrete way to represent and train such a representation can vary, but, as you may have already guessed from this section‚Äôs title, using a deep NN is one of the most popular options, especially when dealing with observations represented as screen images. With this in mind, let‚Äôs make modifications to the Q-learning algorithm:</p>
<ol>
<li>
<div id="x1-93005x1">
<p>Initialize <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) with some initial approximation.</p>
</div>
</li>
<li>
<div id="x1-93007x2">
<p>By interacting with the environment, obtain the tuple (<span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span>).</p>
</div>
</li>
<li>
<div id="x1-93009x3">
<p>Calculate the loss:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="26" src="../Images/eq20.png" width="406"/>
</div>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="39" src="../Images/eq21.png" width="495"/>
</div>
</div>
</li>
<li>
<div id="x1-93011x4">
<p>Update <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) using the <span class="cmbx-10x-x-109">stochastic gradient descent (SGD) </span>algorithm, by <span id="dx1-93012"/>minimizing the loss with respect to the model parameters.</p>
</div>
</li>
<li>
<div id="x1-93014x5">
<p>Repeat from step 2 until converged.</p>
</div>
</li>
</ol>
<p>This algorithm looks simple, but, unfortunately, it won‚Äôt work very well. Let‚Äôs discuss some of the aspects that could go wrong and the potential ways we could approach these scenarios.</p>
<section class="level4 subsectionHead" id="interaction-with-the-environment">
<h2 class="heading-2" id="sigil_toc_id_83"> <span id="x1-940006.3.1"/>Interaction with the environment</h2>
<p>First of all, we <span id="dx1-94001"/>need to interact with the environment somehow to receive data to train on. In simple environments, such as FrozenLake, we can act randomly, but is this the best strategy to use? Imagine the game of Pong. What‚Äôs the probability of winning a single point by randomly moving the paddle? It‚Äôs not zero, but it‚Äôs extremely small, which just means that we will need to wait for a very long time for such a rare situation. As an alternative, we can use our Q-function approximation as a source of behavior (as we did before in the value iteration method, when we remembered our experience during testing).</p>
<p>If our representation of Q is good, then the experience that we get from the environment will show the agent relevant data to train on. However, we‚Äôre in trouble when our approximation is not perfect (at the beginning of the training, for example). In such a case, our agent can be stuck with bad actions for some states without ever trying to behave differently. This is the <span class="cmti-10x-x-109">exploration versus exploitation </span>dilemma mentioned briefly in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a>, which we will discuss in detail now. On the one hand, our agent needs to explore the environment to build a complete picture of transitions and action outcomes. On the other hand, we should use interaction with the environment efficiently; we shouldn‚Äôt waste time by randomly trying actions that we have already tried and learned outcomes for.</p>
<p>As you can see, random behavior is better at the beginning of the training when our Q approximation is bad, as it gives us more uniformly distributed information about the environment states. As our training progresses, random behavior becomes inefficient, and we want to fall back to our Q approximation to decide how to act.</p>
<p>A method that performs such a mix of <span id="dx1-94002"/>two extreme behaviors is known as an <span class="cmbx-10x-x-109">epsilon-greedy method</span>, which just means switching between random and Q policy using the probability hyperparameter <span class="cmmi-10x-x-109">ùúñ</span>. By varying <span class="cmmi-10x-x-109">ùúñ</span>, we can select the ratio of random actions. The usual practice is to start with <span class="cmmi-10x-x-109">ùúñ </span>= 1<span class="cmmi-10x-x-109">.</span>0 (100% random actions) and slowly decrease it to some small value, such as 5% or 2% random actions. Using an epsilon-greedy method helps us to both explore the environment in the beginning and stick to good policy at the end of the training. There are other solutions to the exploration versus exploitation problem, and we will discuss some of them in the third part of the book. This problem is one of the fundamental open questions in RL and an active area of research that is not even close to being resolved completely.</p>
</section>
<section class="level4 subsectionHead" id="sgd-optimization">
<h2 class="heading-2" id="sigil_toc_id_84"> <span id="x1-950006.3.2"/>SGD optimization</h2>
<p>The core of our Q-learning procedure is borrowed from supervised learning. Indeed, we <span id="dx1-95001"/>are trying to approximate a complex, nonlinear function, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>), with an NN. To do this, we must calculate targets for this function using the Bellman equation and then pretend that we have a supervised learning problem at hand. That‚Äôs okay, but one of the <span id="dx1-95002"/>fundamental requirements for SGD optimization is that the training data is <span class="cmbx-10x-x-109">independent and identically distributed</span> (frequently abbreviated as <span class="cmbx-10x-x-109">iid</span>), which means that our training data is randomly sampled from the underlying dataset we‚Äôre trying to learn on.</p>
<p>In our case, data that we are going to use for the SGD update doesn‚Äôt fulfill these criteria:</p>
<ol>
<li>
<div id="x1-95004x1">
<p>Our samples are not independent. Even if we accumulate a large batch of data samples, they will all be very close to each other, as they will belong to the same episode.</p>
</div>
</li>
<li>
<div id="x1-95006x2">
<p>Distribution of our training data won‚Äôt be identical to samples provided by the optimal policy that we want to learn. Data that we have will be a result of some other policy (our current policy, a random one, or both in the case of epsilon-greedy), but we don‚Äôt want to learn how to play randomly: we want an optimal policy with the best reward.</p>
</div>
</li>
</ol>
<p>To deal with this nuisance, we usually need to use a large buffer of our past experience and sample training data from it, instead of using our latest experience. This<span id="dx1-95007"/> technique is called a <span class="cmbx-10x-x-109">replay buffer</span>. The simplest implementation is a buffer of a fixed size, with new data added to the end of the buffer so that it pushes the oldest experience out of it.</p>
<p>The replay buffer allows us to train on more-or-less independent data, but the data will still be fresh enough to train on samples generated by our recent policy. In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>, we will check another kind of replay buffer, <span class="cmbx-10x-x-109">prioritized</span>, which provides a more sophisticated sampling approach.</p>
</section>
<section class="level4 subsectionHead" id="correlation-between-steps">
<h2 class="heading-2" id="sigil_toc_id_85"> <span id="x1-960006.3.3"/>Correlation between steps</h2>
<p>Another <span id="dx1-96001"/>practical issue with the default training procedure is also related to the lack of iid data, but in a slightly different manner. The <span id="dx1-96002"/>Bellman equation provides us with the value of <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) via <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>) (this process is called <span class="cmti-10x-x-109">bootstrapping</span>, when we use the formula recursively). However, both the states <span class="cmmi-10x-x-109">s</span> and <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤ </span>have only one step between them. This makes them very similar, and it‚Äôs very hard for NNs to distinguish between them. When we perform an update of our NNs‚Äô parameters to make <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) closer to the desired result, we can indirectly alter the value produced for <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>) and other states nearby. This can make our training very unstable, like chasing our own tail; when we update <span class="cmmi-10x-x-109">Q </span>for state <span class="cmmi-10x-x-109">s</span>, then on subsequent states, we will discover that <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>) becomes worse but attempts to update it can spoil our <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) approximation even more, and so on.</p>
<p>To make training more stable, there is a trick, called <span class="cmbx-10x-x-109">target network</span>, by <span id="dx1-96003"/>which we keep a copy of our network and use it for the <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>) value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in <span class="cmmi-10x-x-109">N </span>steps (where <span class="cmmi-10x-x-109">N </span>is usually quite a large hyperparameter, such as 1k or 10k training iterations).</p>
</section>
<section class="level4 subsectionHead" id="the-markov-property">
<h2 class="heading-2" id="sigil_toc_id_86"> <span id="x1-970006.3.4"/>The Markov property</h2>
<p>Our RL methods use <span class="cmbx-10x-x-109">Markov decision process </span>(<span class="cmbx-10x-x-109">MDP</span>) formalism as their basis, which <span id="dx1-97001"/>assumes <span id="dx1-97002"/>that the environment obeys the Markov property: observations from the environment are all that we need to act optimally. In other words, our observations allow us to distinguish states from one another.</p>
<p>As you saw from the preceding Pong screenshot in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-93003r2"><span class="cmti-10x-x-109">6.2</span></a>, one single image from the Atari game is <span id="dx1-97003"/>not enough to capture all the important information (using only one image, we have no idea about the speed and direction of objects, like the ball and our opponent‚Äôs paddle). This obviously violates the <span id="dx1-97004"/>Markov property and moves our single-frame Pong environment into the area of <span class="cmbx-10x-x-109">partially observable MDPs </span>(<span class="cmbx-10x-x-109">POMDPs</span>). A POMDP is basically an MDP without the Markov property, and it is very important in practice. For example, for most card games in which you don‚Äôt see your opponents‚Äô cards, game observations are POMDPs because the current observation (i.e., your cards and the cards on the table) could correspond to different cards in your opponents‚Äô hands.</p>
<p>We won‚Äôt discuss POMDPs in detail in this book, but we will use a small technique to push our environment back into the MDP domain. The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, we usually stack <span class="cmmi-10x-x-109">k </span>subsequent frames together and use them as the observation at every state. This allows our agent to deduct the dynamics of the current state, for instance, to get the speed of the ball and its direction. The usual ‚Äúclassical‚Äù number of <span class="cmmi-10x-x-109">k </span>for Atari is four. Of course, it‚Äôs just a hack, as there can be longer dependencies in the environment, but for most of the games, it works well.</p>
</section>
<section class="level4 subsectionHead" id="the-final-form-of-dqn-training">
<h2 class="heading-2" id="sigil_toc_id_87"> <span id="x1-980006.3.5"/>The final form of DQN training</h2>
<p>There are many more tips and tricks <span id="dx1-98001"/>that researchers have discovered to make DQN training <span id="dx1-98002"/>more stable and efficient, and we will cover the best of them in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>. However, epsilon-greedy, the replay buffer, and the target network form a basis that has allowed the company DeepMind to successfully train a DQN on a set of 49 Atari games, demonstrating the efficiency of this approach when applied to complicated environments.</p>
<p>The original paper <span class="cmti-10x-x-109">Playing Atari with deep reinforcement learning </span>[<span id="x1-98003"/><a href="#">Mni13</a>] (without a target network) was published at the end of 2013 and used seven games for testing. Later, at the beginning of 2015, a revised version of the article with the title <span class="cmti-10x-x-109">Human-level control through deep reinforcement learning </span>[<span id="x1-98004"/><a href="#">Mni+15</a>], already with 49 different games, was published in <span class="cmti-10x-x-109">Nature</span>.</p>
<p>The algorithm for <span id="dx1-98005"/>DQN from the preceding papers has the following steps:</p>
<ol>
<li>
<div id="x1-98007x1">
<p>Initialize parameters for <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) and QÃÇ(<span class="cmmi-10x-x-109">s,a</span>) with random weights, <span class="cmmi-10x-x-109">ùúñ </span><span class="cmsy-10x-x-109">‚Üê </span>1<span class="cmmi-10x-x-109">.</span>0, and empty the replay buffer.</p>
</div>
</li>
<li>
<div id="x1-98009x2">
<p>With probability <span class="cmmi-10x-x-109">ùúñ</span>, select a random action <span class="cmmi-10x-x-109">a</span>; otherwise, <span class="cmmi-10x-x-109">a </span>= arg max<sub><span class="cmmi-8">a</span></sub><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>).</p>
</div>
</li>
<li>
<div id="x1-98011x3">
<p>Execute action <span class="cmmi-10x-x-109">a </span>in an emulator and observe the reward, <span class="cmmi-10x-x-109">r</span>, and the next state, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span>.</p>
</div>
</li>
<li>
<div id="x1-98013x4">
<p>Store the transition (<span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span>) in the replay buffer.</p>
</div>
</li>
<li>
<div id="x1-98015x5">
<p>Sample a random mini-batch of transitions from the replay buffer.</p>
</div>
</li>
<li>
<div id="x1-98017x6">
<p>For every transition in the buffer, calculate the target:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="21" src="../Images/eq22.png" width="297"/>
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="43" src="../Images/eq23.png" width="348"/>
</div>
</div>
</li>
<li>
<div id="x1-98019x7">
<p>Calculate the loss: <span class="cmsy-10x-x-109">‚Ñí </span>= (<span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">y</span>)<sup><span class="cmr-8">2</span></sup>.</p>
</div>
</li>
<li>
<div id="x1-98021x8">
<p>Update <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) using the SGD algorithm <span id="dx1-98022"/>by minimizing the loss in respect to the model parameters.</p>
</div>
</li>
<li>
<div id="x1-98024x9">
<p>Every <span class="cmmi-10x-x-109">N </span>steps, copy weights from <span class="cmmi-10x-x-109">Q </span>to QÃÇ.</p>
</div>
</li>
<li>
<div id="x1-98026x10">
<p>Repeat from step 2 until converged.</p>
</div>
</li>
</ol>
<p>Let‚Äôs implement this <span id="dx1-98027"/>algorithm now and try to beat some of the Atari games!</p>
</section>
</section>
<section class="level3 sectionHead" id="dqn-on-pong">
<h1 class="heading-1" id="sigil_toc_id_88"> <span id="x1-990006.4"/>DQN on Pong</h1>
<p>Before we jump into the code, some introduction is needed. Our examples are <span id="dx1-99001"/>becoming increasingly challenging and complex, which is not surprising, as the complexity of the problems that we are trying to tackle is also growing. The examples are as simple and concise as possible, but some of the code may be difficult to understand at first.</p>
<p>Another thing to note is performance. Our previous examples for FrozenLake, or CartPole, were not demanding from a resource perspective, as observations were small, NN parameters were tiny, and shaving off extra milliseconds in the training loop wasn‚Äôt important. However, from now on, that‚Äôs not the case. One single observation from the Atari environment is 100k values, which have to be preprocessed, rescaled, and stored in the replay buffer. One extra copy of this data array can cost you training speed, which will not be <span id="dx1-99002"/>seconds and minutes anymore but, instead, hours on even the fastest <span class="cmbx-10x-x-109">graphics processing unit</span> (<span class="cmbx-10x-x-109">GPU</span>) available.</p>
<p>The NN training <span id="dx1-99003"/>loop could also be a bottleneck. Of course, RL models are not as huge monsters as state-of-the-art <span class="cmbx-10x-x-109">large language models </span>(<span class="cmbx-10x-x-109">LLMs</span>), but even the DQN model from 2015 has more than 1.5M parameters, which has to be adjusted millions of times. So, to cut a long story short, performance matters, especially when you are experimenting with hyperparameters and need to wait not for a single model to train but dozens of them.</p>
<p>PyTorch is quite <span id="dx1-99004"/>expressive, so more-or-less efficient processing code could look much less cryptic than optimized TensorFlow graphs, but there is still a significant opportunity to do things slowly and make mistakes. For example, a na√Øve version of DQN loss computation, which loops over every batch sample, is about two times slower than a parallel version. However, a single extra copy of the data batch could make the speed of the same code 13 times slower, which is quite significant.</p>
<p>This example has been split into three modules due to its length, logical structure, and reusability. The modules are as follows:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">Chapter06/lib/wrappers.py</span>: These<span id="dx1-99005"/> are Atari environment wrappers, mostly taken from the <span class="cmbx-10x-x-109">Stable Baselines3 </span>(<span class="cmbx-10x-x-109">SB3</span>) project: <a class="url" href="https://github.com/DLR-RM/stable-baselines3"><span class="cmtt-10x-x-109">https://github.com/DLR-RM/stable-baselines3</span></a>.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Chapter06/lib/dqn</span><span class="cmtt-10x-x-109">_model.py</span>: This is the DQN NN layer, with the same architecture as the DeepMind DQN from the <span class="cmti-10x-x-109">Nature </span>paper.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Chapter06/02</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_pong.py</span>: This is the main module, with the training loop, loss function calculation, and experience replay buffer.</p>
</li>
</ul>
<section class="level4 subsectionHead" id="wrappers-1">
<h2 class="heading-2" id="sigil_toc_id_89"> <span id="x1-1000006.4.1"/>Wrappers</h2>
<p>Tackling Atari <span id="dx1-100001"/>games with RL is quite demanding from a resource perspective. To make things faster, several transformations are applied to the Atari platform interaction, which are described in DeepMind‚Äôs paper. Some of these transformations influence only performance, but some address Atari platform features that make learning long and unstable. Transformations are implemented as Gym wrappers of various kinds. The full list is quite lengthy and there are several implementations of the same wrappers in various sources. My personal favorite is the SB3 repository, which is an evolution of OpenAI Baselines code.</p>
<p>SB3 includes lots of RL methods implemented using PyTorch and is supposed to be a unifying benchmark to compare various methods. At the moment, we‚Äôre not interested in those methods‚Äô implementation (we‚Äôre going to reimplement most of them ourselves), but some wrappers are very useful. The repository is avaliable at <a class="url" href="https://github.com/DLR-RM/stable-baselines3"><span class="cmtt-10x-x-109">https://github.com/DLR-RM/stable-baselines3</span></a> and wrappers are documented at <a class="url" href="https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml"><span class="cmtt-10x-x-109">https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml</span></a>. The list of the most popular Atari transformations used by RL researchers includes:</p>
<ul>
<li>
<p><span class="cmti-10x-x-109">Converting individual lives in the game into separate episodes</span>: In general, an episode contains all the steps from the beginning of the game until the ‚ÄúGame over‚Äù screen, which can last for thousands of game steps (observations and actions). Usually, in arcade games, the player is given several lives, which provide several attempts in the game. This transformation splits a full episode into individual small episodes for every life that a player has. Internally, this is implemented as checking an emulator‚Äôs information about remaining lives. Not all games support this feature (although Pong does), but for the supported environments, it usually helps to speed up convergence, as our episodes become shorter. This logic is implemented in the <span class="cmtt-10x-x-109">EpisodicLifeEnv </span>wrapper in SB3 code.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">At the beginning of the game, performing a random amount (up to</span> <span class="cmti-10x-x-109">30) of empty actions (also called ‚Äúno-op‚Äù)</span>: This skips intro screens in some Atari games, which are not relevant for the gameplay. It is implemented in the <span class="cmtt-10x-x-109">NoopResetEnv </span>wrapper.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Making an action decision every K steps, where K is usually 3 or 4</span>: On intermediate frames, the chosen action is simply repeated. This allows training to speed up significantly, as processing every <span id="dx1-100002"/>frame with an NN is quite a demanding operation, but the difference between consequent frames is usually minor. This is implemented in the <span class="cmtt-10x-x-109">MaxAndSkipEnv </span>wrapper, which also includes the next transformation in the list (the maximum between two frames).</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Taking the maximum of every pixel in the last two frames and using it</span> <span class="cmti-10x-x-109">as an observation</span>: Some Atari games have a flickering effect, which is due to the platform‚Äôs limitation. (Atari has a limited number of sprites that can be shown on a single frame.) For the human eye, such quick changes are not visible, but they can confuse NNs.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Pressing </span><span class="cmbxti-10x-x-109">FIRE </span><span class="cmti-10x-x-109">at the beginning of the game</span>: Some games (including Pong and Breakout) require a user to press the <span class="cmbx-10x-x-109">FIRE </span>button to start the game. Without this, the environment becomes a POMDP, as from observation, an agent cannot tell whether <span class="cmbx-10x-x-109">FIRE </span>was already pressed. This is implemented in the <span class="cmtt-10x-x-109">FireResetEnv </span>wrapper class.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Scaling every frame down from </span>210 <span class="cmsy-10x-x-109">√ó </span>160<span class="cmti-10x-x-109">, with three color frames, to</span> <span class="cmti-10x-x-109">a single-color </span>84 <span class="cmsy-10x-x-109">√ó </span>84 <span class="cmti-10x-x-109">image</span>: Different approaches are possible. For example, the DeepMind paper describes this transformation as taking the Y-color channel from the YCbCr color space and then rescaling the full image to an 84 <span class="cmsy-10x-x-109">√ó </span>84 resolution. Some other researchers do grayscale transformation, cropping non-relevant parts of an image and then scaling down. In the SB3 repository, the latter approach is used. This is implemented in the <span class="cmtt-10x-x-109">WarpFrame </span>wrapper class.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Stacking several (usually four) subsequent frames together to give the</span> <span class="cmti-10x-x-109">network information about the dynamics of the game‚Äôs objects</span>: This approach was already discussed as a quick solution to the lack of game dynamics in a single game frame. There is no wrapper in the SB3 project, I implemented my version in <span class="cmtt-10x-x-109">wrappers.BufferWrapper</span>.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Clipping the reward to -1, 0, and 1 values</span>: The obtained score can vary wildly among the games. For example, in Pong, you get a score of 1 for every ball you pass behind the opponent‚Äôs paddle. However, in some games, like KungFuMaster, you get a reward of 100 for every enemy killed. This spread in reward values makes our loss have completely different scales between the games, which makes it harder to find common hyperparameters for a set of games. To fix this, the reward just gets clipped to the range <span class="cmsy-10x-x-109">‚àí</span>1<span class="cmmi-10x-x-109">‚Ä¶</span>1. This is implemented in the <span class="cmtt-10x-x-109">ClipRewardEnv </span>wrapper.</p>
</li>
<li>
<p><span class="cmti-10x-x-109">Rearrange observation dimensions to meet the PyTorch convolution</span> <span class="cmti-10x-x-109">layer</span>: As we‚Äôre going to use convolution, our tensors have to be rearranged the way PyTorch expects them. The Atari environment returns the observation in a (height, width, color) format, but the PyTorch convolution layer wants the channel layer to come first. This is implemented in <span class="cmtt-10x-x-109">wrappers.ImageToPyTorch</span>.</p>
</li>
</ul>
<p>Most of those <span id="dx1-100003"/>wrappers are implemented in the <span class="cmtt-10x-x-109">stable-baseline3 </span>library, which provides the <span class="cmtt-10x-x-109">AtariWrapper </span>class that applies wrappers in the required order, according to the constructor‚Äôs parameters. It also detects the underlying environment properties and enables <span class="cmtt-10x-x-109">FireResetEnv </span>if needed. Not all of the wrappers are required for the Pong game, but you should be aware of existing wrappers, just in case you decide to experiment with other games. Sometimes, when the DQN does not converge, the problem is not in the code but in the wrongly wrapped environment. I once spent several days debugging convergence issues, which were caused by missing the <span class="cmbx-10x-x-109">FIRE </span>button press at the beginning of a game!</p>
<p>Let‚Äôs take a look at the implementation of individual wrappers. We will start with classes provided by <span class="cmtt-10x-x-109">stable-baseline3</span>:</p>
<div class="tcolorbox" id="tcolobox-94">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-118"><code>class FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): 
    def __init__(self, env: gym.Env) -&gt; None: 
        super().__init__(env) 
        assert env.unwrapped.get_action_meanings()[1] == "FIRE" 
        assert len(env.unwrapped.get_action_meanings()) &gt;= 3 
 
    def reset(self, **kwargs) -&gt; AtariResetReturn: 
        self.env.reset(**kwargs) 
        obs, _, terminated, truncated, _ = self.env.step(1) 
        if terminated or truncated: 
            self.env.reset(**kwargs) 
        obs, _, terminated, truncated, _ = self.env.step(2) 
        if terminated or truncated: 
            self.env.reset(**kwargs) 
        return obs, {}</code></pre>
</div>
</div>
<p>The preceding wrapper presses the <span class="cmbx-10x-x-109">FIRE </span>button in environments that require that for the game to start. In addition to pressing <span class="cmbx-10x-x-109">FIRE</span>, this wrapper checks for several corner cases that are present in some games.</p>
<p>This wrapper combines the repetition of actions during <span class="cmmi-10x-x-109">K </span>frames and pixels from two consecutive frames:</p>
<div class="tcolorbox" id="tcolobox-95">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-119"><code>class MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): 
    def __init__(self, env: gym.Env, skip: int = 4) -&gt; None: 
        super().__init__(env) 
        self._obs_buffer = np.zeros((2, *env.observation_space.shape), 
            dtype=env.observation_space.dtype) 
        self._skip = skip 
 
    def step(self, action: int) -&gt; AtariStepReturn: 
        total_reward = 0.0 
        terminated = truncated = False 
        for i in range(self._skip): 
            obs, reward, terminated, truncated, info = self.env.step(action) 
            done = terminated or truncated 
            if i == self._skip - 2: 
                self._obs_buffer[0] = obs 
            if i == self._skip - 1: 
                self._obs_buffer[1] = obs 
            total_reward += float(reward) 
            if done: 
                break 
        # Note that the observation on the done=True frame 
        # doesn‚Äôt matter 
        max_frame = self._obs_buffer.max(axis=0) 
 
        return max_frame, total_reward, terminated, truncated, info</code></pre>
</div>
</div>
<p>The goal of the following <span id="dx1-100044"/>wrapper is to convert input observations from the emulator, which has a resolution of 210 <span class="cmsy-10x-x-109">√ó </span>160 pixels with RGB color channels, to a grayscale 84 <span class="cmsy-10x-x-109">√ó </span>84 image. It does this using CV2 library‚Äôs function <span class="cmtt-10x-x-109">cvtColor</span>, which does a colorimetric grayscale conversion (which is closer to human color perception than a simple averaging of color channels), and then the image is resized:</p>
<div class="tcolorbox" id="tcolobox-96">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-120"><code>class WarpFrame(gym.ObservationWrapper[np.ndarray, int, np.ndarray]): 
    def __init__(self, env: gym.Env, width: int = 84, height: int = 84) -&gt; None: 
        super().__init__(env) 
        self.width = width 
        self.height = height 
        self.observation_space = spaces.Box( 
            low=0, high=255, shape=(self.height, self.width, 1), 
            dtype=env.observation_space.dtype, 
        ) 
 
    def observation(self, frame: np.ndarray) -&gt; np.ndarray: 
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) 
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA) 
        return frame[:, :, None]</code></pre>
</div>
</div>
<p>So far, we‚Äôve used <span id="dx1-100059"/>wrappers from <span class="cmtt-10x-x-109">stable-baseline3 </span>(I skipped <span class="cmtt-10x-x-109">EpisodicLifeEnv</span> wrapper, as it is a bit complicated and not very relevant); you can find the code of other available wrappers in the repo <span class="cmtt-10x-x-109">stable</span><span class="cmtt-10x-x-109">_baselines3/common/atari</span><span class="cmtt-10x-x-109">_wrappers.py</span>. Now, let‚Äôs check out two wrappers from <span class="cmtt-10x-x-109">lib/wrappers.py</span>:</p>
<div class="tcolorbox" id="tcolobox-97">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-121"><code>class BufferWrapper(gym.ObservationWrapper): 
    def __init__(self, env, n_steps): 
        super(BufferWrapper, self).__init__(env) 
        obs = env.observation_space 
        assert isinstance(obs, spaces.Box) 
        new_obs = gym.spaces.Box( 
            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0), 
            dtype=obs.dtype) 
        self.observation_space = new_obs 
        self.buffer = collections.deque(maxlen=n_steps) 
 
    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None): 
        for _ in range(self.buffer.maxlen-1): 
            self.buffer.append(self.env.observation_space.low) 
        obs, extra = self.env.reset() 
        return self.observation(obs), extra 
 
    def observation(self, observation: np.ndarray) -&gt; np.ndarray: 
        self.buffer.append(observation) 
        return np.concatenate(self.buffer)</code></pre>
</div>
</div>
<p>The <span class="cmtt-10x-x-109">BufferWrapper </span>class creates a stack (implemented with the <span class="cmtt-10x-x-109">deque </span>class) of subsequent frames along the first dimension and returns them as an observation. The purpose is to give the network an idea about the dynamics of the objects, such as the speed and direction of the ball in Pong or how enemies are moving. This is very important information, which it is not possible to obtain from a single image.</p>
<p>One very important but not very obvious detail about this wrapper is that the <span class="cmtt-10x-x-109">observation </span>method returns the <span class="cmbx-10x-x-109">copy </span>of our buffered observations. This is very important, as we‚Äôre going to keep our observations in the replay buffer, so the copy is needed to avoid buffer modification on the future environment‚Äôs steps. In principle, we can avoid making<span id="dx1-100080"/> a copy (and reduce our memory footprint four times) by keeping the episodes‚Äô observations and indices in them, but it will require much more sophisticated data structure management. What is important currently is that this wrapper has to be the last in the chain of the wrappers applied to the environment.</p>
<p>The last <span id="dx1-100081"/>wrapper is <span class="cmtt-10x-x-109">ImageToPyTorch</span>, and it <span id="dx1-100082"/>changes the shape of the observation from <span class="cmbx-10x-x-109">height, width, channel </span>(<span class="cmbx-10x-x-109">HWC</span>) to the <span class="cmbx-10x-x-109">channel, height, width</span> (<span class="cmbx-10x-x-109">CHW</span>) format required by PyTorch:</p>
<div class="tcolorbox" id="tcolobox-98">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-122"><code>class ImageToPyTorch(gym.ObservationWrapper): 
    def __init__(self, env): 
        super(ImageToPyTorch, self).__init__(env) 
        obs = self.observation_space 
        assert isinstance(obs, gym.spaces.Box) 
        assert len(obs.shape) == 3 
        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1]) 
        self.observation_space = gym.spaces.Box( 
            low=obs.low.min(), high=obs.high.max(), 
            shape=new_shape, dtype=obs.dtype) 
 
    def observation(self, observation): 
        return np.moveaxis(observation, 2, 0)</code></pre>
</div>
</div>
<p>The input shape of the tensor has a color channel as the last dimension, but PyTorch‚Äôs convolution layers assume the color channel to be the first dimension.</p>
<p>At the end of the file is a simple function that creates an environment with a name and applies all the required wrappers to it:</p>
<div class="tcolorbox" id="tcolobox-99">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-123"><code>def make_env(env_name: str, **kwargs): 
    env = gym.make(env_name, **kwargs) 
    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0) 
    env = ImageToPyTorch(env) 
    env = BufferWrapper(env, n_steps=4) 
    return env</code></pre>
</div>
</div>
<p>As you can see, we‚Äôre using the <span class="cmtt-10x-x-109">AtariWrapper </span>class from <span class="cmtt-10x-x-109">stable-baseline3 </span>and disabling some unnecessary wrappers.</p>
<p>That‚Äôs it for wrappers; let‚Äôs look at our model.</p>
</section>
<section class="level4 subsectionHead" id="the-dqn-model">
<h2 class="heading-2" id="sigil_toc_id_90"> <span id="x1-1010006.4.2"/>The DQN model</h2>
<p>The <span id="dx1-101001"/>model <span id="dx1-101002"/>published in <span class="cmti-10x-x-109">Nature </span>has three convolution layers followed by two fully connected layers. All layers are separated by <span class="cmbx-10x-x-109">rectified linear unit </span>(<span class="cmbx-10x-x-109">ReLU</span>) nonlinearities. The output of the model is Q-values for every action available in the environment, without nonlinearity applied (as Q-values can have any value). The approach of having all Q-values calculated with one pass through the network helps us to increase speed significantly in comparison to treating <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) literally, feeding observations and actions to the network to obtain the value of the action.</p>
<p>The code of the model is in <span class="cmtt-10x-x-109">Chapter06/lib/dqn</span><span class="cmtt-10x-x-109">_model.py</span>:</p>
<div class="tcolorbox" id="tcolobox-100">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-124"><code>import torch 
import torch.nn as nn 
 
class DQN(nn.Module): 
    def __init__(self, input_shape, n_actions): 
        super(DQN, self).__init__() 
 
        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), 
            nn.ReLU(), 
            nn.Conv2d(32, 64, kernel_size=4, stride=2), 
            nn.ReLU(), 
            nn.Conv2d(64, 64, kernel_size=3, stride=1), 
            nn.ReLU(), 
            nn.Flatten(), 
        ) 
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.fc = nn.Sequential( 
            nn.Linear(size, 512), 
            nn.ReLU(), 
            nn.Linear(512, n_actions) 
        )</code></pre>
</div>
</div>
<p>To be able to write our network in a generic way, it was implemented in two parts: convolution and linear. The convolution part processes the imput image, which is a 4 <span class="cmsy-10x-x-109">√ó </span>84 <span class="cmsy-10x-x-109">√ó </span>84 tensor. The output from the last convolution filter is flattened into a one-dimensional vector and fed into two <span class="cmtt-10x-x-109">Linear</span> layers.</p>
<p>Another small problem is that we don‚Äôt know the exact number of values in the output from the convolution layer produced with the input of the given shape. However, we need to pass this number to the first fully connected layer constructor. One possible solution would be to hard-code this number, which is a function of the input shape and the last convolution layer configuration (for 84 <span class="cmsy-10x-x-109">√ó </span>84 input, the output from the convolution layer will have 3,136 values); however, it‚Äôs not the best way, as our code will become less robust to input shape change. The better solution is to obtain the required dimension in runtime, by applying the convolution part to a fake input tensor. The dimension of the result would be equal to the number of parameters returned by this application. It would be fast, as this call would be done once on model creation, and it would also allow us to have generic code.</p>
<p>The final piece of <span id="dx1-101025"/>the model is the <span class="cmtt-10x-x-109">forward() </span>function, which accepts the 4D input tensor. The first dimension is the batch size and the second is the color channel, which is our stack of subsequent frames; the third and fourth are image dimensions:</p>
<div class="tcolorbox" id="tcolobox-101">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-125"><code>    def forward(self, x: torch.ByteTensor): 
        # scale on GPU 
        xx = x / 255.0 
        return self.fc(self.conv(xx))</code></pre>
</div>
</div>
<p>Here, before applying our network, we perform the scaling and type conversion of the input data. This requires a bit of explanation.</p>
<p>Every pixel in the Atari image is represented as an unsigned byte with a value from 0 to 255. This is convenient in two aspects: memory efficiency and GPU bandwidth. From the memory standpoint, we should keep environment observations as small as possible because our replay buffer will keep thousands of observations, and we want to keep it small. On the other hand, during the training, we need to transfer those observations into GPU memory to calculate the gradients and update the network parameters. The bandwidth between the main memory and GPU is a limited resource, so it also makes sense to keep observations as small as possible.</p>
<p>That‚Äôs why we keep observations as a numpy array with <span class="cmtt-10x-x-109">dtype=uint8</span>, and the input tensor to the network is <span class="cmtt-10x-x-109">ByteTensor</span>. But the <span class="cmtt-10x-x-109">Conv2D </span>layer expects the float tensor as an input, so by dividing the input tensor by 255<span class="cmmi-10x-x-109">.</span>0, we scale to the 0<span class="cmmi-10x-x-109">‚Ä¶</span>1 range and do type conversion. This is fast, as the input byte tensor is already inside the GPU memory. After that, we apply both parts of our network to the resulting scaled tensor.</p>
</section>
<section class="level4 subsectionHead" id="training">
<h2 class="heading-2" id="sigil_toc_id_91"> <span id="x1-1020006.4.3"/>Training</h2>
<p>The third module <span id="dx1-102001"/>contains the experience replay buffer, the agent, the loss function calculation, and the training loop itself. Before going into the code, something needs to be said about the training hyperparameters.</p>
<p>DeepMind‚Äôs <span class="cmti-10x-x-109">Nature </span>paper contained a table with all the details about the hyperparameters used to train its model on <span class="cmti-10x-x-109">all </span>49 Atari games used for evaluation. DeepMind kept all those parameters the same for all games (but trained individual models for every game), and it was the team‚Äôs intention to show that the method is robust enough to solve lots of games with varying complexity, action space, reward structure, and other details using one single model architecture and hyperparameters. However, our goal here is much more modest: we want to solve just the Pong game.</p>
<p>Pong is quite simple and straightforward in comparison to other games in the Atari test set, so the hyperparameters in the paper are overkill for our task. For example, to get the best result on all 49 games, DeepMind used a million-observations replay buffer, which requires approximately 20 GB of RAM to store it and lots of samples from the environment to populate it.</p>
<p>The <span id="dx1-102002"/>epsilon decay schedule that was used is also not the best for a single Pong game. In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1 during the first million frames obtained from the environment. However, my own experiments have shown that for Pong, it‚Äôs enough to decay epsilon over the first 150k frames and then keep it stable. The replay buffer also can be much smaller: 10k transitions will be enough.</p>
<p>In the following example, I‚Äôve used my parameters. These differ from the parameters in the paper but will allow us to solve Pong about 10 times faster. On a GeForce GTX 1080 Ti, the following version converges to a mean score of 19.0 in about 50 minutes, but with DeepMind‚Äôs hyperparameters, it will require at least a day.</p>
<p>This speedup, of course, involves fine-tuning for one particular environment and can break convergence on other games. You are free to play with the options and other games from the Atari set.</p>
<p>First, we import the required modules:</p>
<div class="tcolorbox" id="tcolobox-102">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-126"><code>import gymnasium as gym 
from lib import dqn_model 
from lib import wrappers 
 
from dataclasses import dataclass 
import argparse 
import time 
import numpy as np 
import collections 
import typing as tt 
 
import torch 
import torch.nn as nn 
import torch.optim as optim 
 
from torch.utils.tensorboard.writer import SummaryWriter</code></pre>
</div>
</div>
<p>We then define the hyperparameters:</p>
<div class="tcolorbox" id="tcolobox-103">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-127"><code>DEFAULT_ENV_NAME = "PongNoFrameskip-v4" 
MEAN_REWARD_BOUND = 19</code></pre>
</div>
</div>
<p>These two values set the <span id="dx1-102021"/>default environment to train on and the reward boundary for the last 100 episodes to stop training. If you want, you can redefine the environment name using the command-line <span class="cmtt-10x-x-109">--env </span>argument:</p>
<div class="tcolorbox" id="tcolobox-104">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-128"><code>GAMMA = 0.99 
BATCH_SIZE = 32 
REPLAY_SIZE = 10000 
LEARNING_RATE = 1e-4 
SYNC_TARGET_FRAMES = 1000 
REPLAY_START_SIZE = 10000</code></pre>
</div>
</div>
<p>The preceding parameters define the following:</p>
<ul>
<li>
<p>Our <span class="cmmi-10x-x-109">Œ≥ </span>value used for the Bellman approximation (<span class="cmtt-10x-x-109">GAMMA</span>)</p>
</li>
<li>
<p>The batch size sampled from the replay buffer (<span class="cmtt-10x-x-109">BATCH</span><span class="cmtt-10x-x-109">_SIZE</span>)</p>
</li>
<li>
<p>The maximum capacity of the buffer (<span class="cmtt-10x-x-109">REPLAY</span><span class="cmtt-10x-x-109">_SIZE</span>)</p>
</li>
<li>
<p>The count of frames we wait for before starting training to populate the replay buffer (<span class="cmtt-10x-x-109">REPLAY</span><span class="cmtt-10x-x-109">_START</span><span class="cmtt-10x-x-109">_SIZE</span>)</p>
</li>
<li>
<p>The learning rate used in the Adam optimizer, which is used in this example (<span class="cmtt-10x-x-109">LEARNING</span><span class="cmtt-10x-x-109">_RATE</span>)</p>
</li>
<li>
<p>How frequently we sync model weights from the training model to the target model, which is used to get the value of the next state in the Bellman approximation (<span class="cmtt-10x-x-109">SYNC</span><span class="cmtt-10x-x-109">_TARGET</span><span class="cmtt-10x-x-109">_FRAMES</span>)</p>
</li>
</ul>
<div class="tcolorbox" id="tcolobox-105">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-129"><code>EPSILON_DECAY_LAST_FRAME = 150000 
EPSILON_START = 1.0 
EPSILON_FINAL = 0.01</code></pre>
</div>
</div>
<p>The last batch of hyperparameters is related to the epsilon decay schedule. To achieve proper exploration, we start with <span class="cmmi-10x-x-109">ùúñ </span>= 1<span class="cmmi-10x-x-109">.</span>0 at the early stages of training, which causes all actions to be selected randomly. Then, during the first 150,000 frames, <span class="cmmi-10x-x-109">ùúñ </span>is linearly decayed to 0.01, which corresponds to the random action taken in 1% of steps. A similar scheme was used in the original DeepMind paper, but the duration of decay was almost 10 times longer (so <span class="cmmi-10x-x-109">ùúñ </span>= 0<span class="cmmi-10x-x-109">.</span>01 was reached after a million frames).</p>
<p>Here, we define our type aliases and the dataclass <span class="cmtt-10x-x-109">Experience</span>, used to keep entries in the experience replay buffer. It contains the current state, the action taken, the reward obtained, the termination or truncation flag, and the new state:</p>
<div class="tcolorbox" id="tcolobox-106">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-130"><code>State = np.ndarray 
Action = int 
BatchTensors = tt.Tuple[ 
    torch.ByteTensor,           # current state 
    torch.LongTensor,           # actions 
    torch.Tensor,               # rewards 
    torch.BoolTensor,           # done || trunc 
    torch.ByteTensor            # next state 
] 
 
@dataclass 
class Experience: 
    state: State 
    action: Action 
    reward: float 
    done_trunc: bool 
    new_state: State</code></pre>
</div>
</div>
<p>The next chunk of code defines our experience replay buffer, the purpose of which is to keep the transitions obtained from the environment:</p>
<div class="tcolorbox" id="tcolobox-107">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-131"><code>class ExperienceBuffer: 
    def __init__(self, capacity: int): 
        self.buffer = collections.deque(maxlen=capacity) 
 
    def __len__(self): 
        return len(self.buffer) 
 
    def append(self, experience: Experience): 
        self.buffer.append(experience) 
 
    def sample(self, batch_size: int) -&gt; tt.List[Experience]: 
        indices = np.random.choice(len(self), batch_size, replace=False) 
        return [self.buffer[idx] for idx in indices]</code></pre>
</div>
</div>
<p>Each time we do a step in <span id="dx1-102061"/>the environment, we push the transition into the buffer, keeping only a fixed number of steps (in our case, 10k transitions). For training, we randomly sample the batch of transitions from the replay buffer, which allows us to break the correlation between subsequent steps in the environment.</p>
<p>Most of the experience replay buffer code is quite straightforward: it basically exploits the capability of the <span class="cmtt-10x-x-109">deque </span>class to maintain the given number of entries in the buffer. In the <span class="cmtt-10x-x-109">sample() </span>method, we create a list of random indices and return a list of <span class="cmtt-10x-x-109">Experience </span>items to be repackaged and converted into tensors.</p>
<p>The next class we need to have is an <span class="cmtt-10x-x-109">Agent</span>, which interacts with the environment and saves the result of the interaction in the experience replay buffer that you have just seen:</p>
<div class="tcolorbox" id="tcolobox-108">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-132"><code>class Agent: 
    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer): 
        self.env = env 
        self.exp_buffer = exp_buffer 
        self.state: tt.Optional[np.ndarray] = None 
        self._reset() 
 
    def _reset(self): 
        self.state, _ = env.reset() 
        self.total_reward = 0.0</code></pre>
</div>
</div>
<p>During the agent‚Äôs <span id="dx1-102072"/>initialization, we need to store references to the environment and experience replay buffer, tracking the current observation and the total reward accumulated so far.</p>
<p>The main method of the agent is to perform a step in the environment and store its result in the buffer. To do this, we need to select the action first:</p>
<div class="tcolorbox" id="tcolobox-109">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-133"><code>    @torch.no_grad() 
    def play_step(self, net: dqn_model.DQN, device: torch.device, 
                  epsilon: float = 0.0) -&gt; tt.Optional[float]: 
        done_reward = None 
 
        if np.random.random() &lt; epsilon: 
            action = env.action_space.sample() 
        else: 
            state_v = torch.as_tensor(self.state).to(device) 
            state_v.unsqueeze_(0) 
            q_vals_v = net(state_v) 
            _, act_v = torch.max(q_vals_v, dim=1) 
            action = int(act_v.item())</code></pre>
</div>
</div>
<p>With the probability epsilon (passed as an argument), we take the random action; otherwise, we use the model to obtain the Q-values for all possible actions and choose the best. In this method, we use the PyTorch <span class="cmtt-10x-x-109">no</span><span class="cmtt-10x-x-109">_grad() </span>decorator to disable gradient tracking during the whole method, as we don‚Äôt need them anyway.</p>
<p>As the action has been chosen, we pass it to the environment to get the next observation and reward, store the data in the experience buffer, and then handle the end-of-episode situation:</p>
<div class="tcolorbox" id="tcolobox-110">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-134"><code>        new_state, reward, is_done, is_tr, _ = self.env.step(action) 
        self.total_reward += reward 
 
        exp = Experience( 
            state=self.state, action=action, reward=float(reward), 
            done_trunc=is_done or is_tr, new_state=new_state 
        ) 
        self.exp_buffer.append(exp) 
        self.state = new_state 
        if is_done or is_tr: 
            done_reward = self.total_reward 
            self._reset() 
        return done_reward</code></pre>
</div>
</div>
<p>The result of the function is the <span id="dx1-102099"/>total accumulated reward if we have reached the end of the episode with this step, or <span class="cmtt-10x-x-109">None </span>otherwise.</p>
<p>The function <span class="cmtt-10x-x-109">batch</span><span class="cmtt-10x-x-109">_to</span><span class="cmtt-10x-x-109">_tensors </span>takes the batch of <span class="cmtt-10x-x-109">Experience </span>objects and returns a tuple with states, actions, rewards, done flags, and new states repacked as PyTorch tensors of the corresponding types:</p>
<div class="tcolorbox" id="tcolobox-111">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-135"><code>def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -&gt; BatchTensors: 
    states, actions, rewards, dones, new_state = [], [], [], [], [] 
    for e in batch: 
        states.append(e.state) 
        actions.append(e.action) 
        rewards.append(e.reward) 
        dones.append(e.done_trunc) 
        new_state.append(e.new_state) 
    states_t = torch.as_tensor(np.asarray(states)) 
    actions_t = torch.LongTensor(actions) 
    rewards_t = torch.FloatTensor(rewards) 
    dones_t = torch.BoolTensor(dones) 
    new_states_t = torch.as_tensor(np.asarray(new_state)) 
    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \ 
           dones_t.to(device),  new_states_t.to(device)</code></pre>
</div>
</div>
<p>When we work with states, we try to avoid memory copy (by using <span class="cmtt-10x-x-109">np.asarray() </span>function), which is important, as Atari observations are large (4 frames with 84 <span class="cmsy-10x-x-109">√ó </span>84 bytes each), and we have a batch of 32 such objects. Without this optimization, performance drops about 20 times.</p>
<p>Now, it is time for the last function in the training module, which calculates the loss for the sampled batch. This function is written in a form to maximally exploit GPU parallelism by processing all batch samples with vector operations, which makes it harder to understand when compared with a na√Øve loop over the batch. Yet this optimization pays off: the parallel version is more than two times faster than an explicit loop.</p>
<p>As a reminder, here is the loss expression we need to calculate:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="43" src="../Images/eq24.png" width="372"/>
</div>
<p>We use the <span id="dx1-102115"/>preceding equation for steps that aren‚Äôt at the end of the episode and the following for the final steps:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="26" src="../Images/eq25.png" width="182"/>
</div>
<div class="tcolorbox" id="tcolobox-112">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-136"><code>def calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, 
              device: torch.device) -&gt; torch.Tensor: 
    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)</code></pre>
</div>
</div>
<p>In the arguments, we pass our batch, the network that we are training, and the target network, which is periodically synced with the trained one.</p>
<p>The first model (passed as the parameter <span class="cmtt-10x-x-109">net</span>) is used to calculate gradients; the second model in the <span class="cmtt-10x-x-109">tgt</span><span class="cmtt-10x-x-109">_net </span>argument is used to calculate values for the next states, and this calculation shouldn‚Äôt affect gradients. To achieve this, we use the <span class="cmtt-10x-x-109">detach() </span>function of the PyTorch tensor to prevent gradients from flowing into the target network‚Äôs graph. This function was described in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch007.xhtml#x1-530003"><span class="cmti-10x-x-109">3</span></a>.</p>
<p>At the beginning of the function, we call the function <span class="cmtt-10x-x-109">batch</span><span class="cmtt-10x-x-109">_to</span><span class="cmtt-10x-x-109">_tensors </span>to repack the batch into individual tensor variables.</p>
<p>The next line is a bit tricky:</p>
<div class="tcolorbox" id="tcolobox-113">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-137"><code>    state_action_values = net(states_t).gather( 
        1, actions_t.unsqueeze(-1) 
    ).squeeze(-1)</code></pre>
</div>
</div>
<p>Let‚Äôs discuss it in detail. Here, we pass observations to the first model and extract the specific Q-values for the taken actions, using the <span class="cmtt-10x-x-109">gather() </span>tensor operation. The first argument to the <span class="cmtt-10x-x-109">gather() </span>call is a dimension index that we want to perform gathering on (in our case, it is equal to 1, which corresponds to actions).</p>
<p>The second argument is a tensor of indices of elements to be chosen. Extra <span class="cmtt-10x-x-109">unsqueeze() </span>and <span class="cmtt-10x-x-109">squeeze() </span>calls are required to compute the index argument for the <span class="cmtt-10x-x-109">gather() </span>function and to get rid of the extra dimensions that we created, respectively. (The index should have the same number of dimensions as the data we are processing.) In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-102122r3"><span class="cmti-10x-x-109">6.3</span></a>, you can see an illustration of what <span class="cmtt-10x-x-109">gather() </span>does on the example case, with a batch of six entries and four actions.</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/file33.png" width="251"/> <span id="x1-102122r3"/></p>
<span class="id">Figure¬†6.3: Transformation of tensors during a DQN loss calculation </span>
</div>
<p>Keep in mind <span id="dx1-102123"/>that the result of <span class="cmtt-10x-x-109">gather() </span>applied to tensors is a differentiable operation that will keep all gradients with respect to the final loss value.</p>
<p>Next, we disable the gradients‚Äô calculations (which ends up in a small speedup), apply the target network to our next state observations, and calculate the maximum Q-value along the same action dimension, 1:</p>
<div class="tcolorbox" id="tcolobox-114">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-138"><code>    with torch.no_grad(): 
        next_state_values = tgt_net(new_states_t).max(1)[0]</code></pre>
</div>
</div>
<p>The function <span class="cmtt-10x-x-109">max() </span>returns both maximum values and indices of those values (so it calculates both <span class="cmtt-10x-x-109">max </span>and <span class="cmtt-10x-x-109">argmax</span>), which is very convenient. However, in this case, we are interested only in values, so we take the first entry of the result (max values).</p>
<p>The following is the next line:</p>
<div class="tcolorbox" id="tcolobox-115">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-139"><code>        next_state_values[dones_t] = 0.0</code></pre>
</div>
</div>
<p>Here, we make one simple but very important transformation: if the transition in the batch is from the last step in the episode, then our value of the action doesn‚Äôt have a discounted reward for the next state, as there is no next state from which to gather the reward. This may look minor, but it is very important in practice; without this, training will not converge (I personally have wasted several hours debugging this case).</p>
<p>In the next line, we detach the value from its computation graph to prevent gradients from flowing into the NN used to calculate the Q approximation for the next states:</p>
<div class="tcolorbox" id="tcolobox-116">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-140"><code>        next_state_values = next_state_values.detach()</code></pre>
</div>
</div>
<p>This is important, as without this, our <span id="dx1-102128"/>backpropagation of the loss will start to affect both predictions for the current state and the next state. However, we don‚Äôt want to touch predictions for the next state, as they are used in the Bellman equation to calculate the reference Q-values. To block gradients from flowing into this branch of the graph, we use the <span class="cmtt-10x-x-109">detach() </span>method of the tensor, which returns the tensor without connection to its calculation history.</p>
<p>Finally, we calculate the Bellman approximation value and the mean squared error loss:</p>
<div class="tcolorbox" id="tcolobox-117">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-141"><code>    expected_state_action_values = next_state_values * GAMMA + rewards_t 
    return nn.MSELoss()(state_action_values, expected_state_action_values)</code></pre>
</div>
</div>
<p>To get the full picture of the loss function calculation code, let‚Äôs look at this function in full:</p>
<div class="tcolorbox" id="tcolobox-118">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-142"><code>def calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, 
              device: torch.device) -&gt; torch.Tensor: 
    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device) 
 
    state_action_values = net(states_t).gather( 
        1, actions_t.unsqueeze(-1) 
    ).squeeze(-1) 
    with torch.no_grad(): 
        next_state_values = tgt_net(new_states_t).max(1)[0] 
        next_state_values[dones_t] = 0.0 
        next_state_values = next_state_values.detach() 
 
    expected_state_action_values = next_state_values * GAMMA + rewards_t 
    return nn.MSELoss()(state_action_values, expected_state_action_values)</code></pre>
</div>
</div>
<p>This ends our loss function calculation.</p>
<p>The rest of the code is our training loop. To begin with, we create a parser of command-line arguments:</p>
<div class="tcolorbox" id="tcolobox-119">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-143"><code>if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("--dev", default="cpu", help="Device name, default=cpu") 
    parser.add_argument("--env", default=DEFAULT_ENV_NAME, 
                        help="Name of the environment, default=" + DEFAULT_ENV_NAME) 
    args = parser.parse_args() 
    device = torch.device(args.dev)</code></pre>
</div>
</div>
<p>Our script allows us to specify a device for computation and train on environments that are different from the default.</p>
<p>Here, we create our environment:</p>
<div class="tcolorbox" id="tcolobox-120">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-144"><code>    env = wrappers.make_env(args.env) 
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device) 
    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)</code></pre>
</div>
</div>
<p>Our <span id="dx1-102155"/>environment has all the required wrappers applied, the NN that we are going to train, and our target network with the same architecture. At first, they will be initialized with different random weights, but it doesn‚Äôt matter much, as we will sync them every 1k frames, which roughly corresponds to one episode of Pong.</p>
<p>Then, we create our experience replay buffer of the required size and pass it to the agent:</p>
<div class="tcolorbox" id="tcolobox-121">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-145"><code>    writer = SummaryWriter(comment="-" + args.env) 
    print(net) 
    buffer = ExperienceBuffer(REPLAY_SIZE) 
    agent = Agent(env, buffer) 
    epsilon = EPSILON_START</code></pre>
</div>
</div>
<p>Epsilon is initially initialized to 1.0 but will be decreased every iteration. Here are the last things we do before the training loop:</p>
<div class="tcolorbox" id="tcolobox-122">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-146"><code>    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) 
    total_rewards = [] 
    frame_idx = 0 
    ts_frame = 0 
    ts = time.time() 
    best_m_reward = None</code></pre>
</div>
</div>
<p>We create an optimizer, a buffer for full episode rewards, a counter of frames and several variables to track our speed, and the best mean reward reached. Every time our mean reward beats the record, we will save the model in the file.</p>
<p>At the beginning of the training loop, we count the number of iterations completed and decrease epsilon according to our schedule:</p>
<div class="tcolorbox" id="tcolobox-123">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-147"><code>    while True: 
        frame_idx += 1 
        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)</code></pre>
</div>
</div>
<p>Epsilon will <span id="dx1-102170"/>drop linearly during the given number of frames (<span class="cmtt-10x-x-109">EPSILON</span><span class="cmtt-10x-x-109">_DECAY</span><span class="cmtt-10x-x-109">_LAST</span><span class="cmtt-10x-x-109">_FRAME=150k</span>) and then be kept on the same level as <span class="cmtt-10x-x-109">EPSILON</span><span class="cmtt-10x-x-109">_FINAL=0.01</span>.</p>
<p>In this block of code, we ask our agent to make a single step in the environment (using our current network and value for epsilon):</p>
<div class="tcolorbox" id="tcolobox-124">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-148"><code>        reward = agent.play_step(net, device, epsilon) 
        if reward is not None: 
            total_rewards.append(reward) 
            speed = (frame_idx - ts_frame) / (time.time() - ts) 
            ts_frame = frame_idx 
            ts = time.time() 
            m_reward = np.mean(total_rewards[-100:]) 
            print(f"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, " 
                  f"eps {epsilon:.2f}, speed {speed:.2f} f/s") 
            writer.add_scalar("epsilon", epsilon, frame_idx) 
            writer.add_scalar("speed", speed, frame_idx) 
            writer.add_scalar("reward_100", m_reward, frame_idx) 
            writer.add_scalar("reward", reward, frame_idx)</code></pre>
</div>
</div>
<p>This function returns a float value only if this step is the final step in the episode. In that case, we report our progress. Specifically, we calculate and show, both in the console and in TensorBoard, these values:</p>
<ul>
<li>
<p>Speed as a count of frames processed per second</p>
</li>
<li>
<p>Count of episodes played</p>
</li>
<li>
<p>Mean reward for the last 100 episodes</p>
</li>
<li>
<p>Current value for epsilon</p>
</li>
</ul>
<p>Every time our mean reward for the last 100 episodes reaches a maximum, we report this and save the model parameters:</p>
<div class="tcolorbox" id="tcolobox-125">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-149"><code>            if best_m_reward is None or best_m_reward &lt; m_reward: 
                torch.save(net.state_dict(), args.env + "-best_%.0f.dat" % m_reward) 
                if best_m_reward is not None: 
                    print(f"Best reward updated {best_m_reward:.3f} -&gt; {m_reward:.3f}") 
                best_m_reward = m_reward 
            if m_reward &gt; MEAN_REWARD_BOUND: 
                print("Solved in %d frames!" % frame_idx) 
                break</code></pre>
</div>
</div>
<p>If our mean reward exceeds the specified boundary, then we stop training. For Pong, the boundary is 19.0, which means winning more than 19 from 21 total games.</p>
<p>Here, we check whether our buffer is large enough for training:</p>
<div class="tcolorbox" id="tcolobox-126">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-150"><code>        if len(buffer) &lt; REPLAY_START_SIZE: 
            continue 
        if frame_idx % SYNC_TARGET_FRAMES == 0: 
            tgt_net.load_state_dict(net.state_dict())</code></pre>
</div>
</div>
<p>First, we should <span id="dx1-102196"/>wait for enough data to be accumulated, which in our case is 10k transitions. The next condition syncs parameters from our main network to the target network every <span class="cmtt-10x-x-109">SYNC</span><span class="cmtt-10x-x-109">_TARGET</span><span class="cmtt-10x-x-109">_FRAMES</span>, which is 1k by default.</p>
<p>The last piece of the training loop is very simple but requires the most time to execute:</p>
<div class="tcolorbox" id="tcolobox-127">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-151"><code>        optimizer.zero_grad() 
        batch = buffer.sample(BATCH_SIZE) 
        loss_t = calc_loss(batch, net, tgt_net, device) 
        loss_t.backward() 
        optimizer.step()</code></pre>
</div>
</div>
<p>Here, we zero gradients, sample data batches from the experience replay buffer, calculate loss, and perform the optimization step to minimize the loss.</p>
</section>
<section class="level4 subsectionHead" id="running-and-performance">
<h2 class="heading-2" id="sigil_toc_id_92"> <span id="x1-1030006.4.4"/>Running and performance</h2>
<p>This example is <span id="dx1-103001"/>demanding on resources. On Pong, it requires about 400k frames to reach a mean reward of 17 (which means winning more than 80% of games). A similar number of frames will be required to get from 17 to 19, as our learning progress will saturate, and it will be hard for the model to ‚Äúpolish the policy‚Äù and further improve the score. So, on average, a million game frames are needed to train it fully. On the GTX 1080Ti, I have a speed of about 250 frames per second, which is about an hour of training. On a CPU (i5-7600k), the speed is much slower, about 40 frames per second, which will take about seven hours. Remember that this is for Pong, which is relatively easy to solve. Other games might require hundreds of millions of frames and a 100 times larger experience replay buffer.</p>
<p>In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>, we will look at various approaches found by researchers since 2015 that can help to increase both training speed and data efficiency. <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a> will be devoted to engineering tricks to speed up RL methods‚Äô performance. Nevertheless, for Atari, you will need resources and patience. The following figure shows a chart with reward <span id="dx1-103002"/>dynamics during the training:</p>
<div class="minipage">
<p><img alt="PIC" height="200" src="../Images/B22150_06_04.png" width="300"/> <span id="x1-103003r4"/></p>
<span class="id">Figure¬†6.4: Dynamics of average reward calculated over the last 100 episodes </span>
</div>
<p>Now, let‚Äôs look at the console output from our training process (only the beginning of the output is shown):</p>
<pre class="lstlisting" id="listing-152"><code>Chapter06$ ./02_dqn_pong.py --dev cuda 
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) 
[Powered by Stella] 
DQN( 
¬†¬†(conv): Sequential( 
¬†¬†¬†(0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) 
¬†¬†¬†(3): ReLU() 
¬†¬†¬†(4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) 
¬†¬†¬†(5): ReLU() 
¬†¬†¬†(6): Flatten(start_dim=1, end_dim=-1) 
¬†¬†) 
¬†¬†(fc): Sequential( 
¬†¬†¬†(0): Linear(in_features=3136, out_features=512, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=512, out_features=6, bias=True) 
¬†¬†) 
) 
940: done 1 games, reward -21.000, eps 0.99, speed 1214.95 f/s 
1946: done 2 games, reward -20.000, eps 0.99, speed 1420.09 f/s 
Best reward updated -21.000 -&gt; -20.000 
2833: done 3 games, reward -20.000, eps 0.98, speed 1416.26 f/s 
3701: done 4 games, reward -20.000, eps 0.98, speed 1421.84 f/s 
4647: done 5 games, reward -20.200, eps 0.97, speed 1421.63 f/s 
5409: done 6 games, reward -20.333, eps 0.96, speed 1395.67 f/s 
6171: done 7 games, reward -20.429, eps 0.96, speed 1411.90 f/s 
7063: done 8 games, reward -20.375, eps 0.95, speed 1404.49 f/s 
7882: done 9 games, reward -20.444, eps 0.95, speed 1388.26 f/s 
8783: done 10 games, reward -20.400, eps 0.94, speed 1283.64 f/s 
9545: done 11 games, reward -20.455, eps 0.94, speed 1376.47 f/s 
10307: done 12 games, reward -20.500, eps 0.93, speed 431.94 f/s 
11362: done 13 games, reward -20.385, eps 0.92, speed 276.14 f/s 
12420: done 14 games, reward -20.214, eps 0.92, speed 276.44 f/s</code></pre>
<p>During the <span id="dx1-103038"/>first 10k steps, our speed is very high, as we don‚Äôt do any training, which is the most expensive operation in our code. After 10k, we start sampling the training batches and the performance drops to more representative numbers. During the training, the performance also decreases slightly, just because of the epsilon decrease. When <span class="cmmi-10x-x-109">ùúñ </span>is high, the actions are chosen randomly. As <span class="cmmi-10x-x-109">ùúñ</span> approaches zero, we need to perform inference to get Q-values for action selection, which also costs time.</p>
<p>Several dozens of games later, our DQN should start to figure out how to win 1 or 2 games out of 21, and an average reward begins to grow (this normally happens around <span class="cmmi-10x-x-109">ùúñ </span>= 0<span class="cmmi-10x-x-109">.</span>5):</p>
<pre class="lstlisting" id="listing-153"><code>66024: done 68 games, reward -20.162, eps 0.56, speed 260.89 f/s 
67338: done 69 games, reward -20.130, eps 0.55, speed 257.63 f/s 
68440: done 70 games, reward -20.100, eps 0.54, speed 260.17 f/s 
69467: done 71 games, reward -20.113, eps 0.54, speed 260.02 f/s 
70792: done 72 games, reward -20.125, eps 0.53, speed 258.88 f/s 
72031: done 73 games, reward -20.123, eps 0.52, speed 259.54 f/s 
73314: done 74 games, reward -20.095, eps 0.51, speed 258.16 f/s 
74815: done 75 games, reward -20.053, eps 0.50, speed 257.56 f/s 
76339: done 76 games, reward -20.026, eps 0.49, speed 256.79 f/s 
77576: done 77 games, reward -20.013, eps 0.48, speed 257.86 f/s 
78978: done 78 games, reward -19.974, eps 0.47, speed 255.90 f/s 
80093: done 79 games, reward -19.962, eps 0.47, speed 256.84 f/s 
81565: done 80 games, reward -19.938, eps 0.46, speed 256.34 f/s 
83365: done 81 games, reward -19.901, eps 0.44, speed 254.22 f/s 
84841: done 82 games, reward -19.878, eps 0.43, speed 254.80 f/s</code></pre>
<p>Finally, after many more games, our DQN can finally dominate and beat the (not very sophisticated) built-in Pong AI opponent:</p>
<pre class="lstlisting" id="listing-154"><code>737860: done 371 games, reward 18.540, eps 0.01, speed 225.22 f/s 
739935: done 372 games, reward 18.650, eps 0.01, speed 232.70 f/s 
Best reward updated 18.610 -&gt; 18.650 
741910: done 373 games, reward 18.650, eps 0.01, speed 231.66 f/s 
743964: done 374 games, reward 18.760, eps 0.01, speed 231.59 f/s 
Best reward updated 18.650 -&gt; 18.760 
745939: done 375 games, reward 18.770, eps 0.01, speed 223.45 f/s 
Best reward updated 18.760 -&gt; 18.770 
747950: done 376 games, reward 18.810, eps 0.01, speed 229.84 f/s 
Best reward updated 18.770 -&gt; 18.810 
749925: done 377 games, reward 18.810, eps 0.01, speed 228.05 f/s 
752008: done 378 games, reward 18.910, eps 0.01, speed 225.41 f/s 
Best reward updated 18.810 -&gt; 18.910 
753983: done 379 games, reward 18.920, eps 0.01, speed 229.75 f/s 
Best reward updated 18.910 -&gt; 18.920 
755958: done 380 games, reward 19.030, eps 0.01, speed 228.71 f/s 
Best reward updated 18.920 -&gt; 19.030 
Solved in 755958 frames!</code></pre>
<p>Due to <span id="dx1-103072"/>randomness in the training process, your actual dynamics might differ from what is displayed here. In some rare cases (1 run from 10 according to my experiments), the training does not converge at all, which looks like a constant stream of rewards <span class="cmsy-10x-x-109">‚àí</span>21 for a long time. This is not an uncommon situation in deep learning (due to the randomness of the training) and might occur even more often in RL (due to the added randomness of environment communication). If your training doesn‚Äôt show any positive dynamics for the first 100k‚Äì200k iterations, you should restart it.</p>
</section>
<section class="level4 subsectionHead" id="your-model-in-action">
<h2 class="heading-2" id="sigil_toc_id_93"> <span id="x1-1040006.4.5"/>Your model in action</h2>
<p>The training <span id="dx1-104001"/>process is just one part of the picture. Our final goal is not only to train the model; we also want our model to play the game with a good outcome. During the training, every time we update the maximum of the mean reward for the last 100 games, we save the model into the file <span class="cmtt-10x-x-109">PongNoFrameskip-v4-best</span><span class="cmtt-10x-x-109">_&lt;score&gt;.dat</span>. In the <span class="cmtt-10x-x-109">Chapter06/03</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_play.py</span> file, we have a program that can load this model file and play one episode, displaying the model‚Äôs dynamics.</p>
<p>The code is very simple, but it can be like magic seeing how several matrices, with just a million parameters, can play Pong with superhuman accuracy by observing only the pixels.</p>
<p>First, we import the familiar PyTorch and Gym modules:</p>
<div class="tcolorbox" id="tcolobox-128">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-155"><code>import gymnasium as gym 
import argparse 
import numpy as np 
import typing as tt 
 
import torch 
 
from lib import wrappers 
from lib import dqn_model 
 
import collections 
 
DEFAULT_ENV_NAME = "PongNoFrameskip-v4"</code></pre>
</div>
</div>
<p>The script accepts the filename of the saved model and allows the specification of the Gym environment (of course, the model and environment have to match):</p>
<div class="tcolorbox" id="tcolobox-129">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-156"><code>if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("-m", "--model", required=True, help="Model file to load") 
    parser.add_argument("-e", "--env", default=DEFAULT_ENV_NAME, 
                        help="Environment name to use, default=" + DEFAULT_ENV_NAME) 
    parser.add_argument("-r", "--record", required=True, help="Directory for video") 
    args = parser.parse_args()</code></pre>
</div>
</div>
<p>Additionally, you have to pass option <span class="cmtt-10x-x-109">-r </span>with the name of a nonexistent directory, which will be used to save a video of your game.</p>
<p>The following code is also not very complicated:</p>
<div class="tcolorbox" id="tcolobox-130">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-157"><code>    env = wrappers.make_env(args.env, render_mode="rgb_array") 
    env = gym.wrappers.RecordVideo(env, video_folder=args.record) 
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n) 
    state = torch.load(args.model, map_location=lambda stg, _: stg) 
    net.load_state_dict(state) 
 
    state, _ = env.reset() 
    total_reward = 0.0 
    c: tt.Dict[int, int] = collections.Counter()</code></pre>
</div>
</div>
<p>We <span id="dx1-104031"/>create the environment, wrap it in the <span class="cmtt-10x-x-109">RecordVideo </span>wrapper, create the model, and then we load weights from the file passed in the arguments. The argument <span class="cmtt-10x-x-109">map</span><span class="cmtt-10x-x-109">_location</span>, passed to the <span class="cmtt-10x-x-109">torch.load() </span>function, is needed to map the loaded tensor location from the GPU to the CPU. By default, torch tries to load tensors on the same device where they were saved, but if you copy the model from the machine you used for training (with a GPU) to a laptop without a GPU, the locations need to be remapped. Our example doesn‚Äôt use the GPU at all, as inference is fast enough without acceleration.</p>
<p>This is almost an exact copy of the <span class="cmtt-10x-x-109">Agent </span>class‚Äô method <span class="cmtt-10x-x-109">play</span><span class="cmtt-10x-x-109">_step() </span>from the training code, without the epsilon-greedy action selection:</p>
<div class="tcolorbox" id="tcolobox-131">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-158"><code>    while True: 
        state_v = torch.tensor([state]) 
        q_vals = net(state_v).data.numpy()[0] 
        action = int(np.argmax(q_vals)) 
        c[action] += 1</code></pre>
</div>
</div>
<p>We just pass our observation to the agent and select the action with the maximum value.</p>
<p>The rest of the code is also simple:</p>
<div class="tcolorbox" id="tcolobox-132">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-159"><code>        state, reward, is_done, is_trunc, _ = env.step(action) 
        total_reward += reward 
        if is_done or is_trunc: 
            break 
    print("Total reward: %.2f" % total_reward) 
    print("Action counts:", c) 
    env.close()</code></pre>
</div>
</div>
<p>We pass the action to the environment, count the total reward, and stop our loop when the episode ends. After <span id="dx1-104044"/>the episode, we show the total reward and the number of times that the agent executed the action.</p>
<p>In this YouTube playlist, you can find recordings of the gameplay at different stages of the training: <a class="url" href="https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu"><span class="cmtt-10x-x-109">https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu</span></a>.</p>
</section>
</section>
<section class="level3 sectionHead" id="things-to-try">
<h1 class="heading-1" id="sigil_toc_id_94"> <span id="x1-1050006.5"/>Things to try</h1>
<p>If you are curious and want to experiment with this chapter‚Äôs material on your own, then here is a shortlist of directions to explore. Be warned though: they can take lots of time and may cause you some moments of frustration during your experiments. However, these experiments are a very efficient way to really master the material from a practical point of view:</p>
<ul>
<li>
<p>Try to take some other games from the Atari set, such as Breakout, Atlantis, or River Raid (my childhood favorite). This could require the tuning of hyperparameters.</p>
</li>
<li>
<p>As an alternative to FrozenLake, there is another tabular environment, Taxi, which emulates a taxi driver who needs to pick up passengers and take them to a destination.</p>
</li>
<li>
<p>Play with Pong hyperparameters. Is it possible to train faster? OpenAI claims that it can solve Pong in 30 minutes using the asynchronous advantage actor-critic method (which is a subject of <span class="cmti-10x-x-109">Part 3 </span>of this book). Maybe it‚Äôs possible with a DQN.</p>
</li>
<li>
<p>Can you make the DQN training code faster? The OpenAI Baselines project has shown 350 FPS using TensorFlow on GTX 1080 Ti. So, it looks like it‚Äôs possible to optimize the PyTorch code. We will discuss this topic in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a>, but meanwhile, you can do your own experiments.</p>
</li>
<li>
<p>In the video recording, you might notice that models with a mean score around zero play quite well. In fact, I had the impression that those models play better than models with mean scores of 10‚Äì19. This might be the case due to overfitting to the particular game situations. Could you try to fix this? Maybe it would be possible to use a generative adversarial network-style approach to make one model play with another?</p>
</li>
<li>
<p>Can you get the Ultimate Pong Dominator model with a mean score of 21? It shouldn‚Äôt be very hard ‚Äì the learning rate decay is the obvious method to try.</p>
</li>
</ul>
</section>
<section class="level3 sectionHead" id="summary-5">
<h1 class="heading-1" id="sigil_toc_id_95"> <span id="x1-1060006.6"/>Summary</h1>
<p>In this chapter, we covered a lot of new and complex material. You became familiar with the limitations of value iteration in complex environments with large observation spaces, and we discussed how to overcome them with Q-learning. We checked the Q-learning algorithm on the FrozenLake environment and discussed the approximation of Q-values with NNs, as well as the extra complications that arise from this approximation.</p>
<p>We covered several tricks with DQNs to improve their training stability and convergence, such as an experience replay buffer, target networks, and frame stacking. Finally, we combined those extensions into one single implementation of DQN that solves the Pong environment from the Atari games suite.</p>
<p>In the next chapter, we will take a quick look at higher-level RL libraries, and after that, we will take a look at a set of tricks that researchers have found since 2015 to improve DQN convergence and quality, which (combined) can produce state-of-the-art results on most of the 54 (newly added) Atari games. This set was published in 2017, and we will analyze and reimplement all of the tricks.</p>
</section>
</section>
</div></body></html>