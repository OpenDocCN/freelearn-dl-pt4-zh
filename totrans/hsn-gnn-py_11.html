<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer533">
<h1 class="chapter-number" id="_idParaDest-127"><a id="_idTextAnchor131"/>11</h1>
<h1 id="_idParaDest-128"><a id="_idTextAnchor132"/>Generating Graphs Using Graph Neural Networks</h1>
<p>Graph generation consists of finding methods to create new graphs. As a field of study, it provides insights into understanding how graphs work and evolve. It also has direct applications in data augmentation, anomaly detection, drug discovery, and so on. We can distinguish two types of<a id="_idIndexMarker632"/> generation: <strong class="bold">realistic graph generation</strong>, which imitates a given graph (for example, in data augmentation), and <strong class="bold">goal-directed graph generation</strong>, which<a id="_idIndexMarker633"/> creates graphs that optimize a specific metric (for instance, in <span class="No-Break">molecule generation).</span></p>
<p>In this chapter, we will explore traditional techniques to understand how graph generation works. We will focus on two popular algorithms: the <strong class="bold">Erdős–Rényi</strong> and the <strong class="bold">small-world</strong> models. They present interesting properties but also issues that motivate the need for GNN-based graph generation. In the second section, we will describe three families of solutions: <strong class="bold">variational autoencoder</strong> (<strong class="bold">VAE</strong>)-based, autoregressive, and <strong class="bold">GAN</strong>-based models. Finally, we will implement a GAN-based framework with <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>) to generate new chemical compounds. Instead of PyTorch Geometric, we will use the <strong class="bold">DeepChem</strong> library <span class="No-Break">with TensorFlow.</span></p>
<p>By the end of this chapter, you will be able to generate graphs using traditional and GNN-based techniques. You will have a good overview of this field and the different applications you can build with it. You will know how to implement a hybrid architecture to guide the generation into generating valid molecules with your <span class="No-Break">desired properties.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Generating graphs with <span class="No-Break">traditional techniques</span></li>
<li>Generating graphs with graph <span class="No-Break">neural networks</span></li>
<li>Generating molecules <span class="No-Break">with MolGAN</span></li>
</ul>
<h1 id="_idParaDest-129"><a id="_idTextAnchor133"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11</span></a><span class="No-Break">.</span></p>
<p>Installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> of <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor134"/>Generating graphs with traditional techniques</h1>
<p>Traditional graph<a id="_idIndexMarker634"/> generation techniques have been studied for decades. This is why they are well understood and can be used as <a id="_idIndexMarker635"/>baselines in various applications. However, they are often limited in the type of graphs they can generate. Most of them are specialized to output certain topologies, which is why they cannot simply imitate a <span class="No-Break">given network.</span></p>
<p>In this section, we will introduce two classical techniques: the Erdős–Rényi and the <span class="No-Break">small-world models.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor135"/>The Erdős–Rényi model</h2>
<p>The <a id="_idIndexMarker636"/>Erdős–Rényi model is the <a id="_idIndexMarker637"/>simplest and most popular random graph model. It was introduced by Hungarian mathematicians Paul Erdős and Alfréd Rényi in 1959 [1] and was independently proposed by Edgar Gilbert the same year [2]. This model has two variants: <img alt="" height="42" src="image/Formula_B19153_11_001.png" width="120"/> <span class="No-Break">and <img alt="" height="40" src="image/Formula_B19153_11_002.png" width="133"/>.</span></p>
<p>The <img alt="" height="41" src="image/Formula_B19153_11_003.png" width="119"/> model is straightforward: we are given <img alt="" height="22" src="image/Formula_B19153_11_004.png" width="25"/> nodes and a probability <img alt="" height="30" src="image/Formula_B19153_11_005.png" width="23"/> of connecting a pair of nodes. We try to randomly connect every node to each other to create the final graph. It means that there are <img alt="" height="61" src="image/Formula_B19153_11_006.png" width="49"/> possible links. Another way of understanding the probability <img alt="" height="29" src="image/Formula_B19153_11_007.png" width="21"/> is to consider it as a parameter to change the density of <span class="No-Break">the network.</span></p>
<p>The <strong class="source-inline">networkx</strong> library <a id="_idIndexMarker638"/>has a<a id="_idIndexMarker639"/> direct implementation of the <img alt="" height="44" src="image/Formula_B19153_11_008.png" width="126"/> <span class="No-Break">model:</span></p>
<ol>
<li>We import the <span class="No-Break"><strong class="source-inline">networkx</strong></span><span class="No-Break"> library:</span><pre class="source-code">
import networkx as nx
import matplotlib.pyplot as plt</pre></li>
<li>We generate a <strong class="source-inline">G</strong> graph using the <strong class="source-inline">nx.erdos_renyi_graph()</strong> function with <strong class="source-inline">10</strong> nodes (<img alt="" height="31" src="image/Formula_B19153_11_009.png" width="124"/>) and a probability for edge creation of <span class="No-Break"><strong class="source-inline">0.5</strong></span><span class="No-Break"> (<img alt="" height="38" src="image/Formula_B19153_11_010.png" width="120"/>):</span><pre class="source-code">
G = nx.erdos_renyi_graph(10, 0.5, seed=0)</pre></li>
<li>We position the resulting nodes using the <strong class="source-inline">nx.circular_layout()</strong> function. Other layouts can be used, but this one is handy for comparing different values <span class="No-Break">of <img alt="" height="30" src="image/Formula_B19153_11_011.png" width="22"/>:</span><pre class="source-code">
pos = nx.circular_layout(G)</pre></li>
<li>We<a id="_idIndexMarker640"/> draw the <strong class="source-inline">G</strong> graph with the <strong class="source-inline">pos</strong> layout using <strong class="source-inline">nx.draw()</strong>. Global heuristics are usually more accurate <a id="_idIndexMarker641"/>but require knowing the entirety of the graph. However, it is not the only way to predict links with <span class="No-Break">this knowledge:</span><pre class="source-code">
nx.draw(G, pos=pos, with_labels=True)</pre></li>
</ol>
<p>This gives us the <span class="No-Break">following graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer473">
<img alt="Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5" height="299" src="image/B19153_11_001.jpg" width="386"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5</p>
<p>We can repeat this process with a probability of <strong class="bold">0.1</strong> and <strong class="bold">0.9</strong> to obtain the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer474">
<img alt="Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation" height="422" src="image/B19153_11_002.jpg" width="1633"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation</p>
<p>We can<a id="_idIndexMarker642"/> see that many nodes are<a id="_idIndexMarker643"/> isolated when <img alt="" height="31" src="image/Formula_B19153_11_012.png" width="25"/> is low, while the graph is highly interconnected when <img alt="" height="30" src="image/Formula_B19153_11_013.png" width="23"/> <span class="No-Break">is high.</span></p>
<p>In the <img alt="" height="41" src="image/Formula_B19153_11_014.png" width="133"/> model, we randomly choose a graph from all graphs with <img alt="" height="23" src="image/Formula_B19153_11_015.png" width="27"/> nodes and <img alt="" height="31" src="image/Formula_B19153_11_016.png" width="34"/> links. For instance, if <img alt="" height="30" src="image/Formula_B19153_11_017.png" width="96"/> and <img alt="" height="33" src="image/Formula_B19153_11_018.png" width="118"/>, there are three possible graphs (see <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em>). The <img alt="" height="41" src="image/Formula_B19153_11_019.png" width="135"/> model will just randomly select one of these graphs. This is a different approach to the same problem, but it is not as popular as the <img alt="" height="45" src="image/Formula_B19153_11_020.png" width="126"/> model because it is more challenging to analyze <span class="No-Break">in general:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer484">
<img alt="Figure 11.3 – A set of graphs with three nodes and two links" height="365" src="image/B19153_11_003.jpg" width="1617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – A set of graphs with three nodes and two links</p>
<p>We<a id="_idIndexMarker644"/> can <a id="_idIndexMarker645"/>also implement the <img alt="" height="43" src="image/Formula_B19153_11_021.png" width="140"/> model in Python using the <span class="No-Break"><strong class="source-inline">nx.gnm_random_graph()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
G = nx.gnm_random_graph(3, 2, seed=0)
pos = nx.circular_layout(G)
nx.draw(G, pos=pos, with_labels=True)</pre>
<div>
<div class="IMG---Figure" id="_idContainer486">
<img alt="Figure 11.4 – A graph randomly sampled from the set of graphs with three nodes and two links" height="311" src="image/B19153_11_004.jpg" width="394"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – A graph randomly sampled from the set of graphs with three nodes and two links</p>
<p>The strongest and most interesting assumption made by the <img alt="" height="42" src="image/Formula_B19153_11_022.png" width="126"/> model is that links are independent (meaning that they do not interfere with each other). Unfortunately, it is not true <a id="_idIndexMarker646"/>for most real-world graphs, where<a id="_idIndexMarker647"/> we observe clusters and communities that contradict <span class="No-Break">this rule.</span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor136"/>The small-world model</h2>
<p>Introduced <a id="_idIndexMarker648"/>in 1998 by Duncan Watts and Steven Strogatz [3], the small-world model tries to imitate the behavior of <a id="_idIndexMarker649"/>biological, technological, and social networks. The main concept is that real-world networks are not completely random (as in the Erdős–Rényi model) but not totally regular either (as in a grid). This kind of topology is somewhere in between, which is why we can interpolate it using a coefficient. The small-world model produces graphs that <span class="No-Break">have both:</span></p>
<ul>
<li><strong class="bold">Short paths</strong>: The average<a id="_idIndexMarker650"/> distance between any two nodes in the network is relatively small, which makes it easy for information to spread quickly throughout <span class="No-Break">the network</span></li>
<li><strong class="bold">High clustering coefficients</strong>: Nodes<a id="_idIndexMarker651"/> in the network tend to be closely connected to one another, creating dense clusters <span class="No-Break">of nodes</span></li>
</ul>
<p>Many<a id="_idIndexMarker652"/> algorithms <a id="_idIndexMarker653"/>display small-world properties. In the following, we will describe the original <strong class="bold">Watts–Strogatz</strong> model proposed in [3]. It can be implemented using the <span class="No-Break">following steps:</span></p>
<ol>
<li>We initialize a graph with <img alt="" height="24" src="image/Formula_B19153_11_023.png" width="27"/> <span class="No-Break">nodes.</span></li>
<li>Each node is connected to its <img alt="" height="29" src="image/Formula_B19153_11_024.png" width="21"/> nearest neighbors (or <img alt="" height="28" src="image/Formula_B19153_11_025.png" width="83"/> neighbors if <img alt="" height="31" src="image/Formula_B19153_11_026.png" width="22"/> <span class="No-Break">is odd).</span></li>
<li>Each <a id="_idIndexMarker654"/>link between nodes <img alt="" height="32" src="image/Formula_B19153_11_027.png" width="13"/> and <img alt="" height="34" src="image/Formula_B19153_11_028.png" width="17"/> has a probability <img alt="" height="33" src="image/Formula_B19153_11_029.png" width="25"/> of being rewired between <img alt="" height="34" src="image/Formula_B19153_11_030.png" width="14"/> and <img alt="" height="31" src="image/Formula_B19153_11_031.png" width="22"/>, where <img alt="" height="33" src="image/Formula_B19153_11_032.png" width="25"/> is another <span class="No-Break">random node.</span></li>
</ol>
<p>In Python, we can implement it by calling the <span class="No-Break"><strong class="source-inline">nx.watts_strogatz_graph()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
G = nx.watts_strogatz_graph(10, 4, 0.5, seed=0)
pos = nx.circular_layout(G)
nx.draw(G, pos=pos)</pre>
<p>This produces the <span class="No-Break">following graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer498">
<img alt="Figure 11.5 – A small-world network obtained with the Watts–Strogatz model" height="306" src="image/B19153_11_005.jpg" width="391"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – A small-world network obtained with the Watts–Strogatz model</p>
<p>As with the Erdős–Rényi model, we can<a id="_idIndexMarker655"/> repeat the same process with different probabilities <img alt="" height="31" src="image/Formula_B19153_11_033.png" width="23"/> to obtain <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer500">
<img alt="Figure 11.6 – A small-world model with different probabilities for rewiring" height="405" src="image/B19153_11_006.jpg" width="1600"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – A small-world model with different probabilities for rewiring</p>
<p>We can see that when <img alt="" height="38" src="image/Formula_B19153_11_034.png" width="100"/>, the graph is completely regular. On the opposite end, when <img alt="" height="38" src="image/Formula_B19153_11_035.png" width="95"/>, the graph<a id="_idIndexMarker656"/> is completely random as every link has been rewired. We obtain a balanced graph between these two extremes with hubs and <span class="No-Break">local clustering.</span></p>
<p>Nonetheless, the <a id="_idIndexMarker657"/>Watts–Strogatz model does not produce a realistic degree distribution. It also requires a fixed number of nodes, which means it cannot be used for network growth. In general, classical methods fail to capture real-world graphs’ full diversity and complexity. This motivated the creation of a new family of techniques, often referred to as deep <span class="No-Break">graph generation.</span></p>
<h1 id="_idParaDest-133"><a id="_idTextAnchor137"/>Generating graphs with graph neural networks</h1>
<p>Deep graph generative<a id="_idIndexMarker658"/> models are GNN-based architectures that are more expressive than traditional techniques. However, it comes at a cost: they are often too complex to be analyzed and understood, like classical methods. We list three main families of architecture for deep graph generation: VAEs, GANs, and autoregressive models. Other techniques exist, such as normalizing flows or diffusion models, but they are less popular and mature than <span class="No-Break">these three.</span></p>
<p>This section will describe how to use VAEs, GANs, and autoregressive models to <span class="No-Break">generate graphs.</span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor138"/>Graph variational autoencoders</h2>
<p>As seen in the last chapter, VAEs can <a id="_idIndexMarker659"/>be used to approximate an adjacency matrix. The <strong class="bold">Graph Variational Autoencoder</strong> (<strong class="bold">GVAE</strong>) model we<a id="_idIndexMarker660"/> saw has two components: an encoder and a decoder. The encoder uses two GCNs that share their first layer to learn the mean and the variance of each latent normal distribution. The decoder then samples the learned distributions to perform the inner product between latent variables <img alt="" height="30" src="image/Formula_B19153_11_036.png" width="25"/>. In the end, we obtained the approximated adjacency <span class="No-Break">matrix <img alt="" height="50" src="image/Formula_B19153_11_037.png" width="211"/>.</span></p>
<p>In the previous chapter, we used <img alt="" height="44" src="image/Formula_B19153_11_038.png" width="29"/> to predict links. However, it is not its only application: it directly gives us the adjacency matrix of a network that imitates graphs seen <a id="_idIndexMarker661"/>during training. Instead of predicting links, we can use this output to generate new graphs. Here is <a id="_idIndexMarker662"/>an example of the adjacency matrix created by the VGAE model from <a href="B19153_10.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">:</span></p>
<pre class="source-code">
z = model.encode(test_data.x, test_data.edge_index)
adj = torch.where((z @ z.T) &gt; 0.9, 1, 0)
adj
<strong class="bold">tensor([[1, 0, 0,  ..., 0, 1, 1],</strong>
<strong class="bold">        [0, 1, 1,  ..., 0, 0, 0],</strong>
<strong class="bold">        [0, 1, 1,  ..., 0, 1, 1],</strong>
<strong class="bold">        ...,</strong>
<strong class="bold">        [0, 0, 0,  ..., 1, 0, 0],</strong>
<strong class="bold">        [1, 0, 1,  ..., 0, 1, 1],</strong>
<strong class="bold">        [1, 0, 1,  ..., 0, 1, 1]])</strong></pre>
<p>Since 2016, this technique has been expanded beyond the GVAE model to also output node and edge features. A good example<a id="_idIndexMarker663"/> is one of the most popular VAE-based graph generative models: <strong class="bold">GraphVAE</strong> [4]. Introduced in 2018 by Simonovsky and Komodakis, it is designed to generate realistic molecules. This requires the ability to differentiate nodes (atoms) and edges (<span class="No-Break">chemical bonds).</span></p>
<p>GraphVAE considers graphs <img alt="" height="41" src="image/Formula_B19153_11_039.png" width="217"/>, where <img alt="" height="33" src="image/Formula_B19153_11_040.png" width="28"/> is the adjacency matrix, <img alt="" height="32" src="image/Formula_B19153_11_041.png" width="28"/> is the edge attribute tensor, and <img alt="" height="31" src="image/Formula_B19153_11_042.png" width="26"/> is the node attribute matrix. It learns a probabilistic version of the graph <img alt="" height="46" src="image/Formula_B19153_11_043.png" width="210"/> with a predefined number of nodes. In this probabilistic version, <img alt="" height="44" src="image/Formula_B19153_11_038.png" width="29"/> contains node (<img alt="" height="52" src="image/Formula_B19153_11_045.png" width="74"/>) and edge (<img alt="" height="53" src="image/Formula_B19153_11_046.png" width="74"/>) probabilities, <img alt="" height="34" src="image/Formula_B19153_11_047.png" width="28"/> indicates class probabilities for <a id="_idIndexMarker664"/>edges, and <img alt="" height="38" src="image/Formula_B19153_11_048.png" width="31"/> contains class probabilities for nodes. Compared to GVAE, GraphVAE’s encoder is a feed forward network <a id="_idIndexMarker665"/>with <strong class="bold">edge-conditional graph convolutions</strong> (<strong class="bold">ECC</strong>), and its decoder is a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) with <a id="_idIndexMarker666"/>three outputs. The entire architecture is summarized in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer516">
<img alt="Figure 11.7 – GraphVAE’s inference process" height="520" src="image/B19153_11_007.jpg" width="1441"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – GraphVAE’s inference process</p>
<p>There are <a id="_idIndexMarker667"/>many other VAE-based graph generative architectures. However, their role is not limited to imitating graphs: they can also embed constraints to guide the type of graphs <span class="No-Break">they produce.</span></p>
<p>A popular <a id="_idIndexMarker668"/>way of adding these constraints is to check them during the decoding phase, such as the <strong class="bold">Constrained Graph Variational Autoencoder</strong> (<strong class="bold">CGVAE</strong>) [5]. In<a id="_idIndexMarker669"/> this architecture, the encoder is a <strong class="bold">Gated Graph Convolutional Network</strong> (<strong class="bold">GGCN</strong>), and<a id="_idIndexMarker670"/> the decoder is an autoregressive model. Autoregressive decoders are particularly suited for this task, as<a id="_idIndexMarker671"/> they can verify every constraint for each step of the process. Finally, another technique to add constraints consists of using Lagrangian-based regularizers that are faster to compute but less strict in terms of <span class="No-Break">generation [6].</span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor139"/>Autoregressive models</h2>
<p>Autoregressive models <a id="_idIndexMarker672"/>can also be used<a id="_idIndexMarker673"/> on their own. The difference with other models is that past outputs become part of the current input. In this framework, graph generation becomes a sequential decision-making process that considers both data and past decisions. For instance, at each step, the autoregressive model can create a new node or a new link. Then, the resulting graph is fed to the model for the next generation step until we stop it. The following diagram illustrates <span class="No-Break">this process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer517">
<img alt="Figure 11.8 – The autoregressive process for graph generation" height="212" src="image/B19153_11_008.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – The autoregressive process for graph generation</p>
<p>In practice, we<a id="_idIndexMarker674"/> use <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) to implement this autoregressive ability. In this architecture, previous outputs are used as inputs to compute the current hidden state. In addition, they can process inputs of arbitrary length, which is crucial for generating graphs iteratively. However, this computation is slower than feedforward networks, as the entire sequence must be processed to obtain the final output. The two most popular types of RNNs <a id="_idIndexMarker675"/>are<a id="_idIndexMarker676"/> the <strong class="bold">Gated Recurrent Unit</strong> (<strong class="bold">GRU</strong>) and <strong class="bold">Long Short-Term Memory</strong> (<span class="No-Break"><strong class="bold">LSTM</strong></span><span class="No-Break">) networks.</span></p>
<p>Introduced in 2018 by You et al., <strong class="bold">GraphRNN</strong> [7] is a <a id="_idIndexMarker677"/>direct implementation of these techniques for deep graph generation. This architecture uses <span class="No-Break">two RNNs:</span></p>
<ul>
<li>A <em class="italic">graph-level RNN</em> to <a id="_idIndexMarker678"/>generate a sequence of nodes (including the <span class="No-Break">initial state)</span></li>
<li>An <em class="italic">edge-level RNN</em> to<a id="_idIndexMarker679"/> predict connections for each newly <span class="No-Break">added node</span></li>
</ul>
<p>The edge-level <a id="_idIndexMarker680"/>RNN takes the hidden state of the graph-level RNN as input and then feeds it<a id="_idIndexMarker681"/> with its own output. This mechanism is illustrated in the following diagram at <span class="No-Break">inference time:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer518">
<img alt="Figure 11.9 – GraphRNN’s architecture at inference time" height="637" src="image/B19153_11_009.jpg" width="1586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – GraphRNN’s architecture at inference time</p>
<p>Both RNNs are actually completing an adjacency matrix: each new node created by the graph-level RNN adds a row and a column, which are filled with zeros and ones by the edge-level RNN. In summary, GraphRNN performs the <span class="No-Break">following steps:</span></p>
<ol>
<li><em class="italic">Add new node</em>: The graph-level RNN initializes the graph and its output if fed to the <span class="No-Break">edge-level RNN.</span></li>
<li><em class="italic">Add new connections</em>: The edge-level RNN predicts if the new node is connected to each of the <span class="No-Break">previous nodes.</span></li>
<li><em class="italic">Stop graph generation</em>: The two first steps are repeated until the edge-level RNN outputs an EOS token, marking the end of <span class="No-Break">the process.</span></li>
</ol>
<p>The <a id="_idIndexMarker682"/>GraphRNN <a id="_idIndexMarker683"/>can<a id="_idIndexMarker684"/> learn different types of graphs (grids, social networks, proteins, and so on) and completely outperform traditional techniques. It is an architecture of choice to imitate given graphs that should be preferred <span class="No-Break">to GraphVAE.</span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor140"/>Generative adversarial networks</h2>
<p>Like VAEs, GANs <a id="_idIndexMarker685"/>are a<a id="_idIndexMarker686"/> well-known generative model in <strong class="bold">ML</strong>. In this framework, two neural networks compete in a zero-sum game with two different goals. The first neural network is a generator that creates new data, and the second one is a discriminator that classifies each sample as real (from the training set) or fake (made by <span class="No-Break">the generator).</span></p>
<p>Over the years, two main improvements to the original architecture have been proposed. The first one is called <a id="_idIndexMarker687"/>the <strong class="bold">Wasserstein GAN</strong> (<strong class="bold">WGAN</strong>). It improves learning stability by minimizing the Wasserstein distance (or Earth Mover’s distance) between two probability distributions. This variant is further refined by introducing a gradient penalty instead of the original gradient <span class="No-Break">clipping scheme.</span></p>
<p>Multiple works applied this framework to deep graph generation. Like previous techniques, GANs can imitate graphs or generate networks that optimize certain constraints. The latter option is handy in applications such as finding new chemical compounds with specific properties. This problem is exceptionally vast (over <img alt="" height="39" src="image/Formula_B19153_11_049.png" width="79"/> possible combinations) and complex due to its <span class="No-Break">discrete nature.</span></p>
<p>Proposed by De Cao and Kipf in 2018 [8], the <strong class="bold">molecular GAN</strong> (<strong class="bold">MolGAN</strong>) is a popular solution to this <a id="_idIndexMarker688"/>problem. It combines a WGAN with a gradient penalty that directly processes graph-structured data and an RL objective to generate molecules with desired chemical properties. This<a id="_idIndexMarker689"/> RL objective is <a id="_idIndexMarker690"/>based on the <strong class="bold">Deep Deterministic Policy Gradient</strong> (<strong class="bold">DDPG</strong>) algorithm, an off-policy actor-critic model that uses deterministic policy gradients. MolGAN’s architecture is summarized in the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer520">
<img alt="Figure 11.10 – MolGAN’s architecture at inference time" height="498" src="image/B19153_11_010.jpg" width="1578"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – MolGAN’s architecture at inference time</p>
<p>This framework is <a id="_idIndexMarker691"/>divided into three <span class="No-Break">main components:</span></p>
<ul>
<li>The <strong class="bold">generator</strong> is an<a id="_idIndexMarker692"/> MLP that outputs a node matrix <img alt="" height="31" src="image/Formula_B19153_11_050.png" width="30"/> containing the atom types and an adjacency matrix <img alt="" height="33" src="image/Formula_B19153_11_051.png" width="29"/>, which is actually a tensor containing both the edges and bond types. The generator is trained using a linear combination of the WGAN and RL loss. We<a id="_idIndexMarker693"/> translate these dense representations into sparse objects (<img alt="" height="42" src="image/Formula_B19153_11_052.png" width="37"/> and <img alt="" height="41" src="image/Formula_B19153_11_053.png" width="29"/>) via <span class="No-Break">categorical sampling.</span></li>
<li>The <strong class="bold">discriminator</strong> receives <a id="_idIndexMarker694"/>graphs from the generator and the dataset and learns to distinguish them. It is solely trained using the <span class="No-Break">WGAN loss.</span></li>
<li>The <strong class="bold">reward network</strong> scores<a id="_idIndexMarker695"/> each graph. It is trained using the MSE loss based on the real score provided by an external system (RDKit in <span class="No-Break">this case).</span></li>
</ul>
<p>The discriminator and the reward network use the GNN mode: the Relational-GCN, a GCN variant that supports multiple edge types. After several layers of graph convolutions, node embeddings are aggregated into a graph-level <span class="No-Break">vector output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer525">
<img alt="" height="163" src="image/Formula_B19153_11_054.jpg" width="1174"/>
</div>
</div>
<p>Here, <img alt="" height="24" src="image/Formula_B19153_11_055.png" width="26"/> denotes the logistic sigmoid function, <img alt="" height="37" src="image/Formula_B19153_11_056.png" width="90"/> and <img alt="" height="38" src="image/Formula_B19153_11_057.png" width="93"/> are two MLPs with linear output, and <img alt="" height="34" src="image/Formula_B19153_11_058.png" width="34"/> is <a id="_idIndexMarker696"/>the element-wise multiplication. A third MLP further processes this graph embedding to produce a value between 0 and 1 for the reward network and between <img alt="" height="19" src="image/Formula_B19153_11_059.png" width="66"/> and <img alt="" height="30" src="image/Formula_B19153_11_060.png" width="66"/> for <span class="No-Break">the discriminator.</span></p>
<p>MolGAN <a id="_idIndexMarker697"/>produces valid chemical compounds that optimize properties such as drug likeliness, synthesizability, and solubility. We will implement this architecture in the next section to generate <span class="No-Break">new molecules.</span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor141"/>Generating molecules with MolGAN</h1>
<p>Deep graph <a id="_idIndexMarker698"/>generation is not well covered by PyTorch Geometric. Drug discovery is the main application of this subfield, which is why generative models can be found in specialized libraries. More specifically, there are two popular Python libraries for ML-based drug discovery: <strong class="source-inline">DeepChem</strong> and <strong class="source-inline">torchdrug</strong>. In this section, we will use DeepChem as it is more mature and directly <span class="No-Break">implements </span><span class="No-Break"><a id="_idIndexMarker699"/></span><span class="No-Break">MolGAN.</span></p>
<p>Let’s see how we can use it with <strong class="source-inline">DeepChem</strong> and <strong class="source-inline">tensorflow</strong>. The following procedure is based on <span class="No-Break">DeepChem’s example:</span></p>
<ol>
<li>We install <strong class="source-inline">DeepChem</strong> (<a href="https://deepchem.io">https://deepchem.io</a>), which requires the following libraries: <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">joblib</strong>, <strong class="source-inline">NumPy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">SciPy</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">rdkit</strong></span><span class="No-Break">:</span><pre class="source-code">
!pip install deepchem==2.7.1</pre></li>
<li>Then, we import the <span class="No-Break">required packages:</span><pre class="source-code">
import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow import one_hot
import deepchem as dc
from deepchem.models.optimizers import ExponentialDecay
from deepchem.models import BasicMolGANModel as MolGAN
from deepchem.feat.molecule_featurizers.molgan_featurizer import GraphMatrix
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem import rdmolfiles
from rdkit.Chem import rdmolops
from rdkit.Chem.Draw import IpythonConsole</pre></li>
<li>We <a id="_idIndexMarker700"/>download the <strong class="source-inline">tox21</strong> (<em class="italic">Toxicology in the 21st Century</em>) dataset, which comprises over 6,000 chemical <a id="_idIndexMarker701"/>compounds, to analyze their toxicity. We only need their <strong class="bold">simplified molecular-input line-entry system</strong> (<strong class="bold">SMILES</strong>) representations<a id="_idIndexMarker702"/> in <span class="No-Break">this example:</span><pre class="source-code">
_, datasets, _ = dc.molnet.load_tox21()
df = pd.DataFrame(datasets[0].ids, columns=['smiles'])</pre></li>
<li>Here is an output of these <span class="No-Break"><strong class="source-inline">smiles</strong></span><span class="No-Break"> strings:</span><pre class="source-code">
<strong class="bold">0  CC(O)(P(=O)(O)O)P(=O)(O)O</strong>
<strong class="bold">1  CC(C)(C)OOC(C)(C)CCC(C)(C)OOC(C)(C)C</strong>
<strong class="bold">2  OC[C@H](O)[C@@H](O)[C@H](O)CO</strong>
<strong class="bold">3  CCCCCCCC(=O)[O-].CCCCCCCC(=O)[O-].[Zn+2]</strong>
<strong class="bold">... ...</strong>
<strong class="bold">6260 Cc1cc(CCCOc2c(C)cc(-c3noc(C(F)(F)F)n3)cc2C)on1</strong>
<strong class="bold">6261 O=C1OC(OC(=O)c2cccnc2Nc2cccc(C(F)(F)F)c2)c2ccc...</strong>
<strong class="bold">6262 CC(=O)C1(C)CC2=C(CCCC2(C)C)CC1C</strong>
<strong class="bold">6263 CC(C)CCC[C@@H](C)[C@H]1CC(=O)C2=C3CC[C@H]4C[C@...</strong></pre></li>
<li>We only consider molecules with a maximum number of 15 atoms. We filter our dataset and create a <strong class="source-inline">featurizer</strong> to convert the <strong class="source-inline">smiles</strong> strings into <span class="No-Break">input features:</span><pre class="source-code">
max_atom = 15
molecules = [x for x in df['smiles'].values if Chem.MolFromSmiles(x).GetNumAtoms() &lt; max_atom]
featurizer = dc.feat.MolGanFeaturizer(max_atom_count=max_atom)</pre></li>
<li>We <a id="_idIndexMarker703"/>manually loop through our dataset <a id="_idIndexMarker704"/>to convert the <span class="No-Break"><strong class="source-inline">smiles</strong></span><span class="No-Break"> strings:</span><pre class="source-code">
features = []
for x in molecules:
    mol = Chem.MolFromSmiles(x)
    new_order = rdmolfiles.CanonicalRankAtoms(mol)
    mol = rdmolops.RenumberAtoms(mol, new_order)
    feature = featurizer.featurize(mol)
    if feature.size != 0:
        features.append(feature[0])</pre></li>
<li>We remove invalid molecules from <span class="No-Break">the dataset:</span><pre class="source-code">
features = [x for x in features if type(x) is GraphMatrix]</pre></li>
<li>Then, we create the <strong class="source-inline">MolGAN</strong> model. It will be trained with a learning rate that has an exponential <span class="No-Break">delay schedule:</span><pre class="source-code">
gan = MolGAN(learning_rate=ExponentialDecay(0.001, 0.9, 5000), vertices=max_atom)</pre></li>
<li>We create the dataset to feed to <strong class="source-inline">MolGAN</strong> in <span class="No-Break">DeepChem’s format:</span><pre class="source-code">
dataset = dc.data.NumpyDataset(X=[x.adjacency_matrix for x in features], y=[x.node_features for x in features])</pre></li>
<li><strong class="source-inline">MolGAN</strong> uses <a id="_idIndexMarker705"/>batch training, which is why we need to define an iterable <span class="No-Break">as follows:</span><pre class="source-code">
def iterbatches(epochs):
    for i in range(epochs):
        for batch in dataset.iterbatches(batch_size=gan.batch_size, pad_batches=True):
            adjacency_tensor = one_hot(batch[0], gan.edges)
            node_tensor = one_hot(batch[1], gan.nodes)
            yield {gan.data_inputs[0]: adjacency_tensor, gan.data_inputs[1]: node_tensor}</pre></li>
<li>We train<a id="_idIndexMarker706"/> the model for <span class="No-Break"><strong class="source-inline">25</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
gan.fit_gan(iterbatches(25), generator_steps=0.2)</pre></li>
<li>We generate <span class="No-Break"><strong class="source-inline">1000</strong></span><span class="No-Break"> molecules:</span><pre class="source-code">
generated_data = gan.predict_gan_generator(1000)
nmols = feat.defeaturize(generated_data)</pre></li>
<li>Then, we check whether these molecules are valid <span class="No-Break">or not:</span><pre class="source-code">
valid_mols = [x for x in generated_mols if x is not None]
print (f'{len(valid_mols)} valid molecules (out of {len((generated_mols))} generated molecules)')
<strong class="bold">31 valid molecules (out of 1000 generated molecules)</strong></pre></li>
<li>We compare them to see how many molecules <span class="No-Break">are unique:</span><pre class="source-code">
generated_smiles = [Chem.MolToSmiles(x) for x in valid_mols]
generated_smiles_viz = [Chem.MolFromSmiles(x) for x in set(generated_smiles)]
print(f'{len(generated_smiles_viz)} unique valid molecules ({len(generated_smiles)-len(generated_smiles_viz)} redundant molecules)')
<strong class="bold">24 unique valid molecules (7 redundant molecules)</strong></pre></li>
<li>We <a id="_idIndexMarker707"/>print <a id="_idIndexMarker708"/>the generated molecules in <span class="No-Break">a grid:</span><pre class="source-code">
img = Draw.MolsToGridImage(generated_smiles_viz, molsPerRow=6, subImgSize=(200, 200), returnPNG=False)</pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer532">
<img alt="Figure 11.11 – Molecules generated with MolGAN" height="683" src="image/B19153_11_011.jpg" width="1050"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Molecules generated with MolGAN</p>
<p>Despite the <a id="_idIndexMarker709"/>GAN’s improvements, this training process is quite unstable and can fail to produce any meaningful result. The code we presented is sensitive to hyperparameter changes and does <a id="_idIndexMarker710"/>not generalize well to other datasets, including the <strong class="source-inline">QM9</strong> dataset used in the <span class="No-Break">original paper.</span></p>
<p>Nonetheless, MolGAN’s concept of mixing RL and GANs can be employed beyond drug discovery to optimize any type of graph, such as computer networks, recommender systems, and <span class="No-Break">so on.</span></p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor142"/>Summary</h1>
<p>In this chapter, we saw different techniques to generate graphs. First, we explored traditional methods based on probabilities with interesting mathematical properties. However, due to their lack of expressiveness, we switched to GNN-based techniques that are much more flexible. We covered three families of deep generative models: VAE-based, autoregressive, and GAN-based methods. We introduced a model from each family to understand how they work in <span class="No-Break">real life.</span></p>
<p>Finally, we implemented a GAN-based model that combines a generator, a discriminator, and a reward network from RL. Instead of simply imitating graphs seen during training, this architecture can also optimize desired properties such as solubility. We used DeepChem and TensorFlow to create 24 unique and valid molecules. Nowadays, this pipeline is common in the drug discovery industry, where ML can drastically speed up <span class="No-Break">drug development.</span></p>
<p>In <a href="B19153_12.xhtml#_idTextAnchor144"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">Handling Heterogeneous Graphs</em>, we will explore a new kind of graph that we previously encountered in recommender systems and molecules. These heterogeneous graphs contain multiple types of nodes and/or links, which requires specific processing. They are more general than the regular graphs we talked about and particularly useful in applications such as <span class="No-Break">knowledge graphs.</span></p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor143"/>Further reading</h1>
<ul>
<li>[1] P. Erdös and A. Rényi. <em class="italic">On random graphs I</em>, Publicationes Mathematicae Debrecen, vol. 6, p. 290, 1959. Available <span class="No-Break">at </span><a href="https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf"><span class="No-Break">https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf</span></a><span class="No-Break">.</span></li>
<li>[2] E. N. Gilbert, <em class="italic">Random Graphs</em>, The Annals of Mathematical Statistics, vol. 30, no. 4, pp. 1141–1144, 1959, DOI: 10.1214/aoms/1177706098. Available <span class="No-Break">at: </span><a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full"><span class="No-Break">https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full</span></a><span class="No-Break">.</span></li>
<li>[3] Duncan J. Watts and Steven H. Strogatz. <em class="italic">Collective dynamics of small-world networks</em>, Nature, 393, pp. 440–442, 1998. Available <span class="No-Break">at </span><a href="http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf"><span class="No-Break">http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf</span></a><span class="No-Break">.</span></li>
<li>[4] M. Simonovsky and N. Komodakis. <em class="italic">GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders</em> CoRR, vol. abs/1802.03480, 2018, [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/1802.03480"><span class="No-Break">http://arxiv.org/abs/1802.03480</span></a><span class="No-Break">.</span></li>
<li>[5] Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt. <em class="italic">Constrained Graph Variational Autoencoders for Molecule Design</em>. arXiv, 2018. DOI: 10.48550/ARXIV.1805.09076. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1805.09076"><span class="No-Break">https://arxiv.org/abs/1805.09076</span></a><span class="No-Break">.</span></li>
<li>[6] T. Ma, J. Chen, and C. Xiao, Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders. arXiv, 2018. DOI: 10.48550/ARXIV.1809.02630. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1809.02630"><span class="No-Break">https://arxiv.org/abs/1809.02630</span></a><span class="No-Break">.</span></li>
<li>[7] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. <em class="italic">GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models</em>. arXiv, 2018. DOI: 10.48550/ARXIV.1802.08773. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1802.08773"><span class="No-Break">https://arxiv.org/abs/1802.08773</span></a><span class="No-Break">.</span></li>
<li>[8] N. De Cao and T. Kipf. <em class="italic">MolGAN: An implicit generative model for small molecular graphs</em>. arXiv, 2018. DOI: 10.48550/ARXIV.1805.11973. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/1805.11973"><span class="No-Break">https://arxiv.org/abs/1805.11973</span></a><span class="No-Break">.</span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer534">
</div>
</div>
</div></body></html>