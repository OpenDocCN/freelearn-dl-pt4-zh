["```py\nimport pandas as pd\ndata = pd.read_csv('assets/daily_multivariate_timeseries.csv',\n                   parse_dates=['Datetime'],\n                   index_col='Datetime')\n```", "```py\nTARGET = 'Incoming Solar'\nN_LAGS = 7\nHORIZON = 1\ninput_data = []\noutput_data = []\nfor i in range(N_LAGS, data.shape[0]-HORIZON+1):\n    input_data.append(data.iloc[i - N_LAGS:i].values)\n    output_data.append(data.iloc[i:(i+HORIZON)][TARGET])\ninput_data, output_data = np.array(input_data), np.array(output_data)\n```", "```py\nmvtseries['group_id'] = 0\nmvtseries['time_index'] = np.arange(mvtseries.shape[0])\n```", "```py\ndataset = TimeSeriesDataSet(\n    data=mvtseries,\n    group_ids=[\"group_id\"],\n    target=\"Incoming Solar\",\n    time_idx=\"time_index\",\n    max_encoder_length=7,\n    max_prediction_length=1,\n    time_varying_unknown_reals=['Incoming Solar',\n                                'Wind Dir',\n                                'Snow Depth',\n                                'Wind Speed',\n                                'Dewpoint',\n                                'Precipitation',\n                                'Vapor Pressure',\n                                'Relative Humidity',\n                                'Air Temp'],\n)\n```", "```py\ndata_loader = dataset.to_dataloader(batch_size=1, shuffle=False)\n```", "```py\nx, y = next(iter(data_loader))\nx['encoder_cont']\ny\n```", "```py\nimport pandas as pd\nmvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',\n            parse_dates=['datetime'],\n            index_col='datetime')\n```", "```py\n    mvtseries[\"target\"] = mvtseries[\"Incoming Solar\"]\n    mvtseries[\"time_index\"] = np.arange(mvtseries.shape[0])\n    mvtseries[\"group_id\"] = 0\n    ```", "```py\n    time_indices = data[\"time_index\"].values\n    train_indices, _ = train_test_split(\n        time_indices,\n        test_size=test_size,\n        shuffle=False)\n    train_indices, _ = train_test_split(train_indices,\n                                        test_size=0.1,\n                                        shuffle=False)\n    train_df = data.loc[data[\"time_index\"].isin(train_indices)]\n     train_df_mod = train_df.copy()\n    ```", "```py\n    target_scaler = StandardScaler()\n    target_scaler.fit(train_df_mod[[\"target\"]])\n    train_df_mod[\"target\"] = target_scaler.transform\n        (train_df_mod[[\"target\"]])\n    train_df_mod = train_df_mod.drop(\"Incoming Solar\", axis=1)\n     feature_names = [\n        col for col in data.columns\n        if col != \"target\" and col != \"Incoming Solar\"\n    ]\n    ```", "```py\n    training_dataset = TimeSeriesDataSet(\n        train_df_mod,\n        time_idx=\"time_index\",\n        target=\"target\",\n        group_ids=[\"group_id\"],\n        max_encoder_length=n_lags,\n        max_prediction_length=horizon,\n        time_varying_unknown_reals=feature_names,\n        scalers={name: StandardScaler()\n                 for name in feature_names},\n    )\n    loader = training_dataset.to_dataloader(batch_size=batch_size,\n                                            shuffle=False)\n    ```", "```py\n    N_LAGS = 7\n    HORIZON = 1\n    BATCH_SIZE = 10\n    data_loader = create_training_set(\n        data=mvtseries,\n        n_lags=N_LAGS,\n        horizon=HORIZON,\n        batch_size=BATCH_SIZE,\n        test_size=0.3\n    )\n    ```", "```py\n    import torch\n    from torch import nn\n    class LinearRegressionModel(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super(LinearRegressionModel, self).__init__()\n            self.linear = nn.Linear(input_dim, output_dim)\n        def forward(self, X):\n            X = X.view(X.size(0), -1)\n            return self.linear(X)\n    ```", "```py\n    num_vars = mvtseries.shape[1] + 1\n    model = LinearRegressionModel(N_LAGS * num_vars, HORIZON)\n    ```", "```py\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        for batch in data_loader:\n            x, y = batch\n            X = x[\"encoder_cont\"].squeeze(-1)\n            y_pred = model(X)\n            y_pred = y_pred.squeeze(1)\n            y_actual = y[0].squeeze(1)\n            loss = criterion(y_pred, y_actual)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        print(f\"epoch: {epoch + 1}, loss = {loss.item():.4f}\")\n    ```", "```py\nimport pandas as pd\nmvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',\n                        parse_dates=['datetime'],\n                        index_col='datetime')\nn_vars = mvtseries.shape[1]\n```", "```py\nimport lightning.pytorch as pl\nclass ExampleDataModule(pl.LightningDataModule):\n    def __init__(self,\n                 data: pd.DataFrame,\n                 batch_size: int):\n        super().__init__()\n        self.data = data\n        self.batch_size = batch_size\n    def setup(self, stage=None):\n        pass\n    def train_dataloader(self):\n        pass\n    def val_dataloader(self):\n        pass\n    def test_dataloader(self):\n        pass\n    def predict_dataloader(self):\n        pass\n```", "```py\nclass ExampleModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.network = ...\n    def forward(self, x):\n        pass\n    def training_step(self, batch, batch_idx):\n        pass\n    def validation_step(self, batch, batch_idx):\n        pass\n    def test_step(self, batch, batch_idx):\n        pass\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        pass\n    def configure_optimizers(self):\n        pass\n```", "```py\nfrom pytorch_forecasting import TimeSeriesDataSet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nclass MultivariateSeriesDataModule(pl.LightningDataModule):\n    def __init__(\n            self,\n            data: pd.DataFrame,\n            n_lags: int,\n            horizon: int,\n            test_size: float,\n            batch_size: int\n    ):\n        super().__init__()\n        self.data = data\n        self.feature_names = \n            [col for col in data.columns if col != \"Incoming Solar\"]\n        self.batch_size = batch_size\n        self.test_size = test_size\n        self.n_lags = n_lags\n        self.horizon = horizon\n        self.target_scaler = StandardScaler()\n        self.training = None\n        self.validation = None\n        self.test = None\n        self.predict_set = None\n```", "```py\ndef setup(self, stage=None):\n    self.preprocess_data()\n    train_indices, val_indices, test_indices = self.split_data()\n    train_df = self.data.loc\n        [self.data[\"time_index\"].isin(train_indices)]\n    val_df = self.data.loc[self.data[\"time_index\"].isin(val_indices)]\n    test_df = self.data.loc\n        [self.data[\"time_index\"].isin(test_indices)]\n     self.target_scaler.fit(train_df[[\"target\"]])\n    self.scale_target(train_df, train_df.index)\n    self.scale_target(val_df, val_df.index)\n    self.scale_target(test_df, test_df.index)\n    train_df = train_df.drop(\"Incoming Solar\", axis=1)\n    val_df = val_df.drop(\"Incoming Solar\", axis=1)\n    test_df = test_df.drop(\"Incoming Solar\", axis=1)\n    self.training = TimeSeriesDataSet(\n        train_df,\n        time_idx=\"time_index\",\n        target=\"target\",\n        group_ids=[\"group_id\"],\n        max_encoder_length=self.n_lags,\n        max_prediction_length=self.horizon,\n        time_varying_unknown_reals=self.feature_names,\n        scalers={name: StandardScaler() for name in \n            self.feature_names},\n    )\n    self.validation = TimeSeriesDataSet.from_dataset\n        (self.training, val_df)\n    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)\n    self.predict_set = TimeSeriesDataSet.from_dataset(\n    self.training, self.data, predict=True)\n```", "```py\n    def train_dataloader(self):\n        return self.training.to_dataloader\n            (batch_size=self.batch_size, shuffle=False)\n    def val_dataloader(self):\n        return self.validation.to_dataloader\n            (batch_size=self.batch_size, shuffle=False)\n    def test_dataloader(self):\n        return self.test.to_dataloader\n            (batch_size=self.batch_size, shuffle=False)\n    def predict_dataloader(self):\n        return self.predict_set.to_dataloader\n            (batch_size=1, shuffle=False)\n```", "```py\nimport torch\nfrom torch import nn\nclass FeedForwardNet(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8),\n            nn.ReLU(),\n            nn.Linear(8, output_size),\n        )\n    def forward(self, X):\n        X = X.view(X.size(0), -1)\n        return self.net(X)\n```", "```py\nfrom pytorch_forecasting.models import BaseModel\nclass FeedForwardModel(BaseModel):\n    def __init__(self, input_dim: int, output_dim: int):\n        self.save_hyperparameters()\n        super().__init__()\n        self.network = FeedForwardNet(\n            input_size=input_dim,\n            output_size=output_dim,\n        )\n        self.train_loss_history = []\n        self.val_loss_history = []\n        self.train_loss_sum = 0.0\n        self.val_loss_sum = 0.0\n        self.train_batch_count = 0\n        self.val_batch_count = 0\n    def forward(self, x):\n        network_input = x[\"encoder_cont\"].squeeze(-1)\n        prediction = self.network(network_input)\n        output = self.to_network_output(prediction=prediction)\n        return output\n```", "```py\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    y_pred = self(x).prediction\n    y_pred = y_pred.squeeze(1)\n    y_actual = y[0].squeeze(1)\n    loss = F.mse_loss(y_pred, y_actual)\n    self.train_loss_sum += loss.item()\n    self.train_batch_count += 1\n    self.log(\"train_loss\", loss)\n    return loss\ndef configure_optimizers(self):\n    return torch.optim.Adam(self.parameters(), lr=0.01)\n```", "```py\ndef validation_step(self, batch, batch_idx):\n    x, y = batch\n    y_pred = self(x).prediction\n    y_pred = y_pred.squeeze(1)\n    y_actual = y[0].squeeze(1)\n    loss = F.mse_loss(y_pred, y_actual)\n    self.val_loss_sum += loss.item()\n    self.val_batch_count += 1\n    self.log(\"val_loss\", loss)\n    return loss\ndef test_step(self, batch, batch_idx):\n    x, y = batch\n    y_pred = self(x).prediction\n    y_pred = y_pred.squeeze(1)\n    y_actual = y[0].squeeze(1)\n    loss = F.mse_loss(y_pred, y_actual)\n    self.log(\"test_loss\", loss)\n```", "```py\ndef predict_step(self, batch, batch_idx):\n    x, y = batch\n    y_pred = self(x).prediction\n    y_pred = y_pred.squeeze(1)\n    return y_pred\n```", "```py\ndatamodule = MultivariateSeriesDataModule(data=mvtseries,\n                                          n_lags=7,\n                                          horizon=1,\n                                          batch_size=32,\n                                          test_size=0.3)\nmodel = FeedForwardModel(input_dim=N_LAGS * n_vars, output_dim=1)\ntrainer = pl.Trainer(max_epochs=30)\ntrainer.fit(model, datamodule)\n```", "```py\ntrainer.test(model=model, datamodule=datamodule)\nforecasts = trainer.predict(model=model, datamodule=datamodule)\n```", "```py\nscalers={name: StandardScaler() for name in self.feature_names}\n```", "```py\nclass FeedForwardNetAlternative(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.l1 = nn.Linear(input_size, 16)\n        self.relu_l1 = nn.ReLU()\n        self.l2 = nn.Linear(16, 8)\n        self.relu_l2 = nn.ReLU()\n        self.l3 = nn.Linear(8, output_size)\n    def forward(self, x):\n        X = X.view(X.size(0), -1)\n        l1_output = self.l1(x)\n        l1_actf_output = self.relu_l1(l1_output)\n        l2_output = self.l2(l1_actf_output)\n        l2_actf_output = self.relu_l2(l2_output)\n        l3_output = self.l3(l2_actf_output)\n        return l3_output\n```", "```py\nclass MultivariateLSTM(pl.LightningModule):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n            batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    def forward(self, x):\n        h0 = torch.zeros(self.lstm.num_layers, x.size(0), \n            self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.lstm.num_layers, x.size(0), \n            self.hidden_dim).to(self.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n```", "```py\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_pred = self(x['encoder_cont'])\n        y_pred = y_pred.squeeze(1)\n        loss = F.mse_loss(y_pred, y[0])\n        self.log('train_loss', loss)\n        return loss\n```", "```py\nn_vars = mvtseries.shape[1] - 1\nmodel = MultivariateLSTM(input_dim=n_vars,\n                         hidden_dim=10,\n                         num_layers=1,\n                         output_dim=1)\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(model, datamodule)\ntrainer.test(model, datamodule.test_dataloader())\nforecasts = trainer.predict(model=model, datamodule=datamodule)\n```", "```py\npip install -U tensorboard\n```", "```py\n    from lightning.pytorch.loggers import TensorBoardLogger\n    import lightning.pytorch as pl\n    logger = TensorBoardLogger('logs/')\n    trainer = pl.Trainer(logger=logger)\n    ```", "```py\n    tensorboard --logdir=logs/\n    ```", "```py\npip install sktime\n```", "```py\nfrom sklearn.metrics import mean_squared_error\nfrom sktime.performance_metrics.forecasting \nimport mean_absolute_scaled_error, MeanAbsolutePercentageError\nimport numpy as np\n```", "```py\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\ny_pred = model(X_test).detach().numpy()\ny_true = y_test.detach().numpy()\nrmse_sklearn = np.sqrt(mean_squared_error(y_true, y_pred)) print(f\"RMSE (scikit-learn): {rmse_sklearn}\")\nmape = mean_absolute_percentage_error(y_true, y_pred) \nprint(f\"MAPE: {mape}\")\nmase_sktime = mean_absolute_scaled_error(y_true, y_pred) \nprint(f\"MASE (sktime): {mase_sktime}\")\nsmape_sktime = symmetric_mean_absolute_percentage_error\n    (y_true, y_pred)\n print(f\"SMAPE (sktime): {smape_sktime}\")\n```", "```py\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import EarlyStopping\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.00,\n    patience=3,\n    verbose=False,\n    mode=\"min\"\n)\ntrainer = pl.Trainer(max_epochs=100,\n                     callbacks=[early_stop_callback]) \ntrainer.fit(model, datamodule)\n```"]