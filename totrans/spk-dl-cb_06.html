<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using LSTMs in Generative Networks</h1>
                </header>
            
            <article>
                
<p><span>After reading this chapter, you will be able to accomplish the following:</span></p>
<ul>
<li>Downloading novels/books that will be used as input text</li>
<li>Preparing and cleansing data</li>
<li>Tokenizing sentences</li>
<li>Training and saving the LSTM model</li>
<li>Generating similar text using the model</li>
</ul>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><span>Due to the drawbacks of </span><strong>recurrent neural networks</strong><span> (</span><strong>RNNs</strong><span>) when it comes to backpropagation, </span><strong>Long Short-Term Memory Units</strong><span> (</span><strong>LSTMs</strong><span>) and </span><strong>Gated Recurrent Units</strong><span> (</span><strong>GRUs</strong><span>) have been gaining popularity in recent times when it comes to learning sequential input data as they are better suited to tackle problems of vanishing and exploding gradients.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading novels/books that will be used as input text</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will go the steps that we need to download the novels/books which we will use as input text for the execution of this recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<ul>
<li>Place the input data in the form of a <kbd>.txt</kbd> file in the working directory.</li>
<li>The input may be any kind of text, such as song lyrics, novels, magazine articles, and source code.</li>
<li>Most of the classical texts are no longer protected by copyright and may be downloaded for free and used in experiments. The best place to get access to free books is Project <a href="http://www.gutenberg.org/">Gutenberg</a>.</li>
<li>In this chapter, we will be using <em>The Jungle book</em> by Rudyard Kipling as the input to train our model and generate statistically similar text as output. The following screenshot shows you how to download the necessary file in <kbd>.txt</kbd> format:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1481 image-border" src="assets/cefd5b72-ded3-45fc-8617-3ac54c0ca5e8.png" style="width:107.17em;height:57.33em;"/></div>
<ul>
<li>After visiting the website and searching for the required book, click on <span class="packt_screen">Plain Text UTF-8</span> and download it. UTF-8 basically specifies the type of encoding. The text may be copied and pasted or saved directly to the working directory by clicking on the link.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Before beginning, it always helps to take a look at the data and analyze it. After looking at the data, we can see that there are a lot of punctuation marks, blank spaces, quotes, and uppercase as well as lowercase letters. We need to prepare the data first before performing any kind of analysis on it or feeding it into the LSTM network. We require a number of libraries that will make handling data easier :</p>
<ol>
<li>Import the necessary libraries by issuing the following commands:</li>
</ol>
<pre style="padding-left: 60px">from keras.preprocessing.text import Tokenizer<br/>from keras.utils import to_categorical<br/>from keras.models import Sequential<br/>from keras.layers import Dense, lSTM, Dropout, Embedding<br/>import numpy as np<br/>from pickle import dump<br/>import string</pre>
<ol start="2">
<li>The output to the preceding commands looks like the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7cf320aa-709e-4ec0-9387-3072025116e8.png" style="width:64.25em;height:36.00em;"/></div>
<ol start="3">
<li>It is always a good idea to double check the current working directory and choose the required folder as the working directory. In our case, the <kbd>.txt</kbd> file is named <kbd>junglebook.txt</kbd> and is held in the folder named <kbd>Chapter 8</kbd>. So, we will select that folder as the working directory for the whole chapter. This may be done as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/213ed09c-fcfd-4047-984d-dfd893747cc1.png" style="width:41.67em;height:15.33em;"/></div>
<ol start="4">
<li>Next, load the file into the program's memory by defining a function named <kbd>load_document</kbd>, which can be done by issuing the following commands:</li>
</ol>
<pre style="padding-left: 60px">def load_document(name):<br/>    file = open(name, 'r')<br/>    text = file.read()<br/>    file.close()<br/>    return text</pre>
<ol start="5">
<li>Use the previously defined function to load the document into memory and print the first <kbd>2000</kbd> characters of the text file using the following script:</li>
</ol>
<pre style="padding-left: 60px">input_filename = 'junglebook.txt'<br/>doc = load_document(input_filename)<br/>print(doc[:2000])</pre>
<ol start="6">
<li>Running the preceding function as well as the commands produces the output  shown in the following screenshots:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d383a5a-8337-473b-ad24-362275e51ff0.png" style="width:32.42em;height:14.25em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px"><span>The output to the above code is shown in the screenshot here:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1573 image-border" src="assets/0437d675-32ce-47d2-8cd6-638188e83b29.png" style="width:38.33em;height:37.92em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">The following screenshot is a continuation of the previous output:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1574 image-border" src="assets/0b7dafab-2a29-4a29-8081-00fb977e0a41.png" style="width:44.58em;height:44.83em;"/></div>
<ol start="7">
<li>As seen in the preceding screenshots, the first <kbd>2000</kbd> characters from the <kbd>.txt</kbd> file are printed. It is always a good idea to analyze the data by looking at it before performing any preprocessing on it. It will give a better idea of how to approach the preprocessing steps.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<ol>
<li>The <kbd>array</kbd> function will be used to handle data in the form of arrays. The <kbd>numpy</kbd> library provides this function readily.</li>
<li>Since our data is only text data, we will require the string library to handle all input data as strings before encoding the words as integers, which can be fed.</li>
<li>The <kbd>tokenizer</kbd> function will be used to split all the sentences into tokens, where each token represents a word.</li>
<li>The pickle library will be required in order to save the dictionary into a pickle file by using the <kbd>dump</kbd> function.</li>
<li><span>The <kbd>to_categorical</kbd> function from the <kbd>keras</kbd> library converts a class vector (</span><span>integers) to a binary class matrix, for example, for use with <kbd>categorical_crossentropy</kbd>, which we will require at a later stage in order to map tokens to unique integers and vice versa.</span></li>
<li>Some of the other Keras layers required in this chapter are the LSTM layer, dense layer, dropout layer, and the embedding layer. The model will be defined sequentially, for which we require the sequential model from the <kbd>keras</kbd> library.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<ul>
<li>You may also use the same model with different types of texts, such as customer reviews on websites, tweets, structured text such as source code, mathematics theories, and so on.</li>
<li>The idea of this chapter to understand how LSTMs learn long-term dependencies and how they perform better at processing sequential data when compared to recurrent neural networks.</li>
<li>Another good idea would be to input <em>Pokémon</em> names into the model and try to generate your own Pokémon names.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>More information about the different libraries used can be found at the following links:</p>
<ul>
<li><a href="https://www.scipy-lectures.org/intro/numpy/array_object.html">https://www.scipy-lectures.org/intro/numpy/array_object.html</a></li>
<li><a href="https://docs.python.org/2/library/string.html">https://docs.python.org/2/library/string.html</a></li>
<li><a href="https://wiki.python.org/moin/UsingPickle">https://wiki.python.org/moin/UsingPickle</a></li>
<li><a href="https://keras.io/preprocessing/text/">https://keras.io/preprocessing/text/</a></li>
<li><a href="https://keras.io/layers/core/">https://keras.io/layers/core/</a></li>
<li><a href="https://keras.io/layers/recurrent/">https://keras.io/layers/recurrent/<br/></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing and cleansing data</h1>
                </header>
            
            <article>
                
<p>This section of this chapter will discuss the various data preparation and text preprocessing steps involved before feeding it into the model as input. The specific way we prepare the data really depends on how we intend to model it, which in turn depends on how we intend to use it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The language model will be based on statistics and predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to the model, to, in turn, generate the next word.</p>
<p>A key decision is how long the input sequences should be. They need to be long enough to allow the model to learn the context for the words to predict. This input length will also define the length of the seed text used to generate new sequences when we use the model.</p>
<p>For the purpose of simplicity, we will arbitrarily pick a length of 50 words for the length of the input sequences.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Based on reviewing the text (which we did previously), the following are some operations that could be performed to clean and preprocess the text in the input file. We have presented a few options regarding text preprocessing. However, you may want to explore more cleaning operations as an exercise:</p>
<ul>
<li>Replace dashes <kbd>–</kbd> with whitespaces so you can split words better</li>
<li>Split words based on whitespaces</li>
<li>Remove all punctuation from the input text in order to reduce the number of unique characters in the text that is fed into the model (for example, Why? becomes Why)</li>
<li>Remove all words that are not alphabetic to remove standalone punctuation tokens and emoticons</li>
<li>Convert all words from uppercase to lowercase in order to reduce the size of the total number of tokens further and remove any discrepancies and data redundancy</li>
</ul>
<p>Vocabulary size is a decisive factor in language modeling and deciding the training time for the model. A smaller vocabulary results in a more efficient model that trains faster. While it is good to have a small vocabulary in some cases, it helps to have a larger vocabulary in other cases in order to prevent overfitting. In order to preprocess the data, we are going to need a function that takes in the entire input text, splits it up based on white spaces, removes all punctuation, normalizes all cases, and returns a sequence of tokens. For this purpose, define the <kbd>clean_document</kbd> function by issuing the following commands:</p>
<pre> import string<br/> def clean_document(doc):<br/>     doc = doc.replace('--', ' ')<br/>     tokens = doc.split()<br/>     table = str.maketrans('', '', string.punctuation)<br/>     tokens = [w.translate(table) for w in tokens]<br/>     tokens = [word for word in tokens if word.isalpha()]<br/>     tokens = [word.lower() for word in tokens]<br/>     return tokens</pre>
<ol>
<li>The previously defined function will basically take the loaded document/file as its argument and return an array of clean tokens, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7fc71d44-55e1-47bf-aa15-a10245e53dc9.png" style="width:44.00em;height:14.67em;"/></div>
<ol start="2">
<li>Next, print out some of the tokens and statistics just to develop a better understanding of what the <kbd>clean_document</kbd> function is doing. This step is done by issuing the following commands:</li>
</ol>
<pre style="padding-left: 60px">tokens = clean_document(doc)<br/>print(tokens[:200])<br/>print('Total Tokens: %d' % len(tokens))<br/>print('Total Unique Tokens: %d' % len(set(tokens)))</pre>
<ol start="3">
<li>The output of the preceding set of commands prints the first two hundred tokens and is as shown in the following screenshots:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/70c89890-2b03-4307-8469-ab2ce81fb726.png" style="width:37.58em;height:7.67em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7ac2d64a-0cb2-4175-812d-a516e1c03953.png" style="width:73.08em;height:22.17em;"/></div>
<ol start="4">
<li>Next, organize all these tokens into sequences, with each sequence containing 50 words (chosen arbitrarily) using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> length = 50 + 1<br/> sequences = list()<br/> for i in range(length, len(tokens)):<br/>     seq = tokens[i-sequence_length:i]<br/>     line = ' '.join(seq)<br/>     sequences.append(line)<br/> print('Total Sequences: %d' % len(sequences))</pre>
<p style="padding-left: 60px">The total number of sequences formed from the document may be viewed by printing them out, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0a5eb950-c902-485f-849e-adc3fd169a64.png" style="width:37.25em;height:15.00em;"/></div>
<ol start="5">
<li>Save all the generated tokens as well as sequences into a file in the working directory by defining the <kbd>save_doc</kbd> function using the following commands:</li>
</ol>
<pre style="padding-left: 60px">def save_document(lines, name):<br/>    data = '\n'.join(lines)<br/>    file = open(name, 'w')<br/>    file.write(data)<br/>    file.close()</pre>
<p style="padding-left: 60px">To save the sequences, use the following two commands:</p>
<pre style="padding-left: 60px"> output_filename = 'junglebook_sequences.txt'<br/> save_document(sequences, output_filename)</pre>
<ol start="6">
<li>This process is illustrated in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a93679f6-9abc-479d-9718-1634ceb2092c.png" style="width:37.33em;height:14.92em;"/></div>
<ol start="7">
<li>Next, load the saved document, which contains all the saved tokens and sequences, into the memory using the <kbd>load_document</kbd> function, which is defined as follows:</li>
</ol>
<pre style="padding-left: 60px">def load_document(name):<br/>    file = open(name, 'r')<br/>    text = file.read()<br/>    file.close()<br/>    return text<br/><br/># function to load document and split based on lines<br/>input_filename = 'junglebook_sequences.txt'<br/>doc = load_document(input_filename)<br/>lines = doc.split('\n')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b3dc8275-2494-4658-8aa6-5651f8259036.png" style="width:33.67em;height:14.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<ol>
<li>The <kbd>clean_document</kbd> function removes all whitespaces, punctuation, uppercase text, and quotation marks, and splits the entire document into tokens, where each token is a word.</li>
<li>By printing the total number of tokens and total unique tokens in the document, we will note that the <kbd>clean_document</kbd> function generated 51,473 tokens, out of which 5,027 tokens (or words) are unique.</li>
<li>The <kbd>save_document</kbd> function then saves all of these tokens as well as unique tokens which are required to generate our sequences of 50 words each. Note how, by looping through all the generated tokens, we are able to generate a long list of 51,422 sequences. These are the same sequences that will be used as input to train the language model.</li>
</ol>
<ol start="4">
<li>Before training the model on all 51,422 sequences, it is always a good practice to save the tokens as well as sequences to file. Once saved, the file can be loaded back into the memory using the defined <kbd>load_document</kbd> function.</li>
<li>The sequences are organized as 50 input tokens and one output token (which means that there are 51 tokens per sequence). For predicting each output token, the previous 50 tokens will be used as the input to the model. <span>We can do this by iterating over the list of tokens from token 51 onwards and taking the previous 50 tokens as a sequence, then repeating this process until the end of the list of all tokens.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Visit the following links for a better understanding of data preparation using various functions:</p>
<ul>
<li><a href="https://docs.python.org/3/library/tokenize.html">https://docs.python.org/3/library/tokenize.html</a></li>
<li><a href="https://keras.io/utils/">https://keras.io/utils/</a></li>
<li><a href="http://www.pythonforbeginners.com/dictionary/python-split">http://www.pythonforbeginners.com/dictionary/python-split</a></li>
<li><a href="https://www.tutorialspoint.com/python/string_join.htm">https://www.tutorialspoint.com/python/string_join.htm</a></li>
<li><a href="https://www.tutorialspoint.com/python/string_lower.htm">https://www.tutorialspoint.com/python/string_lower.htm</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenizing sentences</h1>
                </header>
            
            <article>
                
<p>Before defining and feeding data into an LSTM network it is important that the data is converted into a form which can be understood by the neural network. Computers understand everything in binary code (0s and 1s) and therefore, the textual or data in string format needs to be converted into one hot encoded variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For understanding how one hot encoding works, visit the following links:</p>
<ul>
<li><a href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/">https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"/></li>
<li><a href="https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python">https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python</a></li>
<li><a href="https://www.ritchieng.com/machinelearning-one-hot-encoding/">https://www.ritchieng.com/machinelearning-one-hot-encoding/</a></li>
<li><a href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f">https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>After the going through the previous section you should be able to clean the entire corpus and split up sentences. The next steps which involve one hot encoding and tokenizing sentences can be done in the following manner:</p>
<ol>
<li>Once the tokens and sequences are saved to a file and loaded into memory, they have to be encoded as integers since t<span>he word embedding layer in the model expects input sequences to be comprised of integers and not strings.</span></li>
<li><span>This is done by mapping each word in the vocabulary to a unique integer and encoding the input sequences. Later, while making predictions, the predictions can be converted (or mapped) back to numbers to look up their associated words in the same mapping and reverse map back from integers to words.</span></li>
<li>To perform this encoding, utilize the<span> </span><kbd>Tokenizer</kbd><span> </span>class in the Keras API. Before encoding, the tokenizer must be trained on the entire dataset so it finds all the unique tokens and assigns each token a unique integer. The commands to do so as are  follows:</li>
</ol>
<pre style="padding-left: 60px">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(lines)<br/>sequences = tokenizer.texts_to_sequences(lines)</pre>
<ol start="4">
<li><span>You also need to calculate the size of the vocabulary before defining the embedding layer later. This is determined by calculating the size of the mapping dictionary.</span></li>
<li>Therefore, when specifying the vocabulary size to the Embedding layer, specify it as 1 larger than the actual vocabulary. The vocabulary size is therefore defined as follows:</li>
</ol>
<pre style="padding-left: 60px">vocab_size = len(tokenizer.word_index) + 1<br/>print('Vocabulary size : %d' % vocab_size)</pre>
<ol start="6">
<li>
<p>Now that once the input sequences have been encoded, they need to be separated into input and output elements, which can be done by array slicing.</p>
</li>
<li>
<p><span>After separating, one hot encode the output word. This means converting it from an integer to an n-dimensional vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the word's integer value. Keras provides the </span><kbd>to_categorical()</kbd><span> function, which can be used to one hot encode the output words for each input-output sequence pair.</span></p>
</li>
<li><span>Finally, specify to the Embedding layer how long input sequences are. We know that there are 50 words because the model was designed by specifying the sequence length as 50, but a good generic way to specify the sequence length is to use the second dimension (number of columns) of the input data’s shape.</span></li>
<li><span>This can be done by issuing the following commands:</span></li>
</ol>
<p> </p>
<pre style="padding-left: 60px">sequences = array(sequences)<br/>Input, Output = sequences[:,:-1], sequences[:,-1]<br/>Output = to_categorical(Output, num_classes=vocab_size)<br/>sequence_length = Input.shape[1]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section will describe the outputs you must see on executing the commands in the previous section:</p>
<ol>
<li>After running the commands for tokenizing the sentences and calculating vocabulary length you must see an output as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fc6fb70e-e203-4bbe-9352-b1f710c3cb26.png" style="width:37.25em;height:13.50em;"/></div>
<ol start="2">
<li>Words are assigned values starting from 1 up to the total number of words (for example, 5,027 in this case). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index. The index of the word at the end of the vocabulary will be 5,027; that means the array must be 5,027 + 1 in length.</li>
<li>The output after array slicing and separating sentences into sequences of 50 words per sequence must look like the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a23d1d44-9eef-43c6-aed7-84cd516d9229.png" style="width:40.42em;height:6.67em;"/></div>
<ol start="4">
<li><span>The <kbd>to_categorical()</kbd> function is used so that the model learns to predict the probability distribution for the next word.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>More information on reshaping arrays in Python can be found at the following links:</p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html">https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html</a></li>
<li><a href="https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/">https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and saving the LSTM model</h1>
                </header>
            
            <article>
                
<p>You can now train a statistical language model from the prepared data.</p>
<p>The model that will be trained is a neural language model. It has a few unique characteristics:</p>
<ul>
<li>It uses a distributed representation for words so that different words with similar meanings will have a similar representation</li>
<li>It learns the representation at the same time as learning the model</li>
<li>It learns to predict the probability for the next word using the context of the previous 50 words</li>
</ul>
<p>Specifically, you will use an Embedding Layer to learn the representation of words, and a <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) recurrent neural network to learn to predict words based on their context.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is the size of the embedding vector space.</p>
<p><span>Common values are 50, 100, and 300. We will use 100 here, but consider testing smaller or larger values and evaluating metrics for those values.</span></p>
<p>The network will be comprised of the following:</p>
<ul>
<li>
<p>Two LSTM hidden layers with 200 memory cells each. More memory cells and a deeper network may achieve better results.</p>
</li>
<li>
<p><span>A dropout layer with a dropout of 0.3 or 30%, which will aid the network to depend less on each neuron/unit and reduce overfitting the data.</span></p>
</li>
<li>
<p><span>A dense fully connected layer with 200 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence.</span></p>
</li>
<li>
<p><span>The output layer, which predicts the next word as a single vector of the size of the vocabulary with a probability for each word in the vocabulary.</span></p>
</li>
<li>
<p><span>A softmax classifier is used in the second dense or fully connected layer to ensure the outputs have the characteristics of normalized probabilities (such as between 0 and 1).</span></p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>The model is defined using the following commands and is also illustrated in the following screenshot:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Embedding(vocab_size, 100, input_length=sequence_length))<br/>model.add(LSTM(200, return_sequences=True))<br/>model.add(LSTM(200))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(200, activation='relu'))<br/>model.add(Dense(vocab_size, activation='softmax'))<br/>print(model.summary())</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/89b30c21-36b6-4d4e-803a-eb80c3dcb512.png" style="width:42.92em;height:34.42em;"/></div>
<ol start="2">
<li>Print the model summary just to ensure that the model is constructed as intended.</li>
<li><span>Compile the model, specifying the categorical cross entropy loss needed to fit the model. The number of epochs is set to 75 and the model is trained in mini batches with a batch size of 250. This is done using the following commands:</span></li>
</ol>
<pre style="padding-left: 60px"> model.compile(loss='categorical_crossentropy', optimizer='adam', <br/>        metrics=['accuracy'])<br/><br/> model.fit(Input, Output, batch_size=250, epochs=75)</pre>
<ol start="4">
<li>The output of the preceding commands is illustrated in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1486 image-border" src="assets/a495a034-ddc3-4bb8-a358-30d6a7f422f9.png" style="width:70.58em;height:35.42em;"/></div>
<ol start="5">
<li>Once the model is done compiling, it is saved using the following commands:</li>
</ol>
<pre style="padding-left: 60px">model.save('junglebook_trained.h5')<br/><br/>dump(tokenizer, open('tokenizer.pkl', 'wb'))</pre>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1484 image-border" src="assets/78e7b73d-812f-46db-8f49-10f95283d075.png" style="width:38.75em;height:7.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<ol>
<li>The model is built using the <kbd>Sequential()</kbd> function in the Keras framework. The first layer in the model is an embedding layer that takes in the vocabulary size, vector dimension, and the input sequence length as its arguments.</li>
</ol>
<ol start="2">
<li>The next two layers are LSTM layers with 200 memory cells each. More memory cells and a deeper network can be experimented with to check if it improves accuracy.</li>
<li>The next layer is a dropout layer with a dropout probability of 30%, which means that there is a 30% chance a certain memory unit is not used during training. This prevents overfitting of data. Again, the dropout probabilities can be played with and tuned accordingly.</li>
<li>The final two layers are two fully connected layers. The first one has a <kbd>relu</kbd> activation function and the second has a softmax classifier. The model summary is printed to check whether the model is built according to requirements.</li>
<li>Notice that in this case, the total number of trainable parameters are 2,115,228. The model summary also shows the number of parameters that will be trained by each layer in the model.</li>
<li>The model is trained in mini batches of 250 over 75 epochs, in our case, to minimize training time. Increasing the number of epochs to over 100 and utilizing smaller batches while training greatly improves the model's accuracy while simultaneously reducing loss.</li>
<li><span>During training, you will see a summary of performance, including the loss and accuracy evaluated from the training data at the end of each batch update. In our case, after running the model for 75 epochs, we obtained an accuracy of close to 40%.</span></li>
<li>The aim of the model is not to remember the text with 100% accuracy, but rather to capture the properties of the input text, such as long-term dependencies and structures that exist in natural language and sentences.</li>
<li>The model, after it is done training, is saved in the working directory named <kbd>junglebook_trained.h5</kbd>.</li>
<li>We also require the mapping of words to integers when the model is later loaded into memory to make predictions. This is present in the <kbd>Tokenizer</kbd> object, which is also saved using the <kbd>dump ()</kbd> function in the <kbd>Pickle</kbd> library.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Jason Brownlee's blogs on Machine Learning Mastery have a lot of useful information on developing, training, and tuning machine learning models for natural language processing. They can be found at the following links:<br/>
<a href="https://machinelearningmastery.com/deep-learning-for-nlp/">https://machinelearningmastery.com/deep-learning-for-nlp/</a><br/>
<a href="https://machinelearningmastery.com/lstms-with-python/">https://machinelearningmastery.com/lstms-with-python/</a><br/>
<a href="https://machinelearningmastery.com/deep-learning-for-nlp/">https://machinelearningmastery.com/blog/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Further information about different keras layers and other functions used in this section can be found at the following links:</p>
<ul>
<li><a href="https://keras.io/models/sequential/">https://keras.io/models/sequential/</a></li>
<li><a href="https://docs.python.org/2/library/pickle.html">https://docs.python.org/2/library/pickle.html</a></li>
<li><a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a></li>
<li><a href="https://keras.io/models/model/">https://keras.io/models/model/<br/></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating similar text using the model</h1>
                </header>
            
            <article>
                
<p>Now that you have a trained language model, it can be used. In this case, you can use it to generate new sequences of text that have the same statistical properties as the source text. This is not practical, at least not for this example, but it gives a concrete example of what the language model has learned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<ol>
<li>Begin by loading the training sequences again. You may do so by using the <kbd>load_document()</kbd> function, which we developed initially. This is done by using the following code:</li>
</ol>
<pre style="padding-left: 60px">def load_document(name):<br/>    file = open(name, 'r')<br/>    text = file.read()<br/>    file.close()<br/>    return text<br/><br/># load sequences of cleaned text<br/>input_filename = 'junglebook_sequences.txt'<br/>doc = load_document(input_filename)<br/>lines = doc.split('\n')</pre>
<p style="padding-left: 60px"><span>The output of the preceding code is illustrated in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c88a273c-a0d6-4be1-8cbb-1b15be720983.png" style="width:37.50em;height:15.17em;"/></div>
<ol start="2">
<li>Note that the input filename is now <kbd>'junglebook_sequences.txt'</kbd>, which will load the saved training sequences into the memory. <span>We need the text so that we can choose a source sequence as input to the model for generating a new sequence of text.</span></li>
<li>
<p>The model will require 50 words as input.</p>
<p>Later, the expected length of input needs to be specified. This can be determined from the input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line, as follows:<br/>
<kbd>sequence_length = len(lines[0].split()) - 1<br/></kbd></p>
</li>
<li>Next, load the trained and saved model into memory by executing the following commands:</li>
</ol>
<pre style="padding-left: 60px"> from keras.models import load_model<br/> model = load_model('junglebook.h5')</pre>
<ol start="5">
<li>
<p>The first step in generating text is preparing a seed input. Select a random line of text from the input text for this purpose. Once selected, print it so that you have some idea of what was used. This is done as follows:</p>
</li>
</ol>
<pre style="padding-left: 60px"><span>from random import randint</span><br/><span>seed_text = lines[randint(0,len(lines))]</span><br/><span>print(seed_text + '\n')</span></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7b4a194c-a290-49eb-8e61-2d4f94174bdd.png" style="width:113.42em;height:30.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>You are now ready to generate new words, one at a time. First, encode the seed text to integers using the same tokenizer that was used when training the model, which is done using the following code:<br/>
<kbd>encoded = tokenizer.texts_to_sequences([seed_text])[0]</kbd></p>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8eb637cf-8e57-41ef-a90f-bf076e3f1cdf.png" style="width:41.83em;height:4.83em;"/></div>
<ol start="2">
<li><span>The model can predict the next word directly by calling </span><kbd>model.predict_classes()</kbd><span>, which will return the index of the word with the highest probability:</span></li>
</ol>
<pre style="padding-left: 60px"> prediction = model.predict_classes(encoded, verbose=0)<span><br/></span></pre>
<ol start="3">
<li><span>Look up the index in the Tokenizers mapping to get the associated word, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"> out_word = ''<br/> for word, index in tokenizer.word_index.items():<br/>         if index == prediction:<br/>                 out_word = word<br/>                 break</pre>
<ol start="4">
<li><span>Append this word to the seed text and repeat the process. Importantly, the input sequence is going to get too long. We can truncate it to the desired length after the input sequence has been encoded to integers. Keras provides the <kbd>pad_sequences()</kbd> function which we can use to perform this truncation, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"> encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')<span><br/></span></pre>
<ol start="5">
<li><span>Wrap all of this into a function called </span><kbd>generate_sequence()</kbd><span> that takes as input the model, the tokenizer, the input sequence length, the seed text, and the number of words to generate. It then returns a sequence of words generated by the model. You may use the following code to do so:</span></li>
</ol>
<pre style="padding-left: 60px"><span> </span>from random import randint<br/> from pickle import load<br/> from keras.models import load_model<br/> from keras.preprocessing.sequence import pad_sequences<br/> <br/> def load_document(filename):<br/>     file = open(filename, 'r')<br/>     text = file.read()<br/>     file.close()<br/>     return text<br/> <br/> def generate_sequence(model, tokenizer, sequence_length, seed_text, n_words):<br/>     result = list()<br/>     input_text = seed_text<br/>     for _ in range(n_words):<br/>         encoded = tokenizer.texts_to_sequences([input_text])[0]<br/>         encoded = pad_sequences([encoded], maxlen=seq_length,                 truncating='pre')<br/>         prediction = model.predict_classes(encoded, verbose=0)<br/>         out_word = ''<br/>             for word, index in tokenizer.word_index.items():<br/>                 if index == prediction:<br/>                     out_word = word<br/>                     break<br/>      input_text += ' ' + out_word<br/>      result.append(out_word)<br/>    return ' '.join(result)<br/>     <br/> input_filename = 'junglebook_sequences.txt'<br/> doc = load_document(input_filename)<br/> lines = doc.split('\n')<br/> seq_length = len(lines[0].split()) - 1</pre>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1639 image-border" src="assets/e7d8b604-cebd-4406-bef0-97f6aed4fe00.png" style="width:44.67em;height:38.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>We are now ready to generate a sequence of new words, given that we have some seed text :</span></p>
<ol>
<li>Start by loading the model into memory again using the following command:</li>
</ol>
<pre style="padding-left: 60px"> model = load_model('junglebook.h5')</pre>
<ol start="2">
<li>Next, load the tokenizer by typing the following command:</li>
</ol>
<pre style="padding-left: 60px"> tokenizer = load(open('tokenizer.pkl', 'rb'))</pre>
<ol start="3">
<li>Select a seed text randomly by using the following command:</li>
</ol>
<pre style="padding-left: 60px"> seed_text = lines[randint(0,len(lines))]<br/> print(seed_text + '\n')</pre>
<ol start="4">
<li>Finally, a new sequence is generated by using the following command:</li>
</ol>
<pre style="padding-left: 60px"> generated = generate_sequence(model, tokenizer, sequence_length,             seed_text, 50)<br/> print(generated)</pre>
<ol start="5">
<li>On printing the generated sequence, you will see an output similar to the one shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3d641bd0-01a3-47a7-aa6b-d66f83034217.png" style="width:88.50em;height:29.67em;"/></div>
<ol start="6">
<li>The model first prints 50 words of the random seed text followed by 50 words of the generated text. In this case, the random seed text is as follows:<br/>
<em>Baskets of dried grass and put grasshoppers in them or catch two praying mantises and make them fight or string a necklace of red and black jungle nuts or watch a lizard basking on a rock or a snake hunting a frog near the wallows then they sing long long songs</em><strong><em><br/></em></strong><br/>
The 50 words of text generated by the model, in this case, are as follows:<br/>
<em>with odd native quavers at the end of the review and the hyaena whom he had seen the truth they feel twitched to the noises round him for a picture of the end of the ravine and snuffing bitten and best of the bulls at the dawn is a native</em></li>
<li>Note how the model outputs a sequence of random words it generated based on what it learned from the input text. You will also notice that the model does a reasonably good job of mimicking the input text and generating its own stories. Though the text does not make much sense, it gives valuable insight into how the model learns to place statistically similar words next to each other.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<ul>
<li>Upon changing the random seed that was set, the output generated by the network also changes. You may not get the exact same output text as the preceding example, but it will be very similar to the input used to train the model.</li>
<li>The following are some screenshots of different results that were obtained by running the generated text piece multiple times:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/18257086-668e-4abb-9f15-4abb5826d268.png" style="width:66.33em;height:22.33em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8d7ad7ba-8b9b-4bd6-9b5b-cc3b54a721a9.png" style="width:73.33em;height:25.08em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1485 image-border" src="assets/beedf83f-1eb8-47cb-972f-fdc105c84b59.png" style="width:64.67em;height:21.92em;"/></div>
<ul>
<li>The model even generates its own version of the project Gutenberg license, as can be seen in the following screenshot:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c954fc4c-15a4-4076-9d34-2e6512aa3827.png" style="width:57.50em;height:19.25em;"/></div>
<ul>
<li>The model's accuracy can be improved to about 60% by increasing the number of epochs from about 100 to 200. Another method to increase the learning is by training the model in mini batches of about 50 and 100. Try to play around with the different hyperparameters and activation functions to see what affects the results in the best possible way.</li>
<li>The model may also be made denser by including more LSTM and dropout layers while defining the model. However, know that it will only increase the training time if the model is more complex and runs over more epochs. </li>
<li>After much experimentation, the ideal batch size was found to be between 50 to 100, and the ideal number of epochs to train the model was determined to be between 100 and 200.</li>
<li>There is no definitive way of performing the preceding task. You can also experiment with different text inputs to the model such as tweets, customer reviews, or HTML code.</li>
<li>Some of the other tasks that can be performed include using a simplified vocabulary (such as with all the stopwords removed) to further enhance the unique words in the dictionary; tuning the size of the embedding layer and the number of memory cells in the hidden layers; and extending the model to use a pre-trained model such as Google's Word2Vec (pre-trained word model) to see whether it results in a better model.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>More information about the various functions and libraries used in the final section of the chapter can be found by visiting the following links:</p>
<ul>
<li><a href="https://keras.io/preprocessing/sequence/">https://keras.io/preprocessing/sequence/</a></li>
<li><a href="https://wiki.python.org/moin/UsingPickle">https://wiki.python.org/moin/UsingPickle</a></li>
<li><a href="https://docs.python.org/2/library/random.html">https://docs.python.org/2/library/random.html</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model">https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model</a></li>
</ul>


            </article>

            
        </section>
    </body></html>