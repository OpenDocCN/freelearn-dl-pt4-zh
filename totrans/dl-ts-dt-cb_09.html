<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer057">
			<h1 id="_idParaDest-346" class="chapter-number"><a id="_idTextAnchor463"/>9</h1>
			<h1 id="_idParaDest-347"><a id="_idTextAnchor464"/>Deep Learning for Time Series Anomaly Detection</h1>
			<p>In this chapter, we’ll delve into anomaly detection problems using time series data. This task involves detecting rare observations that are significantly different from most samples in a dataset. We’ll explore different approaches to tackle this problem, such as prediction-based methods or reconstruction-based methods. This includes using <a id="_idIndexMarker579"/>powerful <a id="_idIndexMarker580"/>methods<a id="_idIndexMarker581"/> such as <strong class="bold">autoencoders</strong> (<strong class="bold">AEs</strong>), <strong class="bold">variational AEs</strong> (<strong class="bold">VAEs</strong>), or <strong class="bold">generative adversarial </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GANs</strong></span><span class="No-Break">).</span></p>
			<p>By the end of this chapter, you’ll be able to define time series anomaly detection problems using different approaches <span class="No-Break">with Python.</span></p>
			<p> The chapter covers the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Time series anomaly detection with <strong class="bold">Autoregressive Integrated Moving </strong><span class="No-Break"><strong class="bold">Average</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ARIMA</strong></span><span class="No-Break">)</span></li>
				<li>Prediction-based anomaly detection using <strong class="bold">deep </strong><span class="No-Break"><strong class="bold">learning</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DL</strong></span><span class="No-Break">)</span></li>
				<li>Anomaly detection using a <strong class="bold">long short-term memory</strong> (<span class="No-Break"><strong class="bold">LSTM</strong></span><span class="No-Break">) AE</span></li>
				<li>Building an AE <span class="No-Break">using PyOD</span></li>
				<li>Creating a VAE for time series <span class="No-Break">anomaly detection</span></li>
				<li>Using GANs for time series <span class="No-Break">anomaly detection</span></li>
			</ul>
			<h1 id="_idParaDest-348"><a id="_idTextAnchor465"/>Technical requirements</h1>
			<p>The models developed in this chapter are based on different frameworks. First, we show how to develop prediction-based methods using the <strong class="source-inline">statsforecast</strong> and <strong class="source-inline">neuralforecast</strong> libraries. Other methods, such as an LSTM AE, will be explored using the PyTorch Lightning ecosystem. Finally, we’ll also use the PyOD library to create anomaly detection models based on approaches such as GANs or VAEs. Of course, we also rely on typical data manipulation libraries such as <strong class="source-inline">pandas or NumPy.</strong> The following list contains all the required libraries for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break"> (1.3.2)</span></li>
				<li><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> (2.1.3)</span></li>
				<li><span class="No-Break">NumPy (1.26.2)</span></li>
				<li><span class="No-Break"><strong class="source-inline">statsforecast</strong></span><span class="No-Break"> (1.6.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">datasetsforecast</strong></span><span class="No-Break"> (0.08)</span></li>
				<li><span class="No-Break"><strong class="source-inline">0neuralforecast</strong></span><span class="No-Break"> (1.6.4)</span></li>
				<li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> (2.1.1)</span></li>
				<li>PyTorch <span class="No-Break">Lightning (2.1.2)</span></li>
				<li>PyTorch <span class="No-Break">Forecasting (1.0.0)</span></li>
				<li><span class="No-Break">PyOD (1.1.2)</span></li>
			</ul>
			<p>The code and datasets used in this chapter can be found at the following GitHub <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookboo<span id="_idTextAnchor466"/><span id="_idTextAnchor467"/>k</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor468"/>Time series anomaly detection with ARIMA</h1>
			<p>Time series<a id="_idIndexMarker582"/> anomaly<a id="_idIndexMarker583"/> detection is an important task in application domains such as healthcare or manufacturing, among many others. Anomaly detection methods aim to identify observations that do not conform to the typical behavior of a dataset. In practice, anomalies can represent phenomena such as faults in machinery or fraudulent activity. Anomaly detection is a common<a id="_idIndexMarker584"/> task in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), and it has a few dedicated methods when it involves time series data. This type of dataset and the patterns therein can evolve over time, which complicates the modeling process and the effectiveness of the detectors. Statistical learning methods for time series anomaly detection problems usually follow a prediction-based approach or a reconstruction-based approach. In this recipe, we describe how to use an ARIMA method to create a prediction-based anomaly detection system for univariate <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor469"/>Getting ready</h2>
			<p>We’ll focus on a univariate time series from the <strong class="source-inline">M3</strong> dataset, which is available in the <strong class="source-inline">datasetsforecast</strong> library. Here’s how to get <span class="No-Break">this data:</span></p>
			<pre class="source-code">
from datasetsforecast.m3 import M3
dataset, *_ = M3.load('./data', 'Quarterly')
q1 = dataset.query('unique_id=="Q1"')</pre>			<p>In the preceding code, we start by loading the <strong class="source-inline">M3</strong> dataset using the <strong class="source-inline">load()</strong> method. Then, we<a id="_idIndexMarker585"/> use<a id="_idIndexMarker586"/> the <strong class="source-inline">query()</strong> method to get the univariate time series with an identifier (<strong class="source-inline">unique_id</strong> column) equal to <strong class="source-inline">Q1</strong>. Now, let’s see how to detect anomalies in <span class="No-Break">this d<a id="_idTextAnchor470"/>ataset.</span></p>
			<h2 id="_idParaDest-351"><a id="_idTextAnchor471"/>How to do it…</h2>
			<p>We’ll build a forecasting model and use the corresponding prediction intervals to <span class="No-Break">detect anomalies.</span></p>
			<ol>
				<li>We start by creating a forecasting model. While any model would work, in this recipe, we focus on ARIMA. Here’s how to define this model using the <span class="No-Break"><strong class="source-inline">statsforecast</strong></span><span class="No-Break"> library:</span><pre class="source-code">
from statsforecast import StatsForecast
from statsforecast.models import AutoARIMA
models = [AutoARIMA(season_length=4)]
sf = StatsForecast(
    df=q1,
    models=models,
    freq='Q',
    n_jobs=1,
)</pre></li>				<li>Now, we are ready to fit the model and get <span class="No-Break">the forecasts:</span><pre class="source-code">
forecasts = sf.forecast(h=8, level=[99], 
    fitted=True).reset_index()
insample_forecasts = sf.forecast_fitted_values().reset_index()</pre><p class="list-inset">First, we use the <strong class="source-inline">forecast()</strong> method to get the predictions. In this example, we set the forecasting horizon to <strong class="source-inline">8</strong> (<strong class="source-inline">h=8</strong>). We also pass two additional parameters: <strong class="source-inline">level=[99]</strong>, which means that we also want the model to predict<a id="_idIndexMarker587"/> the <a id="_idIndexMarker588"/>intervals with a <strong class="source-inline">99</strong>% confidence level; <strong class="source-inline">fitted=True</strong>, which tells the model to compute the training forecasts. We use the <strong class="source-inline">forecast_fitted_values()</strong> method to get the forecasts from the <span class="No-Break">training set.</span></p></li>				<li>Then, we identify anomalies based on whether the point forecasts are within the prediction intervals made by the model. This is done <span class="No-Break">as follows:</span><pre class="source-code">
anomalies = insample_forecasts.loc[
    (
        insample_forecasts['y'] &gt;= 
        insample_forecasts['AutoARIMA-hi-99']
    ) | (
        insample_forecasts['y'] &lt;= 
        insample_forecasts['AutoARIMA-lo-99'])]</pre><p class="list-inset">The preceding code checks whether the training predictions (<strong class="source-inline">insample_forecasts['y']</strong> object) are within the <strong class="source-inline">99</strong>% prediction intervals. Any observation that does not pass this check is considered <span class="No-Break">an anomaly.</span></p></li>				<li>Finally, we use the <strong class="source-inline">plot()</strong> method from the <strong class="source-inline">StatsForecast</strong> class to plot <span class="No-Break">the anomalies:</span><pre class="source-code">
StatsForecast.plot(insample_forecasts, unique_ids=['Q1'], 
    plot_anomalies=True)</pre><p class="list-inset">Here’s what the plot <span class="No-Break">looks like:</span></p></li>			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B21145_09_001.jpg" alt="Figure 9.1: Example of an anomaly identified by ARIMA" width="1650" height="480"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1: Example of an anomaly identified by ARIMA</p>
			<h2 id="_idParaDest-352"><a id="_idTextAnchor472"/>How it works…</h2>
			<p>We used the <strong class="source-inline">AutoARIMA</strong> implementation available in the <strong class="source-inline">statsforecast</strong> library to create the ARIMA model. This approach automatically selects the best parameters for the model. We set the seasonal length to <strong class="source-inline">4</strong> since the frequency of the data is quarterly. The fitting process is carried out by a <strong class="source-inline">StatsForecast</strong> <span class="No-Break">class instance.</span></p>
			<p>Prediction-based methods work by comparing the forecasts of a given model with the actual values of the series. In this case, we use an ARIMA model, but other methods can also<a id="_idIndexMarker589"/> be <a id="_idIndexMarker590"/>used. Moreover, we consider an approach based on prediction intervals. Specifically, an observation is considered an anomaly if its value is outside of the predicted interval. In the code shown in the previous section, we considered a prediction interval with a 99% level, but you can test a different value for <span class="No-Break">your problem.</span></p>
			<h2 id="_idParaDest-353"><a id="_idTextAnchor473"/>There’s more…</h2>
			<p>In this recipe, we focus on ARIMA to get prediction intervals, but you can use any other model with <span class="No-Break">such capabilities.</span></p>
			<p>You can check the following URL for more details about how to use the <strong class="source-inline">statsforecast</strong> library for prediction-based anomaly <span class="No-Break">detection: </span><a href="https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html"><span class="No-Break">https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetect<span id="_idTextAnchor474"/><span id="_idTextAnchor475"/>ion.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-354"><a id="_idTextAnchor476"/>Prediction-based anomaly detection using DL</h1>
			<p>We continue<a id="_idIndexMarker591"/> to explore <a id="_idIndexMarker592"/>prediction-based methods in this recipe. This time, we’ll create a forecasting model based on DL. Besides, we’ll use the point forecasts’ error  as a reference for <span class="No-Break">detecting anomalies.</span></p>
			<h2 id="_idParaDest-355"><a id="_idTextAnchor477"/>Getting ready</h2>
			<p>We’ll use a time series dataset about the number of taxi trips in New York City. This dataset is considered a benchmark problem for time series anomaly detection tasks. You can check the source at the following <span class="No-Break">link: </span><a href="https://databank.illinois.edu/datasets/IDB-9610843"><span class="No-Break">https://databank.illinois.edu/datasets/IDB-9610843</span></a><span class="No-Break">.</span></p>
			<p>Let’s start by loading the time series <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from datetime import datetime
import pandas as pd
dataset = pd.read_csv('assets/datasets/taxi/taxi_data.csv')
labels = pd.read_csv('assets/datasets/taxi/taxi_labels.csv')
dataset['ds'] = pd.Series([datetime.fromtimestamp(x) 
    for x in dataset['timestamp']])
dataset = dataset.drop('timestamp', axis=1)
dataset['unique_id'] = 'NYT'
dataset = dataset.rename(columns={'value': 'y'})
is_anomaly = []
for i, r in labels.iterrows():
    dt_start = datetime.fromtimestamp(r.start)
    dt_end = datetime.fromtimestamp(r.end)
    anomaly_in_period = [dt_start &lt;= x &lt;= dt_end 
        for x in dataset['ds']]
    is_anomaly.append(anomaly_in_period)
dataset['is_anomaly']=pd.DataFrame(is_anomaly).any(axis=0).astype(int)
dataset['ds'] = pd.to_datetime(dataset['ds'])</pre>			<p>The preceding <a id="_idIndexMarker593"/>code<a id="_idIndexMarker594"/> involves <span class="No-Break">several steps:</span></p>
			<ol>
				<li>Loading the dataset and corresponding labels using the <span class="No-Break"><strong class="source-inline">pd.read_csv</strong></span><span class="No-Break"><strong class="source-inline">()</strong></span><span class="No-Break"> function.</span></li>
				<li>Processing this dataset into a tabular format with three main pieces of information: the time series identifier (<strong class="source-inline">unique_id</strong>), the timestamp (<strong class="source-inline">ds</strong>), and the value of the <span class="No-Break">observation (</span><span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break">).</span></li>
				<li>Processing the labels into a new Boolean column called <strong class="source-inline">is_anomaly</strong> that denotes whether the corresponding observation is <span class="No-Break">an anomaly.</span><p class="list-inset">Here’s what the series <span class="No-Break">looks like:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B21145_09_002.jpg" alt="Figure 9.2: New York City dataset with marked anomalies" width="1134" height="645"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2: New York City dataset with mar<a id="_idTextAnchor478"/>ked anomalies</p>
			<h2 id="_idParaDest-356"><a id="_idTextAnchor479"/>How to do it…</h2>
			<p>Now, we use the taxi <a id="_idIndexMarker595"/>trips<a id="_idIndexMarker596"/> dataset to train a forecasting model. In this recipe, we’ll resort to the <strong class="source-inline">neuralforecast</strong> library, which contains implementation for several <span class="No-Break">DL algorithm:</span></p>
			<ol>
				<li>Let’s start by defining the model <span class="No-Break">as follows:</span><pre class="source-code">
from neuralforecast import NeuralForecast
from neuralforecast.models import NHITS
horizon = 1
n_lags = 144
models = [NHITS(h=horizon,
    input_size=n_lags,
    max_steps=30,
    n_freq_downsample=[2, 1, 1],
    mlp_units=3 * [[128, 128]],
    accelerator='cpu')]
nf = NeuralForecast(models=models, freq='30T')</pre><p class="list-inset">We use an input size (<strong class="source-inline">n_lags</strong>) of <strong class="source-inline">144</strong>, which corresponds to 3 days of data as the time series is collected every <strong class="source-inline">30</strong> <span class="No-Break">minutes (</span><span class="No-Break"><strong class="source-inline">freq='30T'</strong></span><span class="No-Break">).</span></p></li>				<li>After defining the model, we can train it using the <span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
nf.fit(df=dataset.drop('is_anomaly', axis=1), val_size=n_lags)</pre><p class="list-inset">Before the fitting process, we drop the <strong class="source-inline">is_anomaly</strong> variable that contains anomaly information. Now, the idea is to use the model to forecast the values of the <a id="_idIndexMarker597"/>time <a id="_idIndexMarker598"/>series. Any significant deviation from the actual value is considered an anomaly. Let’s look at the <span class="No-Break">training predictions.</span></p></li>				<li>We can get the training (or insample) predictions by calling the <strong class="source-inline">predict_insample()</strong> method, <span class="No-Break">like so:</span><pre class="source-code">
insample = nf.predict_insample()
insample = insample.tail(-n_lags)
abs_error = (insample['NHITS'] - insample['y']).abs()</pre><p class="list-inset">In the preceding code, we get the training sample and remove the initial <strong class="source-inline">n_lag</strong> observations to align the predictions with the actual data. Then, we measure the absolute error of the model by taking the absolute difference between the predictions and <span class="No-Break">actual values.</span></p></li>				<li>Visualize the absolute error in the training data along with the <span class="No-Break">marked anomalies:</span><pre class="source-code">
preds = pd.DataFrame(
    {
        "Error": abs_error.values,
        "ds": dataset["ds"].tail(-n_lags),
        "is_anomaly": dataset["is_anomaly"].tail(-n_lags),
    }
)
preds = preds.set_index("ds")
predicted_anomaly_periods = find_anomaly_periods(
    preds["is_anomaly"])
setup_plot(preds.rename(columns={"Error": "y"}), 
    predicted_anomaly_periods, "Error")</pre><p class="list-inset">In the interest <a id="_idIndexMarker599"/>of <a id="_idIndexMarker600"/>conciseness, the plotting functions are not shown. You can check them out in the GitHub repository. The plot is shown in the <span class="No-Break">following figure:</span></p></li>			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B21145_09_003.jpg" alt="Figure 9.3: Absolute error by the Neural Hierarchical Implementation for Time Series (NHITS) model and marked anomalies" width="1497" height="745"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3: Absolute error by the Neural Hierarchical Implementation for Time Series (NHITS) model and marked anomalies</p>
			<p>Large errors occur during two of the anomalies, though the model also misses <span class="No-Break">some anomalies.</span></p>
			<h2 id="_idParaDest-357"><a id="_idTextAnchor480"/>How it works…</h2>
			<p>As seen in the previous recipe, we use a forecasting model to identify anomalies in a time series. In this case, instead of using prediction intervals, we rely on the absolute error<a id="_idIndexMarker601"/> of the <a id="_idIndexMarker602"/>model. A large error indicates a potential anomaly in the <span class="No-Break">time series.</span></p>
			<p>We use the <strong class="source-inline">neuralforecast</strong> framework to build a DL forecasting model based on the NHITS method. NHITS is a<a id="_idIndexMarker603"/> model that extends <strong class="bold">Neural Basis Expansion Analysis</strong> (<strong class="bold">NBEATS</strong>) and is based on a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) type <a id="_idIndexMarker604"/><span class="No-Break">of architecture.</span></p>
			<p>This involves transforming the data into an appropriate format and training the model <span class="No-Break">using auto-regression.</span></p>
			<h2 id="_idParaDest-358"><a id="_idTextAnchor481"/>There’s more…</h2>
			<p>In this recipe, we focus on a univariate time series dataset and a particular forecasting method (NHITS). Yet, it’s important to note that the prediction-based approach for anomaly detection can be applied to different settings (such as multivariate time <a id="_idIndexMarker605"/>series) and with other <span class="No-Break">forecasting methods.</span></p>
			<p>During the training phase, we need to define an error threshold above which we flag an observation as an anomaly. We will explore several implementations with this feature in <span class="No-Break">subsequent recipes.</span></p>
			<h1 id="_idParaDest-359"><a id="_idTextAnchor482"/>Anomaly detection using an LSTM AE</h1>
			<p>In this recipe, we’ll<a id="_idIndexMarker606"/> build an AE<a id="_idIndexMarker607"/> to detect anomalies in time series. An AE is a type of <strong class="bold">neural network</strong> (<strong class="bold">NN</strong>) that <a id="_idIndexMarker608"/>tries to reconstruct the input data. The motivation to use this kind of model for anomaly detection is that the reconstruction process of anomalous data is more difficult than that of <span class="No-Break">typical observations.</span></p>
			<h2 id="_idParaDest-360"><a id="_idTextAnchor483"/>Getting ready</h2>
			<p>We’ll continue with the New York City taxi time series in this recipe. In terms of framework, we’ll show how to build an AE using PyTorch Lightning. This means that we’ll build a data module to handle the data preprocessing and another module for handling the training and inference of <span class="No-Break">the NN.</span></p>
			<h2 id="_idParaDest-361"><a id="_idTextAnchor484"/>How to do it…</h2>
			<p>This recipe is split into three parts. First, we build the data module based on PyTorch. Then, we create an AE module. Finally, we combine the two parts to build an anomaly <span class="No-Break">detection system:</span></p>
			<ol>
				<li>Let’s start by building the data module. We create a class called <strong class="source-inline">TaxiDataModule</strong> that extends <strong class="source-inline">pl.LightningDataModule</strong>. Here’s the constructor of <span class="No-Break">the class:</span><pre class="source-code">
Import numpy as np
import pandas as pd
import lightning.pytorch as pl
from pytorch_forecasting import TimeSeriesDataSet
from sklearn.model_selection import train_test_split
class TaxiDataModule(pl.LightningDataModule):
    def __init__(self,
                 data: pd.DataFrame,
                 n_lags: int,
                 batch_size: int):
        super().__init__()
        self.data = data
        self.batch_size = batch_size
        self.n_lags = n_lags
        self.train_df = None
        self.test_df = None
        self.training = None
        self.validation = None
        self.predict_set = None</pre><p class="list-inset">The <strong class="source-inline">TaxiDataModule</strong> class<a id="_idIndexMarker609"/> takes<a id="_idIndexMarker610"/> two inputs besides the dataset: the number of lags (context length) and the <span class="No-Break">batch size.</span></p></li>				<li>Next, we code the <strong class="source-inline">setup()</strong> method, where the data is prepared for training and testing <span class="No-Break">the model:</span><pre class="source-code">
    def setup(self, stage=None):
        self.data['timestep'] = np.arange(self.data.shape[0])
        unique_times = \
            self.data['timestep'].sort_values().unique()
        tr_ind, ts_ind = \
            train_test_split(unique_times, test_size=0.4,
                shuffle=False)
        tr_ind, vl_ind = \
            train_test_split(tr_ind, test_size=0.1,
                shuffle=False)
        self.train_df = \
            self.data.loc[self.data['timestep'].isin(tr_ind), :]
        self.test_df = \
            self.data.loc[self.data['timestep'].isin(ts_ind), :]
        validation_df = \
            self.data.loc[self.data['timestep'].isin(vl_ind), :]
        self.training = TimeSeriesDataSet(
            data=self.train_df,
            time_idx="timestep",
            target="y",
            group_ids=['unique_id'],
            max_encoder_length=self.n_lags,
            max_prediction_length=1,
            time_varying_unknown_reals=['y'],
        )
        self.validation = \
            TimeSeriesDataSet.from_dataset(
                self.training, validation_df)
        self.test = \
            TimeSeriesDataSet.from_dataset(
                self.training, self.test_df)
        self.predict_set = \
            TimeSeriesDataSet.from_dataset(
                self.training, self.data, predict=True)</pre><p class="list-inset">In the preceding code, we start by splitting the data into training, validation, and testing<a id="_idIndexMarker611"/> sets. Each<a id="_idIndexMarker612"/> of these is transformed into a <strong class="source-inline">TimeSeriesDataSet</strong> <span class="No-Break">class instance.</span></p></li>				<li>The data loaders are implemented <span class="No-Break">as follows:</span><pre class="source-code">
    def train_dataloader(self):
        return self.training.to_dataloader(
            batch_size=self.batch_size, shuffle=False)
    def val_dataloader(self):
        return self.validation.to_dataloader(
            batch_size=self.batch_size, shuffle=False)
    def predict_dataloader(self):
        return self.predict_set.to_dataloader(
            batch_size=1, shuffle=False)</pre><p class="list-inset">Essentially, the data loading process is similar to what we did before in the forecasting tasks. You can check, for example, the <em class="italic">Multi-step and multi-output forecasting with multivariate time series</em> recipe in <a href="B21145_05.xhtml#_idTextAnchor306"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p></li>				<li>Now, we focus on the AE model, which is split into two parts: an encoder and a decoder. Here’s<a id="_idIndexMarker613"/> an<a id="_idIndexMarker614"/> implementation of the encoder in a class <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">Encoder</strong></span><span class="No-Break">:</span><pre class="source-code">
from torch import nn
import torch
class Encoder(nn.Module):
    def __init__(self, context_len, n_variables, 
        embedding_dim=2):
        super(Encoder, self).__init__()
        self.context_len, self.n_variables = \
            context_len, n_variables
        self.embedding_dim, self.hidden_dim = \
            embedding_dim, 2 * embedding_dim
        self.lstm1 = nn.LSTM(
            input_size=self.n_variables,
            hidden_size=self.hidden_dim,
            num_layers=1,
            batch_first=True
        )
        self.lstm2 = nn.LSTM(
            input_size=self.hidden_dim,
            hidden_size=embedding_dim,
            num_layers=1,
            batch_first=True
        )
    def forward(self, x):
        batch_size = x.shape[0]
        x, (_, _) = self.lstm1(x)
        x, (hidden_n, _) = self.lstm2(x)
        return hidden_n.reshape((batch_size, 
            self.embedding_dim))</pre></li>				<li>The decoder<a id="_idIndexMarker615"/> is<a id="_idIndexMarker616"/> implemented in a class called <strong class="source-inline">Decoder</strong> that also <span class="No-Break">extends </span><span class="No-Break"><strong class="source-inline">nn.Module</strong></span><span class="No-Break">:</span><pre class="source-code">
class Decoder(nn.Module):
    def __init__(self, context_len, n_variables=1, input_dim=2):
        super(Decoder, self).__init__()
        self.context_len, self.input_dim = \
            context_len, input_dim
        self.hidden_dim, self.n_variables = \
            2 * input_dim, n_variables
        self.lstm1 = nn.LSTM(
            input_size=input_dim,
            hidden_size=input_dim,
            num_layers=1,
            batch_first=True
        )
        self.lstm2 = nn.LSTM(
            input_size=input_dim,
            hidden_size=self.hidden_dim,
            num_layers=1,
            batch_first=True
        )
        self.output_layer = nn.Linear(self.hidden_dim, 
            self.n_variables)
    def forward(self, x):
        batch_size = x.shape[0]
        x = x.repeat(self.context_len, self.n_variables)
        x = x.reshape((batch_size, self.context_len, 
            self.input_dim))
        x, (hidden_n, cell_n) = self.lstm1(x)
        x, (hidden_n, cell_n) = self.lstm2(x)
        x = x.reshape((batch_size, self.context_len, 
            self.hidden_dim))
        return self.output_layer(x)</pre></li>				<li>The two parts <a id="_idIndexMarker617"/>are <a id="_idIndexMarker618"/>combined in an <strong class="source-inline">AutoencoderLSTM</strong> class that extends <span class="No-Break"><strong class="source-inline">pl.LightningModule</strong></span><span class="No-Break">:</span><pre class="source-code">
import torch
class AutoencoderLSTM(pl.LightningModule):
    def __init__(self, context_len, n_variables, embedding_dim):
        super().__init__()
        self.encoder = Encoder(context_len, n_variables, 
            embedding_dim)
        self.decoder = Decoder(context_len, n_variables, 
            embedding_dim)
    def forward(self, x):
        xh = self.encoder(x)
        rec_x = self.decoder(xh)
        return rec_x
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)</pre><p class="list-inset">In the <strong class="source-inline">forward()</strong> method, the encoder part takes the original input (<strong class="source-inline">self.encoder(x)</strong>) and transforms it into a reduced dimension (<strong class="source-inline">xh </strong>object). Then, the<a id="_idIndexMarker619"/> decoder <a id="_idIndexMarker620"/>reconstructs the original input data based <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">xh</strong></span><span class="No-Break">.</span></p></li>				<li>Then, we implement the training, validation, and <span class="No-Break">prediction steps:</span><pre class="source-code">
    import torch.nn.functional as F
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, x['encoder_cont'])
        self.log('train_loss', loss)
        return loss
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, x['encoder_cont'])
        self.log('val_loss', loss)
        return loss
    def predict_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, x['encoder_cont'])
        return loss</pre></li>				<li>We train the NN using the <strong class="source-inline">Trainer</strong> class from PyTorch Lightning. We use <strong class="source-inline">144</strong> lags, which<a id="_idIndexMarker621"/> amounts<a id="_idIndexMarker622"/> to 3 days of data. We also apply early stopping to guide the <span class="No-Break">training process:</span><pre class="source-code">
N_LAGS = 144
N_VARIABLES = 1
from lightning.pytorch.callbacks import EarlyStopping
datamodule = \
    TaxiDataModule(
        data=dataset.drop('is_anomaly', axis=1),
        n_lags=N_LAGS,
        batch_size=32)
model = AutoencoderLSTM(n_variables=1,
        context_len=N_LAGS,
        embedding_dim=4)
early_stop_callback = EarlyStopping(monitor="val_loss",
    min_delta=1e-4,
    patience=5,
    verbose=False,
    mode="min")
trainer = pl.Trainer(max_epochs=20,
    accelerator='cpu',
    callbacks=[early_stop_callback])
trainer.fit(model, datamodule)</pre></li>				<li>After training, we<a id="_idIndexMarker623"/> can <a id="_idIndexMarker624"/>apply the model to the test data <span class="No-Break">as follows:</span><pre class="source-code">
dl = datamodule.test.to_dataloader(batch_size=1, shuffle=False)
preds = trainer.predict(model, dataloaders=dl)
preds = pd.Series(np.array([x.numpy() for x in preds]))</pre><p class="list-inset">In the preceding code, we transform the <strong class="source-inline">test</strong> object from the data module into a data loader. We use a batch size of <strong class="source-inline">1</strong> without shuffling to process each instance sequentially. Then, we use the <strong class="source-inline">trainer</strong> object to get the predictions. The<a id="_idIndexMarker625"/> following<a id="_idIndexMarker626"/> figure shows the reconstructed error in the <span class="No-Break">test set:</span></p></li>			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B21145_09_004.jpg" alt="Figure 9.4: Reconstruction error by the AE and marked anomalies" width="1500" height="927"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4: Reconstruction error by the AE and marked anomalies</p>
			<p>In most cases, the peaks in reconstruction error coincide with <span class="No-Break">the anomalies.</span></p>
			<h2 id="_idParaDest-362"><a id="_idTextAnchor485"/>How it works…</h2>
			<p>The workflow in the data module may be familiar because it follows the same ideas behind the forecasting models we’ve built in other chapters; for example, in the <em class="italic">Multi-step and multi-output forecasting with multivariate time series</em> recipe in <a href="B21145_05.xhtml#_idTextAnchor306"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. But, in this case, we’re not interested in predicting the future values of the series. Instead, at each time step, both the input and the output of the model are recent lags of <span class="No-Break">the series.</span></p>
			<p>An AE is composed of two main parts: an encoder and a decoder. The encoder aims to compress the input data into a small dimension, which is referred to as the bottleneck. Turning the input data into a small dimension is important to make the NN focus on the most important patterns in the data, disregarding noise. Then, the decoder takes the data encoded in the reduced dimension and tries to reconstruct the original input data. Both the encoder and the decoder of the NN are based on a stacked LSTM AE. Yet, you can use different architectures for <span class="No-Break">these components.</span></p>
			<p>The <strong class="source-inline">Encoder</strong> class extends the <strong class="source-inline">nn.Module</strong> class from <strong class="source-inline">torch</strong>. This particular encoder consists of two LSTM layers. These layers stack on top of each other as detailed in the <strong class="source-inline">forward()</strong> method. The <strong class="source-inline">Decoder</strong> class also contains two stacked LSTM layers that are followed by a densely <span class="No-Break">connected layer.</span></p>
			<p>In the training step of the AE, we pass a batch of the lagged time series (<strong class="source-inline">x['encoder_cont']</strong>) to the model. It produces an object called <strong class="source-inline">y_pred</strong>, which is the <a id="_idIndexMarker627"/>reconstructed<a id="_idIndexMarker628"/> input. Then, we compute <a id="_idIndexMarker629"/>the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) (<strong class="source-inline">F.mse_loss</strong>), which compares the original input with the <span class="No-Break">reconstructed one.</span></p>
			<h1 id="_idParaDest-363"><a id="_idTextAnchor486"/>Building an AE using PyOD</h1>
			<p>PyOD is a Python <a id="_idIndexMarker630"/>library that is devoted to anomaly detection. It<a id="_idIndexMarker631"/> contains several reconstruction-based algorithms such as AEs. In this recipe, we’ll build an AE using PyOD to detect anomalies in <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-364"><a id="_idTextAnchor487"/>Getting ready</h2>
			<p>You can install PyOD using the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install pyod</pre>			<p>We’ll use the same dataset as in the previous recipe. So, we start with the dataset object created in the <em class="italic">Prediction-based anomaly detection using DL</em> recipe. Let’s see how to transform this data to build an AE <span class="No-Break">with PyOD.</span></p>
			<h2 id="_idParaDest-365"><a id="_idTextAnchor488"/>How to do it…</h2>
			<p>The following steps show how to build an AE and predict the probability <span class="No-Break">of anomalies:</span></p>
			<ol>
				<li>We start<a id="_idIndexMarker632"/> by <a id="_idIndexMarker633"/>transforming the time series using a sliding window with the <span class="No-Break">following code:</span><pre class="source-code">
import pandas as pd
from sklearn.preprocessing import StandardScaler
N_LAGS = 144
series = dataset['y']
input_data = []
for i in range(N_LAGS, series.shape[0]):
    input_data.append(series.iloc[i - N_LAGS:i].values)
input_data = np.array(input_data)
input_data_n = StandardScaler().fit_transform(input_data)
input_data_n = pd.DataFrame(input_data_n)</pre><p class="list-inset">In the <span class="No-Break">preceding code:</span></p><ul><li>We get the value column of the time series and store it in the <span class="No-Break">series object.</span></li><li>Then, we iterate over the dataset using a sliding window similar to an auto-regressive approach. This way, the time series is represented by its past number of lags(<strong class="source-inline">N_LAGS</strong>) at each <span class="No-Break">time step.</span></li><li>We standardize the data using <strong class="source-inline">StandardScaler</strong> from <strong class="source-inline">scikit-learn</strong>, which is an important step for training NNs such <span class="No-Break">as AEs.</span></li></ul></li>				<li>After preprocessing the data, we define the AE based on PyOD and fit it using <span class="No-Break">the dataset:</span><pre class="source-code">
from pyod.models.auto_encoder_torch import AutoEncoder
model = AutoEncoder(
    hidden_neurons=[144, 4, 4, 144],
    hidden_activation="relu",
    epochs=20,
    batch_norm=True,
    learning_rate=0.001,
    batch_size=32,
    dropout_rate=0.2,
)
model.fit(input_data_n)</pre><p class="list-inset">The fitted <a id="_idIndexMarker634"/>model <a id="_idIndexMarker635"/>contains outlier scores based on the reconstruction process. Anomalies tend to have higher scores. These scores are available in the <strong class="source-inline">decision_scores_</strong> attribute of <span class="No-Break">the model:</span></p><pre class="source-code">anomaly_scores = model.decision_scores_</pre><p class="list-inset">Here’s the distribution of <span class="No-Break">the scores:</span></p></li>			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B21145_09_005.jpg" alt="Figure 9.5: Histogram with the anomaly scores produced by the AE" width="1527" height="1013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5: Histogram with the anomaly scores produced by the AE</p>
			<ol>
				<li value="3">Regarding the<a id="_idIndexMarker636"/> inference<a id="_idIndexMarker637"/> step, we can use the <strong class="source-inline">predict</strong><strong class="source-inline">()</strong> and <strong class="source-inline">predict_proba</strong><strong class="source-inline">()</strong> methods. The <strong class="source-inline">predict</strong><strong class="source-inline">()</strong> method works <span class="No-Break">as follows:</span><pre class="source-code">
predictions = model.predict(input_data_n)</pre></li>				<li>The <strong class="source-inline">predict_proba()</strong>method works <span class="No-Break">as follows:</span><pre class="source-code">
probs = model.predict_proba(input_data_n)[:, 1]
probabilities = pd.Series(probs, \
    index=series.tail(len(probs)).index)</pre></li>				<li>The probabilities represent the probability of each observation being an anomaly. You can plot the probabilities using the <span class="No-Break">following code:</span><pre class="source-code">
ds = dataset.tail(-144)
ds['Predicted Probability'] = probabilities
ds = ds.set_index('ds')
anomaly_periods = find_anomaly_periods(ds['is_anomaly'])
setup_plot(ds, anomaly_periods)</pre><p class="list-inset">Here’s what the probabilities look like along the <span class="No-Break">training set:</span></p></li>			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B21145_09_006.jpg" alt="Figure 9.6: Anomaly probability scores produced by the AE" width="1500" height="927"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6: Anomaly probability scores produced by the AE</p>
			<p>Again, the<a id="_idIndexMarker638"/> anomaly <a id="_idIndexMarker639"/>probability score peaks coincide with <span class="No-Break">some anomalies.</span></p>
			<h2 id="_idParaDest-366"><a id="_idTextAnchor489"/>How it works…</h2>
			<p>The PyOD library follows a design pattern similar to <strong class="source-inline">scikit-learn</strong>. So, each method, such as <strong class="source-inline">AutoEncoder</strong>, is trained using the <strong class="source-inline">fit</strong><strong class="source-inline">()</strong> method and produces predictions based on a <strong class="source-inline">predict</strong> or <span class="No-Break"><strong class="source-inline">predict_proba</strong></span><span class="No-Break"><strong class="source-inline">()</strong></span><span class="No-Break"> method.</span></p>
			<p>We use the <strong class="source-inline">AutoEncoder</strong> class instance from the <strong class="source-inline">auto_encoder_torch</strong> module. The library also contains the equivalent method but with a TensorFlow backend. We create an instance of the model and set up a <span class="No-Break">few parameters:</span></p>
			<ul>
				<li><strong class="source-inline">hidden_neurons=[144, 2, 2, 144]</strong>: These parameters detail the number of hidden units per layer. The input and output layers have a number of units equal to the input size, which is the number of lags. The hidden layers of the AE typically have a low number of units to compress the input data <span class="No-Break">before reconstruction.</span></li>
				<li><strong class="source-inline">hidden_activation</strong>: The activation function, which is set to the rectified <span class="No-Break">linear function.</span></li>
				<li><strong class="source-inline">batch_norm</strong>: A Boolean input that represents whether batch normalization should be applied. You can learn more about this at the following <span class="No-Break">link: </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"><span class="No-Break">https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html</span></a><span class="No-Break">.</span></li>
				<li><strong class="source-inline">learning_rate</strong>: The learning rate, which is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.001</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">batch_size</strong>: The batch size, which is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">64</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">dropout_rate</strong>: A dropout rate between the layers for regularization, which is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.3</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>In this recipe, we created another AE for anomaly detection. This involves transforming the time series using a sliding window, similar to what we did for building forecasting<a id="_idIndexMarker640"/> models <a id="_idIndexMarker641"/>for auto-regression. The model predicts whether each observation is an anomaly based on the outlier scores. The threshold is set automatically by the model, though you can pick your own <span class="No-Break">as well.</span></p>
			<h2 id="_idParaDest-367"><a id="_idTextAnchor490"/>There’s more…</h2>
			<p>Deciding whether an observation is an anomaly involves analyzing the anomaly scores of the model. You can use different approaches, such as percentiles or standard deviations. For example, consider an observation an anomaly if the reconstruction error is above some percentile (such as 95) or if the reconstruction error is above two <span class="No-Break">standard deviations.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We used the prediction from the training data for illustration purposes. Working with a test follows a <span class="No-Break">similar approach.</span></p>
			<h1 id="_idParaDest-368"><a id="_idTextAnchor491"/>Creating a VAE for time series anomaly detection</h1>
			<p>Building on<a id="_idIndexMarker642"/> the <a id="_idIndexMarker643"/>foundation laid in the previous recipe, we now turn our attention to VAEs, a more sophisticated and probabilistic approach to anomaly detection in time series data. Unlike traditional AEs, VAEs introduce a probabilistic interpretation, making them more adept at handling inherent uncertainties in <span class="No-Break">real-world data.</span></p>
			<h2 id="_idParaDest-369"><a id="_idTextAnchor492"/>Getting ready</h2>
			<p>This code in this recipe is based on PyOD. We also use the same dataset as in the <span class="No-Break">previous recipe:</span></p>
			<pre class="source-code">
N_LAGS = 144
series = dataset['y']</pre>			<p>Now, let’s see how to create a VAE for time series <span class="No-Break">anomaly detection.</span></p>
			<h2 id="_idParaDest-370"><a id="_idTextAnchor493"/>How to do it…</h2>
			<p>We begin by preparing our dataset, as in the <span class="No-Break">previous recipe:</span></p>
			<ol>
				<li>The dataset is first transformed using a sliding window, a technique that helps the model understand temporal dependencies within the <span class="No-Break">time series:</span><pre class="source-code">
import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
input_data = []
for i in range(N_LAGS, series.shape[0]):
    input_data.append(series.iloc[i - N_LAGS:i].values)
    input_data = np.array(input_data)
    input_data_n = StandardScaler().fit_transform(input_data)
    input_data_n = pd.DataFrame(input_data_n)</pre></li>				<li>After transforming the dataset, we define and fit the VAE model using PyOD’s <strong class="source-inline">VAE</strong> class. The configuration of the <strong class="source-inline">VAE</strong> class includes specifying the architecture of the <a id="_idIndexMarker644"/>encoder<a id="_idIndexMarker645"/> and decoder networks and various <span class="No-Break">training parameters:</span><pre class="source-code">
from pyod.models.vae import VAE
from tensorflow.keras.losses import mean_squared_error
model = VAE(encoder_neurons=[144, 4],
        decoder_neurons=[4, 144],
        latent_dim=2,
        hidden_activation='relu',
        output_activation='sigmoid',
        loss=mean_squared_error,
        optimizer='adam',
        epochs=20,
        batch_size=32,
        dropout_rate=0.2,
        l2_regularizer=0.1,
        validation_size=0.1,
        preprocessing=True,
        verbose=1)
model.fit(input_data_n)</pre></li>				<li>The fitted VAE model is then used to generate anomaly scores. These scores reflect how well each data point conforms to the pattern learned by the model. Points with higher scores are more likely to <span class="No-Break">be anomalies:</span><pre class="source-code">
anomaly_scores = model.decision_scores_</pre></li>			</ol>
			<h2 id="_idParaDest-371"><a id="_idTextAnchor494"/>How it works…</h2>
			<p>A VAE is a NN model that stands out for its ability to handle data’s latent or hidden aspects. Unlike traditional AEs, which map inputs to a fixed point in a latent space, VAEs transform inputs into a probability distribution, usually a normal distribution, characterized by mean and variance. This way, every input is associated with a region in the latent<a id="_idIndexMarker646"/> space <a id="_idIndexMarker647"/>rather than a single point, introducing an element of randomness <span class="No-Break">and variability.</span></p>
			<p>The decoder network then samples points from these estimated distributions and attempts to reconstruct the original input data. The training process involves two <span class="No-Break">key objectives:</span></p>
			<ul>
				<li>Minimizing the reconstruction error ensures that the decoder can accurately recreate the input data from <span class="No-Break">latent representations.</span></li>
				<li>Regularizing latent space distributions to be close to a standard normal distribution. This is typically achieved by minimizing the Kullback-Leibler divergence. The regularization process prevents overfitting and ensures a well-structured and continuous <span class="No-Break">latent space.</span></li>
			</ul>
			<p>Once trained, the VAE can be employed for anomaly detection. The VAE should be able to reconstruct normal data (similar to what it was trained on) with relatively low error. Conversely, data points significantly different from the training set (potential anomalies) will likely be reconstructed with higher error. Therefore, the reconstruction error serves as an <span class="No-Break">anomaly score.</span></p>
			<p>A high reconstruction error suggests that the data point does not conform well to the learned data distribution, flagging it as <span class="No-Break">an anomaly:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B21145_09_007.jpg" alt="Figure 9.7: True values, true anomalies, and the probability of anomalies predicted by the VAE" width="956" height="633"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7: True values, true anomalies, and the probability of anomalies predicted by the VAE</p>
			<p>This comparison helps <a id="_idIndexMarker648"/>us <a id="_idIndexMarker649"/>evaluate the performance of our VAE in <span class="No-Break">real-world scenarios.</span></p>
			<h2 id="_idParaDest-372"><a id="_idTextAnchor495"/>There’s more…</h2>
			<p>One of the most interesting aspects of VAEs is their ability to generate new data points. By sampling from learned distributions in the latent space, we can generate new instances that are similar to the training data. This property can be particularly useful in scenarios where data augmentation <span class="No-Break">is required.</span></p>
			<p>Moreover, the probabilistic nature of VAEs offers a natural way to quantify uncertainty. This can be particularly beneficial in settings where it’s relevant to assess the confidence of the <span class="No-Break">model’s predictions.</span></p>
			<h1 id="_idParaDest-373"><a id="_idTextAnchor496"/>Using GANs for time series anomaly detection</h1>
			<p>GANs have <a id="_idIndexMarker650"/>gained<a id="_idIndexMarker651"/> significant popularity in various fields of ML, particularly in image generation and modification. However, their application in time series data, especially for anomaly detection, is an emerging area of research and practice. In this recipe, we focus on utilizing GANs, specifically <strong class="bold">Anomaly Detection with Generative Adversarial Networks</strong> (<strong class="bold">AnoGAN</strong>), to <a id="_idIndexMarker652"/>detect time series <span class="No-Break">data anomalies.</span></p>
			<h2 id="_idParaDest-374"><a id="_idTextAnchor497"/>Getting ready…</h2>
			<p>Before diving into the implementation, ensure that you have the PyOD library installed. We will continue using the taxi trip dataset for this recipe, which provides a real-world context for time series <span class="No-Break">anomaly detection.</span></p>
			<h2 id="_idParaDest-375"><a id="_idTextAnchor498"/>How to do it…</h2>
			<p>The implementation involves several steps: data preprocessing, defining and training the AnoGAN model, and finally, performing <span class="No-Break">anomaly detection:</span></p>
			<ol>
				<li>We start by loading the dataset and preparing it for the AnoGAN model. The dataset is transformed in the same way as before using a sliding <span class="No-Break">window approach:</span><pre class="source-code">
import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
N_LAGS = 144
series = dataset['y']
input_data = []
for i in range(N_LAGS, series.shape[0]):
    input_data.append(series.iloc[i - N_LAGS:i].values)
    input_data = np.array(input_data)
    input_data_n = StandardScaler().fit_transform(input_data)
    input_data_n = pd.DataFrame(input_data_n)</pre></li>				<li>AnoGAN is<a id="_idIndexMarker653"/> then <a id="_idIndexMarker654"/>defined with specific hyperparameters and trained on the <span class="No-Break">preprocessed data:</span><pre class="source-code">
from pyod.models.anogan import AnoGAN
model = AnoGAN(activation_hidden='tanh',
    dropout_rate=0.2,
    latent_dim_G=2,
    G_layers=[20, 10, 3, 10, 20],
    verbose=1,
    D_layers=[20, 10, 5],
    index_D_layer_for_recon_error=1,
    epochs=20,
    preprocessing=False,
    learning_rate=0.001,
    learning_rate_query=0.01,
    epochs_query=1,
    batch_size=32,
    output_activation=None,
    contamination=0.1)
model.fit(input_data_n)</pre></li>				<li>Once the model is trained, we use it to predict anomalies in <span class="No-Break">the data:</span><pre class="source-code">
anomaly_scores = model.decision_scores_
predictions = model.predict(input_data_n)</pre></li>				<li>Finally, we visualize <a id="_idIndexMarker655"/>the results to compare the model’s predictions with <span class="No-Break">actual anomalies:</span></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B21145_09_008.jpg" alt="Figure 9.8: True values, true anomalies, and the probability of anomalies predicted by a GAN" width="952" height="634"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8: True values, true anomalies, and the probability of anomalies predicted by a GAN</p>
			<h2 id="_idParaDest-376"><a id="_idTextAnchor499"/>How it works…</h2>
			<p>AnoGAN is a model that employs the principles of GANs for the specific task of anomaly detection in time series data. The core idea behind AnoGAN is the interaction between two key components: the generator and <span class="No-Break">the discriminator.</span></p>
			<p>The Generator is tasked with creating synthetic data that resembles the true time series data it has been trained on. It learns to capture the underlying patterns and distributions of the input data, trying to generate outputs that are indistinguishable from the <span class="No-Break">real data.</span></p>
			<p>The Discriminator, on the other hand, acts as a critic. Its role is to discern whether the data it reviews are genuine (actual data points from the dataset) or fabricated (outputs generated by the Generator). During training, these two components engage in a continuous game: the Generator improves its ability to produce realistic data, while the Discriminator becomes better at <span class="No-Break">detecting fakes.</span></p>
			<p>The reconstruction error is once again used to identify anomalies. The Generator, being trained only on normal data, will struggle to reproduce outliers or anomalous instances. Thus, when the reconstructed version of a data point diverges significantly from<a id="_idIndexMarker656"/> the <a id="_idIndexMarker657"/>original, we find a <span class="No-Break">potential anomaly.</span></p>
			<p>In practice, the reconstruction error can be calculated using various methods, such as MSE or other distance metrics, depending on the nature of the data and the specific requirements of the task <span class="No-Break">at hand.</span></p>
			<h2 id="_idParaDest-377"><a id="_idTextAnchor500"/>There’s more…</h2>
			<p>While AnoGAN provides a novel approach to time series anomaly detection, it is worth exploring variations and improvements. For instance, one might consider tuning the model’s <a id="_idIndexMarker658"/>architecture or experimenting with different types of <a id="_idIndexMarker659"/>GANs, such as <strong class="bold">conditional GANs</strong> (<strong class="bold">CGANs</strong>) or <strong class="bold">Wasserstein </strong><span class="No-Break"><strong class="bold">GANs</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">WGANs</strong></span><span class="No-Break">).</span></p>
		</div>
	</div>
</div>
</body></html>