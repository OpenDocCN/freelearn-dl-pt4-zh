- en: Object Detection Using OpenCV and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the second chapter focusing on computer vision in *Python Deep Learning
    Projects* (a data science pun to kick us off!). Let's think about what we accomplished
    in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml), *Handwritten Digits
    Classification Using ConvNets*, where we were able to train an image classifier
    with a **convolutional neural network** (**CNN**) to accurately classify handwritten
    digits in an image. What was a key characteristic of the raw data, and what was
    our business objective? The data was less complicated than it could have been
    because each image only had one handwritten digit in it and our goal was to accurately
    assign a digital label to the image.
  prefs: []
  type: TYPE_NORMAL
- en: What would have happened if each image had multiple handwritten digits in it?
    What would have happened if we had a video of the digits? What if we want to identify
    where the digits are in the image? These questions represent challenges that real-world
    data embodies, and they drive our data science innovation to new models and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Let's expand our line of questions and imagination to the next (hypothetical)
    business use case for our Python deep learning project, where we're looking to
    build, train, and test an object detection and classification model to be used
    by an automobile manufacturer in their new line of self-driving cars. Autonomous
    vehicles need to have fundamental computer vision capabilities that you and I
    have organically by way of our physiology and experiential learning. We as humans
    can examine our field of vision and report whether or not a specific item is present
    and where in relation to other objects that item (if present) is located. So,
    if I were to ask you if you see a chicken, you'd likely say no, unless you live
    on a farm and are looking out your window. But if I ask you if you see a keyboard,
    you'd likely say yes, and could even say that the keyboard is different from other
    objects and is in front of the wall before you.
  prefs: []
  type: TYPE_NORMAL
- en: This is no trivial task for a computer. As Deep Learning Engineers, you are
    going to learn the intuition and model architecture that empowers you to build
    a powerful object detection and classification engine that we can envision being
    tested for use in autonomous vehicles. The data inputs that we're going to be
    working with in this chapter will be much more informationally complex than what
    we've had in previous projects, and the outcomes when we get them right will be
    that much more impressive.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Object detection intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you need your application to find and name things in an image, you need
    to build a deep neural network for object detection. The visual field is very
    complex, and a camera for still images and video captures frames with many, many
    objects in them. Object detection is used in manufacturing for process automation
    in production lines; autonomous vehicles sensing pedestrians, other cars, the
    road, and signs, for example; and, of course, facial recognition. Computer vision
    solutions based on machine learning and deep learning require you, the Data Scientist,
    to build, train, and evaluate models that can differentiate one object from another
    and then accurately classify those detected objects.
  prefs: []
  type: TYPE_NORMAL
- en: As you've seen in other projects we've worked on, CNNs are very powerful models
    for image data. We need to look at expansions on the basic architecture that has
    performed so well on a single (still) image with simple information to see what
    works best for complex images and video.
  prefs: []
  type: TYPE_NORMAL
- en: 'Progress recently has been made with these networks: Faster R-CNN, **region-based
    fully convolutional network** (**R-FCN**), MultiBox, **solid-state drive** (**SSD**),
    and **you only look once** (**YOLO**). We''ve seen the value of these models in
    common consumer applications such as Google Photos and Pinterest Visual Search.
    We are even seeing some of these that are lightweight and fast enough to perform
    well on mobile devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent progress in the field can be researched with the following list of references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection*,
    arXiv:1608.08021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R-CNN: Rich feature hierarchies for accurate object detection and semantic
    segmentation*, CVPR, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SPP: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition*,
    ECCV, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fast R-CNN*, arXiv:1504.08083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*,
    arXiv:1506.01497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R-CNN minus R*, arXiv:1506.06981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*End-to-end people detection in crowded scenes*, arXiv:1506.04878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*YOLO – You Only Look Once: Unified, Real-Time Object Detection*, arXiv:1506.02640'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent
    Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Residual Network: Deep Residual Learning for Image Recognition*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R-FCN: Object Detection via Region-based Fully Convolutional Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SSD: Single Shot MultiBox Detector*, arXiv:1512.02325'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, following is the timeline of how the evolution of object detection has
    developed from 1999–2017:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53eba6c6-940b-430f-a304-a6e636fbc0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: The timeline of the evolution of object detection from 1999 to
    2017'
  prefs: []
  type: TYPE_NORMAL
- en: The files for this chapter can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: Improvements in object detection models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection and classification has been the subject of study for quite
    some time. The models that have been used build on the great success of previous
    researchers. A brief summary of progress history starts by highlighting the computer
    vision model called **Histogram of Oriented Gradients** (**HOG**) features that
    was developed by Navneet Dalal and Bill Triggs in 2005.
  prefs: []
  type: TYPE_NORMAL
- en: HOG features were fast and performed well. Interest in deep learning and the
    great success of CNNs that were more accurate classifiers due to their deep networks.
    But the problem was that the CNNs of the time were too slow in comparison.
  prefs: []
  type: TYPE_NORMAL
- en: The solution was to take advantage of the CNNs, improved classification capabilities
    and improve their speed with a technique and employ a selective search paradigm
    in what became known as R-CNN. Reducing the number of bounding boxes did show
    improvements in speed, but not sufficiently for the expectations.
  prefs: []
  type: TYPE_NORMAL
- en: SPP-net was a proposed solution, wherein a CNN representation for the whole
    image was calculated and drove CNN-calculated representations for each sub-section
    generated by selective search. Selective search uses image features to generate
    all the possible locations for an object by looking at pixel intensity, color,
    image texture, and a measure of insideness. These identified objects are then
    fed into the CNN model for classification.
  prefs: []
  type: TYPE_NORMAL
- en: This, in turn, saw improvements in a model named Fast R-CNN that trained end-to-end,
    and thereby fixed the primary problems with SPP-net and R-CNN. Advancing this
    technology further with a model named Faster R-CNN, the technique of using small
    regional proposal CNNs in place of the selective search performed very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick overview of the Faster R-CNN object detection pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/368acba3-00c2-434a-8654-fbc475c089d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A quick benchmark comparison of the versions of R-CNN discussed previously
    shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | R-CNN | Fast R-CNN | Faster R-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Average response time |  ~50 sec | ~2 sec | ~0.2 sec |'
  prefs: []
  type: TYPE_TB
- en: '| Speed boost | 1x | 25x | 250x |'
  prefs: []
  type: TYPE_TB
- en: The performance improvement is impressive, with Faster R-CNN being one of the
    most accurate and fastest object detection algorithms deployed in real-time use
    cases. Other recent powerful alternatives include YOLO models, which we will look
    into in detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start our project with a basic or traditional implementation of **Open
    Source Computer Vision** (**OpenCV**). This library is primarily targeted at real-time
    applications that need computer vision capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has its API wrappers in various languages such as C, C++, Python, and
    so on, and the best way forward is to build a quick prototype using Python wrappers
    or any other language you are comfortable with, and once you are ready with your
    code, rewrite it in C/C++ for production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using the Python wrappers to create our initial
    object detection module.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's do it.
  prefs: []
  type: TYPE_NORMAL
- en: A handcrafted red object detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to create a feature extractor that will be
    able to detect any red object from the provided image using various image processing
    techniques such as erosion, dilation, blurring, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Installing dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to install OpenCV, which we do with this simple `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will import it along with other modules for visualizations and matrix
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, let''s define some helper functions that will help us to plot the images
    and the contours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Exploring image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing in any data science problem is to explore and understand the
    data. This helps us to make our objective clear. So, let''s first load the image
    and examine the properties of that image, such as the color spectrum and the dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/098e0712-7ea7-4ac1-90da-ed992e3e6110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the order of the image stored in the memory is **Blue Green Red** (**BGR**),
    we need to convert it into **Red Green Blue** (**RGB**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac13e819-06e9-48c4-bc4b-484e1a59adc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: The raw input image in RGB color format.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be scaling down the image dimensions, for which we will be using the
    `cv2.resize()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will perform a blur operation to make the pixels more normalized, for
    which we will be using the Gaussian kernel. Gaussian filters are very popular
    in the research field and are used for various operations, one of which is the
    blurring effect that reduces the noise and balances the image. The following code
    performs a blur operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will convert the RGB-based image into an HSV color spectrum, which will
    help us to extract other characteristics of the image using color intensity, brightness,
    and shades:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d00d3547-1d33-4c34-8ee6-1abfc475d0f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure: 9.3: The raw input image in HSV color format.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to create a mask that can detect the specific color spectrum; let''s
    say red in our case. Now we will create two masks that will be performing feature
    extraction using the color values and the brightness factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is how our mask looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e454d53-c8fa-420c-bbd0-267d80f9645d.png)'
  prefs: []
  type: TYPE_IMG
- en: Post-processing of a mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we are able to create our mask successfully, we need to perform some morphological
    operations, which are basic image processing operations used for the analysis
    and processing of geometrical structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a kernel that will perform various morphological operations
    over the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Closing**: *Dilation followed by erosion* is helpful to close small pieces
    inside the foreground objects or small black points on the object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s perform the close operation over the mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The opening operation *erosion followed by dilation* is used to remove noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we perform the opening operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42972971-f2af-4212-a1e9-c8e181bf2560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: This figure illustrated the output of morphological close and open
    operation (left side) and we combine the both to get the final processed mask(right
    side).'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot you can see (in the left part of the screenshot)
    how the morphological operation changes the structure of the mask and when combining
    both the operations (in the right side of the screenshot) you get a denoised cleaner
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s time to use the mask that we created to extract the object from the image.
    First, we will find the biggest contour using the helper function, which is the
    largest region of our object that we need to extract. Then apply the mask to the
    image and draw a circle bounding box on the extracted object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e4e5617-5b0c-483e-97a5-244910c92c22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: This figure shows that we have detected the red region (car body)
    from the image and plotted an ellipes around it.'
  prefs: []
  type: TYPE_NORMAL
- en: Voila! So, we successfully extracted the image and also drew the bounding box
    around the object using simple image processing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection using deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to build a world-class object detection module
    without much use of traditional handcrafting techniques. Here, will be using the
    deep learning approach, which is powerful enough to extract features automatically
    from the raw image and then use those features for classification and detection
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will build an object detector using a pre-baked Python library that
    can use most of the state-of-the-art pre-trained models, and later on, we will
    learn how to implement a really fast and accurate object detector using YOLO architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Quick implementation of object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection saw an increase in adoption as a result of the industry trend
    towards deep learning after 2012\. Accurate and increasingly fast models such as
    R-CNN, Fast-RCNN, Faster-RCNN, and RetinaNet, and fast yet highly accurate ones
    like SSD and YOLO are in production today. In this section, we will use fully-functional
    pre-baked feature extractors in a Python library that can be used in just a few lines
    of code. Also, we will touch base regarding the production-grade setup for the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's do it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing all the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the same drill that we performed in the previous chapters. First let''s
    install all the dependencies. Here, we are using a Python module called ImageAI
    ([https://github.com/OlafenwaMoses/ImageAI](https://github.com/OlafenwaMoses/ImageAI)),
    which is an effective way to start building your own object detection application
    from scratch in no time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We will be using the Python 3.x environment to run this module.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this implementation, we are going to use a pre-trained ResNet model that
    is trained on the COCO dataset ([http://cocodataset.org/#home](http://cocodataset.org/#home)) (a
    large-scale object detection, segmentation, and captioning dataset). You can also
    use other pre-trained models such as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DenseNet-BC-121-32.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5)) (31.7
    MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inception_v3_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5)) (91.7
    MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resnet50_coco_best_v2.0.1.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5)) (146
    MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resnet50_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5))
    (98.1 MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`squeezenet_weights_tf_dim_ordering_tf_kernels.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5))
    (4.83 MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yolo-tiny.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5))
    (33.9 MB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yolo.h5` ([https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5)):
    237 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get the dataset, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have all the dependencies and pre-trained models ready, we will
    implement a state-of-the-art object detection model. We will import the ImageAI''s`ObjectDetection`
    class using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create the instance for the `ObjectDetection` object and set the model
    type as `RetinaNet()`. Next, we set the part of the ResNet model that we downloaded
    and call the `loadModel()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is loaded into the memory, we can feed a new image to the model,
    which can be of any popular image format, such as JPEG, PNG, and so on. Also,
    the function has no constraint on the size of the image, so, you can use any dimensional
    data and the model will handle it internally. We are using `detectObjectsFromImage()`
    to feed the input image. This method returns the image with some more information
    such as the bounding box coordinates of the detected object, the label of the
    detected object, and the confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some images that are used as input into the model and to perform
    the object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f81fd00b-6c4a-4762-bb4d-db472d850275.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Since I was traveling to Asia (Malaysia/Langkawi) while writing
    this chapter, I decided to give it a shot and use some real images that I captured
    on the go.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is used for inputting images into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, we iterate over the `object_detection` object to read all the objects
    that the model predicted with the respective confidence score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Following are how the results look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed6f0e48-301f-4510-87c2-034ed0882f5e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f0a2c2e0-4b28-4521-8feb-b59dc0bae9bd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/875aae8e-22f7-4858-9fdd-2728bee7c7a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: The results extracted from the object detection model with the
    bounding box around the detected object. Results contain the name of the object
    and the confidence score.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that the pre-trained models performed well enough with very few
    lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have all base code ready, let's deploy the `ObjectDetection` modules
    into production. In this section, we will write a RESTful service that will accept
    the image as an input and returns the detected object as a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a `POST` function that accepts the image files with the PNG, JPG, JPEG, and
    GIF extensions. The uploaded image path is sent to the `ObjectDetection` module,
    which performs the detection and returns the following JSON results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file as `object_detection_ImageAI.py` and execute the following command
    to run the web services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71725c7a-3fab-4516-8cd7-1dd49fe574d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Output on the Terminal screen after successful execution of the
    web service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a separate Terminal, you can now try to call the API, as shown in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Following will be the response output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So, this was awesome; with just a few hours' work, you are ready with a production-grade
    object detection module that is something close to state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection In Real-Time Using YOLOv2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great advancement in object detection and classification was made possible
    with a process where You Only Look Once (YOLO) at an input image. In this single
    pass, the goal is to set the coordinates for the corners of the bounding box to
    be drawn around the detected object and to then classify the object with a regression
    model. This process is capable of avoiding false positives because it takes into
    account contextual information from the whole image, and not just a smaller section
    as in a regional proposal of earlier described methods. The **convolutional neural
    network** (**CNN**) as follows can pass over the image once, and therefore be
    fast enough to function in applications where real-time processing is a requirement.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv2 predicts an N number of bounding boxes and associates a confidence level
    for the classification of the object for each individual grid in an S-by-S grid
    that is established in the immediately preceding step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/294ebbd4-3a6f-4fbf-ad74-66368c53fa24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: The overview of how YOLO works. The input image is divided into
    grids and then been sent into the detection process which results in lots of bounding
    boxes which is further been filtered by applying some thresholds.'
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of this process is to produce a total of S-by-S by N complement
    of boxes. For a great percentage of these boxes you’ll get confidence scores that
    are quite low, and by applying a lower threshold (30% in this case), you can eliminate
    a majority of inaccurately classified objects as shown in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a pre-trained YOLOv2 model in this section for object detection
    and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will explore the data preparation using the existing the COCO
    dataset and a custom dataset. If you want to train the YOLO model with lots of
    classes, then you can follow the instructions provided in the pre-existing part,
    or else if you want to build your custom object detector, then follow the instructions
    provided in the custom build section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the pre-existing COCO dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this implementation, we will be using the COCO dataset. This is a great
    resource dataset for training YOLOv2 to detect, segment, and caption images on
    a large scale. Download the dataset from [http://cocodataset.org](http://cocodataset.org)
    and run the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the train and validation annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s convert the annotations in the COCO format to VOC format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Baker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the folders to store the images and annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Unzip `train2014.zip` and `val2014.zip` under the `images` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Unzip `annotations_trainval2014.zip` into `annotations` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a folder to store the converted data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the folder structure will look after the final transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7be97663-700b-4520-8000-42130ae0f9b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: The illustration of the COCO data extraction and formatting process'
  prefs: []
  type: TYPE_NORMAL
- en: This establishes a perfect correspondence between the image and the annotation.
    When the validation set is empty, we will use a ratio of eight to automatically
    split the training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: The result is that we will have two folders, `./images` and `./annotation`,
    for the training purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Using the custom dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, if you want to build an object detector for your specific use case, then
    you will need to scrape around 100–200 images from the web and annotate them.
    There are lots of annotation tools available online, such as LabelImg ([https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg))
    or **Fast Image Data Annotation Tool** (**FIAT**) ([https://github.com/christopher5106/FastAnnotationTool](https://github.com/christopher5106/FastAnnotationTool)).
  prefs: []
  type: TYPE_NORMAL
- en: For you to play around with the custom object detector, we have provided some
    sample images with respective annotations. Look into the repository folder called `Chapter0
    9/yolo/new_class/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image has its respective annotations, as shown in the following picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/930a677f-c2de-4451-9dff-262b62d6ffbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: The relation between the image and the annotation which is shown
    here'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, let''s download the pre-trained weights from [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/),
    which we will use to initialize our model, and which will train the custom object
    detector on top of these pretrained weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Installing all the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the Keras APIs with a TensorFlow approach to create the YOLOv2
    architecture. Let''s import all the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It is always recommended to use GPUs to train any YOLO models.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the YOLO model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'YOLO models are designed with the set of hyperparameter and some other configuration.
    This configuration defines the type of model to construct, as well as other parameters
    of the model such as the input image size and the list of anchors. You have two
    options at the moment: tiny YOLO and full YOLO. The following code defines the
    type of model to construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the path of the pre-trained model and the images, as in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Defining the YOLO v2 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s have a look at the model architecture of the YOLOv2 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The network architecture that we just created can be found here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are the steps to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the weights that we downloaded and use them to initialize the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomize the weights of the last layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the configurations as in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a training and validation batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Set early stop and checkpoint callbacks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the TensorBoard plots output for just two epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3263e010-3d23-4ae6-8524-a956ff552158.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: The figure represents the loss plots for 2 epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the training is complete, let''s perform the prediction by feeding an
    input image into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will load the model into the memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now set the test image path and read it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'So, here are some of the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b84c805d-2900-4d56-94f2-46ec024be573.png)![](img/1798f426-6f94-43c4-a20f-ec39fcf72364.png)![](img/9a416a09-dfae-4b5a-a7f6-8b5b2b55c087.png)![](img/cb951cf2-9574-4e15-b96c-2f5612422785.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations—you have developed a state-of-the-art object detector that is
    very fast and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about building a world class object detection model using YOLO architecture
    and the results seems to be very promising. Now you can also deploy the same on
    other mobile devices or Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image segmentation is the process of categorizing what is in a picture at a
    pixel level. For example, if you were given a picture with a person in it, separating
    the person from the image is known as segmentation and is done using pixel-level
    information.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the COCO dataset for image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is what you should do before executing any of the SegNet scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: When executing SegNet scripts, make sure that your present working directory
    is `SegNet`.
  prefs: []
  type: TYPE_NORMAL
- en: Importing all the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure to restart the session before proceeding forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using `numpy`, `pandas`, `keras`, `pylab`, `skimage`, `matplotlib`,
    and `pycocotools`, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start off by defining the location of the annotation file we will be
    using for image segmentation, and then we will initialize the COCO API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we are building a binary segmentation model, let us consider the images
    from the `images/train2014` folder that are only tagged with the person label
    so that we can segment the person out of the image. The COCO API provides us with
    easy-to-use methods, two of which are the `getCatIds` and `getImgIds`. The following
    snippet will help us extract the image IDs of all the images with the label `person` tagged
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us use the following snippet to plot an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following picture as an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/121f058f-bee6-4231-9169-85637d6d25df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: The plot representation a sample image from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous code snippet, we feed in an image ID to the `loadImgs` method
    of COCO to extract the details of the image it corresponds to. If you look at
    the output of the `img` variable, one of the keys listed is the `file_name` key.
    This key holds the name of the image located in the `images/train2014/` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Then we read the image using the `imread` method of the `io` module we have
    already imported and plot it using `matplotlib.pyplot`.
  prefs: []
  type: TYPE_NORMAL
- en: Annotations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let us load the annotation corresponding to the previous picture and plot
    the annotation on top of the picture. The `coco.getAnnIds()` function helps load
    the annotation info of an image using its ID. Then, with the help of the `coco.loadAnns()`
    function, we load the annotations and plot it using the `coco.showAnns()` function.
    It is important that you first plot the image and then perform the annotation
    operations as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2953e1ec-c34f-4069-a3f0-92297a412b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Visualizing annotation on an image'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to obtain the annotation label array, use the `coco.annToMask()`
    function as shown in the following code snippet. This array will help us form
    the segmentation target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16120870-43e2-45da-8eea-aa4a4388ac25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: Visualizing just the annotation'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us now define a `data_list()` function that will automate the process of
    loading an image and its segmentation array into memory and resize them to the
    shape of 360*480 using OpenCV. This function returns two lists containing images
    and segmentation array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Normalizing the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s define the `make_normalize()` function, which accepts an image
    and performs the histogram normalization operation on it. The return object is
    a normalized array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbc0136c-88e6-40b6-9fae-5f3968f90ac7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Before and After histogram normalization on an image'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we see the original picture on the left, which
    is very visible, and on the right we see the normalized picture, which is not
    at all visible.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the `make_normalize()` function defined, let''s now define a `make_target`
    function. This function accepts the segmentation array of shape (360,480) and
    then returns a segmentation target of shape (`360`,`480`,`2`). In the target,
    channel `0` represents the background and will have `1` in locations that represent
    the background in the image and zero elsewhere. Channel `1` represents the person
    and will have `1` in locations that represent the person in the image and `0` elsewhere. The
    following code implements the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cc9dfd9-3ec8-42d1-a7b1-ac13c6fc0845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: Visualizing the encoded target arrays'
  prefs: []
  type: TYPE_NORMAL
- en: Model data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now define a function called `model_data()` that accepts a list of images
    and a list of labels. This function will apply the `make_normalize()` function on
    each image for the purpose of normalizing, and it will apply the `make_encode()` function
    on each label/segmentation array to obtain the encoded array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The return of this function is two lists, one containing the normalized images
    and the other containing the corresponding target arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we have also split the data into train, test, and
    validation sets, with the train set containing `1500` data points, the validation
    set containing `400` data points, and the test set containing `97` data points.
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the defined hyperparameters that we will be using
    throughout the code, and they are totally configurable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about `optimizers` and their APIs in Keras, visit [https://keras.io/optimizers/](https://keras.io/optimizers/).
    Reduce `batch_size` if you get a resource exhaustion error with respect to the
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with different learning rates, `optimizers`, and `batch_size` to
    see how these factors affect the quality of your model, and if you get better
    results, show them to the deep learning community.
  prefs: []
  type: TYPE_NORMAL
- en: Define SegNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the purpose of image segmentation, we will build a SegNet model, which
    is very similar to the autoencoder we built in [Chapter 8](acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml): *Handwritten
    Digits Classification Using ConvNets*, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/695f6e37-8388-4a91-9d86-fffda1adfe5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: SegNet architecture used in this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SegNet model we''ll define will accept (*3,360, 480*) images as input with
    (*172800, 2*) segmentation arrays as the targets, and it will have the following
    characteristics in the encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is a Convolution 2D layer with 64 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is a Convolution 2D layer with 128 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third layer is a Convolution 2D layer with 256 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with MaxPooling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth layer is again a Convolution 2D layer with 512 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And the model will have the following characteristics in the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is a Convolution 2D layer with 512 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is a Convolution 2D layer with 256 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third layer is a Convolution 2D layer with 128 filters of size 3*3, with
    `activation` as `relu`, followed by batch normalization, followed by downsampling
    with UpSampling2D of size 2*2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth layer is a Convolution 2D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by batch normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fifth layer is a Convolution 2D layer with 2 filters of size 1*1, followed
    by Reshape, Permute and a `softmax` as `activation` layer for predicting scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model is described with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the model defined, compile the model with ''`categorical_crossentropy`''
    as `loss` and `optimizer` as `Adam`, as defined by the `optimizer` variable in
    the hyperparameters section. We will also define `ReduceLROnPlateau` to reduce
    the learning rate as needed when training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the model compiled, we will now fit the model on the data using the `fit`
    method of the model. Here, since we are training on a small set of data, it is
    important to set the parameter shuffle to `True` so that the images are shuffled
    after each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28aa5d4e-2914-4b58-9c56-19f9ff8f4da2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19: Training output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the accuracy and loss plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ed63cdf-9267-4845-9e39-e472d5301333.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.20: Plot showing training progression'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the model trained, evaluate the model on test data, as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We see that the SegNet model we built has a loss of 0.539 and accuracy of 76.33
    on test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the test images and their corresponding generated segmentations
    to understand model learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Following should be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24987ab2-aacf-4f3d-8d4f-d94bc4787c61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.21: Segmentation generated on test images'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding figure, we see that the model was able to segment the person
    from the image.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of the project was to build an object detection classifier using
    YOLO architecture in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the project was to build a binary image segmentation model
    on COCO images that contain just a person, aside from the background. The goal
    was to build a good enough model to segment out the person from the background
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: The model we build by training on 1500 images, each of shape 360*480*3, has
    an accuracy of 79% on train data, and 78% on validation and test data. The model
    is successfully able to segment the person in the image, but the borders of the
    segmentations are slightly off from where they should be. This is due to using
    a small training set. Considering the number of images used for training, the
    model did a good job of segmenting.
  prefs: []
  type: TYPE_NORMAL
- en: There are more images available in this dataset that can be used for training,
    and it might take over a day to train on all of them using a Nvidia Tesla K80
    GPU, but doing so will give you really good segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this chapter, we learnt how to build a RESTful service
    for object detection using an existing classifier, and we also learned to build
    an accurate object detector using the YOLO architecture object detection classifier using Keras,
    while also implementing transfer learning. In the second part of the chapter,
    we understood what image segmentation is and built an image segmentation model
    on images from the COCO dataset. We also tested the performance of the object
    detector and the image segmenter on test data, and determined that we succeeded
    in achieving the goal.
  prefs: []
  type: TYPE_NORMAL
