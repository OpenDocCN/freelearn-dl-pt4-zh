<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Face Recognition Using FaceNet</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned how to detect objects in an image. In this chapter, we will look into a specific use case of object detection<span>—</span>face recognition. Face recognition is a combination of two major operations: face detection, followed by face classification.</p>
<p>The (hypothetical) client that provides our business use case for us in this project is a high-performance computing data center Tier III, certified for sustainability. They have designed the facility to meet the very highest standards for protection against natural disa<span>sters, with many redundant systems.</span></p>
<p><span>The facility currently has ultra-high security protocols in place to prevent malicious, man-made disasters, and they are looking to augment their security profile with facial recognition for access to secure areas throughout the facility.</span></p>
<p><span>The stakes are high, as the servers they house and maintain process some of the most sensitive, valuable, and influential data in the world:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1416 image-border" src="assets/502f99fb-81dc-4e6b-be61-e3723e779922.png" style="width:17.75em;height:18.75em;"/></p>
<p class="mce-root"><span>This facial recognition system would need to be able to accurately identify not only their own employees, but employees of their clients, who occasionally tour the data center for inspection.</span></p>
<p class="mce-root">They have asked us to provide a POC for this intelligence-based capability, for review and later inclusion throughout their data center.</p>
<p>So, in this chapter, we will learn how to build a world-class face recognition system. We will define the pipeline as follows:</p>
<ol class="postList">
<li class="graf graf--li graf-after--p"><strong>Face detection</strong>: First, look at an image and find all the possible faces in it</li>
<li class="graf graf--li graf-after--li"><strong>Face extraction</strong>: Second, focus on each face image and understand it, for example if it is turned sideways or badly lit</li>
<li class="graf graf--li graf-after--li"><strong>Feature extraction</strong>: Third, extract unique features from the faces using convolutional neural networks (CNNs)</li>
<li class="graf graf--li graf-after--li"><strong>Classifier training</strong>: Finally, compare the unique features of that face to all the people already known, to determine the person's name</li>
</ol>
<p><span>You will learn the main ideas behind each step, and how to build your own facial recognition system in Python using the following deep-learning technologies</span>:</p>
<ul>
<li><span><strong>dlib</strong> (<strong><a href="http://dlib.net/" target="_blank">http://dlib.net/</a></strong>): Provides a library that can be used for facial detection and alignment.</span></li>
<li class="mce-root"><span><strong>OpenFace</strong> (<strong><a href="https://cmusatyalab.github.io/openface/" target="_blank">https://cmusatyalab.github.io/openface/</a></strong>): A deep-learning facial recognition model, developed by </span>Brandon Amos <em>et al</em> (<a href="http://bamos.github.io/" target="_blank">http://bamos.github.io/</a>). It is able to run on<span> </span>real-time<span> </span>mobile devices<span> </span>as well.</li>
<li><strong>FaceNet<span> </span></strong><span>(</span><strong><a href="https://arxiv.org/abs/1503.03832" target="_blank">https://arxiv.org/abs/1503.03832</a></strong><span>)</span><span>: A</span><span> CNN architecture that is used for feature extraction. For a loss function, FaceNet uses triplet loss. Triplet loss relies on minimizing the distance from positive examples, while maximizing the distance from negative examples.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setup environment</h1>
                </header>
            
            <article>
                
<p>Since setup can get very complicated and take a long time, which is not on the agenda for this chapter, <span>we will be building a Docker image that contains all the dependencies, including dlib, OpenFace, and FaceNet.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the code</h1>
                </header>
            
            <article>
                
<p>Fetch the code that we will use to build face recognition <span>from the repository</span>:</p>
<pre><strong>git clone https://github.com/PacktPublishing/Python-Deep-Learning-Projects</strong><br/><strong>cd Chapter10/</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the Docker image</h1>
                </header>
            
            <article>
                
<p class="graf graf--p graf-after--p">Docker is a container platform that simplifies deployment. It solves the problem of installing software dependencies onto different server environments. <span>If you are new to Docker, you can read more at <a href="https://www.docker.com/" target="_blank">https://www.docker.com/</a></span><span>.</span></p>
<p class="graf graf--p graf-after--p"><span>To install Docker on Linux machines, run the following command:</span></p>
<pre class="graf graf--pre graf-after--p"><strong>curl https://get.docker.com | sh</strong></pre>
<p>For other systems such as macOS and Windows, visit <a href="https://docs.docker.com/install/" target="_blank">https://docs.docker.com/install/</a>. <span>You can skip this step if you already have Docker installed. </span></p>
<p class="mce-root"/>
<p>Once Docker is installed, you should be able to use the <kbd>docker</kbd> command in the Terminal, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b83035db-a849-4191-b03a-ac2606bb586d.png"/></p>
<p>Now we will create a <kbd>docker</kbd> file that will install all the dependencies, including OpenCV, dlib, and TensorFlow. This file is available in the repository at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile</a>:</p>
<pre><strong>#Dockerfile for our env setup</strong><br/>FROM tensorflow/tensorflow:latest<br/><br/>RUN apt-get update -y --fix-missing<br/>RUN apt-get install -y ffmpeg<br/>RUN apt-get install -y build-essential cmake pkg-config \<br/>                    libjpeg8-dev libtiff5-dev libjasper-dev libpng12-dev \<br/>                    libavcodec-dev libavformat-dev libswscale-dev libv4l-dev \<br/>                    libxvidcore-dev libx264-dev \<br/>                    libgtk-3-dev \<br/>                    libatlas-base-dev gfortran \<br/>                    libboost-all-dev \<br/>                    python3 python3-dev python3-numpy<br/><br/>RUN apt-get install -y wget vim python3-tk python3-pip<br/><br/>WORKDIR /<br/>RUN wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.2.0.zip \<br/>    &amp;&amp; unzip opencv.zip \<br/>    &amp;&amp; wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.2.0.zip \<br/>    &amp;&amp; unzip opencv_contrib.zip<br/><br/># install opencv3.2<br/>RUN cd /opencv-3.2.0/ \<br/>   &amp;&amp; mkdir build \<br/>   &amp;&amp; cd build \<br/>   &amp;&amp; cmake -D CMAKE_BUILD_TYPE=RELEASE \<br/>            -D INSTALL_C_EXAMPLES=OFF \<br/>            -D INSTALL_PYTHON_EXAMPLES=ON \<br/>            -D OPENCV_EXTRA_MODULES_PATH=/opencv_contrib-3.2.0/modules \<br/>            -D BUILD_EXAMPLES=OFF \<br/>            -D BUILD_opencv_python2=OFF \<br/>            -D BUILD_NEW_PYTHON_SUPPORT=ON \<br/>            -D CMAKE_INSTALL_PREFIX=$(python3 -c "import sys; print(sys.prefix)") \<br/>            -D PYTHON_EXECUTABLE=$(which python3) \<br/>            -D WITH_FFMPEG=1 \<br/>            -D WITH_CUDA=0 \<br/>            .. \<br/>    &amp;&amp; make -j8 \<br/>    &amp;&amp; make install \<br/>    &amp;&amp; ldconfig \<br/>    &amp;&amp; rm /opencv.zip \<br/>    &amp;&amp; rm /opencv_contrib.zip<br/><br/><br/># Install dlib 19.4<br/>RUN wget -O dlib-19.4.tar.bz2 http://dlib.net/files/dlib-19.4.tar.bz2 \<br/>    &amp;&amp; tar -vxjf dlib-19.4.tar.bz2<br/><br/>RUN cd dlib-19.4 \<br/>    &amp;&amp; cd examples \<br/>    &amp;&amp; mkdir build \<br/>    &amp;&amp; cd build \<br/>    &amp;&amp; cmake .. \<br/>    &amp;&amp; cmake --build . --config Release \<br/>    &amp;&amp; cd /dlib-19.4 \<br/>    &amp;&amp; pip3 install setuptools \<br/>    &amp;&amp; python3 setup.py install \<br/>    &amp;&amp; cd $WORKDIR \<br/>    &amp;&amp; rm /dlib-19.4.tar.bz2<br/><br/><br/><br/>ADD $PWD/requirements.txt /requirements.txt<br/>RUN pip3 install -r /requirements.txt<br/><br/><br/>CMD ["/bin/bash"]</pre>
<p>Now execute the following command to build the image:</p>
<pre class="graf graf--pre graf-after--p"><strong>docker build -t hellorahulk/facerecognition -f Dockerfile</strong></pre>
<p>It will take approximately 20-30 mins to install all the dependencies and build the Docker image: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/72357caa-7331-4479-aca1-1b83aa503441.png" style="width:298.83em;height:33.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading pre-trained models</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will download a few more artifacts, which we will use and discuss in detail later in this chapter.</p>
<p class="mce-root">Download dlib's<span> face landmark pr</span><span>edictor,</span> <span>using the following commands:</span></p>
<pre class="graf graf--pre graf-after--p"><strong>curl -O http://dlib.net/</strong></pre>
<pre class="graf graf--pre graf-after--p"><strong>files/shape_predictor_68_face_landmarks.dat.bz2</strong><br/><strong>bzip2 -d shape_predictor_68_face_landmarks.dat.bz2</strong><br/><strong>cp shape_predictor_68_face_landmarks.dat facenet/</strong></pre>
<p>Download the pre-trained Inception model:</p>
<pre class="graf graf--pre graf-after--p"><strong><span>curl -L -O https://www.dropbox.com/s/hb75vuur8olyrtw/Resnet-185253.pb</span></strong><br/><strong>cp <span>Resnet-185253.pb pre-model</span>/</strong></pre>
<p>Once we have all the components ready, the folder structure should look roughly as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43e24df1-b3c1-427d-bec4-5f4588a01464.png" style="width:28.58em;height:35.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The folder structure of the code</div>
<p>Make sure that you keep the images of the person you want to train the model with <span>in the </span><kbd>/data</kbd><span> folder</span>, and name the folder as <kbd>/data/&lt;class_name&gt;/&lt;class_name&gt;_000&lt;count&gt;.jpg</kbd>.</p>
<p>The <kbd>/output</kbd> folder will contain the trained SVM classifier and all preprocessed images inside a subfolder <kbd>/intermediate</kbd>, using the same folder nomenclature as in the <kbd>/data</kbd> folder.</p>
<div class="packt_tip"><strong>Pro tip</strong>: For better performance in terms of accuracy, always keep more than five samples of images for each class. This will help the model to converge faster and generalize better.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the pipeline</h1>
                </header>
            
            <article>
                
<p><span>Facial recognition is a biometric solution that measures the unique characteristics of faces. To perform facial recognition, you'll need a way to uniquely represent a face.</span></p>
<p><span>The main idea behind any face recognition system is to break the face down into unique features, and then use those features to represent identity.</span></p>
<div class="packt_infobox">Building a robust pipeline for feature extraction is very important, as it will directly affect the performance and accuracy of our system. In <span>1960, Woodrow Bledsoe used a technique involving marking the coordinates of prominent features of a face. Among these features were the location of hairline, eyes, and nose.<br/></span><span><br/>
L</span><span>ater, in 2005, a much robust technique was invented, <strong>Histogram of Oriented Gradients</strong> (<strong>HOG</strong>). This captured the orientation of the dense pixels in the provided image.<br/>
<br/></span> <span>The most advanced technique yet, outperforming all others at the time of writing, uses CNNs. In 2015, researchers from Google released a paper describing their system, FaceNet (<a href="https://arxiv.org/abs/1503.03832" target="_blank">https://arxiv.org/abs/1503.03832</a>), which uses a CNN relying on image pixels to identify features, rather than extracting them manually.</span></div>
<p>To build the face recognition pipeline, we will devise the following flow (represented by orange blocks in the diagram):</p>
<ul>
<li><strong>Preprocessing</strong>: Finding all the faces, fixing the orientation of the faces</li>
<li><strong>Feature extraction</strong>: Extracting unique features from the processed faces</li>
<li><strong>Classifier training</strong>: Training the SVM classifier with 128 dimensional features</li>
</ul>
<p>The diagram is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1417 image-border" src="assets/2ac0f9bc-860a-4d3e-bb7e-03fe36b45693.png" style="width:162.50em;height:95.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">This image illustrates the end to end flow for face recognition pipeline</div>
<p>We will look into each of the steps, and build our world-class face recognition system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing of images</h1>
                </header>
            
            <article>
                
<p><span>The first step in our pipeline is </span>face detection<span>. We will then align the faces, extract features, and then finalize our preprocessing on Docker.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection</h1>
                </header>
            
            <article>
                
<p><span>Obviously, it's very important to first locate the faces in the given photograph so that they can be fed into the later part of the pipeline. There are lots of ways to detect faces, such as detecting skin textures, oval/round shape detection, and other statistical methods. We're going to use a method called HOG.</span></p>
<div class="packt_infobox"><span><strong>HOG</strong> is a feature descriptor that <span>represen</span><span>ts</span><strong> </strong>the distribution (histograms) of directions of gradients (oriented gradients), which are used as features. Gradients (<em>x</em> and <em>y</em> derivatives) of an image are useful, because the magnitude of gradients is large around edges and corners (regions of abrupt intensity changes), which are <span>excellent</span> <span>features</span> in a given image.</span></div>
<div>
<p><span><span>To find faces in an image, we'll convert the image into greyscale. </span></span>Then we'll look at every single pixel in our image, one at a time, and try to extract the orientation of the pixels using the HOG detector. We'll be using <kbd>dlib.get_frontal_face_detector()</kbd> to create our face detector.</p>
<p>The following small snippet demonstrates the HOG-based face detector being used in the implementation:</p>
<pre>import sys<br/>import dlib<br/>from skimage import io<br/><br/><br/><strong># Create a HOG face detector using the built-in dlib class</strong><br/>face_detector = dlib.get_frontal_face_detector()<br/><br/><strong># Load the image into an array</strong><br/>file_name = 'sample_face_image.jpeg'<br/>image = io.imread(file_name)<br/><br/><strong># Run the HOG face detector on the image data.</strong><br/><strong># The result will be the bounding boxes of the faces in our image.</strong><br/>detected_faces = face_detector(image, 1)<br/><br/>print("Found {} faces.".format(len(detected_faces)))<br/><br/><strong># Loop through each face we found in the image</strong><br/>for i, face_rect in enumerate(detected_faces):<br/><strong>  # Detected faces are returned as an object with the coordinates </strong><br/><strong>  # of the top, left, right and bottom edges</strong><br/>  print("- Face #{} found at Left: {} Top: {} Right: {} Bottom: {}".format(i+1, face_rect.left(), face_rect.top(), face_rect.right(), face_rect.bottom()))</pre>
<p>The output is as follows:</p>
<pre><strong>Found 1 faces. </strong><br/><strong>-Face #1 found at Left: 365 Top: 365 Right: 588 Bottom: 588</strong></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aligning faces</h1>
                </header>
            
            <article>
                
<p>Once we know the region in which the face is located, we can perform various kinds of isolation techniques to extract the face from the overall image.</p>
<p>One challenge to deal with is that faces in images may be turned in different directions, making them look different to the machine.</p>
<p class="graf graf--p graf-after--figure">To solve this issue, we will warp each image so that the eyes and lips are always in the sample place in the provided images. This will make it a lot easier for us to compare faces in the next steps. To do so, we are going to use an algorithm called<span> </span><strong>face landmark estimation</strong>.</p>
<div class="packt_infobox"><span>The basic idea is we will come up with 68 specific points (called </span><em>landmarks</em><span>) that exist on every face—the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, and so on. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face.</span></div>
<p><span>The 68 landmarks we will locate on every face are shown in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9bef96ab-549f-4ff0-827b-a61f8b83857f.png" style="width:31.67em;height:30.17em;"/></div>
<p>This image was created by<span> </span>Brandon Amos (<a href="http://bamos.github.io/">http://bamos.github.io/</a>), who works on<span> </span>OpenFace (<a href="https://github.com/cmusatyalab/openface" target="_blank">https://github.com/cmusatyalab/openface</a>).</p>
<p class="graf graf--pre graf-after--p">Here is a small snippet demonstrating how to use face landmarks, which we downloaded in the <em>Setup environment</em> section<span>:</span></p>
<pre>import sys<br/>import dlib<br/>import cv2<br/>import openface<br/><br/>predictor_model = "shape_predictor_68_face_landmarks.dat"<br/><br/><strong># Create a HOG face detector , Shape Predictor and Aligner</strong><br/>face_detector = dlib.get_frontal_face_detector()<br/>face_pose_predictor = dlib.shape_predictor(predictor_model)<br/>face_aligner = openface.AlignDlib(predictor_model)<br/><br/><strong># Take the image file name from the command line</strong><br/>file_name = 'sample_face_image.jpeg'<br/><br/><strong># Load the image</strong><br/>image = cv2.imread(file_name)<br/><br/><strong># Run the HOG face detector on the image data</strong><br/>detected_faces = face_detector(image, 1)<br/><br/>print("Found {} faces.".format(len(detected_faces))<br/><br/># Loop through each face we found in the image<br/>for i, face_rect in enumerate(detected_faces):<br/><br/>  # Detected faces are returned as an object with the coordinates <br/>  # of the top, left, right and bottom edges<br/>  print("- Face #{} found at Left: {} Top: {} Right: {} Bottom: {}".format(i, face_rect.left(), face_rect.top(), face_rect.right(), face_rect.bottom()))<br/><br/><strong>  # Get the the face's pose</strong><br/>  pose_landmarks = face_pose_predictor(image, face_rect)<br/><br/><strong>  # Use openface to calculate and perform the face alignment</strong><br/>  alignedFace = face_aligner.align(534, image, face_rect, landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)<br/><br/><strong>  # Save the aligned image to a file</strong><br/>  cv2.imwrite("aligned_face_{}.jpg".format(i), alignedFace)</pre>
<p><span>Using this, we can perform various </span><span>basic image transformations such as rotation and scaling while preserving parallel lines. These are also known as </span><span>affine transformations (</span><a href="https://en.wikipedia.org/wiki/Affine_transformation" target="_blank">https://en.wikipedia.org/wiki/Affine_transformation</a><span>).</span></p>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5904612e-0818-43da-84a5-495b38a742f2.png"/></p>
<p>With <span>segmentation, we solved finding the largest face in an image, and with alignment, we standardized the input image to be in the center based on the location of eyes and bottom lip.</span></p>
<p><span>Here is a sample from our dataset, showing the raw image and processed image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1418 image-border" src="assets/e94c82b8-af23-45bf-b3c4-b75e74487302.png" style="width:38.92em;height:45.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature extraction</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Now that we've segmented and aligned the data, we'll generate vector embeddings of each identity. These embeddings can then be used as input to a classification, regression, or clustering task. </span></p>
<p class="mce-root"><span>This process of training a CNN to output face embeddings requires a lot of data and computer power. However, once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once.</span></p>
<p><span>For convenience, we have provided a model that has been pre-trained on Inception-Resnet-v1, which you can run over any face image to get the 128 dimension feature vectors. We downloaded this file in the <em>Setup environment</em> section, and it's located in the <kbd>/pre-model/Resnet-185253.pb</kbd> directory.</span></p>
<div class="mce-root packt_tip"><span>If you want to try this step yourself, OpenFace provides a Lua script (<a href="https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua" target="_blank">https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua</a>) that will generate embeddings for all images in a folder and write them to a CSV file.<br/>
<br/>
<a href="https://gist.github.com/ageitgey/ddbae3b209b6344a458fa41a3cf75719" target="_blank"/></span></div>
<p>The code to create the embeddings for the input images can be found further after the paragraph. The code is available in the repository at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py</a>. </p>
<p><span>In the process, we are loading trained components from the Resnet model such as </span><kbd>embedding_layer</kbd><span>, </span><kbd>images_placeholder</kbd><span>, and </span><kbd>phase_train_placeholder</kbd><span>, along with the images and the labels</span>:</p>
<pre class="mce-root">def _create_embeddings(embedding_layer, images, labels, images_placeholder, phase_train_placeholder, sess):<br/>    """<br/>    Uses model to generate embeddings from :param images.<br/>    :param embedding_layer: <br/>    :param images: <br/>    :param labels: <br/>    :param images_placeholder: <br/>    :param phase_train_placeholder: <br/>    :param sess: <br/>    :return: (tuple): image embeddings and labels<br/>    """<br/>    emb_array = None<br/>    label_array = None<br/>    try:<br/>        i = 0<br/>        while True:<br/>            batch_images, batch_labels = sess.run([images, labels])<br/>            logger.info('Processing iteration {} batch of size: {}'.format(i, len(batch_labels)))<br/>            emb = sess.run(embedding_layer,<br/>                           feed_dict={images_placeholder: batch_images, phase_train_placeholder: False})<br/><br/>            emb_array = np.concatenate([emb_array, emb]) if emb_array is not None else emb<br/>            label_array = np.concatenate([label_array, batch_labels]) if label_array is not None else batch_labels<br/>            i += 1<br/><br/>    except tf.errors.OutOfRangeError:<br/>        pass<br/><br/>    return emb_array, label_array</pre>
<p>Here is a quick view of the embedding creating process. We fed the image and the label data along with few components from the pre-trained model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d76fb9cd-290b-4b07-b66d-9abfa52b0cde.png" style="width:89.83em;height:37.33em;"/></p>
<p>The output of the process will be a vector of 128 dimensions, representing the facial image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Execution on Docker</h1>
                </header>
            
            <article>
                
<p><span>We will implement preprocessing on our Docker image. </span><span>We'll mount the <kbd>project</kbd> directory as a volume inside the Docker container (using a <kbd>-v</kbd> flag), and run the preprocessing script on the input data. The results will be written to a directory specified with command-line arguments.</span></p>
<p class="graf graf--p graf-after--figure">The <span><kbd>align_dlib.py</kbd> </span>file is sourced from CMU. It provides methods for detecting a face in an image, finding facial landmarks, and aligning these landmarks:</p>
<pre class="graf graf--pre graf-after--p">docker run -v $PWD:/facerecognition \<br/>-e PYTHONPATH=$PYTHONPATH:/facerecognition \<br/>-it hellorahulk/facerecognition python3 /facerecognition/facenet/preprocess.py \<br/>--input-dir /facerecognition/data \<br/>--output-dir /facerecognition/output/intermediate \<br/>--crop-dim 180</pre>
<p>In the preceding command we are setting the input data path using a<span> </span><kbd>--input-dir</kbd><span> </span>flag. This directory should contain the images that we want to process.</p>
<p>We are also setting the output path using a<span> </span><kbd>--output-dir</kbd><span> </span>flag, which will store the segmented aligned images. We will be using these output images as input for training.</p>
<p>The<span> </span><kbd>--crop-dim</kbd><span> </span>flag is to define the output dimensions of the image. In this case, all images will be stored at 180 × 180.</p>
<p>The outcome of this process will be an<span> </span><kbd>/intermediate</kbd><span> </span>folder being created inside the<span> </span><kbd>/output</kbd><span> </span>folder, containing all the preprocessed images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the classifier</h1>
                </header>
            
            <article>
                
<p><span>First, we'll load the segmented and aligned images from the <kbd>input</kbd> directory <kbd>--input-dir</kbd> flag. While training, we'll apply preprocessing to the image. This preprocessing will add random transformations to the image, creating more images to train on.</span></p>
<p><span>These images will be fed in a batch size of 128 into the pre-trained model. This model will return a 128-dimensional embedding for each image, returning a 128 x 128 matrix for each batch.</span></p>
<p><span>After these embeddings are created, we'll use them as feature inputs into a scikit-learn SVM classifier to train on each identity.</span></p>
<p>The following command will start the process, and train the classifier. The classifier will be dumped as a <kbd>pickle</kbd> file in the path defined in the <kbd>--classifier-path</kbd> argument:</p>
<pre class="graf graf--pre graf-after--figure"><strong>docker run -v $PWD:/facerecognition \</strong><br/><strong>-e PYTHONPATH=$PYTHONPATH:/facerecognition \</strong><br/><strong>-it hellorahulk/facerecognition \</strong><br/><strong>python3 /facerecognition/facenet/train_classifier.py \</strong><br/><strong>--input-dir /facerecognition/output/intermediate \</strong><br/><strong>--model-path /facerecognition/pre-model/Resnet-185253.pb \</strong><br/><strong>--classifier-path /facerecognition/output/classifier.pkl \</strong><br/><strong>--num-threads 16 \</strong><br/><strong>--num-epochs 25 \</strong><br/><strong>--min-num-images-per-class 10 \</strong><br/><strong>--is-train</strong></pre>
<p>A few custom arguments are tunable:</p>
<ul>
<li class="mce-root"><kbd>--num-threads</kbd>: Modify according to the CPU/GPU config</li>
<li class="mce-root"><kbd>--num-epochs</kbd>: Change according to your dataset</li>
<li class="mce-root"><kbd>--min-num-images-per-class</kbd>: Change according to your dataset</li>
<li class="mce-root"><kbd>--is-train</kbd>: Set the <kbd>True</kbd> flag for training</li>
</ul>
<p>This process will take a while, depending on the number of images you are training on. Once the process is completed, you will find a <kbd>classifier.pkl</kbd> file inside the <kbd>/output</kbd> folder.</p>
<p>Now you can use the <kbd>classifier.pkl</kbd> file to make predictions, and deploy it on production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>We will evaluate the performance of the trained model. To do that, we will execute the following command:</p>
<pre class="graf graf--pre graf-after--figure"><strong>docker run -v $PWD:/facerecognition \</strong><br/><strong>-e PYTHONPATH=$PYTHONPATH:/facerecognition \</strong><br/><strong>-it hellorahulk/facerecognition \</strong><br/><strong>python3 /facerecognition/facenet/train_classifier.py \</strong><br/><strong>--input-dir /facerecognition/output/intermediate \</strong><br/><strong>--model-path /facerecognition/pre-model/Resnet-185253.pb \</strong><br/><strong>--classifier-path /facerecognition/output/classifier.pkl \</strong><br/><strong>--num-threads 16 \</strong><br/><strong>--num-epochs 2 \</strong><br/><strong>--min-num-images-per-class 10 \</strong></pre>
<p>Once the execution is completed, you will see predictions with a confidence score, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e955ea4-dd86-46cd-a2ad-ba86ed8d6d9d.png"/></p>
<p>We can see that the model is able to predict with 99.5% accuracy. It is also relatively fast.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have successfully completed a <span>world-class </span>facial recognition POC for our hypothetical high-performance data center, utilizing the deep-learning technologies of OpenFace, dlib, and FaceNet.</p>
<p class="mce-root">We built a pipeline that included:</p>
<ul>
<li class="mce-root"><strong>Face detection</strong>: To examine an image and find all the faces it contains</li>
<li class="mce-root"><strong>Face extraction</strong>: To focus on each face and understand its general qualities </li>
<li class="mce-root"><strong>Feature extraction</strong>: To pull out unique features from the faces using CNNs</li>
<li class="mce-root"><strong>Classifier training</strong>: To compare those unique features to all the people already known, and determine the person's name</li>
</ul>
<p class="mce-root"><span>The added security level of a robust facial recognition system for access control is in keeping with the high standards demanded by this Tier III facility</span><span>. </span>This project is a great example of the power of deep learning to produce solutions that make a meaningful impact on the business operations of our clients.</p>


            </article>

            
        </section>
    </body></html>