["```py\n@dataclasses.dataclass \nclass Hyperparams: \n    env_name: str \n    stop_reward: float \n    run_name: str \n    replay_size: int \n    replay_initial: int \n    target_net_sync: int \n    epsilon_frames: int \n\n    learning_rate: float = 0.0001 \n    batch_size: int = 32 \n    gamma: float = 0.99 \n    epsilon_start: float = 1.0 \n    epsilon_final: float = 0.1 \n\n    tuner_mode: bool = False \n    episodes_to_solve: int = 500 \n\nGAME_PARAMS = { \n    ’pong’: Hyperparams( \n        env_name=\"PongNoFrameskip-v4\", \n        stop_reward=18.0, \n        run_name=\"pong\", \n        replay_size=100_000, \n        replay_initial=10_000, \n        target_net_sync=1000, \n        epsilon_frames=100_000, \n        epsilon_final=0.02, \n    ),\n```", "```py\ndef unpack_batch(batch: tt.List[ExperienceFirstLast]): \n    states, actions, rewards, dones, last_states = [],[],[],[],[] \n    for exp in batch: \n        states.append(exp.state) \n        actions.append(exp.action) \n        rewards.append(exp.reward) \n        dones.append(exp.last_state is None) \n        if exp.last_state is None: \n            lstate = exp.state  # the result will be masked anyway \n        else: \n            lstate = exp.last_state \n        last_states.append(lstate) \n    return np.asarray(states), np.array(actions), np.array(rewards, dtype=np.float32), \\ \n        np.array(dones, dtype=bool), np.asarray(last_states)\n```", "```py\ndef calc_loss_dqn( \n        batch: tt.List[ExperienceFirstLast], net: nn.Module, tgt_net: nn.Module, \n        gamma: float, device: torch.device) -> torch.Tensor: \n    states, actions, rewards, dones, next_states = unpack_batch(batch) \n\n    states_v = torch.as_tensor(states).to(device) \n    next_states_v = torch.as_tensor(next_states).to(device) \n    actions_v = torch.tensor(actions).to(device) \n    rewards_v = torch.tensor(rewards).to(device) \n    done_mask = torch.BoolTensor(dones).to(device) \n\n    actions_v = actions_v.unsqueeze(-1) \n    state_action_vals = net(states_v).gather(1, actions_v) \n    state_action_vals = state_action_vals.squeeze(-1) \n    with torch.no_grad(): \n        next_state_vals = tgt_net(next_states_v).max(1)[0] \n        next_state_vals[done_mask] = 0.0 \n\n    bellman_vals = next_state_vals.detach() * gamma + rewards_v \n    return nn.MSELoss()(state_action_vals, bellman_vals)\n```", "```py\nclass EpsilonTracker: \n    def __init__(self, selector: EpsilonGreedyActionSelector, params: Hyperparams): \n        self.selector = selector \n        self.params = params \n        self.frame(0) \n\n    def frame(self, frame_idx: int): \n        eps = self.params.epsilon_start - frame_idx / self.params.epsilon_frames \n        self.selector.epsilon = max(self.params.epsilon_final, eps)\n```", "```py\ndef batch_generator(buffer: ExperienceReplayBuffer, initial: int, batch_size: int) -> \\ \n        tt.Generator[tt.List[ExperienceFirstLast], None, None]: \n    buffer.populate(initial) \n    while True: \n        buffer.populate(1) \n        yield buffer.sample(batch_size)\n```", "```py\ndef setup_ignite( \n        engine: Engine, params: Hyperparams, exp_source: ExperienceSourceFirstLast, \n        run_name: str, extra_metrics: tt.Iterable[str] = (), \n        tuner_reward_episode: int = 100, tuner_reward_min: float = -19, \n): \n    handler = ptan_ignite.EndOfEpisodeHandler( \n        exp_source, bound_avg_reward=params.stop_reward) \n    handler.attach(engine) \n    ptan_ignite.EpisodeFPSHandler().attach(engine)\n```", "```py\n @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED) \n    def episode_completed(trainer: Engine): \n        passed = trainer.state.metrics.get(’time_passed’, 0) \n        print(\"Episode %d: reward=%.0f, steps=%s, speed=%.1f f/s, elapsed=%s\" % ( \n            trainer.state.episode, trainer.state.episode_reward, \n            trainer.state.episode_steps, trainer.state.metrics.get(’avg_fps’, 0), \n            timedelta(seconds=int(passed)))) \n\n    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED) \n    def game_solved(trainer: Engine): \n        passed = trainer.state.metrics[’time_passed’] \n        print(\"Game solved in %s, after %d episodes and %d iterations!\" % ( \n            timedelta(seconds=int(passed)), trainer.state.episode, \n            trainer.state.iteration)) \n        trainer.should_terminate = True \n        trainer.state.solved = True\n```", "```py\n now = datetime.now().isoformat(timespec=’minutes’).replace(’:’, ’’) \n    logdir = f\"runs/{now}-{params.run_name}-{run_name}\" \n    tb = tb_logger.TensorboardLogger(log_dir=logdir) \n    run_avg = RunningAverage(output_transform=lambda v: v[’loss’]) \n    run_avg.attach(engine, \"avg_loss\")\n```", "```py\n metrics = [’reward’, ’steps’, ’avg_reward’] \n    handler = tb_logger.OutputHandler(tag=\"episodes\", metric_names=metrics) \n    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED \n    tb.attach(engine, log_handler=handler, event_name=event)\n```", "```py\n ptan_ignite.PeriodicEvents().attach(engine) \n    metrics = [’avg_loss’, ’avg_fps’] \n    metrics.extend(extra_metrics) \n    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=metrics, \n                                      output_transform=lambda a: a) \n    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED \n    tb.attach(engine, log_handler=handler, event_name=event)\n```", "```py\n env = gym.make(params.env_name) \n    env = ptan.common.wrappers.wrap_dqn(env) \n\n    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device) \n    tgt_net = ptan.agent.TargetNet(net)\n```", "```py\n selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start) \n    epsilon_tracker = common.EpsilonTracker(selector, params) \n    agent = ptan.agent.DQNAgent(net, selector, device=device)\n```", "```py\n exp_source = ptan.experience.ExperienceSourceFirstLast( \n        env, agent, gamma=params.gamma, env_seed=common.SEED) \n    buffer = ptan.experience.ExperienceReplayBuffer( \n        exp_source, buffer_size=params.replay_size)\n```", "```py\n optimizer = optim.Adam(net.parameters(), lr=params.learning_rate) \n\n    def process_batch(engine, batch): \n        optimizer.zero_grad() \n        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, \n                                      gamma=params.gamma, device=device) \n        loss_v.backward() \n        optimizer.step() \n        epsilon_tracker.frame(engine.state.iteration) \n        if engine.state.iteration % params.target_net_sync == 0: \n            tgt_net.sync() \n        return { \n            \"loss\": loss_v.item(), \n            \"epsilon\": selector.epsilon, \n        }\n```", "```py\n engine = Engine(process_batch) \n    common.setup_ignite(engine, params, exp_source, NAME) \n    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))\n```", "```py\nTrainFunc = tt.Callable[ \n    [Hyperparams, torch.device, dict], \n    tt.Optional[int] \n] \n\nBASE_SPACE = { \n    \"learning_rate\": tune.loguniform(1e-5, 1e-4), \n    \"gamma\": tune.choice([0.9, 0.92, 0.95, 0.98, 0.99, 0.995]), \n}\n```", "```py\ndef tune_params( \n        base_params: Hyperparams, train_func: TrainFunc, device: torch.device, \n        samples: int = 10, extra_space: tt.Optional[tt.Dict[str, tt.Any]] = None, \n): \n    search_space = dict(BASE_SPACE) \n    if extra_space is not None: \n        search_space.update(extra_space) \n    config = tune.TuneConfig(num_samples=samples) \n\n    def objective(config: dict, device: torch.device) -> dict: \n        keys = dataclasses.asdict(base_params).keys() \n        upd = {\"tuner_mode\": True} \n        for k, v in config.items(): \n            if k in keys: \n                upd[k] = v \n        params = dataclasses.replace(base_params, **upd) \n        res = train_func(params, device, config) \n        return {\"episodes\": res if res is not None else 10**6}\n```", "```py\n obj = tune.with_parameters(objective, device=device) \n    if device.type == \"cuda\": \n        obj = tune.with_resources(obj, {\"gpu\": 1}) \n    tuner = tune.Tuner(obj, param_space=search_space, tune_config=config) \n    results = tuner.fit() \n    best = results.get_best_result(metric=\"episodes\", mode=\"min\") \n    print(best.config) \n    print(best.metrics)\n```", "```py\n if params.tuner_mode: \n        @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED) \n        def episode_completed(trainer: Engine): \n            avg_reward = trainer.state.metrics.get(’avg_reward’) \n            max_episodes = params.episodes_to_solve * 1.1 \n            if trainer.state.episode > tuner_reward_episode and \\ \n                    avg_reward < tuner_reward_min: \n                trainer.should_terminate = True \n                trainer.state.solved = False \n            elif trainer.state.episode > max_episodes: \n                trainer.should_terminate = True \n                trainer.state.solved = False \n            if trainer.should_terminate: \n                print(f\"Episode {trainer.state.episode}, \" \n                      f\"avg_reward {avg_reward:.2f}, terminating\")\n```", "```py\nChapter08$ ./01_dqn_basic.py --dev cuda --params common \nA.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) \n[Powered by Stella] \nEpisode 1: reward=-21, steps=848, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 2: reward=-21, steps=850, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 3: reward=-19, steps=1039, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 4: reward=-21, steps=884, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 5: reward=-19, steps=1146, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 6: reward=-20, steps=997, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 7: reward=-21, steps=972, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 8: reward=-21, steps=882, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 9: reward=-21, steps=898, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 10: reward=-20, steps=947, speed=0.0 f/s, elapsed=0:00:11 \nEpisode 11: reward=-21, steps=762, speed=227.7 f/s, elapsed=0:00:12 \nEpisode 12: reward=-20, steps=991, speed=227.8 f/s, elapsed=0:00:17 \nEpisode 13: reward=-21, steps=762, speed=227.9 f/s, elapsed=0:00:20 \nEpisode 14: reward=-20, steps=948, speed=227.9 f/s, elapsed=0:00:24 \nEpisode 15: reward=-20, steps=992, speed=228.0 f/s, elapsed=0:00:28 \n......\n```", "```py\n learning_rate=9.932831968547505e-05, \n    gamma=0.98,\n```", "```py\n exp_source = ptan.experience.ExperienceSourceFirstLast( \n        env, agent, gamma=params.gamma, env_seed=common.SEED, \n        steps_count=n_steps \n    )\n```", "```py\n loss_v = common.calc_loss_dqn( \n            batch, net, tgt_net.target_model, \n            gamma=params.gamma**n_steps, device=device)\n```", "```py\ndef calc_loss_double_dqn( \n        batch: tt.List[ptan.experience.ExperienceFirstLast], \n        net: nn.Module, tgt_net: nn.Module, gamma: float, device: torch.device): \n    states, actions, rewards, dones, next_states = common.unpack_batch(batch) \n\n    states_v = torch.as_tensor(states).to(device) \n    actions_v = torch.tensor(actions).to(device) \n    rewards_v = torch.tensor(rewards).to(device) \n    done_mask = torch.BoolTensor(dones).to(device)\n```", "```py\n actions_v = actions_v.unsqueeze(-1) \n    state_action_vals = net(states_v).gather(1, actions_v) \n    state_action_vals = state_action_vals.squeeze(-1) \n    with torch.no_grad(): \n        next_states_v = torch.as_tensor(next_states).to(device) \n        next_state_acts = net(next_states_v).max(1)[1] \n        next_state_acts = next_state_acts.unsqueeze(-1) \n        next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1) \n        next_state_vals[done_mask] = 0.0 \n        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v \n    return nn.MSELoss()(state_action_vals, exp_sa_vals)\n```", "```py\n@torch.no_grad() \ndef calc_values_of_states(states: np.ndarray, net: nn.Module, device: torch.device): \n    mean_vals = [] \n    for batch in np.array_split(states, 64): \n        states_v = torch.tensor(batch).to(device) \n        action_values_v = net(states_v) \n        best_action_values_v = action_values_v.max(1)[0] \n        mean_vals.append(best_action_values_v.mean().item()) \n    return np.mean(mean_vals)\n```", "```py\n if engine.state.iteration % EVAL_EVERY_FRAME == 0: \n            eval_states = getattr(engine.state, \"eval_states\", None) \n            if eval_states is None: \n                eval_states = buffer.sample(STATES_TO_EVALUATE) \n                eval_states = [ \n                    np.asarray(transition.state) \n                    for transition in eval_states \n                ] \n                eval_states = np.asarray(eval_states) \n                engine.state.eval_states = eval_states \n            engine.state.metrics[\"values\"] = \\ \n                common.calc_values_of_states(eval_states, net, device)\n```", "```py\nclass NoisyLinear(nn.Linear): \n    def __init__( \n        self, in_features: int, out_features: int, bias: bool = True, \n        device: Optional[DEVICE_TYPING] = None, dtype: Optional[torch.dtype] = None, \n        std_init: float = 0.1, \n    ): \n        nn.Module.__init__(self) \n        self.in_features = int(in_features) \n        self.out_features = int(out_features) \n        self.std_init = std_init \n\n        self.weight_mu = nn.Parameter( \n            torch.empty(out_features, in_features, device=device, \n                        dtype=dtype, requires_grad=True) \n        ) \n        self.weight_sigma = nn.Parameter( \n            torch.empty(out_features, in_features, device=device, \n                        dtype=dtype, requires_grad=True) \n        ) \n        self.register_buffer( \n            \"weight_epsilon\", \n            torch.empty(out_features, in_features, device=device, dtype=dtype), \n        ) \n        if bias: \n            self.bias_mu = nn.Parameter( \n                torch.empty(out_features, device=device, dtype=dtype, requires_grad=True) \n            ) \n            self.bias_sigma = nn.Parameter( \n                torch.empty(out_features, device=device, dtype=dtype, requires_grad=True) \n            ) \n            self.register_buffer( \n                \"bias_epsilon\", torch.empty(out_features, device=device, dtype=dtype), \n            ) \n        else: \n            self.bias_mu = None \n        self.reset_parameters() \n        self.reset_noise()\n```", "```py\n def reset_parameters(self) -> None: \n        mu_range = 1 / math.sqrt(self.in_features) \n        self.weight_mu.data.uniform_(-mu_range, mu_range) \n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features)) \n        if self.bias_mu is not None: \n            self.bias_mu.data.uniform_(-mu_range, mu_range) \n            self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features)) \n\n    def reset_noise(self) -> None: \n        epsilon_in = self._scale_noise(self.in_features) \n        epsilon_out = self._scale_noise(self.out_features) \n        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in)) \n        if self.bias_mu is not None: \n            self.bias_epsilon.copy_(epsilon_out) \n\n    def _scale_noise( \n            self, size: Union[int, torch.Size, Sequence]) -> torch.Tensor: \n        if isinstance(size, int): \n            size = (size,) \n        x = torch.randn(*size, device=self.weight_mu.device) \n        return x.sign().mul_(x.abs().sqrt_())\n```", "```py\n @property \n    def weight(self) -> torch.Tensor: \n        if self.training: \n            return self.weight_mu + self.weight_sigma * self.weight_epsilon \n        else: \n            return self.weight_mu \n\n    @property \n    def bias(self) -> Optional[torch.Tensor]: \n        if self.bias_mu is not None: \n            if self.training: \n                return self.bias_mu + self.bias_sigma * self.bias_epsilon \n            else: \n                return self.bias_mu \n        else: \n            return None\n```", "```py\n learning_rate=7.142520950425814e-05, \n    gamma=0.99,\n```", "```py\nBETA_START = 0.4 \nBETA_FRAMES = 100_000\n```", "```py\nclass PrioReplayBuffer(ExperienceReplayBuffer): \n    def __init__(self, exp_source: ExperienceSource, buf_size: int, \n                 prob_alpha: float = 0.6): \n        super().__init__(exp_source, buf_size) \n        self.experience_source_iter = iter(exp_source) \n        self.capacity = buf_size \n        self.pos = 0 \n        self.buffer = [] \n        self.prob_alpha = prob_alpha \n        self.priorities = np.zeros((buf_size, ), dtype=np.float32) \n        self.beta = BETA_START\n```", "```py\n def update_beta(self, idx: int) -> float: \n        v = BETA_START + idx * (1.0 - BETA_START) / BETA_FRAMES \n        self.beta = min(1.0, v) \n        return self.beta \n\n    def populate(self, count: int): \n        max_prio = self.priorities.max(initial=1.0) \n        for _ in range(count): \n            sample = next(self.experience_source_iter) \n            if len(self.buffer) < self.capacity: \n                self.buffer.append(sample) \n            else: \n                self.buffer[self.pos] = sample \n            self.priorities[self.pos] = max_prio \n            self.pos = (self.pos + 1) % self.capacity\n```", "```py\n def sample(self, batch_size: int) -> tt.Tuple[ \n        tt.List[ExperienceFirstLast], np.ndarray, np.ndarray \n    ]: \n        if len(self.buffer) == self.capacity: \n            prios = self.priorities \n        else: \n            prios = self.priorities[:self.pos] \n        probs = prios ** self.prob_alpha \n        probs /= probs.sum()\n```", "```py\n indices = np.random.choice(len(self.buffer), batch_size, p=probs) \n        samples = [self.buffer[idx] for idx in indices]\n```", "```py\n total = len(self.buffer) \n        weights = (total * probs[indices]) ** (-self.beta) \n        weights /= weights.max() \n        return samples, indices, np.array(weights, dtype=np.float32)\n```", "```py\n def update_priorities(self, batch_indices: np.ndarray, batch_priorities: np.ndarray): \n        for idx, prio in zip(batch_indices, batch_priorities): \n            self.priorities[idx] = prio\n```", "```py\ndef calc_loss(batch: tt.List[ExperienceFirstLast], batch_weights: np.ndarray, \n              net: nn.Module, tgt_net: nn.Module, gamma: float, \n              device: torch.device) -> tt.Tuple[torch.Tensor, np.ndarray]: \n    states, actions, rewards, dones, next_states = common.unpack_batch(batch) \n\n    states_v = torch.as_tensor(states).to(device) \n    actions_v = torch.tensor(actions).to(device) \n    rewards_v = torch.tensor(rewards).to(device) \n    done_mask = torch.BoolTensor(dones).to(device) \n    batch_weights_v = torch.tensor(batch_weights).to(device) \n\n    actions_v = actions_v.unsqueeze(-1) \n    state_action_vals = net(states_v).gather(1, actions_v) \n    state_action_vals = state_action_vals.squeeze(-1) \n    with torch.no_grad(): \n        next_states_v = torch.as_tensor(next_states).to(device) \n        next_s_vals = tgt_net(next_states_v).max(1)[0] \n        next_s_vals[done_mask] = 0.0 \n        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v \n    l = (state_action_vals - exp_sa_vals) ** 2 \n    losses_v = batch_weights_v * l \n    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()\n```", "```py\n def process_batch(engine, batch_data): \n        batch, batch_indices, batch_weights = batch_data \n        optimizer.zero_grad() \n        loss_v, sample_prios = calc_loss( \n            batch, batch_weights, net, tgt_net.target_model, \n            gamma=params.gamma, device=device) \n        loss_v.backward() \n        optimizer.step() \n        buffer.update_priorities(batch_indices, sample_prios) \n        epsilon_tracker.frame(engine.state.iteration) \n        if engine.state.iteration % params.target_net_sync == 0: \n            tgt_net.sync() \n        return { \n            \"loss\": loss_v.item(), \n            \"epsilon\": selector.epsilon, \n            \"beta\": buffer.update_beta(engine.state.iteration), \n        }\n```", "```py\n learning_rate=8.839010139505506e-05, \n    gamma=0.99,\n```", "```py\nclass DuelingDQN(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): \n        super(DuelingDQN, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), \n            nn.ReLU(), \n            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, kernel_size=3, stride=1), \n            nn.ReLU(), \n            nn.Flatten() \n        )\n```", "```py\n size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.fc_adv = nn.Sequential( \n            nn.Linear(size, 256), \n            nn.ReLU(), \n            nn.Linear(256, n_actions) \n        ) \n        self.fc_val = nn.Sequential( \n            nn.Linear(size, 256), \n            nn.ReLU(), \n            nn.Linear(256, 1) \n        )\n```", "```py\n def forward(self, x: torch.ByteTensor): \n        adv, val = self.adv_val(x) \n        return val + (adv - adv.mean(dim=1, keepdim=True)) \n\n    def adv_val(self, x: torch.ByteTensor): \n        xx = x / 255.0 \n        conv_out = self.conv(xx) \n        return self.fc_adv(conv_out), self.fc_val(conv_out)\n```", "```py\ndef distr_projection(next_distr: np.ndarray, rewards: np.ndarray, \n                     dones: np.ndarray, gamma: float): \n    batch_size = len(rewards) \n    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32) \n    delta_z = (Vmax - Vmin) / (N_ATOMS - 1)\n```", "```py\n for atom in range(N_ATOMS): \n        v = rewards + (Vmin + atom * delta_z) * gamma \n        tz_j = np.minimum(Vmax, np.maximum(Vmin, v))\n```", "```py\n b_j = (tz_j - Vmin) / delta_z\n```", "```py\n l = np.floor(b_j).astype(np.int64) \n        u = np.ceil(b_j).astype(np.int64) \n        eq_mask = u == l \n        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n```", "```py\n ne_mask = u != l \n        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask] \n        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n```", "```py\n if dones.any(): \n        proj_distr[dones] = 0.0 \n        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones])) \n        b_j = (tz_j - Vmin) / delta_z \n        l = np.floor(b_j).astype(np.int64) \n        u = np.ceil(b_j).astype(np.int64) \n        eq_mask = u == l \n        eq_dones = dones.copy() \n        eq_dones[dones] = eq_mask \n        if eq_dones.any(): \n            proj_distr[eq_dones, l[eq_mask]] = 1.0 \n        ne_mask = u != l \n        ne_dones = dones.copy() \n        ne_dones[dones] = ne_mask \n        if ne_dones.any(): \n            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask] \n            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask] \n    return proj_distr\n```", "```py\nVmax = 10 \nVmin = -10 \nN_ATOMS = 51 \nDELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1) \n\nclass DistributionalDQN(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): \n        super(DistributionalDQN, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), \n            nn.ReLU(), \n            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, kernel_size=3, stride=1), \n            nn.ReLU(), \n            nn.Flatten() \n        ) \n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.fc = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, n_actions * N_ATOMS) \n        ) \n\n        sups = torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z) \n        self.register_buffer(\"supports\", sups) \n        self.softmax = nn.Softmax(dim=1)\n```", "```py\n def forward(self, x: torch.ByteTensor) -> torch.Tensor: \n        batch_size = x.size()[0] \n        xx = x / 255 \n        fc_out = self.fc(self.conv(xx)) \n        return fc_out.view(batch_size, -1, N_ATOMS) \n\n    def both(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]: \n        cat_out = self(x) \n        probs = self.apply_softmax(cat_out) \n        weights = probs * self.supports \n        res = weights.sum(dim=2) \n        return cat_out, res\n```", "```py\n def qvals(self, x: torch.ByteTensor) -> torch.Tensor: \n        return self.both(x)[1] \n\n    def apply_softmax(self, t: torch.Tensor) -> torch.Tensor: \n        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())\n```", "```py\ndef calc_loss(batch: tt.List[ExperienceFirstLast], net: dqn_extra.DistributionalDQN, \n              tgt_net: dqn_extra.DistributionalDQN, gamma: float, \n              device: torch.device) -> torch.Tensor: \n    states, actions, rewards, dones, next_states = common.unpack_batch(batch) \n    batch_size = len(batch) \n\n    states_v = torch.as_tensor(states).to(device) \n    actions_v = torch.tensor(actions).to(device) \n    next_states_v = torch.as_tensor(next_states).to(device) \n\n    # next state distribution \n    next_distr_v, next_qvals_v = tgt_net.both(next_states_v) \n    next_acts = next_qvals_v.max(1)[1].data.cpu().numpy() \n    next_distr = tgt_net.apply_softmax(next_distr_v) \n    next_distr = next_distr.data.cpu().numpy() \n\n    next_best_distr = next_distr[range(batch_size), next_acts] \n    proj_distr = dqn_extra.distr_projection(next_best_distr, rewards, dones, gamma) \n\n    distr_v = net(states_v) \n    sa_vals = distr_v[range(batch_size), actions_v.data] \n    state_log_sm_v = F.log_softmax(sa_vals, dim=1) \n    proj_distr_v = torch.tensor(proj_distr).to(device) \n\n    loss_v = -state_log_sm_v * proj_distr_v \n    return loss_v.sum(dim=1).mean()\n```"]