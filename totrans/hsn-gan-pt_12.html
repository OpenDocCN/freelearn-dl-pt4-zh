<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Sequence Synthesis with GANs</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will work on GANs that directly generate sequential data, such as text and audio. While doing so, we will go back to the previous image-synthesizing models we've looked at so that you can become familiar with NLP models quickly.</p>
<p class="mce-root">Throughout this chapter, you will get to know the commonly used techniques of the NLP field, such as RNN and LSTM. You will also get to know some of the basic concepts of <strong>reinforcement learning</strong> (<strong>RL</strong>) and how it differs from supervised learning (such as SGD-based CNNs). Later on, we will learn how to build a custom vocabulary from a collection of text so that we can train our own NLP models and learn how to train SeqGAN so that it can generate short English jokes. You will also learn how to use SEGAN to remove background noise and enhance the quality of speech audio.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Text generation via SeqGAN <span>–</span> teaching GANs how to tell jokes</li>
<li>Speech quality enhancement with SEGAN</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text generation via SeqGAN – teaching GANs how to tell jokes</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned how to generate high-quality images based on description <span>text with GANs. Now, we will move on and look at sequential data synthesis, such as text and audio, using various GAN models.</span></p>
<p class="mce-root"/>
<p>When it comes to the generation of text, the biggest difference in terms of image generation is that text data is discrete while image pixel values are more continuous, though digital images and text are both essentially discrete. A pixel typically has 256 values and slight changes in the pixels won't necessarily affect the image's meaning to us. However, a slight change in the sentence <span>–</span> even a single letter (for example, turning <em>we</em> into <em>he</em>) <span>– </span>may change the whole meaning of the sentence. Also, we tend to have a higher tolerance bar for <span>synthesized </span>images compared to text. For example, if 90% of the pixels in the generated image of a dog are nearly perfect, we may have little trouble recognizing the dog because our brains are smart enough to automatically fill in the missing pixels. However, if you are reading a piece of news in which every one out of 10 words doesn't make any sense, you will definitely find it hard to enjoy reading it. This is why text generation is hard and there's less remarkable progress in text generation than image synthesis.</p>
<p>SeqGAN was one of the first successful attempts of text generation with adversarial learning. It was proposed by Lantao Yu, Weinan Zhang, and Jun Wang, et. al. in their paper, <em>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</em>. In this section, we will walk you through the design of SeqGAN, how to create your own vocabulary for NLP tasks, and how to train SeqGAN so that it can generate short jokes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Design of SeqGAN – GAN, LSTM, and RL</h1>
                </header>
            
            <article>
                
<p>Like other GAN models, SeqGAN is built upon the idea of adversarial learning. Some major changes have to be made so that it can accommodate NLP tasks. For example, the generation network is built with LSTM instead of CNNs, similar to some of the other GANs we looked at in the previous chapters. Also, reinforcement learning is used to optimize discrete objectives, unlike the SGD-family methods that were used in previous GAN models.</p>
<p>Here, we will provide a quick introduction to LSTM and RL. However, we won't go too deep into these topics since we want to focus on the adversarial learning part of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A quick introduction to RNN and LSTM</h1>
                </header>
            
            <article>
                
<p><strong>Recurrent neural networks</strong> (<strong>RNNs</strong>) are designed to process sequential data such as text and audio. Their biggest difference to CNNs is that the weights in the hidden layers (that is, certain functions) are used repeatedly on multiple inputs and the order of the inputs affects the final results of the functions. The typical design of an RNN can be seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-810 image-border" src="assets/59435150-acdf-4e3b-99c8-68dd0f502411.png" style="width:32.92em;height:10.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.1 Basic computational units of a recurrent neural network</div>
<p>As we can see, the most distinctive characteristic of an RNN unit is that the hidden state, <img class="fm-editor-equation" src="assets/54184b1f-1edb-4bf2-a92f-130f4bfc14b4.png" style="width:1.08em;height:1.25em;"/>, has an outgoing connection pointing to itself. This self-loop is where the name "recurrent" comes from. Let's say the self-loop is performed three times. The extended version of this computational unit is shown on the right in the preceding diagram. The computational process is expressed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8ffcd1a0-c001-48f7-a31b-da950dd05d5a.png" style="width:11.33em;height:4.08em;"/></p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/bd4676e8-94ff-4550-9319-f85aad178b67.png" style="width:11.25em;height:4.25em;"/></p>
<p>Thus, after proper training, this RNN unit is capable of handling sequential data with a maximum length of 3.</p>
<p>RNNs are widely used in voice recognition, natural language translation, language modeling, and image captioning. However, a critical flaw remains in RNN that we need to address with LSTM.</p>
<p>An RNN model assumes that a strong connection only exists between the neighboring inputs (for example, <img class="fm-editor-equation" src="assets/3b929bf1-1c43-45d5-96f2-0c8b330bdc0c.png" style="width:0.92em;height:0.67em;"/> and <img class="fm-editor-equation" src="assets/2e5776b3-de79-4395-b77d-e941cba14a75.png" style="width:1.17em;height:0.92em;"/>, as shown in the preceding diagram) and that the connections between the inputs that are far apart from each other are ignored (<span>for example,</span> <img class="fm-editor-equation" src="assets/f60506e1-eeb9-4c81-9a89-d56b18043f1c.png" style="width:1.17em;height:0.92em;"/> and <img class="fm-editor-equation" src="assets/8baa2dde-5c82-4cf0-afb4-4b797035ef16.png" style="width:1.08em;height:0.83em;"/>). This becomes troublesome when we try to translate a long sentence into another language that has totally different grammatical rules and we need to look through all the parts of the sentence to make sense of it.</p>
<p class="mce-root"/>
<p><strong>LSTM</strong> (<strong>Long Short-Term Memory</strong>) was proposed by Sepp Hochreiter and Jürgen Schmidhuber in 1997 to preserve the long-term memory of sequential data and address the gradient explosion and vanishing issues in RNNs. Its computational process is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-811 image-border" src="assets/d7025e9b-c99f-41e3-b7ba-93443626748e.png" style="width:28.17em;height:17.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.2 Computational process of LSTM</div>
<p>As we can see, an addition term, <img class="fm-editor-equation" src="assets/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png" style="width:1.08em;height:1.08em;"/>, is included to help us choose what long-term information should be memorized. The detailed computational process is as follows:</p>
<ol>
<li><img class="fm-editor-equation" src="assets/2dfda1e3-d735-4bf0-9784-8bea0de02215.png" style="width:2.00em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/15c720a7-08a8-45e7-8ba8-ef713ce6eca4.png" style="width:1.17em;height:1.00em;"/> are passed through the Forget Gate to decide what information should be forgotten:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c75f9a60-d34c-48f0-add0-b9b3c60d463c.png" style="width:14.42em;height:1.25em;"/></p>
<ol start="2">
<li>The same inputs are also passed through the Input Gate so that we can calculate the updated <img class="fm-editor-equation" src="assets/2c9501bf-f00a-4967-a448-ff5210863135.png" style="width:1.17em;height:1.17em;"/> at the next step:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8ad3118d-ae7c-4b58-bd9d-dab83283b4b3.png" style="width:16.17em;height:2.92em;"/></p>
<ol start="3">
<li>The updated <img class="fm-editor-equation" src="assets/f6adc698-8d81-4745-ba20-f770ee4c1205.png" style="width:1.00em;height:1.00em;"/> and <img class="fm-editor-equation" src="assets/38a54135-0454-41f2-92c6-242b2ad566be.png" style="width:1.08em;height:1.25em;"/> are calculated by the Output Gate:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5991df60-f54b-4c6b-a98f-94a18ed6137f.png" style="width:15.67em;height:4.08em;"/></p>
<p>Then, the new <img class="fm-editor-equation" src="assets/2d0aaa6b-e7ba-4a49-a362-8018613c4751.png" style="width:1.00em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/3f4f4e24-7a3e-401e-93f8-457cb2e03a18.png" style="width:1.08em;height:1.08em;"/> are used to calculate the next pair of <img class="fm-editor-equation" src="assets/d77ae7d9-a5b5-43f0-872e-da13bf39d901.png" style="width:2.17em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/7555bd34-4191-4e83-8ddc-4a3ae8084bcc.png" style="width:2.08em;height:1.17em;"/>. Although the structure of an LSTM cell is much more complicated than the vanilla RNN cell, thanks to the delicate design of the three gates (Forget, Input, and Output), LSTM can be seen in almost every milestone NLP model in the past few years. If you want to find out more about LSTM and its variants, check out <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a> and <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning versus supervised learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is another optimization method family in machine learning. It is often used when it's hard to provide standard correct answers for the tasks that the model is trying to solve, especially when the solution involves <em>free exploration</em> and the end goal of the task is somewhat <em>vague</em> compared to the specific decisions the model needs to make.</p>
<p>For example, if we want to teach a robot to walk, we can use reinforcement learning to let the robot teach itself to walk. We don't need to tell the robot how to move which body part at what time. We only tell it that its final goal is to <em>take yourself to that location 10 meters in front of you</em> and let it randomly move its limbs. At some point, a certain combination of movements for the robot's legs will bring the robot a step forward and a certain combination of movements for the robot's arms makes sure it won't fall out of balance. Similarly, reinforcement learning is also used to teach machines to play Go (<a href="https://www.alphago-games.com">https://www.alphago-games.com</a>) and video games (<a href="https://openai.com/blog/openai-five">https://openai.com/blog/openai-five</a>).</p>
<p>SGD-based optimization methods are often used in supervised learning (they were used in the models in the previous chapters where real data is always used to measure the quality of synthesized data), whereas, in unsupervised learning, the optimization strategies are totally different.</p>
<p>Currently, Policy Gradients and Q-Learning are two of the most commonly used methods in RL. Let's explain them in brief:</p>
<ol>
<li><strong>Policy Gradient</strong> is a policy-based method. The model directly gives actions (output) based on the current states (input). It alternates between evaluating the policy (takes actions based on states) and updating the policy (updates the mappings between states and actions). It is often used in large and continuous action spaces.</li>
</ol>
<ol start="2">
<li><strong>Q-Learning</strong> is a value-based method. It maintains a Q-table that keeps track of the rewards of various actions. It chooses the action that leads to the maximum reward value and then updates the Q-table, based on the new environment as a result of the action. It can be trained faster than the Policy Gradient method and is often used for simple tasks with small action spaces.</li>
</ol>
<div class="packt_tip">So, how can we choose between reinforcement learning and supervised learning (such as SGD methods in CNNs) when both of them are available? A simple rule of thumb is the <strong>continuity</strong> of the search space and the <strong>differentiability</strong> of the objective function. If the objective function is differentiable and the search space is continuous, it's better to use SGD methods. If the search space is discrete or the objective function is nondifferentiable, we need to stick to reinforcement learning. However, if the search space isn't very large and you have extra computing power to spare, <strong>Evolutionary Search</strong> (<strong>ES</strong>) methods are also a good option. When your variables are assumed to obey Gaussian distribution, you can always give the CMA-ES (<a href="http://cma.gforge.inria.fr">http://cma.gforge.inria.fr</a>) method a try.</div>
<div class="packt_infobox">Here are two extra reading materials if you want to learn more about Policy Gradients:<br/>
<ul>
<li><a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture of SeqGAN</h1>
                </header>
            
            <article>
                
<p>The idea behind SeqGAN is to get it to solve problems that vanilla GANs can't, since they are good at synthesizing discrete data, and discriminator networks can't, since they can't evaluate sequential data with various lengths. To solve the first problem, Policy Gradients are used for updating the generator network. The second problem is addressed by generating the remaining data with the <strong>Monte Carlo Tree Search</strong> (<strong>MCTS</strong>) method.</p>
<p>The reinforcement learning strategy in SeqGAN is designed as follows. Let's assume that at time <img class="fm-editor-equation" src="assets/968d471c-2b1b-42a8-9c28-ad6358aa650f.png" style="width:0.42em;height:0.92em;"/>, the generated sequence is denoted as <img class="fm-editor-equation" src="assets/6cc4ad05-2814-4019-827b-9c763601285a.png" style="width:13.08em;height:1.67em;"/> and that the current action, <img class="fm-editor-equation" src="assets/fecafdf9-7338-4c4b-9d12-8cb53b5d2d35.png" style="width:1.42em;height:1.42em;"/>, needs to be given by the generator network, <img class="fm-editor-equation" src="assets/d3cc5c60-c47a-47a8-aacd-68b41d882508.png" style="width:6.67em;height:1.50em;"/>, in which <img class="fm-editor-equation" src="assets/555b91db-e108-404f-a68b-5e0b61d13df5.png" style="width:1.17em;height:1.00em;"/> is the initial state. The generation of <img class="fm-editor-equation" src="assets/43f5aadd-a184-4c99-a92f-3951ac78e84a.png" style="width:1.42em;height:1.33em;"/> based on <img class="fm-editor-equation" src="assets/fef5a57f-bb22-48a5-9a1b-92290afb70e9.png" style="width:3.33em;height:1.50em;"/> is done by LSTM (or any of its variants). The objective of the generator is to maximize the cumulative rewards:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0b2631cf-8640-4697-9e51-9bc966795221.png" style="width:26.92em;height:3.58em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/e6c5ed41-a6a3-4921-95c1-a83888146822.png" style="width:1.25em;height:0.92em;"/> is the cumulative rewards, <img class="fm-editor-equation" src="assets/bc008ce8-371b-4d4b-9eca-189aad473605.png" style="width:0.58em;height:1.08em;"/> is the parameters to be optimized (that is, parameters in <img class="fm-editor-equation" src="assets/66c2b854-dc58-45e0-970c-e555c8362e9a.png" style="width:0.67em;height:0.83em;"/>), and <img class="fm-editor-equation" src="assets/db34895b-1687-40ee-ad8c-b8090906a2c1.png" style="width:0.75em;height:0.92em;"/> is called the <strong>action-value function</strong>. The action-value function, <img class="fm-editor-equation" src="assets/26713f87-4914-4848-a523-a54070d61713.png" style="width:4.25em;height:1.58em;"/>, gives us the reward of taking the action, <img class="fm-editor-equation" src="assets/1e773778-6365-4fe5-abe8-6146c0cd9943.png" style="width:0.58em;height:1.00em;"/>, by following policy, <img class="fm-editor-equation" src="assets/c826ad81-ef12-430c-aedc-59d6b1608688.png" style="width:1.25em;height:1.08em;"/>, starting from the initial state, <img class="fm-editor-equation" src="assets/e5f6cb1c-7e78-41e1-bc98-c7aaeabcceff.png" style="width:1.17em;height:1.00em;"/>.</p>
<p>Normally, we would expect to use the discriminator network to give us reward values. However, the discriminator cannot be used directly to calculate the cumulative rewards because it can only evaluate a full-length sequence, <img class="fm-editor-equation" src="assets/be89d748-38d2-4897-b0ec-506ae4cd6f31.png" style="width:2.67em;height:1.50em;"/>. At time <img class="fm-editor-equation" src="assets/9e8f42b8-3dfc-4277-b2a6-c46e8e71f07b.png" style="width:0.42em;height:0.92em;"/>, all we have is <img class="fm-editor-equation" src="assets/b50d186b-900f-4e29-bdad-8ecb9903eddf.png" style="width:2.58em;height:1.75em;"/>. How do we get the rest of the sequence?</p>
<p>In SeqGAN, the remaining sequence, <img class="fm-editor-equation" src="assets/2aec922b-2d4a-45d9-a159-7fa86af0acb2.png" style="width:4.08em;height:1.67em;"/>, is generated by the MCTS method. MCTS is a tree-based search method and widely used in chess- and poker-playing programs and video game AI algorithms. All the actions that can be made are represented by nodes in a very large tree. It takes four steps to do a complete search in the Monte Carlo tree, as follows:</p>
<ol>
<li><strong>Selection</strong>, which is where you select a path from the root node to a leaf node. Normally, the selection of the existing nodes is based on <strong>Upper Confidence Bounds</strong> (<strong>UCB</strong>). Nodes with high scores are more likely to be chosen and nodes that haven't been chosen that many times before are more likely to be selected. It is a balance between <strong>exploration and exploitation</strong>.</li>
<li><strong>Expansion</strong>, which is where you add new child nodes to the selected leaf node.</li>
<li><strong>Simulation</strong>, which is where you evaluate the newly added nodes and get the final results (rewards).</li>
<li><strong>Backpropagation</strong>, which is where you update the scores and counts statistics of all the nodes on the selected path.</li>
</ol>
<p>In fact, only the third step, simulation, is used to generate the remaining sequence, where it performs the simulation (generating the remaining sequence with <img class="fm-editor-equation" src="assets/d8b164aa-836e-4c56-898c-e5ba4f5d43fe.png" style="width:0.67em;height:0.75em;"/>) multiple times generate and get the averaged reward.</p>
<p class="mce-root"/>
<p>Therefore, the definition of <img class="fm-editor-equation" src="assets/68e90681-3d82-4476-84d9-ec342c6a7d4f.png" style="width:2.92em;height:1.08em;"/> is as follows:</p>
<p>                         <img class="alignnone size-full wp-image-812 image-border" src="assets/5716dd43-c145-4ce2-9891-57ac1383612d.png" style="width:31.50em;height:4.33em;"/></p>
<p>The generator network is an LSTM network with an embedding layer as an input layer and a linear layer as an output layer. The discriminator network consists of an embedding layer, a convolution layer, a max-pooling layer, and a softmax layer. The code that was published by the authors of this paper was written for TensorFlow. Luckily, a PyTorch version can be found on GitHub at <a href="https://github.com/suragnair/seqGAN">https://github.com/suragnair/seqGAN</a>. In this version, two differences should be noted: first, the Monte Carlo simulation is only performed once, and second, the discriminator network is also a recurrent network and a variant of LSTM called <strong>Gated Recurrent Unit</strong> (<strong>GRU</strong>) is used in both networks. Feel free to adjust the network architectures and try out the tricks and techniques we have learned in the previous chapters of this book. Our modified code is also available under the <kbd>seqgan</kbd> folder in the code repository for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your own vocabulary for training</h1>
                </header>
            
            <article>
                
<p>Reading code that's been written by someone else in GitHub is easy. The most important thing we need to do is apply the models we know to new applications and create our own samples. Here, we will walk through the basic steps of creating a vocabulary from a huge collection of text and use it to train our NLP models.</p>
<p>In the NLP model, a vocabulary set is normally a table that maps each word or symbol to a unique token (typically, an <kbd>int</kbd> value) so that any sentence can be represented by a vector of <kbd>int</kbd>.</p>
<p>First, let's find some data to play with. To get started, here's a list of NLP datasets available on GitHub: <a href="https://github.com/niderhoff/nlp-datasets">https://github.com/niderhoff/nlp-datasets</a>. From this list, you will find an English joke dataset (<a href="https://github.com/taivop/joke-dataset">https://github.com/taivop/joke-dataset</a>) that contains more than 200,000 jokes parsed from Reddit (<a href="https://www.reddit.com/r/jokes">https://www.reddit.com/r/jokes</a>), Stupid Stuff (<a href="http://stupidstuff.org/">stupidstuff.org</a>), and Wocka (<a href="http://wocka.com/">wocka.com</a>).  The joke text will be in three different files (<kbd>reddit_jokes.json</kbd>, <kbd>stupidstuff.json</kbd>, and <kbd>wocka.json</kbd>). Please don't hold us responsible for the content of these jokes!</p>
<p>Now, let's create our vocabulary. First, create a folder named <kbd>data</kbd> in the project code folder and copy the aforementioned files into it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, let's create a small program so that we can parse the JSON files and put them in CSV format. Let's call it <kbd>parse_jokes.py</kbd>:</p>
<pre>import sys<br/>import platform<br/>import os<br/>import json<br/>import csv<br/>import re<br/><br/>datapath = './data'<br/>redditfile = 'reddit_jokes.json'<br/>stupidfile = 'stupidstuff.json'<br/>wockafile = 'wocka.json'<br/>outfile = 'jokes.csv'<br/>headers = ['row', 'Joke', 'Title', 'Body', 'ID',<br/>           'Score', 'Category', 'Other', 'Source']</pre>
<p>I'm sure that the import section entries are obvious. The definitions of the constants should be fairly obvious as well. The headers variable is simply a list of the column names that will be used when we create the CSV file.</p>
<p class="mce-root">We want all of the jokes that will be stored in our files to be in plain text. To do this, get rid of all the non-letter symbols. This is done by cleaning the text using <kbd>clean_str()</kbd>, which uses Python's <kbd>str_translate</kbd> parameter, as shown here:</p>
<pre>def clean_str(text):<br/>    fileters = '"#$%&amp;()*+-/;&lt;=&gt;@[\\]^_`{|}~\t\n\r\"'<br/>    trans_map = str.maketrans(fileters, " " * len(fileters))<br/>    text = text.translate(trans_map)<br/>    re.sub(r'[^a-zA-Z,. ]+', '', text)<br/>    return text</pre>
<p>Feel free to tweak the <kbd>filters</kbd> string so that you can add or remove any special characters. The next function will read one of our three JSON files and return it as a JSON object. I've made it rather generic, so that the only thing it needs to know about is the filename to deal with:</p>
<pre>def get_data(fn):<br/>    with open(fn, 'r') as f:<br/>        extracted = json.load(f)<br/>    return extracted</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Next, we'll create three functions that will handle converting the three JSON objects into CSV files. It is important to remember that none of the three JSON files have the same structure. Due to this, we'll make all three handler functions fairly similar and handle the differences between them at the same time. Each of the functions will take the JSON object that was created by the <kbd>get_data</kbd> function, as well as an integer value called <kbd>startcount</kbd>. This will provide a row number for the CSV file.  This value will be incremented for each line in the JSON object. Then, we will create a dictionary out of each piece of data and write it to the CSV file.  Finally, we will return our counter so that the next function knows what the row value should be. This is the function that will handle the Reddit file:</p>
<pre>def handle_reddit(rawdata, startcount):<br/>    global writer<br/>    print(f'Reddit file has {len(rawdata)} items...')<br/>    cntr = startcount<br/>    with open(outfile, mode='a') as csv_file:<br/>        writer = csv.DictWriter(csv_file, fieldnames=headers)<br/>        for d in rawdata:<br/>            title = clean_str(d['title'])<br/>            body = clean_str(d['body'])<br/>            id = d['id']<br/>            score = d['score']<br/>            category = ''<br/>            other = ''<br/>            dict = {}<br/>            dict['row'] = cntr<br/>            dict['Joke'] = title + ' ' + body<br/>            dict['Title'] = title<br/>            dict['Body'] = body<br/>            dict['ID'] = id<br/>            dict['Category'] = category<br/>            dict['Score'] = score<br/>            dict['Other'] = other<br/>            dict['Source'] = 'Reddit'<br/>            writer.writerow(dict)<br/>            cntr += 1<br/>            if cntr % 10000 == 0:<br/>                print(cntr)<br/>    return cntr</pre>
<p>Next, we have the other two functions: one for the <kbd>StupidStuff</kbd> file and the other for the <kbd>Wocka</kbd> file:</p>
<pre>def handle_stupidstuff(rawdata, startcount):<br/>    global writer<br/>    print(f'StupidStuff file has {len(rawdata)} items...')<br/>    with open(outfile, mode='a') as csv_file:<br/>        writer = csv.DictWriter(csv_file, fieldnames=headers)<br/>        cntr = startcount<br/>        for d in rawdata:<br/>            body = clean_str(d['body'])<br/>            id = d['id']<br/>            score = d['rating']<br/>            category = d['category']<br/>            other = ''<br/>            dict = {}<br/>            dict['row'] = cntr<br/>            dict['Joke'] = body<br/>            dict['Title'] = ''<br/>            dict['Body'] = body<br/>            dict['ID'] = id<br/>            dict['Category'] = category<br/>            dict['Score'] = score<br/>            dict['Other'] = other<br/>            dict['Source'] = 'StupidStuff'<br/>            writer.writerow(dict)<br/>            cntr += 1<br/>            if cntr % 1000 == 0:<br/>                print(cntr)<br/>    return cntr<br/><br/><br/>def handle_wocka(rawdata, startcount):<br/>    global writer<br/>    print(f'Wocka file has {len(rawdata)} items...')<br/>    with open(outfile, mode='a') as csv_file:<br/>        writer = csv.DictWriter(csv_file, fieldnames=headers)<br/>        cntr = startcount<br/>        for d in rawdata:<br/>            other = clean_str(d['title'])<br/>            title = ''<br/>            body = clean_str(d['body'])<br/>            id = d['id']<br/>            category = d['category']<br/>            score = ''<br/>            other = ''<br/>            dict = {}<br/>            dict['row'] = cntr<br/>            dict['Joke'] = body<br/>            dict['Title'] = title<br/>            dict['Body'] = body<br/>            dict['ID'] = id<br/>            dict['Category'] = category<br/>            dict['Score'] = score<br/>            dict['Other'] = other<br/>            dict['Source'] = 'Wocka'<br/>            writer.writerow(dict)<br/>            cntr += 1<br/>            if cntr % 1000 == 0:<br/>                print(cntr)<br/>    return cntr</pre>
<p>The second to last function will create the actual CSV file and write the header:</p>
<pre>def prep_CVS():<br/>    global writer<br/>    with open(outfile, mode='a') as csv_file:<br/>        writer = csv.DictWriter(csv_file, fieldnames=headers)<br/>        writer.writeheader()</pre>
<p>Finally, we have the main function and the entry point for the program. Here, we will call the preceding functions in any order we like:</p>
<pre>def main():<br/>    pv = platform.python_version()<br/>    print(f"Running under Python {pv}")<br/>    path1 = os.getcwd()<br/>    print(path1)<br/>    prep_CVS()<br/>    print('Dealing with Reddit file')<br/>    extracted = get_data(datapath + "/" + redditfile)<br/>    count = handle_reddit(extracted, 0)<br/>    print('Dealing with StupidStuff file')<br/>    extracted = get_data(datapath + "/" + stupidfile)<br/>    count = handle_stupidstuff(extracted, count)<br/>    print('Dealing with Wocka file')<br/>    extracted = get_data(datapath + "/" + wockafile)<br/>    count = handle_wocka(extracted, count)<br/>    print(f'Finished processing! Total items processed: {count}')<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Now, all we have to do is run the script:</p>
<pre><strong>$ python parse_jokes.py</strong></pre>
<p>When we're finished, the joke text will be stored in the <kbd>jokes.csv</kbd> file. Now, we need to use TorchText to build the vocabulary. TorchText (<a href="https://github.com/pytorch/text">https://github.com/pytorch/text</a>) is a data loading tool for NLP that works directly with PyTorch.</p>
<div class="packt_infobox"><strong>Note for Windows 10 users</strong>:<br/>
At the time of writing this book, there appears to be an issue in <kbd>torchtext\utils.py</kbd>. If you install the <kbd>torchtext</kbd> package directly from PyPi, you could run into an error while trying to execute some of the code.<br/>
<br/>
The best way around this is to head over to the GitHub source repository (<a href="https://github.com/pytorch/text">https://github.com/pytorch/text</a>) and download the source code. Then, unpack the code into a safe folder. In Command Prompt, navigate to the folder that contains the source code and enter the following command to install the library:<br/>
<br/>
<kbd>pip install -e .</kbd><br/>
<br/>
This will install torchtext directly from the source code.</div>
<p>For other OS, you can install it with the following command:</p>
<pre><strong>$ pip install torchtext</strong></pre>
<p>Please make sure that you have installed the latest version of <kbd>torchtext</kbd> (0.4.0, at the time of writing this book); otherwise, the code we will use later may not work for you. If <kbd>pip</kbd> doesn't install the latest version for you, you can find the <kbd>whl</kbd> file at <a href="https://pypi.org/project/torchtext/#files">https://pypi.org/project/torchtext/#files</a> and install it manually.</p>
<p>We will use the default vocab tool provided by <kbd>torchtext</kbd> for this. You can also try using <kbd>spaCy</kbd> (<a href="https://spacy.io">https://spacy.io</a>) if you want to build vocab for more complex NLP tasks. Create a new file and call it <kbd>mymain.py</kbd>. Start by adding the following code to it:</p>
<pre>import torchtext as tt<br/>import numpy as np<br/>import torch<br/>from datetime import datetime<br/><br/>VOCAB_SIZE = 5000<br/>MAX_SEQ_LEN = 30<br/>BATCH_SIZE = 32<br/><br/>src = tt.data.Field(tokenize=tt.data.utils.get_tokenizer("basic_english"),<br/>                    fix_length=MAX_SEQ_LEN,<br/>                    lower=True)<br/><br/>datafields = [('row', None),<br/>              ('Joke', src),<br/>              ('Title', None),<br/>              ('Body', None),<br/>              ('ID', None),<br/>              ('Score', None),<br/>              ('Category', None),<br/>              ('Other', None),<br/>              ('Source', None)]</pre>
<p>The <kbd>datafields</kbd> structure describes the CSV file we just created. Each column in the file is described and the only column we want the <kbd>torchtext</kbd> library to be concerned with is the <kbd>'Joke'</kbd> column, so we mark that as <kbd>'src'</kbd> and all the other columns as <kbd>'None'</kbd>.</p>
<p>Now, we will create the dataset object and start to build a vocabulary object:</p>
<pre>dataset = tt.data.TabularDataset(path='jokes.csv', format='csv',<br/>                                 fields=[('id', None), <br/>                                         ('text', src)])<br/><br/>src.build_vocab(dataset, max_size=VOCAB_SIZE)</pre>
<p>We'll use the <kbd>torchtext</kbd> library's <kbd>BucketIterator</kbd> to go through the data in the dataset and create sequences of equal length:</p>
<pre>src_itr = tt.data.BucketIterator(dataset=dataset,<br/>                                 batch_size=BATCH_SIZE,<br/>                                 sort_key=lambda x: len(x.text),<br/>                                 device=torch.device("cuda:0"))</pre>
<p>Now that we've built our vocabulary, we need to build a small data loader that will feed the batch data into SeqGAN during training:</p>
<pre>class BatchLoader:<br/>    def __init__(self, dl, x_field):<br/>        self.dl, self.x_field = dl, x_field<br/><br/>    def __len__(self):<br/>        return len(self.dl)<br/><br/>    def __iter__(self):<br/>        for batch in self.dl:<br/>            x = getattr(batch, self.x_field)<br/>            yield x.t()<br/><br/>train_loader = BatchLoader(src_itr, 'text')</pre>
<p class="mce-root"/>
<p>We also need a mapping from tokens back to words so that we can see the generated text when the training process is complete:</p>
<pre>vocab_max = 0<br/>for i, batch in enumerate(train_loader):<br/>    _max = torch.max(batch)<br/>    if _max &gt; vocab_max:<br/>        vocab_max = _max<br/><br/>VOCAB_SIZE = vocab_max.item() + 1<br/><br/>inv_vocab = {v: k for k, v in src.vocab.stoi.items()}</pre>
<p>Here, our vocabulary is stored in <kbd>src.vocab</kbd>. <kbd>src.vocab.stoi</kbd> is a Python <kbd>defaultdict</kbd> that maps words to <kbd>int</kbd> values. The last line in the preceding code snippet inverses the dictionary and stores the mappings from the <kbd>int</kbd> values as words in <kbd>inv_vocab</kbd>.</p>
<p>You can test the vocabulary with the following code:</p>
<pre>sentence = ['a', 'man', 'walks', 'into', 'a', 'bar']<br/>for w in sentence:<br/>    v = src.vocab[w]<br/>    print(v)<br/>    print(inv_vocab[v])</pre>
<p class="mce-root">If you're curious, you can view the contents of <kbd>inv_vocab</kbd> by adding the following code after the preceding code:</p>
<pre>for i in inv_vocab:<br/>    print(f'Counter: {i} inv_vocab: {inv_vocab[i]}')</pre>
<p class="mce-root">However, remember that around 5,000 lines will be printed, so it will be a long list:</p>
<pre><strong>$ python mymain.py</strong></pre>
<p>Now, we need to work on the rest of the SeqGAN program. This includes the generator and the discriminator. As we mentioned in the <em>Architecture of SeqGAN</em> section, these modules can be found at <a href="https://github.com/suragnair/seqGAN">https://github.com/suragnair/seqGAN</a>. Download the source code and unpack it into a folder in your working directory.</p>
<p>To train SeqGAN, run the following script under the code folder:</p>
<pre><strong>$ python main.py</strong></pre>
<p class="mce-root"/>
<p>The generator network is pretrained with <strong>Maximum Likelihood Estimation</strong> (<strong>MLE</strong>) against the real data for 100 epochs so that it will be trained faster later. Then, the discriminator network is pretrained against real data and some generated data for 150 epochs, in which the generated data is kept the same for every three epochs so that the discriminator becomes familiar with fake data. Finally, both networks are trained together in an adversarial fashion for 50 epochs, in which the discriminator network is trained 15 times more than the generator network. On a single GTX 1080Ti graphics card, the pretraining process takes about <strong>33 hours</strong>, and 17 epochs of the final training can take long as <strong>48 hours</strong> to complete. GPU memory consumption is about 4,143 MB.</p>
<p>The following are some of the jokes that were generated by SeqGAN. Unfortunately, most of the sentences don't make sense due to mode collapse (which means that the same random word will appear anywhere in the sentences in one batch).</p>
<p>Still, let's take a look. Note that sentences shorter than <kbd>MAX_SEQ_LEN</kbd> are filled with <kbd>&lt;pad&gt;</kbd> at the end and have been omitted here:</p>
<ul>
<li>"have you ever make like a tomato of jokes ? . there d call out of vegetables !"</li>
<li><span>"the patriots weren't invited camping ! . because i can rather have been born in tents ."</span></li>
<li>"trainees. it is a train for christmas pockets"</li>
<li><span>"what do you get when you cross a kangaroo and a rhino ? . spanish"</span></li>
</ul>
<p>The following sentences were generated by the model:</p>
<ul>
<li>"i can't stop a joke . . . . it's all ."</li>
<li>"i can't see a new joke ."</li>
</ul>
<div class="packt_infobox"><br/>
Our model also created some jokes that were too inappropriate to print, which is an interesting demonstration of its attempt to emulate human humor!</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech quality enhancement with SEGAN</h1>
                </header>
            
            <article>
                
<p>In <a href="c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml">Chapter 7</a>, <em>Image Restoration with GANs</em>, we explored how GANs can restore some of the pixels in images. Researchers have found a similar application in NLP where GANs can be trained to get rid of the noises in audio in order to enhance the quality of the recorded speeches. In this section, we will learn how to use SEGAN to reduce background noise in the audio and make the human voice in the noisy audio more audible.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SEGAN architecture</h1>
                </header>
            
            <article>
                
<p><span><strong>Speech Enhancement GAN</strong> (<strong>SEGAN</strong>) was proposed by </span><span>Santiago Pascual, Antonio Bonafonte, and Joan Serrà in their paper, </span><em>SEGAN: Speech Enhancement Generative Adversarial Network</em><span>. It uses 1D convolutions to successfully remove noise from speech audio. You can check out the noise removal results compared to other methods here at <a href="http://veu.talp.cat/segan">http://veu.talp.cat/segan</a>. There's also an upgraded version, which can be found at <a href="http://veu.talp.cat/seganp">http://veu.talp.cat/seganp</a>.</span></p>
<p>Images are two-dimensional, while sounds are one-dimensional. Considering GANs are so good at synthesizing 2D images, it is rather obvious to consider using 1D convolution layers instead of 2D convolutions in order to harness the power of GANs when it comes to synthesizing audio data. This is exactly how SEGAN is built.</p>
<p>The generator network in SEGAN employs an <span>architecture of</span> <strong>Encoder-Decoder</strong> with skip connections, which you may be familiar with since we have already met other GANs that use a similar architecture (such as <kbd>pix2pixHD</kbd>). The architecture of the generator network is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="assets/193453c0-4b63-4017-9e2a-53f9ef467039.png" style="width:51.67em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.3 Architecture of the generator network in SEGAN</div>
<p>First, the audio samples are cropped to a fixed length of 16,384 and are passed through five of the layers of the 1D convolution with a kernel size of 31 and a stride size of 4. The compressed <span class="packt_screen">1,024 x 16</span> vector (ignoring the batch channel) is concatenated with the latent vector (that's 1,024 x 16 in size) so that it can be fed through another five transposed convolution layers. The feature maps with the same shape in the mirrored convolution and transposed convolution layers are connected with skip connections. This is because the basic structures of noisy and clean audio are pretty much the same and skip connections help the generator reconstruct the structure of enhanced audio a lot faster. Finally, a denoised audio sample with a length of 16,384 is generated.</p>
<p>However, the discriminator network of SEGAN is a single encoder network since all we need from the discriminator is the fidelity score of the input audio. The architecture of the discriminator network is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="assets/6800dd57-6543-465b-ad22-4184f8b5544f.png" style="width:43.33em;height:11.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.4 Architecture of the discriminator network in SEGAN</div>
<p><span>The noisy audio and the clean (real data or synthesized data) audio are concatenated together to form a <span class="packt_screen">2 x 16,384</span> tensor, which is passed through five convolution layers and three fully-connected layers to get the final output, which indicates whether the clean audio is real or synthesized. In both networks, <strong>Parametric ReLU</strong> (<strong>PReLU</strong>) is used as an activation function in hidden layers.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training SEGAN to enhance speech quality</h1>
                </header>
            
            <article>
                
<p>Training SEGAN isn't much different from training a normal image-synthesizing GAN. The training process of SEGAN is as follows:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-815 image-border" src="assets/b81b6510-8f7b-4cae-8bb6-8a469e0d2c59.png" style="width:31.00em;height:22.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10.5 The training process of SEGAN. Networks that are updated in each stage are marked with red boundaries. Here, <kbd>c*</kbd> denotes real clean audio, <kbd>n</kbd> denotes noisy audio, and <kbd>c</kbd> denotes synthesized clean audio.</div>
<p><span>First, the clean audio and the noisy audio from the training data are fed into the discriminator network to calculate MSE loss. The synthesized audio that's generated by the generator, as well as the noisy audio, are also fed into the discriminator network. In this stage, the discriminator network is trained to be better at knowing the difference between real and synthesized clean audio. Then, the generated audio is used to fool the discriminator (by minimizing the MSE loss against 1) so that our generator network will get better at synthesizing realistic clean audio. Also, the L1 loss between the synthesized audio (</span><kbd>c*</kbd><span>) and real audio is calculated (with a scale factor of 100) to force the two to have similar basic structures. </span><span>RMSprop is used as an optimization method in which the learning rate is set to a very small value (for example, </span><img class="fm-editor-equation" src="assets/6e06c8fc-847e-4e9f-ad07-156b2dd74463.png" style="width:3.58em;height:1.00em;"/><span>).</span></p>
<p>Now, let's get some audio data and see what SEGAN can do. A paired clean-noisy audio dataset is available here: <a href="https://datashare.is.ed.ac.uk/handle/10283/1942">https://datashare.is.ed.ac.uk/handle/10283/1942</a>. We need to download both the clean and noisy 48 kHz speech training sets. The <kbd>clean</kbd> dataset is about 822 MB in size while the <kbd>noisy</kbd> dataset is about 913 MB in size. There are 11,572 pieces of speech inside both sets, most of which are single lines of English spoken by humans. The <kbd>noisy</kbd> audio is contaminated by several people speaking simultaneously.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The source code of SEGAN for PyTorch has been kindly provided by the authors of the paper: <a href="https://github.com/santi-pdp/segan_pytorch">https://github.com/santi-pdp/segan_pytorch</a>. Follow these steps to prepare your code and start training SEGAN:</p>
<ol>
<li>Run the following script to get the code and install the prerequisites:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/santi-pdp/segan_pytorch.git</strong><br/><strong>$ pip install soundfile scipy librosa h5py numba matplotlib pyfftw tensorboardX</strong></pre>
<ol start="2">
<li>An additional tool called <kbd>ahoproc_tools</kbd> (<a href="https://github.com/santi-pdp/ahoproc_tools">https://github.com/santi-pdp/ahoproc_tools</a>) is also required. We need to download the source code of <kbd>ahoproc_tools</kbd> and copy the <kbd>ahoproc_tools</kbd> inside it into the root folder of <kbd>segan_pytorch</kbd>. Alternatively, you can access the full source code inside the code repository for this chapter directly. You need to run the following script to make sure that all the submodules have been downloaded:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git submodule update --init --recursive</strong></pre>
<ol start="3">
<li>Extract the <kbd>.wav</kbd> files from the downloaded <kbd>.zip</kbd> dataset files and move them into the <kbd>data/clean_trainset_wav</kbd> and <kbd>data/noisy_trainset_wav</kbd> folders, respectively.</li>
<li>Finally, run the following script to start the training process:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python train.py --save_path ckpt_segan+ --batch_size 300 --clean_trainset data/clean_trainset_wav --noisy_trainset data/noisy_trainset_wav --cache_dir data/cache</strong></pre>
<p>First, the training script will create a cache folder (<kbd>data/cache</kbd>) where it will temporarily store the slicing results of the audio files (because we want the inputs of both networks to be 16,384 in length).</p>
<p>With a batch size of 300, it takes about 10.7 hours to finish 100 epochs of training on a single GTX 1080Ti graphics card and costs about 10,137 MB of GPU memory.</p>
<p>Once the training process has finished, run the following script to test the trained model and remove the background noises from any audio file that's put inside the <kbd>data/noisy_testset</kbd> folder:</p>
<pre><strong>$ python clean.py --g_pretrained_ckpt ckpt_segan+/weights_EOE_G-Generator-16301.ckpt --cfg_file ckpt_segan+/train.opts --synthesis_path enhanced_results --test_files data/noisy_testset --soundfile</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to generate plain text with SeqGAN and remove background noises in speech audio with SEGAN. We also experimented with how to build a custom vocabulary from a collection of sentences for NLP tasks.</p>
<p>In the next chapter, we will learn how to train GANs so that we can directly generate 3D models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li>Yu L, Zhang W, Wang J. (2017). <em>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</em>. AAAI.</li>
<li>Hochreiter S and Schmidhuber J. (1997). <em>Long Short-Term Memory. Neural computation</em>. 9. 1735-80. 10.1162/neco.1997.9.8.1735.</li>
<li>Olah C. (Aug 27, 2015). <em>Understanding LSTM Networks</em>. Retrieved from <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</li>
<li>Nguyen M. (Sep 25, 2018). <em>Illustrated Guide to LSTMs and GRUs: A step by step explanation</em>. Retrieved from <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a>.</li>
<li>Hui J. (Sep 12, 2018). <em>RL <span>–</span> Policy Gradient Explained</em>. Retrieved from <a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146</a>.</li>
<li>Weng L. (Apr 8, 2018). <em>Policy Gradient Algorithms</em>. Retrieved from <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a>.</li>
<li>Pascual S, Bonafonte A and Serrà J. (2017). <em>SEGAN: Speech Enhancement Generative Adversarial Network</em>. INTERSPEECH.</li>
</ol>


            </article>

            
        </section>
    </body></html>