<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Machine Learning and Deep Learning Models</h1>
                
            
            <article>
                
<p class="calibre2">In almost all of the applications that we have been discussing up to now, the implicit assumption has been that you are creating a new machine learning NLP pipeline. Now, that may not always be the case. If you are already working on an established platform, fastText may also be a good addition to make the pipeline better.</p>
<p class="calibre2">This chapter will give you some of the methods and recipes for implementing fastText using popular frameworks such as scikit-learn, Keras, TensorFlow, and PyTorch. We will look at how we can augment the power of word embeddings in fastText, using other deep neural architectures such as <strong class="calibre4">convolutional neural networks</strong> (<strong class="calibre4">CNN</strong>) or attention networks to solve various NLP problems.</p>
<p class="calibre2">The topics covered in this chapter are as follows:</p>
<ul class="calibre10">
<li class="calibre11">Scikit-learn and fastText</li>
<li class="calibre11">Embeddings</li>
<li class="calibre11">Keras</li>
<li class="calibre11">Embeddings layer in Keras</li>
<li class="calibre11">Convolutional neural network architectures</li>
<li class="calibre11">TensorFlow</li>
<li class="calibre11">PyTorch</li>
<li class="calibre11">Torchtext</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Scikit-learn and fastText</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will be talking about how to integrate fastText into your statistical models. The most common and popular library for statistical machine learning is scikit-learn, so we will focus on that.</p>
<p class="calibre2"/>
<p class="calibre2">scikit-learn is one of the most popular machine learning tools and the reason is that the API is very simple and uniform. The flow is like this:</p>
<ol class="calibre13">
<li value="1" class="calibre11">You basically convert your data into matrix format.</li>
<li value="2" class="calibre11">Then, you create an instance of the predictor class.</li>
<li value="3" class="calibre11">Using the instance, you run the <kbd class="calibre12">fit</kbd> method on the data.</li>
<li value="4" class="calibre11">Once the model is created, you can run <kbd class="calibre12">predict</kbd> on it.</li>
</ol>
<p class="calibre2">This means that you can create a custom classifier by defining the fit and predict methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Custom classifiers for fastText</h1>
                
            
            <article>
                
<p class="calibre2">Since we are interested in combining fastText word vectors with the linear classifiers, you cannot pass the whole vectors and would need a way to define a single vector. In this case, let's go with the mean:</p>
<pre class="calibre17">class MeanEmbeddingVectorizer(object):<br class="title-page-name"/>    def __init__(self, ft_wv):<br class="title-page-name"/>        self.ft_wv = ft_wv<br class="title-page-name"/>        if len(ft_wv)&gt;0:<br class="title-page-name"/>            self.dim = ft_wv[next(iter(all_words))].shape[0] <br class="title-page-name"/>        else:<br class="title-page-name"/>            self.dim=0<br class="title-page-name"/>            <br class="title-page-name"/>    def fit(self, X, y):<br class="title-page-name"/>        return self <br class="title-page-name"/><br class="title-page-name"/>    def transform(self, X):<br class="title-page-name"/>        return np.array([<br class="title-page-name"/>            np.mean([self.ft_wv[w] for w in words if w in self.ft_wv] <br class="title-page-name"/>                    or [np.zeros(self.dim)], axis=0)<br class="title-page-name"/>            for words in X<br class="title-page-name"/>        ])</pre>
<p class="calibre2">Now, you will need to pass the token dictionary to the model, which can be built from the fastText library:</p>
<pre class="calibre17">f = load_model(FT_MODEL)<br class="title-page-name"/><br class="title-page-name"/>all_words = set([x for tokens in data['tokens'].values for x in tokens])<br class="title-page-name"/><br class="title-page-name"/>wv_dictionary = {w: f.get_word_vector(w) for w in all_words}</pre>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Bringing the whole thing together</h1>
                
            
            <article>
                
<p class="calibre2">You can use scikit-learn's <kbd class="calibre12">Pipeline</kbd> to combine the whole pipeline, demonstrated as follows:</p>
<pre class="calibre17">etree_w2v = Pipeline([("fasttext word-vector vectorizer",                                 MeanEmbeddingVectorizer(wv_dictionary)), <br class="title-page-name"/>                        ("extra trees", ExtraTreesClassifier(n_estimators=200))])</pre>
<p class="calibre2">The whole code is shown in the statistical machine learning notebook. For further ways of building a better model is if you can find better ways of reducing the word vectors, TF-IDF is shown in the shared notebook. Another way of reducing the word vectors is looking at the hashing transformer.</p>
<p class="calibre2">In the next sections, we will take a look at how to embed fastText vectors in deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Embeddings</h1>
                
            
            <article>
                
<p class="calibre2">As you have seen, when you need to work with text in machine learning, you need to convert the text into numerical values. The logic is the same in neural architectures as well. In neural networks, you implement this using the embeddings layer. All modern deep learning libraries provide an embeddings API for use.</p>
<p class="calibre2">The embeddings layer is a useful and versatile layer used for various purposes:</p>
<ul class="calibre10">
<li class="calibre11">It can be used to learn word embeddings to be used in an application later</li>
<li class="calibre11">It can be used with a larger model where the embeddings are also tuned as part of the model</li>
<li class="calibre11">It can be used to load a pretrained word embedding</li>
</ul>
<p class="calibre2">It is in the third point that will be the focus of this section. The idea is to utilize fastText to create superior embeddings, which can then be injected into your model using this embedding layer. Normally the embeddings layer is initialized with random weights, but in this case we will be injecting it with the word embeddings from our fastText model.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Keras</h1>
                
            
            <article>
                
<p class="calibre2">Keras is a widely popular high-level neural network API. It supports TensorFlow, CNTK, and Theano as the backend. Due to the user-friendly API of Keras, many people use it in lieu of the base libraries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Embedding layer in Keras</h1>
                
            
            <article>
                
<p class="calibre2">The embedding layer will be the first hidden layer of the Keras network and you will need to specify three arguments: input dimension, output dimension, and input length. Since we will be using fastText to make our model better, we will also need to pass the weights parameter with the embedding matrix and make the trainable matrix to be false:</p>
<pre class="calibre17">embedding_layer = Embedding(num_words,<br class="title-page-name"/>                            EMBEDDING_DIM, <br class="title-page-name"/>                            weights=[embedding_matrix],<br class="title-page-name"/>                            input_length=MAX_SEQUENCE_LENGTH,<br class="title-page-name"/>                            trainable=False)</pre>
<p class="calibre2">Another thing that we need to take care of is that we need to map words to integers and integers to words. In Keras, you do this using the <kbd class="calibre12">Tokenizer</kbd> class.</p>
<p class="calibre2">Let's look at this in action as part of a CNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Convolutional neural networks</h1>
                
            
            <article>
                
<p class="calibre2">When we talk in terms of mixing word embeddings and neural networks, convolutional networks are something that have yielded good results. CNNs are created by applying several layers of convolutions with nonlinear activation functions such as ReLu or <em class="calibre16">tanh</em> applied to the results.</p>
<p class="calibre2">Lets talk a little bit about what a convolution means. A convolution of any function with respect to another function is the integral that expresses the amount of overlap of one function as it is passed over the other function. You can think of this as blending one function into another. In signal theory, this is how experts understand convolutions: the output signal is a convolution of the input signal with the impulse response of the environment. The impulse response of any environment essentially identifies and distinguishes the environment.</p>
<p class="calibre2">In a traditional feedforward network, we connect each input neuron to each output neuron in the next layer. In CNNs, we <span class="calibre5">instead</span><span class="calibre5"> </span><span class="calibre5">use convolutions on the input to compute the output. During the training phase, a CNN will automatically learn the values of the filters.</span></p>
<p class="calibre2">CNNs are generally used with word embeddings, and it is here that fastText comes into the picture and has the potential to bring huge gains in terms of classification accuracy by providing better word representations. The architecture is thus composed of three key sections:</p>
<div class="cdpaligncenter"><img src="../images/00088.jpeg" class="calibre55"/></div>
<p class="calibre2">If you have these three pieces of the architecture already in place in your classification pipeline, you can identify the word embeddings part and see whether changing that to fasttext gives you an improvement in terms of the predictions.</p>
<p class="calibre2">In this example, we will be taking a look at the previous example of Yelp reviews and trying to classify them using a convolution neural network. For the sake of brevity, we will fit a pretrained dataset that has been released, but you will probably be working on a domain-specific use case and hence you should be integrating the creation of the model, as shown in the previous chapter. As you have seen, you can use fastText library or the Gensim library.</p>
<p class="calibre2">From a high level, the steps are as follows:</p>
<ol class="calibre13">
<li value="1" class="calibre11">Text samples in the dataset are converted into sequences of word indices. A <em class="calibre21">word index</em> would simply be an integer ID for the word. We will only consider the top 20,000 most common words in the dataset, and we will truncate the sequences to a maximum of 1,000 words. This is done for ease of computation. You can play around with this approach and find the approach that brings the greatest generalization.</li>
<li value="2" class="calibre11">Prepare an embedding matrix, which will contain at index <kbd class="calibre12">i</kbd> the embedding vector for the word <kbd class="calibre12">i</kbd> in the word index.</li>
</ol>
<ol start="3" class="calibre13">
<li value="3" class="calibre11">The embedding matrix is then loaded to the Keras embedding layer, and will set the layer to be frozen so that it is not updated while training.</li>
<li value="4" class="calibre11">The layers that come after it will be the convolution networks. At the end, there will be a softmax layer to converge the output to our five categories.</li>
</ol>
<p class="calibre2">In this case, we can use the pandas library to create the list of input text and the list of the output labels:</p>
<pre class="calibre17">&gt;&gt;&gt; df = pd.read_csv('yelp_review.csv')<br class="title-page-name"/>&gt;&gt;&gt; texts = df.text.values<br class="title-page-name"/>&gt;&gt;&gt; labels = df.stars.values<br class="title-page-name"/>&gt;&gt;&gt; texts = texts[:20000]<br class="title-page-name"/>&gt;&gt;&gt; labels = labels[:20000]<br class="title-page-name"/>&gt;&gt;&gt; print('Found %s texts.' % len(texts))<br class="title-page-name"/>Found 20000 texts.</pre>
<p class="calibre2">Now, we will need to format our text samples and labels into tensors that can be fed into the neural network. This is the place where we will be using the <kbd class="calibre12">Tokenizer</kbd> class. We will also need to pad the sequences as we need matrices of equal lengths:</p>
<pre class="calibre17">&gt;&gt;&gt; # finally, vectorize the text samples into a 2D integer tensor<br class="title-page-name"/>&gt;&gt;&gt; tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)<br class="title-page-name"/>&gt;&gt;&gt; tokenizer.fit_on_texts(texts)<br class="title-page-name"/>&gt;&gt;&gt; sequences = tokenizer.texts_to_sequences(texts)<br class="title-page-name"/>&gt;&gt;&gt; word_index = tokenizer.word_index<br class="title-page-name"/>&gt;&gt;&gt; print('Found %s unique tokens.' % len(word_index))<br class="title-page-name"/>Found 45611 unique tokens.<br class="title-page-name"/>&gt;&gt;&gt; data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)<br class="title-page-name"/>&gt;&gt;&gt; labels = to_categorical(np.asarray(labels))<br class="title-page-name"/>&gt;&gt;&gt; print('Shape of data tensor:', data.shape)<br class="title-page-name"/>&gt;&gt;&gt; print('Shape of label tensor:', labels.shape)<br class="title-page-name"/>Shape of data tensor: (20000, 1000)<br class="title-page-name"/>Shape of label tensor: (20000, 6)<br class="title-page-name"/>&gt;&gt;&gt; # split the data into a training set and a validation set<br class="title-page-name"/>&gt;&gt;&gt; indices = np.arange(data.shape[0])<br class="title-page-name"/>&gt;&gt;&gt; np.random.shuffle(indices)<br class="title-page-name"/>&gt;&gt;&gt; data = data[indices]<br class="title-page-name"/>&gt;&gt;&gt; labels = labels[indices]<br class="title-page-name"/>&gt;&gt;&gt; num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])<br class="title-page-name"/>&gt;&gt;&gt; x_train = data[:-num_validation_samples]<br class="title-page-name"/>&gt;&gt;&gt; y_train = labels[:-num_validation_samples]<br class="title-page-name"/>&gt;&gt;&gt; x_val = data[-num_validation_samples:]<br class="title-page-name"/>&gt;&gt;&gt; y_val = labels[-num_validation_samples:]</pre>
<p class="calibre2">Now we will be using our fastText embeddings. In this case, we are using pretrained embeddings, but you can train your own embeddings on the fly during the training process. You have the choice of loading from the <kbd class="calibre12">.vec</kbd> file, but since this is fastText, we will load from the BIN file. The advantage of using the BIN file is that the out of vocabulary case will be avoided to a large extent. We will use the fastText model and generate an embedding matrix:</p>
<pre class="calibre17"><span>&gt;&gt;&gt; print</span><span>(</span><span>'Preparing embedding matrix.'</span><span>)</span>

<span>&gt;&gt;&gt;# load the fasttext model</span>
<span>&gt;&gt;&gt; f</span> <span>=</span> <span>load_model</span><span>(</span><span>FT_MODEL</span><span>)</span>

<span>&gt;&gt;&gt; # prepare embedding matrix</span>
<span>&gt;&gt;&gt; num_words</span> <span>=</span> <span>min</span><span>(</span><span>MAX_NUM_WORDS</span><span>,</span> <span>len</span><span>(</span><span>word_index</span><span>)</span> <span>+</span> <span>1</span><span>)</span>
<span>&gt;&gt;&gt; embedding_matrix</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_words</span><span>,</span> <span>EMBEDDING_DIM</span><span>))</span>
<span>&gt;&gt;&gt; for</span> <span>word</span><span>,</span> <span>i</span> <span>in</span> <span>word_index</span><span>.</span><span>items</span><span>():</span>
...     <span>if</span> <span>i</span> <span>&gt;=</span> <span>MAX_NUM_WORDS</span><span>:</span>
...         <span>continue</span>
...     <span>embedding_vector</span> <span>=</span> <span>f</span><span>.</span><span>get_word_vector</span><span>(</span><span>word</span><span>)</span>
...     <span>if</span> <span>embedding_vector</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
...         <span># words not found in embedding index will be all-zeros.</span>
...         <span>embedding_matrix</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>embedding_vector</span></pre>
<p class="calibre2">We load this to the embedding layer. It is important to note that the trainable parameter should be set to <kbd class="calibre12">False</kbd> to prevent the weights from being updated, as follows:</p>
<pre class="calibre17"><span>&gt;&gt;&gt; # load pre-trained word embeddings into an Embedding layer</span>
<span>&gt;&gt;&gt; embedding_layer</span> <span>=</span> <span>Embedding</span><span>(</span><span>num_words</span><span>,</span>
...                             <span>EMBEDDING_DIM</span><span>,</span>
...                             <span>weights</span><span>=</span><span>[</span><span>embedding_matrix</span><span>],</span>
...                             <span>input_length</span><span>=</span><span>MAX_SEQUENCE_LENGTH</span><span>,</span>
...                             <span>trainable</span><span>=</span><span>False</span><span>)</span></pre>
<p class="calibre2">Now, we can build a 1D ConvNet to apply to our Yelp classification problem:</p>
<pre class="calibre17"><span>&gt;&gt;&gt; # train a 1D convnet with global maxpooling</span>
<span>&gt;&gt;&gt; sequence_input</span> <span>=</span> <span>Input</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>MAX_SEQUENCE_LENGTH</span><span>,),</span> <span>dtype</span><span>=</span><span>'int32'</span><span>)</span>
<span>&gt;&gt;&gt; embedded_sequences</span> <span>=</span> <span>embedding_layer</span><span>(</span><span>sequence_input</span><span>)</span>
<span>&gt;&gt;&gt; x</span> <span>=</span> <span>Conv1D</span><span>(</span><span>128</span><span>,</span> <span>5</span><span>,</span> <span>activation</span><span>=</span><span>'relu'</span><span>)(</span><span>embedded_sequences</span><span>)</span>
<span>&gt;&gt;&gt; x</span> <span>=</span> <span>MaxPooling1D</span><span>(</span><span>5</span><span>)(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; x</span> <span>=</span> <span>Conv1D</span><span>(</span><span>128</span><span>,</span> <span>5</span><span>,</span> <span>activation</span><span>=</span><span>'relu'</span><span>)(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>MaxPooling1D</span><span>(</span><span>5</span><span>)(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>Conv1D</span><span>(</span><span>128</span><span>,</span> <span>5</span><span>,</span> <span>activation</span><span>=</span><span>'relu'</span><span>)(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>GlobalMaxPooling1D</span><span>()(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>x</span> <span>=</span> <span>Dense</span><span>(</span><span>128</span><span>,</span> <span>activation</span><span>=</span><span>'relu'</span><span>)(</span><span>x</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>preds</span> <span>=</span> <span>Dense</span><span>(</span><span>6</span><span>,</span> <span>activation</span><span>=</span><span>'softmax'</span><span>)(</span><span>x</span><span>)</span></pre>
<p class="calibre2">The summary of this model is shown as follows:</p>
<pre class="calibre17"><span>_________________________________________________________________ <br class="title-page-name"/>Layer (type) Output Shape Param #<br class="title-page-name"/>=================================================================<br class="title-page-name"/>input_1 (InputLayer) (None, 1000) 0<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>embedding_1 (Embedding) (None, 1000, 300) 6000000 _______________________________________________________________<br class="title-page-name"/>conv1d_1 (Conv1D) (None, 996, 128) 192128<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>max_pooling1d_1 (MaxPooling1 (None, 199, 128) 0 _______________________________________________________________<br class="title-page-name"/>conv1d_2 (Conv1D) (None, 195, 128) 82048<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>max_pooling1d_2 (MaxPooling1 (None, 39, 128) 0 _______________________________________________________________<br class="title-page-name"/>conv1d_3 (Conv1D) (None, 35, 128) 82048<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>global_max_pooling1d_1 (Glob (None, 128) 0<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>dense_1 (Dense) (None, 128) 16512<br class="title-page-name"/>_______________________________________________________________<br class="title-page-name"/>dense_2 (Dense) (None, 6) 774<br class="title-page-name"/>=================================================================<br class="title-page-name"/>Total params: 6,373,510 <br class="title-page-name"/>Trainable params: 373,510 <br class="title-page-name"/>Non-trainable params: 6,000,000<br class="title-page-name"/>_________________________________________________________________</span></pre>
<p class="calibre2">Now, you can try out some other hyperparameters and try to improve the accuracy from here.</p>
<p class="calibre2">In this section, you saw how to use the fastText word embeddings as part of a larger CNN Keras classifier. Using a similar approach, you can use fastText embeddings with whichever neural architectures benefit from word embeddings in Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">TensorFlow</h1>
                
            
            <article>
                
<p class="calibre2">TensorFlow is a computation library developed by Google. It is quite popular now and is used by many companies to create their neural network models. After what you have seen in Keras, the logic behind augmenting TensorFlow models using fastText is the same.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Word embeddings in TensorFlow</h1>
                
            
            <article>
                
<p class="calibre2">To create word embeddings in TensorFlow, you will need to create an embeddings matrix where all the tokens in your list of documents have unique IDs, and so each document is a vector of these IDs. <span class="calibre5">Now, let's say you have an embedding in a NumPy array called <kbd class="calibre12">word_embedding</kbd>, with <kbd class="calibre12">vocab_size</kbd> rows and <kbd class="calibre12">embedding_dim</kbd> columns, and you want to create a tensor <kbd class="calibre12">W</kbd>. Taking a specific example,</span> the sentence "I have a cat." can be split up into ["I", "have", "a", "cat", "."], and the tensor of<span class="calibre5"> the corresponding <kbd class="calibre12">word_ids</kbd></span> will be of shape 5. To map these word IDs into vectors, create the word embedding variable and use the <kbd class="calibre12">tf.nn.embedding_lookup</kbd> function:</p>
<pre class="calibre17">word_embeddings = tf.get_variable(“word_embeddings”,<br class="title-page-name"/>                                  [vocabulary_size, embedding_size])<br class="title-page-name"/>embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)</pre>
<p class="calibre2">After this, the <kbd class="calibre12">embedded_word_ids</kbd> <span class="calibre5">tensor</span><span class="calibre5"> </span><span class="calibre5">will have</span> <kbd class="calibre12">shape [5, embedding_size]</kbd> <span class="calibre5">in our example and contain the embeddings (dense vectors) for each of the five words.</span></p>
<p class="calibre2">To be able to use a word embedding with pretrained vectors, create <kbd class="calibre12">W</kbd> as a <kbd class="calibre12">tf.Variable</kbd> and initialize it from the NumPy array via a <kbd class="calibre12">tf.placeholder()</kbd>:</p>
<pre class="calibre17">with tf.name_scope("embedding"):<br class="title-page-name"/>    W = tf.Variable(tf.constant(0.0,<br class="title-page-name"/>                                shape=[doc_vocab_size,<br class="title-page-name"/>                                       embedding_dim]),<br class="title-page-name"/>                    trainable=False, <br class="title-page-name"/>                    name="W")<br class="title-page-name"/> embedding_placeholder = tf.placeholder(tf.float32,<br class="title-page-name"/>    [doc_vocab_size, embedding_dim])<br class="title-page-name"/> embedding_init = W.assign(embedding_placeholder)<br class="title-page-name"/> embedded_chars = tf.nn.embedding_lookup(W,x)</pre>
<p class="calibre2">You can then pass the actual embeddings in the TensorFlow session:</p>
<pre class="calibre17">sess = tf.Session()<br class="title-page-name"/>sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})</pre>
<p class="calibre2">This will avoid storing a copy of the embeddings in the graph, but it does require enough memory to keep two copies of the matrix at once (one for the NumPy array and one for the <kbd class="calibre12">tf.Variable</kbd>). You would want to have word embeddings constant during the training process, so as discussed earlier, the trainable parameter for the embeddings needs to be <kbd class="calibre12">False</kbd>.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">RNN architectures</h1>
                
            
            <article>
                
<p class="calibre2">NLP has always been considered to be an excellent use case for LSTMs and RNN-type neural architectures. LSTMs and RNNs use sequential processing. NLP has always been considered one of the biggest use cases because the meaning of any sentence is context-based. The meaning of a word can be considered to have meaning based on all the words that came before it:</p>
<div class="cdpaligncenter"><img src="../images/00089.jpeg" class="calibre56"/></div>
<p class="calibre2">Now, when you are running an LSTM network, you need to convert the words into an embedding layer. Generally in such cases, a random initializer is used. But, you <span class="calibre5">probably</span><span class="calibre5"> </span><span class="calibre5">should be able to increase the performance of the model using a fastText model. Let's take a look at how a fastText model is used in such cases.</span></p>
<p class="calibre2">In this example, the crawl vectors that are released by Facebook are loaded into memory. In your use case, you should probably train a fastText model on your text corpus and load that model. We are creating the embedding using the VEC file here, but you can chose to load from a <kbd class="calibre12">.bin</kbd> file as well, as shown in the Keras example:</p>
<pre class="calibre17">#Load fasttext vectors<br class="title-page-name"/>filepath_glove = 'crawl-300d-2M.vec'<br class="title-page-name"/>glove_vocab = []<br class="title-page-name"/>glove_embd=[]<br class="title-page-name"/>embedding_dict = {}<br class="title-page-name"/><br class="title-page-name"/>with open(filepath_glove) as file:<br class="title-page-name"/>    for index, line in enumerate(file):<br class="title-page-name"/>        values = line.strip().split() # Word and weights separated by space<br class="title-page-name"/>        if index == 0:<br class="title-page-name"/>            glove_vocab_size = int(values[0])<br class="title-page-name"/>            embedding_dim = int(values[1])<br class="title-page-name"/>        else:<br class="title-page-name"/>            row = line.strip().split(' ')<br class="title-page-name"/>            vocab_word = row[0]<br class="title-page-name"/>            glove_vocab.append(vocab_word)<br class="title-page-name"/>            embed_vector = [float(i) for i in row[1:]] # convert to list of float<br class="title-page-name"/>            embedding_dict[vocab_word]=embed_vector</pre>
<p class="calibre2"><span class="calibre5">Call the block of text that is your target of interest, and then run the normal cleaning step. Similar to the example in Keras, you will next need a mechanism that maps the tokens to a unique integer and a way to get the token back from the integer. So, we will need to create a dictionary and a reverse dictionary with the words:</span></p>
<pre class="calibre17">#Create dictionary and reverse dictionary with word ids<br class="title-page-name"/> <br class="title-page-name"/>def build_dictionaries(words):<br class="title-page-name"/>    count = collections.Counter(words).most_common() #creates list of word/count pairs;<br class="title-page-name"/>    dictionary = dict()<br class="title-page-name"/>    for word, _ in count:<br class="title-page-name"/>        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration<br class="title-page-name"/>        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))<br class="title-page-name"/>    return dictionary, reverse_dictionary<br class="title-page-name"/> <br class="title-page-name"/>dictionary, reverse_dictionary = build_dictionaries(training_data)</pre>
<p class="calibre2">We create the embedding array from the dictionary that we created using the fastText model: </p>
<pre class="calibre17">#Create embedding array<br class="title-page-name"/> <br class="title-page-name"/>doc_vocab_size = len(dictionary)<br class="title-page-name"/>dict_as_list = sorted(dictionary.items(), key = lambda x : x[1])<br class="title-page-name"/> <br class="title-page-name"/>embeddings_tmp=[]<br class="title-page-name"/> <br class="title-page-name"/>for i in range(doc_vocab_size):<br class="title-page-name"/>    item = dict_as_list[i][0]<br class="title-page-name"/>    if item in glove_vocab:<br class="title-page-name"/>        embeddings_tmp.append(embedding_dict[item])<br class="title-page-name"/>    else:<br class="title-page-name"/>        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)<br class="title-page-name"/>        embeddings_tmp.append(rand_num)<br class="title-page-name"/> <br class="title-page-name"/># final embedding array corresponds to dictionary of words in the document<br class="title-page-name"/>embedding = np.asarray(embeddings_tmp)<br class="title-page-name"/> <br class="title-page-name"/># create tree so that we can later search for closest vector to prediction<br class="title-page-name"/>tree = spatial.KDTree(embedding)</pre>
<p class="calibre2">Next, we set up the RNN model. We will be reading three words at a time and hence our <kbd class="calibre12">x</kbd> is a matrix with an undetermined number of rows and three columns wide. Another input of note is the <kbd class="calibre12">embedding_placeholder</kbd> has one row per word and has a width of 300 dimensions, corresponding to the number of dimensions for the input word vector.</p>
<p class="calibre2">Then, the TensorFlow <kbd class="calibre12">tf.nn.embedding_lookup()</kbd> <span class="calibre5">function </span><span class="calibre5">can be used to look up each of our inputs from</span> <kbd class="calibre12">x</kbd> <span class="calibre5">in matrix</span> <kbd class="calibre12">W</kbd><span class="calibre5">, resulting in the 3D tensor</span> <kbd class="calibre12">embedded_chars</kbd><span class="calibre5">. This can </span><span class="calibre5">then</span><span class="calibre5"> </span><span class="calibre5">be fed into the RNN:</span></p>
<pre class="calibre17"># create input placeholders<br class="title-page-name"/>x = tf.placeholder(tf.int32, [None, n_input])<br class="title-page-name"/>y = tf.placeholder(tf.float32, [None, embedding_dim])<br class="title-page-name"/> <br class="title-page-name"/># RNN output node weights and biases<br class="title-page-name"/>weights = { 'out': tf.Variable(tf.random_normal([n_hidden, embedding_dim])) }<br class="title-page-name"/>biases = { 'out': tf.Variable(tf.random_normal([embedding_dim])) }<br class="title-page-name"/> <br class="title-page-name"/>with tf.name_scope("embedding"):<br class="title-page-name"/>    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=False, name="W")<br class="title-page-name"/>    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])<br class="title-page-name"/>    embedding_init = W.assign(embedding_placeholder)<br class="title-page-name"/>    embedded_chars = tf.nn.embedding_lookup(W,x)<br class="title-page-name"/>  <br class="title-page-name"/># reshape input data<br class="title-page-name"/>x_unstack = tf.unstack(embedded_chars, n_input, 1)<br class="title-page-name"/> <br class="title-page-name"/># create RNN cells<br class="title-page-name"/>rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])<br class="title-page-name"/>outputs, states = rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)<br class="title-page-name"/> <br class="title-page-name"/># capture only the last output<br class="title-page-name"/>pred = tf.matmul(outputs[-1], weights['out']) + biases['out']</pre>
<p class="calibre2">Now that we have the RNN, we need to figure out how to train it and what kind of cost function can be used. FastText internally uses the softmax function. The softmax function may not be suitable as a cost function in this case because, by definition, softmax normalizes the vectors before comparing. Thus, the actual vector may grow or shrink in an arbitrary manner. There may be a good case for having the final vectors with the same magnitude as the vectors in the training set, and thus with the same magnitudes as the pretrained vectors. In this example, the focus is on L2 loss:</p>
<pre class="calibre17">cost = tf.reduce_mean(tf.nn.l2_loss(pred-y))<br class="title-page-name"/>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</pre>
<p class="calibre2">Only the important bit of the code snippet is shown here. You will find the whole code in the TensorFlow notebook in the <kbd class="calibre12">chapter6</kbd> folder: <a href="https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter6/tensorflow%20rnn.ipynb" class="calibre9">https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter6/TensorFlow%20rnn.ipynb</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">PyTorch</h1>
                
            
            <article>
                
<p class="calibre2">Following the same logic as the previous two libraries, you can use the <kbd class="calibre12">torch.nn.EmbeddingBag</kbd> class to inject the pretrained embeddings. There is a small drawback though. Keras and TensorFlow make the assumption that your tensors are actually implemented as NumPy arrays, while in the case of PyTorch, that's not the case. PyTorch implements the torch tensor. Generally, this is not an issue, but this means that you will need to write your own text conversion and tokenizing pipelines. To circumvent all this rewriting and reinvention of the wheel, you can use the torchtext library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The torchtext library</h1>
                
            
            <article>
                
<p class="calibre2">The torchtext is an excellent library that takes care of most of the preprocessing steps that you need to build your NLP model. Basically, think of torchtext as something that acts like <em class="calibre16">configuration as code</em> in a loose sense of the term. So, it makes sense to understand the torchtext data paradigm, which takes around three hours, instead of writing custom code, which will probably seem easier but would involve countless hours of confusion and debugging. And to top it off, it can build prebuilt models, including fastText.</p>
<p class="calibre2">Now, let's take a look at how that is done.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data classes in torchtext</h1>
                
            
            <article>
                
<p class="calibre2">We will first call all the required libraries. Take note that you are calling data that contains the required data classes for our use:</p>
<pre class="calibre17">from torchtext import data<br class="title-page-name"/>import spacy<br class="title-page-name"/>...</pre>
<p class="calibre2">We will use <kbd class="calibre12">spacy</kbd> for the tokenization step, for which torchtext has excellent support. torchtext provides excellent support for calling and loading fastText libraries:</p>
<pre class="calibre17">from torchtext.vocab import FastText<br class="title-page-name"/>vectors = FastText('simple')</pre>
<p class="calibre2">This will download the <kbd class="calibre12">wiki.simple.bin</kbd> model. If you provide the name <kbd class="calibre12">en</kbd>, it will download and load <kbd class="calibre12">wiki.en.bin</kbd>. If you load <kbd class="calibre12">fr</kbd>, then it will load <kbd class="calibre12">wiki.fr.bin</kbd>, and so on.</p>
<p class="calibre2">You will probably be loading the data from a CSV or from a text file. In that case, you will need to open the file, possibly in pandas, extract the relevant fields, and then save them in a separate file. torchtext is not able to distinguish between training and validation, and hence you will probably need to separate out those files as well:</p>
<pre class="calibre17">def clean_str(string):<br class="title-page-name"/>    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)<br class="title-page-name"/>    string = re.sub(r"\'s", " \'s", string)<br class="title-page-name"/>    string = re.sub(r"\'ve", " \'ve", string)<br class="title-page-name"/>    string = re.sub(r"n\'t", " n\'t", string)<br class="title-page-name"/>    string = re.sub(r"\'re", " \'re", string)<br class="title-page-name"/>    string = re.sub(r"\'d", " \'d", string)<br class="title-page-name"/>    string = re.sub(r"\'ll", " \'ll", string)<br class="title-page-name"/>    string = re.sub(r",", " , ", string)<br class="title-page-name"/>    string = re.sub(r"!", " ! ", string)<br class="title-page-name"/>    string = re.sub(r"\(", " \( ", string)<br class="title-page-name"/>    string = re.sub(r"\)", " \) ", string)<br class="title-page-name"/>    string = re.sub(r"\?", " \? ", string)<br class="title-page-name"/>    string = re.sub(r"\s{2,}", " ", string)<br class="title-page-name"/>    <br class="title-page-name"/>    return string.strip().lower()<br class="title-page-name"/><br class="title-page-name"/>def prepare_csv(df, seed=999):<br class="title-page-name"/>    df['text'] = df['text'].apply(clean_str)<br class="title-page-name"/>    df_train, df_test = train_test_split(df, test_size=0.2)<br class="title-page-name"/>    df_train.to_csv("yelp_tmp/dataset_train.csv", index=False)<br class="title-page-name"/>    df_test.to_csv("yelp_tmp/dataset_val.csv", index=False)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">You will now need to define the data and build the vocabulary. You can do this using the data module. This module has the data classes to define the pipelining steps and run the batching, padding, and numericalization. First, you will need to define the type of fields using <kbd class="calibre12">data.Fields</kbd>. This class defines the common datatypes that can be used to create the required tensor. You can also define some common instructions to define how the tensor should be created. Once the fields are created, you can call <kbd class="calibre12">TabularDataset</kbd> to create the dataset using the instructions defined in the field. The common instructions are passed as parameters:</p>
<pre class="calibre17"># Define all the types of fields<br class="title-page-name"/># pip install spacy for the tokenizer to work (or remove to use default)<br class="title-page-name"/>TEXT = data.Field(lower=True, include_lengths=True, fix_length=150, tokenize='spacy')<br class="title-page-name"/>LABEL = data.Field(sequential=True, use_vocab=False)<br class="title-page-name"/><br class="title-page-name"/># we use the index field to re-sort test data after processing<br class="title-page-name"/>INDEX = data.Field(sequential=False)<br class="title-page-name"/><br class="title-page-name"/>train_fields=[<br class="title-page-name"/>    (text_label, TEXT),<br class="title-page-name"/>    (stars_label, LABEL)<br class="title-page-name"/>]<br class="title-page-name"/><br class="title-page-name"/>train_fields=[<br class="title-page-name"/>    (text_label, TEXT),<br class="title-page-name"/>    (stars_label, LABEL)<br class="title-page-name"/>]<br class="title-page-name"/><br class="title-page-name"/>train = data.TabularDataset(<br class="title-page-name"/>    path='yelp_tmp/dataset_train.csv', format='csv', skip_header=True,<br class="title-page-name"/>    fields=train_fields)<br class="title-page-name"/><br class="title-page-name"/>test_fields=[<br class="title-page-name"/>    (id_label, INDEX),<br class="title-page-name"/>    (text_label, TEXT),<br class="title-page-name"/>    (stars_label, LABEL)<br class="title-page-name"/>]<br class="title-page-name"/>test = data.TabularDataset(<br class="title-page-name"/>        path='yelp_tmp/dataset_val.csv', format='csv', skip_header=True,<br class="title-page-name"/>        fields=test_fields)</pre>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">sequential=True</kbd> means that the column has sequences. We probably want that to be the case for labels as the example is pretty much a comparison, but in cases where that is not the case, set this to false.</li>
<li class="calibre11">We are specifying the tokenizer to be spacy in this case, but you can specify custom functions.</li>
<li class="calibre11"><kbd class="calibre12">fix_length</kbd> pads or trims all sequences to fixed lengths of 150 in this case.</li>
<li class="calibre11"><kbd class="calibre12">lower</kbd> specifies we are setting all English letters to lowercase.</li>
</ul>
<p class="calibre2">Once the datasets are created, you will need to create the vocabulary so that we can convert the tokens into integer numbers later. It is here that we are going to build the vocabulary from the fastText vectors that we loaded earlier:</p>
<pre class="calibre17">max_size = 30000<br class="title-page-name"/>TEXT.build_vocab(train, test, vectors=vectors, max_size=max_size)<br class="title-page-name"/>INDEX.build_vocab(test)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Using the iterators</h1>
                
            
            <article>
                
<p class="calibre2">You can now use an iterator to iterate over the dataset. In this case, we are using <kbd class="calibre12">BucketIterator</kbd>, which has the additional advantage that it batches examples of similar lengths together. This reduces the amount of padding needed:</p>
<pre class="calibre17">train = data.BucketIterator(train, batch_size=32,<br class="title-page-name"/>                            sort_key=lambda x: len(x.text),<br class="title-page-name"/>                            sort_within_batch=True, repeat=False)<br class="title-page-name"/>test = data.BucketIterator(test, batch_size=128,<br class="title-page-name"/>                           sort_key=lambda x: len(x.text),<br class="title-page-name"/>                           sort_within_batch=True, train=False,<br class="title-page-name"/>                           repeat=False)</pre>
<p class="calibre2">So, you will be able to run simple <kbd class="calibre12">for</kbd> loops on these iterators and they will provide inputs based on batches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Bringing it all together</h1>
                
            
            <article>
                
<p class="calibre2">Finally, once all these steps are done, you can initialize your PyTorch model, and you will need to set the pretrained vectors as the weights of the model. In the example, an RNN model was created and the word vectors were initialized from the earlier field vectors. This will take care of handling the <kbd class="calibre12">lookup_table</kbd> in PyTorch:</p>
<pre class="calibre17">model = RNNModel('GRU', ntokens, emsize, nhidden, 6,<br class="title-page-name"/>                  nlayers, dropemb=dropemb, droprnn=droprnn, <br class="title-page-name"/>                  bidirectional=True)<br class="title-page-name"/>model.encoder.weight.data.copy_(TEXT.vocab.vectors)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The code shown here includes only the things that should be new to you. For the full code, take a look at the <kbd class="calibre12">pytorch torchtext rnn.ipynb</kbd> notebook in the repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we took a look at how to integrate fastText word vectors into either linear machine learning models or deep learning models created in Keras, TensorFlow, and PyTorch. You also saw how word vectors can be easily assimilated into existing neural architectures that you might be using in your business application. If you are initializing the embeddings from random values, I would highly recommend that you try to initialize them using fastText values, and then see whether there are performance improvements in your model.</p>


            </article>

            
        </section>
    </body></html>