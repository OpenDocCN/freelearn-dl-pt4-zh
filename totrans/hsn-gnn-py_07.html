<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer327">
<h1 class="chapter-number" id="_idParaDest-79"><a id="_idTextAnchor082"/>7</h1>
<h1 id="_idParaDest-80"><a id="_idTextAnchor083"/>Graph Attention Networks</h1>
<p><strong class="bold">Graph Attention Networks</strong> (<strong class="bold">GATs</strong>) are a <a id="_idIndexMarker355"/>theoretical improvement over GCNs. Instead of static normalization coefficients, they propose weighting factors calculated by a process<a id="_idIndexMarker356"/> called <strong class="bold">self-attention</strong>. The same <a id="_idIndexMarker357"/>process is at the core of one of the most successful deep learning <a id="_idIndexMarker358"/>architectures: the <strong class="bold">transformer</strong>, popularized <a id="_idIndexMarker359"/>by <strong class="bold">BERT</strong> and <strong class="bold">GPT-3</strong>. Introduced by Veličković et al. in 2017, GATs have become one of the most popular GNN architectures thanks to excellent <span class="No-Break">out-of-the-box performance.</span></p>
<p>In this chapter, we will learn how the graph attention layer works in four steps. This is actually the perfect example for understanding how self-attention works in general. This theoretical background will allow us to implement a graph attention layer from scratch in <strong class="source-inline">NumPy</strong>. We will build the matrices by ourselves to understand how their values are calculated at <span class="No-Break">each step.</span></p>
<p>In the last section, we’ll use a GAT on two node classification datasets: <strong class="source-inline">Cora</strong>, and a new one called <strong class="source-inline">CiteSeer</strong>. As anticipated in the last chapter, this will be a good opportunity to analyze our results a little further. Finally, we will compare the accuracy of this architecture with <span class="No-Break">a GCN.</span></p>
<p>By the end of this chapter, you will be able to implement a graph attention layer from scratch and a GAT <a id="_idIndexMarker360"/>in <strong class="bold">PyTorch Geometric</strong> (<strong class="bold">PyG</strong>). You will learn about the differences between this architecture and a GCN. Furthermore, you will master an error analysis tool for <span class="No-Break">graph data.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Introducing the graph <span class="No-Break">attention layer</span></li>
<li>Implementing the graph attention layer <span class="No-Break">in NumPy</span></li>
<li>Implementing a GAT in <span class="No-Break">PyTorch Geometric</span></li>
</ul>
<h1 id="_idParaDest-81"><a id="_idTextAnchor084"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter07"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter07</span></a><span class="No-Break">.</span></p>
<p>The installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> section of <span class="No-Break">this boo<a id="_idTextAnchor085"/>k.</span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor086"/>Introducing the graph attention layer</h1>
<p>The main idea <a id="_idIndexMarker361"/>behind GATs is that some nodes are more important than others. In fact, this was already the case with the graph convolutional layer: nodes with few neighbors were more important than others, thanks to the normalization coefficient <img alt="" height="67" src="image/Formula_B19153_07_001.png" width="182"/>. This approach is limiting because it only takes into account node degrees. On the other hand, the goal of the graph attention layer is to produce weighting factors that also consider the importance of <span class="No-Break">node features.</span></p>
<p>Let’s call our weighting <a id="_idIndexMarker362"/>factors <strong class="bold">attention scores</strong> and note, <img alt="" height="38" src="image/Formula_B19153_07_002.png" width="51"/>, the attention score between the nodes <img alt="" height="30" src="image/Formula_B19153_07_005.png" width="13"/> and <img alt="" height="38" src="image/Formula_B19153_07_007.png" width="19"/>. We can define the graph attention operator <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer267">
<img alt="" height="133" src="image/Formula_B19153_07_005.jpg" width="363"/>
</div>
</div>
<p>An important characteristic of GATs is that the attention scores are calculated implicitly by comparing inputs to each other (hence the name <em class="italic">self</em>-attention). In this section, we will see how to calculate these attention scores in four steps and also how to make an improvement to the graph <span class="No-Break">attention layer:</span></p>
<ul>
<li><span class="No-Break">Linear transformation</span></li>
<li><span class="No-Break">Activation function</span></li>
<li><span class="No-Break">Softmax normalization</span></li>
<li><span class="No-Break">Multi-head attention</span></li>
<li>Improved graph <span class="No-Break">attention layer</span></li>
</ul>
<p>First things first, let’s see how the linear transformation differs from <span class="No-Break">previous architectures.</span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor087"/>Linear transformation</h2>
<p>The attention score<a id="_idIndexMarker363"/> represents the importance between a <a id="_idIndexMarker364"/>central node <img alt="" height="36" src="image/Formula_B19153_07_0051.png" width="16"/> and a neighbor <img alt="" height="42" src="image/Formula_B19153_07_0071.png" width="24"/>. As stated previously, it requires <a id="_idIndexMarker365"/>node features from both nodes. In the graph attention layer, it is represented by a concatenation between the hidden vectors <img alt="" height="36" src="image/Formula_B19153_07_008.png" width="63"/> and <img alt="" height="44" src="image/Formula_B19153_07_009.png" width="74"/>, <img alt="" height="53" src="image/Formula_B19153_07_010.png" width="194"/>. Here, <img alt="" height="29" src="image/Formula_B19153_07_011.png" width="40"/> is a classic shared weight matrix to compute hidden vectors. An additional linear transformation is applied to this result with a dedicated learnable weight matrix <img alt="" height="42" src="image/Formula_B19153_07_012.png" width="83"/>. During training, this matrix learns weights to produce attention coefficients <img alt="" height="38" src="image/Formula_B19153_07_013.png" width="46"/>. This process is summarized by the <span class="No-Break">following formula:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer276">
<img alt="" height="74" src="image/Formula_B19153_07_014.jpg" width="530"/>
</div>
</div>
<p>This <a id="_idIndexMarker366"/>output is given to an activation function like in traditional <a id="_idIndexMarker367"/><span class="No-Break">neural networks.</span></p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor088"/>Activation function</h2>
<p>Nonlinearity is an<a id="_idIndexMarker368"/> essential <a id="_idIndexMarker369"/>component in neural networks to approximate nonlinear target functions. Such functions could not be captured by simply stacking linear layers, as their final outcome would still behave like a single <span class="No-Break">linear layer.</span></p>
<p>In the official implementation (<a href="https://github.com/PetarV-/GAT/blob/master/utils/layers.py">https://github.com/PetarV-/GAT/blob/master/utils/layers.py</a>), the authors chose the <strong class="bold">Leaky Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>) activation<a id="_idIndexMarker370"/> function (see <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em>). This function fixes the <em class="italic">dying ReLU</em> problem, where ReLU neurons only <span class="No-Break">output zero:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer277">
<img alt="Figure 7.1 – ReLU versus Leaky ReLU functions" height="396" src="image/B19153_07_001.jpg" width="839"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – ReLU versus Leaky ReLU functions</p>
<p>This is<a id="_idIndexMarker371"/> implemented by applying the Leaky ReLU function to the output of the <a id="_idIndexMarker372"/><span class="No-Break">previous step:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<img alt="" height="53" src="image/Formula_B19153_07_015.jpg" width="444"/>
</div>
</div>
<p>However, we are now facing a new problem: the resulting values are <span class="No-Break">not normalized!</span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor089"/>Softmax normalization</h2>
<p>We want to compare different<a id="_idIndexMarker373"/> attention scores, which means we <a id="_idIndexMarker374"/>need normalized values on the same scale. In machine learning, it is common to use the softmax function for this purpose. Let’s call <img alt="" height="39" src="image/Formula_B19153_07_016.png" width="47"/> the neighboring nodes of node <img alt="" height="34" src="image/Formula_B19153_07_0052.png" width="15"/>, <span class="No-Break">including itself:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<img alt="" height="123" src="image/Formula_B19153_07_018.jpg" width="704"/>
</div>
</div>
<p>The result of this operation gives us our final attention scores <img alt="" height="36" src="image/Formula_B19153_07_019.png" width="48"/>. But there’s another problem: self-attention is not <span class="No-Break">very stable.</span></p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor090"/>Multi-head attention</h2>
<p>This issue was already noticed <a id="_idIndexMarker375"/>by Vaswani et al. (2017) in the original <a id="_idIndexMarker376"/>transformer paper. Their proposed solution consists of calculating multiple embeddings with their own attention scores instead of a single one. This technique is called <span class="No-Break">multi-head attention.</span></p>
<p>The implementation is straightforward, as we just have to repeat the three previous steps multiple times. Each instance produces an embedding <img alt="" height="49" src="image/Formula_B19153_07_020.png" width="41"/>, where <img alt="" height="32" src="image/Formula_B19153_07_021.png" width="23"/> is the index of the attention head. There are two ways of combining <span class="No-Break">these results:</span></p>
<ul>
<li><strong class="bold">Averaging</strong>: With this, we <a id="_idIndexMarker377"/>sum the different embeddings and normalize the result by the number of attention <span class="No-Break">heads </span><span class="No-Break"><img alt="" height="27" src="image/Formula_B19153_07_022.png" width="29"/>:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer286">
<img alt="" height="179" src="image/Formula_B19153_07_023.jpg" width="744"/>
</div>
</div>
<ul>
<li><strong class="bold">Concatenation</strong>: Here, we<a id="_idIndexMarker378"/> concatenate the different embeddings, which will produce a <span class="No-Break">larger matrix:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer287">
<img alt="" height="137" src="image/Formula_B19153_07_024.jpg" width="662"/>
</div>
</div>
<p>In practice, there is a<a id="_idIndexMarker379"/> simple rule to know which one to use: we<a id="_idIndexMarker380"/> choose the concatenation scheme when it’s a hidden layer and the average scheme when it’s the last layer of the network. The entire process can be summarized by the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer288">
<img alt="Figure 7.2 – Calculating attention scores with multi-head attention" height="804" src="image/B19153_07_002.jpg" width="1352"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Calculating attention scores with multi-head attention</p>
<p>This is all there is to know <a id="_idIndexMarker381"/>about the theoretical aspect of the<a id="_idIndexMarker382"/> graph attention layer. However, since its inception in 2017, an improvement has <span class="No-Break">been suggested.</span></p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor091"/>Improved graph attention layer</h2>
<p>Brody et al. (2021) argued that the <a id="_idIndexMarker383"/>graph attention layer only computes a static type of attention. This is an issue because there are simple graph problems we cannot express with a GAT. So they introduced an improved version, called GATv2, which computes a strictly more expressive <span class="No-Break">dynamic</span><span class="No-Break"><a id="_idIndexMarker384"/></span><span class="No-Break"> attention.</span></p>
<p>Their solution consists of modifying the order of operations. The weight matrix <img alt="" height="38" src="image/Formula_B19153_07_025.png" width="44"/> is applied after the concatenation and the attention weight matrix <img alt="" height="41" src="image/Formula_B19153_07_026.png" width="81"/> after the <img alt="" height="46" src="image/Formula_B19153_07_027.png" width="211"/> function. In summary, here is <a id="_idIndexMarker385"/>the original <strong class="bold">Graph Attentional Operator</strong>, <span class="No-Break">also </span><span class="No-Break"><strong class="bold">GAT</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="" height="134" src="image/Formula_B19153_07_028.jpg" width="829"/>
</div>
</div>
<p>And this is the modified <span class="No-Break">operator, GATv2:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer293">
<img alt="" height="131" src="image/Formula_B19153_07_029.jpg" width="777"/>
</div>
</div>
<p>Which one should we use? According to Brody et al., GATv2 <a id="_idIndexMarker386"/>consistently outperforms the GAT and thus should be preferred. In addition to the theoretical proof, they also ran several experiments to show the performance of GATv2 compared to the original GAT. In the rest of this chapter, we will consider both options: the GAT in the second section and GATv2 in the <span class="No-Break">third section.</span></p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor092"/>Implementing the graph attention layer in NumPy</h1>
<p>As <a id="_idIndexMarker387"/>previously stated, neural networks work in<a id="_idIndexMarker388"/> terms of matrix multiplications. Therefore, we need to translate our individual embeddings into operations for the entire graph. In this section, we will implement the original graph attention layer from scratch to properly understand the inner workings of self-attention. Naturally, this process can be repeated several times to create <span class="No-Break">multi-head attention.</span></p>
<p>The first step consists of translating the original graph attention operator in terms of matrices. This is how we defined it in the <span class="No-Break">last section:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer294">
<img alt="" height="136" src="image/Formula_B19153_07_059.jpg" width="323"/>
</div>
</div>
<p>By taking inspiration from the graph linear layer, we can write <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer295">
<img alt="" height="53" src="image/Formula_B19153_07_031.jpg" width="292"/>
</div>
</div>
<p>Where <img alt="" height="39" src="image/Formula_B19153_07_032.png" width="48"/> is a matrix that stores <span class="No-Break">every <img alt="" height="35" src="image/Formula_B19153_07_033.png" width="45"/>.</span></p>
<p>In this<a id="_idIndexMarker389"/> example, we will use the following <a id="_idIndexMarker390"/>graph from the <span class="No-Break">previous chapter:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer298">
<img alt="Figure 7.3 – Simple graph where nodes have different numbers of neighbors" height="382" src="image/B19153_07_003.jpg" width="473"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Simple graph where nodes have different numbers of neighbors</p>
<p>The graph must provide two important pieces of information: the adjacency matrix with self-loops <img alt="" height="41" src="image/Formula_B19153_07_034.png" width="29"/> and the node features <img alt="" height="35" src="image/Formula_B19153_07_035.png" width="33"/>. Let’s see how to implement it <span class="No-Break">in NumPy:</span></p>
<ol>
<li>We<a id="_idIndexMarker391"/> can build the adjacency <a id="_idIndexMarker392"/>matrix from the connections in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span><pre class="source-code">
import numpy as np
np.random.seed(0)
A = np.array([
    [1, 1, 1, 1],
    [1, 1, 0, 0],
    [1, 0, 1, 1],
    [1, 0, 1, 1]
])
array([[1, 1, 1, 1],
       [1, 1, 0, 0],
       [1, 0, 1, 1],
       [1, 0, 1, 1]])</pre></li>
<li>For <img alt="" height="32" src="image/Formula_B19153_07_036.png" width="31"/>, we generate a random matrix of node features <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">np.random.uniform()</strong></span><span class="No-Break">:</span><pre class="source-code">
X = np.random.uniform(-1, 1, (4, 4))
array([[ 0.0976270,  0.4303787,  0.2055267,  0.0897663],
       [-0.1526904,  0.2917882, -0.1248255,  0.783546 ],
       [ 0.9273255, -0.2331169,  0.5834500,  0.0577898],
       [ 0.1360891,  0.8511932, -0.8579278, -0.8257414]])</pre></li>
<li>The<a id="_idIndexMarker393"/> next step is to define our weight<a id="_idIndexMarker394"/> matrices. Indeed, in graph attention layers, there are two of them: the regular weight matrix <img alt="" height="30" src="image/Formula_B19153_07_037.png" width="40"/>, and the attention weight matrix <img alt="" height="41" src="image/Formula_B19153_07_038.png" width="81"/>. There are different ways to initialize them (Xavier or He initialization, for example), but we can just reuse the same random function in <span class="No-Break">this example.</span></li>
</ol>
<p>The matrix <img alt="" height="29" src="image/Formula_B19153_07_039.png" width="38"/> has to be carefully designed as its dimensions are <img alt="" height="42" src="image/Formula_B19153_07_040.png" width="604"/> Notice that <img alt="" height="43" src="image/Formula_B19153_07_041.png" width="285"/> is already fixed because it represents the number of nodes in <img alt="" height="32" src="image/Formula_B19153_07_042.png" width="31"/>. On the contrary, the value of <img alt="" height="45" src="image/Formula_B19153_07_043.png" width="420"/> is arbitrary: we’ll choose <img alt="" height="30" src="image/Formula_B19153_07_044.png" width="19"/> in <span class="No-Break">this example:</span></p>
<pre class="source-code">
W = np.random.uniform(-1, 1, (2, 4))
array([[-0.9595632,  0.6652396,  0.556313 ,  0.740024 ],
       [ 0.9572366,  0.5983171, -0.0770412,  0.5610583]])</pre>
<ol>
<li value="4">This<a id="_idIndexMarker395"/> attention matrix is applied<a id="_idIndexMarker396"/> to the concatenation of hidden vectors to produce a unique value. Thus, its size needs to <span class="No-Break">be <img alt="" height="46" src="image/Formula_B19153_07_045.png" width="251"/>:</span><pre class="source-code">
W_att = np.random.uniform(-1, 1, (1, 4))
array([[-0.7634511,  0.2798420, -0.7132934,  0.8893378]])</pre></li>
<li>We want to concatenate hidden vectors from source and destination nodes. A simple way to obtain pairs of source and destination nodes is to look at our adjacency matrix <img alt="" height="40" src="image/Formula_B19153_07_046.png" width="29"/> in COO format: rows store source nodes, and columns store destination nodes. NumPy provides a quick and efficient way of doing it <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">np.where()</strong></span><span class="No-Break">:</span><pre class="source-code">
connections = np.where(A &gt; 0)
(array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3]),
 array([0, 1, 2, 3, 0, 1, 0, 2, 3, 0, 2, 3]))</pre></li>
<li>We can <a id="_idIndexMarker397"/>concatenate hidden <a id="_idIndexMarker398"/>vectors of source and destination nodes <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">np.concatenate</strong></span><span class="No-Break">:</span><pre class="source-code">
np.concatenate([(X @ W.T)[connections[0]], (X @ W.T)[connections[1]]], axis=1)
array([[ 0.3733923,  0.3854852,  0.3733923,  0.3854852],
       [ 0.3733923,  0.3854852,  0.8510261,  0.4776527],
       [ 0.3733923,  0.3854852, -0.6775590,  0.7356658],
       [ 0.3733923,  0.3854852, -0.6526841,  0.2423597],
       [ 0.8510261,  0.4776527,  0.3733923,  0.3854852],
       [ 0.8510261,  0.4776527,  0.8510261,  0.4776527],
       [-0.6775590,  0.7356658,  0.3733923,  0.3854852],
       [-0.6775590,  0.7356658, -0.6775590,  0.7356658],
       [-0.6775590,  0.7356658, -0.6526841,  0.2423597],
       [-0.6526841,  0.2423597,  0.3733923,  0.3854852],
       [-0.6526841,  0.2423597, -0.6775590,  0.7356658],
       [-0.6526841,  0.2423597, -0.6526841,  0.2423597]])</pre></li>
<li>We then<a id="_idIndexMarker399"/> apply a linear transformation<a id="_idIndexMarker400"/> to this result with the attention <span class="No-Break">matrix <img alt="" height="40" src="image/Formula_B19153_07_047.png" width="74"/>:</span><pre class="source-code">
a = W_att @ np.concatenate([(X @ W.T)[connections[0]], (X @ W.T)[connections[1]]], axis=1).T
array([[-0.1007035 , -0.35942847,  0.96036209,  0.50390318, -0.43956122, -0.69828618,  0.79964181,  1.8607074 ,  1.40424849,  0.64260322, 1.70366881,  1.2472099 ]])</pre></li>
<li>The second step consists of applying a Leaky ReLU function to the <span class="No-Break">previous outcome:</span><pre class="source-code">
def leaky_relu(x, alpha=0.2):
    return np.maximum(alpha*x, x)
e = leaky_relu(a)
array([[-0.0201407 , -0.07188569,  0.96036209,  0.50390318, -0.08791224,  -0.13965724,  0.79964181,  1.8607074 ,  1.40424849,  0.64260322,  1.70366881,  1.2472099 ]])</pre></li>
<li>We have the right values but need to place them correctly in a matrix. This matrix should look like <img alt="" height="41" src="image/Formula_B19153_07_048.png" width="29"/> because there is no need for unnormalized attention scores when there is no connection between two nodes. To build this matrix, we know the sources <img alt="" height="27" src="image/Formula_B19153_07_049.png" width="11"/> and destinations <img alt="" height="34" src="image/Formula_B19153_07_050.png" width="17"/> thanks to <strong class="source-inline">connections</strong>. So, the first value in <strong class="source-inline">e</strong> corresponds to <img alt="" height="31" src="image/Formula_B19153_07_051.png" width="50"/>, the second value to <img alt="" height="32" src="image/Formula_B19153_07_052.png" width="51"/>, but the seventh value corresponds <a id="_idIndexMarker401"/>to <img alt="" height="30" src="image/Formula_B19153_07_053.png" width="50"/> and not to <img alt="" height="31" src="image/Formula_B19153_07_054.png" width="51"/>. We <a id="_idIndexMarker402"/>can fill the matrix <span class="No-Break">as follows:</span><pre class="source-code">
E = np.zeros(A.shape)
E[connections[0], connections[1]] = e[0]
array([[-0.020140 , -0.0718856,  0.9603620,  0.5039031],
       [-0.0879122, -0.1396572,  0.       ,  0.       ],
       [ 0.7996418,  0.       ,  1.8607074,  1.4042484],
       [ 0.6426032,  0.       ,  1.7036688,  1.247209 ]])</pre></li>
<li>The next step is to normalize every row of attention scores. This requires a custom <strong class="source-inline">softmax</strong> function to produce our final <span class="No-Break">attention scores:</span><pre class="source-code">
def softmax2D(x, axis):
    e = np.exp(x - np.expand_dims(np.max(x, axis=axis), axis))
    sum = np.expand_dims(np.sum(e, axis=axis), axis)
    return e / sum
W_alpha = softmax2D(E, 1)
array([[0.15862414, 0.15062488, 0.42285965, 0.26789133],
       [0.24193418, 0.22973368, 0.26416607, 0.26416607],
       [0.16208847, 0.07285714, 0.46834625, 0.29670814],
       [0.16010498, 0.08420266, 0.46261506, 0.2930773 ]])</pre></li>
<li>This <a id="_idIndexMarker403"/>attention matrix <img alt="" height="40" src="image/Formula_B19153_07_055.png" width="49"/> provides <a id="_idIndexMarker404"/>weights for every possible connection in the network. We can use it to calculate our matrix of embeddings <img alt="" height="34" src="image/Formula_B19153_07_056.png" width="34"/>, which should give us two-dimensional vectors for <span class="No-Break">each node:</span><pre class="source-code">
H = A.T @ W_alpha @ X @ W.T
array([[-1.10126376,  1.99749693],
       [-0.33950544,  0.97045933],
       [-1.03570438,  1.53614075],
       [-1.03570438,  1.53614075]])</pre></li>
</ol>
<p>Our graph<a id="_idIndexMarker405"/> attention layer is now <a id="_idIndexMarker406"/>complete! Adding multi-head attention consists of repeating these steps with different <img alt="" height="33" src="image/Formula_B19153_07_057.png" width="44"/> and <img alt="" height="45" src="image/Formula_B19153_07_058.png" width="90"/> before aggregating <span class="No-Break">the results.</span></p>
<p>The graph attention operator is an essential building block to developing GNNs. In the next section, we will use a PyG implementation to create <span class="No-Break">a GAT.</span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor093"/>Implementing a GAT in PyTorch Geometric</h1>
<p>We now <a id="_idIndexMarker407"/>have a complete picture of how the graph attention<a id="_idIndexMarker408"/> layer works. These layers can be stacked to create our new architecture of choice: the GAT. In this section, we will follow the guidelines from the original GAT paper to implement our own model using PyG. We will use it to perform node classification on the <strong class="source-inline">Cora</strong> and <strong class="source-inline">CiteSeer</strong> datasets. Finally, we will comment on these results and <span class="No-Break">compare them.</span></p>
<p>Let’s start with the <span class="No-Break"><strong class="source-inline">Cora</strong></span><span class="No-Break"> dataset:</span></p>
<ol>
<li>We import <strong class="source-inline">Cora</strong> from the <strong class="source-inline">Planetoid</strong> class <span class="No-Break">using PyG:</span><pre class="source-code">
from torch_geometric.datasets import Planetoid
dataset = Planetoid(root=".", name="Cora")
data = dataset[0]
Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])</pre></li>
<li>We <a id="_idIndexMarker409"/>import the necessary libraries to <a id="_idIndexMarker410"/>create our own GAT class, using the <span class="No-Break">GATv2 layer:</span><pre class="source-code">
import torch
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv
from torch.nn import Linear, Dropout</pre></li>
<li>We implement the <strong class="source-inline">accuracy()</strong> function to evaluate the performance of <span class="No-Break">our model:</span><pre class="source-code">
def accuracy(y_pred, y_true):
    return torch.sum(y_pred == y_true) / len(y_true)</pre></li>
<li>The class is initialized with two improved graph attention layers. Note it is important to declare the number of heads used for multi-head attention. The authors stated that eight heads improved performance for the first layer, but it didn’t make any difference for the <span class="No-Break">second one:</span><pre class="source-code">
class GAT(torch.nn.Module):
    def __init__(self, dim_in, dim_h, dim_out, heads=8):
        super().__init__()
        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)
        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)</pre></li>
<li>Compared to the previous implementation of a GCN, we’re adding two dropout layers to prevent overfitting. These layers randomly zero some values from the input tensor with a predefined probability (<strong class="source-inline">0.6</strong> in this case). Conforming to the original paper, we also use<a id="_idIndexMarker411"/> the <strong class="bold">Exponential Linear Unit</strong> (<strong class="bold">ELU</strong>) function, which is the <a id="_idIndexMarker412"/>exponential <a id="_idIndexMarker413"/>version of the <span class="No-Break">Leaky ReLU:</span><pre class="source-code">
    def forward(self, x, edge_index):
        h = F.dropout(x, p=0.6, training=self.training)
        h = self.gat1(h, edge_index)
        h = F.elu(h)
        h = F.dropout(h, p=0.6, training=self.training)
        h = self.gat2(h, edge_index)
        return F.log_softmax(h, dim=1)</pre></li>
<li>The <strong class="source-inline">fit()</strong> function is identical to the GCN’s. The parameters of the Adam optimizer have been tuned to match the best values for the <strong class="source-inline">Cora</strong> dataset, according to <span class="No-Break">the authors:</span><pre class="source-code">
    def fit(self, data, epochs):
        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.01)
        self.train()
        for epoch in range(epochs+1):
            optimizer.zero_grad()
            out = self(data.x, data.edge_index)
            loss = criterion(out[data.train_mask], data.y[data.train_mask])
            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])
            loss.backward()
            optimizer.step()
            if(epoch % 20 == 0):
                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])
                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])
                print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:&gt;5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')</pre></li>
<li>The <strong class="source-inline">test()</strong> function<a id="_idIndexMarker414"/> is exactly<a id="_idIndexMarker415"/> <span class="No-Break">the same:</span><pre class="source-code">
    @torch.no_grad()
    def test(self, data):
        self.eval()
        out = self(data.x, data.edge_index)
        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])
        return acc</pre></li>
<li>We create a GAT and train it for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
gat = GAT(dataset.num_features, 32, dataset.num_classes)
gat.fit(data, epochs=100)
GAT(
  (gat1): GATv2Conv(1433, 32, heads=8)
  (gat2): GATv2Conv(256, 7, heads=1)
)
Epoch 0 | Train Loss: 1.978 | Train Acc: 12.86% | Val Loss: 1.94 | Val Acc: 13.80%
Epoch 20 | Train Loss: 0.238 | Train Acc: 96.43% | Val Loss: 1.04 | Val Acc: 67.40%
Epoch 40 | Train Loss: 0.165 | Train Acc: 98.57% | Val Loss: 0.95 | Val Acc: 71.00%
Epoch 60 | Train Loss: 0.209 | Train Acc: 96.43% | Val Loss: 0.91 | Val Acc: 71.80%
Epoch 80 | Train Loss: 0.172 | Train Acc: 100.00% | Val Loss: 0.93 | Val Acc: 70.80%
Epoch 100 | Train Loss: 0.190 | Train Acc: 97.86% | Val Loss: 0.96 | Val Acc: 70.80%</pre></li>
<li>This <a id="_idIndexMarker416"/>outputs the final <a id="_idIndexMarker417"/><span class="No-Break">test accuracy:</span><pre class="source-code">
acc = gat.test(data)
print(f'GAT test accuracy: {acc*100:.2f}%')
GAT test accuracy: 81.10%</pre></li>
</ol>
<p>This accuracy score is slightly better than the average score we obtained with a GCN. We’ll make a proper comparison after applying the GAT architecture to the <span class="No-Break">second dataset.</span></p>
<p>We will <a id="_idIndexMarker418"/>use a new <a id="_idIndexMarker419"/>popular dataset for node classification called <strong class="source-inline">CiteSeer</strong> (MIT License). Like <strong class="source-inline">Cora</strong>, it represents a network of research papers where each connection is a citation. <strong class="source-inline">CiteSeer</strong> involves <strong class="source-inline">3327</strong> nodes, whose features represent the presence (1) or absence (0) of <strong class="source-inline">3703</strong> words in a paper. The goal of this dataset is to correctly classify these nodes into six categories. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> shows a plot of <strong class="source-inline">CiteSeer</strong> made with <span class="No-Break">yEd Live:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer324">
<img alt="Figure 7.4 – The CiteSeer dataset (made with yEd Live)" height="932" src="image/B19153_07_004.jpg" width="1002"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – The CiteSeer dataset (made with yEd Live)</p>
<p>Compared to <strong class="source-inline">Cora</strong>, this <a id="_idIndexMarker420"/>dataset is larger in terms of the number of nodes (from 2,708 to 3,327) and also in terms of feature <a id="_idIndexMarker421"/>dimensionality (from 1,433 to 3,703). However, the exact same process can be applied <span class="No-Break">to it:</span></p>
<ol>
<li>First, we load the <span class="No-Break"><strong class="source-inline">CiteSeer</strong></span><span class="No-Break"> dataset:</span><pre class="source-code">
dataset = Planetoid(root=".", name="CiteSeer")
data = dataset[0]
Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])</pre></li>
<li>For good measure, we plot the number of nodes per node degree, using the code from the <span class="No-Break">last chapter:</span><pre class="source-code">
import matplotlib.pyplot as plt
from torch_geometric.utils import degree
from collections import Counter
degrees = degree(dataset[0].edge_index[0]).numpy()
numbers = Counter(degrees)
fig, ax = plt.subplots(dpi=300)
ax.set_xlabel('Node degree')
ax.set_ylabel('Number of nodes')
plt.bar(numbers.keys(), numbers.values())</pre></li>
<li>It gives us the <span class="No-Break">following output:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer325">
<img alt="Figure 7.5 – Number of nodes per node degree (CiteSeer)" height="667" src="image/B19153_07_005.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Number of nodes per node degree (CiteSeer)</p>
<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> looks like <a id="_idIndexMarker422"/>a typical heavy-tailed distribution but with a twist: some nodes have a degree of zero! In <a id="_idIndexMarker423"/>other words, they are not connected to any other node. We can assume that they will be much more difficult to classify than <span class="No-Break">the rest.</span></p>
<ol>
<li value="4">We initialize a new GAT model with the correct number of input and output nodes and train it for <span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break"> epochs:</span><pre class="source-code">
gat = GAT(dataset.num_features, 16, dataset.num_classes)
gat.fit(data, epochs=100)
Epoch   0 | Train Loss: 1.815 | Train Acc: 15.00% | Val Loss: 1.81 | Val Acc: 14.20%
Epoch  20 | Train Loss: 0.173 | Train Acc: 99.17% | Val Loss: 1.15 | Val Acc: 63.80%
Epoch  40 | Train Loss: 0.113 | Train Acc: 99.17% | Val Loss: 1.12 | Val Acc: 64.80%
Epoch  60 | Train Loss: 0.099 | Train Acc: 98.33% | Val Loss: 1.12 | Val Acc: 62.40%
Epoch  80 | Train Loss: 0.130 | Train Acc: 98.33% | Val Loss: 1.19 | Val Acc: 62.20%
Epoch 100 | Train Loss: 0.158 | Train Acc: 98.33% | Val Loss: 1.10 | Val Acc: 64.60%</pre></li>
<li>We obtain the following test <span class="No-Break">accuracy score:</span><pre class="source-code">
acc = gat.test(data)
print(f'GAT test accuracy: {acc*100:.2f}%')
GAT test accuracy: 68.10%</pre></li>
</ol>
<p>Is it a good result? This time, we have no point <span class="No-Break">of comparison.</span></p>
<p>According to<a id="_idIndexMarker424"/> Schur et al. in <em class="italic">Pitfalls of Graph Neural Network Evaluation</em>, the GAT is slightly better than the GCN (82.8% ± 0.6% versus 81.9% ± 0.8%) on <strong class="source-inline">Cora</strong> and <strong class="source-inline">CiteSeer</strong> (71.0 ± 0.6% versus 69.5% ± 0.9%). The authors also note that the accuracy scores are not normally<a id="_idIndexMarker425"/> distributed, making the usage of standard deviation less relevant. It is important to keep that in mind in this type <span class="No-Break">of benchmark.</span></p>
<p>Previously, I speculated that poorly connected nodes might negatively impact performance. We can verify this hypothesis by plotting the average accuracy score for each <span class="No-Break">node degree:</span></p>
<ol>
<li>We get the <span class="No-Break">model’s classifications:</span><pre class="source-code">
out = gat(data.x, data.edge_index)</pre></li>
<li>We calculate the degree of <span class="No-Break">each node:</span><pre class="source-code">
degrees = degree(data.edge_index[0]).numpy()</pre></li>
<li>We store the accuracy scores and <span class="No-Break">sample sizes:</span><pre class="source-code">
accuracies = []
sizes = []</pre></li>
<li>We get the average accuracy for each node degree between zero and five using a mask <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">np.where()</strong></span><span class="No-Break">:</span><pre class="source-code">
for i in range(0, 6):
    mask = np.where(degrees == i)[0]
    accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask]))
    sizes.append(len(mask))</pre></li>
<li>We repeat this process for every node with a degree higher <span class="No-Break">than five:</span><pre class="source-code">
mask = np.where(degrees &gt; 5)[0]
accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask]))
sizes.append(len(mask))</pre></li>
<li>We plot these <a id="_idIndexMarker426"/>accuracy scores <a id="_idIndexMarker427"/>with the corresponding <span class="No-Break">node degrees:</span><pre class="source-code">
fig, ax = plt.subplots(dpi=300)
ax.set_xlabel('Node degree')
ax.set_ylabel('Accuracy score')
plt.bar(['0','1','2','3','4','5','6+'], accuracies)
for i in range(0, 7):
    plt.text(i, accuracies[i], f'{accuracies[i]*100:.2f}%', ha='center', color='black')
for i in range(0, 7):
    plt.text(i, accuracies[i]//2, sizes[i], ha='center', color='white')</pre></li>
<li>It<a id="_idIndexMarker428"/> outputs<a id="_idIndexMarker429"/> the <span class="No-Break">following graph:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer326">
<img alt="Figure 7.6 – Accuracy score per node degree (CiteSeer)" height="784" src="image/B19153_07_006.jpg" width="1161"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Accuracy score per node degree (CiteSeer)</p>
<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> confirms our hypothesis: nodes with few neighbors are harder to classify correctly. Furthermore, it even shows that, in general, the higher the node degree, the better the accuracy score. This is quite natural because a higher number of neighbors will provide more information to the GNN to make <span class="No-Break">its predictions.</span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor094"/>Summary</h1>
<p>In this chapter, we introduced a new essential architecture: the GAT. We saw its inner workings with four main steps, from linear transformation to multi-head attention. We saw how it works in practice by implementing a graph attention layer in NumPy. Finally, we applied a GAT model (with GATv2) to the <strong class="source-inline">Cora</strong> and <strong class="source-inline">CiteSeer</strong> datasets, where it provided excellent accuracy scores. We showed that these scores were dependent on the number of neighbors, which is a first step toward <span class="No-Break">error analysis.</span></p>
<p>In <a href="B19153_08.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Scaling Graph Neural Networks with GraphSAGE</em>, we will introduce a new architecture dedicated to managing large graphs. To test this claim, we will implement it on a new dataset several times bigger than what we’ve seen so far. We will talk about transductive and inductive learning, which is an important distinction for <span class="No-Break">GNN practitioners.</span></p>
</div>
<div>
<div class="IMG---Figure" id="_idContainer328">
</div>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer329">
<h1 id="_idParaDest-91"><a id="_idTextAnchor095"/>Part 3: Advanced Techniques</h1>
<p>In this third part of the book, we will delve into the more advanced and specialized GNN architectures that have been developed to solve a variety of graph-related problems. We will cover state-of-the-art GNN models designed for specific tasks and domains, which can address challenges and requirements more effectively. In addition, we will provide an overview of several new graph-based tasks that can be tackled using GNNs, such as link prediction and graph classification, and demonstrate their applications through practical code examples <span class="No-Break">and implementations.</span></p>
<p>By the end of this part, you will be able to understand and implement advanced GNN architectures and apply them to solve your own graph-based problems. You will have a comprehensive understanding of specialized GNNs and their respective strengths, as well as hands-on experience with code examples. This knowledge will equip you with the skills to apply GNNs to real-world use cases and potentially contribute to the development of new and innovative <span class="No-Break">GNN architectures.</span></p>
<p>This part comprises the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B19153_08.xhtml#_idTextAnchor096"><em class="italic">Chapter 8</em></a><em class="italic">, Scaling Up Graph Neural Networks with GraphSAGE</em></li>
<li><a href="B19153_09.xhtml#_idTextAnchor106"><em class="italic">Chapter 9</em></a><em class="italic">, Defining Expressiveness for Graph Classification</em></li>
<li><a href="B19153_10.xhtml#_idTextAnchor116"><em class="italic">Chapter 10</em></a><em class="italic">, Predicting Links with Graph Neural Networks</em></li>
<li><a href="B19153_11.xhtml#_idTextAnchor131"><em class="italic">Chapter 11</em></a><em class="italic">, Generating Graphs Using Graph Neural Networks</em></li>
<li><a href="B19153_12.xhtml#_idTextAnchor144"><em class="italic">Chapter 12</em></a><em class="italic">, Learning from Heterogeneous Graphs</em></li>
<li><a href="B19153_13.xhtml#_idTextAnchor153"><em class="italic">Chapter 13</em></a><em class="italic">, Temporal Graph Neural Networks</em></li>
<li><a href="B19153_14.xhtml#_idTextAnchor165"><em class="italic">Chapter 14</em></a><em class="italic">, Explaining Graph Neural Networks</em></li>
</ul>
</div>
<div>
<div id="_idContainer330">
</div>
</div>
</div></body></html>