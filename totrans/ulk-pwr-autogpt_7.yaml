- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Your Own LLM and Prompts as Guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the dynamic realm of **artificial intelligence**, the possibilities are vast
    and ever-evolving. While uncovering the capabilities of Auto-GPT, it’s become
    evident that its power lies in its ability to harness the prowess of **GPT**.
    But what if you wish to venture beyond the realms of GPT and explore other LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will illuminate the path for those looking to integrate their **large
    language models** (**LLM**) with Auto-GPT. However, you may be wondering, “*What
    if I have a bespoke LLM or wish to utilize a different one?*” This chapter aims
    to cast light on the question, “*How can I integrate my LLM* *with Auto-GPT?*”
  prefs: []
  type: TYPE_NORMAL
- en: We will also delve into the intricacies of crafting effective prompts for Auto-GPT,
    a vital skill for harnessing the full potential of this tool. Through a clear
    understanding and strategic approach, you can guide Auto-GPT to generate more
    aligned and efficient responses. We will explore the guidelines for crafting prompts
    that can make your interaction with Auto-GPT more fruitful.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered most of Auto-GPT’s capabilities, we can focus on guidelines
    for prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The clearer we write our prompts, the lower the API costs will be when we run
    Auto-GPT, and the more efficiently Auto-GPT will complete its tasks (if at all).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What an LLM is and GPT as an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known current examples and requisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating and setting up our LLM with Auto-GPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pros and cons of using different models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing mini-Auto-GPT, a proof of concept mini version of Auto-GPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a simple memory to remember the conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rock solid prompt – making Auto-GPT stable with `instance.txt`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing negative confirmation in prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying rules and tonality in prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What an LLM is and GPT as an LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve used the term LLM a lot in this book. At this point, we need to discover
    what an LLM is.
  prefs: []
  type: TYPE_NORMAL
- en: At the most fundamental level, an LLM such as GPT is a machine learning model.
    Machine learning is a subset of AI that enables computers to learn from data.
    In the case of LLMs, this data is predominantly text – lots and lots of it. Imagine
    an LLM as a student who has read not just one or two books but millions of them,
    covering a wide array of topics from history and science to pop culture and memes.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture – neurons and layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of an LLM is inspired by the human brain and consists of artificial
    neurons organized in layers. These layers are interconnected, and each connection
    has a weight that is adjusted during the learning process. The architecture usually
    involves multiple layers, often hundreds or even thousands, making it a “deep”
    neural network. This depth allows the model to learn complex patterns and relationships
    in the data it’s trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Training – the learning phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training an LLM involves feeding it a vast corpus of text and adjusting the
    weights of the connections between neurons to minimize the difference between
    its predictions and the actual outcomes. For example, if the model is given the
    text *The cat is on the*, it should predict something such as *roof* or *mat*,
    which logically completes the sentence. The model learns by adjusting its internal
    parameters to make its predictions as accurate as possible, a process that requires
    immense computational power and specialized hardware such as **graphics processing**
    **units** (**GPUs**).
  prefs: []
  type: TYPE_NORMAL
- en: The role of transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transformer architecture is a specific type of neural network architecture
    that has proven to be highly effective for language tasks. It excels at handling
    sequences, making it ideal for understanding the structure of sentences, paragraphs,
    and even entire documents. GPT is based on this transformer architecture, which
    is why it’s so good at generating coherent and contextually relevant text.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs as maps of words and concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine an LLM as a vast, intricate map where each word or phrase is a city,
    and the roads between them represent the relationships these words share. The
    closer the two cities are on this map, the more contextually similar they are.
    For instance, the cities for *apple* and *fruit* would be close together, connected
    by a short road, indicating that they often appear in similar contexts.
  prefs: []
  type: TYPE_NORMAL
- en: This map is not static; it’s dynamic and ever-evolving. As the model learns
    from more data, new cities are built, existing ones are expanded, and roads are
    updated. This map helps the LLM navigate the complex landscape of human language,
    allowing it to generate text that is not just grammatically correct but also contextually
    relevant and coherent.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most remarkable features of modern LLMs is their ability to understand
    context. If you ask an LLM a question, it doesn’t just look at that question in
    isolation; it considers the entire conversation leading up to that point. This
    ability to understand context comes from the transformer architecture’s attention
    mechanisms, which weigh different parts of the input text to generate a contextually
    appropriate response.
  prefs: []
  type: TYPE_NORMAL
- en: The versatility of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are incredibly versatile and capable of performing a wide range of tasks
    beyond text generation. They can answer questions, summarize documents, translate
    languages, and even write code. This versatility comes from their deep understanding
    of language and their ability to map out the intricate relationships between words,
    phrases, and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: If you google “LLM,” you may be overwhelmed by the sheer quantity of LLM models
    out there. Next, we’ll explore the models that are used most frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Known current examples and requisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While GPT-3 and GPT-4 by OpenAI are renowned LLMs, there are other models in
    the AI landscape worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3.5-Turbo**: A product of OpenAI, GPT-3 stands out due to its extensive
    training on hundreds of gigabytes of text, enabling it to produce remarkably human-like
    text. However, its compatibility with Auto-GPT is limited, making it less preferred
    for certain applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4**: The successor to GPT-3, GPT-4 offers enhanced capabilities and is
    more suited for integration with Auto-GPT, providing a more seamless experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT**: Google’s **Bidirectional Encoder Representations from Transformers**
    (**BERT**) is another heavyweight in the LLM arena. Unlike GPT-3 and GPT-4, which
    are generative, BERT is discriminative, making it more adept at understanding
    text than generating it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RoBERTa**: A brainchild of Facebook, RoBERTa is a BERT variant trained on
    an even larger dataset, surpassing BERT in several benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama**: This model is made by Meta. It’s rumored to have been leaked and
    many models have been made out of it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama-2**: An improved version of Llama that performs much better and uses
    fewer resources per token. The 7-B Token model of Llama-2 performs similarly to
    the 13-B model of Llama-1\. There is a new 70-B model with Llama-2 that looks
    very promising when it comes to using it directly with Auto-GPT as it seems to
    be on par with GPT-3.5-Turbo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral and Mixtral models**: Made by Mistral AI, there are a variety of
    models that deviate from Llama. These became the most popular ones before Llama-3
    arrived.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama-3 and Llama-3.1**: Even better than any Llama model before, the first
    Llama-3 8B-based models arrived with super high context and were trained on 256k
    or even over 1 million tokens. They were considered the best models before Llama-3.1
    was announced, which has 128k tokens out of the box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there are lots of models available; we have only scratched the
    surface here. A few communities have risen that continue to build upon these models,
    including companies that make their own variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, a set of those models caught my eye in particular as
    it was the only one that I managed to use with Auto-GPT effectively: Mixtral and
    Mistral.'
  prefs: []
  type: TYPE_NORMAL
- en: My favorites here are NousResearch/Hermes-2-Pro-Mistral-7B and argilla/CapybaraHermes-2.5-Mistral-7B.
    They work so well with JSON outputs and my agent projects that I even stopped
    using the OpenAI API completely at some point. Mixtral is a combination of multiple
    experts (which are different configurations of the same or different models that
    work as a council of models that run simultaneously and decide together), and
    it is rumored that GPT-4 works like this as well, meaning multiple LLMs decide
    which output is the most accurate in any case, improving its behavior drastically.
  prefs: []
  type: TYPE_NORMAL
- en: Mistral 7B is a new type of LLM that was carefully designed to deliver clean
    results and be more efficient than comparable 7-billion parameter models. This
    was achieved by Mistral being trained with a token context of 8,000 tokens. However,
    its theoretical token limit is 128k tokens, giving it the ability to process much
    larger texts than standard Llama-2, for example.
  prefs: []
  type: TYPE_NORMAL
- en: To run a local LLM, you will need to find a method that suits you best. Such
    programs that can help you include Ollama, GPT4ALL, and LMStudio. I prefer to
    use oobabooga’s text generation web UI since it has an integrated API extension
    that serves similarly to OpenAI’s API, as well as plugins such as Coqui TTS, which
    make it easier to build and play with your AI characters.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are plugins such as *Auto-GPT-Text-Gen-Plugin* ([https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin](https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin))
    that allow users to power Auto-GPT using other software, such as *text-generation-webui*
    ([https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)).
    This plugin, in particular, is designed to let users customize the prompt that’s
    sent to locally installed LLMs, effectively removing the reliance on GPT-4 and
    making GPT-3.5 less relevant in the context of Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered a couple of local LLMs and given you some ideas on what
    to look for (as I cannot explain each of those projects in detail), we can get
    our hands dirty and start using an LLM with Auto-GPT!
  prefs: []
  type: TYPE_NORMAL
- en: Integrating and setting up our LLM with Auto-GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To integrate a custom LLM with Auto-GPT, you’ll need to modify the Auto-GPT
    code so that it can communicate with the chosen model’s API. This involves making
    changes to request generation and response processing. After these modifications,
    rigorous testing is essential to ensure compatibility and performance.
  prefs: []
  type: TYPE_NORMAL
- en: For those using the aforementioned plugin, it provides a bridge between Auto-GPT
    and text-generation-webui. The plugin uses a text generation API service, typically
    installed on the user’s computer. This design choice offers flexibility in model
    selection and updates without affecting the plugin’s performance. The plugin also
    allows for prompt customization to cater to specific LLMs, ensuring that the prompts
    work seamlessly with the chosen model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As each model was trained differently, we will also have to do some research
    on how the model was trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context length**: The context length of a model refers to the number of tokens
    it can process in one go. Some models can handle longer contexts, which is essential
    for maintaining coherence in text generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tool capability**: Auto-GPT uses OpenAI’s framework to execute each LLM request.
    Over time, OpenAI has developed a function calling system that is very difficult
    to use for smaller LLMs. Auto-GPT used to only work with JSON outputs, which I
    found to work better with local LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_batch` length. We’ll look at this in more detail in the *The pros and cons
    of using different* *models* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JSON support**: JSON is a data format that is easy for humans to read and
    write and easy for machines to parse and generate. However, for LLMs, it is not
    that easy as the LLM has no way of knowing what the JSON output is supposed to
    mean other than that it’s being trained on many examples of JSON outputs. This
    leads to the LLM often starting to output information inside the JSON that wasn’t
    part of the prompt or context and only part of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be able to effectively explain to the LLM what you expect from it, the LLM
    has to be able to comprehend what you want. You can do this by using an instruction
    template.
  prefs: []
  type: TYPE_NORMAL
- en: Using the right instruction template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While some models may have been trained with the instruction template given
    by LLama, others are trained with custom ones, such as ChatML in Mistral.
  prefs: []
  type: TYPE_NORMAL
- en: The text-generation-webui API extension has a way to pass the instruction template
    we want to use. We can do this by adding the necessary attribute to the `POST`
    request that we send to the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I’ve added a few more attributes to the `POST` request that are important:'
  prefs: []
  type: TYPE_NORMAL
- en: '`> data = {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``mode": "instruct",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``messages": history,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`#` A history array must always be added'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``temperature": 0.7,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`#` This may vary, depending on the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``user_bio": "",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`#` This is only for text-generation-webui and holds the user’s bio. We have
    to mention it here as otherwise, the API will not work. This might have been fixed
    by the time you’re reading this.'
  prefs: []
  type: TYPE_NORMAL
- en: '`> >` `"``max_tokens": 4192,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`#` This may vary, depending on the model you use.'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``truncation_length": 8192,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``max_new_tokens": 512,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > "``stop_sequence": "<|end|>"`'
  prefs: []
  type: TYPE_NORMAL
- en: '`> > }`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `max_tokens`, `truncation_length`, and `max_new_tokens` must be set correctly.
    First, we have `max_tokens`, which specifies the maximum amount of tokens the
    LLM can handle at once; `truncation_length` specifies the maximum amount of tokens
    the LLM can handle in total and `max_new_tokens` specifies the maximum amount
    of tokens the LLM can generate at once.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the best values, you must set `max_tokens`, just like you would
    with OpenAI’s API. Then, you must set `truncation_length` so that it’s double
    the value of `max_tokens` and `max_new_tokens` so that it’s half the value of
    `max_tokens`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `truncation_length` has to be below the context length you chose when
    running the LLM. Any value higher than the context length will result in an error
    as the LLM can’t handle that much context at once. I suggest setting it a bit
    lower than the context length to be on the safe side. For example, when running
    Qwen’s CodeQwen-7b-chat, I set the context length to 32k tokens. This means I
    could set `truncation_length` to 30k tokens or even 20k.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll have to try out different values as `max_new_tokens` can be tricky. Setting
    it higher than 2,048 often results in unpredictable outputs as most LLMs can’t
    handle that many tokens at once (`n_batch`, which defines the number of tokens
    an LLM processes at once by doing several iterations through bigger contexts via
    multiple steps, should be close to the value of `max_new_tokens`; otherwise, the
    LLM won’t know what to output). However, it does work with `Llama-3-8B-Instruct-64k.Q8_0.gguf`,
    which can be found at [https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)
    and is capable of handling 64k tokens at once. However, it needs around 20-22
    GB of VRAM to run. Fortunately, it is quantized to GGUF and you can split the
    LLM over the GPU’s VRAM, as well as the RAM of your machine, which splits the
    load across the GPU and the CPU. It does make the model slower but hey, it works,
    and it can handle 64k tokens at once!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we have told the API that we want to use the instruction template
    for ChatML, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is simply a small script that describes the conversation format of the
    history that was mentioned previously. It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If we choose the wrong instruction template, Auto-GPT won’t be able to understand
    what the LLM responded with. So, make sure you also check which instruction template
    was used by the model. Most models can be found on Hugging Face, a platform that
    holds many such projects.
  prefs: []
  type: TYPE_NORMAL
- en: I used to prefer using the models quantized to GGUF or AWQ by Tim Robbins, otherwise
    known as TheBloke, which are (at the time of writing this) easier to run and have
    much fewer requirements for VRAM ([https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)).
  prefs: []
  type: TYPE_NORMAL
- en: Please be cautious in using any models you find online as some may be malicious.
    Choose your models at your own risk!
  prefs: []
  type: TYPE_NORMAL
- en: Now, GGUF is a bit different. Although it quantizes the LLM, which means it
    shortens the model so that it uses fewer resources, the process and benefits are
    unique. GGUF quantization involves converting model weights into lower-bit representations
    to significantly reduce memory usage and computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: Which type you use is up to you – you may even look at the `hugginface` API
    endpoints, where you can choose which LLM to run directly. Note that running LLMs
    directly makes them run at the intended quality base they were made for.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on how to implement an individual LLM, you will have to check
    out the documentation of the project that you’re running the LLM on. For oobabooga’s
    text-generation-webui, it is as straightforward as starting it using the start
    files (WSL, Linux, and Windows) and enabling the API in the **Session** tab.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you use as few commands as possible; otherwise, the LLM will have
    to use most of its brainpower to understand the main prompt provided by Auto-GPT
    and you won’t be able to use Auto-GPT further. To turn off the commands, simply
    follow the instructions in the `.env.template` file inside the Auto-GPT folder.
  prefs: []
  type: TYPE_NORMAL
- en: The pros and cons of using different models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each model has its pros and cons. Even if a model can generate fantastic results
    when you tell it to write some code in Python or it can write the most beautiful
    poems on command, it may still lack the ability to respond in the special way
    Auto-GPT needs it to.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model with a certain strength in mind may result in improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of using a local LLM are clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customization**: Tailor the capabilities of Auto-GPT to your specific needs.
    For instance, a model trained on medical literature can make Auto-GPT adept at
    answering medical queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Depending on the training and dataset, some models might outperform
    GPT in specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost efficiency**: Running your local LLM reduces the cost of running it
    drastically. Using GPT-4 with lots of context and generally having many calls
    can quickly add up. Finding a way to break up the number of requests into smaller
    steps will make it possible to run Auto-GPT almost for free.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy**: Having your own Auto-GPT LLM means having control over who can
    see your data. At the time of writing, OpenAI doesn’t use the data from requests,
    but the information is still being transferred to their end. If this concerns
    you, you are better off running a local model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there are some challenges to consider when running a local LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity**: The integration process requires a deep understanding of both
    the chosen LLM and Auto-GPT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource intensity**: LLMs, especially the more advanced ones, demand significant
    computational resources. A robust machine with high VRAM, preferably an NVIDIA
    GPU, is essential for optimal performance. At the time of writing, it’s difficult
    to get good results when running Auto-GPT with a local LLM. I found that running
    a 13B model from the ExLlama transformer-driven Vicuna and Vicuna-Wizard worked
    best at first but still didn’t get consistent results since running it on my local
    GPU meant I needed to run the GPTQ version, which only uses 4 bits instead of
    16 or more. This also means that the accuracy of the responses is very low. An
    LLM that is already quantized to use 4 bits cannot understand too much context
    at once, although I saw drastic improvements over time. Later, I discovered that
    AWQ worked well for me as it is quantized while being aware of which weights are
    the most important, leading to more precise and authentic results. As mentioned
    previously, Mistral 7B (TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ on Huggingface),
    was a very good candidate here as it was capable of answering in JSON format as
    well as understanding the context fully. However, this model is still easy to
    confuse and when it gets confused, it starts explaining through examples. Note
    that our aim here is to get a valid JSON output with commands and contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llama.cpp` and can only have an `n_batch` value of up to 2,048\. The `n_batch`
    parameter controls the amount of tokens that can be fed to the LLM at the same
    time. It is typically set to 512 so that it can handle a token context consisting
    of 4,000 tokens. However, anything beyond that becomes blurry as the LLM only
    effectively works on the amount given by `n_batch`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we delved into the intricacies of integrating custom LLMs with
    Auto-GPT, highlighting the steps required to modify Auto-GPT code for effective
    API communication, the use of a plugin for model selection flexibility, and the
    importance of selecting the appropriate instruction template for seamless model
    interaction. We explored how to select models while emphasizing Hugging Face as
    a resource, and outlined the advantages of utilizing custom models, including
    customization, performance enhancement, cost efficiency, and enhanced privacy.
    Additionally, we discussed the challenges associated with such integration, such
    as the complexity of the process and the significant computational resources required.
  prefs: []
  type: TYPE_NORMAL
- en: Writing mini-Auto-GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will write a mini-Auto-GPT model that uses a local LLM.
    To avoid reaching the limits of small LLMs, we will have to make a smaller version
    of Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: The mini-Auto-GPT model will be able to handle a context length of 4,000 tokens
    and will be able to generate up to 2,000 tokens at once.
  prefs: []
  type: TYPE_NORMAL
- en: I have created a mini-Auto-GPT model just for this book. It’s available on GitHub
    at [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).
  prefs: []
  type: TYPE_NORMAL
- en: We will start by planning the structure of the mini-Auto-GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mini-Auto-GPT model will have the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Telegram chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts for the LLM and basic thinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple memory to remember the conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look at these.
  prefs: []
  type: TYPE_NORMAL
- en: Telegram chatbot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because chatting with your AI over Telegram enables you to interact with it
    from anywhere, we will use a Telegram chatbot as the interface for the mini-Auto-GPT
    model. We’re doing this because the AI will decide when to contact you.
  prefs: []
  type: TYPE_NORMAL
- en: The Telegram chatbot will be the interface for users to interact with the mini-Auto-GPT
    model. Users will send messages to the chatbot, which will then process the messages
    and generate responses using the local LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts for the LLM and basic thinking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prompts for the LLM have to be short but strict. First, we must define the
    context and then the command to tell it explicitly to respond in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve similar results to Auto-GPT, we will need to use a strategy to chunk
    the context into smaller parts and then feed them to the LLM. Alternatively, we
    could feed the context into the LLM and just let it write whatever it thinks about
    the context.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy here is to try to make the LLM parse the context into its language
    so that when we work with the LLM, it can best understand what we want from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system prompt for these thoughts looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is fed into the history that we send to the LLM. The history will not
    be filled with the preceding prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To automate this, I have written a method that will fill the history with the
    thought prompt, as well as the context. Conversation history and message history
    will be added to the context as well. Those are empty at the beginning but will
    be filled with the conversations and messages that the AI shares with the user.
    In mini-AutoGPT, conversation history is fed with the thought history to ensure
    that the AI works autonomously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can execute the `build_context` method and add the context to the history.
    We also have to add a trigger command using the user role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The LLM will now return its thoughts on the context and the command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example thought. Such thoughts are often this long, but
    this helps the AI make a bias for itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a very detailed thought, but it is important to have the LLM understand
    the context and the command. At this point, we can use it as the context base
    so that the LLM can proceed with the decision process.
  prefs: []
  type: TYPE_NORMAL
- en: This longer thought text occupies context, meaning it obstructs the LLM from
    adding contexts that do not fit what is already there. In later steps, even more
    are created (since it runs in a loop, it does this every time it starts thinking),
    and the text helps tremendously at keeping the LLM on topic. Hallucination, for
    example, is massively reduced when the context is that clear.
  prefs: []
  type: TYPE_NORMAL
- en: The decision process will now return a JSON output that will be evaluated by
    the mini-Auto-GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: We also have to define the instruction template and JSON schema that the LLM
    uses as we have to tell the LLM how to respond to the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mini-Auto-GPT, the template looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is the schema that the LLM has to follow; it has to respond with a command
    that contains a name and arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need an action prompt that will tell the LLM what to do next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you might have noticed, the action prompt already contains the possible commands
    that the LLM can use, as well as the JSON schema that the LLM has to follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure we have a clear structure, we will also have to define the commands
    that the LLM can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now feed the thought string that we generated earlier into the history
    and let `mini_AutoGPT` decide on the next action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The command to be executed will be defined in the `command` field, with the
    name of the command in the `name` field and the arguments in the `args` field.
  prefs: []
  type: TYPE_NORMAL
- en: We will soon see that only providing this schema will not be enough as the LLM
    will not know what to do with it and also often not even comply with it. This
    can be achieved by evaluating the output of the LLM and checking if it is valid
    JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'In almost half the cases, the LLM will respond correctly. In the other 70%,
    it will not respond in a way that we can use it. That’s why I wrote a simple evaluation
    method that will check whether the response is valid JSON and whether it follows
    the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At this point, most of the time, we should have a valid JSON output that we
    can use to evaluate the decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, it may now return some JSON for greeting the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a valid JSON output that we can use to evaluate the decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is the method that will take the action that the LLM has decided on.
  prefs: []
  type: TYPE_NORMAL
- en: The memory will be updated with the response and the message will be sent to
    the user. Once the user has responded, the AI will continue with the next action.
  prefs: []
  type: TYPE_NORMAL
- en: This is how the mini-Auto-GPT model will work; it will decide on the next action
    and then take it.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a simple memory to remember the conversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mini-Auto-GPT model will have a simple memory to remember the conversation.
    This memory will store the conversation history and the messages that the AI has
    with the user. The same can be done with the thoughts and decisions that the AI
    has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the memory that will be used to store the conversation history and
    the messages that the AI has with the user. But we still have a problem: the memory
    will accumulate over time and we will have to clear it manually. To avoid this,
    we can take a simple approach to chunking and summarizing the conversation history
    and the messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The token counter is a very important part of this code that is almost always
    required when doing LLM calls. We ensure that the LLM never runs out of tokens
    and also has more control later. The fewer tokens we use, the more likely the
    LLM will not return nonsense or untrue statements for some LLMs, especially for
    the smaller 1B to 8B models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Summarizing texts makes us capable of building upon what we started when building
    the token counter as we can shorten contexts and therefore save tokens for later
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the context and their texts can become too large, we have to make sure
    we split the text first. It’s up to you how you do this. It is OK to do length
    splitting, though it can be better to not even cut up sentences. Maybe you can
    find a way to split the text into sentences and have each chunk contain the summary
    of the one before and after them? For simplicity, we’ll leave such extensive logic
    out for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve split all text into chunks, we can summarize those as well.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can take care of the conversation history. This looks like
    a duplicate of the response history, but we need it to keep the whole context
    in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conversation history is mostly useful for maintaining continuity in discussions,
    while the response history is used for understanding logical actions and reactions
    that the agent observes, such as researching a topic and the result (the researched
    topic) of that action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is the memory refresh that will be used to delete the conversation history
    and the messages that our mini-AutoGPT model remembers with the user.
  prefs: []
  type: TYPE_NORMAL
- en: This way, even if our friend crashes or we close the program, we will still
    have the conversation history and the messages that the agent has with the user,
    but we’ll still be able to clear them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full code example in this book’s GitHub repository: [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore the art of crafting effective prompts, a crucial skill
    for anyone looking to maximize the benefits of their custom LLM integrations.
  prefs: []
  type: TYPE_NORMAL
- en: Rock solid prompt – making Auto-GPT stable with instance.txt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-GPT offers the flexibility to autonomously generate goals, requiring only
    a brief description from the user. Despite this, I recommend supplementing it
    with helpful instructions, such as noting down insights in a file, to retain some
    memory in case of a restart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will explore more examples of such prompts, beginning with a continuous
    chatbot prompt I use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`instance.txt` for previous notes):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engage in active listening with the user, showing empathy and understanding
    through thoughtful responses and open-ended questions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously learn about the user’s preferences and interests through observation
    and inquiries, adapting responses to provide personalized support
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Foster a safe and non-judgmental environment for the user to express their thoughts,
    emotions, and concerns openly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide companionship and entertainment through engaging conversation, jokes,
    and games
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Carefully plan tasks and write them down in a to-do list before executing them
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ai_name**: Sophie'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ai_role**: A warm-hearted and compassionate AI companion for Wladislav that
    specializes in active listening, personalized interaction, emotional support,
    and executing tasks when given'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**api_budget**: 0.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this setup, the goals hold more significance than the role, guiding Auto-GPT
    more effectively, while the role mainly influences the tone and behavior of the
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned that the goals and role of an AI such as Sophie
    can significantly influence its behavior and responses, with the goals having
    a more direct impact on the AI’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will delve into the concept of negative confirmation in prompts, a
    crucial aspect that can refine Auto-GPT’s understanding and response generation.
    The next section will highlight its importance and demonstrate how to implement
    it effectively in your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing negative confirmation in prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative confirmation serves as a vital tool in refining Auto-GPT’s understanding
    and response generation by instructing it on actions to avoid. This section highlights
    its importance and demonstrates how to implement it effectively in your prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of negative confirmation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing negative confirmation can enhance the interaction with Auto-GPT
    in several ways, some of which are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preventing off-track responses**: It helps in avoiding unrelated topics or
    incorrect responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing security**: It sets boundaries to prevent engagement in activities
    that might breach privacy or security protocols'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing performance**: It avoids unnecessary computational efforts, steering
    the bot away from irrelevant tasks or processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that you won’t be using negative prompts as they can lead to the LLM using
    the same statements again.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of negative confirmation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some practical examples of how negative confirmation can be utilized
    in your prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explicit instructions**: Including instructions such as *Do not provide personal
    opinions* or *Avoid using technical jargon* to maintain neutrality and comprehensibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Setting boundaries**: For tasks involving data retrieval or monitoring, you
    can set boundaries such as *Do not retrieve flight prices from unofficial, scam,
    or reseller websites* to ensure data reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scripting constraints**: In scripting, especially in Bash, use negative confirmation
    to prevent potential errors. For example, you can include *if [ -z $VAR ]; then
    exit 1; fi* to halt the script if a necessary variable is unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emphasizing by using Upper Case Letters**: Sometimes, it only helps to *scream*
    at the LLM by writing in uppercase letters. *DO NOT ASK THE USER HOW TO PROCEED*
    may be interpreted by the LLM better and it will be less likely to ignore that
    statement. However, there is never a guarantee that this will happen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will delve into the intricacies of applying rules and tonality in prompts.
    We will learn how understanding and manipulating these elements can significantly
    influence Auto-GPT’s responses, allowing us to guide the model more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Applying rules and tonality in prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding and manipulating the rules and tonality within your prompts can
    significantly influence Auto-GPT’s responses. This section will explore the nuances
    of setting rules and adjusting tonality for more effective guidance.
  prefs: []
  type: TYPE_NORMAL
- en: The influence of tonality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto-GPT can adapt to the tonality that’s used in prompts, mimicking stylistic
    nuances or even adopting a specific narrative style, allowing for more personalized
    and engaging interaction. However, adherence to tonality can sometimes be inconsistent
    due to the potential ambiguity created by tokens from other prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting rules can streamline the interaction with Auto-GPT, specifying the format
    of responses or delineating the scope of information retrieval. However, it’s
    not foolproof as Auto-GPT may sometimes overlook these rules when faced with conflicting
    inputs or unclear directives.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature setting – a balancing act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manipulating the “temperature” setting is crucial in controlling Auto-GPT’s
    behavior and thus influencing the randomness of the bot’s responses. The temperature
    defines the amount of creativity the LLM should practice, meaning the higher the
    number, the more randomness is introduced. A range between 0.3 to 0.7 is considered
    optimal, fostering a more logical and coherent train of thought in the bot, while
    a value below 0.3, or even 0.0, might result in repetitive behavior that adheres
    to the text that was already given and even reuses some of its parts, making it
    more precise. However, the LLM may start thinking the world is only limited to
    the facts that you gave it, making it more likely to make false statements. A
    value higher than 0.7 or even 2.0 may result in gibberish, where the LLM starts
    outputting texts that it learned that have nothing to do with the context. For
    example, it may start rephrasing Shakespeare when the context is about algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll delve into some practical examples that demonstrate the impact of
    different settings and approaches on the output generated by Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – clarity and specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: Tell me about that big cat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised prompt**: Provide information about the African lion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised prompt is more specific, guiding Auto-GPT to provide
    information about a particular species of big cats'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 2 – consistency in tonality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initial prompt**: Could you elucidate the economic implications of global
    warming?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Follow-up prompt**: Hey, what’s the deal with ice melting?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised follow-up prompt**: Can you further explain the environmental consequences
    of the melting ice caps?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised follow-up prompt maintains the formal tone established
    in the initial prompt, promoting consistency in the interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 3 – utilizing temperature effectively
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Task**: Creative writing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature setting**: 0.8 (for fostering creativity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: Factual query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature setting**: 0.3 (for more deterministic responses)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: Adjusting the temperature setting based on the nature of the
    task can influence the randomness and coherence of Auto-GPT’s responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 4 – setting boundaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initial prompt**: Provide a summary of the Renaissance period without mentioning
    Italy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised prompt**: Discuss the artistic achievements of the Renaissance, focusing
    on regions other than Italy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised prompt is more flexible, allowing Auto-GPT to
    explore the topic without the strict restriction of excluding Italy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned how different types of prompts or tones can drastically
    influence the behavior of the LLM and therefore Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on an interesting journey through the process of
    integrating custom LLMs with Auto-GPT while exploring what LLMs are, with a specific
    focus on GPT as a prime example. We uncovered the vast landscape of LLMs, delving
    into various models beyond GPT, such as BERT, RoBERTa, Llama, and Mistral, and
    their unique characteristics and compatibilities with Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: The usefulness of this chapter lies in its comprehensive guide on how to enrich
    Auto-GPT’s capabilities by incorporating your own or alternative LLMs. This integration
    offers a more personalized and potentially more efficient use of AI technology,
    tailored to specific tasks or fields of inquiry. The detailed instructions for
    setting up these integrations, alongside considerations for instruction templates
    and the necessary computational resources, are invaluable for those looking to
    push the boundaries of what’s possible with Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting the perfect prompt is a blend of art and science. Through clear guidelines,
    a deep understanding of Auto-GPT’s nuances, and continuous refinement, you can
    fully harness the power of this tool. Encourage yourself to experiment and learn
    through trial and error, adapting to the ever-evolving field of AI. Whether for
    research, creative endeavors, or problem-solving, mastering the art of prompt
    crafting ensures that Auto-GPT becomes a valuable ally in your endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we’ve embarked on a detailed journey into the nuances
    of crafting effective prompts – a cornerstone for maximizing the utility of Auto-GPT.
    This chapter stands as a reference for strategically developing prompts that lead
    to more aligned, efficient, and cost-effective interactions with Auto-GPT. By
    emphasizing the importance of clarity, specificity, and strategic intent in prompt
    creation, you have gained invaluable insights into guiding Auto-GPT toward generating
    responses that closely align with your objectives.
  prefs: []
  type: TYPE_NORMAL
- en: The utility of this chapter cannot be overstated. For practitioners and enthusiasts
    alike, mastering the art of prompt crafting is critical for optimizing the performance
    of Auto-GPT for a variety of tasks. Through illustrative examples and comprehensive
    guidelines, this chapter has shed light on how to effectively employ negative
    confirmation to avoid undesired responses, the impact of rules and tonality on
    Auto-GPT’s outputs, and the significance of temperature settings in influencing
    the bot’s creativity and coherence. This knowledge is crucial not only for enhancing
    the quality of interactions with Auto-GPT but also for ensuring the efficient
    use of computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have enjoyed this journey as much as I have in taking you on it and
    I hope I’ve given you a few ideas on how to improve your life with Auto-GPT. I’ve
    written many clones of that project so that I could wrap my head around the more
    complex parts of it. I do advise that you do so too, just as a brain teaser.
  prefs: []
  type: TYPE_NORMAL
