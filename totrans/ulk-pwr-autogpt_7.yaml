- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Using Your Own LLM and Prompts as Guidelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自己的LLM和提示词作为指南
- en: In the dynamic realm of **artificial intelligence**, the possibilities are vast
    and ever-evolving. While uncovering the capabilities of Auto-GPT, it’s become
    evident that its power lies in its ability to harness the prowess of **GPT**.
    But what if you wish to venture beyond the realms of GPT and explore other LLMs?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在**人工智能**的动态领域，可能性广阔且不断发展。在揭示Auto-GPT的能力时，显而易见，它的强大之处在于能够利用**GPT**的强大功能。但如果你希望超越GPT的范畴，探索其他LLM，该怎么办呢？
- en: This chapter will illuminate the path for those looking to integrate their **large
    language models** (**LLM**) with Auto-GPT. However, you may be wondering, “*What
    if I have a bespoke LLM or wish to utilize a different one?*” This chapter aims
    to cast light on the question, “*How can I integrate my LLM* *with Auto-GPT?*”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为那些希望将**大型语言模型**（**LLM**）与Auto-GPT集成的人们指引道路。然而，你可能会想，“*如果我有一个定制的LLM，或者希望使用其他LLM，该怎么办？*”
    本章旨在解答这个问题，“*如何将我的LLM* *与Auto-GPT集成？*”
- en: We will also delve into the intricacies of crafting effective prompts for Auto-GPT,
    a vital skill for harnessing the full potential of this tool. Through a clear
    understanding and strategic approach, you can guide Auto-GPT to generate more
    aligned and efficient responses. We will explore the guidelines for crafting prompts
    that can make your interaction with Auto-GPT more fruitful.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将深入探讨如何为Auto-GPT制定有效提示词的细节，这对充分利用这一工具至关重要。通过清晰的理解和策略性的方法，你可以引导Auto-GPT生成更符合需求和高效的回应。我们将探讨制定提示词的指南，帮助你与Auto-GPT的互动更加富有成效。
- en: Now that we have covered most of Auto-GPT’s capabilities, we can focus on guidelines
    for prompts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了Auto-GPT的大部分功能，我们可以专注于提示词的使用指南。
- en: The clearer we write our prompts, the lower the API costs will be when we run
    Auto-GPT, and the more efficiently Auto-GPT will complete its tasks (if at all).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写的提示词越清晰，当我们运行Auto-GPT时，API的费用就越低，Auto-GPT完成任务的效率就越高（如果能够完成的话）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What an LLM is and GPT as an LLM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM是什么以及GPT作为LLM的应用
- en: Known current examples and requisites
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知的当前示例和要求
- en: Integrating and setting up our LLM with Auto-GPT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的LLM与Auto-GPT进行集成和设置
- en: The pros and cons of using different models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同模型的优缺点
- en: Writing mini-Auto-GPT, a proof of concept mini version of Auto-GPT
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写迷你Auto-GPT，Auto-GPT的概念验证迷你版本
- en: Adding a simple memory to remember the conversation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加简单的记忆功能以记住对话
- en: Rock solid prompt – making Auto-GPT stable with `instance.txt`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的提示词——通过`instance.txt`使Auto-GPT稳定
- en: Implementing negative confirmation in prompts
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示词中实施负确认
- en: Applying rules and tonality in prompts
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示词中应用规则和语气
- en: What an LLM is and GPT as an LLM
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM是什么以及GPT作为LLM的应用
- en: We’ve used the term LLM a lot in this book. At this point, we need to discover
    what an LLM is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们多次提到了LLM。此时，我们需要了解LLM到底是什么。
- en: At the most fundamental level, an LLM such as GPT is a machine learning model.
    Machine learning is a subset of AI that enables computers to learn from data.
    In the case of LLMs, this data is predominantly text – lots and lots of it. Imagine
    an LLM as a student who has read not just one or two books but millions of them,
    covering a wide array of topics from history and science to pop culture and memes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从最基本的角度看，像GPT这样的LLM是一个机器学习模型。机器学习是人工智能的一个子集，使计算机能够从数据中学习。在LLM的情况下，这些数据主要是文本——大量的文本。可以将LLM看作是一个学生，他阅读的不仅仅是一本或两本书，而是数百万本书，涵盖了从历史、科学到流行文化和网络迷因等各种话题。
- en: The architecture – neurons and layers
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构——神经元与层级
- en: The architecture of an LLM is inspired by the human brain and consists of artificial
    neurons organized in layers. These layers are interconnected, and each connection
    has a weight that is adjusted during the learning process. The architecture usually
    involves multiple layers, often hundreds or even thousands, making it a “deep”
    neural network. This depth allows the model to learn complex patterns and relationships
    in the data it’s trained on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的架构灵感来自人类大脑，由按层组织的人工神经元组成。这些层是相互连接的，每个连接都有一个权重，这个权重在学习过程中会不断调整。该架构通常涉及多个层级，通常是数百甚至数千个层次，使其成为一个“深度”神经网络。这种深度使得模型能够学习复杂的数据模式和关系。
- en: Training – the learning phase
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练——学习阶段
- en: Training an LLM involves feeding it a vast corpus of text and adjusting the
    weights of the connections between neurons to minimize the difference between
    its predictions and the actual outcomes. For example, if the model is given the
    text *The cat is on the*, it should predict something such as *roof* or *mat*,
    which logically completes the sentence. The model learns by adjusting its internal
    parameters to make its predictions as accurate as possible, a process that requires
    immense computational power and specialized hardware such as **graphics processing**
    **units** (**GPUs**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The role of transformers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transformer architecture is a specific type of neural network architecture
    that has proven to be highly effective for language tasks. It excels at handling
    sequences, making it ideal for understanding the structure of sentences, paragraphs,
    and even entire documents. GPT is based on this transformer architecture, which
    is why it’s so good at generating coherent and contextually relevant text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: LLMs as maps of words and concepts
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine an LLM as a vast, intricate map where each word or phrase is a city,
    and the roads between them represent the relationships these words share. The
    closer the two cities are on this map, the more contextually similar they are.
    For instance, the cities for *apple* and *fruit* would be close together, connected
    by a short road, indicating that they often appear in similar contexts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: This map is not static; it’s dynamic and ever-evolving. As the model learns
    from more data, new cities are built, existing ones are expanded, and roads are
    updated. This map helps the LLM navigate the complex landscape of human language,
    allowing it to generate text that is not just grammatically correct but also contextually
    relevant and coherent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Contextual understanding
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most remarkable features of modern LLMs is their ability to understand
    context. If you ask an LLM a question, it doesn’t just look at that question in
    isolation; it considers the entire conversation leading up to that point. This
    ability to understand context comes from the transformer architecture’s attention
    mechanisms, which weigh different parts of the input text to generate a contextually
    appropriate response.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The versatility of LLMs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are incredibly versatile and capable of performing a wide range of tasks
    beyond text generation. They can answer questions, summarize documents, translate
    languages, and even write code. This versatility comes from their deep understanding
    of language and their ability to map out the intricate relationships between words,
    phrases, and concepts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: If you google “LLM,” you may be overwhelmed by the sheer quantity of LLM models
    out there. Next, we’ll explore the models that are used most frequently.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Known current examples and requisites
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While GPT-3 and GPT-4 by OpenAI are renowned LLMs, there are other models in
    the AI landscape worth noting:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3.5-Turbo**: A product of OpenAI, GPT-3 stands out due to its extensive
    training on hundreds of gigabytes of text, enabling it to produce remarkably human-like
    text. However, its compatibility with Auto-GPT is limited, making it less preferred
    for certain applications.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3.5-Turbo**：OpenAI 的产品，GPT-3 因其在数百 GB 的文本数据上进行深度训练而脱颖而出，能够生成极其接近人类的文本。然而，它与
    Auto-GPT 的兼容性有限，因此在某些应用中并不是首选。'
- en: '**GPT-4**: The successor to GPT-3, GPT-4 offers enhanced capabilities and is
    more suited for integration with Auto-GPT, providing a more seamless experience.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-4**：GPT-3 的继任者，GPT-4 提供了更强大的能力，更适合与 Auto-GPT 集成，提供更加流畅的体验。'
- en: '**BERT**: Google’s **Bidirectional Encoder Representations from Transformers**
    (**BERT**) is another heavyweight in the LLM arena. Unlike GPT-3 and GPT-4, which
    are generative, BERT is discriminative, making it more adept at understanding
    text than generating it.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT**：谷歌的 **双向编码器表示模型**（**BERT**）是 LLM 领域的另一位重量级选手。与 GPT-3 和 GPT-4 的生成式模型不同，BERT
    是判别式的，使得它在理解文本方面比生成文本更为擅长。'
- en: '**RoBERTa**: A brainchild of Facebook, RoBERTa is a BERT variant trained on
    an even larger dataset, surpassing BERT in several benchmarks.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoBERTa**：Facebook 的创新之作，RoBERTa 是 BERT 的变种，在一个更大的数据集上进行训练，在多个基准测试中超过了 BERT。'
- en: '**Llama**: This model is made by Meta. It’s rumored to have been leaked and
    many models have been made out of it.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama**：这个模型由 Meta 制作。传闻它曾被泄露，许多基于它的模型应运而生。'
- en: '**Llama-2**: An improved version of Llama that performs much better and uses
    fewer resources per token. The 7-B Token model of Llama-2 performs similarly to
    the 13-B model of Llama-1\. There is a new 70-B model with Llama-2 that looks
    very promising when it comes to using it directly with Auto-GPT as it seems to
    be on par with GPT-3.5-Turbo.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-2**：Llama 的改进版，性能更好，每个 token 的资源消耗更少。Llama-2 的 7-B Token 模型与 Llama-1
    的 13-B 模型表现相似。Llama-2 有一款新的 70-B 模型，看起来在直接与 Auto-GPT 配合使用时非常有前景，它似乎与 GPT-3.5-Turbo
    不相上下。'
- en: '**Mistral and Mixtral models**: Made by Mistral AI, there are a variety of
    models that deviate from Llama. These became the most popular ones before Llama-3
    arrived.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mistral 和 Mixtral 模型**：由 Mistral AI 制作，有多种模型不同于 Llama，这些模型在 Llama-3 发布之前非常流行。'
- en: '**Llama-3 and Llama-3.1**: Even better than any Llama model before, the first
    Llama-3 8B-based models arrived with super high context and were trained on 256k
    or even over 1 million tokens. They were considered the best models before Llama-3.1
    was announced, which has 128k tokens out of the box.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-3 和 Llama-3.1**：比之前的任何 Llama 模型都要更强大，第一个基于 Llama-3 8B 的模型以超高的上下文处理能力问世，并且在
    256k 或甚至超过 100 万个 tokens 上进行训练。在 Llama-3.1 发布之前，它们被认为是最好的模型，而 Llama-3.1 的原生支持
    128k tokens。'
- en: As you can see, there are lots of models available; we have only scratched the
    surface here. A few communities have risen that continue to build upon these models,
    including companies that make their own variations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，目前有许多模型可供选择；我们这里只是刚刚触及表面。几个社区已经崭露头角，继续在这些模型的基础上进行开发，包括一些公司也在制作自己的变种。
- en: 'As mentioned earlier, a set of those models caught my eye in particular as
    it was the only one that I managed to use with Auto-GPT effectively: Mixtral and
    Mistral.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有一组模型特别吸引了我的注意，因为它是我唯一能够有效与 Auto-GPT 配合使用的模型：Mixtral 和 Mistral。
- en: My favorites here are NousResearch/Hermes-2-Pro-Mistral-7B and argilla/CapybaraHermes-2.5-Mistral-7B.
    They work so well with JSON outputs and my agent projects that I even stopped
    using the OpenAI API completely at some point. Mixtral is a combination of multiple
    experts (which are different configurations of the same or different models that
    work as a council of models that run simultaneously and decide together), and
    it is rumored that GPT-4 works like this as well, meaning multiple LLMs decide
    which output is the most accurate in any case, improving its behavior drastically.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的模型是 NousResearch/Hermes-2-Pro-Mistral-7B 和 argilla/CapybaraHermes-2.5-Mistral-7B。它们与
    JSON 输出以及我的代理项目配合得非常好，甚至有一段时间我完全停止使用 OpenAI API。Mixtral 是多个专家模型的组合（这些专家模型是同一模型或不同模型的不同配置，它们作为一个模型委员会同时运行并共同做出决策），传闻
    GPT-4 也是如此运作的，这意味着多个 LLM 会共同决定哪个输出是最准确的，从而显著提高其表现。
- en: Mistral 7B is a new type of LLM that was carefully designed to deliver clean
    results and be more efficient than comparable 7-billion parameter models. This
    was achieved by Mistral being trained with a token context of 8,000 tokens. However,
    its theoretical token limit is 128k tokens, giving it the ability to process much
    larger texts than standard Llama-2, for example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B是一种新型的LLM，经过精心设计，能够提供更干净的结果，并且比同类的70亿参数模型更高效。Mistral通过使用8,000令牌的上下文进行训练，达到了这个目标。然而，它的理论令牌限制是128k令牌，这使得它能够处理比标准Llama-2更大的文本内容。
- en: To run a local LLM, you will need to find a method that suits you best. Such
    programs that can help you include Ollama, GPT4ALL, and LMStudio. I prefer to
    use oobabooga’s text generation web UI since it has an integrated API extension
    that serves similarly to OpenAI’s API, as well as plugins such as Coqui TTS, which
    make it easier to build and play with your AI characters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本地LLM，你需要找到最适合你的方法。一些可以帮助你的程序包括Ollama、GPT4ALL和LMStudio。我个人喜欢使用oobabooga的文本生成Web
    UI，因为它集成了类似OpenAI API的API扩展，并且有像Coqui TTS这样的插件，便于构建和玩转你的AI角色。
- en: Additionally, there are plugins such as *Auto-GPT-Text-Gen-Plugin* ([https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin](https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin))
    that allow users to power Auto-GPT using other software, such as *text-generation-webui*
    ([https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)).
    This plugin, in particular, is designed to let users customize the prompt that’s
    sent to locally installed LLMs, effectively removing the reliance on GPT-4 and
    making GPT-3.5 less relevant in the context of Auto-GPT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些插件，例如*Auto-GPT-Text-Gen-Plugin*（[https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin](https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin)），可以让用户通过其他软件为Auto-GPT提供支持，如*text-generation-webui*（[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)）。这个插件特别设计用来让用户自定义发送给本地安装的LLM的提示，从而有效地摆脱对GPT-4的依赖，并在Auto-GPT的使用环境下让GPT-3.5变得不那么重要。
- en: Now that we’ve covered a couple of local LLMs and given you some ideas on what
    to look for (as I cannot explain each of those projects in detail), we can get
    our hands dirty and start using an LLM with Auto-GPT!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些本地LLM，并给你提供了一些选择时需要注意的事项（由于无法详细解释每个项目的内容），接下来我们可以动手实践，开始使用带有Auto-GPT的LLM！
- en: Integrating and setting up our LLM with Auto-GPT
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLM与Auto-GPT集成和设置
- en: To integrate a custom LLM with Auto-GPT, you’ll need to modify the Auto-GPT
    code so that it can communicate with the chosen model’s API. This involves making
    changes to request generation and response processing. After these modifications,
    rigorous testing is essential to ensure compatibility and performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要将自定义LLM与Auto-GPT集成，你需要修改Auto-GPT代码，以便它能够与所选模型的API进行通信。这涉及到请求生成和响应处理的修改。完成这些修改后，进行严格的测试是确保兼容性和性能的关键。
- en: For those using the aforementioned plugin, it provides a bridge between Auto-GPT
    and text-generation-webui. The plugin uses a text generation API service, typically
    installed on the user’s computer. This design choice offers flexibility in model
    selection and updates without affecting the plugin’s performance. The plugin also
    allows for prompt customization to cater to specific LLMs, ensuring that the prompts
    work seamlessly with the chosen model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用上述插件的用户，它提供了Auto-GPT和text-generation-webui之间的桥梁。该插件使用一个文本生成API服务，通常安装在用户的计算机上。这种设计方式提供了在不影响插件性能的情况下选择和更新模型的灵活性。插件还允许定制提示，以适应特定的LLM，确保提示能够与所选模型无缝对接。
- en: 'As each model was trained differently, we will also have to do some research
    on how the model was trained:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型的训练方式不同，我们还需要进行一些研究，了解该模型是如何训练的：
- en: '**Context length**: The context length of a model refers to the number of tokens
    it can process in one go. Some models can handle longer contexts, which is essential
    for maintaining coherence in text generation.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文长度**：模型的上下文长度是指它一次可以处理的令牌数量。一些模型可以处理更长的上下文，这对于保持文本生成的一致性至关重要。'
- en: '**Tool capability**: Auto-GPT uses OpenAI’s framework to execute each LLM request.
    Over time, OpenAI has developed a function calling system that is very difficult
    to use for smaller LLMs. Auto-GPT used to only work with JSON outputs, which I
    found to work better with local LLMs.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具能力**：Auto-GPT使用OpenAI的框架来执行每个LLM请求。随着时间的推移，OpenAI开发了一个功能调用系统，对于较小的LLM来说，这个系统非常难以使用。Auto-GPT曾只与JSON输出兼容，而我发现这种方式在本地LLM上效果更好。'
- en: '`n_batch` length. We’ll look at this in more detail in the *The pros and cons
    of using different* *models* section.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_batch`长度。我们将在*使用不同模型的优缺点*部分详细探讨这个问题。'
- en: '**JSON support**: JSON is a data format that is easy for humans to read and
    write and easy for machines to parse and generate. However, for LLMs, it is not
    that easy as the LLM has no way of knowing what the JSON output is supposed to
    mean other than that it’s being trained on many examples of JSON outputs. This
    leads to the LLM often starting to output information inside the JSON that wasn’t
    part of the prompt or context and only part of the training data.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JSON支持**：JSON是一种易于人类阅读和编写，并且易于机器解析和生成的数据格式。然而，对于LLM来说，这并不容易，因为LLM无法知道JSON输出应该表示什么，除了它被训练在许多JSON输出示例上。这导致LLM经常开始在JSON内部输出一些并非提示或上下文的一部分的信息，而这些内容仅是训练数据的一部分。'
- en: To be able to effectively explain to the LLM what you expect from it, the LLM
    has to be able to comprehend what you want. You can do this by using an instruction
    template.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够有效地向LLM解释你期望它做什么，LLM必须能够理解你想要的内容。你可以通过使用指令模板来做到这一点。
- en: Using the right instruction template
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正确的指令模板
- en: While some models may have been trained with the instruction template given
    by LLama, others are trained with custom ones, such as ChatML in Mistral.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些模型可能已经使用LLama提供的指令模板进行训练，但其他模型则使用定制的模板，如Mistral中的ChatML。
- en: The text-generation-webui API extension has a way to pass the instruction template
    we want to use. We can do this by adding the necessary attribute to the `POST`
    request that we send to the API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: text-generation-webui API扩展提供了一种传递我们想要使用的指令模板的方法。我们可以通过向发送给API的`POST`请求添加必要的属性来做到这一点。
- en: 'Here, I’ve added a few more attributes to the `POST` request that are important:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我为`POST`请求添加了一些重要的属性：
- en: '`> data = {`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`> data = {`'
- en: '`> > "``mode": "instruct",`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``mode": "instruct",`'
- en: '`> > "``messages": history,`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``messages": history,`'
- en: '`#` A history array must always be added'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 始终需要添加一个历史数组'
- en: '`> > "``temperature": 0.7,`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``temperature": 0.7,`'
- en: '`#` This may vary, depending on the model.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这可能会有所不同，取决于所使用的模型。'
- en: '`> > "``user_bio": "",`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``user_bio": "",`'
- en: '`#` This is only for text-generation-webui and holds the user’s bio. We have
    to mention it here as otherwise, the API will not work. This might have been fixed
    by the time you’re reading this.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这是仅适用于text-generation-webui，并包含用户的个人信息。我们必须在这里提到它，否则API将无法正常工作。你阅读时这个问题可能已经修复。'
- en: '`> >` `"``max_tokens": 4192,`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`> >` `"``max_tokens": 4192,`'
- en: '`#` This may vary, depending on the model you use.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`#` 这可能会有所不同，取决于你使用的模型。'
- en: '`> > "``truncation_length": 8192,`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``truncation_length": 8192,`'
- en: '`> > "``max_new_tokens": 512,`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``max_new_tokens": 512,`'
- en: '`> > "``stop_sequence": "<|end|>"`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > "``stop_sequence": "<|end|>"`'
- en: '`> > }`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`> > }`'
- en: Here, `max_tokens`, `truncation_length`, and `max_new_tokens` must be set correctly.
    First, we have `max_tokens`, which specifies the maximum amount of tokens the
    LLM can handle at once; `truncation_length` specifies the maximum amount of tokens
    the LLM can handle in total and `max_new_tokens` specifies the maximum amount
    of tokens the LLM can generate at once.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`max_tokens`、`truncation_length`和`max_new_tokens`必须正确设置。首先是`max_tokens`，它指定LLM一次可以处理的最大token数量；`truncation_length`指定LLM可以处理的总token数量；`max_new_tokens`指定LLM一次可以生成的最大token数量。
- en: To calculate the best values, you must set `max_tokens`, just like you would
    with OpenAI’s API. Then, you must set `truncation_length` so that it’s double
    the value of `max_tokens` and `max_new_tokens` so that it’s half the value of
    `max_tokens`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算最佳值，必须设置`max_tokens`，就像在使用OpenAI的API时一样。然后，你需要设置`truncation_length`，使其是`max_tokens`的两倍，并设置`max_new_tokens`，使其是`max_tokens`的一半。
- en: Note that `truncation_length` has to be below the context length you chose when
    running the LLM. Any value higher than the context length will result in an error
    as the LLM can’t handle that much context at once. I suggest setting it a bit
    lower than the context length to be on the safe side. For example, when running
    Qwen’s CodeQwen-7b-chat, I set the context length to 32k tokens. This means I
    could set `truncation_length` to 30k tokens or even 20k.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`truncation_length`必须低于你在运行LLM时选择的上下文长度。任何高于上下文长度的值都会导致错误，因为LLM无法一次处理这么多的上下文。我建议将其设置为稍低于上下文长度，以确保安全。例如，在运行Qwen的CodeQwen-7b-chat时，我将上下文长度设置为32k
    tokens。这意味着我可以将`truncation_length`设置为30k tokens，甚至是20k tokens。
- en: You’ll have to try out different values as `max_new_tokens` can be tricky. Setting
    it higher than 2,048 often results in unpredictable outputs as most LLMs can’t
    handle that many tokens at once (`n_batch`, which defines the number of tokens
    an LLM processes at once by doing several iterations through bigger contexts via
    multiple steps, should be close to the value of `max_new_tokens`; otherwise, the
    LLM won’t know what to output). However, it does work with `Llama-3-8B-Instruct-64k.Q8_0.gguf`,
    which can be found at [https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)
    and is capable of handling 64k tokens at once. However, it needs around 20-22
    GB of VRAM to run. Fortunately, it is quantized to GGUF and you can split the
    LLM over the GPU’s VRAM, as well as the RAM of your machine, which splits the
    load across the GPU and the CPU. It does make the model slower but hey, it works,
    and it can handle 64k tokens at once!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要尝试不同的值，因为`max_new_tokens`可能会有些棘手。将其设置高于2,048通常会导致输出不可预测，因为大多数LLM无法一次处理这么多的token（`n_batch`定义了LLM每次处理的token数量，通过多次迭代较大的上下文来处理多个步骤，`n_batch`的值应接近`max_new_tokens`的值；否则，LLM将不知道输出什么）。然而，它适用于`Llama-3-8B-Instruct-64k.Q8_0.gguf`，该模型可以在[https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)找到，能够一次处理64k个token。然而，它需要大约20-22GB的VRAM来运行。幸运的是，它已经量化为GGUF，你可以将LLM分布到GPU的VRAM和计算机的RAM上，这样就能在GPU和CPU之间分担负载。虽然这会让模型运行更慢，但嘿，它确实能工作，并且可以一次处理64k个token！
- en: 'In this example, we have told the API that we want to use the instruction template
    for ChatML, which looks like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们告诉API我们希望使用ChatML的指令模板，格式如下：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is simply a small script that describes the conversation format of the
    history that was mentioned previously. It should look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简单的脚本，描述了之前提到的历史对话格式。它应该是这样的：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If we choose the wrong instruction template, Auto-GPT won’t be able to understand
    what the LLM responded with. So, make sure you also check which instruction template
    was used by the model. Most models can be found on Hugging Face, a platform that
    holds many such projects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择了错误的指令模板，Auto-GPT将无法理解LLM的回应。因此，确保你也检查一下模型使用了哪个指令模板。大多数模型可以在Hugging Face上找到，这个平台上有许多类似的项目。
- en: I used to prefer using the models quantized to GGUF or AWQ by Tim Robbins, otherwise
    known as TheBloke, which are (at the time of writing this) easier to run and have
    much fewer requirements for VRAM ([https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我过去更喜欢使用Tim Robbins（也被称为TheBloke）量化为GGUF或AWQ的模型（在写这篇文章时），这些模型更容易运行，并且对VRAM的需求较少（[https://huggingface.co/TheBloke](https://huggingface.co/TheBloke)）。
- en: Please be cautious in using any models you find online as some may be malicious.
    Choose your models at your own risk!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何你在网上找到的模型时，请小心，因为有些可能是恶意的。选择模型时请自担风险！
- en: Now, GGUF is a bit different. Although it quantizes the LLM, which means it
    shortens the model so that it uses fewer resources, the process and benefits are
    unique. GGUF quantization involves converting model weights into lower-bit representations
    to significantly reduce memory usage and computational demands.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，GGUF稍有不同。虽然它对LLM进行量化，这意味着它缩短了模型，使其使用更少的资源，但该过程和收益是独特的。GGUF量化涉及将模型权重转换为较低位数的表示，从而显著减少内存使用和计算需求。
- en: Which type you use is up to you – you may even look at the `hugginface` API
    endpoints, where you can choose which LLM to run directly. Note that running LLMs
    directly makes them run at the intended quality base they were made for.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种类型由你决定——你甚至可以查看`hugginface`的API端点，直接选择要运行的LLM。请注意，直接运行LLM会使其以原始质量基准运行。
- en: For more details on how to implement an individual LLM, you will have to check
    out the documentation of the project that you’re running the LLM on. For oobabooga’s
    text-generation-webui, it is as straightforward as starting it using the start
    files (WSL, Linux, and Windows) and enabling the API in the **Session** tab.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何实现单个LLM，你需要查看你正在运行LLM的项目文档。对于oobabooga的text-generation-webui，只需通过启动文件（WSL、Linux和Windows）启动它，并在**会话**标签中启用API。
- en: Note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you use as few commands as possible; otherwise, the LLM will have
    to use most of its brainpower to understand the main prompt provided by Auto-GPT
    and you won’t be able to use Auto-GPT further. To turn off the commands, simply
    follow the instructions in the `.env.template` file inside the Auto-GPT folder.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 确保尽量减少使用命令；否则，LLM 将不得不将大部分计算资源用于理解 Auto-GPT 提供的主要提示，而你将无法继续使用 Auto-GPT。要关闭命令，只需按照
    Auto-GPT 文件夹中的 `.env.template` 文件中的说明操作即可。
- en: The pros and cons of using different models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同模型的优缺点
- en: Each model has its pros and cons. Even if a model can generate fantastic results
    when you tell it to write some code in Python or it can write the most beautiful
    poems on command, it may still lack the ability to respond in the special way
    Auto-GPT needs it to.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有其优缺点。即使一个模型在你要求它编写 Python 代码时能生成出色的结果，或者能够按要求创作最美的诗歌，它仍然可能缺乏 Auto-GPT
    所需的特殊响应能力。
- en: Selecting a model with a certain strength in mind may result in improved performance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特定优势选择模型，可能会提升其性能。
- en: 'The main advantages of using a local LLM are clear:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本地 LLM 的主要优势是显而易见的：
- en: '**Customization**: Tailor the capabilities of Auto-GPT to your specific needs.
    For instance, a model trained on medical literature can make Auto-GPT adept at
    answering medical queries.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化**：根据你的具体需求定制 Auto-GPT 的功能。例如，使用医疗文献训练的模型可以使 Auto-GPT 擅长回答医学相关的问题。'
- en: '**Performance**: Depending on the training and dataset, some models might outperform
    GPT in specific tasks.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：根据训练和数据集的不同，某些模型可能在特定任务上优于 GPT。'
- en: '**Cost efficiency**: Running your local LLM reduces the cost of running it
    drastically. Using GPT-4 with lots of context and generally having many calls
    can quickly add up. Finding a way to break up the number of requests into smaller
    steps will make it possible to run Auto-GPT almost for free.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：运行本地 LLM 可以大幅降低运行成本。使用 GPT-4 并且有大量上下文或频繁调用时，费用会迅速累积。找到将请求数量分解为更小步骤的方法，可以使得几乎免费地运行
    Auto-GPT 成为可能。'
- en: '**Privacy**: Having your own Auto-GPT LLM means having control over who can
    see your data. At the time of writing, OpenAI doesn’t use the data from requests,
    but the information is still being transferred to their end. If this concerns
    you, you are better off running a local model.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：拥有自己的 Auto-GPT LLM 意味着可以控制谁能查看你的数据。到目前为止，OpenAI 不会使用请求中的数据，但信息仍然会传输到他们那端。如果你对此有所担忧，那么运行本地模型会是更好的选择。'
- en: 'However, there are some challenges to consider when running a local LLM:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在运行本地 LLM 时有一些挑战需要考虑：
- en: '**Complexity**: The integration process requires a deep understanding of both
    the chosen LLM and Auto-GPT.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：集成过程需要深入了解所选的 LLM 和 Auto-GPT。'
- en: '**Resource intensity**: LLMs, especially the more advanced ones, demand significant
    computational resources. A robust machine with high VRAM, preferably an NVIDIA
    GPU, is essential for optimal performance. At the time of writing, it’s difficult
    to get good results when running Auto-GPT with a local LLM. I found that running
    a 13B model from the ExLlama transformer-driven Vicuna and Vicuna-Wizard worked
    best at first but still didn’t get consistent results since running it on my local
    GPU meant I needed to run the GPTQ version, which only uses 4 bits instead of
    16 or more. This also means that the accuracy of the responses is very low. An
    LLM that is already quantized to use 4 bits cannot understand too much context
    at once, although I saw drastic improvements over time. Later, I discovered that
    AWQ worked well for me as it is quantized while being aware of which weights are
    the most important, leading to more precise and authentic results. As mentioned
    previously, Mistral 7B (TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ on Huggingface),
    was a very good candidate here as it was capable of answering in JSON format as
    well as understanding the context fully. However, this model is still easy to
    confuse and when it gets confused, it starts explaining through examples. Note
    that our aim here is to get a valid JSON output with commands and contexts.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源强度**：LLM，特别是更先进的版本，需要显著的计算资源。一台配置良好的机器，特别是具有高 VRAM 的 NVIDIA GPU，对于实现最佳性能至关重要。在撰写本文时，当在本地LLM上运行Auto-GPT时很难获得良好的结果。我发现使用来自ExLlama变压器驱动的Vicuna和Vicuna-Wizard的13B模型最初效果最好，但由于在本地GPU上运行它需要运行GPTQ版本，后者仅使用4位而非16位或更多。这也意味着响应的准确性非常低。一个已经量化为使用4位的LLM不能理解太多上下文，尽管随着时间的推移我看到了显著的改进。后来，我发现AWQ对我来说效果很好，因为它是量化的同时又知道哪些权重是最重要的，从而导致更精确和真实的结果。正如前面提到的，Mistral
    7B（Huggingface上的TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ），在这里是一个非常好的候选者，因为它能够以JSON格式回答问题，并完全理解上下文。然而，这个模型仍然很容易混淆，当它困惑时，它开始通过示例进行解释。请注意，我们的目标是获得有效的JSON输出，包括命令和上下文。'
- en: '`llama.cpp` and can only have an `n_batch` value of up to 2,048\. The `n_batch`
    parameter controls the amount of tokens that can be fed to the LLM at the same
    time. It is typically set to 512 so that it can handle a token context consisting
    of 4,000 tokens. However, anything beyond that becomes blurry as the LLM only
    effectively works on the amount given by `n_batch`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llama.cpp` 只能有一个 `n_batch` 值高达 2,048\. `n_batch` 参数控制可以同时输入 LLM 的标记数量。通常设置为
    512，以处理由 4,000 个标记组成的标记上下文。但是，超出此范围的任何内容会使得 LLM 仅有效地处理由 `n_batch` 给出的数量。'
- en: In this section, we delved into the intricacies of integrating custom LLMs with
    Auto-GPT, highlighting the steps required to modify Auto-GPT code for effective
    API communication, the use of a plugin for model selection flexibility, and the
    importance of selecting the appropriate instruction template for seamless model
    interaction. We explored how to select models while emphasizing Hugging Face as
    a resource, and outlined the advantages of utilizing custom models, including
    customization, performance enhancement, cost efficiency, and enhanced privacy.
    Additionally, we discussed the challenges associated with such integration, such
    as the complexity of the process and the significant computational resources required.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了将自定义 LLM 与 Auto-GPT 集成的复杂性，重点介绍了修改 Auto-GPT 代码以实现有效 API 通信所需的步骤，以及使用模型选择插件来增强模型选择灵活性，以及选择适当指令模板以实现模型无缝交互的重要性。我们探讨了如何选择模型，强调了
    Hugging Face 作为资源，并概述了利用自定义模型的优势，包括定制化、性能提升、成本效益和增强隐私性。此外，我们还讨论了与此类集成相关的挑战，例如流程复杂性和所需的显著计算资源。
- en: Writing mini-Auto-GPT
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写小型 Auto-GPT
- en: In this section, we will write a mini-Auto-GPT model that uses a local LLM.
    To avoid reaching the limits of small LLMs, we will have to make a smaller version
    of Auto-GPT.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写一个使用本地LLM的小型Auto-GPT模型。为了避免达到小型LLM的极限，我们将制作一个更小版本的Auto-GPT。
- en: The mini-Auto-GPT model will be able to handle a context length of 4,000 tokens
    and will be able to generate up to 2,000 tokens at once.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 小型Auto-GPT模型将能够处理长度为4,000个标记的上下文，并能够一次生成最多2,000个标记。
- en: I have created a mini-Auto-GPT model just for this book. It’s available on GitHub
    at [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为本书创建了一个小型Auto-GPT模型。它在GitHub上可以找到：[https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt)。
- en: We will start by planning the structure of the mini-Auto-GPT model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Planning the structure
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mini-Auto-GPT model will have the following components:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Telegram chatbot
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts for the LLM and basic thinking
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple memory to remember the conversation
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look at these.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Telegram chatbot
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because chatting with your AI over Telegram enables you to interact with it
    from anywhere, we will use a Telegram chatbot as the interface for the mini-Auto-GPT
    model. We’re doing this because the AI will decide when to contact you.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The Telegram chatbot will be the interface for users to interact with the mini-Auto-GPT
    model. Users will send messages to the chatbot, which will then process the messages
    and generate responses using the local LLM.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Prompts for the LLM and basic thinking
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prompts for the LLM have to be short but strict. First, we must define the
    context and then the command to tell it explicitly to respond in JSON format.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: To achieve similar results to Auto-GPT, we will need to use a strategy to chunk
    the context into smaller parts and then feed them to the LLM. Alternatively, we
    could feed the context into the LLM and just let it write whatever it thinks about
    the context.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The strategy here is to try to make the LLM parse the context into its language
    so that when we work with the LLM, it can best understand what we want from it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The system prompt for these thoughts looks like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is fed into the history that we send to the LLM. The history will not
    be filled with the preceding prompt:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To automate this, I have written a method that will fill the history with the
    thought prompt, as well as the context. Conversation history and message history
    will be added to the context as well. Those are empty at the beginning but will
    be filled with the conversations and messages that the AI shares with the user.
    In mini-AutoGPT, conversation history is fed with the thought history to ensure
    that the AI works autonomously:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can execute the `build_context` method and add the context to the history.
    We also have to add a trigger command using the user role:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The LLM will now return its thoughts on the context and the command.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example thought. Such thoughts are often this long, but
    this helps the AI make a bias for itself:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is a very detailed thought, but it is important to have the LLM understand
    the context and the command. At this point, we can use it as the context base
    so that the LLM can proceed with the decision process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: This longer thought text occupies context, meaning it obstructs the LLM from
    adding contexts that do not fit what is already there. In later steps, even more
    are created (since it runs in a loop, it does this every time it starts thinking),
    and the text helps tremendously at keeping the LLM on topic. Hallucination, for
    example, is massively reduced when the context is that clear.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The decision process will now return a JSON output that will be evaluated by
    the mini-Auto-GPT model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We also have to define the instruction template and JSON schema that the LLM
    uses as we have to tell the LLM how to respond to the prompt.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'In mini-Auto-GPT, the template looks like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the schema that the LLM has to follow; it has to respond with a command
    that contains a name and arguments.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need an action prompt that will tell the LLM what to do next:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you might have noticed, the action prompt already contains the possible commands
    that the LLM can use, as well as the JSON schema that the LLM has to follow.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure we have a clear structure, we will also have to define the commands
    that the LLM can use:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now feed the thought string that we generated earlier into the history
    and let `mini_AutoGPT` decide on the next action:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The command to be executed will be defined in the `command` field, with the
    name of the command in the `name` field and the arguments in the `args` field.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: We will soon see that only providing this schema will not be enough as the LLM
    will not know what to do with it and also often not even comply with it. This
    can be achieved by evaluating the output of the LLM and checking if it is valid
    JSON.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'In almost half the cases, the LLM will respond correctly. In the other 70%,
    it will not respond in a way that we can use it. That’s why I wrote a simple evaluation
    method that will check whether the response is valid JSON and whether it follows
    the schema:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At this point, most of the time, we should have a valid JSON output that we
    can use to evaluate the decision.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, it may now return some JSON for greeting the user:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is a valid JSON output that we can use to evaluate the decision:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is the method that will take the action that the LLM has decided on.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The memory will be updated with the response and the message will be sent to
    the user. Once the user has responded, the AI will continue with the next action.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: This is how the mini-Auto-GPT model will work; it will decide on the next action
    and then take it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Adding a simple memory to remember the conversation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mini-Auto-GPT model will have a simple memory to remember the conversation.
    This memory will store the conversation history and the messages that the AI has
    with the user. The same can be done with the thoughts and decisions that the AI
    has:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is the memory that will be used to store the conversation history and
    the messages that the AI has with the user. But we still have a problem: the memory
    will accumulate over time and we will have to clear it manually. To avoid this,
    we can take a simple approach to chunking and summarizing the conversation history
    and the messages:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The token counter is a very important part of this code that is almost always
    required when doing LLM calls. We ensure that the LLM never runs out of tokens
    and also has more control later. The fewer tokens we use, the more likely the
    LLM will not return nonsense or untrue statements for some LLMs, especially for
    the smaller 1B to 8B models:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Summarizing texts makes us capable of building upon what we started when building
    the token counter as we can shorten contexts and therefore save tokens for later
    use:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since the context and their texts can become too large, we have to make sure
    we split the text first. It’s up to you how you do this. It is OK to do length
    splitting, though it can be better to not even cut up sentences. Maybe you can
    find a way to split the text into sentences and have each chunk contain the summary
    of the one before and after them? For simplicity, we’ll leave such extensive logic
    out for now:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we’ve split all text into chunks, we can summarize those as well.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can take care of the conversation history. This looks like
    a duplicate of the response history, but we need it to keep the whole context
    in some cases.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'The conversation history is mostly useful for maintaining continuity in discussions,
    while the response history is used for understanding logical actions and reactions
    that the agent observes, such as researching a topic and the result (the researched
    topic) of that action:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is the memory refresh that will be used to delete the conversation history
    and the messages that our mini-AutoGPT model remembers with the user.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: This way, even if our friend crashes or we close the program, we will still
    have the conversation history and the messages that the agent has with the user,
    but we’ll still be able to clear them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full code example in this book’s GitHub repository: [https://github.com/Wladastic/mini_autogpt](https://github.com/Wladastic/mini_autogpt).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore the art of crafting effective prompts, a crucial skill
    for anyone looking to maximize the benefits of their custom LLM integrations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Rock solid prompt – making Auto-GPT stable with instance.txt
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-GPT offers the flexibility to autonomously generate goals, requiring only
    a brief description from the user. Despite this, I recommend supplementing it
    with helpful instructions, such as noting down insights in a file, to retain some
    memory in case of a restart.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will explore more examples of such prompts, beginning with a continuous
    chatbot prompt I use:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '`instance.txt` for previous notes):'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engage in active listening with the user, showing empathy and understanding
    through thoughtful responses and open-ended questions
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously learn about the user’s preferences and interests through observation
    and inquiries, adapting responses to provide personalized support
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Foster a safe and non-judgmental environment for the user to express their thoughts,
    emotions, and concerns openly
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide companionship and entertainment through engaging conversation, jokes,
    and games
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Carefully plan tasks and write them down in a to-do list before executing them
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ai_name**: Sophie'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ai_role**: A warm-hearted and compassionate AI companion for Wladislav that
    specializes in active listening, personalized interaction, emotional support,
    and executing tasks when given'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**api_budget**: 0.0'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this setup, the goals hold more significance than the role, guiding Auto-GPT
    more effectively, while the role mainly influences the tone and behavior of the
    responses.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned that the goals and role of an AI such as Sophie
    can significantly influence its behavior and responses, with the goals having
    a more direct impact on the AI’s effectiveness.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will delve into the concept of negative confirmation in prompts, a
    crucial aspect that can refine Auto-GPT’s understanding and response generation.
    The next section will highlight its importance and demonstrate how to implement
    it effectively in your prompts.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Implementing negative confirmation in prompts
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative confirmation serves as a vital tool in refining Auto-GPT’s understanding
    and response generation by instructing it on actions to avoid. This section highlights
    its importance and demonstrates how to implement it effectively in your prompts.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The importance of negative confirmation
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing negative confirmation can enhance the interaction with Auto-GPT
    in several ways, some of which are listed here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Preventing off-track responses**: It helps in avoiding unrelated topics or
    incorrect responses'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing security**: It sets boundaries to prevent engagement in activities
    that might breach privacy or security protocols'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing performance**: It avoids unnecessary computational efforts, steering
    the bot away from irrelevant tasks or processes'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that you won’t be using negative prompts as they can lead to the LLM using
    the same statements again.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Examples of negative confirmation
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some practical examples of how negative confirmation can be utilized
    in your prompts:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '**Explicit instructions**: Including instructions such as *Do not provide personal
    opinions* or *Avoid using technical jargon* to maintain neutrality and comprehensibility.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Setting boundaries**: For tasks involving data retrieval or monitoring, you
    can set boundaries such as *Do not retrieve flight prices from unofficial, scam,
    or reseller websites* to ensure data reliability.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scripting constraints**: In scripting, especially in Bash, use negative confirmation
    to prevent potential errors. For example, you can include *if [ -z $VAR ]; then
    exit 1; fi* to halt the script if a necessary variable is unset.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emphasizing by using Upper Case Letters**: Sometimes, it only helps to *scream*
    at the LLM by writing in uppercase letters. *DO NOT ASK THE USER HOW TO PROCEED*
    may be interpreted by the LLM better and it will be less likely to ignore that
    statement. However, there is never a guarantee that this will happen.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will delve into the intricacies of applying rules and tonality in prompts.
    We will learn how understanding and manipulating these elements can significantly
    influence Auto-GPT’s responses, allowing us to guide the model more effectively.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Applying rules and tonality in prompts
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding and manipulating the rules and tonality within your prompts can
    significantly influence Auto-GPT’s responses. This section will explore the nuances
    of setting rules and adjusting tonality for more effective guidance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The influence of tonality
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto-GPT can adapt to the tonality that’s used in prompts, mimicking stylistic
    nuances or even adopting a specific narrative style, allowing for more personalized
    and engaging interaction. However, adherence to tonality can sometimes be inconsistent
    due to the potential ambiguity created by tokens from other prompts.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating rules
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting rules can streamline the interaction with Auto-GPT, specifying the format
    of responses or delineating the scope of information retrieval. However, it’s
    not foolproof as Auto-GPT may sometimes overlook these rules when faced with conflicting
    inputs or unclear directives.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Temperature setting – a balancing act
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manipulating the “temperature” setting is crucial in controlling Auto-GPT’s
    behavior and thus influencing the randomness of the bot’s responses. The temperature
    defines the amount of creativity the LLM should practice, meaning the higher the
    number, the more randomness is introduced. A range between 0.3 to 0.7 is considered
    optimal, fostering a more logical and coherent train of thought in the bot, while
    a value below 0.3, or even 0.0, might result in repetitive behavior that adheres
    to the text that was already given and even reuses some of its parts, making it
    more precise. However, the LLM may start thinking the world is only limited to
    the facts that you gave it, making it more likely to make false statements. A
    value higher than 0.7 or even 2.0 may result in gibberish, where the LLM starts
    outputting texts that it learned that have nothing to do with the context. For
    example, it may start rephrasing Shakespeare when the context is about algebra.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll delve into some practical examples that demonstrate the impact of
    different settings and approaches on the output generated by Auto-GPT.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – clarity and specificity
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prompt**: Tell me about that big cat'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised prompt**: Provide information about the African lion'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised prompt is more specific, guiding Auto-GPT to provide
    information about a particular species of big cats'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 2 – consistency in tonality
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initial prompt**: Could you elucidate the economic implications of global
    warming?'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Follow-up prompt**: Hey, what’s the deal with ice melting?'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised follow-up prompt**: Can you further explain the environmental consequences
    of the melting ice caps?'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised follow-up prompt maintains the formal tone established
    in the initial prompt, promoting consistency in the interaction.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 3 – utilizing temperature effectively
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Task**: Creative writing'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature setting**: 0.8 (for fostering creativity)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: Factual query'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature setting**: 0.3 (for more deterministic responses)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: Adjusting the temperature setting based on the nature of the
    task can influence the randomness and coherence of Auto-GPT’s responses'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 4 – setting boundaries
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initial prompt**: Provide a summary of the Renaissance period without mentioning
    Italy'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revised prompt**: Discuss the artistic achievements of the Renaissance, focusing
    on regions other than Italy'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanation**: The revised prompt is more flexible, allowing Auto-GPT to
    explore the topic without the strict restriction of excluding Italy'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned how different types of prompts or tones can drastically
    influence the behavior of the LLM and therefore Auto-GPT.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on an interesting journey through the process of
    integrating custom LLMs with Auto-GPT while exploring what LLMs are, with a specific
    focus on GPT as a prime example. We uncovered the vast landscape of LLMs, delving
    into various models beyond GPT, such as BERT, RoBERTa, Llama, and Mistral, and
    their unique characteristics and compatibilities with Auto-GPT.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The usefulness of this chapter lies in its comprehensive guide on how to enrich
    Auto-GPT’s capabilities by incorporating your own or alternative LLMs. This integration
    offers a more personalized and potentially more efficient use of AI technology,
    tailored to specific tasks or fields of inquiry. The detailed instructions for
    setting up these integrations, alongside considerations for instruction templates
    and the necessary computational resources, are invaluable for those looking to
    push the boundaries of what’s possible with Auto-GPT.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Crafting the perfect prompt is a blend of art and science. Through clear guidelines,
    a deep understanding of Auto-GPT’s nuances, and continuous refinement, you can
    fully harness the power of this tool. Encourage yourself to experiment and learn
    through trial and error, adapting to the ever-evolving field of AI. Whether for
    research, creative endeavors, or problem-solving, mastering the art of prompt
    crafting ensures that Auto-GPT becomes a valuable ally in your endeavors.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we’ve embarked on a detailed journey into the nuances
    of crafting effective prompts – a cornerstone for maximizing the utility of Auto-GPT.
    This chapter stands as a reference for strategically developing prompts that lead
    to more aligned, efficient, and cost-effective interactions with Auto-GPT. By
    emphasizing the importance of clarity, specificity, and strategic intent in prompt
    creation, you have gained invaluable insights into guiding Auto-GPT toward generating
    responses that closely align with your objectives.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The utility of this chapter cannot be overstated. For practitioners and enthusiasts
    alike, mastering the art of prompt crafting is critical for optimizing the performance
    of Auto-GPT for a variety of tasks. Through illustrative examples and comprehensive
    guidelines, this chapter has shed light on how to effectively employ negative
    confirmation to avoid undesired responses, the impact of rules and tonality on
    Auto-GPT’s outputs, and the significance of temperature settings in influencing
    the bot’s creativity and coherence. This knowledge is crucial not only for enhancing
    the quality of interactions with Auto-GPT but also for ensuring the efficient
    use of computational resources.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have enjoyed this journey as much as I have in taking you on it and
    I hope I’ve given you a few ideas on how to improve your life with Auto-GPT. I’ve
    written many clones of that project so that I could wrap my head around the more
    complex parts of it. I do advise that you do so too, just as a brain teaser.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
