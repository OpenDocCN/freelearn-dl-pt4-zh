- en: Image Generation from Description Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have been mainly dealing with image synthesis and
    image-to-image translation tasks. Now, it's time for us to move from the CV field
    to the NLP field and discover the potential of GANs in other applications. Perhaps
    you have seen some CNN models being used for image/video captioning. Wouldn't
    it be great if we could reverse this process and generate images from description
    text?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the basics of word embeddings and how
    are they used in the NLP field. You will also learn how to design a text-to-image
    GAN model so that you can generate images based on one sentence of description
    text. Finally, you will understand how to stack two or more Conditional GAN models
    to perform text-to-image synthesis with much higher resolution with StackGAN and
    StackGAN++.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image synthesis with GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating photo-realistic images with StackGAN++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-to-image synthesis with GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From [Chapter 4](3894df8d-1a40-418e-ac36-d9357abdfd6a.xhtml), *Building Your
    First GAN with PyTorch*, to [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml),
    *Training Your GANs to Break Different Models*, we have learned almost every basic
    application of GANs in computer vision, especially when it comes to image synthesis.
    You're probably wondering how GANs are used in other fields, such as text or audio
    generation. In this chapter, we will gradually move from CV to NLP by combining
    the two fields together and try to generate realistic images from description
    text. This process is called **text-to-image ****synthesis** (or text-to-image translation).
  prefs: []
  type: TYPE_NORMAL
- en: We know that almost every GAN model generates synthesized data by establishing
    a definite mapping from a certain form of input data to the output data. Therefore,
    in order to generate an image from a corresponding description sentence, we need
    to understand how to represent sentences with vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Quick introduction to word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's rather easy to define an approach for transforming the words in a sentence
    into vectors. We can simply assign different values to all the possible words
    (for example, let 001 represent *I*, 002 represent *eat*, and 003 represent *apple*)
    so that the sentence can be uniquely represented by a vector (for example, *I
    eat apple* would become [001, 002, 003]). This is basically how words are represented
    in computers. However, languages are much more complicated and flexible than cold
    digits. Without knowing the meaning of words (for example, a noun or a verb, positive
    or negative), it is nearly impossible to establish the relationship between the
    words and understand the meaning of the sentence. Furthermore, it is very hard
    to find a synonym of a word based on hardcoded values since the distance between
    the values does not represent the similarity between the corresponding words.
  prefs: []
  type: TYPE_NORMAL
- en: Methods that have been designed to map words, phrases, or sentences to vectors
    are called **word embeddings**. One of the most successful word embedding techniques
    is called **word2vec**. If you want to learn more about word2vec, feel free to
    check out the paper *word2vec Parameter Learning Explained*, [https://arxiv.org/pdf/1411.2738.pdf](https://arxiv.org/pdf/1411.2738.pdf),
    by Xin Rong.
  prefs: []
  type: TYPE_NORMAL
- en: The term **embedding** means projecting data to a different space so that it's
    easier to analyze. You may have seen this term being used in some old papers or
    articles about CNNs, where the output vector of a learned fully connected layer
    is used to visualize whether the models are trained properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings are mostly used to solve two types of problems in NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CBOW** (**Continuous Bag-of-Word**) models, which are used to predict a single
    word based on several other words in the context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip-Gram models, which are the opposite of CBOWs and are used to predict the
    context words based on the target word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram provides us with an overview of the CBOW and Skip-Gram
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9d6576-7fd3-4acb-b186-a7b7d208c6cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Two types of word embeddings. Image retrieved from Xin Rong, 2014
  prefs: []
  type: TYPE_NORMAL
- en: Another common term in NLP is **language modeling**. Compared to word embeddings,
    language models predict the possibilities of sentences or, more specifically, the
    possibilities of words appearing at the next position in a sentence. Since language
    modeling takes the order of words into consideration, many language models are
    built upon word embeddings to get good results.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a learned word embedding is a vector that represents a sentence
    that is easier for machine learning algorithms to analyze and understand the meaning
    of the original sentence. Check out the official tutorial about word embedding
    to learn how to implement CBOW and Skip-Gram models in PyTorch: [https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py).
  prefs: []
  type: TYPE_NORMAL
- en: Translating text to image with zero-shot transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 8](f05fbf9f-30b6-41d4-b706-5f3ef0d6fff7.xhtml), *Training Your GANs
    to Break Different Models*, we learned about the basic steps we need to take in
    order to perform transfer learning in image classification tasks. Under more realistic
    circumstances, it becomes harder to transfer this learned knowledge to another
    domain because there can be many new forms of data that the pretrained model hasn't
    met before, especially when we try to generate images based on description text
    (or in a reverse process where we generate description text from given images).
    For example, if the model is only trained on white cats, it won't know what to
    do when we ask it to generate images of black cats. This is where zero-shot transfer
    learning comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Zero-shot learning** refers to a machine learning process where we need to
    predict new samples with labels that haven''t been seen before. It is often done
    by providing additional information to the pretraining process. For example, we
    can tell the model that the objects known as **white cats** have two properties:
    a color, that is, white, and the shape of a cat. This makes it easy for the model
    to know that replacing the white color with black would give us *black cats* when
    we ask for them.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the machine learning process where the new samples are only labeled
    once per class (or very few samples are labeled per class) is called **one-shot
    learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to establish the zero-shot learning ability between text and image,
    we will use the word embedding model proposed by Scott Reed, Zeynep Akata, and
    Bernt Schiele, et al in their paper, *Learning Deep Representations of Fine-Grained
    Visual Descriptions*. Their model is designed for one purpose: finding the most
    matching images from a large collection based on a single query sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is an example of image search results from a single query
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb24ab6f-7baa-4120-888d-0b5566458c51.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of image search results from a single query sentence on the CUB-200-2011
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: We won't dive into the implementation details of the word embedding method here
    and instead use the pretrained `char-CNN-RNN` results provided by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: GAN architecture and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The design of the GAN model in this section is based on the text-to-image model
    proposed by Scott Reed, Zeynep Akata, and Xinchen Yan, et al in their paper, *Generative
    Adversarial Text to Image Synthesis*. Here, we will describe and define the architectures
    of the generator and discriminator networks and the training process.
  prefs: []
  type: TYPE_NORMAL
- en: The generator network has two inputs, including a latent noise vector, ![](img/1c8d67dc-79fd-4232-8a34-666a701583bd.png), and
    the embedding vector, ![](img/e747bae2-9b44-4376-b675-4e9729446fcd.png), of the
    description sentence. The embedding vector, ![](img/11b94c85-71d6-420b-9a62-79bfa784a8ba.png), has
    a length of 1,024, which is mapped by a fully-connected layer to a vector of 128\.
    This vector is concatenated with the noise vector, ![](img/11db02c9-d480-43ee-a5f7-913ae50abb22.png), to
    form a tensor with a size of `[B, 228, 1, 1]` (in which B represents the batch
    size and is omitted from now on). Five transposed convolution layers (with a kernel
    size of 4, a stride size of 2, and a padding size of 1) are used to gradually
    expand the size of feature map (while decreasing the channel width) to `[3, 64,
    64]`, which is the generated image after a `Tanh` activation function. Batch normalization
    layers and `ReLU` activation functions are used in the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new file named `gan.py` to define the networks. Here is the
    code definition of the generator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The discriminator network also has two inputs, which are the generated/real
    image, ![](img/0436cf09-e622-40e4-8157-8609fe701657.png), and the embedding vector, ![](img/9f0ec161-c717-458c-b755-b6afd4802065.png).
    The input image, ![](img/59d4e74c-a099-4663-af65-b478c69c594d.png), is a tensor
    with a size of `[3, 64, 64]` and is mapped to `[512, 4, 4]` through four convolution
    layers. The discriminator network has two outputs and the `[512, 4, 4]` feature
    map is also the second output tensor. The embedding vector, ![](img/7fc07115-6a51-46f0-b573-b19b8a5b963f.png), is
    mapped to a vector with a length of 128 and expanded to a tensor of size `[128,
    4, 4]`, which is then concatenated with the image feature map. Finally, the concatenated
    tensor (with a size of `[640, 4, 4]`) is fed into another convolution layer that
    gives us the prediction value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code definition of the discriminator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The training process of both networks can be seen in the following diagram.
    We can see that training text-to-image GANs is very similar to the vanilla GAN,
    except that the intermediate outputs (the second output tensors) of the discriminator
    from the real and generated images are used to calculate the L1 loss, while the
    real/generated images are used to calculate the L2 loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39c793ee-d4b7-484b-9544-c55fee800106.png)'
  prefs: []
  type: TYPE_IMG
- en: Training process of a text-to-image GAN, in which *x** represents the real image,
    *x* represents the generated image, *t* represents the text embedding vector,
    and *z* represents the latent noise vector. The dotted arrows coming out of the
    discriminator, *D*, represent the intermediate output tensors
  prefs: []
  type: TYPE_NORMAL
- en: As we introduce the following code, things are going to be presented in a somewhat
    non-linear fashion. This is to ensure that you understand the processes that are
    involved at each point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new file named `build_gan.py` and create a one-stop train/eval
    API, just like we did in some of the previous chapters. We will only show the
    crucial parts of the training process. You may fill in the blanks yourself as
    an exercise or refer to the full source code under the `text2image` folder in
    the code repository for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s work on the training process (which is defined in `Model.train()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the Caltech-UCSD Birds-200-2011 (**CUB-200-2011**) dataset, which
    contains 11,788 annotated bird images. Instead of processing the bird images and
    training the word embedding vectors by ourselves, we will use the pretrained embeddings
    by the authors directly ([https://github.com/reedscot/icml2016](https://github.com/reedscot/icml2016)).
    In the GitHub repository ([https://github.com/aelnouby/Text-to-Image-Synthesis](https://github.com/aelnouby/Text-to-Image-Synthesis)),
    an HDF5 database file containing image files, embedding vectors, and original
    description text is kindly provided.
  prefs: []
  type: TYPE_NORMAL
- en: Let's download the database file (which is around 5.7 GB in size) from the Google
    Drive link that's provided ([https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j](https://drive.google.com/open?id=1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j))
    and put it in a folder (for example, `/media/john/DataAsgard/text2image/birds`).
    Let's also download the custom dataset class ([https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py](https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py))
    because it's a little bit tricky to get exported HDF5 database elements into PyTorch
    tensors correctly. This also means that we need to install the `h5py` library
    before running the script with `pip install h5py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s create a `main.py` file and fill in the argument parsing code,
    as we have done many times already, and call `Model.train()` from it. Again, we
    omit most of the code in `main.py`. You can refer to the full source code in this
    chapter''s code repository if you need any help:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes about 2 and a half hours to finish 200 epochs of training and costs
    about 1,753 MB of GPU memory with a batch size of 256\. Some of the results by
    the end of training are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca438617-3483-41df-bc7b-2fedd6e06e4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Images generated by a text-to-image GAN on the CUB-200-2011 dataset
  prefs: []
  type: TYPE_NORMAL
- en: The method that we used in this section was proposed more than 3 years ago,
    and so the quality of the generated images is not as good as it should be in this
    day and age. Therefore, we will introduce you to StackGAN and StackGAN++ so that
    you can generate high-resolution results.
  prefs: []
  type: TYPE_NORMAL
- en: Generating photo-realistic images with StackGAN++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generation of images from description text can be considered as a **Conditional
    GAN** (**CGAN**) process in which the embedding vector of the description sentence
    is used as the additional label information. Luckily for us, we already know how
    to use CGAN models to generate convincing images. Now, we need to figure out how
    to generate large images with CGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Do you remember how we used two generators and two discriminators to fill out
    the missing holes in images (image inpainting) in [Chapter 7](c9fec01a-2b58-4de3-a62d-da11928e5afe.xhtml),
    *Image Restoration with GANs*? It's also possible to stack two CGANs together
    so that we can get high-quality images. This is exactly what StackGAN does.
  prefs: []
  type: TYPE_NORMAL
- en: High-resolution text-to-image synthesis with StackGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**StackGAN** was proposed by Han Zhang, Tao Xu, and Hongsheng Li, et al in
    their paper, *StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative
    Adversarial Networks*.'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding vector, ![](img/c7f5af9d-724c-4630-9d94-ddae4343edd5.png), of
    the description sentence is processed by the Conditioning Augmentation step to
    create a conditional vector, ![](img/93aef638-6af8-466f-a4e7-94b7a5037865.png).
    In Conditioning Augmentation, a pair of mean, ![](img/759a63f4-c271-44c0-9583-202625fd74fa.png), and
    standard deviation, ![](img/6c7dc87d-7ac5-40a3-bab8-1b6ac7c86251.png), vectors
    are calculated from the embedding vector, ![](img/5d858e81-a298-40db-a739-282de8b317b3.png), to
    generate the conditional vector, ![](img/6649db62-a8e5-4699-910f-bc54419ebfdc.png), based
    on the Gaussian distribution, ![](img/f96b7563-a795-4250-86c9-a307f98a1df3.png).
    This process lets us create much more unique conditional vectors from limited
    text embeddings and ensures that all the conditional variables obey the same Gaussian
    distribution. At the same time, ![](img/9dbac81e-46b0-4230-9c91-8857b5fb2b43.png) and ![](img/a60ba181-13b0-40aa-adcc-49d5bbcc7e25.png) are
    restrained so that they're not too far away from ![](img/6329474d-f9f9-4b0b-ae9b-bf551ef8809c.png).
    This is done by adding a Kullback-Leibler divergence (**KL divergence**) term
    to the generator's loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A latent vector, ![](img/d95c20b8-4a36-4510-9505-ed6745508a7b.png) (which is
    sampled from ![](img/58bb6f57-35ad-4540-9427-61f6369f215c.png)), is combined with
    the conditional vector, ![](img/238c0643-d964-4c2d-8d3e-438eaf7112df.png), to
    serve as the input of the **Stage-I Generator**. The first generator network generates
    a low-resolution image with a size of 64 x 64\. The low-resolution image is passed
    to the **Stage-I Discriminator**, which also takes the embedding vector, ![](img/c681f0ec-8d8f-43ae-ad5a-adf43138327c.png), as
    input to predict the fidelity of the low-resolution image. The loss functions
    of the Stage-I Generator and Discriminator are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3599625-7cd2-4d78-be35-7eecdeb8edda.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equations, ![](img/27d8a610-f64a-41a2-a357-58a8ab0b3df6.png) in ![](img/93bbbd44-48d1-4c9b-a9d8-7561dc2de372.png) is
    the output of the Stage-I Generator ![](img/68201ed0-605a-4081-981c-29f3055abe4c.png),
    in which ![](img/6cb26ab9-a50b-4c09-b227-d64470257f64.png) is the conditional
    vector and ![](img/b9920f48-2f92-4098-b1ab-6e5d49abcaeb.png)represents the KL
    divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the low-resolution image is fed into the **Stage-II Generator**. Again, the embedding
    vector, ![](img/c096dd4e-3d3c-4cd5-9266-26dcf2bb073b.png), is also passed to the
    second generator to help create the high-resolution image that''s 256 x 256 in
    size. The quality of the high-resolution image is judged by the **Stage-II Discriminator**, which
    also takes ![](img/71aff809-f328-4c93-8222-8f5e9486d639.png) as input. The loss
    functions in the second stage are similar to the first stage, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86afb79c-8af9-46cf-82e8-aee5f5cd0487.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equations , ![](img/cc9a10ce-ed57-4f71-ad42-55d19c91d2b2.png) in ![](img/cd3dfe50-c9df-489a-a1cc-c0467eeb951a.png) is
    the output of the Stage-II Generator, ![](img/e9134e36-e6cb-41de-97b9-715bb863bff2.png),
    in which ![](img/88488533-71cb-47cb-b839-c2ea31bd15ce.png) is the output of the Stage-I
    Generator and ![](img/af026797-9233-4afb-80a2-d8258da7aa46.png) is the conditional
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Some images that have been generated by StackGAN will be provided in the upcoming
    sections. If you are interested in trying out StackGAN, the authors of the paper
    have opensourced a PyTorch version here: [https://github.com/hanzhanggit/StackGAN-Pytorch](https://github.com/hanzhanggit/StackGAN-Pytorch).
  prefs: []
  type: TYPE_NORMAL
- en: From StackGAN to StackGAN++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**StackGAN++** (also called StackGAN v2) is an improved version of StackGAN
    and was proposed by Han Zhang, Tao Xu, and Hongsheng Li, et al in their paper,
    *StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks*.
    Compared to StackGAN, there are three main differences in the design of StackGAN++,
    which are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-scale image synthesis**: It uses a tree-like structure (as shown in
    the following diagram) in which each branch represents an individual generator
    network and the size of generated image increases as the tree becomes higher.
    The quality of the images that are generated by each branch is estimated by a
    different discriminator network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Employment of unconditional loss**: Besides using label information (calculated
    from text embedding) to estimate the fidelity of images, additional loss terms,
    where the images are the only inputs, are added to the loss function of every
    generator and discriminator (as shown in the following equation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The loss functions of the discriminator and generator at the ![](img/e5ab09e2-29b8-4f74-8d79-99967bc59a1d.png) th
    branch are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb599287-74c7-4877-ad25-1d1f92ea0dad.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the first line in each loss function is called the
    conditional loss, and the second line is called the unconditional loss. They are
    calculated by the **JCU Discriminator**, which was illustrated in the previous
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Color-consistency restraints**: Since there can be several branches in the
    tree structure, it is important to ensure that the images that are generated by
    different branches are similar to each other. Therefore, a color-consistency regularization
    term is added to the generator''s loss function (with a scale factor, of course).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The color-consistency regularization is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9c0f454-d81e-4113-8967-9fd21a1a7d7b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, ![](img/38e5f946-4502-4b72-b44c-931f0436ef6c.png) represents
    the batch size, while ![](img/5bea8296-a644-474c-86e8-41508819fe46.png) and ![](img/d76aef4f-8ff0-4d0d-8489-31b5077db110.png) represent
    the mean and covariance of the ![](img/5f6dd24c-3cd2-4375-8067-9fe9084b84f0.png) th
    image generated by the ![](img/abff9797-76f8-4e79-88e8-b77d97d73a58.png) th generator.
    This makes sure that the images that are generated by neighboring branches have
    similar color structures.
  prefs: []
  type: TYPE_NORMAL
- en: Training StackGAN++ to generate images with better quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The authors of StackGAN++ have kindly open sourced the full source code here: [https://github.com/hanzhanggit/StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2).
    Follow these steps to train StackGAN++ on the CUB-200-2011 dataset. Make sure
    you have created a **Python 2.7** environment with PyTorch in Anaconda since there
    will be decoding errors from `pickle` when loading the pretrained text embeddings.
    You can follow the steps in [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml),
    *Getting Started with PyTorch 1.3*, to create a new environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the prerequisites by running the following command in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Make sure you don't have `tensorboard` installed in your Python 2.7 environment
    since StackGAN++ calls `FileWriter` to write logging information to TensorBoard
    and `FileWriter` has been removed in the latest version of TensorBoard. If you
    don't want to uninstall TensorBoard, you can downgrade it by running `pip install
    tensorboard==1.0.0a6`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the source code of StackGAN++:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Download the CUB-200-2011 dataset from [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)
    and put the `CUB_200_2011` folder in the `data/birds` directory, so that the images
    are located at paths such as `data/birds/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg`.
    The compressed file that needs to be downloaded is about 1.1 GB in size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the pretrained text embeddings from [https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE](https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE)
    and move the three folders in it to `data/birds`. Make sure that you rename the
    `text_c10` folder to `text`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the code folder and start the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You only need to make a few changes to the source code of StackGAN++ so that
    it can run under PyTorch 1.1; for example, you can replace all the `.data[0]`
    with `.item()` in `trainer.py`. There are also several deprecation warnings that
    we can fix. You can refer to the source code located under the `stackgan-v2` folder
    in this book's code repository for this chapter for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '(Optional) Test your trained model. Specify the model file in the `code/cfg/eval_birds.yml`
    file like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following script in your Terminal to begin the evaluation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation costs about 7,819 MB of GPU memory and takes 12 minutes to finish.
    The generated images will be located in the `output/birds_3stages_2019_07_16_23_57_11/Model/iteration220800/single_samples/valid`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes about 48 hours to finish 600 epochs of training on a GTX 1080Ti graphics
    card and costs about 10,155 MB of GPU memory. Here are some of the images that
    are generated by the end of the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06294c97-6264-40bf-8ac6-3771f30b8f00.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by StackGAN++
  prefs: []
  type: TYPE_NORMAL
- en: While this process takes a very long time and a large amount of GPU memory,
    you can see that the results are very nice.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to generate low-resolution and high-resolution
    images based on description text.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on directly generating sequence data, such
    as text and audio, with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rong X. (2014). *word2vec Parameter Learning Explained*. arXiv:1411.2738.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reed S, Akata Z, Schiele B, et. al. (2016). *Learning Deep Representations of
    Fine-Grained Visual Descriptions*. CVPR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reed S, Akata Z, Yan X, et al (2016). *Generative Adversarial Text to Image
    Synthesis*. ICML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhang H, Xu T, Li H, et al (2017). *StackGAN: Text to Photo-realistic Image
    Synthesis with Stacked Generative Adversarial Networks*. ICCV.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhang H, Xu T, Li H, et al (2018). *StackGAN++: Realistic Image Synthesis with
    Stacked Generative Adversarial Networks*. IEEE Trans. on Pattern Analysis and
    Machine Intelligence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
