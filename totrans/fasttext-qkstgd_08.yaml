- en: FastText in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of fastText is specifically to transform words and sentences into efficient
    vector representations. Although fastText is written in C++, there are community-written
    Python bindings to train and use the models. Along with that, Python is one of
    the most popular languages used for NLP, and hence there are many other popular
    libraries in Python that support fastText models and the training of fastText
    models. Gensim and Spacy are two popular libraries that make it easy to load these
    vectors, transform, lemmatize, and perform other NLP tasks efficiently. This chapter
    will focus on how to use fastText with Python and its popular libraries. This
    chapter will also focus on showing you some common tasks that the two libraries
    can do to work with fastText models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that are covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: FastText official bindings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyBind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a fastText model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation using Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastText official bindings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The steps to install the official bindings for Python are covered in the first
    chapter. In this section, we will cover how to use the official fastText Python
    package to train, load, and use the models.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Python fastText library, you will be able to implement all the necessary
    features that can be done using the command line. Lets take a look at the ways
    to implement unsupervised and supervised learning using Python fastText.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: In this chapter, we will be using Python3 and so the code examples will
    be in that. For users who are using Python2, please take a look at the *Appendix* for
    notes on the considerations that you need to bear in mind when using Python2.'
  prefs: []
  type: TYPE_NORMAL
- en: PyBind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python bindings for fastText are made using the excellent PyBind library. PyBind
    is a lightweight library meant to expose C++ types in Python and vice versa, making
    it an excellent choice for creating the Python bindings for fastText. It supports
    almost all the popular C++ compilers such as Clang, GCC, Visual Studio, and so
    on. Also, the creators of PyBind claim that the binaries that are generated are
    smaller.
  prefs: []
  type: TYPE_NORMAL
- en: The Python-fastText library uses the fastText C++ API.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the performance of fastText is quite good on raw text, it''s still advisable
    to preprocess the data before running the unsupervised algorithms or the classifier.
    Some points to be remembered are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To train fastText, the encoding needs to be in UTF-8\. PyBind does an excellent
    job of converting almost all text to UTF-8 if it''s a string in Python3\. If you
    are using Python2, then there is an extra technical detail that you need to take
    care of: you have to encode all of the string that you are using in UTF-8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing some basic string processing and normalizing should make the model
    perform better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a simple function that can be used for normalizing your documents.
    This function is used in Python fastText notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using pandas to extract text from your dataset and clean it, you
    can also replace the missing text values in your dataset with an `_empty_` label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fastText command line implements two algorithms, `cbow` and `skip-gram`.
    Using the Python library, you should be able to train your models in both algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Training in fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training in fastText is done using the `train_unsupervised` function. You can
    choose which algorithm to use from the `model` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you can train a `skipgram` model using the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is similar to the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, to train a `cbow` model you can use the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The equivalent statement on the command line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between the Python code and the command line is that the command
    line will save the model in a file, while in the Python code, the model will be
    in memory, referenced by the variable. To save the model, you will need to pass
    explicit commands in your Python app, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to pass all the other training parameters as well. The parameters,
    as well as the default values, are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These parameters hold the same meaning that you have seen while exploring the
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lack of labels in the case of unsupervised learning makes evaluation a bit
    problematic as there is nothing to meaningfully compare the results of the model
    with. In the case of word embeddings, we have the same problem, but since this
    is a somewhat narrow domain, we can make some subjective claims. The fastText
    command line gives us the options of nearest neighbors and finding word similarities,
    which we can replicate in the Python library as we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques include using the syntactic and semantic performance of words
    based on the question—`words.txt` released by Google and the morphological similarity
    of rare words using the Stanford rare word database. Please keep in mind if you
    are creating word representations for a niche domain that these exact model evaluation
    techniques may not give good results, but the techniques should hold.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, the word vectors that are created are of 100 dimensions. They are
    saved in memory as NumPy arrays. So, you should be able to see the word vectors
    using the `get_word_vector` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Nearest neighbor queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally k nearest neighbors are used to rate differentiate between models.
    The vector representation of a target word is taken, the neighbors of the vectors
    are found and then it is seen if the neighbors are closer to its meaning. Since
    fastText representations are meant to be distributional, this assumption should
    hold true.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fastText command line gives us a tool to get the nearest neighbors easily,
    but there is no easy way to find them in Python. There is a `find_nearest_neighbor`
    function in `util`, but it takes vectors as input. Hence, we will need to write
    some code to create a function that takes in words and the target model, and gives
    back the nearest neighbors according to the model. You can take a look at `python
    fastText unsupervised learning.ipynb` for the code to get the nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output may be refined with some pre-normalization on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Word similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various ways to find the similarity between words. In the case of
    fastText, one way of finding the similarity between words is to find the cosine
    distance between the words in the vector space. However, this method will probably
    not find the similarity between synonyms and antonyms, and other minute language
    constructs, but will solely give you a similarity score based on the context in
    which they are used. The words "water" and "cup" do not necessarily have anything
    that is similar between the two, but in context they are generally taken together
    and hence you may find the similarity score between them to be high.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Python library, you can write a small function to get the cosine similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In essence, you find the word vectors of the two target words using the `get_word_vector` method, and
    then find the cosine between them.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the performance of the model using the rare words dataset that
    was released by Stanford NLP. Using the `compute_similarity` function that is
    shared in the examples folder, we can change the function a little bit so that
    it works in a Python app. The implementation of the function can be seen in the
    unsupervised notebook. Download the rare words dataset, the link to which you
    will find in the references, unzip it, and then pass the text file as the first
    argument and the model as the second argument. You should be able to see how well
    your model has been able to evaluate the rare words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Model visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing how the word vectors happen in space is an effective way to understand
    the distributional properties of the model. Since the dimensions of the vectors
    are quite high, you will need a good dimensionality reduction technique so that
    the vectors can be shown in a two-dimensional frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The t-SNE is popular technique for dimensionality reduction that is well suited
    for the visualization of high-dimensional datasets. The idea in this case is to
    keep similar words as close together as possible, while maximizing the distance
    between dissimilar words. The unsupervised notebook shows the code for the t-SNE
    model. In our case, we have taken some words and plotted them in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Words plotted on a graph
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, "water" and "cup" are together, as they are generally used in
    the same context. Another two vectors that are together are "drink" and "tea."
    Using t-SNE to understand your model will give you a good idea of how good your
    model is.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to unsupervised learning, the fastText library provides access to the
    internal API for running supervised learning as well. Hence, running the fastText
    supervised Python API will also create the same model, which can be trained using
    the command line app. The advantage is that you will be able to leverage all the
    Python data science tools available for building an NLP classifier.
  prefs: []
  type: TYPE_NORMAL
- en: To show how to leverage the fastText classifier can be trained in Python, you
    can take a look at the `python fastText supervised learning.ipynb` notebook in
    the code. The dataset consists of reviews of fine foods from Amazon and can be
    downloaded from the Kaggle website, the links for which are given in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing and normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data preprocessing and normalization steps are similar to what you have
    seen in the case of unsupervised learning. In this case though, the major difference
    is that you will need to prefix the label with the `__label__` prefix or a label
    prefix of your choice. Also, it has to be saved in the fastText file in a format
    that is similar to the fastText command line. Since this is a classifier, you
    will need to actually create two files, one for training and one for model validation.
    One of the popular ways to split a dataset into training and testing is using
    the scikit-learn `train_test_split` function.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the model, you will need to use the `train_supervised` method on the
    training file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is similar to running this on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The hyperparameters are the same as what you pass in the case of supervised
    learning. The only difference with the unsupervised case is that the default loss
    function is `softmax` instead of `ns` and there is an additional `label` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the case of unsupervised learning, the Python code will not save
    the model to a file but will save it to the variable that you defined, `su_model`
    in this case. This variable, `su_model`, is a Python NumPy matrix and hence we
    can manipulate it in standard ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'To save the model, you will need to invoke the `save_model` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can get the word vector of the word using the `get_word_vector` method,
    the sentence vector of a document using the `get_sentence_vector` method, and
    the predicted label of a model using the predict method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also perform predict probabilities on your test document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This is similar to this on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting the precision and recall of your model is similar to what was seen using
    the command line. Similar to the command line, you will need to pass the test
    file and the number of labels that you need to find the precision of and recall
    against.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, `test_file` contains the path to the test file, and
    the second argument is the number of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix is a nice way to visualize the performance of a supervised
    model, specifically a classifier. It also shines when understanding which classes
    are performing better in a multiclass classifier. When you are creating a confusion
    matrix, you are essentially describing the performance of the model classifier
    on a test set for which the ground truth is known. Since fastText supports a classifier,
    you can create a confusion matrix out of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to get the confusion matrix is shown in the supervised notebook. The `fasttext_confusion_matrix` function takes
    in a model variable, pandas test data, the label column name, and the text column
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The predicted labels are shown against the true values.
  prefs: []
  type: TYPE_NORMAL
- en: Gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gensim is a popular open source library for processing raw, unstructured human-generated
    text created by Radim Řehůřek. Some of the features that Gensim boasts are:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory independence is one of the core value propositions of Gensim, which is
    that it should be scalable and not hold all the document in the RAM. Hence, you
    will be able to train documents that are significantly larger than the memory
    of your machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gensim has efficient implementations of various popular vector space algorithms.
    There has been a recent implementation of fastText in gensim as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are IO/wrappers and converters around several popular data formats as
    well. Remember that fastText only supports UTF-8 formats and hence Gensim might
    be a good choice if you have data that is in different formats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different algorithms for similarity queries. So, you are not stuck with the
    ones that are available in fastText.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways you can use fastText through Gensim: Using the Gensim''s
    native implementation of fastText and by using Gensim''s wrapper over fastText.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how you can use Gensim to train a fastText model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a fastText model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the example that is shown here, we will be using the Lee Corpus for training
    your model. To get the required data, I would recommend that you clone the Gensim
    repository from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code examples shown here, we will be taking a look at Gensim fastText
    using the fake news dataset from Kaggle. First, download the data and clean the
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The first case we will take a look at is how to train the models using the
    fastText wrapper. To use the fastText wrapper, you will need to have fastText
    installed in your machine. You should have fastText installed if you have followed
    the instructions in [Chapter 1](part0021.html#K0RQ0-05950c18a75943d0a581d9ddc51f2755),
    *Introducing FastText*. This wrapper is deprecated though, and the recommendation
    is to use the Gensim implementation of fastText:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are interested in using the fastText implementation in Gensim, you will
    need to use the FastText class in `gensim.models`, which also, in addition to
    fastText, has word2vec and many other models that can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gensim supports the same hyperparameters that are supported in the native implementation
    of fastText. You should be able to set most of the hyperparameters that are there
    in the Facebook fastText implementation. The defaults are also mostly there already.
    Some differences are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sentences`: This can be a list of list of tokens. In general, a stream of
    tokens is recommended, such as `LineSentence` from the word2vec module as you
    have seen already. In the Facebook fastText library, this is given by the path
    to the file and is given by the `-input` parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_vocab_size`: This is to limit the RAM size. In case there are more unique
    words, then, this will prune the less frequent ones. This needs to be decided
    based on the RAM that you have. For example, if you have 2 GB memory, then the
    value of `max_vocab_size` is used as a parameter for that 2 GB of memory. Also,
    if you have not set it manually, then there is no limit set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cbow_mean`: There is a difference from the fastText command here. In the original
    implementation for cbow, the mean of the vectors is taken. But in this case, you
    have the option to use the sum by passing 0 and 1 in case you want to try out
    the mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_words`: This is the target size of the batches that are passed. The
    default value is 10,000\. This is similar to`-lrUpdateRate` in the command line,
    as the number of batches determines when the weights will be updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callbacks`: A list of callback functions to be executed at specific stages
    of the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no parallels for the `-supervised` and `-labels` parameters, as Gensim
    focuses on unsupervised learning only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model saving and loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gensim provides the save and load methods for all models, and this is implemented
    in the case of fastText as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading a binary fastText model can also be achieved using the `load_fasttext_format`
    class method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Gensim, you can check whether the words are present in the vocabulary and
    then get the word vectors for the words. Since fastText supports out-of-vector
    for the words, you should be able to get the word vectors even if the words are
    not present in the vocabulary. This will not work in cases where none of the character
    n-grams were present in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Gensim implements an unsupervised algorithm, there is no direct way of
    measuring how good the resulting model is. Evaluating models depends on your use
    case and how well it's working out in your end applications.
  prefs: []
  type: TYPE_NORMAL
- en: Gensim fastText has various methods that you can use for finding the similarity
    between words. The following results were received by loading the `wiki.simple.bin` model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to calculate the similarity between two words is using the
    `similarity` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: FastText computes sentence or document vectors only during supervised learning.
    Depending on the task, simple average word embeddings of all the normalized words
    in the sentence should suffice.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the similarities between two documents using the `n_similarity` method.
    According to the Gensim documentation, this method will give the cosine similarity
    between the two documents. Those documents need to be passed as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Gensim gives you ability to search for the most irrelevant document as well,
    kind of like finding the odd man out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `most_similar` method will give you the most similar words according to
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Gensim provides an easy-to-use method to evaluate the model on the WordSim
    353 benchmark. This dataset is a standard dataset for evaluating vector space
    models. There is no context around each word and the rating between the similarity
    of the words is on a scale of 0 to 10 in increasing order. You can find the file
    in `gensim/test/test_data/wordsim353.tsv` in the Gensim GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first result is the Pearson correlation coefficient (which is the normal
    correlation coefficient that we know of) and the second result is the Spearman
    coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `most_similar` method to get queries of the type of *A
    - B + C:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Similar to this type of syntactic and semantic similarity test, if you are creating
    word vectors in English, you can use the `question-words.txt` task that has been
    prepared and released by Google. You can find the text file in `gensim/docs/notebooks/datasets/question-words.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can run the following code. Also, set the logging to info so that
    you can get the accuracy in terms of percentages on the different fields. There
    are nine types of syntactic comparisons in the dataset, family, comparative, superlative,
    present-participle, nationality-adjective, past-tense, and plural:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This will give you an output and show you where there is a mismatch where the
    answers don't match with the list of words. You can evaluate based on that. If
    you are training for a different language, then one good investment may be to
    create a similar `question-words.txt` in the target language, based on different
    grammatical focal points in that language.
  prefs: []
  type: TYPE_NORMAL
- en: Word Mover's Distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Word Mover''s Distance** (**WMD**) is a good way of capturing two documents,
    even when there are no common words between them. Take a look at the following
    example. Words like **greet** and **speaks** are fairly near to each other if
    we consider the WMD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'In Gensim, you can find the distance between two documents using the `wmdistance`
    method, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You can initialize a word mover similarity class on your corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, `wmd_corpus` is your corpus and `model` is your trained fastText model.
    Now, you can run a query on the instance, which is simply a *lookup* on the class.
  prefs: []
  type: TYPE_NORMAL
- en: Getting more out of the training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are going through the model training process, you will also be interested
    in knowing the progress and performance of the model. Understanding how the model
    learns can be very helpful and makes it easier to debug the model and improve
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Another concern that may arise is training on large corpora. Training multiple
    epochs on large corpora may take a lot of time and hence you may want to save
    the model after the completion of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such scenarios, Gensim implements the callback parameter, which takes a
    sequence of subclasses of `CallbackAny2Vec` from `gensim.models.callbacks module`.
    Using this class, you can create classes that save the function at specific points
    in the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `EpochSaver` class saves the model at every ending of the epoch cycle. The
    `EpochLogger` class does two things. It prints the epoch start and stop, and whenever
    there is a batch begin cycle, it saves the similarity score to a list named similarity.
    We will use this list later for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, instantiate these classes and pass them to the model training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: When you run this code, you should be able to see the logger working and logging
    the epochs. Also, the different models will get saved onto disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how the similarity scores have progressed with the training, you can
    start a visdom server. Visdom is a visualization package by Facebook, which runs
    as a server. Its advantage is that you can send data to it, and the update parameters
    can be monitored using a web browser. To start a visdom server, you will need
    to have visdom installed and then you can run it from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can pass the similarity scores to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you open the server at `http://localhost:8097`, you should be able to see
    this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a generated visdom graph
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation using Gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to Mikolov''s 2013 paper, the link to which is given in the references,
    you can use the following method, which consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, monolingual models of languages are built using large amounts of text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A small bilingual dictionary is used to learn a linear projection between languages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So for the first step, you can simply use the fastText models that are prebuilt
    and shared on the [fasttext.cc](https://fasttext.cc/) website. In this section,
    we will take a look at how to implement the second step using Gensim.
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to train a translation matrix, which is essentially a linear transformation
    matrix that links the source word vectors and the target word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the transformation file from source language to target language;
    a good source is the Facebook muse documentation and your languages of interest
    may be listed there. If not, then you will need to put the effort into creating
    the transformation file yourself. In the example for this section, which you can
    find in the repo, the `en-it.txt` file was used for the English to Italian translation
    and it had 103,612 similar words, and hence you should probably create similar
    word transformation files for your models to be somewhat good in performance.
    Once you have the models and the transformation file, load the transformation
    file to a `word_pair` tuple and load the vectors to respective source target models.
    Once done, you can run code that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'At prediction time, for any given new word, we can map it to the other language
    space by computing *z* = *Wx*, and then we find the word that is closest in representation
    to the *z* vector in the target language space. The distance metric that is considered
    is the cosine similarity. This works similarly to the code shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see an output that looks similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the translations are convincing. The vectors are plotted on
    the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Vectors plotted on graph
  prefs: []
  type: TYPE_NORMAL
- en: Code on model training and assessing the visualizations in more detail is shown
    in the Jupyter notebook `gensim translation matrix with fasttext.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this, we have come to the end of this chapter, where we discussed how to
    perform training, validation, and prediction in a Python environment. To achieve
    that, we focused on two packages, the official fastText Python package and the
    Gensim package.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at how to integrate fastText into a
    machine learning or a deep learning pipeline.
  prefs: []
  type: TYPE_NORMAL
