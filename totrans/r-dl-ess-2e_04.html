<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training Deep Prediction Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">The previous chapters covered a bit of the theory behind neural networks and used some neural network packages in R. Now it is time to dive in and look at training deep learning models. In this chapter, we will explore how to train and build feedforward neural networks, which are the most common type of deep learning model. We will use MXNet to build deep learning models to perform classification and regression using a retail dataset.</p>
<p class="mce-root">This chapter will cover the following topics:</p>
<ul>
<li class="mce-root">Getting started with deep feedforward neural networks</li>
<li class="mce-root">Common activation functions <span>– </span>rectifiers, hyperbolic tangent, and maxout</li>
<li class="mce-root">Introduction to the MXNet deep learning library</li>
<li class="mce-root">Use case <span>– </span>Using MXNet for classification and regression</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with deep feedforward neural networks</h1>
                </header>
            
            <article>
                
<p>A deep feedforward neural network is designed to approximate a function, <em>f()</em>, that maps some set of input variables, <em>x</em>, to an output variable, <em>y</em>. They are called feedforward neural networks because information flows from the input through each successive layer as far as the output, and there are no feedback or recursive loops (models including both forward and backward connections are referred to as recurrent neural networks).</p>
<p class="mce-root">Deep feedforward neural networks are applicable to a wide range of problems, and are particularly useful for applications such as image classification. More generally, feedforward neural networks are useful for prediction and classification where there is a clearly defined outcome (what digit an image contains, whether someone is walking upstairs or walking on a flat surface, the presence/absence of disease, and so on).</p>
<p class="mce-root">Deep feedforward neural networks can be constructed by chaining layers or functions together. For example, a network with four hidden layers is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-580 image-border" src="assets/e278f4de-76ae-4249-bfba-ac2915bfdd11.png" style="width:19.33em;height:27.17em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 4.1: A d<span>eep feedforward neural network</span></div>
<p class="mce-root"><span>This diagram of the model is a directed acyclic graph. Represented as a function, the overall mapping from the input, <em>X</em>, to the output, <em>Y</em>, is a multilayered function. The first hidden layer is <em>H</em></span><em><sub>1</sub><span>=f</span><sup>(1)</sup><span>(X, w</span><sub>1</sub><span> a</span><sub>1</sub></em><span><em>)</em>, the second hidden layer is </span><em><span>H</span><sub>2</sub><span>=f</span><sup>(2)</sup><span>(H<sub>1</sub>, w</span><sub>2</sub><span> a</span><sub>2</sub><span>)</span></em><span>, and so on. These multiple layers can allow complex functions and transformations to be built up from relatively simple ones.</span></p>
<p class="mce-root"/>
<p class="mce-root">If sufficient hidden neurons are included in a layer, it can approximate to the desired degree of precision with many different types of functions. Feedforward neural networks can approximate non-linear functions by applying non-linear transformations between layers. These non-linear functions are known as activation functions, which we will cover in the next section.</p>
<p class="mce-root">The weights for each layer will be learned as the model is trained through forward- and backward-propagation. Another key piece of the model that must be determined is the cost, or loss, function. The two most commonly used cost functions are cross-entropy, which is used for classification tasks, and <strong>mean squared error</strong> (<strong>MSE</strong>), which is used for regression tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p class="mce-root">The activation function determines the mapping between input and a hidden layer. It defines the functional form for how a neuron gets activated. For example, a linear activation function could be defined as: <em>f(x) = x</em>, in which case the value for the neuron would be the raw input, <em>x</em>. A linear activation function is shown in the top panel of <em>Figure 4.2</em>. L<span>inear </span>activation functions are rarely used because in practice deep learning models would find it difficult to learn non-linear functional forms using l<span>inear </span><span>activation functions</span>. In previous chapters, we used the hyperbolic tangent as an activation function, namely <em>f(x) = tanh(x)</em>. Hyperbolic tangent can work well in some cases, but a potential limitation is that at either low or high values, it saturates, as shown in the middle panel of the figure  4.2.</p>
<p class="mce-root">Perhaps the most popular activation function currently, and a good first choice (Nair, V., and Hinton, G. E. (2010)), is known as a <em>rectifier</em>. There are different kinds of rectifiers, but the most common is defined by the <em>f(x) = max(0, x)</em> function, which is known as <strong>relu</strong>. The relu activation is flat below zero and linear above zero; an example is shown in Figure 4.2.</p>
<p class="mce-root">The final type of activation function we will discuss is maxout (Goodfellow, Warde­-Farley, Mirza, Courville, and Bengio (2013)). A maxout unit takes the maximum value of its input, although as usual, this is after weighting so it is not the case that the input variable with the highest value will always win. Maxout activation functions seem to work particularly well with dropout.</p>
<p class="mce-root"/>
<p class="mce-root">The relu activation is the most commonly-used activation function and it is the default option for the deep learning models in the rest of this book. The following graphs for some of the <span>activation functions we have discussed:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-581 image-border" src="assets/d1f135e3-40af-4b26-a5f3-0848bfd14096.png" style="width:80.25em;height:17.17em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 4.2: Common activation functions</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the MXNet deep learning library</h1>
                </header>
            
            <article>
                
<p>The deep learning libraries we will use in this book are MXNet, Keras, and TensorFlow. Keras is a frontend API, which means it is not a standalone library as it requires a lower-level library in the backend, usually TensorFlow. The advantage of using Keras rather than <span>TensorFlow </span>is that it has a simpler interface. We will use Keras in later chapters in this book.</p>
<p>Both MXNet and TensorFlow are multipurpose numerical computation libraries that can use GPUs for mass parallel matrix operations. As such, multi-dimensional matrices are central to both libraries. In R, we are familiar with the vector, which is a one-dimensional array of values of the same type. The R data frame is a two-dimensional array of values, where each column can have different types. The R matrix is a two-dimensional array of values with the same type. Some machine learning algorithms in R require a matrix as input. We saw an example of this in <a href="cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml">Chapter 2</a>, <em>Training a Prediction Model</em>, with the RSNSS package.</p>
<p class="mce-root"/>
<p class="mce-root">In R, it is unusual to use data structures with more than two dimensions, but deep learning uses them extensively. For example, if you have a 32 x 32 color image, you could store the pixel values in a 32 x 32 x 3 matrix, where the first two dimensions are the width and height, and the last dimension is for the red, green, and blue colors. This can be extended further by adding another dimension for a collection of images. This is called a batch and allows the processor (CPU/GPU) to process multiple images concurrently. The batch size is a hyper-parameter and the value selected depends on the size of the input data and memory capacity. If our batch size were 64, our matrix would be a 4-dimensional matrix of size 32 x 32 x 3 x 64 where the first 2 dimensions are the width and height, the third dimension is the colors, and the last dimension is the batch size, 64. The important thing to realize is that this is just another way of representing data. In R, we would store the same data as a 2-dimensional matrix (or dataframe) with 64 rows and 32 x 32 x 3 = 3,072 columns. All we are doing is reshaping the data, we are not changing it.</p>
<p class="mce-root">These n-dimensional matrices, which contain elements of the same type, are the cornerstone of using MXNet and TensorFlow. In MXNet, they are referred to as NDArrays. In TensorFlow, they are known as <strong>tensors</strong>. These n-dimensional matrices are important because they mean that we can feed the data into GPUs more efficiently; GPUs can process data in batches more efficiently than processing single rows of data. In the preceding example, we use 64 images in a batch, so the deep learning library will process input data in chunks of 32 x 32 x 3 x 64.</p>
<p><span>This chapter will use the MXNet deep learning library. MXNet originated at Carnegie Mellon University and is heavily supported by Amazon, they choose it as their default Deep Learning library in 2016. In 2017, MXNet was accepted as an Apache Incubator project, ensuring that it would remain as open source software. Here is a very simple example of an NDArray (matrix) operation in MXNet in R. If you have not already installed the MXNet package for R, go back to <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, for instructions, or use this link: <a href="https://mxnet.apache.org/install/index.html">https://mxnet.apache.org/install/index.html</a>:</span></p>
<pre>library(mxnet) # 1<br/>ctx = mx.cpu() # 2<br/>a &lt;- mx.nd.ones(c(2,3),ctx=ctx) # 3<br/>b &lt;- a * 2 + 1 # 4<br/>typeof(b) # 5<br/>[1] "externalptr"<br/>class(b) # 6<br/>[1] "MXNDArray"<br/>b # 7<br/>     [,1] [,2] [,3]<br/>[1,]    3    3    3<br/>[2,]    3    3    3</pre>
<p>We can break down this code line by line:</p>
<ul>
<li>Line 1 loads the MXNet package.</li>
<li>Line 2 sets the CPU context. This tells MXNet where to process your computations, either on the CPU or on a GPU, if one is available.</li>
<li>Line 3 creates a 2-dimensional NDArray of size 2 x 3 where each value is 1.</li>
<li>Line 4 creates another <span>2-dimensional NDArray of size 2 x 3. Each value will be 3 because we perform element-wise multiplication and add 1.</span></li>
<li>Line 5 shows that b is an external pointer.</li>
<li>Line 6 shows that the class of b is MXNDArray.</li>
<li>Line 7 displays the results.</li>
</ul>
<p><span>We can perform mathematical operations, such as multiplication and addition, on the </span><kbd>b</kbd><em><strong> </strong></em><span>variable. However, i</span>t is important to realize that, while this behaves similarly to an R matrix, it is not a native R object. We can see this when we output the type and class of this variable.</p>
<p>When developing deep learning models, there are usually two distinct steps. First you create the model architecture and then you train the model. The main reason for this is because most deep learning libraries employ s<span>ymbolic programming rather than the imperative programming you are used to. Most of the</span> code you have previously written in R is an imperative program, which executes code sequentially. For mathematical optimization tasks, such as deep learning, this may not be the most efficient method of execution. Most deep learning libraries, including MXNet and TensorFlow, use symbolic programming. For symbolic programming, a computation graph for the program execution is designed first. This graph is then compiled and executed. When the computation graph is generated, the input, output, and graph operations are already defined, meaning that the code can be optimized. This means that for deep learning, symbolic programs are usually more efficient than imperative programs.</p>
<p>Here is a simple example of the type of optimization using <span>symbolic programs:</span></p>
<p style="padding-left: 30px"><em>M = (M1 * M2) + (M3* M4)</em></p>
<p>An <span>imperative program would calculate this as follows:</span></p>
<p style="padding-left: 30px"><em>Mtemp1 = (M1 * M2)</em></p>
<p style="padding-left: 30px"><em>Mtemp2 = (M3* M4)</em></p>
<p style="padding-left: 30px"><em>M = Mtemp1 + Mtemp2</em></p>
<p>A symbolic program would first create a <span>computation graph, which might look like the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2905dc18-fd1f-43ca-8d79-a409fa5c3261.png" style="width:18.58em;height:16.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.3: Example of a computation graph</div>
<p><em>M1</em>, <em>M2</em>, <em>M3</em>, and <em>M4</em> are symbols that need to be operated on. <span>The graph shows the dependencies for operations; the <em>+</em> operation requires the two preceding multiplication operations to be done before it can execute. </span>But there is no dependency between the two multiplication steps, so these can be executed in parallel. This type of optimization means the code can execute much faster.</p>
<p class="mce-root">From a coding point of view, this means is that you have two steps in creating a deep learning model <span>– </span>first you define the architecture of the model and then you train the model. You create <em>layers </em>for your deep learning model and each layer has symbols that are placeholders. So for example, the first layer is usually:</p>
<pre style="padding-left: 30px">data &lt;- mx.symbol.Variable("data")</pre>
<p><kbd>data</kbd> is a placeholder for the input, which we will insert later. The output of each layer feeds into the next layer as input. This might be a convolutional layer, a dense layer, an activation layer a dropout layer, and so on. The following code example shows how the layers continue to feed into each other; this is taken from a full example later in this chapter. Notice how the symbol for each layer is used as input in the next layer, this is how the model is built layer after layer. The <kbd>data1</kbd> symbol is passed into the first call to <kbd>mx.symbol.FullyConnected</kbd>, the <kbd>fc1</kbd> symbol is passed into <span>the first call to <kbd>mx.symbol.Activation</kbd>, and so on.</span></p>
<pre>data &lt;- mx.symbol.Variable("data")<br/>fc1 &lt;- mx.symbol.FullyConnected(data, name="fc1", num_hidden=64)<br/>act1 &lt;- mx.symbol.Activation(fc1, name="activ1", act_type=activ)<br/><br/>drop1 &lt;- mx.symbol.Dropout(data=act1,p=0.2)<br/>fc2 &lt;- mx.symbol.FullyConnected(drop1, name="fc2", num_hidden=32)<br/>act2 &lt;- mx.symbol.Activation(fc2, name="activ2", act_type=activ)<br/><br/>.....<br/>softmax &lt;- mx.symbol.SoftmaxOutput(fc4, name="sm")</pre>
<p class="mce-root">When you execute this code, it runs instantly as nothing is executed at this stage. Eventually, you pass the last layer into a function to train the model. In MXNet, this is the <kbd>mx.model.FeedForward.create</kbd> function. At this stage, the computation graph is computed and the model begins to be trained:</p>
<pre>softmax &lt;- mx.symbol.SoftmaxOutput(fc4, name="sm")<br/>model &lt;- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,<br/>                                     ctx = devices,num.round = num_epochs,<br/>                                     ................</pre>
<p class="mce-root">This is when the deep learning model is created and trained. More information on the MXNet architecture is available online; the following links will get you started:</p>
<ul>
<li class="mce-root"><a href="https://mxnet.apache.org/tutorials/basic/symbol.html">https://mxnet.apache.org/tutorials/basic/symbol.html</a></li>
<li class="mce-root"><a href="https://mxnet.incubator.apache.org/architecture/program_model.html">https://mxnet.incubator.apache.org/architecture/program_model.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning layers</h1>
                </header>
            
            <article>
                
<p>In the earlier code snippets, we saw some layers for a deep learning model, including <kbd>mx.symbol.FullyConnected</kbd>, <kbd>mx.symbol.Activation</kbd>, and <kbd>mx.symbol.Dropout</kbd>. Layers are how models are constructed; they are computational transformations of data. For example, <kbd>mx.symbol.FullyConnected</kbd> is the first type of layer operation we matrix operation <span>we introduced in <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a>, <em>Getting Started with Deep Learning</em>. </span>It is <em>fully connected</em> because <span>all input values are connected to all nodes in the layer</span>. <span>In other deep learning libraries, such as Keras, it is called a </span><strong>dense</strong><span> layer.</span><span> </span></p>
<p>The <kbd>mx.symbol.Activation</kbd><span> layer performs an activation function on the output of the previous layer. The <kbd>mx.symbol.Dropout</kbd> layer performs dropout on the output from the previous layer. Other common layer types in MXNet are:</span></p>
<ul>
<li><span><span><kbd>mxnet.symbol.Convolution</kbd>: Performs a convolutional operation that matches patterns across the data. It is mostly used in computer vision tasks, which we will see in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapte</a><a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">r</a> <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">5</a>, <em>Image Classification Using Convolutional Neural Networks</em>. They can also be used for Natural Language Processing, which we will see in <a href="03f666ab-60ce-485a-8090-c158b29ef306.xhtml">Chapter 6</a>, <em>Natural Language Processing Using Deep Learning</em>.</span></span></li>
<li><kbd>mx.symbol.Pooling</kbd>: Performs pooling on the<span> output from the previous layer. Pooling reduces the number of elements by taking the average, or max value, from sections of the input. These are commonly used with convolutional layers.</span></li>
<li><kbd>mx.symbol.BatchNorm</kbd>: Used to normalize the weights from the previous layer. This is done for the same reason you normalize input data before model-building: it helps the model to train better. It also prevents vanishing and exploding gradients where gradients get very, very small or <span>very, very large during training. This can cause the model to fail to converge, that is, training will fail.</span></li>
<li><kbd>mx.symbol.SoftmaxOutput</kbd>: Calculates a softmax result from the<span> output from the previous layer.</span></li>
</ul>
<p>There are recognized patterns for using these layers, for example, an activation layer normally follows a fully-connected layer. A dropout layer is usually applied after the activation function, but can be between the fully connected layer and the activation function. Convolutional layers and pooling layers are often used together in image tasks in that order. At this stage, there is no need to try to memorize when to use these layers; you will encounter plenty of examples in the rest of this book!</p>
<div class="packt_infobox">If all this seems confusing, take some comfort in knowing that a lot of the difficult work in applying these layers is abstracted away from you. In the previous chapter, when we built a neural network, we had to manage all the input output from the layers. This meant ensuring that the matrix dimensions were correct so that the operations worked. Deep Learning libraries, such as MXNet and TensorFlow, take care of this for you.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a deep learning model</h1>
                </header>
            
            <article>
                
<p>Now that we have covered the basics, let's look at building our first true deep learning model! We will use the <kbd>UHI HAR</kbd> dataset that we used in <a href="cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml">Chapter 2</a>, <em>Training a Prediction Model</em>. The following code does some data preparation: it loads the data and selects only the columns that store mean values (those that have the word <kbd>mean</kbd> in the column name). The <kbd>y</kbd> variables are from 1 to 6; we will subtract one so that the range is 0 to 5. The code for this section is in <kbd>Chapter4/uci_har.R</kbd>. It requires the <span><kbd>UHI HAR</kbd> dataset to be in the data folder; download it from <a href="https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones">https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones</a> and unzip it into the data folder:</span></p>
<pre>train.x &lt;- read.table("../data/UCI HAR Dataset/train/X_train.txt")<br/>train.y &lt;- read.table("../data/UCI HAR Dataset/train/y_train.txt")[[1]]<br/>test.x &lt;- read.table("../data/UCI HAR Dataset/test/X_test.txt")<br/>test.y &lt;- read.table("../data/UCI HAR Dataset/test/y_test.txt")[[1]]<br/>features &lt;- read.table("../data/UCI HAR Dataset/features.txt")<br/>meanSD &lt;- grep("mean\\(\\)|std\\(\\)", features[, 2])<br/>train.y &lt;- train.y-1<br/>test.y &lt;- test.y-1</pre>
<p>Next, we will transpose the data and convert it into a matrix. MXNet expects the data to be width <kbd>x</kbd> height rather than height <kbd>x</kbd> width:</p>
<pre>train.x &lt;- t(train.x[,meanSD])<br/>test.x &lt;- t(test.x[,meanSD])<br/>train.x &lt;- data.matrix(train.x)<br/>test.x &lt;- data.matrix(test.x)</pre>
<p>The next step is to define the computation graph. We create a placeholder for the data and create two fully connected (or dense) layers followed by relu activations. The first layer has 64 nodes and the second layer has 32 nodes. We create a final <span>fully-connected layer with six nodes – the number of distinct classes in our y variable. We use a softmax activation to convert the numbers from the last six nodes into probabilities for each class:</span></p>
<pre>data &lt;- mx.symbol.Variable("data")<br/>fc1 &lt;- mx.symbol.FullyConnected(data, name="fc1", num_hidden=64)<br/>act1 &lt;- mx.symbol.Activation(fc1, name="relu1", act_type="relu")<br/>fc2 &lt;- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=32)<br/>act2 &lt;- mx.symbol.Activation(fc2, name="relu2", act_type="relu")<br/>fc3 &lt;- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=6)<br/>softmax &lt;- mx.symbol.SoftmaxOutput(fc3, name="sm")</pre>
<p>When you run the previous code, nothing actually executes. To train the model, we create a <kbd>devices</kbd> object to indicate where the code should be run, CPU or GPU. Then you pass the symbol for last layer (softmax) into the <kbd>mx.model.FeedForward.create</kbd> function. This function has other parameters, which are more properly known as hyper-parameters. These include the epochs (<kbd>num.round</kbd>), which control how many times we pass through the data, the learning rate (<kbd>learning.rate</kbd>), which controls how much the gradients are updated during each pass, momentum (<kbd>momentum</kbd>), which is a hyper-parameter that can help the model to train faster, and the weights initializer (<kbd>initializer</kbd>), which controls how the weights and biases for nodes are initially set. We also pass in the evaluation metric (<kbd>eval.metric</kbd>),which is how the model is to be evaluated, and a callback function (<kbd>epoch.end.callback</kbd>), which is used to output progress information. When we run the function, it trains the model and outputs the progress as per the value we used for the <span><kbd>epoch.end.callback</kbd> parameter, namely every epoch:</span></p>
<pre>devices &lt;- mx.cpu()<br/>mx.set.seed(0)<br/>tic &lt;- proc.time()<br/>model &lt;- mx.model.FeedForward.create(softmax, X = train.x, y = train.y,<br/>                                      ctx = devices,num.round = 20,<br/>                                      learning.rate = 0.08, momentum = 0.9,<br/>                                      eval.metric = mx.metric.accuracy,<br/>                                      initializer = mx.init.uniform(0.01),<br/>                                      epoch.end.callback =<br/>                                        mx.callback.log.train.metric(1))<br/>Start training with 1 devices<br/>[1] Train-accuracy=0.185581140350877<br/>[2] Train-accuracy=0.26104525862069<br/>[3] Train-accuracy=0.555091594827586<br/>[4] Train-accuracy=0.519127155172414<br/>[5] Train-accuracy=0.646551724137931<br/>[6] Train-accuracy=0.733836206896552<br/>[7] Train-accuracy=0.819100215517241<br/>[8] Train-accuracy=0.881869612068966<br/>[9] Train-accuracy=0.892780172413793<br/>[10] Train-accuracy=0.908674568965517<br/>[11] Train-accuracy=0.898572198275862<br/>[12] Train-accuracy=0.896821120689655<br/>[13] Train-accuracy=0.915544181034483<br/>[14] Train-accuracy=0.928879310344828<br/>[15] Train-accuracy=0.926993534482759<br/>[16] Train-accuracy=0.934401939655172<br/>[17] Train-accuracy=0.933728448275862<br/>[18] Train-accuracy=0.934132543103448<br/>[19] Train-accuracy=0.933324353448276<br/>[20] Train-accuracy=0.934132543103448<br/>print(proc.time() - tic)<br/>   user system elapsed <br/>   7.31 3.03 4.31 </pre>
<p>Now that we have trained our model, let's see how it does on the test set:</p>
<pre><br/>preds1 &lt;- predict(model, test.x)<br/>pred.label &lt;- max.col(t(preds1)) - 1<br/>t &lt;- table(data.frame(cbind(test.y,pred.label)),<br/>            dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test.y),2)<br/>print(t)<br/>      Predicted<br/>Actual   0   1   2   3   4   5<br/>     0 477  15   4   0   0   0<br/>     1 108 359   4   0   0   0<br/>     2  13  42 365   0   0   0<br/>     3   0   0   0 454  37   0<br/>     4   0   0   0 141 391   0<br/>     5   0   0   0  16   0 521<br/>print(sprintf(" Deep Learning Model accuracy = %1.2f%%",acc))<br/>[1] " Deep Learning Model accuracy = 87.11%"</pre>
<p>Not bad! We have achieved an accuracy of <kbd>87.11%</kbd> on our test set.</p>
<div class="packt_infobox">Wait, where are the backward propagation, derivatives, and so on, that we covered in previous chapters? The answer to that is deep learning libraries largely manage this automatically for you. In MXNet, automatic differentiation is included in a package called the autograd package, which differentiates a graph of operations with the chain rule. It is one less thing to worry about when building deep learning models. For more information, go to <a href="https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html">https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case – using MXNet for classification and regression</h1>
                </header>
            
            <article>
                
<p>In this section, we will use a new dataset to create a binary classification task. The dataset we will use here is a transactional dataset that is available at <a href="https://www.dunnhumby.com/sourcefiles">https://www.dunnhumby.com/sourcefiles</a>. This dataset has been made available from dunnhumby, which is perhaps best known for its link to the Tesco (a British grocery store) club-card, which is one of the largest retail loyalty systems in the world. I recommend the following book, which describes how dunnhumby helped Tesco to become the number one retailer by applying analytics to their retail loyalty program: <em>Humby, Clive, Terry Hunt, and Tim Phillips. Scoring points. Kogan Page Publishers, 2008</em>. Even though this book is relatively old, it remains one of the best use cases to describe how to roll out a business-transformation program based on data analytics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data download and exploration</h1>
                </header>
            
            <article>
                
<p>When you go to the preceding <span>link, there are a few different data options; the one we will use is called <strong>Let’s Get Sort-of-Real</strong>. This dataset is data for over two years for a fictional retail loyalty scheme. The data consists of purchases that are linked by basket ID and customer code, that is, we can track transactions by customers over time. There are a number of options here, including the full dataset, which is 4.3 GB zipped and over 40 GB unzipped. For our first models, we will use the smallest dataset, and will download the data titled <strong>All transactions for a randomly selected sample of 5,000 customers</strong>; this is 1/100<sup>th</sup> the size of the full database.</span></p>
<div class="packt_tip packt_infobox">I wish to thank dunnhumby for releasing this dataset and for allowing us permission to use it. One of the problems in deep learning and machine learning in general is the lack of large scale real-life datasets that people can practice their skills on. When a company makes the effort to release such a dataset, we should appreciate the effort and not use the dataset outside the terms and conditions specified. Please take the time to read the terms and conditions and use the dataset for personal learning purposes only. Remember that any misuse of this dataset (or datasets released by other companies) means that companies will be more reluctant to make other datasets available in the future.</div>
<p>Once you have read the terms and conditions and downloaded the dataset to your computer, unzip it into a directory called <kbd>dunnhumby/in</kbd> under the <kbd>code</kbd> folder. Ensure the files are unzipped directly under this folder, and not a sub-directory, or you may have to copy them after unzipping the data. The data files are in <strong>comma-delimited</strong> (<strong>CSV</strong>) format, with a separate file for each week of data. The files can be opened and viewed using a text editor. We will use some of the fields in <em>Table 4.1</em> for our analysis:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 138px">
<p><strong>Field name</strong></p>
</td>
<td style="width: 480px">
<p><strong>Description</strong></p>
</td>
<td style="width: 69px">
<p><strong>Format</strong></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>BASKET_ID</span></kbd></p>
</td>
<td style="width: 480px">
<p>Basket ID, or transaction ID. All items in a basket share the same <kbd>basket_id</kbd> value.</p>
</td>
<td style="width: 69px">
<p>Numeric</p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>CUST_CODE</span></kbd></p>
</td>
<td style="width: 480px">
<p>Customer Code. This links the transactions/visits to a customer.</p>
</td>
<td style="width: 69px">
<p>Char</p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>SHOP_DATE</span></kbd></p>
</td>
<td style="width: 480px">
<p>Date when shopping occurred. Date is specified in the yyyymmdd format.</p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>STORE_CODE</span></kbd></p>
</td>
<td style="width: 480px">
<p>Store code.</p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>QUANTITY</span></kbd></p>
</td>
<td style="width: 480px">
<p>Number of items of the same product bought in this basket.</p>
</td>
<td style="width: 69px">
<p><span>Numeric</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>SPEND</span></kbd></p>
</td>
<td style="width: 480px">
<p>Spend associated to the items bought.</p>
</td>
<td style="width: 69px">
<p><span>Numeric</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>PROD_CODE</span></kbd></p>
</td>
<td style="width: 480px">
<p>Product Code.</p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>PROD_CODE_10</span></kbd></p>
</td>
<td style="width: 480px">
<p>Product Hierarchy Level 10 Code.</p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>PROD_CODE_20</span></kbd></p>
</td>
<td style="width: 480px">
<p><span>Product Hierarchy </span><span>Level 20 Code.</span></p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>PROD_CODE_30</span></kbd></p>
</td>
<td style="width: 480px">
<p><span>Product Hierarchy </span><span>Level 30 Code.</span></p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
<tr>
<td style="width: 138px">
<p><kbd><span>PROD_CODE_40</span></kbd></p>
</td>
<td style="width: 480px">
<p><span>Product Hierarchy </span><span>Level 40 Code.</span></p>
</td>
<td style="width: 69px">
<p><span>Char</span></p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign">Table 4.1: <span>Partial d</span>ata dictionary for transactional dataset</div>
<p>The data stores details of customer transactions. Every unique item that a person purchases in a shopping transaction is represented by one line, and items in a transaction will have the same <kbd>BASKET_ID</kbd> field. A transaction can also be linked to a customer using the <kbd>CUST_CODE</kbd> field. <span>A PDF is included in the ZIP files i</span><span>f you want more information on the field types. </span></p>
<p>We are going to use this dataset for a churn prediction task. A churn prediction task is where we predict which customers will return in the next <kbd>x</kbd> days. Churn prediction is used to find customers who are in danger of leaving your program. It is used by companies in shopping loyalty schemes, mobile phone subscriptions, TV subscriptions, and so on to ensure they maintain enough customers. For most companies that rely on revenue from recurring subscriptions, <span>it is more effective to spend resources on maintaining their existing customer base than trying to acquire new customers. This is because of </span>the high cost of acquiring new customers. Also, as time progresses <span>after </span><span>a customer </span><span>has left</span>, it is harder to win them back, so there is a small window of time in which to send them special offers that may entice them to stay.</p>
<p>As well as binary classification, we will build a regression model. This will predict the amount that a person will spend in the next 14 days. Fortunately, we can build a dataset that is suitable for both prediction tasks.</p>
<p>The data was supplied as 117 CSV files (ignore <kbd>time.csv</kbd>, which is a lookup file). The first step is to perform some basic data exploration to verify that the data was downloaded successfully and then perform some basic data <span>quality checks.</span> This is an important first step in any analysis: especially when you are using an external dataset, you should run some validation checks on the data before creating any machine learning models. <span>The <kbd>Chapter4/0_Explore.Rmd</kbd></span><span> </span><span>script creates a summary file and does some exploratory analysis of the data. This is an</span> RMD <span>file, so it needs to be run from RStudio. For brevity, and because this book is about deep learning and not data processing, I will include just some of the output and plots from this script rather than reproducing all the code. You should also run the code in this file to ensure the data was imported correctly, although it may take a few minutes the first time it runs. Here are some summaries on the data from that script:</span></p>
<pre style="padding-left: 30px">Number of weeks we have data: 117.<br/>Number of transaction lines: 2541019.<br/>Number of transactions (baskets): 390320.<br/>Number of unique Customers: 5000.<br/>Number of unique Products: 4997.<br/>Number of unique Stores: 761.</pre>
<p>If we compare this to the website and the PDF, it looks in order. We have over 2.5 million records, and data for 5,000 customers across 761 stores. The data-exploration script also creates some plots to give us a feel for the data. <em>Figure 4.3</em> shows the sales over the 117 weeks; we see the variety in the data (it is not a flat line indicating that each day is different) and there are no gaps indicating missing data. There are seasonal patterns, with large peaks toward the end of the calendar year, namely the holiday season:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8165fb22-2fed-4e42-bfbc-46083cd50ba8.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.3: Sales plotted over time.</div>
<p>The plot in figure 4.3 shows that the data has been imported successfully. The data looks consistent and is what we expect for a retail transaction file, we do not see any gaps and there is seasonality.</p>
<p>For each item a person purchases, there is a product code (<kbd>PROD_CODE</kbd>) and four department codes <span>(</span><kbd>PROD_CODE_10</kbd><span>, </span><kbd>PROD_CODE_20</kbd><span>, </span><kbd>PROD_CODE_30</kbd><span>, </span><kbd>PROD_CODE_40</kbd><span>). We will use these department codes in our analysis; the code in <kbd>Chapter4/0_Explore.Rmd</kbd> creates a summary for them. We want to see how many unique values there are for each department code, whether the codes represent a hierarchy (each code has at most one parent), and whether there are repeated codes:</span></p>
<pre style="padding-left: 30px"><span>PROD_CODE: Number of unique codes: 4997. Number of repeated codes: 0.</span><br/><span>PROD_CODE_10: Number of unique codes:250. Number of repeated codes: 0.</span><br/><span>PROD_CODE_20: Number of unique codes:90. Number of repeated codes: 0.</span><br/><span>PROD_CODE_30: Number of unique codes:31. Number of repeated codes: 0.</span><br/><span>PROD_CODE_40: Number of unique codes:9.</span></pre>
<p>We have 4,997 unique product codes with 4 department codes. Our department codes go from <kbd>PROD_CODE_10</kbd>, which has 250 unique codes, to <kbd>PROD_CODE_40</kbd><span>, which has 9 unique codes. This is a product department code hierarchy, where <kbd>PROD_CODE_40</kbd> is the primary category and <kbd>PROD_CODE_10</kbd> is the lowest department code in the hierarchy. Each code in <kbd>PROD_CODE_10</kbd>,<em><strong> </strong></em><kbd>PROD_CODE_20</kbd>, and<em><strong> </strong></em></span><kbd>PROD_CODE_30</kbd> has only one parent; for example, there are no repeating codes, that is, a department code belongs in only one super-category. We are not given a lookup file to say what these codes represent, but an example of a product code hierarchy for a product might be something similar to this:</p>
<pre style="padding-left: 30px"><span>PROD_CODE_40</span> : Chilled goods<br/><span>  PROD_CODE_30</span> : Dairy<br/><span>    PROD_CODE_20 : Fresh Milk<br/></span><span>      PROD_CODE_10 : Full-fat Milk<br/></span><span>        PROD_CODE : Brand x Full-fat Milk</span></pre>
<p>To get a sense of these department codes, we can also plot the sales data over time by the number of unique product department codes in <em>Figure 4.4</em>. <span>This plot is also created in </span><span><kbd>Chapter4/0_Explore.Rmd</kbd>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6eabed35-9c9b-4357-951d-7fe5c1e250a5.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.4: Unique product codes purchased by date</div>
<p><span>Note that for this graph, the <em>y</em> axis is unique product codes, not sales. </span>T<span>his data also looks consistent; </span>there are some peaks and dips in the data, but they are not as pronounced as in <em>Figure 4.3</em>, which is as expected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data for our models</h1>
                </header>
            
            <article>
                
<p>Now that we have downloaded and validated the data, we can use it to create a dataset for our binary classification and regression model tasks. We want to be able to predict which customers will visit the shop in the next two weeks for the <span>binary classification task, and how much they will spend in the next two weeks for the regression task</span>. The <span><kbd>Chapter4/prepare_data.R</kbd> </span><span>script transforms the raw transactional data into a format suitable for machine learning. You need to run the code to create the dataset for the models, but you do not have to understand exactly how it works. Feel free to </span><span>skip ahead if you want to focus on the deep learning model building.</span></p>
<p><span>We need to transform our data into a suitable format for prediction tasks. This should be a single row for each instance we want to predict. The columns will include some fields that are features (<kbd>X</kbd> variables) and another field that is our predictor value (<kbd>Y</kbd> variable). We want to predict whether a customer returns or not and their spend, so our dataset will have a single row per customer with features and predictor variables.</span></p>
<p><span>The first step is to find the cut-off date that separates the variables used to predict (<kbd>X</kbd>) and the variable we will predict for (<kbd>Y</kbd>). The code looks at the data, finds the last transaction date; and then subtracts 13 days from that date. This is a cut-off date, we want to predict which customers will spend in our shops <em>on or after</em> the cut-off date; based on what happens <em>before</em> the cut-off date. The data before the cut-off date will be used to make our X, or feature variables, and sales data on or after the cut-off date will be used to make our Y, or predictor variables. The following is that part of the code:</span></p>
<pre>library(readr)<br/>library(reshape2)<br/>library(dplyr)<br/><br/>source("import.R")<br/><br/># step 1, merge files<br/>import_data(data_directory,bExploreData=0)<br/><br/># step 2, group and pivot data<br/>fileName &lt;- paste(data_directory,"all.csv",sep="")<br/>fileOut &lt;- paste(data_directory,"predict.csv",sep="")<br/>df &lt;- read_csv(fileName,col_types = cols(.default = col_character()))<br/><br/># convert spend to numeric field<br/>df$SPEND&lt;-as.numeric(df$SPEND)<br/><br/># group sales by date. we have not converted the SHOP_DATE to date<br/># but since it is in yyyymmdd format,<br/># then ordering alphabetically will preserve date order<br/>sumSalesByDate&lt;-df %&gt;%<br/>   group_by(SHOP_WEEK,SHOP_DATE) %&gt;%<br/>   summarise(sales = sum(SPEND)<br/>   )<br/><br/># we want to get the cut-off date to create our data model<br/># this is the last date and go back 13 days beforehand<br/># therefore our X data only looks at everything from start to max date - 13 days<br/># and our Y data only looks at everything from max date - 13 days to end (i.e. 14 days)<br/>max(sumSalesByDate$SHOP_DATE)<br/>[1] "20080706"<br/>sumSalesByDate2 &lt;- sumSalesByDate[order(sumSalesByDate$SHOP_DATE),]<br/>datCutOff &lt;- as.character(sumSalesByDate2[(nrow(sumSalesByDate2)-13),]$SHOP_DATE)<br/>datCutOff<br/>[1] "20080623"<br/>rm(sumSalesByDate,sumSalesByDate2)</pre>
<div class="packt_tip">If this code does not run, the most probable reason is that the source data was not saved in the correct location. The dataset must be unzipped into a directory called dunnhumby/in under the code folder, that is, at the same level as the chapter folders.</div>
<p><span>The last date in our data is <kbd>20080706</kbd>, which July 7<sup>th</sup>, 2008, and the cut-off date is June 23<sup>rd</sup>, 2008. Although we have data going back to 2006, we will only use sales data from 2008. Any data that is older than six months is unlikely to influence a future customer sale. The task is to predict whether a customer will return between June 23<sup>rd</sup>, 2008 - July 7<sup>th</sup>, 2008 based on their activity before June 23<sup>rd</sup>, 2008.</span></p>
<p><span>We now need to create features from our data; so that we can use the spend broken down by department code, we will use the <kbd>PROD_CODE_40</kbd> field. We could just group the sales on this department code, but that would give equal weighting to spends in Jan 2008 as to spends in June 2008. We would like to incorporate some time factor in our predictor columns. Instead, we will create features on a combination of the department code and the week</span><span>. This will allow our models to place more importance on recent activities. First, we group by customer code, week, and department code, and create the </span><kbd>fieldName</kbd><span> column. We then pivot this data to create </span><span>our features (<kbd>X</kbd>) dataset</span><span>.</span><span> The cell values in this dataset are the sales for that customer (row) and that week-department code (column). </span><span>Here is an example of how the data is transformed for two customers. <em>Table 4.2</em> shows the sales spend by week and the <kbd>PROD_CODE_40</kbd> field. <em>Table 4.3</em> then uses a pivot to create a dataset that has a single row per customer and the aggregate fields are now columns with the spend as values:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 23%">
<p><kbd><span>CUST_CODE</span></kbd></p>
</td>
<td style="width: 21%">
<p><kbd><span>PROD_CODE_40</span></kbd></p>
</td>
<td style="width: 19%">
<p><kbd>SHOP_WEEK</kbd></p>
</td>
<td style="width: 24.4745%">
<p><kbd>fieldName</kbd></p>
</td>
<td style="width: 23.5255%">
<p><kbd>Sales</kbd></p>
</td>
</tr>
<tr>
<td style="width: 23%">
<p><kbd>cust_001</kbd></p>
</td>
<td style="width: 21%">
<p>D00001</p>
</td>
<td style="width: 19%">
<p>200801</p>
</td>
<td style="width: 24.4745%">
<p><kbd><span>D00001_200801</span></kbd></p>
</td>
<td style="width: 23.5255%">
<p>10.00</p>
</td>
</tr>
<tr>
<td style="width: 23%">
<p><kbd>cust_002</kbd></p>
</td>
<td style="width: 21%">
<p><span>D00001</span></p>
</td>
<td style="width: 19%">
<p><span>200801</span></p>
</td>
<td style="width: 24.4745%">
<p><kbd><span>D00001_200801</span></kbd></p>
</td>
<td style="width: 23.5255%">
<p>12.00</p>
</td>
</tr>
<tr>
<td style="width: 23%">
<p><kbd><span>cust_001</span></kbd></p>
</td>
<td style="width: 21%">
<p><span>D00015</span></p>
</td>
<td style="width: 19%">
<p>200815</p>
</td>
<td style="width: 24.4745%">
<p><kbd><span>D00015_200815</span></kbd></p>
</td>
<td style="width: 23.5255%">
<p>15.00</p>
</td>
</tr>
<tr>
<td style="width: 23%">
<p><kbd><span>cust_001</span></kbd></p>
</td>
<td style="width: 21%">
<p><span>D00020</span></p>
</td>
<td style="width: 19%">
<p><span>200815</span></p>
</td>
<td style="width: 24.4745%">
<p><kbd><span>D00020_200815</span></kbd></p>
</td>
<td style="width: 23.5255%">
<p>20.00</p>
</td>
</tr>
<tr>
<td style="width: 23%">
<p><kbd><span>cust_002</span></kbd></p>
</td>
<td style="width: 21%">
<p><span>D00030</span></p>
</td>
<td style="width: 19%">
<p><span>200815</span></p>
</td>
<td style="width: 24.4745%">
<p><kbd><span>D00030_200815</span></kbd></p>
</td>
<td style="width: 23.5255%">
<p>25.00</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref">Table 4.2: Summary of sales by customer code, department code, and week</div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><kbd>CUST_CODE</kbd></p>
</td>
<td>
<p><kbd>D00001_200801</kbd></p>
</td>
<td>
<p><kbd>D00015_200815</kbd></p>
</td>
<td>
<p><kbd>D00020_200815</kbd></p>
</td>
<td>
<p><kbd>D00030_200815</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>cust_001</kbd></p>
</td>
<td>
<p>10.00</p>
</td>
<td>
<p>15.00</p>
</td>
<td>
<p>20.00</p>
</td>
<td/>
</tr>
<tr>
<td>
<p><kbd>cust_002</kbd></p>
</td>
<td>
<p>12.00</p>
</td>
<td/>
<td/>
<td>
<p>25.00</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Table 4.3: Data from Table 4.2 after transformation</span></div>
<p>Here is the code that does this transformation:</p>
<pre># we are going to limit our data here from year 2008 only<br/># group data and then pivot it<br/>sumTemp &lt;- df %&gt;%<br/>   filter((SHOP_DATE &lt; datCutOff) &amp; (SHOP_WEEK&gt;="200801")) %&gt;%<br/>   group_by(CUST_CODE,SHOP_WEEK,PROD_CODE_40) %&gt;%<br/>   summarise(sales = sum(SPEND)<br/>   )<br/>sumTemp$fieldName &lt;- paste(sumTemp$PROD_CODE_40,sumTemp$SHOP_WEEK,sep="_")<br/>df_X &lt;- dcast(sumTemp,CUST_CODE ~ fieldName, value.var="sales")<br/>df_X[is.na(df_X)] &lt;- 0</pre>
<p>The predictor<span> </span><span>(<kbd>Y</kbd>) </span>variable is a flag as to whether the customer visited the site on the weeks from <span><kbd>200818</kbd></span> to <span><kbd>200819</kbd>. </span><span>We perform a grouping on the data after the cut-off date and group the sales by customer, these form the basis of our <kbd>Y</kbd> values. We join the <kbd>X</kbd> and <kbd>Y</kbd> datasets, ensuring that we keep all rows on the <kbd>X</kbd> side by doing a left-join. Finally we create a 1/0 flag for our binary classification task. When we are done, we see that we have <kbd>3933</kbd> records in our dataset : <kbd>1560</kbd> customers who did not return and <kbd>2373</kbd> customers who did. We finish by saving our file for the model-building. The following code shows these steps:</span></p>
<pre># y data just needs a group to get sales after cut-off date<br/>df_Y &lt;- df %&gt;%<br/>   filter(SHOP_DATE &gt;= datCutOff) %&gt;%<br/>   group_by(CUST_CODE) %&gt;%<br/>   summarise(sales = sum(SPEND)<br/>   )<br/>colnames(df_Y)[2] &lt;- "Y_numeric"<br/><br/># use left join on X and Y data, need to include all values from X<br/># even if there is no Y value<br/>dfModelData &lt;- merge(df_X,df_Y,by="CUST_CODE", all.x=TRUE)<br/># set binary flag<br/>dfModelData$Y_categ &lt;- 0<br/>dfModelData[!is.na(dfModelData$Y_numeric),]$Y_categ &lt;- 1<br/>dfModelData[is.na(dfModelData$Y_numeric),]$Y_numeric &lt;- 0<br/>rm(df,df_X,df_Y,sumTemp)<br/><br/>nrow(dfModelData)<br/>[1] 3933<br/>table(dfModelData$Y_categ)<br/>   0    1 <br/>1560 2373 <br/><br/># shuffle data<br/>dfModelData &lt;- dfModelData[sample(nrow(dfModelData)),]<br/><br/>write_csv(dfModelData,fileOut)</pre>
<p class="mce-root">We use the sales data to create our predictor fields, but there were some customer attributes that we ignored for this task. These fields included <kbd>Customers Price Sensitivity</kbd> and <kbd>Customers Lifestage</kbd>. The main reason we did not use these fields was to avoid data-leakage. Data-leakage can occur when building prediction models; it occurs when some of your fields have values that will not be available or different when creating datasets in production. These fields could cause data-leakage because we do not know when they were set; it could have been when a customer signs up, or it could be a process that runs nightly. If these were created after our cut-off date, this would mean that these fields could unfairly predict our <kbd>Y</kbd> variables.</p>
<p class="mce-root">For example, <kbd>Customers Price Sensitivity</kbd> <span>has values for </span><kbd>Less Affluent</kbd>, <kbd>Mid Market</kbd>, and <kbd>Up Market</kbd>, which probably are derived from what the customer purchases. Therefore, using these fields in a churn-prediction task would result in data-leakage if these fields were updated after the cut-off date used to create our dataset for our prediction models. A value of <span><kbd>Up Market</kbd> for <kbd>Customers Price Sensitivity</kbd> could be strongly linked to return spend, but this value is actually a summary of the value it is predicting. </span>Data-leakage is one of the main causes of data models performing worse in production as the model was trained with data that is linked to the Y variable and can never exist in reality. You should always check for data-leakage for time-series tasks and ask yourself whether any field (especially lookup attributes) could have been modified after the date used to create the model data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The binary classification model</h1>
                </header>
            
            <article>
                
<p>The code from the previous section creates a new file called <kbd>predict.csv</kbd> in the <kbd>dunnhumby</kbd> folder. This dataset has a single row for each customer with a 0/1 field indicating whether they visited in the last two weeks and predictor variables based on sales data before those two weeks. Now <span>we can proceed to build some machine learning models. The <kbd>Chapter4/binary_predict.R</kbd> file contains the code for our first prediction task, binary classification. The first part of the code loads the data and creates an array of  predictor variables by including all columns except the customer ID, the binary classification predictor variable, and the regression predictor variable. The feature columns are all numeric fields that are heavily right-skewed distributed, so we apply a log transformation to those fields. We add <kbd>0.01</kbd> first to avoid getting a non-numeric result from attempting to get a log of a zero value <em>(log(0)= -Inf)</em>.</span></p>
<p><span>The following plot shows the data before transformation, on the left, and the data after transformation, on the right:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/024086e4-ca40-4310-977a-96187c710ba4.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 4.5: Distribution of a feature variable before and after transformation.</span></div>
<p><span>The large bar on the left in the second plot is where the original field was zero <em>(log(0+0.01) = -4.6)</em>. The following code loads the data, performs the log transformation, and creates the previous plot:</span></p>
<pre>set.seed(42)<br/>fileName &lt;- "../dunnhumby/predict.csv"<br/>dfData &lt;- read_csv(fileName,<br/>                    col_types = cols(<br/>                      .default = col_double(),<br/>                      CUST_CODE = col_character(),<br/>                      Y_categ = col_integer())<br/>                    )<br/>nobs &lt;- nrow(dfData)<br/>train &lt;- sample(nobs, 0.9*nobs)<br/>test &lt;- setdiff(seq_len(nobs), train)<br/>predictorCols &lt;- colnames(dfData)[!(colnames(dfData) %in% c("CUST_CODE","Y_numeric","Y_categ"))]<br/><br/># data is right-skewed, apply log transformation<br/>qplot(dfData$Y_numeric, geom="histogram",binwidth=10,<br/>       main="Y value distribution",xlab="Spend")+theme(plot.title = element_text(hjust = 0.5))<br/>dfData[, c("Y_numeric",predictorCols)] &lt;- log(0.01+dfData[, c("Y_numeric",predictorCols)])<br/>qplot(dfData$Y_numeric, geom="histogram",binwidth=0.5,<br/>       main="log(Y) value distribution",xlab="Spend")+theme(plot.title = element_text(hjust = 0.5))<br/>trainData &lt;- dfData[train, c(predictorCols)]<br/>testData &lt;- dfData[test, c(predictorCols)]<br/>trainData$Y_categ &lt;- dfData[train, "Y_categ"]$Y_categ<br/>testData$Y_categ &lt;- dfData[test, "Y_categ"]$Y_categ</pre>
<p><span>Before we train a deep learning model, we train three machine learning models – a logistic regression model, a <kbd>Random Forest</kbd> model, and an <kbd>XGBoost</kbd> model – on the data as a benchmark. This code section contains the data load, transformation, and three models:</span></p>
<pre>#Logistic Regression Model<br/>logReg=glm(Y_categ ~ .,data=trainData,family=binomial(link="logit"))<br/>pr &lt;- as.vector(ifelse(predict(logReg, type="response",<br/>                                testData) &gt; 0.5, "1", "0"))<br/># Generate the confusion matrix showing counts.<br/>t&lt;-table(dfData[test, c(predictorCols, "Y_categ")]$"Y_categ", pr,<br/>          dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test),2)<br/>print(t)<br/>      Predicted<br/>Actual   0   1<br/>     0 130  42<br/>     1  48 174<br/>print(sprintf(" Logistic regression accuracy = %1.2f%%",acc))<br/>[1] " Logistic regression accuracy = 77.16%"<br/>rm(t,pr,acc)<br/><br/>rf &lt;- randomForest::randomForest(as.factor(Y_categ) ~ .,<br/>                                  data=trainData,<br/>                                  na.action=randomForest::na.roughfix)<br/>pr &lt;- predict(rf, newdata=testData, type="class")<br/># Generate the confusion matrix showing counts.<br/>t&lt;-table(dfData[test, c(predictorCols, "Y_categ")]$Y_categ, pr,<br/>          dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test),2)<br/>print(t)<br/>      Predicted<br/>Actual   0   1<br/>     0 124  48<br/>     1  30 192<br/>print(sprintf(" Random Forest accuracy = %1.2f%%",acc))<br/>[1] " Random Forest accuracy = 80.20%"<br/>rm(t,pr,acc)<br/><br/>xgb &lt;- xgboost(data=data.matrix(trainData[,predictorCols]), label=trainData[,"Y_categ"]$Y_categ,<br/>                nrounds=75, objective="binary:logistic")<br/>pr &lt;- as.vector(ifelse(<br/>   predict(xgb, data.matrix(testData[, predictorCols])) &gt; 0.5, "1", "0"))<br/>t&lt;-table(dfData[test, c(predictorCols, "Y_categ")]$"Y_categ", pr,<br/>          dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test),2)<br/>print(t)<br/>      Predicted<br/>Actual   0   1<br/>     0 125  47<br/>     1  44 178<br/>print(sprintf(" XGBoost accuracy = %1.2f%%",acc))<br/>[1] " XGBoost accuracy = 76.90%"<br/>rm(t,pr,acc)</pre>
<p>We create logistic regression, <kbd>Random Forest</kbd>, and <kbd>XGBoost</kbd> models for a number of reasons. Firstly, most of the work is already done in preparing the data, so it<span> is trivial to do so</span><span>. Secondly, i</span>t gives us a benchmark to compare our deep learning model to. Thirdly, if there were a problem in the data-preparation tasks, these machine learning algorithms would highlight these problems more rapidly because they will be quicker than training a deep learning model. In this case, we only have a few thousand records, so these machine learning algorithms will easily run on this data. If the data were too large for these algorithms, I would consider taking a smaller sample and running our benchmark tasks on <span>that smaller </span>sample. There are many machine learning algorithms to choose from, but I used these algorithms as benchmarks for the following reasons:</p>
<ul>
<li class="mce-root">Logistic regression is a basic model and is always a good benchmark to use</li>
<li class="mce-root"><kbd>Random Forest</kbd> is known to train well using the default parameters and is robust to overfitting and correlated variables (which we have here)</li>
<li class="mce-root"><kbd>XGBoost</kbd> is consistently rated as the one of the best-performing machine learning algorithms</li>
</ul>
<p class="mce-root">All three algorithms achieve a similar amount of accuracy, the highest accuracy was achieved by <kbd>Random Forest</kbd> with an 80.2% accuracy. We now know that this dataset is suitable for prediction tasks and we have a benchmark to compare against.</p>
<p class="mce-root">Now we will build a deep learning model using MXNet:</p>
<pre>require(mxnet)<br/><br/># MXNet expects matrices<br/>train_X &lt;- data.matrix(trainData[, predictorCols])<br/>test_X &lt;- data.matrix(testData[, predictorCols])<br/>train_Y &lt;- trainData$Y_categ<br/><br/># hyper-parameters<br/>num_hidden &lt;- c(128,64,32)<br/>drop_out &lt;- c(0.2,0.2,0.2)<br/>wd=0.00001<br/>lr &lt;- 0.03<br/>num_epochs &lt;- 40<br/>activ &lt;- "relu"<br/><br/># create our model architecture<br/># using the hyper-parameters defined above<br/>data &lt;- mx.symbol.Variable("data")<br/>fc1 &lt;- mx.symbol.FullyConnected(data, name="fc1", num_hidden=num_hidden[1])<br/>act1 &lt;- mx.symbol.Activation(fc1, name="activ1", act_type=activ)<br/><br/>drop1 &lt;- mx.symbol.Dropout(data=act1,p=drop_out[1])<br/>fc2 &lt;- mx.symbol.FullyConnected(drop1, name="fc2", num_hidden=num_hidden[2])<br/>act2 &lt;- mx.symbol.Activation(fc2, name="activ2", act_type=activ)<br/><br/>drop2 &lt;- mx.symbol.Dropout(data=act2,p=drop_out[2])<br/>fc3 &lt;- mx.symbol.FullyConnected(drop2, name="fc3", num_hidden=num_hidden[3])<br/>act3 &lt;- mx.symbol.Activation(fc3, name="activ3", act_type=activ)<br/><br/>drop3 &lt;- mx.symbol.Dropout(data=act3,p=drop_out[3])<br/>fc4 &lt;- mx.symbol.FullyConnected(drop3, name="fc4", num_hidden=2)<br/>softmax &lt;- mx.symbol.SoftmaxOutput(fc4, name="sm")<br/><br/># run on cpu, change to 'devices &lt;- mx.gpu()'<br/># if you have a suitable GPU card<br/>devices &lt;- mx.cpu()<br/>mx.set.seed(0)<br/>tic &lt;- proc.time()<br/># This actually trains the model<br/>model &lt;- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,<br/>                                      ctx = devices,num.round = num_epochs,<br/>                                      learning.rate = lr, momentum = 0.9,<br/>                                      eval.metric = mx.metric.accuracy,<br/>                                      initializer = mx.init.uniform(0.1),<br/>                                      wd=wd,<br/>                                      epoch.end.callback = mx.callback.log.train.metric(1))<br/>print(proc.time() - tic)<br/>   user system elapsed <br/>   9.23 4.65 4.37 <br/><br/>pr &lt;- predict(model, test_X)<br/>pred.label &lt;- max.col(t(pr)) - 1<br/>t &lt;- table(data.frame(cbind(testData[,"Y_categ"]$Y_categ,pred.label)),<br/>            dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test),2)<br/>print(t)<br/>      Predicted<br/>Actual   0   1<br/>     0 136  36<br/>     1  54 168<br/>print(sprintf(" Deep Learning Model accuracy = %1.2f%%",acc))<br/>[1] " Deep Learning Model accuracy = 77.16%"<br/>rm(t,pr,acc)<br/>rm(data,fc1,act1,fc2,act2,fc3,act3,fc4,softmax,model)</pre>
<p>The deep learning model achieved a <kbd>77.16%</kbd> accuracy on the test data, which is only beaten by the <kbd>Random Forest</kbd> model. This shows that a deep learning model can be competitive against the best machine learning algorithms. It also shows that deep learning models on classification tasks do not always beat other machine learning algorithms. We used these models to provide a benchmark, so that we would know that our deep learning model was getting decent results; it gives us confidence that our deep learning model is competitive.</p>
<p class="mce-root">Our deep learning model uses 2<span>0% </span>dropout in each layer and weight decay for regularization. Without dropout, the model overtrained significantly. This was probably because the features are highly correlated, as our columns are the spend in various departments. It figures that if one column is for a type of bread, and another column is for a type of milk, then these change together, namely someone who has more transactions and spends more is likely to buy both.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The regression model</h1>
                </header>
            
            <article>
                
<p>The previous section developed a deep learning model for a binary classification task, this section develops a deep learning model to predict a continuous numeric value, regression analysis. We use the same dataset that we used for the binary classification task, but we use a different target column to predict for. In that task, we wanted to predict whether a customer would return to our stores in the next 14 days. In this task, we want to predict how much a customer will spend in our stores in the next 14 days. We follow a similar process; we load and prepare our dataset by applying log transformations to the data. The code is in <kbd>Chapter4/regression.R</kbd>:</p>
<pre>set.seed(42)<br/>fileName &lt;- "../dunnhumby/predict.csv"<br/>dfData &lt;- read_csv(fileName,<br/>                    col_types = cols(<br/>                      .default = col_double(),<br/>                      CUST_CODE = col_character(),<br/>                      Y_categ = col_integer())<br/> )<br/>nobs &lt;- nrow(dfData)<br/>train &lt;- sample(nobs, 0.9*nobs)<br/>test &lt;- setdiff(seq_len(nobs), train)<br/>predictorCols &lt;- colnames(dfData)[!(colnames(dfData) %in% c("CUST_CODE","Y_numeric","Y_numeric"))]<br/><br/>dfData[, c("Y_numeric",predictorCols)] &lt;- log(0.01+dfData[, c("Y_numeric",predictorCols)])<br/>trainData &lt;- dfData[train, c(predictorCols,"Y_numeric")]<br/>testData &lt;- dfData[test, c(predictorCols,"Y_numeric")]<br/><br/>xtrain &lt;- model.matrix(Y_numeric~.,trainData)<br/>xtest &lt;- model.matrix(Y_numeric~.,testData)</pre>
<p>We then perform regression analysis on the data using <kbd>lm</kbd> to create a <span>benchmark before creating a deep learning model:</span></p>
<pre># lm Regression Model<br/>regModel1=lm(Y_numeric ~ .,data=trainData)<br/>pr1 &lt;- predict(regModel1,testData)<br/>rmse &lt;- sqrt(mean((exp(pr1)-exp(testData[,"Y_numeric"]$Y_numeric))^2))<br/>print(sprintf(" Regression RMSE = %1.2f",rmse))<br/>[1] " Regression RMSE = 29.30"<br/>mae &lt;- mean(abs(exp(pr1)-exp(testData[,"Y_numeric"]$Y_numeric)))<br/>print(sprintf(" Regression MAE = %1.2f",mae))<br/>[1] " Regression MAE = 13.89"</pre>
<p>We output two metrics, rmse and mae, for our regression task. We covered these earlier in the chapter. <span>Mean absolute error measures</span><span> the absolute differences between the predicted value and the actual value. </span><strong>Root mean squared error</strong> (<strong>rmse</strong>) penalizes the square of the differences between the predicted value and the actual value, so one big error costs more than the sum of the small errors. <span>Now let's look at the deep learning regression code. First we load the data and define the model:</span></p>
<pre>require(mxnet)<br/>Loading required package: mxnet<br/><br/># MXNet expects matrices<br/>train_X &lt;- data.matrix(trainData[, predictorCols])<br/>test_X &lt;- data.matrix(testData[, predictorCols])<br/>train_Y &lt;- trainData$Y_numeric<br/><br/>set.seed(42)<br/># hyper-parameters<br/>num_hidden &lt;- c(256,128,128,64)<br/>drop_out &lt;- c(0.4,0.4,0.4,0.4)<br/>wd=0.00001<br/>lr &lt;- 0.0002<br/>num_epochs &lt;- 100<br/>activ &lt;- "tanh"<br/><br/># create our model architecture<br/># using the hyper-parameters defined above<br/>data &lt;- mx.symbol.Variable("data")<br/>fc1 &lt;- mx.symbol.FullyConnected(data, name="fc1", num_hidden=num_hidden[1])<br/>act1 &lt;- mx.symbol.Activation(fc1, name="activ1", act_type=activ)<br/>drop1 &lt;- mx.symbol.Dropout(data=act1,p=drop_out[1])<br/><br/>fc2 &lt;- mx.symbol.FullyConnected(drop1, name="fc2", num_hidden=num_hidden[2])<br/>act2 &lt;- mx.symbol.Activation(fc2, name="activ2", act_type=activ)<br/>drop2 &lt;- mx.symbol.Dropout(data=act2,p=drop_out[2])<br/><br/>fc3 &lt;- mx.symbol.FullyConnected(drop2, name="fc3", num_hidden=num_hidden[3])<br/>act3 &lt;- mx.symbol.Activation(fc3, name="activ3", act_type=activ)<br/>drop3 &lt;- mx.symbol.Dropout(data=act3,p=drop_out[3])<br/><br/>fc4 &lt;- mx.symbol.FullyConnected(drop3, name="fc4", num_hidden=num_hidden[4])<br/>act4 &lt;- mx.symbol.Activation(fc4, name="activ4", act_type=activ)<br/>drop4 &lt;- mx.symbol.Dropout(data=act4,p=drop_out[4])<br/><br/>fc5 &lt;- mx.symbol.FullyConnected(drop4, name="fc5", num_hidden=1)<br/>lro &lt;- mx.symbol.LinearRegressionOutput(fc5)<br/><br/></pre>
<p>Now we train the model; note that the first comment shows how to switch to using a GPU instead of a CPU:</p>
<pre># run on cpu, change to 'devices &lt;- mx.gpu()'<br/># if you have a suitable GPU card<br/>devices &lt;- mx.cpu()<br/>mx.set.seed(0)<br/>tic &lt;- proc.time()<br/># This actually trains the model<br/>model &lt;- mx.model.FeedForward.create(lro, X = train_X, y = train_Y,<br/> ctx = devices,num.round = num_epochs,<br/> learning.rate = lr, momentum = 0.9,<br/> eval.metric = mx.metric.rmse,<br/> initializer = mx.init.uniform(0.1),<br/> wd=wd,<br/> epoch.end.callback = mx.callback.log.train.metric(1))<br/>print(proc.time() - tic)<br/> user system elapsed <br/> 13.90 1.82 10.50 <br/><br/>pr4 &lt;- predict(model, test_X)[1,]<br/>rmse &lt;- sqrt(mean((exp(pr4)-exp(testData[,"Y_numeric"]$Y_numeric))^2))<br/>print(sprintf(" Deep Learning Regression RMSE = %1.2f",rmse))<br/>[1] " Deep Learning Regression RMSE = 28.92"<br/>mae &lt;- mean(abs(exp(pr4)-exp(testData[,"Y_numeric"]$Y_numeric)))<br/>print(sprintf(" Deep Learning Regression MAE = %1.2f",mae))<br/>[1] " Deep Learning Regression MAE = 14.33"<br/>rm(data,fc1,act1,fc2,act2,fc3,act3,fc4,lro,model)</pre>
<p><span>For regression metrics, lower is better, so our rmse metric on the deep learning model (28.92) is an improvement on the original regression model (29.30). Interestingly, the mae on the the deep learning model (14.33) is actually worse than the original regression model (13.89). Since rsme penalizes big differences between actual and predicted values more, this indicates that the errors in the deep learning model are less extreme than the regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the binary classification model</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>This section builds on the earlier binary classification task and looks to </span>increase the accuracy for that task. The first thing we can do to improve the model is to use more data, 100 times more data in fact! We will download the entire dataset, which is over 4 GB data in zip files and 40 GB of data when the files are unzipped. Go back to the download link (<a href="https://www.dunnhumby.com/sourcefiles">https://www.dunnhumby.com/sourcefiles</a>) and select <strong>Let’s Get Sort-of-Real</strong> again and download all the files for the <strong>Full dataset</strong>. There are nine files to download and the CSV files should be unzipped into the <kbd>dunnhumby/in</kbd> folder. Remember to check that the CSV files are in this folder and not a subfolder. You need to run the code in <kbd>Chapter4/prepare_data.R</kbd> again. When this completes, the <kbd>predict.csv</kbd> file should have 390,000 records.</p>
<div class="packt_tip">You can try to follow along here, but be aware that preparing the data and running the deep learning model are going to take a long time. You also may run into problems if you have a slow computer. I tested this code on an Intel i5 processor with 32 GB RAM, and it took the model 30 minutes to run. It also requires over 50 GB hard disk space to store the unzipped files and temporary files. If you have problems running it on your local computer, another option is to run this example in the cloud, which we will cover in a later chapter.</div>
<p class="mce-root">The code for this section is in the <span><kbd>Chapter4/binary_predict2.R</kbd> script. </span>Since we have more data, we can build a more complicated model. <span>We have 100 times more data, so our new model adds an extra layer, and more nodes to our hidden layers. We have decreased the amount of regularization and the learning rate. We have also added more epochs. Here is the the code in <kbd>Chapter4/binary_predict2.R</kbd>, which constructs and trains the deep learning model. We have not included the boilerplate code to load and prepare the data, as that has not changed from the original script:</span></p>
<pre># hyper-parameters<br/>num_hidden &lt;- c(256,128,64,32)<br/>drop_out &lt;- c(0.2,0.2,0.1,0.1)<br/>wd=0.0<br/>lr &lt;- 0.03<br/>num_epochs &lt;- 50<br/>activ &lt;- "relu"<br/><br/># create our model architecture<br/># using the hyper-parameters defined above<br/>data &lt;- mx.symbol.Variable("data")<br/>fc1 &lt;- mx.symbol.FullyConnected(data, name="fc1", num_hidden=num_hidden[1])<br/>act1 &lt;- mx.symbol.Activation(fc1, name="activ1", act_type=activ)<br/><br/>drop1 &lt;- mx.symbol.Dropout(data=act1,p=drop_out[1])<br/>fc2 &lt;- mx.symbol.FullyConnected(drop1, name="fc2", num_hidden=num_hidden[2])<br/>act2 &lt;- mx.symbol.Activation(fc2, name="activ2", act_type=activ)<br/><br/>drop2 &lt;- mx.symbol.Dropout(data=act2,p=drop_out[2])<br/>fc3 &lt;- mx.symbol.FullyConnected(drop2, name="fc3", num_hidden=num_hidden[3])<br/>act3 &lt;- mx.symbol.Activation(fc3, name="activ3", act_type=activ)<br/><br/>drop3 &lt;- mx.symbol.Dropout(data=act3,p=drop_out[3])<br/>fc4 &lt;- mx.symbol.FullyConnected(drop3, name="fc4", num_hidden=num_hidden[4])<br/>act4 &lt;- mx.symbol.Activation(fc4, name="activ4", act_type=activ)<br/><br/>drop4 &lt;- mx.symbol.Dropout(data=act4,p=drop_out[4])<br/>fc5 &lt;- mx.symbol.FullyConnected(drop4, name="fc5", num_hidden=2)<br/>softmax &lt;- mx.symbol.SoftmaxOutput(fc5, name="sm")<br/><br/># run on cpu, change to 'devices &lt;- mx.gpu()'<br/># if you have a suitable GPU card<br/>devices &lt;- mx.cpu()<br/>mx.set.seed(0)<br/>tic &lt;- proc.time()<br/># This actually trains the model<br/>model &lt;- mx.model.FeedForward.create(softmax, X = train_X, y = train_Y,<br/> ctx = devices,num.round = num_epochs,<br/> learning.rate = lr, momentum = 0.9,<br/> eval.metric = mx.metric.accuracy,<br/> initializer = mx.init.uniform(0.1),<br/> wd=wd,<br/> epoch.end.callback = mx.callback.log.train.metric(1))<br/>print(proc.time() - tic)<br/> user system elapsed <br/>1919.75 1124.94 871.31<br/><br/>pr &lt;- predict(model, test_X)<br/>pred.label &lt;- max.col(t(pr)) - 1<br/>t &lt;- table(data.frame(cbind(testData[,"Y_categ"]$Y_categ,pred.label)),<br/> dnn=c("Actual", "Predicted"))<br/>acc&lt;-round(100.0*sum(diag(t))/length(test),2)<br/>print(t)<br/>      Predicted<br/>Actual     0     1<br/> 0     10714  4756<br/> 1      3870 19649<br/>print(sprintf(" Deep Learning Model accuracy = %1.2f%%",acc))<br/>[1] " Deep Learning Model accuracy = 77.88%"</pre>
<p class="mce-root"><span>The accuracy has increased from <kbd>77.16%</kbd> in the earlier model to <kbd>77.88%</kbd> for this model. This may not seem significant, but if we consider that the large dataset has almost 390,000 rows, the increase in accuracy of 0.72% represents about 2,808 customers that are now classified correctly. If each of these customers is worth $50, that is an additional $140,000 in revenue.</span></p>
<p class="mce-root"><span>In general, as you add more data, your model should become more complicated to generalize across all the patterns in the data. We will cover more of this in <a href="13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml">Chapter 6</a>, <em>Tuning and Optimizing Models</em>, but I would encourage you to experiment with the code in <kbd>Chapter4/binary_predict.R</kbd>. Try changing the hyper-parameters or adding more layers. Even a small improvement of 0.1 - 0.2% in accuracy is significant. If you manage to get over 78% accuracy on this dataset, consider it a good achievement.</span></p>
<p>If you want to explore further, there are other methods to investigate. These involve making changes in how the data for the model is created. If you really want to stretch yourself, here are a few more ideas you can try:</p>
<ul>
<li>Our current features are a combination of department codes and weeks, we use the <kbd>PROD_CODE_40</kbd> <span>field as the department code. This has only nine unique values, so for every week, only nine fields represent that data. If you use </span><kbd>PROD_CODE_30</kbd><span>, </span><kbd>PROD_CODE_20</kbd>, <span>or</span> <kbd>PROD_CODE_10</kbd><span>, you will create a lot more features.</span></li>
<li>In a similar manner, rather than using a combination of <span>department codes and weeks, you could try department codes and day. This might create too many features, but I would consider doing this for the last 14 days before the cut-off date.</span></li>
<li>Experiment with different methods of preparing the data. We use log scale, which works well for our binary classification task, but is not the best method for a regression task, as it does not create data with a normal distribution. Try applying z-scaling and min-max standardization to the data. If you do this, you must ensure that it is applied correctly to the test data before evaluating the model.</li>
<li>The training data uses the sales amount. You could change this to item quantities or the number of transactions an item is in.</li>
<li>You could create new features. One potentially powerful example would be to create fields based on a day of the week, or a day of the month. We could create features for the spend amounts and number of visits for each day of the week.</li>
<li>We could create features based on the average size of a shopping basket, how frequently a customer visits, and so on.</li>
<li>We could try a different model architecture that can take advantage of time-series data.</li>
</ul>
<p>These are all things I would try if I was given this task as a work assignment. In <span>traditional machine learning, adding more features often leads to problems as most traditional machine learning algorithms struggle with high-dimensionality data. D</span>eep learning models can handle these cases, so there usually is no harm in adding more features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The unreasonable effectiveness of data</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Our first deep learning models on the binary classification task had fewer than 4,000 records.</span> <span>We did this so you could run the example quickly. </span>For deep learning, you really need a lot more data, so we created a more complicated model with a lot more data, which gave us an increase in accuracy. This process demonstrated the following:</p>
<ul>
<li class="mce-root">Establishing a baseline with other machine learning algorithms provides a good benchmark before using a deep learning model</li>
<li>We had to create a more complex model and adjust the hyper-parameters for our bigger dataset</li>
<li class="mce-root">The Unreasonable Effectiveness of Data</li>
</ul>
<p class="mce-root">The last point here is borrowed from an article by Peter Norvig, available at <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf</a>. There is also a YouTube video with the same name. One of the main points in Norvig's article is this: invariably simple models and a lot of data trump more elaborate models based on less data.</p>
<p class="mce-root">We have increased the accuracy on our deep learning model by 0.38%. Considering that our dataset has highly correlated variables and that our domain is modelling human activities, this is not bad. People are, well predictable; so when attempting to predict what they do next, a small dataset usually works. In other domains, adding more data has much more of an effect. Consider a complex image-recognition task with color images where the image quality and format are not consistent. In that case, increasing our training data by a factor of 10 would have much more of an effect than in the earlier example. For many deep learning projects, you should include tasks to acquire more data from the very beginning of the project. This can be done by manually labeling the data, by outsourcing <span>tasks </span>(Amazon Turk), or by building some form of feedback mechanism in your application.</p>
<p class="mce-root">While other machine learning algorithms may also see an improvement in <span>performance with more data, </span>eventually adding more data will stop making a difference and performance will stagnate. This is because these algorithms were never designed for large high-dimensional data and so cannot model the <span>complex patterns in very large datasets.</span> However, you can build increasingly complex deep learning architectures that can model these complex patterns. This following plot illustrates how deep learning algorithms can continue to take advantage of more data and performance can still improve after <span>performance on </span>other machine algorithms stagnates:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ae562de4-ee33-4e60-b895-149310aa23f1.png" style="width:30.67em;height:23.92em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 4.6: How model accuracy increases by dataset size for deep learning models versus other machine <span>learning models</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We covered a lot of ground in this chapter. We looked at activation functions and built our first true deep learning models using MXNet. Then we took a real-life dataset and created two use cases for applying a machine learning model. <span>The first use case was to predict which customers will return in the future based on their past activity. This was a binary classification task. The second use case was to predict how much a customer will spend in the future based on their past activity.</span> This was a regression task. We ran both models first on a small dataset and used different machine learning libraries to compare them against our deep learning model. Our deep learning model out-performed all of the algorithms.</p>
<p>We then took this further by using a dataset that was 100 times bigger. We built a larger deep learning model and adjusted our parameters to get an increase in our binary classification task accuracy. We finished the chapter with a brief discussion on how deep learning models out-perform traditional machine learning algorithms on large datasets.</p>
<p>In the next chapter, we will look at computer vision tasks, which deep learning has revolutionized.</p>


            </article>

            
        </section>
    </body></html>