["```py\n$ pip install ptan==0.8\n```", "```py\n>>> import numpy as np \n>>> import ptan \n>>> q_vals = np.array([[1, 2, 3], [1, -1, 0]]) \n>>> q_vals \narray([[ 1,  2,  3], \n      [ 1, -1,  0]]) \n>>> selector = ptan.actions.ArgmaxActionSelector() \n>>> selector(q_vals) \narray([2, 0])\n```", "```py\n>>> selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0, selector=ptan.actions.ArgmaxActionSelector()) \n>>> selector(q_vals) \narray([2, 0])\n```", "```py\n>>> selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0) \n>>> selector(q_vals) \narray([0, 1])\n```", "```py\n>>> selector.epsilon \n1.0 \n>>> selector.epsilon = 0.0 \n>>> selector(q_vals) \narray([2, 0])\n```", "```py\n>>> selector = ptan.actions.ProbabilityActionSelector() \n>>> for _ in range(10): \n...    acts = selector(np.array([ \n...        [0.1, 0.8, 0.1], \n...        [0.0, 0.0, 1.0], \n...        [0.5, 0.5, 0.0] \n...    ])) \n...    print(acts) \n... \n[0 2 1] \n[1 2 1] \n[1 2 1] \n[0 2 1] \n[2 2 0] \n[0 2 0] \n[1 2 1] \n[1 2 0] \n[1 2 1] \n[1 2 0]\n```", "```py\nclass DQNNet(nn.Module): \n    def __init__(self, actions: int): \n        super(DQNNet, self).__init__() \n        self.actions = actions \n\n    def forward(self, x): \n        # we always produce diagonal tensor of shape \n        # (batch_size, actions) \n        return torch.eye(x.size()[0], self.actions)\n```", "```py\n>>> net = DQNNet(actions=3) \n>>> net(torch.zeros(2, 10)) \ntensor([[1., 0., 0.], \n       [0., 1., 0.]])\n```", "```py\n>>> selector = ptan.actions.ArgmaxActionSelector() \n>>> agent = ptan.agent.DQNAgent(model=net, action_selector=selector) \n>>> agent(torch.zeros(2, 5)) \n(array([0, 1]), [None, None])\n```", "```py\n>>> selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0) \n>>> agent = ptan.agent.DQNAgent(model=net, action_selector=selector) \n>>> agent(torch.zeros(10, 5))[0] \narray([2, 0, 0, 0, 1, 2, 1, 2, 2, 1])\n```", "```py\n>>> selector.epsilon = 0.5 \n>>> agent(torch.zeros(10, 5))[0] \narray([0, 1, 2, 2, 0, 0, 1, 2, 0, 2]) \n>>> selector.epsilon = 0.1 \n>>> agent(torch.zeros(10, 5))[0] \narray([0, 1, 2, 0, 0, 0, 0, 0, 0, 0])\n```", "```py\nclass PolicyNet(nn.Module): \n    def __init__(self, actions: int): \n        super(PolicyNet, self).__init__() \n        self.actions = actions \n\n    def forward(self, x): \n        # Now we produce the tensor with first two actions \n        # having the same logit scores \n        shape = (x.size()[0], self.actions) \n        res = torch.zeros(shape, dtype=torch.float32) \n        res[:, 0] = 1 \n        res[:, 1] = 1 \n        return res\n```", "```py\n>>> net = PolicyNet(actions=5) \n>>> net(torch.zeros(6, 10)) \ntensor([[1., 1., 0., 0., 0.], \n       [1., 1., 0., 0., 0.], \n       [1., 1., 0., 0., 0.], \n       [1., 1., 0., 0., 0.], \n       [1., 1., 0., 0., 0.], \n       [1., 1., 0., 0., 0.]])\n```", "```py\n>>> selector = ptan.actions.ProbabilityActionSelector() \n>>> agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True) \n>>> agent(torch.zeros(6, 5))[0] \narray([2, 1, 2, 0, 2, 3])\n```", "```py\n>>> torch.nn.functional.softmax(torch.tensor([1., 1., 0., 0., 0.])) \ntensor([0.3222, 0.3222, 0.1185, 0.1185, 0.1185])\n```", "```py\nclass ToyEnv(gym.Env): \n    def __init__(self): \n        super(ToyEnv, self).__init__() \n        self.observation_space = gym.spaces.Discrete(n=5) \n        self.action_space = gym.spaces.Discrete(n=3) \n        self.step_index = 0 \n\n    def reset(self): \n        self.step_index = 0 \n        return self.step_index, {} \n\n    def step(self, action: int): \n        is_done = self.step_index == 10 \n        if is_done: \n            return self.step_index % self.observation_space.n, 0.0, is_done, False, {} \n        self.step_index += 1 \n        return self.step_index % self.observation_space.n, float(action), \\ \n            self.step_index == 10, False, {}\n```", "```py\nclass DullAgent(ptan.agent.BaseAgent): \n    def __init__(self, action: int): \n        self.action = action \n\n    def __call__(self, observations: tt.List[int], state: tt.Optional[list] = None) -> \\ \n            tt.Tuple[tt.List[int], tt.Optional[list]]: \n        return [self.action for _ in observations], state\n```", "```py\n>>> from lib import * \n>>> env = ToyEnv() \n>>> agent = DullAgent(action=1) \n>>> exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2) \n>>> for idx, exp in zip(range(3), exp_source): \n...    print(exp) \n... \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n```", "```py\n>>> for idx, exp in zip(range(15), exp_source): \n...    print(exp) \n... \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) \n....... \n(Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=True)) \n(Experience(state=4, action=1, reward=1.0, done_trunc=True),) \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))\n```", "```py\n>>> exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4) \n>>> next(iter(exp_source)) \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))\n```", "```py\n>>> exp_source = ptan.experience.ExperienceSource(env=[ToyEnv(), ToyEnv()], agent=agent, steps_count=4) \n>>> for idx, exp in zip(range(5), exp_source): \n...    print(exp) \n... \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False)) \n(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False), Experience(state=0, action=1, reward=1.0, done_trunc=False))\n```", "```py\n>>> exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1) \n>>> for idx, exp in zip(range(11), exp_source): \n...    print(exp) \n... \nExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) \nExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) \nExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) \nExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) \nExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0) \nExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) \nExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) \nExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) \nExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) \nExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None) \nExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n```", "```py\n>>> exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=2) \n>>> for idx, exp in zip(range(11), exp_source): \n...    print(exp) \n... \nExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2) \nExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3) \nExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4) \nExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0) \nExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1) \nExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2) \nExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3) \nExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4) \nExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None) \nExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None) \nExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n```", "```py\nExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None) \nExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n```", "```py\n>>> exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1) \n>>> buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100) \n>>> len(buffer) \n0 \n>>> buffer.populate(1) \n>>> len(buffer) \n1\n```", "```py\n>>> for step in range(6): \n...    buffer.populate(1) \n...    if len(buffer) < 5: \n...        continue \n...    batch = buffer.sample(4) \n...    print(f\"Train time, {len(batch)} batch samples\") \n...    for s in batch: \n...        print(s) \n... \nTrain time, 4 batch samples \nExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) \nExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) \nExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) \nExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) \nTrain time, 4 batch samples \nExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) \nExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0) \nExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) \nExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n```", "```py\nclass DQNNet(nn.Module): \n    def __init__(self): \n        super(DQNNet, self).__init__() \n        self.ff = nn.Linear(5, 3) \n    def forward(self, x): \n        return self.ff(x)\n```", "```py\n>>> net = DQNNet() \n>>> net \nDQNNet( \n  (ff): Linear(in_features=5, out_features=3, bias=True) \n) \n>>> tgt_net = ptan.agent.TargetNet(net)\n```", "```py\n>>> net.ff.weight \nParameter containing: \ntensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], \n       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], \n       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True) \n>>> tgt_net.target_model.ff.weight \nParameter containing: \ntensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], \n       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], \n       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True)\n```", "```py\n>>> net.ff.weight.data += 1.0 \n>>> net.ff.weight \nParameter containing: \ntensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], \n       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], \n       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True) \n>>> tgt_net.target_model.ff.weight \nParameter containing: \ntensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], \n       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], \n       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True)\n```", "```py\n>>> tgt_net.sync() \n>>> tgt_net.target_model.ff.weight \nParameter containing: \ntensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], \n       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], \n       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True)\n```", "```py\n>>> net.ff.weight.data += 1.0 \n>>> net.ff.weight \nParameter containing: \ntensor([[2.2039, 2.1487, 2.4420, 1.9790, 1.7274], \n       [1.7980, 1.9213, 2.2852, 1.8435, 2.4012], \n       [1.9431, 1.5816, 1.6342, 2.4212, 2.3647]], requires_grad=True) \n>>> tgt_net.target_model.ff.weight \nParameter containing: \ntensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], \n       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], \n       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True) \n>>> tgt_net.alpha_sync(0.1) \n>>> tgt_net.target_model.ff.weight \nParameter containing: \ntensor([[2.1039, 2.0487, 2.3420, 1.8790, 1.6274], \n       [1.6980, 1.8213, 2.1852, 1.7435, 2.3012], \n       [1.8431, 1.4816, 1.5342, 2.3212, 2.2647]], requires_grad=True)\n```", "```py\n net = Net(obs_size, HIDDEN_SIZE, n_actions) \n    tgt_net = ptan.agent.TargetNet(net) \n    selector = ptan.actions.ArgmaxActionSelector() \n    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=selector) \n    agent = ptan.agent.DQNAgent(net, selector) \n    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA) \n    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n```", "```py\n while True: \n        step += 1 \n        buffer.populate(1) \n\n        for reward, steps in exp_source.pop_rewards_steps(): \n            episode += 1 \n            print(f\"{step}: episode {episode} done, reward={reward:.2f}, \" \n                  f\"epsilon={selector.epsilon:.2f}\") \n            solved = reward > 150 \n        if solved: \n            print(\"Whee!\") \n            break \n        if len(buffer) < 2*BATCH_SIZE: \n            continue \n        batch = buffer.sample(BATCH_SIZE)\n```", "```py\n batch = buffer.sample(BATCH_SIZE) \n        states_v, actions_v, tgt_q_v = unpack_batch(batch, tgt_net.target_model, GAMMA) \n        optimizer.zero_grad() \n        q_v = net(states_v) \n        q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1) \n        loss_v = F.mse_loss(q_v, tgt_q_v) \n        loss_v.backward() \n        optimizer.step() \n        selector.epsilon *= EPS_DECAY \n\n        if step % TGT_NET_SYNC == 0: \n            tgt_net.sync()\n```", "```py\n@torch.no_grad() \ndef unpack_batch(batch: tt.List[ExperienceFirstLast], net: Net, gamma: float): \n    states = [] \n    actions = [] \n    rewards = [] \n    done_masks = [] \n    last_states = [] \n    for exp in batch: \n        states.append(exp.state) \n        actions.append(exp.action) \n        rewards.append(exp.reward) \n        done_masks.append(exp.last_state is None) \n        if exp.last_state is None: \n            last_states.append(exp.state) \n        else: \n            last_states.append(exp.last_state) \n\n    states_v = torch.as_tensor(np.stack(states)) \n    actions_v = torch.tensor(actions) \n    rewards_v = torch.tensor(rewards) \n    last_states_v = torch.as_tensor(np.stack(last_states)) \n    last_state_q_v = net(last_states_v) \n    best_last_q_v = torch.max(last_state_q_v, dim=1)[0] \n    best_last_q_v[done_masks] = 0.0 \n    return states_v, actions_v, best_last_q_v * gamma + rewards_v\n```", "```py\nChapter07$ python 06_cartpole.py \n26: episode 1 done, reward=25.00, epsilon=1.00 \n52: episode 2 done, reward=26.00, epsilon=0.82 \n67: episode 3 done, reward=15.00, epsilon=0.70 \n80: episode 4 done, reward=13.00, epsilon=0.62 \n112: episode 5 done, reward=32.00, epsilon=0.45 \n123: episode 6 done, reward=11.00, epsilon=0.40 \n139: episode 7 done, reward=16.00, epsilon=0.34 \n148: episode 8 done, reward=9.00, epsilon=0.31 \n156: episode 9 done, reward=8.00, epsilon=0.29 \n... \n2481: episode 113 done, reward=58.00, epsilon=0.00 \n2544: episode 114 done, reward=63.00, epsilon=0.00 \n2594: episode 115 done, reward=50.00, epsilon=0.00 \n2786: episode 116 done, reward=192.00, epsilon=0.00 \nWhee!\n```"]