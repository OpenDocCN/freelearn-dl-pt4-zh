- en: Reinforcement Learning and Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a concise explanation of the basic terminology and concepts
    in reinforcement learning. It will give you a good understanding of the basic
    reinforcement learning framework for developing artificial intelligent agents.
    This chapter will also introduce deep reinforcement learning and provide you with
    a flavor of the types of advanced problems the algorithms enable you to solve.
    You will find mathematical expressions and equations used in quite a few places
    in this chapter. Although there's enough theory behind reinforcement learning
    and deep reinforcement learning to fill a whole book, the key concepts that are
    useful for practical implementation are discussed in this chapter, so that when
    we actually implement the algorithms in Python to train our agents, you can clearly
    understand the logic behind them. It is perfectly alright if you are not able
    to grasp all of it in your first pass. You can always come back to this chapter
    and revise whenever you need a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is reinforcement learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov Decision Process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reinforcement learning framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep reinforcement learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do deep reinforcement learning agents work in practice?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is reinforcement learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are new to the field of **Artificial Intelligence** (**AI**) or machine
    learning, you might be wondering what reinforcement learning is all about. In
    simple terms, it is learning through reinforcement. *Reinforcement*, as you know
    from general English or psychology, is the act of increasing or strengthening
    the choice to take a particular action in response to something, because of the
    perceived benefit of receiving higher rewards for taking that action. We humans
    are good at learning through reinforcement from a very young age. Those who have
    kids may be utilizing this fact more often to teach good habits to them. Nevertheless,
    we will all be able to relate to this, because not so long ago we all went through
    that phase of life! Say parents reward their kid with chocolate if the kid completes
    their homework on time after school every day. The kid *learns* the fact that
    he/she will receive chocolate (*a **reward*) if he/she completes their homework
    every day. Therefore, this strengthens their decision to finish their homework
    every day to receive the chocolate. This process of learning to strengthen a particular
    choice of action, motivated by the reward they will receive for taking such an
    action, is called learning by reinforcement or reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be thinking, "*Oh yeah. That human psychology sounds very familiar
    to me. But what has that got to do with machine learning or AI?"* Good thought.
    The concept of reinforcement learning was in fact inspired by behavioral psychology.
    It is at the nexus of several fields of research, the most important being computer
    science, mathematics, neuroscience, and psychology, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we will soon realize, reinforcement learning is one of the most promising
    methods in machine learning leading towards AI. If all these terms are new to
    you, do not worry! Starting from the next paragraph, we will go over these terms
    and understand their relationship with each other to make you feel comfortable.
    If you already know these terms, it will be a refreshing read with a different
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what AI means and what's in it in an intuitive way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The intelligence demonstrated by humans and animals is called *natural intelligence*,
    but the intelligence demonstrated by machines is called AI, for obvious reasons.
    We humans develop algorithms and technologies that provide intelligence to machines.
    Some of the greatest developments on this front are in the fields of machine learning,
    artificial neural networks, and deep learning. These fields collectively drive
    the development of AI. There are three main types of machine learning paradigms
    that have been developed to some reasonable level of maturity so far, and they
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, you can get an intuitive picture of the field of
    AI. You can see that these learning paradigms are subsets of the field of machine
    learning, and machine learning itself is a subset/branch of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning is similar to how we would teach a kid to recognize someone
    or some object by name. We provide an input and a name/class label (*label* for
    short) associated with that input, and expect the machine to learn that input-to-label
    mapping. This might sound simple if we just want the machine to learn the input-to-label
    mapping for a few objects (like in object recognition-type tasks) or persons (like
    in face/voice/person recognition tasks), but what if we want a machine to learn
    about several thousand classes where each class may have several different variations
    in the input? For example, if the task is to recognize a person's face from image
    inputs, with a thousand other input images with faces to distinguish it from,
    the task might be complicated even for an adult. There might be several variations
    in the input images for the same person's face. The person may be wearing glasses
    in one of the input images, or wearing a hat in another, or sporting a different
    facial expression altogether. It is a much harder task for a machine to be able
    to see the input image, identify the face, and recognize it. With recent advancements
    in the field of deep learning, supervised classification tasks like these are
    no longer hard for machines. Machines can recognize faces, among several other
    things, at an unprecedented level of accuracy. For example, the DeepFace system
    ([https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)),
    developed by the Facebook AI research lab, reached an accuracy of 97.45% in face
    recognition on the Labelled Faces in the Wild dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unsupervised learning is a form of learning in which there is no label provided
    to the learning algorithm along with the inputs, unlike the supervised learning
    paradigm. This class of learning algorithm is typically used to figure out patterns
    in the input data and cluster similar data together. A recent advancement in the
    field of deep learning introduced a new form of learning called Generative Adversarial
    Networks, which have gained massive popularity during the time this book was being
    written. If you are interested, you can learn a lot more about Generative Adversarial
    Networks from this video: [https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video](https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video).'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is kind of a hybrid way of learning compared to supervised
    and unsupervised learning. As we learned at the start of this section, reinforcement
    learning is driven by a reward signal. In the case of the *kid with their homework*
    problem, the reward signal was the chocolate from their parents. In the machine
    learning world, a chocolate may not be enticing for a computer (well, we could
    program a computer to want chocolates, but why would we? Aren't kids enough?!),
    but a mere scalar value (a number) will do the trick! The reward signals are still
    human-specified in some way, signifying the intended goal of the task. For example,
    to train an agent to play Atari games using reinforcement learning, the scores
    from the games can be the reward signal. This makes reinforcement learning much
    easier (for humans and not for the machine!) because we don't have to label the
    button to be pressed at each point in the game to teach the machine how to play
    the game. Instead, we just ask the machine to learn on its own to maximize their
    score. Doesn't it sound fascinating that we could make a machine figure out how
    to play a game, or how to control a car, or how to do its homework all by itself,
    and all we have to do is just say how it did with a score? That is why we are
    learning about it in this chapter. You will develop some of those cool machines
    yourself in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Practical reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have an intuitive understanding of what AI really means and the
    various classes of algorithm that drive its development, we will now focus on
    the practical aspects of building a reinforcement learning machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the core concepts that you need to be aware of to develop reinforcement
    learning systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the reinforcement learning world, a machine is run or instructed by a (software)
    agent. The agent is the part of the machine that possesses intelligence and makes
    decisions on what to do next. You will come across the term "agent" several times
    as we dive deeper into reinforcement learning. Reinforcement learning is based
    on the reward hypothesis, which states that any goal can be described by the maximization
    of the expected cumulative reward. So, what is this reward exactly? That's what
    we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A reward, denoted by  ![](img/00013.jpeg), is usually a scalar quantity that
    is provided as feedback to the agent to drive its learning. The goal of the agent
    is to maximize the sum of the reward, and this signal indicates how well the agent
    is doing at time step ![](img/00014.jpeg).  The following examples of reward signals
    for different tasks may help you get a more intuitive understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: For the Atari games we discussed before, or any computer games in general, the
    reward signal can be `+1` for every increase in score and `-1` for every decrease
    in score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For stock trading, the reward signal can be `+1` for each dollar gained and
    `-1` for each dollar lost by the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For driving a car in simulation, the reward signal can be `+1` for every mile
    driven and `-100` for every collision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, the reward signal can be sparse. For example, for a game of chess
    or Go, the reward signal could be `+1` if the agent wins the game and `-1` if
    the agent loses the game. The reward is sparse because the agent receives the
    reward signal only after it completes one full game, not knowing how good each
    move it made was.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we looked into the different environments provided by
    the OpenAI Gym toolkit. You might have been wondering why they were called environments
    instead of problems, or tasks, or something else. Now that you have progressed
    to this chapter, does it ring a bell in your head?
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment is the platform that represents the problem or task that we
    are interested in, and with which the agent interacts.  The following diagram
    shows the general reinforcement learning paradigm at the highest level of abstraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: At each time step, denoted by ![](img/00016.jpeg), the agent receives an observation ![](img/00017.jpeg) from
    the environment and then executes an action ![](img/00018.jpeg), for which it
    receives a scalar reward ![](img/00019.jpeg) back from the environment, along
    with the next observation ![](img/00020.jpeg), and then this process repeats until
    a terminal state is reached. What is an observation and what is a state? Let's
    look into that next.
  prefs: []
  type: TYPE_NORMAL
- en: State
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the agent interacts with an environment, the process results in a sequence
    of observations (![](img/00021.jpeg)), actions (![](img/00022.jpeg)), and rewards
    (![](img/00023.jpeg)), as described previously.  At some time step ![](img/00024.jpeg),
    what the agent knows so far is the sequence of ![](img/00025.jpeg), ![](img/00026.jpeg),
    and ![](img/00027.jpeg) that it observed until time step ![](img/00028.jpeg). It
    intuitively makes sense to call this the history:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'What happens next at time step  ![](img/00030.jpeg) depends on the history.
    Formally, the information used to determine what happens next is called the *state**. ***Because
    it depends on the history up until that time step, it can be denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg),'
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/00032.jpeg) denotes some function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one subtle piece of information that is important for you to understand
    before we proceed. Let''s have another look at the general representation of a
    reinforcement learning system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, you will notice that the two main entities in the system, the agent and
    the environment, each has its own representation of the state. The *environment
    state*, sometimes denoted by ![](img/00034.jpeg), is the environment's own (private)
    representation, which the environment uses to pick the next observation and reward.
    This state is not usually visible/available to the agent. Likewise, the agent
    has its own internal representation of the state, sometimes denoted by ![](img/00035.jpeg),
    which is the information used by the agent to base its actions on. Because this
    representation is internal to the agent, it is up to the agent to use any function
    to represent it. Typically, it is some function based on the historythat the agent
    has observed so far. On a related note, a *Markov state*is a representation of
    the state using all the useful information from the history. By definition, using
    the Markov property, a state ![](img/00036.jpeg) is Markov or Markovian if, and
    only if, ![](img/00037.jpeg), which means that *the future is independent of the
    past given the present*. In other words, such a state is a sufficient statistic
    of the future. Once the state is known, the history can be thrown away. Usually,
    the environment state, ![](img/00038.jpeg), and the history, ![](img/00039.jpeg),
    satisfy the Markov property.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the environment may make its internal environmental state directly
    visible to the agent. Such environments are called *fully observable environments. *In
    cases where the agent cannot directly observe the environment state, the agent
    must construct its own state representation from what it observes. Such environments
    are called *partially observable environments.* For example, an agent playing
    poker can only observe the public cards and not the cards the other players possess.
    Therefore, it is a partially observed environment. Similarly, an autonomous car
    with just a camera does not know its absolute location in its environment, which
    makes the environment only partially observable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will learn about some of the key components of an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A model is an agent''s representation of the environment. It is similar to
    the mental models we have about people and things around us. An agent uses its
    model of the environment to predict what will happen next. There are two key pieces
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.jpeg): The state transition model/probability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg): The reward model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The state transition model ![](img/00042.jpeg) is a probability distribution
    or a function that predicts the probability of ending up in a state ![](img/00043.jpeg) in
    the next time step ![](img/00044.jpeg) given the state ![](img/00045.jpeg) and
    the action ![](img/00046.jpeg) at time step ![](img/00047.jpeg). Mathematically,
    it is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The agent uses the reward model ![](img/00049.jpeg) to predict the immediate
    next reward that it would get if it were to take action ![](img/00050.jpeg) while
    in state ![](img/00051.jpeg) at time step ![](img/00052.jpeg).  This expectation
    of the reward at the next time step ![](img/00053.jpeg) can be mathematically
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Value function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A value function represents the agent''s prediction of future rewards. There
    are two types of value function: state-value function and action-value function.'
  prefs: []
  type: TYPE_NORMAL
- en: State-value function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A state-value function is a function that represents the agent''s estimate
    of how good it is to be in a state ![](img/00055.jpeg) at time step *t*. It is
    denoted by ![](img/00056.jpeg) and is usually just called the *value function*.
    It represents the agent''s prediction of the future reward it would get if it
    were to end up in state ![](img/00057.jpeg) at time step *t*. Mathematically,
    it can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: What this expression means is that the value of state ![](img/00059.jpeg) under
    policy ![](img/00060.jpeg) is the expected sum of the discounted future rewards,
    where ![](img/00061.jpeg) is the discount factor and is a real number in the range
    [0,1]. Practically, the discount factor is typically set to be in the range of
    [0.95,0.99]. The other new term is ![](img/00062.jpeg), which is the policy of
    the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Action-value function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The action-value function is a function that represents the agent''s estimate
    of how good it is to take action ![](img/00063.jpeg) in state ![](img/00064.jpeg).
    It is denoted by ![](img/00065.jpeg). It is related to the state-value function
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The policy denoted by ![](img/00067.jpeg) prescribes what action is to be taken
    given the state. It can be seen as a function that maps states to actions. There
    are two major types of policy: deterministic policies and stochastic policies.'
  prefs: []
  type: TYPE_NORMAL
- en: A deterministic policy prescribes one action for a given state, that is, there
    is only one action, ![](img/00068.jpeg), given *s*. Mathematically, it means ![](img/00069.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: A stochastic policy prescribes an action distribution given a state ![](img/00070.jpeg) at
    time step ![](img/00071.jpeg), that is, there are multiple actions with a probability
    value for each action. Mathematically, it means ![](img/00072.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Agents following different policies may exhibit different behaviors in the same
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Markov Decision Process **(**MDP**) provides a formal framework for reinforcement
    learning. It is used to describe a fully observable environment where the outcomes
    are partly random and partly dependent on the actions taken by the agent or the
    decision maker. The following diagram is the progression of a Markov Process into
    a Markov Decision Process through the Markov Reward Process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'These stages can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Markov Process** (or a *markov chain) *is a sequence of random states s1,
    s2,...  that obeys the *Markov property.*In simple terms, it is a random process
    without any memory about its history.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Markov Reward Process** (**MRP**) is a *Markov Process (*also called a M*arkov
    chain) *with values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Markov Decision Process** is a *Markov Reward Process* with decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning with dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dynamic programming is a very general method to efficiently solve problems that
    can be decomposed into overlapping sub-problems. If you have used any type of
    recursive function in your code, you might have already got some preliminary flavor
    of dynamic programming. Dynamic programming, in simple terms, tries to cache or
    store the results of sub-problems so that they can be used later if required,
    instead of computing the results again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so how is that relevant here, you may ask. Well, they are pretty useful
    for solving a fully defined MDP, which means that an agent can find the most optimal
    way to act in an environment to achieve the highest reward using dynamic programming
    if it has full knowledge of the MDP! In the following table, you will find a concise
    summary of what the inputs and outputs are when we are interested in sequential
    prediction or control:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task/objective | Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction | MDP or MRP and policy ![](img/00074.jpeg) | Value function ![](img/00075.jpeg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Control | MDP | Optimal value function ![](img/00076.jpeg) and optimal policy ![](img/00077.jpeg)
    |'
  prefs: []
  type: TYPE_TB
- en: Monte Carlo learning and temporal difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we understand that it is very useful for an agent to learn the
    state value function ![](img/00078.jpeg), which informs the agent about the long-term
    value of being in state ![](img/00079.jpeg) so that the agent can decide if it
    is a good state to be in or not. The **Monte Carlo** (**MC**) and **Temporal Difference**
    (**TD**) learning methods enable an agent to learn that!
  prefs: []
  type: TYPE_NORMAL
- en: The goal of MC and TD learning is to learn the value functions from the agent's
    experience as the agent follows its policy ![](img/00080.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the value estimate''s update equation for the
    MC and TD learning methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Learning method** | **State-value function** |'
  prefs: []
  type: TYPE_TB
- en: '| Monte Carlo | ![](img/00081.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal Difference | ![](img/00082.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: MC learning updates the value towards the **actual return** ![](img/00083.jpeg),
    which is the total discounted reward from time step *t*. This means that ![](img/00084.jpeg) until
    the end. It is important to note that we can calculate this value only after the
    end of the sequence, whereas TD learning (TD(0) to be precise), updates the value
    towards the *estimated return* given by ![](img/00085.jpeg), which can be calculated
    after every step.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA and Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is also very useful for an agent to learn the action value function ![](img/00086.jpeg),
    which informs the agent about the long-term value of taking action ![](img/00087.jpeg) in
    state ![](img/00088.jpeg) so that the agent can take those actions that will maximize
    its expected, discounted future reward. The SARSA and Q-learning algorithms enable
    an agent to learn that! The following table summarizes the update equation for
    the SARSA algorithm and the Q-learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Learning method** | **Action-value function** |'
  prefs: []
  type: TYPE_TB
- en: '| SARSA | ![](img/00089.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Q-learning | ![](img/00090.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: 'SARSA is so named because of the sequence State->Action->Reward->State''->Action''
    that the algorithm''s update step depends on. The description of the sequence
    goes like this: the agent, in state *S*, takes an action A and gets a reward R,
    and ends up in the next state S'', after which the agent decides to take an action
    A'' in the new state. Based on this experience, the agent can update its estimate
    of Q(S,A).'
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning is a popular off-policy learning algorithm, and it is similar to
    SARSA, except for one thing. Instead of using the Q value estimate for the new
    state and the action that the agent took in that new state, it uses the Q value
    estimate that corresponds to the action that leads to the *maximum* obtainable
    Q value from that new state, S'.
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a basic understanding of reinforcement learning, you are now in a better
    state (hopefully you are not in a strictly Markov state where you have forgotten
    the history/things you have learned so far) to understand the basics of the cool
    new suite of algorithms that have been rocking the field of AI in recent times.
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning emerged naturally when people made advancements
    in the deep learning field and applied them to reinforcement learning. We learned
    about the state-value function, action-value function, and policy. Let's briefly
    look at how they can be represented mathematically or realized through computer
    code. The state-value function ![](img/00091.jpeg) is a real-value function that
    takes the current state ![](img/00092.jpeg) as the input and outputs a real-value
    number (such as 4.57). This number is the agent's prediction of how good it is
    to be in state ![](img/00093.jpeg) and the agent keeps updating the value function
    based on the new experiences it gains. Likewise, the action-value function ![](img/00094.jpeg)
    is also a real-value function, which takes action ![](img/00095.jpeg) as an input
    in addition to state ![](img/00096.jpeg), and outputs a real number. One way to
    represent these functions is using neural networks because neural networks are
    universal function approximators, which are capable of representing complex, non-linear
    functions. For an agent trying to play a game of Atari by just looking at the
    images on the screen (like we do), state ![](img/00097.jpeg) could be the pixel
    values of the image on the screen. In such cases, we could use a deep neural network
    with convolutional layers to extract the visual features from the state/image,
    and then a few fully connected layers to finally output ![](img/00098.jpeg)  or ![](img/00099.jpeg) ,
    depending on which function we want to approximate.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the earlier sections of this chapter that ![](img/00100.jpeg) is
    the state-value function and provides an estimate of the value of being in state ![](img/00101.jpeg),
    and ![](img/00099.jpeg) is the action-value function, which provides an estimate
    of the value of each action given the  state.
  prefs: []
  type: TYPE_NORMAL
- en: If we do this, then we are doing deep reinforcement learning! Easy enough to
    understand? I hope so. Let's look at some other ways in which we can use deep
    learning in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a policy is represented as ![](img/00102.jpeg) in the case of deterministic
    policies, and as ![](img/00103.jpeg) in the case of stochastic policies, where
    action ![](img/00104.jpeg) could be discrete (such as "move left," "move right,"
    or "move straight ahead") or continuous values (such as "0.05" for acceleration,
    "0.67" for steering, and so on), and they can be single or multi-dimensional.
    Therefore, a policy can be a complicated function at times! It might have to take
    in a multi-dimensional state (such as an image) as input and output a multi-dimensional
    vector of probabilities as output (in the case of stochastic policies). So, this
    does look like it will be a monster function, doesn't it? Yes it does. That's
    where deep neural networks come to the rescue! We could approximate an agent's
    policy using a deep neural network and directly learn to update the policy (by
    updating the parameters of the deep neural network). This is called policy optimization-based
    deep reinforcement learning and it has been shown to be quite efficient in solving
    several challenging control problems, especially in robotics.
  prefs: []
  type: TYPE_NORMAL
- en: So in summary, deep reinforcement learning is the application of deep learning
    to reinforcement learning and so far, researchers have applied deep learning to
    reinforcement learning successfully in two ways. One way is using deep neural
    networks to approximate the value functions, and the other way is to use a deep
    neural network to represent the policy.
  prefs: []
  type: TYPE_NORMAL
- en: These ideas have been known from the early days, when researchers were trying
    to use neural networks as value function approximators, even back in 2005\. But
    it rose to stardom only recently because although neural networks or other non-linear
    value function approximators can better represent the complex values of environment
    states and actions, they were prone to instability and often led to sub-optimal
    functions. Only recently have researchers such as Volodymyr Mnih and his colleagues
    at DeepMind (now part of Google) figured out the trick of stabilizing the learning
    and trained agents with deep, non-linear function approximators that converged
    to near-optimal value functions. In the later chapters of this book, we will in
    fact reproduce some of their then-groundbreaking results, which surpassed human
    Atari game playing capabilities!
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications of reinforcement and deep reinforcement learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until recently, practical applications of reinforcement learning and deep reinforcement
    learning were limited, due to sample complexity and instability. But, these algorithms
    proved to be quite powerful in solving some really hard practical problems. Some
    of them are listed here to give you an idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning to play video games better than humans**: This news has probably
    reached you by now. Researchers at DeepMind and others developed a series of algorithms,
    starting with DeepMind''s Deep-Q-Network, or DQN for short, which reached human-level
    performance in playing Atari games. We will actually be implementing this algorithm
    in a later chapter of this book! In essence, it is a deep variant of the Q-learning
    algorithm we briefly saw in this chapter, with a few changes that increased the
    speed of learning and the stability. It was able to reach human-level performance
    in terms of game scores after several games. What is more impressive is that the
    same algorithm achieved this level of play without any game-specific fine-tuning
    or changes!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mastering the game of Go**: Go is a Chinese game that has challenged AI for
    several decades. It is played on a full-size 19 x 19 board and is orders of magnitude
    more complex than chess because of the large number (![](img/00105.jpeg)) of possible
    board positions. Until recently, no AI algorithm or software was able to play
    anywhere close to the level of humans at this game. AlphaGo—the AI agent from
    DeepMind that uses deep reinforcement learning and Monte Carlo tree search—changed
    this all and beat the human world champions Lee Sedol (4-1) and Fan Hui (5-0).
    DeepMind released more advanced versions of their AI agent, named AlphaGO Zero
    (which uses zero human knowledge and learned to play all by itself!) and AlphaZero
    (which could play the games of Go, chess, and Shogi!), all of which used deep
    reinforcement learning as the core algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Helping AI win Jeopardy!**: IBM''s Watson—an AI system developed by IBM,
    which came to fame by beating humans at Jeopardy!—used an extension of TD learning
    to create its *daily-double wagering* strategies that helped it to win against
    human champions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robot locomotion and manipulation:** Both reinforcement learning and deep
    reinforcement learning have enabled the control of complex robots, both for locomotion
    and navigation. Several recent works from the researchers at UC Berkeley have
    shown how, using deep reinforcement, they train policies that offer vision and
    control for robotic manipulation tasks and generate join actuations for making
    a complex bipedal humanoid walk and run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how an agent interacts with an environment by
    taking an action based on the observation it receives from the environment, and
    the environment responds to the agent's action with an (optional) reward and the
    next observation.
  prefs: []
  type: TYPE_NORMAL
- en: With a concise understanding of the foundations of reinforcement learning, we
    went deeper to understand what deep reinforcement learning is, and uncovered the
    fact that we could use deep neural networks to represent value functions and policies.
    Although this chapter was a little heavy on notation and definitions, hopefully
    it laid a strong foundation for us to develop some cool agents in the upcoming
    chapters. In the next chapter, we will consolidate our learning in the first two
    chapters and put it to use by laying out the groundwork to train an agent to solve
    some interesting problems.
  prefs: []
  type: TYPE_NORMAL
