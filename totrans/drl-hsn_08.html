<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-8-dqn-extensions">
<h1 class="chapterNumber">8</h1>
<h1 class="chapterTitle" id="sigil_toc_id_411">
<span id="x1-1240008"/>DQN Extensions
    </h1>
<p>Since DeepMind published its paper on the <span class="cmbx-10x-x-109">deep Q-network </span>(<span class="cmbx-10x-x-109">DQN</span>) model in 2015, many improvements have been proposed, along with tweaks to the basic architecture, which, significantly, have improved the convergence, stability, and sample efficiency of DeepMind‚Äôs basic DQN. In this chapter, we will take a deeper look at some of those ideas.</p>
<p>In October 2017, Hessel et al. from DeepMind published a paper called <span class="cmti-10x-x-109">Rainbow:</span> <span class="cmti-10x-x-109">Combining improvements in deep reinforcement learning </span>[<span id="x1-124001"/><a href="#">Hes+18</a>], which presented the six most important improvements to DQN; some were invented in 2015, but others are relatively recent. In this paper, state-of-the-art results on the Atari games suite were reached, just by combining those six methods.</p>
<div class="tcolorbox infobox" id="tcolobox-143">
<div class="tcolorbox-content">
<p>Since 2017, more papers have been published and state-of-the-art results have been pushed further, but all the methods presented in the paper are still relevant and widely used in practice. For example, in 2023, Marc Bellemare published the book <span class="cmti-10x-x-109">Distributional</span> <span class="cmti-10x-x-109">reinforcement learning </span>[<span id="x1-124002"/><a href="#">BDR23</a>] about one of the paper‚Äôs methods. In addition, the improvements described are relatively simple to implement and understand, so I have not made any major modifications to this chapter in this edition.</p>
</div>
</div>
<p>The DQN extensions<span id="dx1-124003"/> that we will become familiar with are the following:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">N-step DQN</span>: How to improve convergence speed and stability with a simple unrolling of the Bellman equation, and why it‚Äôs not an ultimate solution</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Double DQN</span>: How to deal with DQN overestimation of the values of the actions</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Noisy networks</span>: How to make exploration more efficient by adding noise to the network weights</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Prioritized replay buffer</span>: Why uniform sampling of our experience is not the best way to train</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Dueling DQN</span>: How to improve convergence speed by making our network‚Äôs architecture more closely represent the problem that we are solving</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Categorical DQN</span>: How to go beyond the single expected value of the action and work with full distributions</p>
</li>
</ul>
<p>This chapter will go through all these methods. We will analyze the ideas behind them, alongside how they can be implemented and compared to the classic DQN performance. Finally, we will analyze how the combined system with all the methods performs.</p>
<section class="level3 sectionHead" id="basic-dqn">
<h1 class="heading-1" id="sigil_toc_id_107"> <span id="x1-1250008.1"/>Basic DQN</h1>
<p>To get started, we will <span id="dx1-125001"/>implement the same DQN method as in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>, but leveraging the high-level primitives described in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch011.xhtml#x1-1070007"><span class="cmti-10x-x-109">7</span></a>. This will make our code much more compact, which is good, as non-relevant details won‚Äôt distract us from the method‚Äôs logic. At the same time, the purpose of this book is not to teach you how to use the existing libraries but rather how to develop intuition about RL methods and, if necessary, implement everything from scratch. From my perspective, this is a much more valuable skill, as libraries come and go, but true understanding of the domain will allow you to quickly make sense of other people‚Äôs code and apply it consciously.</p>
<p>In the basic DQN implementation, we have three modules in the <span class="cmtt-10x-x-109">Chapter08</span> folder of the GitHub repository for this book:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">Chapter08/lib/dqn</span><span class="cmtt-10x-x-109">_model.py</span>: The DQN <span class="cmbx-10x-x-109">neural network </span>(<span class="cmbx-10x-x-109">NN</span>), which is the same as in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>, so I won‚Äôt repeat it</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Chapter08/lib/common.py</span>: Common functions and declarations shared by the code in this chapter</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">Chapter08/01</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_basic.py</span>: 77 lines of code leveraging the PTAN and Ignite libraries, implementing the basic DQN method</p>
</li>
</ul>
<section class="level4 subsectionHead" id="common-library">
<h2 class="heading-2" id="sigil_toc_id_108"> <span id="x1-1260008.1.1"/>Common library</h2>
<p>Let‚Äôs start <span id="dx1-126001"/>with the contents of <span class="cmtt-10x-x-109">lib/common.py</span>. First of all, we have hyperparameters for our Pong environment from the previous chapter. The hyperparameters are stored in the <span class="cmtt-10x-x-109">dataclass </span>object, which is a standard way to store a bunch of data fields with their type annotations. This makes it easy to add another configuration set for different, more complicated Atari games and allows us to experiment with hyperparameters:</p>
<div class="tcolorbox" id="tcolobox-144">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-197"><code>@dataclasses.dataclass 
class Hyperparams: 
    env_name: str 
    stop_reward: float 
    run_name: str 
    replay_size: int 
    replay_initial: int 
    target_net_sync: int 
    epsilon_frames: int 
 
    learning_rate: float = 0.0001 
    batch_size: int = 32 
    gamma: float = 0.99 
    epsilon_start: float = 1.0 
    epsilon_final: float = 0.1 
 
    tuner_mode: bool = False 
    episodes_to_solve: int = 500 
 
 
GAME_PARAMS = { 
    ‚Äôpong‚Äô: Hyperparams( 
        env_name="PongNoFrameskip-v4", 
        stop_reward=18.0, 
        run_name="pong", 
        replay_size=100_000, 
        replay_initial=10_000, 
        target_net_sync=1000, 
        epsilon_frames=100_000, 
        epsilon_final=0.02, 
    ),</code></pre>
</div>
</div>
<p>The next <span id="dx1-126033"/>function from <span class="cmtt-10x-x-109">lib/common.py </span>has the name <span class="cmtt-10x-x-109">unpack</span><span class="cmtt-10x-x-109">_batch</span>, and it takes the batch, of transitions and converts it into the set of NumPy arrays suitable for training. Every transition from <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>has a type of <span class="cmtt-10x-x-109">ExperienceFirstLast</span>, which is a dataclass with the following fields:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">state</span>: Observation from the environment.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">action</span>: Integer action taken by the agent.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">reward</span>: If we have created <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>with the attribute <span class="cmtt-10x-x-109">steps</span><span class="cmtt-10x-x-109">_count=1</span>, it‚Äôs just the immediate reward. For larger step counts, it contains the discounted sum of rewards for this number of steps.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">last</span><span class="cmtt-10x-x-109">_state</span>: If the transition corresponds to the final step in the environment, then this field is <span class="cmtt-10x-x-109">None</span>; otherwise, it contains the last observation in the experience chain.</p>
</li>
</ul>
<p>The code of <span class="cmtt-10x-x-109">unpack</span><span class="cmtt-10x-x-109">_batch </span>is as follows:</p>
<div class="tcolorbox" id="tcolobox-145">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-198"><code>def unpack_batch(batch: tt.List[ExperienceFirstLast]): 
    states, actions, rewards, dones, last_states = [],[],[],[],[] 
    for exp in batch: 
        states.append(exp.state) 
        actions.append(exp.action) 
        rewards.append(exp.reward) 
        dones.append(exp.last_state is None) 
        if exp.last_state is None: 
            lstate = exp.state  # the result will be masked anyway 
        else: 
            lstate = exp.last_state 
        last_states.append(lstate) 
    return np.asarray(states), np.array(actions), np.array(rewards, dtype=np.float32), \ 
        np.array(dones, dtype=bool), np.asarray(last_states)</code></pre>
</div>
</div>
<p>Note how we <span id="dx1-126048"/>handle the final transitions in the batch. To avoid the special handling of such cases, for terminal transitions, we store the initial state in the <span class="cmtt-10x-x-109">last</span><span class="cmtt-10x-x-109">_states </span>array. To make our calculations of the Bellman update correct, we have to mask such batch entries during the loss calculation using the <span class="cmtt-10x-x-109">dones </span>array. Another solution would be to calculate the value of the last states only for non-terminal transitions, but it would make our loss function logic a bit more complicated.</p>
<p>Calculation of the DQN loss function is provided by the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dqn </span>function, and the code is almost the same as in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>. One small addition is <span class="cmtt-10x-x-109">torch.no</span><span class="cmtt-10x-x-109">_grad()</span>, which stops the PyTorch calculation graph from being recorded for the target net:</p>
<div class="tcolorbox" id="tcolobox-146">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-199"><code>def calc_loss_dqn( 
        batch: tt.List[ExperienceFirstLast], net: nn.Module, tgt_net: nn.Module, 
        gamma: float, device: torch.device) -&gt; torch.Tensor: 
    states, actions, rewards, dones, next_states = unpack_batch(batch) 
 
    states_v = torch.as_tensor(states).to(device) 
    next_states_v = torch.as_tensor(next_states).to(device) 
    actions_v = torch.tensor(actions).to(device) 
    rewards_v = torch.tensor(rewards).to(device) 
    done_mask = torch.BoolTensor(dones).to(device) 
 
    actions_v = actions_v.unsqueeze(-1) 
    state_action_vals = net(states_v).gather(1, actions_v) 
    state_action_vals = state_action_vals.squeeze(-1) 
    with torch.no_grad(): 
        next_state_vals = tgt_net(next_states_v).max(1)[0] 
        next_state_vals[done_mask] = 0.0 
 
    bellman_vals = next_state_vals.detach() * gamma + rewards_v 
    return nn.MSELoss()(state_action_vals, bellman_vals)</code></pre>
</div>
</div>
<p>Besides those core DQN functions, <span class="cmtt-10x-x-109">common.py </span>provides several utilities related to <span id="dx1-126069"/>our training loop, data generation, and TensorBoard tracking. The first such utility is a small class that implements epsilon decay during the training. Epsilon defines the probability of taking the random action by the agent. It should be decayed from 1.0 in the beginning (fully random agent) to some small number, like 0.02 or 0.01. The code is trivial but is needed in almost any DQN, so it is provided by the following little class:</p>
<div class="tcolorbox" id="tcolobox-147">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-200"><code>class EpsilonTracker: 
    def __init__(self, selector: EpsilonGreedyActionSelector, params: Hyperparams): 
        self.selector = selector 
        self.params = params 
        self.frame(0) 
 
    def frame(self, frame_idx: int): 
        eps = self.params.epsilon_start - frame_idx / self.params.epsilon_frames 
        self.selector.epsilon = max(self.params.epsilon_final, eps)</code></pre>
</div>
</div>
<p>Another small function is <span class="cmtt-10x-x-109">batch</span><span class="cmtt-10x-x-109">_generator</span>, which takes <span class="cmtt-10x-x-109">ExperienceReplayBuffer</span> (the PTAN class described in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch011.xhtml#x1-1070007"><span class="cmti-10x-x-109">7</span></a>) and infinitely generates training batches sampled from the buffer. In the beginning, the function ensures that the buffer contains the required amount of samples:</p>
<div class="tcolorbox" id="tcolobox-148">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-201"><code>def batch_generator(buffer: ExperienceReplayBuffer, initial: int, batch_size: int) -&gt; \ 
        tt.Generator[tt.List[ExperienceFirstLast], None, None]: 
    buffer.populate(initial) 
    while True: 
        buffer.populate(1) 
        yield buffer.sample(batch_size)</code></pre>
</div>
</div>
<p>Finally, a lengthy, but nevertheless very useful, function called <span class="cmtt-10x-x-109">setup</span><span class="cmtt-10x-x-109">_ignite</span> attaches the needed Ignite handlers, showing the training progress and writing metrics to TensorBoard. Let‚Äôs look at this function piece by piece:</p>
<div class="tcolorbox" id="tcolobox-149">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-202"><code>def setup_ignite( 
        engine: Engine, params: Hyperparams, exp_source: ExperienceSourceFirstLast, 
        run_name: str, extra_metrics: tt.Iterable[str] = (), 
        tuner_reward_episode: int = 100, tuner_reward_min: float = -19, 
): 
    handler = ptan_ignite.EndOfEpisodeHandler( 
        exp_source, bound_avg_reward=params.stop_reward) 
    handler.attach(engine) 
    ptan_ignite.EpisodeFPSHandler().attach(engine)</code></pre>
</div>
</div>
<p>Initially, <span class="cmtt-10x-x-109">setup</span><span class="cmtt-10x-x-109">_ignite </span>attaches <span id="dx1-126094"/>two Ignite handlers provided by PTAN:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">EndOfEpisodeHandler</span>, which emits the Ignite event every time a game episode ends. It can also fire an event when the averaged reward for episodes crosses some boundary. We use this to detect when the game is finally solved.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">EpisodeFPSHandler</span>, a small class that tracks the time the episode has taken and the amount of interactions that we have had with the environment. From this, we calculate <span class="cmbx-10x-x-109">frames per second </span>(<span class="cmbx-10x-x-109">FPS</span>), which is an important performance metric to track.</p>
</li>
</ul>
<p>Then, we install two event handlers:</p>
<div class="tcolorbox" id="tcolobox-150">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-203"><code>    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED) 
    def episode_completed(trainer: Engine): 
        passed = trainer.state.metrics.get(‚Äôtime_passed‚Äô, 0) 
        print("Episode %d: reward=%.0f, steps=%s, speed=%.1f f/s, elapsed=%s" % ( 
            trainer.state.episode, trainer.state.episode_reward, 
            trainer.state.episode_steps, trainer.state.metrics.get(‚Äôavg_fps‚Äô, 0), 
            timedelta(seconds=int(passed)))) 
 
    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED) 
    def game_solved(trainer: Engine): 
        passed = trainer.state.metrics[‚Äôtime_passed‚Äô] 
        print("Game solved in %s, after %d episodes and %d iterations!" % ( 
            timedelta(seconds=int(passed)), trainer.state.episode, 
            trainer.state.iteration)) 
        trainer.should_terminate = True 
        trainer.state.solved = True</code></pre>
</div>
</div>
<p>One of the event handlers is called at the end of an episode. It will show information about the completed episode on the console. Another function will be called when the average reward grows above the boundary defined in the hyperparameters (18.0 in the case of Pong). This function shows a message about the solved game and stops the training.</p>
<p>The rest of the function is related to the TensorBoard data that we want to track. First, we create a <span class="cmtt-10x-x-109">TensorboardLogger</span>:</p>
<div class="tcolorbox" id="tcolobox-151">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-204"><code>    now = datetime.now().isoformat(timespec=‚Äôminutes‚Äô).replace(‚Äô:‚Äô, ‚Äô‚Äô) 
    logdir = f"runs/{now}-{params.run_name}-{run_name}" 
    tb = tb_logger.TensorboardLogger(log_dir=logdir) 
    run_avg = RunningAverage(output_transform=lambda v: v[‚Äôloss‚Äô]) 
    run_avg.attach(engine, "avg_loss")</code></pre>
</div>
</div>
<p>This is a special class provided by Ignite to <span id="dx1-126116"/>write into TensorBoard. Our processing function will return the loss value, so we attach the <span class="cmtt-10x-x-109">RunningAverage</span> transformation (also provided by Ignite) to get a smoothed version of the loss over time.</p>
<p>Next, we attach the metrics we want to track to the Ignite events:</p>
<div class="tcolorbox" id="tcolobox-152">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-205"><code>    metrics = [‚Äôreward‚Äô, ‚Äôsteps‚Äô, ‚Äôavg_reward‚Äô] 
    handler = tb_logger.OutputHandler(tag="episodes", metric_names=metrics) 
    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED 
    tb.attach(engine, log_handler=handler, event_name=event)</code></pre>
</div>
</div>
<p><span class="cmtt-10x-x-109">TensorboardLogger </span>can track two groups of values from Ignite: outputs (values returned by the transformation function) and metrics (calculated during the training and kept in the <span class="cmtt-10x-x-109">engine </span>state). <span class="cmtt-10x-x-109">EndOfEpisodeHandler </span>and <span class="cmtt-10x-x-109">EpisodeFPSHandler </span>provide metrics, which are updated at the end of every game episode. So, we attach <span class="cmtt-10x-x-109">OutputHandler</span>, which will write into TensorBoard information about the episode every time it is completed.</p>
<p>Next, we track another group of values, metrics from the training process: loss, FPS, and, possibly, some custom metrics relevant to the specific extension‚Äôs logic:</p>
<div class="tcolorbox" id="tcolobox-153">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-206"><code>    ptan_ignite.PeriodicEvents().attach(engine) 
    metrics = [‚Äôavg_loss‚Äô, ‚Äôavg_fps‚Äô] 
    metrics.extend(extra_metrics) 
    handler = tb_logger.OutputHandler(tag="train", metric_names=metrics, 
                                      output_transform=lambda a: a) 
    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED 
    tb.attach(engine, log_handler=handler, event_name=event)</code></pre>
</div>
</div>
<p>Those values are updated every training iteration, but we are going to do millions of iterations, so we will store values in TensorBoard every 100 training iterations; otherwise, the data files will be huge. All this functionality might look too complicated, but it provides us with the unified set of metrics gathered from the training process. In fact, Ignite is not very tricky, given the flexibility it provides. That‚Äôs it for <span class="cmtt-10x-x-109">common.py</span>.</p>
</section>
<section class="level4 subsectionHead" id="implementation">
<h2 class="heading-2" id="sigil_toc_id_109"> <span id="x1-1270008.1.2"/>Implementation</h2>
<p>Now, let‚Äôs take a look at <span class="cmtt-10x-x-109">01</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_basic.py</span>, which creates the needed classes <span id="dx1-127001"/>and starts the training. I‚Äôm going to omit non-relevant code and focus only on important pieces (the full version is available in the GitHub repo). First, we create the environment:</p>
<div class="tcolorbox" id="tcolobox-154">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-207"><code>    env = gym.make(params.env_name) 
    env = ptan.common.wrappers.wrap_dqn(env) 
 
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device) 
    tgt_net = ptan.agent.TargetNet(net)</code></pre>
</div>
</div>
<p>Here, we apply a set of standard wrappers. We discussed them in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a> and will also touch upon them in the next chapter, when we optimize the performance of the Pong solver. Then, we create the DQN model and the target network.</p>
<p>Next, we create the agent, passing it an epsilon-greedy action selector:</p>
<div class="tcolorbox" id="tcolobox-155">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-208"><code>    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start) 
    epsilon_tracker = common.EpsilonTracker(selector, params) 
    agent = ptan.agent.DQNAgent(net, selector, device=device)</code></pre>
</div>
</div>
<p>During the training, epsilon will be decreased by the <span class="cmtt-10x-x-109">EpsilonTracker </span>class that we have already discussed. This will decrease the amount of randomly selected actions and give more control to our NN.</p>
<p>The next two very important objects are <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>and <span class="cmtt-10x-x-109">ExperienceReplayBuffer</span>:</p>
<div class="tcolorbox" id="tcolobox-156">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-209"><code>    exp_source = ptan.experience.ExperienceSourceFirstLast( 
        env, agent, gamma=params.gamma, env_seed=common.SEED) 
    buffer = ptan.experience.ExperienceReplayBuffer( 
        exp_source, buffer_size=params.replay_size)</code></pre>
</div>
</div>
<p><span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>takes the agent and environment and provides transitions over game episodes. Those transitions will be kept in the experience replay buffer.</p>
<p>Then we create an optimizer and define the processing function:</p>
<div class="tcolorbox" id="tcolobox-157">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-210"><code>    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate) 
 
    def process_batch(engine, batch): 
        optimizer.zero_grad() 
        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, 
                                      gamma=params.gamma, device=device) 
        loss_v.backward() 
        optimizer.step() 
        epsilon_tracker.frame(engine.state.iteration) 
        if engine.state.iteration % params.target_net_sync == 0: 
            tgt_net.sync() 
        return { 
            "loss": loss_v.item(), 
            "epsilon": selector.epsilon, 
        }</code></pre>
</div>
</div>
<p>The processing function will be called for every batch of transitions to train the model. To do this, we call the <span class="cmtt-10x-x-109">common.calc</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dqn </span>function and then backpropagate on the result. This function also asks <span class="cmtt-10x-x-109">EpsilonTracker </span>to decrease the epsilon and does <span id="dx1-127029"/>periodical target network synchronization.</p>
<p>And, finally, we create the Ignite <span class="cmtt-10x-x-109">Engine </span>object:</p>
<div class="tcolorbox" id="tcolobox-158">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-211"><code>    engine = Engine(process_batch) 
    common.setup_ignite(engine, params, exp_source, NAME) 
    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))</code></pre>
</div>
</div>
<p>We configure it using a function from <span class="cmtt-10x-x-109">common.py</span>, and run our training process.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning">
<h2 class="heading-2" id="sigil_toc_id_110"> <span id="x1-1280008.1.3"/>Hyperparameter tuning</h2>
<p>To make our <span id="dx1-128001"/>comparison of DQN extensions fair, we also need to tune hyperparameters. This is essential because even for the same game (Pong), using the fixed set of training parameters might give less optimal results when we change the details of the method.</p>
<p>In principle, every explicit or implicit constant in our code could be tuned, such as:</p>
<ul>
<li>
<p>Network configuration: Amount and size of layers, activation function, dropout, etc.</p>
</li>
<li>
<p>Optimization parameters: Method (vanilla SGD, Adam, AdaGrad, etc.), learning rate, and other optimizer parameters</p>
</li>
<li>
<p>Exploration parameters: Decay rate of <span class="cmmi-10x-x-109">ùúñ</span>, final <span class="cmmi-10x-x-109">ùúñ </span>value</p>
</li>
<li>
<p>Discount factor <span class="cmmi-10x-x-109">Œ≥ </span>in Bellman equation</p>
</li>
</ul>
<p>But every new parameter we tune has a multiplicative effect on the amount of <span id="dx1-128002"/>trial training we need to perform, so having too many hyperparameters might require hundreds or even thousands of trainings. Large companies like Google and Meta have access to a much larger amount of GPUs than individual researchers like us, so we need to keep the balance there.</p>
<p>In my case, I‚Äôm going <span id="dx1-128003"/>to demonstrate how hyperparameter tuning is done in general, but we‚Äôll do the search only on a few values:</p>
<ul>
<li>
<p>Learning rate</p>
</li>
<li>
<p>Discount factor <span class="cmmi-10x-x-109">Œ≥</span></p>
</li>
<li>
<p>Parameters specific to the DQN extension we‚Äôre considering</p>
</li>
</ul>
<p>There are several libraries that might be helpful with hyperparameter tuning. Here, I‚Äôm using Ray Tune (<a class="url" href="https://docs.ray.io/en/latest/tune/index.xhtml"><span class="cmtt-10x-x-109">https://docs.ray.io/en/latest/tune/index.xhtml</span></a>), which is a part of the Ray project ‚Äî a distributed computing framework for ML and DL. At a high level, you need to define:</p>
<ul>
<li>
<p>The hyperparameter space you want to explore (boundaries for values to sample from or an explicit list of values to try)</p>
</li>
<li>
<p>The function that performs the training with specific values of hyperparameters and returns the metric you want to optimize with the tuning</p>
</li>
</ul>
<p>This might look very similar to ML problems, and in fact it is ‚Äî this is also an optimization problem. But there are substantial differences: the function we‚Äôre optimizing is not differentiable (so you cannot perform the gradient descent to push your hyperparameters towards the desired direction of the metric) and the optimization space might be discrete (you cannot train the network with the number of layers equal to 2.435, for example, since we cannot take the derivative of a non-smooth function).</p>
<p>In later chapters, we‚Äôll touch on this problem slightly in the context of black-box optimization methods (<span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch021.xhtml#x1-31100017"><span class="cmti-10x-x-109">17</span></a>) and RL in discrete optimizations (<span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch025.xhtml#x1-39100021"><span class="cmti-10x-x-109">21</span></a>), but for now, we‚Äôll use the simplest approach ‚Äî a random search of hyperparameters. In this case, the <span class="cmtt-10x-x-109">ray.tune </span>library randomly samples concrete parameters several times and calls the function to obtain the metric. The smallest (or highest) metric corresponds to the best hyperparameter combination found in this run.</p>
<p>In this chapter, our metric (optimization objective) will be the <span class="cmti-10x-x-109">number of games</span> <span class="cmti-10x-x-109">the agent needs to play </span>before solving the game (reaching a mean score of greater than 18 for Pong).</p>
<p>To illustrate the effect of tuning, for every DQN extension, we check the training dynamics using a fixed set of parameters (the same as in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>), and the dynamics using the best hyperparameters found after 20-30 rounds of tuning. If you wish, you can do your own experiments, optimizing more hyperparameters. Most likely, this will allow you to find a better configuration for the training.</p>
<p>The core of the process is implemented in the <span class="cmtt-10x-x-109">common.tune</span><span class="cmtt-10x-x-109">_params </span>function. Let‚Äôs take a look at its code. We start with the type declaration and hyperparameter space:</p>
<div class="tcolorbox" id="tcolobox-159">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-212"><code>TrainFunc = tt.Callable[ 
    [Hyperparams, torch.device, dict], 
    tt.Optional[int] 
] 
 
BASE_SPACE = { 
    "learning_rate": tune.loguniform(1e-5, 1e-4), 
    "gamma": tune.choice([0.9, 0.92, 0.95, 0.98, 0.99, 0.995]), 
}</code></pre>
</div>
</div>
<p>Here, we first define <span id="dx1-128013"/>the type for the training function, which takes the <span class="cmtt-10x-x-109">Hyperparams </span>dataclass <span class="cmtt-10x-x-109">torch.device </span>to use and a dictionary with extra parameters (as some DQN extensions we‚Äôre going to present might require extra parameters besides those declared in <span class="cmtt-10x-x-109">Hyperparams</span>).</p>
<p>The result of the function is either the <span class="cmtt-10x-x-109">int </span>value, which will be the amount of games we played before reaching the score of <span class="cmtt-10x-x-109">18</span>, or <span class="cmtt-10x-x-109">None </span>if we decided to stop the training early. This is required, as some hyperparameter combinations might fail to converge or converge too slowly, so to save time we stop the training without waiting for too long.</p>
<p>Then we define the hyperparameter search space ‚Äî which is a <span class="cmtt-10x-x-109">dict </span>with string keys (parameter name) and the <span class="cmtt-10x-x-109">tune </span>declaration of possible values to explore. It could be a probability distribution (uniform, loguniform, normal, etc.) or an explicit list of values to try. You can also use <span class="cmtt-10x-x-109">tune.grid</span><span class="cmtt-10x-x-109">_search </span>declaration with a list of values. In that case, all the values will be tried.</p>
<p>In our case, we sample the learning rate from the loguniform distribution and gamma from the list of 6 values ranging from 0.9 to 0.995.</p>
<p>Next, we have the <span class="cmtt-10x-x-109">tune</span><span class="cmtt-10x-x-109">_params </span>function:</p>
<div class="tcolorbox" id="tcolobox-160">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-213"><code>def tune_params( 
        base_params: Hyperparams, train_func: TrainFunc, device: torch.device, 
        samples: int = 10, extra_space: tt.Optional[tt.Dict[str, tt.Any]] = None, 
): 
    search_space = dict(BASE_SPACE) 
    if extra_space is not None: 
        search_space.update(extra_space) 
    config = tune.TuneConfig(num_samples=samples) 
 
    def objective(config: dict, device: torch.device) -&gt; dict: 
        keys = dataclasses.asdict(base_params).keys() 
        upd = {"tuner_mode": True} 
        for k, v in config.items(): 
            if k in keys: 
                upd[k] = v 
        params = dataclasses.replace(base_params, **upd) 
        res = train_func(params, device, config) 
        return {"episodes": res if res is not None else 10**6}</code></pre>
</div>
</div>
<p>This function is given the following arguments:</p>
<ul>
<li>
<p>Basic set of hyperparameters that will be used for training</p>
</li>
<li>
<p>Training function</p>
</li>
<li>
<p>Torch device to use</p>
</li>
<li>
<p>Amount of samples to perform during the round</p>
</li>
<li>
<p>Additional dictionary with search space</p>
</li>
</ul>
<p>Inside this function, we <span id="dx1-128032"/>have an objective function, which creates the <span class="cmtt-10x-x-109">Hyperparameters </span>object from the sampled <span class="cmtt-10x-x-109">dict</span>, calls the training function, and returns the dictionary (which is a requirement of the <span class="cmtt-10x-x-109">ray.tune </span>library).</p>
<p>The rest of the <span class="cmtt-10x-x-109">tune</span><span class="cmtt-10x-x-109">_params </span>function is simple:</p>
<div class="tcolorbox" id="tcolobox-161">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-214"><code>    obj = tune.with_parameters(objective, device=device) 
    if device.type == "cuda": 
        obj = tune.with_resources(obj, {"gpu": 1}) 
    tuner = tune.Tuner(obj, param_space=search_space, tune_config=config) 
    results = tuner.fit() 
    best = results.get_best_result(metric="episodes", mode="min") 
    print(best.config) 
    print(best.metrics)</code></pre>
</div>
</div>
<p>Here, we wrap the objective function to pass the torch device and take into account GPU resources. This is needed to allow Ray to properly parallelize the tuning process. If you have multiple GPUs installed on the machine, it will run several trainings in parallel. Then, we just create the <span class="cmtt-10x-x-109">Tuner </span>object and ask it to perform the hyperparameter search.</p>
<p>The final piece relevant to hyperparameter tuning is in the <span class="cmtt-10x-x-109">setup</span><span class="cmtt-10x-x-109">_ignite</span> function. It checks for situations when the training process is not converging, so we stop the training to avoid infinite waiting. To do this, we install the Ignite event handler if we‚Äôre in the hyperparameter tuning mode:</p>
<div class="tcolorbox" id="tcolobox-162">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-215"><code>    if params.tuner_mode: 
        @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED) 
        def episode_completed(trainer: Engine): 
            avg_reward = trainer.state.metrics.get(‚Äôavg_reward‚Äô) 
            max_episodes = params.episodes_to_solve * 1.1 
            if trainer.state.episode &gt; tuner_reward_episode and \ 
                    avg_reward &lt; tuner_reward_min: 
                trainer.should_terminate = True 
                trainer.state.solved = False 
            elif trainer.state.episode &gt; max_episodes: 
                trainer.should_terminate = True 
                trainer.state.solved = False 
            if trainer.should_terminate: 
                print(f"Episode {trainer.state.episode}, " 
                      f"avg_reward {avg_reward:.2f}, terminating")</code></pre>
</div>
</div>
<p>Here, we check <span id="dx1-128056"/>for two conditions:</p>
<ul>
<li>
<p>If the mean reward is lower than <span class="cmtt-10x-x-109">tuner</span><span class="cmtt-10x-x-109">_reward</span><span class="cmtt-10x-x-109">_min </span>(which is an argument to the <span class="cmtt-10x-x-109">setup</span><span class="cmtt-10x-x-109">_ignite </span>function and equal to -19 by default) after 100 games (provided in the <span class="cmtt-10x-x-109">tuner</span><span class="cmtt-10x-x-109">_reward</span><span class="cmtt-10x-x-109">_episode </span>argument). This means that it‚Äôs quite unlikely that we‚Äôll converge at all.</p>
</li>
<li>
<p>We played more than <span class="cmtt-10x-x-109">max</span><span class="cmtt-10x-x-109">_episodes </span>amount of games and still haven‚Äôt solved the game. In the default config, we set this limit to 500 games.</p>
</li>
</ul>
<p>In both cases, we stop the training and set the <span class="cmtt-10x-x-109">solved </span>attribute to <span class="cmtt-10x-x-109">False</span>, which will return a high constant metric value in our tuning process.</p>
<p>That‚Äôs it for the hyperparameter tuning code. Before we run it and check the results, let‚Äôs first start a single training using the parameters we used in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>.</p>
</section>
<section class="level4 subsectionHead" id="results-with-common-parameters">
<h2 class="heading-2" id="sigil_toc_id_111"> <span id="x1-1290008.1.4"/>Results with common parameters</h2>
<p>If we run the <span id="dx1-129001"/>training with the argument <span class="cmtt-10x-x-109">--params common</span>, we‚Äôll train the Pong game using hyperparameters from the <span class="cmtt-10x-x-109">common.py </span>module. As an option, you can use the <span class="cmtt-10x-x-109">--params best </span>command line to train on the best values for this particular DQN extension.</p>
<p>Okay, let‚Äôs start the training using the following command:</p>
<pre class="lstlisting" id="listing-216"><code>Chapter08$ ./01_dqn_basic.py --dev cuda --params common 
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) 
[Powered by Stella] 
Episode 1: reward=-21, steps=848, speed=0.0 f/s, elapsed=0:00:11 
Episode 2: reward=-21, steps=850, speed=0.0 f/s, elapsed=0:00:11 
Episode 3: reward=-19, steps=1039, speed=0.0 f/s, elapsed=0:00:11 
Episode 4: reward=-21, steps=884, speed=0.0 f/s, elapsed=0:00:11 
Episode 5: reward=-19, steps=1146, speed=0.0 f/s, elapsed=0:00:11 
Episode 6: reward=-20, steps=997, speed=0.0 f/s, elapsed=0:00:11 
Episode 7: reward=-21, steps=972, speed=0.0 f/s, elapsed=0:00:11 
Episode 8: reward=-21, steps=882, speed=0.0 f/s, elapsed=0:00:11 
Episode 9: reward=-21, steps=898, speed=0.0 f/s, elapsed=0:00:11 
Episode 10: reward=-20, steps=947, speed=0.0 f/s, elapsed=0:00:11 
Episode 11: reward=-21, steps=762, speed=227.7 f/s, elapsed=0:00:12 
Episode 12: reward=-20, steps=991, speed=227.8 f/s, elapsed=0:00:17 
Episode 13: reward=-21, steps=762, speed=227.9 f/s, elapsed=0:00:20 
Episode 14: reward=-20, steps=948, speed=227.9 f/s, elapsed=0:00:24 
Episode 15: reward=-20, steps=992, speed=228.0 f/s, elapsed=0:00:28 
......</code></pre>
<p>Every line in the <span id="dx1-129021"/>output is written at the end of the game episode, showing the episode reward, a count of steps, the speed, and the total training time. For the basic DQN version and common hyperparameters, it usually takes about 700K frames and about 400 games to reach the mean reward of 18, so be patient. During the training, we can check the dynamics of the training process in TensorBoard, which shows charts for epsilon, raw reward values, average reward, and speed. The following charts show the reward and the number of steps for episodes (the bottom <span class="cmti-10x-x-109">x </span>axis shows the wall clock time, and the top axis is the episode number):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_01.png" width="600"/> <span id="x1-129022r1"/></p>
<span class="id">Figure¬†8.1: Plots with reward (left) and count of steps per episode (right) </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_02.png" width="600"/> <span id="x1-129023r2"/></p>
<span class="id">Figure¬†8.2: Plots with training speed (left) and average training loss (right) </span>
</div>
<p>It is also <span id="dx1-129024"/>worth noting how the count of steps per episode changes during the training. Initially, it increases, as our network starts winning more and more games, but after a certain level, the count of steps decreases 2x and stays almost constant. This is driven by our <span class="cmmi-10x-x-109">Œ≥ </span>parameter, which discounts the agent‚Äôs reward over time, so it tries not just to accumulate as much of a reward as possible, but also to do it <span class="cmti-10x-x-109">efficiently</span>.</p>
</section>
<section class="level4 subsectionHead" id="tuned-baseline-dqn">
<h2 class="heading-2" id="sigil_toc_id_112"> <span id="x1-1300008.1.5"/>Tuned baseline DQN</h2>
<p>After running the <span id="dx1-130001"/>baseline DQN <span id="dx1-130002"/>with the command-line argument <span class="cmtt-10x-x-109">--tune 30</span> (which took about a day on one GPU), I was able to find the following parameters, which solves Pong in 340 episodes (instead of 360):</p>
<div class="tcolorbox" id="tcolobox-163">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-217"><code>    learning_rate=9.932831968547505e-05, 
    gamma=0.98,</code></pre>
</div>
</div>
<p>As you can see, the learning rate is almost the same as before (10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">4</span></sup>), but gamma is lower (<span class="cmtt-10x-x-109">0.98 </span>versus <span class="cmtt-10x-x-109">0.99</span>). This might be an indication that Pong has relatively short subtrajectories with action-reward causality, so decreasing the <span class="cmmi-10x-x-109">Œ≥ </span>has a stabilizing effect on the training.</p>
<p>In the following figure, you can see a comparison of the reward and steps per episode for both tuned and untuned versions (and the difference is quite minor):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_03.png" width="600"/> <span id="x1-130005r3"/></p>
<span class="id">Figure¬†8.3: Plots with reward (left) and count of steps per episode (right) for tuned and untuned hyperparameters </span>
</div>
<p>Now we have <span id="dx1-130006"/>our baseline <span id="dx1-130007"/>DQN version and are ready to explore method modifications proposed by Hessel et al.</p>
</section>
</section>
<section class="level3 sectionHead" id="n-step-dqn">
<h1 class="heading-1" id="sigil_toc_id_113"> <span id="x1-1310008.2"/>N-step DQN</h1>
<p>The first <span id="dx1-131001"/>improvement that we will implement and evaluate is quite an old one. It <span id="dx1-131002"/>was first introduced by Sutton in the paper <span class="cmti-10x-x-109">Learning to Predict by the Methods of</span> <span class="cmti-10x-x-109">Temporal Differences </span>[<span id="x1-131003"/><a href="#">Sut88</a>]. To get the idea, let‚Äôs look at the Bellman update used in Q-learning once again:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="34" src="../Images/eq26.png" width="350"/>
</div>
<p>This equation is recursive, which means that we can express <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span><span class="cmr-8">+1</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmmi-8">t</span><span class="cmr-8">+1</span></sub>) in terms of itself, which gives us this result:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="36" src="../Images/eq27.png" width="479"/>
</div>
<p>Value <span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">a,t</span><span class="cmr-8">+1</span></sub> means local reward at time <span class="cmmi-10x-x-109">t </span>+ 1, after issuing action <span class="cmmi-10x-x-109">a</span>. However, if we assume that action <span class="cmmi-10x-x-109">a </span>at step <span class="cmmi-10x-x-109">t </span>+ 1 was chosen optimally, or close to optimally, we can omit the <span class="cmmi-10x-x-109">max</span><sub><span class="cmmi-8">a</span></sub> operation and obtain this:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="38" src="../Images/eq28.png" width="418"/>
</div>
<p>This value can be unrolled again and again any number of times. As you may guess, this unrolling can be easily applied to our DQN update by replacing one-step transition sampling with longer transition sequences of n-steps. To understand why this unrolling will help us to speed up training, let‚Äôs consider the example illustrated in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-131004r4"><span class="cmti-10x-x-109">8.4</span></a>. Here, we have a simple environment of four states (<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">4</span></sub>) and the only action available at every state, except <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">4</span></sub>, which is a terminal state:</p>
<div class="minipage">
<p><img alt="ssssararar1234123 " height="50" src="../Images/B22150_08_04.png" width="300"/> <span id="x1-131004r4"/></p>
<span class="id">Figure¬†8.4: A transition diagram for a simple environment </span>
</div>
<p>So, what happens in a <span id="dx1-131005"/>one-step case? We have three total updates possible (we don‚Äôt use max, as there is only one action available):</p>
<ol>
<li>
<div id="x1-131007x1">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">Œ≥Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>)</p>
</div>
</li>
<li>
<div id="x1-131009x2">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">2</span></sub> + <span class="cmmi-10x-x-109">Œ≥Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>)</p>
</div>
</li>
<li>
<div id="x1-131011x3">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">3</span></sub></p>
</div>
</li>
</ol>
<p>Let‚Äôs imagine <span id="dx1-131012"/>that, at the beginning of the training, we complete the preceding updates in this order. The first two updates will be useless, as our current <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>) and <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>) are incorrect and contain initial random values. The only useful update will be update 3, which will correctly assign reward <span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">3</span></sub> to the state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub> prior to the terminal state.</p>
<p>Now let‚Äôs perform the updates over and over again. On the second iteration, the correct value will be assigned to <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>), but the update of <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span>) will still be noisy. Only on the third iteration will we get the valid values for every <span class="cmmi-10x-x-109">Q</span>. So, even in a one-step case, it takes three steps to propagate the correct values to all the states.</p>
<p>Now let‚Äôs consider a two-step case. This situation again has three updates:</p>
<ol>
<li>
<div id="x1-131014x1">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">Œ≥r</span><sub><span class="cmr-8">2</span></sub> + <span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">2</span></sup><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>)</p>
</div>
</li>
<li>
<div id="x1-131016x2">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">2</span></sub> + <span class="cmmi-10x-x-109">Œ≥r</span><sub><span class="cmr-8">3</span></sub></p>
</div>
</li>
<li>
<div id="x1-131018x3">
<p><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>) <span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">3</span></sub></p>
</div>
</li>
</ol>
<p>In this case, on the first loop over the updates, the correct values will be assigned to both <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,a</span>) and <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub><span class="cmmi-10x-x-109">,a</span>). On the second iteration, the value of <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span>) will also be properly updated. So, multiple steps improve the propagation speed of values, which improves convergence. You may be thinking, ‚ÄúIf it‚Äôs so helpful, let‚Äôs unroll the Bellman equation, say, 100 steps ahead. Will it speed up our convergence 100 times?‚Äù Unfortunately, the answer is no. Despite our expectations, our DQN will fail to converge at all.</p>
<p>To understand why, let‚Äôs again return to our unrolling process, especially where we dropped the <span class="cmmi-10x-x-109">max</span><sub><span class="cmmi-8">a</span></sub>. Was it correct? Strictly speaking, no. We omitted the <span class="cmmi-10x-x-109">max </span>operation at the intermediate step, assuming that our action selection during experience gathering (or our policy) was optimal. What if it wasn‚Äôt, for example, at the beginning of the training, when our agent acted randomly? In that case, our calculated value for <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmmi-8">t</span></sub>) may be smaller than the optimal value of the state (as some steps have been taken randomly, but not following the most promising paths by maximizing the Q-value). The more steps on which we unroll the Bellman equation, the more incorrect our update could be.</p>
<p>Our large experience <span id="dx1-131019"/>replay buffer will make the situation even worse, as it will <span id="dx1-131020"/>increase the chance of getting transitions obtained from the old bad policy (dictated by old bad approximations of <span class="cmmi-10x-x-109">Q</span>). This will lead to a wrong update of the current <span class="cmmi-10x-x-109">Q </span>approximation, so it can easily break our training progress. This problem is a fundamental characteristic of RL methods, as was briefly mentioned in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch008.xhtml#x1-740004"><span class="cmti-10x-x-109">4</span></a>, when we talked about RL methods‚Äô taxonomy.</p>
<p>There are two large classes of methods:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Off-policy methods</span>: The first class of off-policy methods doesn‚Äôt depend on the ‚Äúfreshness of data.‚Äù For example, a simple DQN is off-policy, which means that we can use very old data sampled from the environment several million steps ago, and this data will still be useful for learning. That‚Äôs because we are just updating the value of the action, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmmi-8">t</span></sub>), with the immediate reward, plus the discounted current approximation of the best action‚Äôs value. Even if action <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">t</span></sub> was sampled randomly, it doesn‚Äôt matter because for this particular action <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">t</span></sub>, in the state <span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span></sub>, our update will be correct. That‚Äôs why in off-policy methods, we can use a very large experience buffer to make our data closer to being <span class="cmbx-10x-x-109">independent and identically distributed </span>(<span class="cmbx-10x-x-109">iid</span>) .</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">On-policy methods</span>: On the other hand, on-policy methods heavily depend on the training data to be sampled according to the current policy that we are updating. That happens because on-policy methods try to improve the current policy indirectly (as in the previous n-step DQN) or directly (all of <span class="cmti-10x-x-109">Part 3 </span>of the book is devoted to such methods).</p>
</li>
</ul>
<p>So, which class of methods is better? Well, it depends. Off-policy methods allow you to train on the previous large history of data or even on human demonstrations, but they usually are slower to converge. On-policy methods are typically faster, but require much more fresh data from the environment, which can be costly. Just imagine a self-driving car trained with the on-policy method. It will cost you a lot of crashed cars before the system learns that walls and trees are things that it should avoid!</p>
<p>You may have a question: why are we talking about an n-step DQN if this ‚Äún-stepness‚Äù turns it into an on-policy method, which will make our large experience buffer useless? In practice, this is usually not black and white. You may still use an n-step DQN if it will help to speed up DQNs, but you need to be modest with the selection of <span class="cmtt-10x-x-109">n</span>. Small values of two or three usually work well, because our trajectories in the experience buffer are not that different from one-step transitions. In such cases, convergence speed usually improves proportionally, but large values of <span class="cmtt-10x-x-109">n </span>can break the training process. So, the number of steps should be tuned, but convergence speeding up usually makes it worth doing.</p>
<section class="level4 subsectionHead" id="implementation-1">
<h2 class="heading-2" id="sigil_toc_id_114"> <span id="x1-1320008.2.1"/>Implementation</h2>
<p>As the <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>class already supports the multi-step Bellman unroll, our n-step version of a DQN is extremely simple. There <span id="dx1-132001"/>are only two modifications that we need to make to the basic DQN to turn it into an n-step version:</p>
<ul>
<li>
<p>Pass the count of steps that we want to unroll on <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>creation in the <span class="cmtt-10x-x-109">steps</span><span class="cmtt-10x-x-109">_count</span> parameter.</p>
</li>
<li>
<p>Pass the correct gamma to the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dqn </span>function. This modification is really easy to overlook, which could be harmful to convergence. As our Bellman is now n-steps, the discount coefficient for the last state in the experience chain will no longer be just <span class="cmmi-10x-x-109">Œ≥</span>, but <span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmmi-8">n</span></sup>.</p>
</li>
</ul>
<p>You can find the whole example in <span class="cmtt-10x-x-109">Chapter08/02</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_n</span><span class="cmtt-10x-x-109">_steps.py</span>, with only the modified lines shown here:</p>
<div class="tcolorbox" id="tcolobox-164">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-218"><code>    exp_source = ptan.experience.ExperienceSourceFirstLast( 
        env, agent, gamma=params.gamma, env_seed=common.SEED, 
        steps_count=n_steps 
    )</code></pre>
</div>
</div>
<p>The <span class="cmtt-10x-x-109">n</span><span class="cmtt-10x-x-109">_steps </span>value is a count of steps passed in command-line arguments; the default is to use four steps.</p>
<p>Another modification is in <span class="cmtt-10x-x-109">gamma </span>passed to the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dqn </span>function:</p>
<div class="tcolorbox" id="tcolobox-165">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-219"><code>        loss_v = common.calc_loss_dqn( 
            batch, net, tgt_net.target_model, 
            gamma=params.gamma**n_steps, device=device)</code></pre>
</div>
</div>
</section>
<section class="level4 subsectionHead" id="results">
<h2 class="heading-2" id="sigil_toc_id_115"> <span id="x1-1330008.2.2"/>Results</h2>
<p>The <span id="dx1-133001"/>training module <span class="cmtt-10x-x-109">Chapter08/02</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_n</span><span class="cmtt-10x-x-109">_steps.py </span>can be started as before, with the additional command-line option <span class="cmtt-10x-x-109">-n</span>, which gives a count of steps to unroll the Bellman equation. These are charts for our baseline and n-step DQN (using a common set of parameters), with <span class="cmmi-10x-x-109">n </span>being equal to 2 and 3. As you can see, the Bellman unroll has given a significant convergence speedup:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_05.png" width="600"/> <span id="x1-133002r5"/></p>
<span class="id">Figure¬†8.5: The reward and number of steps for basic (one-step) DQN and n-step versions </span>
</div>
<p>As you can see<span id="dx1-133003"/> in the diagram, the three-step DQN converges significantly faster than the simple DQN, which is a nice improvement. So, what about a larger <span class="cmmi-10x-x-109">n</span>? <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-133004r6"><span class="cmti-10x-x-109">8.6</span></a> shows the reward dynamics for <span class="cmmi-10x-x-109">n </span>= 3<span class="cmmi-10x-x-109">‚Ä¶</span>6:</p>
<div class="minipage">
<p><img alt="PIC" height="200" src="../Images/B22150_08_06.png" width="300"/> <span id="x1-133004r6"/></p>
<span class="id">Figure¬†8.6: Reward dynamics for cases with <span class="cmmi-10x-x-109">n </span>= 3<span class="cmmi-10x-x-109">‚Ä¶</span>6 with common hyperparameters </span>
</div>
<p>As you can see, going from three steps to four has given some improvement, but it is much less than before. The variant with <span class="cmmi-10x-x-109">n </span>= 5 is worse and very close to <span class="cmmi-10x-x-109">n </span>= 2. The same is true for <span class="cmmi-10x-x-109">n </span>= 6. So, in our case, <span class="cmmi-10x-x-109">n </span>= 3 looks optimal.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-1">
<h2 class="heading-2" id="sigil_toc_id_116"> <span id="x1-1340008.2.3"/>Hyperparameter tuning</h2>
<p>In this extension, hyperparameter tuning was <span id="dx1-134001"/>done individually for every <span class="cmmi-10x-x-109">n </span>from 2 to 7. The following table shows the best parameters and number of games they require to solve the game:</p>
<div class="table">
<figure class="float">
<div class="center">
<div class="tabular">
<table class="table-container" id="TBL-3">
<tbody>
<tr id="TBL-3-1-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-1-1"><span class="cmbx-10x-x-109">n </span></td>
<td class="table-cell" id="TBL-3-1-2"><span class="cmbx-10x-x-109">Learning rate </span></td>
<td class="table-cell" id="TBL-3-1-3"><span class="cmmi-10x-x-109">Œ≥ </span></td>
<td class="table-cell" id="TBL-3-1-4"><span class="cmbx-10x-x-109">Games </span></td>
</tr>
<tr id="TBL-3-2-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-2-1">2</td>
<td class="table-cell" id="TBL-3-2-2">3<span class="cmmi-10x-x-109">.</span>97 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-2-3">0.98</td>
<td class="table-cell" id="TBL-3-2-4">293</td>
</tr>
<tr id="TBL-3-3-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-3-1">3</td>
<td class="table-cell" id="TBL-3-3-2">7<span class="cmmi-10x-x-109">.</span>82 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-3-3">0.98</td>
<td class="table-cell" id="TBL-3-3-4">260</td>
</tr>
<tr id="TBL-3-4-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-4-1">4</td>
<td class="table-cell" id="TBL-3-4-2">6<span class="cmmi-10x-x-109">.</span>07 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-4-3">0.98</td>
<td class="table-cell" id="TBL-3-4-4">290</td>
</tr>
<tr id="TBL-3-5-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-5-1">5</td>
<td class="table-cell" id="TBL-3-5-2">7<span class="cmmi-10x-x-109">.</span>52 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-5-3">0.99</td>
<td class="table-cell" id="TBL-3-5-4">268</td>
</tr>
<tr id="TBL-3-6-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-6-1">6</td>
<td class="table-cell" id="TBL-3-6-2">6<span class="cmmi-10x-x-109">.</span>78 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-6-3">0.995</td>
<td class="table-cell" id="TBL-3-6-4">261</td>
</tr>
<tr id="TBL-3-7-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-3-7-1">7</td>
<td class="table-cell" id="TBL-3-7-2">8<span class="cmmi-10x-x-109">.</span>59 <span class="cmsy-10x-x-109">‚ãÖ </span>10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">5</span></sup></td>
<td class="table-cell" id="TBL-3-7-3">0.98</td>
<td class="table-cell" id="TBL-3-7-4">284</td>
</tr>
</tbody>
</table>
</div>
<span id="x1-134002r1"/>
<span class="id">Table¬†8.1: The best hyperparameters (learning rate and gamma) for every <span class="cmmi-10x-x-109">n</span> </span>
</div>
</figure>
</div>
<p>This table also confirms the conclusions of the untuned version comparison ‚Äî unrolling the Bellman equation for two and three steps improves the convergence, but a further increase of <span class="cmmi-10x-x-109">n </span>produces worse results. <span class="cmmi-10x-x-109">n </span>= 6 gives us a comparable result to <span class="cmmi-10x-x-109">n </span>= 3, but the outcomes for <span class="cmmi-10x-x-109">n </span>= 4 and <span class="cmmi-10x-x-109">n </span>= 5 are worse, so we should stop at <span class="cmmi-10x-x-109">n </span>= 3.</p>
<p><span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-134003r7"><span class="cmti-10x-x-109">8.7</span></a> compares the training dynamics of tuned versions of the baseline and N-step DQN with <span class="cmmi-10x-x-109">n </span>= 2 and <span class="cmmi-10x-x-109">n </span>= 3.</p>
<div class="minipage">
<p> <img alt="PIC" height="200" src="../Images/B22150_08_07.png" width="600"/> <span id="x1-134003r7"/></p>
<span class="id">Figure¬†8.7: The reward and number of steps after hyperparameter tuning </span>
</div>
</section>
</section>
<section class="level3 sectionHead" id="double-dqn">
<h1 class="heading-1" id="sigil_toc_id_117"> <span id="x1-1350008.3"/>Double DQN</h1>
<p>The <span id="dx1-135001"/>next fruitful <span id="dx1-135002"/>idea on how to improve a basic DQN came from DeepMind researchers in the paper titled <span class="cmti-10x-x-109">Deep reinforcement learning with double</span> <span class="cmti-10x-x-109">Q-learning </span>[<span id="x1-135003"/><a href="#">VGS16</a>]. In the paper, the authors demonstrated that the basic DQN tends to overestimate values for <span class="cmmi-10x-x-109">Q</span>, which may be harmful to training performance and sometimes can lead to suboptimal policies. The root cause of this is the max operation in the Bellman equation, but the strict proof is a bit complicated (you can find the full explanation in the paper). As a solution to this problem, the authors proposed modifying the Bellman update a bit.</p>
<p>In the basic DQN, our target value for <span class="cmmi-10x-x-109">Q </span>looked like this:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="35" src="../Images/eq29.png" width="328"/>
</div>
<p><span class="cmmi-10x-x-109">Q</span><span class="cmsy-10x-x-109">‚Ä≤</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span><span class="cmr-8">+1</span></sub><span class="cmmi-10x-x-109">,a</span>) was Q-values calculated using our target network, the weights of which are copied from the trained network every <span class="cmmi-10x-x-109">n </span>steps. The authors of the paper proposed choosing actions for the next state using the trained network, but taking values of <span class="cmmi-10x-x-109">Q </span>from the target network. So, the new expression for target Q-values will look like this:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="35" src="../Images/eq30.png" width="459"/>
</div>
<p>The authors proved that this simple tweak fixes overestimation completely, and they called this new architecture <span class="cmbx-10x-x-109">double DQN</span>.</p>
<section class="level4 subsectionHead" id="implementation-2">
<h2 class="heading-2" id="sigil_toc_id_118"> <span id="x1-1360008.3.1"/>Implementation</h2>
<p>The core <span id="dx1-136001"/>implementation is very simple. What we need to do is slightly modify our loss function. But let‚Äôs go a step further and compare action values produced by basic DQN and double DQN. According to the paper author‚Äôs our baseline DQN should have consistently higher values predicted for the same states than the double DQN version. To do this, we store a random held-out set of states and periodically calculate the mean value of the best action for every state in the evaluation set.</p>
<p>The complete example is in <span class="cmtt-10x-x-109">Chapter08/03</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_double.py</span>. Let‚Äôs first take a look at the loss function:</p>
<div class="tcolorbox" id="tcolobox-166">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-220"><code>def calc_loss_double_dqn( 
        batch: tt.List[ptan.experience.ExperienceFirstLast], 
        net: nn.Module, tgt_net: nn.Module, gamma: float, device: torch.device): 
    states, actions, rewards, dones, next_states = common.unpack_batch(batch) 
 
    states_v = torch.as_tensor(states).to(device) 
    actions_v = torch.tensor(actions).to(device) 
    rewards_v = torch.tensor(rewards).to(device) 
    done_mask = torch.BoolTensor(dones).to(device)</code></pre>
</div>
</div>
<p>We will use this function instead of <span class="cmtt-10x-x-109">common.calc</span><span class="cmtt-10x-x-109">_loss</span><span class="cmtt-10x-x-109">_dqn </span>and they both share lots of code. The main difference is in the next Q-values estimation:</p>
<div class="tcolorbox" id="tcolobox-167">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-221"><code>    actions_v = actions_v.unsqueeze(-1) 
    state_action_vals = net(states_v).gather(1, actions_v) 
    state_action_vals = state_action_vals.squeeze(-1) 
    with torch.no_grad(): 
        next_states_v = torch.as_tensor(next_states).to(device) 
        next_state_acts = net(next_states_v).max(1)[1] 
        next_state_acts = next_state_acts.unsqueeze(-1) 
        next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1) 
        next_state_vals[done_mask] = 0.0 
        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v 
    return nn.MSELoss()(state_action_vals, exp_sa_vals)</code></pre>
</div>
</div>
<p>The <span id="dx1-136022"/>preceding code snippet calculates the loss in a slightly different way. In the double DQN version, we calculate the best action to take in the next state using our main trained network, but values corresponding to this action come from the target network.</p>
<div class="tcolorbox infobox" id="tcolobox-168">
<div class="tcolorbox-content">
<p>This part could be implemented in a faster way, by combining <span class="cmtt-10x-x-109">next</span><span class="cmtt-10x-x-109">_states</span><span class="cmtt-10x-x-109">_v </span>with <span class="cmtt-10x-x-109">states</span><span class="cmtt-10x-x-109">_v </span>and calling our main network only once, but it will make the code less clear.</p>
</div>
</div>
<p>The rest of the function is the same: we mask completed episodes and compute the <span class="cmbx-10x-x-109">mean squared error </span>(<span class="cmbx-10x-x-109">MSE</span>) loss between Q-values predicted by the network and approximated Q-values.</p>
<p>The last function that we consider calculates the values of our held-out state:</p>
<div class="tcolorbox" id="tcolobox-169">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-222"><code>@torch.no_grad() 
def calc_values_of_states(states: np.ndarray, net: nn.Module, device: torch.device): 
    mean_vals = [] 
    for batch in np.array_split(states, 64): 
        states_v = torch.tensor(batch).to(device) 
        action_values_v = net(states_v) 
        best_action_values_v = action_values_v.max(1)[0] 
        mean_vals.append(best_action_values_v.mean().item()) 
    return np.mean(mean_vals)</code></pre>
</div>
</div>
<p>There is nothing complicated here: we just split our held-out states array into equal chunks and pass every chunk to the network to obtain action values. From those values, we choose the action with the largest value (for every state) and calculate the mean of such values. As our array with states is fixed for the whole training process, and this array is large enough (in the code, we store 1,000 states), we can compare the dynamics of this mean value in both DQN variants. The rest <span id="dx1-136032"/>of the <span class="cmtt-10x-x-109">03</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_double.py </span>file is almost the same; the two differences are usage of our tweaked loss function and keeping a randomly sampled 1,000 states for periodical evaluation. This happens in the <span class="cmtt-10x-x-109">process</span><span class="cmtt-10x-x-109">_batch</span> function:</p>
<div class="tcolorbox" id="tcolobox-170">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-223"><code>        if engine.state.iteration % EVAL_EVERY_FRAME == 0: 
            eval_states = getattr(engine.state, "eval_states", None) 
            if eval_states is None: 
                eval_states = buffer.sample(STATES_TO_EVALUATE) 
                eval_states = [ 
                    np.asarray(transition.state) 
                    for transition in eval_states 
                ] 
                eval_states = np.asarray(eval_states) 
                engine.state.eval_states = eval_states 
            engine.state.metrics["values"] = \ 
                common.calc_values_of_states(eval_states, net, device)</code></pre>
</div>
</div>
</section>
<section class="level4 subsectionHead" id="results-1">
<h2 class="heading-2" id="sigil_toc_id_119"> <span id="x1-1370008.3.2"/>Results</h2>
<p>My experiments <span id="dx1-137001"/>show that with common hyperparameters, double DQN has a negative effect on reward dynamics. Sometimes, double DQN leads to better initial dynamics and the trained agent learns how to win more games faster, but reaching the end reward boundary takes longer. You can perform your own experiment on other games or try parameters from the original paper.</p>
<p>The following are reward charts from the experiment where double DQN was a bit better than the baseline version:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_08.png" width="500"/> <span id="x1-137002r8"/></p>
<span class="id">Figure¬†8.8: Reward dynamics for double and baseline DQN </span>
</div>
<p>Besides the <span id="dx1-137003"/>standard metrics, the example also outputs the mean value for the held-out set of states, which are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-137004r9"><span class="cmti-10x-x-109">8.9</span></a>.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_09.png" width="500"/> <span id="x1-137004r9"/></p>
<span class="id">Figure¬†8.9: Values predicted by the network for held-out states </span>
</div>
<p>As you can see, the <span id="dx1-137005"/>basic DQN does an overestimation of values, so values decrease after a certain level. In contrast, the double DQN grows more consistently. In my experiments, the double DQN has only a small effect on the training time, but this doesn‚Äôt necessarily mean that the double DQN is useless, as Pong is a simple environment. In more complicated games, the double DQN could give better results.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-2">
<h2 class="heading-2" id="sigil_toc_id_120"> <span id="x1-1380008.3.3"/>Hyperparameter tuning</h2>
<p>The tuning of <span id="dx1-138001"/>hyperparameters also wasn‚Äôt very successful for the double DQN. After 30 trials, the best values for the learning rate and gamma were able to solve Pong in 412 games, which is worse than the baseline DQN.</p>
</section>
</section>
<section class="level3 sectionHead" id="noisy-networks">
<h1 class="heading-1" id="sigil_toc_id_121"> <span id="x1-1390008.4"/>Noisy networks</h1>
<p>The next <span id="dx1-139001"/>improvement that we are going to <span id="dx1-139002"/>look at addresses another RL problem: exploration of the environment. The paper that we will draw from is called <span class="cmti-10x-x-109">Noisy networks for exploration </span>[<span id="x1-139003"/><a href="#">For+17</a>] and it has a very simple idea for learning exploration characteristics during training instead of having a separate schedule related to exploration.</p>
<p>A classical DQN achieves exploration by choosing random actions with a specially defined hyperparameter <span class="cmmi-10x-x-109">ùúñ</span>, which is slowly decreased over time from 1.0 (fully random actions) to some small ratio of 0.1 or 0.02. This process works well for simple environments with short episodes, without much non-stationarity during the game; but even in such simple cases, it requires tuning to make the training processes efficient.</p>
<p>In the <span class="cmti-10x-x-109">Noisy Networks </span>paper, the authors proposed a quite simple solution that, nevertheless, works well. They add noise to the weights of fully connected layers of the network and adjust the parameters of this noise during training using backpropagation.</p>
<div class="tcolorbox infobox" id="tcolobox-171">
<div class="tcolorbox-content">
<p>This method shouldn‚Äôt be confused with ‚Äúthe network decides where to explore more,‚Äù which is a much more complex approach that also has widespread support (for example, see articles about intrinsic motivation and count-based exploration methods [<span id="x1-139004"/><a href="#">Ost+17</a>], [<span id="x1-139005"/><a href="#">Mar+17</a>]). We will discuss advanced exploration techniques in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch025.xhtml#x1-39100021"><span class="cmti-10x-x-109">21</span></a>.</p>
</div>
</div>
<p>The authors <span id="dx1-139006"/>proposed two ways of adding the <span id="dx1-139007"/>noise, both of which work according to their experiments, but they have different computational overheads:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Independent Gaussian noise</span>: For every weight in a fully connected layer, we have a random value that we draw from the normal distribution. Parameters of the noise, <span class="cmmi-10x-x-109">Œº </span>and <span class="cmmi-10x-x-109">œÉ</span>, are stored inside the layer and get trained using backpropagation in the same way that we train weights of the standard linear layer. The output of such a ‚Äúnoisy layer‚Äù is calculated in the same way as in a linear layer.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Factorized Gaussian noise</span>: To minimize the number of random values to be sampled, the authors proposed keeping only two random vectors: one with the size of the input and another with the size of the output of the layer. Then, a random matrix for the layer is created by calculating the outer product of the vectors.</p>
</li>
</ul>
<section class="level4 subsectionHead" id="implementation-3">
<h2 class="heading-2" id="sigil_toc_id_122"> <span id="x1-1400008.4.1"/>Implementation</h2>
<p>In PyTorch, both <span id="dx1-140001"/>methods can be easily implemented in a very straightforward way. What we need to do is create our own custom <span class="cmtt-10x-x-109">nn.Linear </span>layer with weights calculated as <span class="cmmi-10x-x-109">w</span><sub><span class="cmmi-8">i,j</span></sub> = <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmmi-8">i,j</span></sub> + <span class="cmmi-10x-x-109">œÉ</span><sub><span class="cmmi-8">i,j</span></sub> <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">ùúñ</span><sub><span class="cmmi-8">i,j</span></sub>, where <span class="cmmi-10x-x-109">Œº </span>and <span class="cmmi-10x-x-109">œÉ </span>are trainable parameters and <span class="cmmi-10x-x-109">ùúñ </span><span class="cmsy-10x-x-109">‚àºùí©</span>(0<span class="cmmi-10x-x-109">,</span>1) is random noise sampled from the normal distribution after every optimization step.</p>
<p>Previous editions of the book used my implementation of both methods, but now we‚Äôll simply use the implementation from the popular TorchRL library I mentioned in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch011.xhtml#x1-1070007"><span class="cmti-10x-x-109">7</span></a>. Let‚Äôs take a look at relevant parts of the implementation (the full code can be found in <span class="cmtt-10x-x-109">torchrl/modules/models/exploration.py </span>in the TorchRL repository). The following is the constructor of the <span class="cmtt-10x-x-109">NoisyLinear </span>class, which creates all the parameters we need to optimize:</p>
<div class="tcolorbox" id="tcolobox-172">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-224"><code>class NoisyLinear(nn.Linear): 
    def __init__( 
        self, in_features: int, out_features: int, bias: bool = True, 
        device: Optional[DEVICE_TYPING] = None, dtype: Optional[torch.dtype] = None, 
        std_init: float = 0.1, 
    ): 
        nn.Module.__init__(self) 
        self.in_features = int(in_features) 
        self.out_features = int(out_features) 
        self.std_init = std_init 
 
        self.weight_mu = nn.Parameter( 
            torch.empty(out_features, in_features, device=device, 
                        dtype=dtype, requires_grad=True) 
        ) 
        self.weight_sigma = nn.Parameter( 
            torch.empty(out_features, in_features, device=device, 
                        dtype=dtype, requires_grad=True) 
        ) 
        self.register_buffer( 
            "weight_epsilon", 
            torch.empty(out_features, in_features, device=device, dtype=dtype), 
        ) 
        if bias: 
            self.bias_mu = nn.Parameter( 
                torch.empty(out_features, device=device, dtype=dtype, requires_grad=True) 
            ) 
            self.bias_sigma = nn.Parameter( 
                torch.empty(out_features, device=device, dtype=dtype, requires_grad=True) 
            ) 
            self.register_buffer( 
                "bias_epsilon", torch.empty(out_features, device=device, dtype=dtype), 
            ) 
        else: 
            self.bias_mu = None 
        self.reset_parameters() 
        self.reset_noise()</code></pre>
</div>
</div>
<p>In the <span id="dx1-140039"/>constructor, we create matrices for <span class="cmmi-10x-x-109">Œº </span>and <span class="cmmi-10x-x-109">œÉ</span>. This implementation inherits from <span class="cmtt-10x-x-109">torch.nn.Linear</span>, but calls the <span class="cmtt-10x-x-109">nn.Module.</span><span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_init</span><span class="cmtt-10x-x-109">_</span><span class="cmtt-10x-x-109">_() </span>method, so normal <span class="cmtt-10x-x-109">Linear weights </span>and <span class="cmtt-10x-x-109">bias </span>buffers are not created.</p>
<p>To make new matrices trainable, we need to wrap their tensors in an <span class="cmtt-10x-x-109">nn.Parameter</span>. The <span class="cmtt-10x-x-109">register</span><span class="cmtt-10x-x-109">_buffer </span>method creates a tensor in the network that won‚Äôt be updated during backpropagation, but will be handled by the <span class="cmtt-10x-x-109">nn.Module </span>machinery (for example, it will be copied to the GPU with the <span class="cmtt-10x-x-109">cuda() </span>call). An extra parameter and buffer are created for the bias of the layer. At the end, we call the <span class="cmtt-10x-x-109">reset</span><span class="cmtt-10x-x-109">_parameters() </span>and <span class="cmtt-10x-x-109">reset</span><span class="cmtt-10x-x-109">_noise() </span>methods, which perform the initialization of the created trainable parameters and the buffer with the epsilon value.</p>
<p>In the following three methods, we initialize the trainable parameters <span class="cmmi-10x-x-109">Œº </span>and <span class="cmmi-10x-x-109">œÉ</span> according to the paper:</p>
<div class="tcolorbox" id="tcolobox-173">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-225"><code>    def reset_parameters(self) -&gt; None: 
        mu_range = 1 / math.sqrt(self.in_features) 
        self.weight_mu.data.uniform_(-mu_range, mu_range) 
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features)) 
        if self.bias_mu is not None: 
            self.bias_mu.data.uniform_(-mu_range, mu_range) 
            self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features)) 
 
    def reset_noise(self) -&gt; None: 
        epsilon_in = self._scale_noise(self.in_features) 
        epsilon_out = self._scale_noise(self.out_features) 
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in)) 
        if self.bias_mu is not None: 
            self.bias_epsilon.copy_(epsilon_out) 
 
    def _scale_noise( 
            self, size: Union[int, torch.Size, Sequence]) -&gt; torch.Tensor: 
        if isinstance(size, int): 
            size = (size,) 
        x = torch.randn(*size, device=self.weight_mu.device) 
        return x.sign().mul_(x.abs().sqrt_())</code></pre>
</div>
</div>
<p>The matrix for <span class="cmmi-10x-x-109">Œº </span>is initialized with uniform random values. The initial value for <span class="cmmi-10x-x-109">œÉ </span>is constant depending on the count of neurons in the layer.</p>
<p>For the noise initialization, factorized Gaussian noise is used ‚Äì we sample two random vectors and calculate the outer product to get the matrix for <span class="cmmi-10x-x-109">ùúñ</span>. The outer product is a linear algebra operation when two vectors of the same size are producing the square matrix filled with product of all combination of each vector‚Äôs element. The rest is simple: we redefine the <span class="cmtt-10x-x-109">weight </span>and <span class="cmtt-10x-x-109">bias </span>properties, which are expected in <span class="cmtt-10x-x-109">nn.Linear </span>layer, so <span class="cmtt-10x-x-109">NoisyLinear </span>could be used everywhere <span class="cmtt-10x-x-109">nn.Linear </span>is used:</p>
<div class="tcolorbox" id="tcolobox-174">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-226"><code>    @property 
    def weight(self) -&gt; torch.Tensor: 
        if self.training: 
            return self.weight_mu + self.weight_sigma * self.weight_epsilon 
        else: 
            return self.weight_mu 
 
    @property 
    def bias(self) -&gt; Optional[torch.Tensor]: 
        if self.bias_mu is not None: 
            if self.training: 
                return self.bias_mu + self.bias_sigma * self.bias_epsilon 
            else: 
                return self.bias_mu 
        else: 
            return None</code></pre>
</div>
</div>
<p>This <span id="dx1-140077"/>implementation is simple, but has one very subtle nuance ‚Äî the <span class="cmmi-10x-x-109">ùúñ </span>values are not updated after every optimization step (and it is not mentioned in the documentation). This issue is already reported in the TorchRL repo, but for the current stable release, we have to call the <span class="cmtt-10x-x-109">reset</span><span class="cmtt-10x-x-109">_noise() </span>method explicitly. Hopefully, it will be fixed and the <span class="cmtt-10x-x-109">NoisyLinear </span>layer will update the noise automatically.</p>
<p>From the implementation point of view, that‚Äôs it. What we now need to do to turn the classic DQN into a noisy network variant is just replace <span class="cmtt-10x-x-109">nn.Linear</span> (which are the two last layers in our DQN network) with the <span class="cmtt-10x-x-109">NoisyLinear </span>layer. Of course, you have to remove all the code related to the epsilon-greedy strategy.</p>
<p>To check the internal noise level during training, we can monitor the <span class="cmbx-10x-x-109">signal-to-noise ratio </span>(<span class="cmbx-10x-x-109">SNR</span>) of our noisy layers, which is <span class="cmmi-10x-x-109">RMS</span>(<span class="cmmi-10x-x-109">Œº</span>)<span class="cmmi-10x-x-109">‚àïRMS</span>(<span class="cmmi-10x-x-109">œÉ</span>), where <span class="cmmi-10x-x-109">RMS </span>is the root mean square of the corresponding weights. In our case, the SNR shows how many times the stationary component of the noisy layer is larger than the injected noise.</p>
</section>
<section class="level4 subsectionHead" id="results-2">
<h2 class="heading-2" id="sigil_toc_id_123"> <span id="x1-1410008.4.2"/>Results</h2>
<p>After the training, the<span id="dx1-141001"/> TensorBoard charts show much better training dynamics. The model was able to reach the mean score of 18 after 250 games, which is an improvement in comparison to 350 for the baseline DQN. But because of extra operations required for noisy networks, their training is a bit slower (194 FPS versus 240 FPS for the baseline), so, time-wise, the difference is less impressive. But still, the results look good:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_10.png" width="600"/> <span id="x1-141002r10"/></p>
<span class="id">Figure¬†8.10: Noisy networks compared to the baseline DQN </span>
</div>
<p>After <span id="dx1-141003"/>checking the SNR chart (<span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-141004r11"><span class="cmti-10x-x-109">8.11</span></a>), you may notice that both layers‚Äô noise levels have decreased very quickly.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_11.png" width="600"/> <span id="x1-141004r11"/></p>
<span class="id">Figure¬†8.11: SNR change in layer 1 (left) and layer 2 (right) </span>
</div>
<p>The first layer has gone from <img alt="1 2" class="frac" data-align="middle" height="20" src="../Images/eq31.png" width="15"/> to almost <img alt="-1- 2.6" class="frac" data-align="middle" height="20" src="../Images/eq32.png" width="20"/> ratio of noise. The second layer is even more interesting, as its noise level decreased from <img alt="1 4" class="frac" data-align="middle" height="20" src="../Images/eq33.png" width="15"/> in the beginning to <img alt="1- 16" class="frac" data-align="middle" height="20" src="../Images/eq34.png" width="20"/>, but after 450K frames (roughly the same time as when raw rewards climbed close to the 20 score), the level of noise in the last layer started to increase again, pushing the agent to explore the environment more. This makes a lot of sense, as after reaching high score levels, the agent basically knows how to play at a good level, but still needs to ‚Äúpolish‚Äù its actions to improve the results even more.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-3">
<h2 class="heading-2" id="sigil_toc_id_124"> <span id="x1-1420008.4.3"/>Hyperparameter tuning</h2>
<p>After the tuning, the <span id="dx1-142001"/>best set of parameters was able to solve the game after 273 rounds, which is an improvement over the baseline:</p>
<div class="tcolorbox" id="tcolobox-175">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-227"><code>    learning_rate=7.142520950425814e-05, 
    gamma=0.99,</code></pre>
</div>
</div>
<p>The following are charts comparing the reward dynamics and steps for tuned baseline DQN and tuned noisy networks:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_12.png" width="600"/> <span id="x1-142004r12"/></p>
<span class="id">Figure¬†8.12: Comparison of tuned baseline DQN and tuned noisy network </span>
</div>
<p>On both charts, we see improvements introduced by noisy networks: it takes fewer games to reach a score of 21 and during the training, games have a smaller amount of steps.</p>
</section>
</section>
<section class="level3 sectionHead" id="prioritized-replay-buffer">
<h1 class="heading-1" id="sigil_toc_id_125"> <span id="x1-1430008.5"/>Prioritized replay buffer</h1>
<p>The next very <span id="dx1-143001"/>useful idea on how to improve DQN training was proposed in 2015 in the paper <span class="cmti-10x-x-109">Prioritized experience replay </span>[<span id="x1-143002"/><a href="#">Sch+15</a>]. This <span id="dx1-143003"/>method tries to improve the efficiency of samples in the replay buffer by prioritizing those samples according to the training loss.</p>
<p>The basic DQN used the replay buffer to break the correlation between immediate transitions in our episodes. As we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>, the examples we experience during the episode will be highly correlated, as most of the time, the environment is ‚Äùsmooth‚Äù and doesn‚Äôt change much according to our actions. However, the <span class="cmbx-10x-x-109">stochastic</span> <span class="cmbx-10x-x-109">gradient descent </span>(<span class="cmbx-10x-x-109">SGD</span>) method assumes that the data we use for training has an iid property. To solve this problem, the classic DQN method uses a large buffer of transitions, randomly and uniformly sampled to get the next training batch.</p>
<p>The authors <span id="dx1-143004"/>of the paper questioned this uniform random sample policy and proved that by assigning priorities to buffer samples, according to training loss and sampling the buffer proportional to those priorities, we can significantly improve convergence and the policy quality of the DQN. This method‚Äôs basic idea could be explained as ‚Äútrain more on data that surprises you.‚Äù The tricky point here is to keep the balance of training on an ‚Äúunusual‚Äù sample and training on the rest of the buffer. If we focus only on a small subset of the buffer, we can lose our i.i.d. property and simply overfit on this subset.</p>
<p>From the mathematical point of view, the priority of every sample in the buffer is calculated as <img alt=" pŒ± ‚àëkipŒ±- k" class="frac" data-align="middle" height="50" src="../Images/eq35.png" width="80"/>, where <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">i</span></sub> is the priority of the <span class="cmmi-10x-x-109">i</span>-th sample in the buffer and <span class="cmmi-10x-x-109">Œ± </span>is the number that shows how much emphasis we give to the priority. If <span class="cmmi-10x-x-109">Œ± </span>= 0, our sampling will become uniform as in the classic DQN method. Larger values for <span class="cmmi-10x-x-109">Œ± </span>put more stress on samples with higher priority. So, it‚Äôs another hyperparameter to tune, and the starting value of <span class="cmmi-10x-x-109">Œ± </span>proposed by the paper is 0<span class="cmmi-10x-x-109">.</span>6.</p>
<p>There were several options <span id="dx1-143005"/>proposed in the paper for how to define the priority, and the most popular is to make it proportional to the loss for this particular example in the Bellman update. New samples added to the buffer need to be assigned a maximum value of priority to be sure that they will be sampled soon.</p>
<p>By adjusting the priorities for the samples, we are introducing bias into our data distribution (we sample some transitions much more frequently than others), which we need to compensate for if SGD is to work. To get this result, the authors of the study used sample weights, which needed to be multiplied by the individual sample loss. The value of the weight for each sample is defined as <span class="cmmi-10x-x-109">w</span><sub><span class="cmmi-8">i</span></sub> = (<span class="cmmi-10x-x-109">N </span><span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">P</span>(<span class="cmmi-10x-x-109">i</span>))<sup><span class="cmsy-8">‚àí</span><span class="cmmi-8">Œ≤</span></sup>, where <span class="cmmi-10x-x-109">Œ≤ </span>is another hyperparameter that should be between 0 and 1.</p>
<p>With <span class="cmmi-10x-x-109">Œ≤ </span>= 1, the bias introduced by the sampling is fully compensated for, but the authors showed that it‚Äôs good for convergence to start with <span class="cmmi-10x-x-109">Œ≤ </span>between 0 and 1 and slowly increase it to 1 during the training.</p>
<section class="level4 subsectionHead" id="implementation-4">
<h2 class="heading-2" id="sigil_toc_id_126"> <span id="x1-1440008.5.1"/>Implementation</h2>
<p>To <span id="dx1-144001"/>implement this method, we have to introduce certain changes in our code:</p>
<ul>
<li>
<p>First of all, we need a new replay buffer that will track priorities, sample a batch according to them, calculate weights, and let us update priorities after the loss has become known.</p>
</li>
<li>
<p>The second change will be the loss function itself. Now we not only need to incorporate weights for every sample, but we need to pass loss values back to the replay buffer to adjust the priorities of the sampled transitions.</p>
</li>
</ul>
<p>In the main module, <span class="cmtt-10x-x-109">Chapter08/05</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_prio</span><span class="cmtt-10x-x-109">_replay.py</span>, we have all those changes implemented. For the sake of simplicity, the new priority replay buffer class uses a very similar storage scheme to our previous replay buffer. Unfortunately, new requirements for prioritization make it impossible to implement sampling in <span class="cmsy-10x-x-109">ùí™</span>(1) time (in other words, sampling time will grow with an increase in buffer size). If we are using simple lists, every time that we sample a new batch, we need to process all the priorities, which makes our sampling have <span class="cmsy-10x-x-109">ùí™</span>(<span class="cmmi-10x-x-109">N</span>) time complexity in proportion to the buffer size. It‚Äôs not a big deal if our buffer is small, such as 100k samples, but may become an issue for real-life large buffers of millions of transitions. There are other storage schemes that support efficient sampling in <span class="cmsy-10x-x-109">ùí™</span>(log <span class="cmmi-10x-x-109">N</span>) time, for example, using the segment tree data structure. There are different versions of such optimized buffers available in various libraries ‚Äì for example, in TorchRL.</p>
<p>The PTAN library <span id="dx1-144002"/>also provides an efficient prioritized replay buffer in the class <span class="cmtt-10x-x-109">ptan.experience.PrioritizedReplayBuffer</span>. You can update the example to use the more efficient version and check the effect on training performance.</p>
<p>But, for now, let‚Äôs take a look at the na√Øve version, whose source code you will find in <span class="cmtt-10x-x-109">lib/dqn</span><span class="cmtt-10x-x-109">_extra.py</span>.</p>
<p>In the beginning, we define parameters for the <span class="cmmi-10x-x-109">Œ≤ </span>increase rate:</p>
<div class="tcolorbox" id="tcolobox-176">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-228"><code>BETA_START = 0.4 
BETA_FRAMES = 100_000</code></pre>
</div>
</div>
<p>Our beta will be changed from 0.4 to 1.0 during the first 100k frames.</p>
<p>Next comes the prioritized replay buffer class:</p>
<div class="tcolorbox" id="tcolobox-177">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-229"><code>class PrioReplayBuffer(ExperienceReplayBuffer): 
    def __init__(self, exp_source: ExperienceSource, buf_size: int, 
                 prob_alpha: float = 0.6): 
        super().__init__(exp_source, buf_size) 
        self.experience_source_iter = iter(exp_source) 
        self.capacity = buf_size 
        self.pos = 0 
        self.buffer = [] 
        self.prob_alpha = prob_alpha 
        self.priorities = np.zeros((buf_size, ), dtype=np.float32) 
        self.beta = BETA_START</code></pre>
</div>
</div>
<p>The class for the priority replay buffer inherits from the simple replay buffer in PTAN, which stores samples in a circular buffer (it allows us to keep a fixed amount of entries without reallocating the list). Our subclass uses a NumPy array to keep priorities.</p>
<p>The <span class="cmtt-10x-x-109">update</span><span class="cmtt-10x-x-109">_beta() </span>method needs to be called periodically to increase beta according to a schedule. The <span class="cmtt-10x-x-109">populate() </span>method needs to pull the given number of transitions from the <span class="cmtt-10x-x-109">ExperienceSource </span>object and store them in the buffer:</p>
<div class="tcolorbox" id="tcolobox-178">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-230"><code>    def update_beta(self, idx: int) -&gt; float: 
        v = BETA_START + idx * (1.0 - BETA_START) / BETA_FRAMES 
        self.beta = min(1.0, v) 
        return self.beta 
 
    def populate(self, count: int): 
        max_prio = self.priorities.max(initial=1.0) 
        for _ in range(count): 
            sample = next(self.experience_source_iter) 
            if len(self.buffer) &lt; self.capacity: 
                self.buffer.append(sample) 
            else: 
                self.buffer[self.pos] = sample 
            self.priorities[self.pos] = max_prio 
            self.pos = (self.pos + 1) % self.capacity</code></pre>
</div>
</div>
<p>As our storage for the<span id="dx1-144031"/> transitions is implemented as a circular buffer, we have two different situations with this buffer:</p>
<ul>
<li>
<p>When our buffer hasn‚Äôt reached the maximum capacity, we just need to append a new transition to the buffer.</p>
</li>
<li>
<p>If the buffer is already full, we need to overwrite the oldest transition, which is tracked by the <span class="cmtt-10x-x-109">pos </span>class field, and adjust this position modulo buffer‚Äôs size.</p>
</li>
</ul>
<p>In the sample method, we need to convert priorities to probabilities using our <span class="cmmi-10x-x-109">Œ±</span> hyperparameter:</p>
<div class="tcolorbox" id="tcolobox-179">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-231"><code>    def sample(self, batch_size: int) -&gt; tt.Tuple[ 
        tt.List[ExperienceFirstLast], np.ndarray, np.ndarray 
    ]: 
        if len(self.buffer) == self.capacity: 
            prios = self.priorities 
        else: 
            prios = self.priorities[:self.pos] 
        probs = prios ** self.prob_alpha 
        probs /= probs.sum()</code></pre>
</div>
</div>
<p>Then, using those probabilities, we sample our buffer to obtain a batch of samples:</p>
<div class="tcolorbox" id="tcolobox-180">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-232"><code>        indices = np.random.choice(len(self.buffer), batch_size, p=probs) 
        samples = [self.buffer[idx] for idx in indices]</code></pre>
</div>
</div>
<p>As the last step, we calculate weights for samples in the batch:</p>
<div class="tcolorbox" id="tcolobox-181">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-233"><code>        total = len(self.buffer) 
        weights = (total * probs[indices]) ** (-self.beta) 
        weights /= weights.max() 
        return samples, indices, np.array(weights, dtype=np.float32)</code></pre>
</div>
</div>
<p>This returns three objects: the batch, indices, and weights. Indices for batch samples are required to update priorities for sampled items.</p>
<p>The last function of the priority replay buffer allows us to update new priorities for the processed batch:</p>
<div class="tcolorbox" id="tcolobox-182">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-234"><code>    def update_priorities(self, batch_indices: np.ndarray, batch_priorities: np.ndarray): 
        for idx, prio in zip(batch_indices, batch_priorities): 
            self.priorities[idx] = prio</code></pre>
</div>
</div>
<p>It‚Äôs the responsibility of the caller to use this function with the calculated losses for the batch.</p>
<p>The next <span id="dx1-144050"/>custom function that we have in our example is the loss calculation. As the <span class="cmtt-10x-x-109">MSELoss </span>class in PyTorch doesn‚Äôt support weights (which is understandable, as MSE is loss used in regression problems, but weighting of the samples is commonly utilized in classification losses), we need to calculate the MSE and explicitly multiply the result on the weights:</p>
<div class="tcolorbox" id="tcolobox-183">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-235"><code>def calc_loss(batch: tt.List[ExperienceFirstLast], batch_weights: np.ndarray, 
              net: nn.Module, tgt_net: nn.Module, gamma: float, 
              device: torch.device) -&gt; tt.Tuple[torch.Tensor, np.ndarray]: 
    states, actions, rewards, dones, next_states = common.unpack_batch(batch) 
 
    states_v = torch.as_tensor(states).to(device) 
    actions_v = torch.tensor(actions).to(device) 
    rewards_v = torch.tensor(rewards).to(device) 
    done_mask = torch.BoolTensor(dones).to(device) 
    batch_weights_v = torch.tensor(batch_weights).to(device) 
 
    actions_v = actions_v.unsqueeze(-1) 
    state_action_vals = net(states_v).gather(1, actions_v) 
    state_action_vals = state_action_vals.squeeze(-1) 
    with torch.no_grad(): 
        next_states_v = torch.as_tensor(next_states).to(device) 
        next_s_vals = tgt_net(next_states_v).max(1)[0] 
        next_s_vals[done_mask] = 0.0 
        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v 
    l = (state_action_vals - exp_sa_vals) ** 2 
    losses_v = batch_weights_v * l 
    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()</code></pre>
</div>
</div>
<p>In the last part of <span id="dx1-144073"/>the loss calculation, we implement the same MSE loss but write our expression explicitly, rather than using the library. This allows us to take into account the weights of samples and keep individual loss values for every sample. Those values will be passed to the priority replay buffer to update priorities. A small value is added to every loss to handle the situation of zero loss value, which will lead to zero priority for an entry in the replay buffer.</p>
<p>In the main section of our program, we have only two updates: the creation of the replay buffer and our processing function. Buffer creation is straightforward, so we will only take a look at a new processing function:</p>
<div class="tcolorbox" id="tcolobox-184">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-236"><code>    def process_batch(engine, batch_data): 
        batch, batch_indices, batch_weights = batch_data 
        optimizer.zero_grad() 
        loss_v, sample_prios = calc_loss( 
            batch, batch_weights, net, tgt_net.target_model, 
            gamma=params.gamma, device=device) 
        loss_v.backward() 
        optimizer.step() 
        buffer.update_priorities(batch_indices, sample_prios) 
        epsilon_tracker.frame(engine.state.iteration) 
        if engine.state.iteration % params.target_net_sync == 0: 
            tgt_net.sync() 
        return { 
            "loss": loss_v.item(), 
            "epsilon": selector.epsilon, 
            "beta": buffer.update_beta(engine.state.iteration), 
        }</code></pre>
</div>
</div>
<p>There are several changes here:</p>
<ul>
<li>
<p>Our batch now contains three entities: the batch of data, indices of sampled items, and samples‚Äô weights.</p>
</li>
<li>
<p>We call our new loss function, which accepts weights and returns the additional items‚Äô priorities. They are passed to the <span class="cmtt-10x-x-109">buffer.update</span><span class="cmtt-10x-x-109">_priorities() </span>function to reprioritize items that we have sampled.</p>
</li>
<li>
<p>We call the <span class="cmtt-10x-x-109">update</span><span class="cmtt-10x-x-109">_beta() </span>method of the buffer to change the <span class="cmtt-10x-x-109">beta</span> parameter according to the schedule.</p>
</li>
</ul>
</section>
<section class="level4 subsectionHead" id="results-3">
<h2 class="heading-2" id="sigil_toc_id_127"> <span id="x1-1450008.5.2"/>Results</h2>
<p>This example <span id="dx1-145001"/>can be trained as usual. According to my experiments, the prioritized replay buffer took almost the same absolute time to solve the environment: almost an hour. But it took fewer training iterations and fewer episodes. So, wall clock time is the same mostly due to the less efficient replay buffer, which, of course, could be resolved by proper <span class="cmsy-10x-x-109">ùí™</span>(log <span class="cmmi-10x-x-109">N</span>) implementation of the buffer.</p>
<p>Here is the comparison of reward dynamics of the baseline and prioritized replay buffer (right). The x axis is the game episodes:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_13.png" width="500"/> <span id="x1-145002r13"/></p>
<span class="id">Figure¬†8.13: Reward dynamics for prioritized replay buffer in comparison to basic DQN </span>
</div>
<p>Another difference to note on the TensorBoard charts is a much lower loss for the prioritized replay buffer. The following chart shows the comparison:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_14.png" width="500"/> <span id="x1-145003r14"/></p>
<span class="id">Figure¬†8.14: The comparison of loss during the training </span>
</div>
<p>Lower loss value is also expected and is a good sign that our implementation works. The idea of prioritization is to train more on samples <span id="dx1-145004"/>with high loss value, so training becomes more efficient. But there is a danger here: loss value during the training is not the primary objective to optimize; we can have very low loss, but due to a lack of exploration, the final policy learned could be far from being optimal.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-4">
<h2 class="heading-2" id="sigil_toc_id_128"> <span id="x1-1460008.5.3"/>Hyperparameter tuning</h2>
<p>Hyperparameter tuning <span id="dx1-146001"/>for the prioritized replay buffer was done with an additional parameter for <span class="cmmi-10x-x-109">Œ±</span>, which was sampled from a fixed list of values ranging from 0.3 to 0.9 (with steps of 0.1). The best combination was able to solve Pong after 330 episodes and had <span class="cmmi-10x-x-109">Œ± </span>= 0<span class="cmmi-10x-x-109">.</span>6 (the same as in the paper):</p>
<div class="tcolorbox" id="tcolobox-185">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-237"><code>    learning_rate=8.839010139505506e-05, 
    gamma=0.99,</code></pre>
</div>
</div>
<p>The following are charts comparing the tuned baseline DQN with the tuned prioritized replay buffer:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_15.png" width="600"/> <span id="x1-146004r15"/></p>
<span class="id">Figure¬†8.15: Comparison of tuned baseline DQN and tuned prioritized replay buffer </span>
</div>
<p>Here, we see the prioritized replay buffer had faster gameplay improvement, but it took <span id="dx1-146005"/>almost the same amount of games to reach score 21. On the right chart (with the amount of game steps), the prioritized replay buffer was also a bit better.</p>
</section>
</section>
<section class="level3 sectionHead" id="dueling-dqn">
<h1 class="heading-1" id="sigil_toc_id_129"> <span id="x1-1470008.6"/>Dueling DQN</h1>
<p>This improvement <span id="dx1-147001"/>to DQN was proposed in 2015, in the paper called <span class="cmti-10x-x-109">Dueling network architectures for deep reinforcement learning </span>[<span id="x1-147002"/><a href="#">Wan+16</a>]. The <span id="dx1-147003"/>core observation of this paper is that the Q-values, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>), that our network is trying to approximate can be divided into quantities: the value of the state, <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>), and the advantage of actions in this state, <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>).</p>
<p>You have seen the quantity <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) before, as it was the core of the value iteration method from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch009.xhtml#x1-820005"><span class="cmti-10x-x-109">5</span></a>. It is just equal to the discounted expected reward achievable from this state. The advantage <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>) is supposed to bridge the gap from <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) to <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>), as, by definition, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) = <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) + <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>). In other words, the advantage <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>) is just the delta, saying how much extra reward some particular action from the state brings us. The advantage could be positive or negative and, in general, could have any magnitude. For example, at some tipping point, the choice of one action over another can cost us a lot of the total reward.</p>
<p>The <span class="cmti-10x-x-109">Dueling </span>paper‚Äôs contribution was an explicit separation of the value and the advantage in the network‚Äôs architecture, which brought better training stability, faster convergence, and better results on the Atari benchmark. The architecture difference from the classic DQN network is shown in the following illustration. The classic DQN network (top) takes features from the convolution layer and, using fully connected layers, transforms them into a vector of Q-values, one for each action. On the other hand, dueling DQN (bottom) takes convolution features and processes them using two independent paths: one path is responsible for <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) prediction, which is just a single number, and another path predicts individual advantage values, having the same dimension as Q-values in the classic case. After that, we add <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) to every value of <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>) to obtain <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>), which is used and trained as normal. <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-147004r16"><span class="cmti-10x-x-109">8.16</span></a> (from the paper) compares the basic DQN and dueling DQN:</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/file58.png" width="251"/> <span id="x1-147004r16"/></p>
<span class="id">Figure¬†8.16: A basic DQN (top) and dueling architecture (bottom) </span>
</div>
<p>These <span id="dx1-147005"/>changes in the <span id="dx1-147006"/>architecture are not enough to make sure that the network will learn <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) and <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>) as we want it to. Nothing prevents the network, for example, from predicting some state, <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) = 0, and <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s</span>) = [1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>3<span class="cmmi-10x-x-109">,</span>4], which is completely wrong, as the predicted <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) is not the expected value of the state. We have yet another constraint to set: we want the mean value of the advantage of any state to be zero. In that case, the correct prediction for the preceding example will be <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) = 2<span class="cmmi-10x-x-109">.</span>5 and <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s</span>) = [<span class="cmsy-10x-x-109">‚àí</span>1<span class="cmmi-10x-x-109">.</span>5<span class="cmmi-10x-x-109">,</span><span class="cmsy-10x-x-109">‚àí</span>0<span class="cmmi-10x-x-109">.</span>5<span class="cmmi-10x-x-109">,</span>0<span class="cmmi-10x-x-109">.</span>5<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">.</span>5].</p>
<p>This constraint could be enforced in various ways, for example, via the loss function; but in the <span class="cmti-10x-x-109">Dueling </span>paper, the authors proposed the very elegant solution of subtracting the mean value of the advantage from the <span class="cmmi-10x-x-109">Q </span>expression in the network, which effectively pulls the mean for the advantage to zero:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="59" src="../Images/eq36.png" width="409"/>
</div>
<p>This keeps the changes that need to be made in the classic DQN very simple: to convert it to the double DQN, you need to change only the network architecture, without affecting other pieces of the implementation.</p>
<section class="level4 subsectionHead" id="implementation-5">
<h2 class="heading-2" id="sigil_toc_id_130"> <span id="x1-1480008.6.1"/>Implementation</h2>
<p>The <span id="dx1-148001"/>complete example is available in <span class="cmtt-10x-x-109">Chapter08/06</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_dueling.py</span>. All the changes sit in the network architecture, so here, I‚Äôll only show the network class (which is in the <span class="cmtt-10x-x-109">lib/dqn</span><span class="cmtt-10x-x-109">_extra.py </span>module).</p>
<p>The convolution part is exactly the same as before:</p>
<div class="tcolorbox" id="tcolobox-186">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-238"><code>class DuelingDQN(nn.Module): 
    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): 
        super(DuelingDQN, self).__init__() 
 
        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), 
            nn.ReLU(), 
            nn.Conv2d(32, 64, kernel_size=4, stride=2), 
            nn.ReLU(), 
            nn.Conv2d(64, 64, kernel_size=3, stride=1), 
            nn.ReLU(), 
            nn.Flatten() 
        )</code></pre>
</div>
</div>
<p>Instead of defining a single path of fully connected layers, we create two different transformations: one for advantages and one for value prediction:</p>
<div class="tcolorbox" id="tcolobox-187">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-239"><code>        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.fc_adv = nn.Sequential( 
            nn.Linear(size, 256), 
            nn.ReLU(), 
            nn.Linear(256, n_actions) 
        ) 
        self.fc_val = nn.Sequential( 
            nn.Linear(size, 256), 
            nn.ReLU(), 
            nn.Linear(256, 1) 
        )</code></pre>
</div>
</div>
<p>Also, to keep the number of parameters in the model comparable to the original network, the inner dimension in both paths is decreased from 512 to 256. The changes in the <span class="cmtt-10x-x-109">forward() </span>function are also very simple, thanks to PyTorch‚Äôs expressiveness:</p>
<div class="tcolorbox" id="tcolobox-188">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-240"><code>    def forward(self, x: torch.ByteTensor): 
        adv, val = self.adv_val(x) 
        return val + (adv - adv.mean(dim=1, keepdim=True)) 
 
    def adv_val(self, x: torch.ByteTensor): 
        xx = x / 255.0 
        conv_out = self.conv(xx) 
        return self.fc_adv(conv_out), self.fc_val(conv_out)</code></pre>
</div>
</div>
<p>Here, we calculate the value and advantage for our batch of samples and add them together, subtracting the mean of the advantage to obtain the final Q-values. A subtle, but important, difference lies in calculating the mean along the second dimension of the tensor, which <span id="dx1-148034"/>produces a vector of the mean advantage for every sample in our batch.</p>
</section>
<section class="level4 subsectionHead" id="results-4">
<h2 class="heading-2" id="sigil_toc_id_131"> <span id="x1-1490008.6.2"/>Results</h2>
<p>After training a <span id="dx1-149001"/>dueling DQN, we can compare it to the classic DQN convergence on our Pong benchmark. Dueling architecture has faster convergence in comparison to the basic DQN version:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_17.png" width="500"/> <span id="x1-149002r17"/></p>
<span class="id">Figure¬†8.17: The reward dynamic of dueling DQN compared to the baseline version </span>
</div>
<p>Our example also outputs the advantage and value for a fixed set of states, shown in the following charts. They meet our expectations: the advantage is not very different from zero, but the value improves over time (and resembles the value from the <span class="cmti-10x-x-109">Double DQN </span>section):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_18.png" width="600"/> <span id="x1-149003r18"/></p>
<span class="id">Figure¬†8.18: Mean advantage (left) and value (right) on a fixed set of states </span>
</div>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-5">
<h2 class="heading-2" id="sigil_toc_id_132"> <span id="x1-1500008.6.3"/>Hyperparameter tuning</h2>
<p>The tuning of the <span id="dx1-150001"/>hyperparameters was not very fruitful. After 30 tuning iterations, there were no combinations of learning rate and gamma that were able to converge faster than the common set of parameters.</p>
</section>
</section>
<section class="level3 sectionHead" id="categorical-dqn">
<h1 class="heading-1" id="sigil_toc_id_133"> <span id="x1-1510008.7"/>Categorical DQN</h1>
<p>The <span id="dx1-151001"/>last, and <span id="dx1-151002"/>the most complicated, method in our <span class="cmti-10x-x-109">DQN improvements toolbox </span>is from the paper published by DeepMind in June 2017, called <span class="cmti-10x-x-109">A distributional</span> <span class="cmti-10x-x-109">perspective on reinforcement learning </span>[<span id="x1-151003"/><a href="#">BDM17</a>]. Although this paper is a few years old now, it remains highly relevant, and active research is still ongoing in this area. The book <span class="cmti-10x-x-109">Distributional reinforcement learning </span>was published in 2023, where the same authors describe the method in greater detail [<span id="x1-151004"/><a href="#">BDR23</a>].</p>
<p>In the paper, the authors questioned the fundamental pieces of Q-learning ‚Äî Q-values ‚Äî and tried to replace them with a more generic Q-value probability distribution. Let‚Äôs try to understand the idea. Both the Q-learning and value iteration methods work with the values of the actions or states represented as simple numbers and showing how much total reward we can achieve from a state, or an action and a state. However, is it practical to squeeze all future possible rewards into one number? In complicated environments, the future could be stochastic, giving us different values with different probabilities.</p>
<p>For example, imagine the commuter scenario when you regularly drive from home to work. Most of the time, the traffic isn‚Äôt that heavy, and it takes you around 30 minutes to reach your destination. It‚Äôs not exactly 30 minutes, but on average it‚Äôs 30. From time to time, something happens, like road repairs or an accident, and due to traffic jams, it takes you three times longer to get to work. The probability of your commute time can be represented as a distribution of the ‚Äúcommute time‚Äù random variable, and it is shown in the following chart:</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/B22150_08_19.png" width="251"/> <span id="x1-151005r19"/></p>
<span class="id">Figure¬†8.19: The probability distribution of commute time </span>
</div>
<p>Now, imagine <span id="dx1-151006"/>that you have an alternative way to get to work: the train. It <span id="dx1-151007"/>takes a bit longer, as you need to get from home to the train station and from the station to the office, but they are much more reliable than traveling by car (in some contries, like Germany, it might not be the case, but let‚Äôs consider Swiss trains for our example). Say, for instance, that the train commute time is 40 minutes on average, with a small chance of train disruption, which adds 20 minutes of extra time to the journey. The distribution of the train commute is shown in the following graph:</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/B22150_08_20.png" width="251"/> <span id="x1-151008r20"/></p>
<span class="id">Figure¬†8.20: The probability distribution of train commute time </span>
</div>
<p>Imagine that<span id="dx1-151009"/> now we want to <span id="dx1-151010"/>make the decision on how to commute. If we know only the mean time for both car and train, a car looks more attractive, as on average it takes 35.43 minutes to travel, which is better than 40.54 minutes for the train.</p>
<p>However, if we look at full distributions, we may decide to go by train, as even in the worst-case scenario, it will be one hour of commuting versus one hour and 30 minutes. Switching to statistical language, the car distribution has much higher <span class="cmbx-10x-x-109">variance</span>, so in situations when you really have to be at the office in 60 minutes max, the train is better.</p>
<p>The situation becomes even more complicated in the <span class="cmbx-10x-x-109">Markov decision process</span> (<span class="cmbx-10x-x-109">MDP</span>) scenario, when the sequence of decisions needs to be made and every decision might influence the future situation. In the commute example, it might be the time of an important meeting that you need to arrange given the way that you are going to commute. In that case, working with mean reward values might mean losing lots of information about the underlying environment dynamics.</p>
<p>Exactly the same idea was proposed by the authors of <span class="cmti-10x-x-109">Distributional Perspective</span> <span class="cmti-10x-x-109">on Reinforcement Learning </span>[9]. Why do we limit ourselves by trying to predict an average value for an action, when the underlying value may have a complicated underlying distribution? Maybe it will help us to work with distributions directly. The results presented in the paper show that, in fact, this idea could be helpful, but at the cost of introducing a more complicated method. I‚Äôm not going to put a strict mathematical definition here, but the overall idea is to predict the distribution of value for every action, similar to the distributions for our car/train example. As the next step, the authors showed that the Bellman equation can be generalized for a distribution case, and it will have the form <span class="cmmi-10x-x-109">Z</span>(<span class="cmmi-10x-x-109">x,a</span>)<img alt="D =" class="stackrel" height="20" src="../Images/eq37.png"/><span class="cmmi-10x-x-109">R</span>(<span class="cmmi-10x-x-109">x,a</span>) + <span class="cmmi-10x-x-109">Œ≥Z</span>(<span class="cmmi-10x-x-109">x</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>), which is very similar to the familiar Bellman equation, but now <span class="cmmi-10x-x-109">Z</span>(<span class="cmmi-10x-x-109">x,a</span>) and <span class="cmmi-10x-x-109">R</span>(<span class="cmmi-10x-x-109">x,a</span>) are the probability distributions and are not single numbers. The notation <span class="cmmi-10x-x-109">A</span><img alt=" D =" class="stackrel" height="20" src="../Images/eq37.png"/><span class="cmmi-10x-x-109">B </span>indicates eqality of distributions <span class="cmmi-10x-x-109">A </span>and <span class="cmmi-10x-x-109">B</span>.</p>
<p>The resulting <span id="dx1-151011"/>distribution can be used to train our network to give better predictions of <span id="dx1-151012"/>value distribution for every action of the given state, exactly in the same way as with Q-learning. The only difference will be in the loss function, which now has to be replaced with something suitable for distribution comparison. There are several alternatives available, for example, <span class="cmbx-10x-x-109">Kullback-Leibler </span>(<span class="cmbx-10x-x-109">KL</span>) divergence (or cross-entropy loss), which is used in classification problems, or the Wasserstein metric. In the paper, the authors gave theoretical justification for the Wasserstein metric, but when they tried to apply it in practice, they faced limitations. So, in the end, the paper used KL divergence.</p>
<section class="level4 subsectionHead" id="implementation-6">
<h2 class="heading-2" id="sigil_toc_id_134"> <span id="x1-1520008.7.1"/>Implementation</h2>
<p>As mentioned, the <span id="dx1-152001"/>method is quite complex, so it took me a while to implement it and make sure it was working. The complete code is in <span class="cmtt-10x-x-109">Chapter08/07</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_distrib.py</span>, which uses the <span class="cmtt-10x-x-109">distr</span><span class="cmtt-10x-x-109">_projection </span>function in <span class="cmtt-10x-x-109">lib/dqn</span><span class="cmtt-10x-x-109">_extra.py </span>to perform distribution projection. Before we check it, I need to say a few words about the implementation logic.</p>
<p>The central part of the method is the probability distribution, which we are approximating. There are lots of ways to represent the distribution, but the authors of the paper chose a quite generic parametric distribution, which is basically a fixed number of values placed regularly on a values range. The range of values should cover the range of possible accumulated discounted reward. In the paper, the authors did experiments with various numbers of atoms, but the best results were obtained with the range split on <span class="cmtt-10x-x-109">N</span><span class="cmtt-10x-x-109">_ATOMS=51 </span>intervals in the range of values from <span class="cmtt-10x-x-109">Vmin=-10 </span>to <span class="cmtt-10x-x-109">Vmax=10</span>.</p>
<p>For every atom (we have 51 of them), our network predicts the probability that the future discounted value will fall into this atom‚Äôs range. The central part of the method is the code, which performs the contraction of distribution of the next state‚Äôs best action using gamma, adds local reward to the distribution, and projects the results back into our original atoms. This logic is implemented in the <span class="cmtt-10x-x-109">dqn</span><span class="cmtt-10x-x-109">_extra.distr</span><span class="cmtt-10x-x-109">_projection </span>function. In the beginning, we allocate the array that will keep the result of the projection:</p>
<div class="tcolorbox" id="tcolobox-189">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-241"><code>def distr_projection(next_distr: np.ndarray, rewards: np.ndarray, 
                     dones: np.ndarray, gamma: float): 
    batch_size = len(rewards) 
    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32) 
    delta_z = (Vmax - Vmin) / (N_ATOMS - 1)</code></pre>
</div>
</div>
<p>This function expects the batch of distributions with a shape <span class="cmtt-10x-x-109">(batch</span><span class="cmtt-10x-x-109">_size,</span> <span class="cmtt-10x-x-109">N</span><span class="cmtt-10x-x-109">_ATOMS)</span>, the array of rewards, flags for completed episodes, and our hyperparameters: <span class="cmtt-10x-x-109">Vmin</span>, <span class="cmtt-10x-x-109">Vmax</span>, <span class="cmtt-10x-x-109">N</span><span class="cmtt-10x-x-109">_ATOMS</span>, and gamma. The <span class="cmtt-10x-x-109">delta</span><span class="cmtt-10x-x-109">_z </span>variable is the width of every atom in our value range.</p>
<p>In the following code, we iterate over every atom in the original distribution that we have and calculate the place that this atom will be projected to by the Bellman operator, taking into account our value bounds:</p>
<div class="tcolorbox" id="tcolobox-190">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-242"><code>    for atom in range(N_ATOMS): 
        v = rewards + (Vmin + atom * delta_z) * gamma 
        tz_j = np.minimum(Vmax, np.maximum(Vmin, v))</code></pre>
</div>
</div>
<p>For example, the <span id="dx1-152010"/>very first atom, with index 0, corresponds with the value <span class="cmtt-10x-x-109">Vmin=-10</span>, but for the sample with reward +1 will be projected into the value <span class="cmsy-10x-x-109">‚àí</span>10 <span class="cmsy-10x-x-109">‚ãÖ </span>0<span class="cmmi-10x-x-109">.</span>99 + 1 = <span class="cmsy-10x-x-109">‚àí</span>8<span class="cmmi-10x-x-109">.</span>9. In other words, it will be shifted to the right (assume <span class="cmtt-10x-x-109">gamma=0.99</span>). If the value falls beyond our value range given by <span class="cmtt-10x-x-109">Vmin </span>and <span class="cmtt-10x-x-109">Vmax</span>, we clip it to the bounds.</p>
<p>In the next line, we calculate the atom numbers that our samples have projected:</p>
<div class="tcolorbox" id="tcolobox-191">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-243"><code>        b_j = (tz_j - Vmin) / delta_z</code></pre>
</div>
</div>
<p>Of course, samples can be projected between atoms. In such situations, we spread the value in the original distribution at the source atom between the two atoms that it falls between. This spreading should be carefully handled, as our target atom can land exactly at some atom‚Äôs position. In that case, we just need to add the source distribution value to the target atom.</p>
<p>The following code handles the situation when the projected atom lands exactly on the target atom. Otherwise, <span class="cmtt-10x-x-109">b</span><span class="cmtt-10x-x-109">_j </span>won‚Äôt be the integer value and variables <span class="cmtt-10x-x-109">l</span> and <span class="cmtt-10x-x-109">u </span>(which correspond to the indices of atoms below and above the projected point):</p>
<div class="tcolorbox" id="tcolobox-192">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-244"><code>        l = np.floor(b_j).astype(np.int64) 
        u = np.ceil(b_j).astype(np.int64) 
        eq_mask = u == l 
        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]</code></pre>
</div>
</div>
<p>When the projected point lands between atoms, we need to spread the probability of the source atom between the atoms below and above. This is carried out by two lines in the following code:</p>
<div class="tcolorbox" id="tcolobox-193">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-245"><code>        ne_mask = u != l 
        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask] 
        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]</code></pre>
</div>
</div>
<p>Of course, we need to properly handle the final transitions of episodes. In that case, our projection shouldn‚Äôt take into account the next distribution and should just have a 1 probability corresponding to the reward obtained.</p>
<p>However, we again <span id="dx1-152019"/>need to take into account our atoms and properly distribute this probability if the reward value falls between atoms. This case is handled by the following code branch, which zeroes the resulting distribution for samples with the <span class="cmtt-10x-x-109">done </span>flag set and then calculates the resulting projection:</p>
<div class="tcolorbox" id="tcolobox-194">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-246"><code>    if dones.any(): 
        proj_distr[dones] = 0.0 
        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones])) 
        b_j = (tz_j - Vmin) / delta_z 
        l = np.floor(b_j).astype(np.int64) 
        u = np.ceil(b_j).astype(np.int64) 
        eq_mask = u == l 
        eq_dones = dones.copy() 
        eq_dones[dones] = eq_mask 
        if eq_dones.any(): 
            proj_distr[eq_dones, l[eq_mask]] = 1.0 
        ne_mask = u != l 
        ne_dones = dones.copy() 
        ne_dones[dones] = ne_mask 
        if ne_dones.any(): 
            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask] 
            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask] 
    return proj_distr</code></pre>
</div>
</div>
<p>To give you an illustration of what this function does, let‚Äôs look at artificially made distributions processed by this function (<span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-152038r21"><span class="cmti-10x-x-109">8.21</span></a>). I used them to debug the function and make sure that it worked as intended. The code for these checks is in <span class="cmtt-10x-x-109">Chapter08/adhoc/distr</span><span class="cmtt-10x-x-109">_test.py</span>.</p>
<div class="minipage">
<p><img alt="PIC" height="180" src="../Images/B22150_08_21.png" width="180"/> <span id="x1-152038r21"/></p>
<span class="id">Figure¬†8.21: The sample of the probability distribution transformation applied to a normal distribution </span>
</div>
<p>The top<span id="dx1-152039"/> chart of <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-152038r21"><span class="cmti-10x-x-109">8.21</span></a> (named <span class="cmti-10x-x-109">Source</span>) is a normal distribution with <span class="cmmi-10x-x-109">Œº </span>= 0 and <span class="cmmi-10x-x-109">œÉ </span>= 3. The second chart (named <span class="cmti-10x-x-109">Projected</span>) is obtained from distribution projection with <span class="cmmi-10x-x-109">Œ≥ </span>= 0<span class="cmmi-10x-x-109">.</span>9 and is shifted to the right with <span class="cmtt-10x-x-109">reward=2</span>.</p>
<p>In the situation where we pass <span class="cmtt-10x-x-109">done=True </span>with the same data, the result will be different and is shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-152040r22"><span class="cmti-10x-x-109">8.22</span></a>. In such cases, the source distribution will be ignored completely, and the result will have only the reward projected.</p>
<div class="minipage">
<p><img alt="PIC" height="180" src="../Images/B22150_08_22.png" width="180"/> <span id="x1-152040r22"/></p>
<span class="id">Figure¬†8.22: The projection of distribution for the final step in the episode</span>
</div>
<p>The <span id="dx1-152041"/>implementation of this method is in <span class="cmtt-10x-x-109">Chapter08/07</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_distrib.py</span>, which has an optional command-line parameter, <span class="cmtt-10x-x-109">--img-path</span>. If this option is given, it has to be a directory where plots with a probability distribution from a fixed set of states will be stored during the training. This is useful to monitor how the model converges from uniform probability in the beginning of the training to a more spiked weight of probability masses. Sample images from my experiments are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-153003r24"><span class="cmti-10x-x-109">8.24</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-153005r25"><span class="cmti-10x-x-109">8.25</span></a>.</p>
<p>I‚Äôm going to show only essential pieces of the implementation here. The core of the method, the <span class="cmtt-10x-x-109">distr</span><span class="cmtt-10x-x-109">_projection </span>function, was already covered, and it is the most complicated piece. What is still missing is the network architecture and modified loss function, which we will describe here.</p>
<p>Let‚Äôs start with the network, which is in <span class="cmtt-10x-x-109">lib/dqn</span><span class="cmtt-10x-x-109">_extra.py</span>, in the <span class="cmtt-10x-x-109">DistributionalDQN </span>class:</p>
<div class="tcolorbox" id="tcolobox-195">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-247"><code>Vmax = 10 
Vmin = -10 
N_ATOMS = 51 
DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1) 
 
class DistributionalDQN(nn.Module): 
    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): 
        super(DistributionalDQN, self).__init__() 
 
        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), 
            nn.ReLU(), 
            nn.Conv2d(32, 64, kernel_size=4, stride=2), 
            nn.ReLU(), 
            nn.Conv2d(64, 64, kernel_size=3, stride=1), 
            nn.ReLU(), 
            nn.Flatten() 
        ) 
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.fc = nn.Sequential( 
            nn.Linear(size, 512), 
            nn.ReLU(), 
            nn.Linear(512, n_actions * N_ATOMS) 
        ) 
 
        sups = torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z) 
        self.register_buffer("supports", sups) 
        self.softmax = nn.Softmax(dim=1)</code></pre>
</div>
</div>
<p>The main <span id="dx1-152070"/>difference is the output of the fully connected layer. Now it outputs the vector of <span class="cmtt-10x-x-109">n</span><span class="cmtt-10x-x-109">_actions * N</span><span class="cmtt-10x-x-109">_ATOMS </span>values, which is 6 <span class="cmsy-10x-x-109">√ó </span>51 = 306 for Pong. For every action, it needs to predict the probability distribution on 51 atoms. Every atom (called <span class="cmti-10x-x-109">support</span>) has a value, which corresponds to a particular reward. Those atoms‚Äô rewards are evenly distributed from -10 to 10, which gives a grid with step 0.4. Those supports are stored in the network‚Äôs buffer.</p>
<p>The <span class="cmtt-10x-x-109">forward() </span>method returns the predicted probability distribution as a 3D tensor (<span class="cmtt-10x-x-109">batch</span>, <span class="cmtt-10x-x-109">actions</span>, and <span class="cmtt-10x-x-109">supports</span>):</p>
<div class="tcolorbox" id="tcolobox-196">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-248"><code>    def forward(self, x: torch.ByteTensor) -&gt; torch.Tensor: 
        batch_size = x.size()[0] 
        xx = x / 255 
        fc_out = self.fc(self.conv(xx)) 
        return fc_out.view(batch_size, -1, N_ATOMS) 
 
    def both(self, x: torch.ByteTensor) -&gt; tt.Tuple[torch.Tensor, torch.Tensor]: 
        cat_out = self(x) 
        probs = self.apply_softmax(cat_out) 
        weights = probs * self.supports 
        res = weights.sum(dim=2) 
        return cat_out, res</code></pre>
</div>
</div>
<p>Besides <span class="cmtt-10x-x-109">forward()</span>, we define the <span class="cmtt-10x-x-109">both() </span>method, which calculates the probability distribution for atoms and Q-values in one call.</p>
<p>The network also defines several helper functions to simplify the calculation of Q-values and apply softmax on the probability distribution:</p>
<div class="tcolorbox" id="tcolobox-197">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-249"><code>    def qvals(self, x: torch.ByteTensor) -&gt; torch.Tensor: 
        return self.both(x)[1] 
 
    def apply_softmax(self, t: torch.Tensor) -&gt; torch.Tensor: 
        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())</code></pre>
</div>
</div>
<p>The final change is the new <span id="dx1-152088"/>loss function that has to apply distribution projection instead of the Bellman equation, and calculate KL divergence between predicted and projected distributions:</p>
<div class="tcolorbox" id="tcolobox-198">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-250"><code>def calc_loss(batch: tt.List[ExperienceFirstLast], net: dqn_extra.DistributionalDQN, 
              tgt_net: dqn_extra.DistributionalDQN, gamma: float, 
              device: torch.device) -&gt; torch.Tensor: 
    states, actions, rewards, dones, next_states = common.unpack_batch(batch) 
    batch_size = len(batch) 
 
    states_v = torch.as_tensor(states).to(device) 
    actions_v = torch.tensor(actions).to(device) 
    next_states_v = torch.as_tensor(next_states).to(device) 
 
    # next state distribution 
    next_distr_v, next_qvals_v = tgt_net.both(next_states_v) 
    next_acts = next_qvals_v.max(1)[1].data.cpu().numpy() 
    next_distr = tgt_net.apply_softmax(next_distr_v) 
    next_distr = next_distr.data.cpu().numpy() 
 
    next_best_distr = next_distr[range(batch_size), next_acts] 
    proj_distr = dqn_extra.distr_projection(next_best_distr, rewards, dones, gamma) 
 
    distr_v = net(states_v) 
    sa_vals = distr_v[range(batch_size), actions_v.data] 
    state_log_sm_v = F.log_softmax(sa_vals, dim=1) 
    proj_distr_v = torch.tensor(proj_distr).to(device) 
 
    loss_v = -state_log_sm_v * proj_distr_v 
    return loss_v.sum(dim=1).mean()</code></pre>
</div>
</div>
<p>The preceding code is not very complicated; it just prepares to call <span class="cmtt-10x-x-109">distr</span><span class="cmtt-10x-x-109">_projection </span>and KL divergence, which is defined as:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="49" src="../Images/eq38.png" width="274"/>
</div>
<p>To calculate the logarithm <span id="dx1-152115"/>of probability, we use the PyTorch <span class="cmtt-10x-x-109">log</span><span class="cmtt-10x-x-109">_softmax</span> function, which combines both <span class="cmtt-10x-x-109">log </span>and <span class="cmtt-10x-x-109">softmax </span>in a numerically stable way.</p>
</section>
<section class="level4 subsectionHead" id="results-5">
<h2 class="heading-2" id="sigil_toc_id_135"> <span id="x1-1530008.7.2"/>Results</h2>
<p>From my <span id="dx1-153001"/>experiments, the distributional version of DQN converged a bit slower and less stably than the original DQN, which is not surprising, as the network output is now 51 times larger and the loss function has changed. Without hyperparameter tuning (which will be described in the next subsection), the distributional version requires 20% more episodes to solve the game.</p>
<p>Another factor that might be important here is that Pong is just too simple a game to draw conclusions. In the <span class="cmti-10x-x-109">A Distributional Perspective </span>paper, the authors reported state-of-the-art scores (at the time of publishing in 2017) for more than half of the games from the Atari benchmark (Pong was not among them).</p>
<p>The following are charts comparing reward dynamics and loss for the distributional DQN. As you can see, the reward dynamics for the distributional method is worse than the baseline DQN:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_23.png" width="600"/> <span id="x1-153002r23"/></p>
<span class="id">Figure¬†8.23: Reward dynamics (left) and loss decrease (right) </span>
</div>
<p>It might be interesting to look into the dynamics of the probability distribution during the training. If you start the training with the <span class="cmtt-10x-x-109">--img-path </span>parameter (providing the directory name), the training process will save plots with the probability distribution for a fixed set of states. For example, the following figure shows the probability distribution for all six actions for one state at the beginning of the training (after 30k frames):</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/file68.png" width="251"/> <span id="x1-153003r24"/></p>
<span class="id">Figure¬†8.24: Probability distribution at the beginning of training </span>
</div>
<p>All the <span id="dx1-153004"/>distributions are very wide (as the network hasn‚Äôt converged yet), and the peak in the middle corresponds to the negative reward that the network expects to get from its actions. The same state after 500k frames of training is shown in the following figure:</p>
<div class="minipage">
<p><img alt="PIC" height="252" src="../Images/file69.png" width="251"/> <span id="x1-153005r25"/></p>
<span class="id">Figure¬†8.25: Probability distribution produced by the trained network </span>
</div>
<p>Now we can see that <span id="dx1-153006"/>different actions have different distributions. The first action (which corresponds to the NOOP, the <span class="cmti-10x-x-109">do nothing action</span>) has its distribution shifted to the left, so doing nothing in this state usually leads to losing. The fifth action, which is RIGHTFIRE, has the mean value shifted to the right, so this action leads to a better score.</p>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-6">
<h2 class="heading-2" id="sigil_toc_id_136"> <span id="x1-1540008.7.3"/>Hyperparameter tuning</h2>
<p>The tuning of <span id="dx1-154001"/>hyperparameters was not very fruitful. After 30 tuning iterations, there were no combinations of learning rate and gamma that were able to converge faster than the common set of parameters.</p>
</section>
</section>
<section class="level3 sectionHead" id="combining-everything">
<h1 class="heading-1" id="sigil_toc_id_137"> <span id="x1-1550008.8"/>Combining everything</h1>
<p>You have now seen all the DQN improvements mentioned in the paper <span class="cmti-10x-x-109">Rainbow: Combining Improvements in Deep Reinforcement Learning</span>, but it was done in an incremental way, which (I hope) was helpful to understand the idea and implementation of every improvement. The main point of the paper was to combine those improvements and check the results. In the final example, I‚Äôve decided to exclude categorical DQN and double DQN from the final system, as they haven‚Äôt shown too much improvement on our guinea pig environment. If you want, you can add them and try using a different game. The complete example is available in <span class="cmtt-10x-x-109">Chapter08/08</span><span class="cmtt-10x-x-109">_dqn</span><span class="cmtt-10x-x-109">_rainbow.py</span>.</p>
<p>First of all, we need to define our network architecture and the methods that have contributed to it:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Dueling DQN</span>: Our network will have two separate paths for the value of the state distribution and advantage distribution. On the output, both paths will be summed together, providing the final value probability distributions for actions. To force the advantage distribution to have a zero mean, we will subtract the distribution with the mean advantage in every atom.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Noisy networks</span>: Our linear layers in the value and advantage paths will be noisy variants of <span class="cmtt-10x-x-109">nn.Linear</span>.</p>
</li>
</ul>
<p>In addition to network architecture changes, we will use the prioritized replay buffer to keep environment transitions and sample them proportionally to the MSE loss.</p>
<p>Finally, we will unroll the Bellman equation to n-steps.</p>
<p>I‚Äôm not going to repeat all the code, as individual methods have already been given in the preceding sections, and it should be obvious what the final result of combining the methods will look like. If you have any trouble, you can find the code on GitHub.</p>
<section class="level4 subsectionHead" id="results-6">
<h2 class="heading-2" id="sigil_toc_id_138"> <span id="x1-1560008.8.1"/>Results</h2>
<p>The following are charts comparing the smoothed reward and count of steps with the baseline DQN. In both, we can see significant improvement in terms of the amount of games played:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_26.png" width="600"/> <span id="x1-156001r26"/></p>
<span class="id">Figure¬†8.26: Comparison of baseline DQN with combined system </span>
</div>
<p>In addition to the averaged reward, it is worth checking the raw reward chart, which is even more dramatic than the smoothed reward. It shows that our system was able to jump from the negative outcome to the positive very quickly ‚Äì after just 100 games, it won almost every game. So, it took us another 100 games to make the smoothed reward reach +18:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_27.png" width="500"/> <span id="x1-156002r27"/></p>
<span class="id">Figure¬†8.27: Raw reward for combined system </span>
</div>
<p>As a downside, the combined system is slower than the baseline, as we have a more complicated NN architecture and prioritized replay buffer. The FPS chart shows that the combined system starts at 170 FPS and degrades to 130 FPS due to the <span class="cmsy-10x-x-109">ùí™</span>(<span class="cmmi-10x-x-109">n</span>) buffer complexity:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_28.png" width="500"/> <span id="x1-156003r28"/></p>
<span class="id">Figure¬†8.28: Performance comparison (in frames per second) </span>
</div>
</section>
<section class="level4 subsectionHead" id="hyperparameter-tuning-7">
<h2 class="heading-2" id="sigil_toc_id_139"> <span id="x1-1570008.8.2"/>Hyperparameter tuning</h2>
<p>Tuning was done as before and was able to further improve the combined system training in terms of games played before solving the game. The following are charts comparing the tuned baseline DQN with the tuned combined system:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_29.png" width="600"/> <span id="x1-157001r29"/></p>
<span class="id">Figure¬†8.29: Comparison of tuned baseline DQN with tuned combined system </span>
</div>
<p>Another chart showing the effect of the tuning is the comparison of raw game rewards before and after the tuning. The tuned system starts to get the maximum score even earlier ‚Äî just after 40 games, which is quite impressive:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_08_30.png" width="500"/> <span id="x1-157002r30"/></p>
<span class="id">Figure¬†8.30: Raw reward for untuned and tuned combined DQN </span>
</div>
</section>
</section>
<section class="level3 sectionHead" id="summary-7">
<h1 class="heading-1" id="sigil_toc_id_140"> <span id="x1-1580008.9"/>Summary</h1>
<p>In this chapter, we have walked through and implemented a lot of DQN improvements that have been discovered by researchers since the first DQN paper was published in 2015. This list is far from complete. First of all, for the list of methods, I used the paper <span class="cmti-10x-x-109">Rainbow: Combining improvements in deep</span> <span class="cmti-10x-x-109">reinforcement learning </span>[<span id="x1-158001"/><a href="#">Hes+18</a>], which was published by DeepMind, so the list of methods is definitely biased to DeepMind papers. Secondly, RL is so active nowadays that new papers come out almost every day, which makes it very hard to keep up, even if we limit ourselves to one kind of RL model, such as a DQN. The goal of this chapter was to give you a practical view of different ideas that the field has developed.</p>
<p>In the next chapter, we will continue discussing practical DQN applications from an engineering perspective by talking about ways to improve DQN performance without touching the underlying method.</p>
</section>
<section class="level3 likesectionHead" id="join-our-community-on-discord-3">
<h1 class="heading-1" id="sigil_toc_id_141"><span id="x1-159000"/>Join our community on Discord</h1>
<p>Read this book alongside other users, Deep Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community. <a class="url" href="https://packt.link/rl"><span class="cmtt-10x-x-109">https://packt.link/rl</span></a></p>
<p><img alt="PIC" height="85" src="../Images/file1.png" width="85"/></p>
</section>
</section>
</div></body></html>