- en: Word Representation Using word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our *Python Deep Learning Projects* team is doing good work, and our (hypothetical)
    business use case has expanded! In the last project, we were asked to accurately
    classify handwritten digits to generate a phone number so that an *available table
    notification* text could be sent out to patrons of a restaurant chain. What we
    learned after the project was that the text that the restaurant sent out had a
    message that was friendly and well received. The restaurant was actually getting
    texts back!
  prefs: []
  type: TYPE_NORMAL
- en: 'The notification text was: *We''re excited that you''re here and your table
    is ready! See the greeter, and we''ll seat you now.*'
  prefs: []
  type: TYPE_NORMAL
- en: Response texts were varied and usually short, but the responses were noticed
    by the greeter and the restaurant management, who started thinking that maybe
    they could use this simple system to get feedback on the dining experience. This
    feedback would provide useful business intelligence on how the food tasted, how
    the service was delivered, and the overall quality of the experience.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define success**: The goal of this project is to build a computational linguistic
    model, using word2vec, that can take a text response (as identified in our hypothetical
    use case for this chapter) and output a sentiment classification that is meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we introduce the foundational knowledge of **deep learning**
    (**DL**) for computational linguistics.
  prefs: []
  type: TYPE_NORMAL
- en: We present the role of the dense vector representation of words in various computational
    linguistic tasks and how to construct them from an unlabeled monolingual corpus.
  prefs: []
  type: TYPE_NORMAL
- en: We'll then present the role of language models in various computational linguistic
    tasks, such as text classification, and how to construct them from an unlabeled
    monolingual corpus using **convolutional neural networks** (**CNNs**). We'll also
    explore CNN architecture for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: While working with machine learning/DL, the structure of data is very important. Unfortunately,
    raw data is often very unclean and unstructured, especially in the practice of
    **natural language processing** (**NLP**). When working with textualdata, we cannot
    feed strings as input in most DL algorithms; hence, **word embedding** methods
    come to the rescue. Word embedding is used to transform the textual data into
    dense vector (tensors) form, which we can feed to the learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways in which we can perform word embeddings, such as one-hot
    encoding, GloVe, word2vec, and many more, and each of them have their own pros
    and cons. Our current favorite is word2vec because it has been proven to be the
    most efficient approach when it comes to learning high quality features.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever worked on a use case where the input data is in text form,
    then you know that it's a really messy affair because you have to teach a computer
    about the irregularities of human language. Language has lots of ambiguities,
    and you have to teach sort of like hierarchical and the sparse nature of grammar.
    So these are the kinds of problems that word vectors solve by removing the ambiguities
    and making all different kinds of concepts similar.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to build word2vec models and analyze what
    characteristics we can learn about the provided corpus. Also, we will learn how
    to build a language model that utilizes a CNN with trained word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Learning word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement a fully functional word embedding model, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading all the dependencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing the text corpus
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plotting the word cluster using the **t-Distributed Stochastic Neighbor Embedding**
    (**t-SNE**) algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plotting the model on TensorBoard
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's make some world-class word embedding models!
  prefs: []
  type: TYPE_NORMAL
- en: The code for this section is available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Loading all the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using the `gensim` module ([https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim))
    to train our `word2vec` model. Gensim provides large-scale multi-core processing
    support to many popular algorithms, including **Latent Dirichlet Allocation**
    (**LDA**), **Hierarchical Dirichlet** **Process** (**HDP**), and word2vec. There
    are other approaches that we could take, such as the use of TensorFlow ([https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py](https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py))
    to define our own computation graph and build the model—this is something that
    we will look into later on.
  prefs: []
  type: TYPE_NORMAL
- en: Know the code! Python dependencies are quite manageable. You can learn more
    at [https://packaging.python.org/tutorials/managing-dependencies/](https://packaging.python.org/tutorials/managing-dependencies/).
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial walks you through the use of Pipenv to manage dependencies for
    an application. It will show you how to install and use the necessary tools and
    make strong recommendations on best practices. Keep in mind that Python is used
    for a great many different purposes, and precisely how you want to manage your
    dependencies may change based on how you decide to publish your software. The
    guidance presented here is most directly applicable to the development and deployment
    of network services (including web applications), but is also very well suited
    to managing development and testing environments for any kind of project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the `seaborn` package to plot the word clusters, `sklearn`
    to implement the t-SNE algorithm, and `tensorflow` for building TensorBoard plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the text corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the previously trained **Natural Language Toolkit** (**NLTK**)
    tokenizer ([http://www.nltk.org/index.html](http://www.nltk.org/index.html)) and
    stop words for the English language to clean our corpus and extract relevant unique
    words from the corpus. We will also create a small module to clean the provided
    collection, with a list of unprocessed sentences, to output the list of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we haven't yet captured the data from the text responses in our hypothetical
    business use case, let's collect a good quality dataset that's available on the
    web. Demonstrating our understanding and skills with this corpus will prepare
    us for the hypothetical business use case data. You can also use your own dataset,
    but it's important to have a huge amount of words so that the `word2vec` model
    can generalize well. So, we will load our data from the Project Gutenberg website,
    available at [Gutenberg.org](http://Gutenberg.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we tokenize the raw corpus into the list of unique clean words, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e713b0f9-7853-4d45-bffa-b1c79dae435e.png)'
  prefs: []
  type: TYPE_IMG
- en: This process depicts the data transformation, from raw data, to the list of
    words that will be fed into the word2vec model
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will download the text data from the URL and process them as shown
    in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Defining our word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's use `gensim` in our definition of the `word2vec` model. To begin,
    let's define a few hyperparameters for our model, such as the dimension, which
    means how many low-level features we want to learn. Each dimension will learn
    a unique concept of gender, objects, age, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational linguistics model tip #1**: Increasing the number of dimensions
    leads to better generalization... but it also adds more computational complexity.
    The right number is an empirical question for you to determine as an applied AI
    deep learning engineer!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational linguistics model tip #2**: Pay attention to `context_size`*. *This
    is important because it sets the upper limit for the distance between the current
    and target word prediction within a sentence. This helps the model in learning
    the deeper relationships between a word and the other nearby words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `gensim` instance, we will define our model, including all the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have configured the `gensim word2vec` object, we need to give the model
    some training. Be prepared, as this might take some time depending on the amount
    of data and the computation power you have. In this process, we have to define
    the number of epochswe need to run, which can vary depending on your data size.
    You can play around with these values and evaluate your `word2vec` model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we will save the trained model so that we can use it later on while building
    our language models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once the training process is complete, you can see a binary file stored in `/trained/sample.w2v`.
    You can share the `sample.w2v` file with others and they can use this word vectors
    in their NLP usecases and load it later into any other NLP task.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have trained our `word2vec` model, let''s explore what our model
    was able to learn. We will use `most_similar()` to explore the relations between
    various words. In the following example, you see that the model was able to learn
    that the word `earth` is related to `crust`, `globe`, and other words. It is interesting
    to see that we only provided the raw data and the model was able to learn all
    of these relations and concepts automatically! The following is the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try to find words related to `human` and see what the model has learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Critical thinking tip**: It''s interesting to observe that `art`, `race`,
    and `industry` are the most similar outputs. Remember that these similarities
    are based on the corpus of text that we used for training, and they should be
    thought of in that context. Generalization, and its unwanted sidekick, bias, can
    come into play when similarities from outdated or dissimilar training corpora
    are used to train a model that is applied to a new set of language data or cultural
    norms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even when we try to derive an analogy by using two positive vectors, `earth`
    and `moon`, and a negative vector, `orbit`, the model predicts the word `sun`,
    which makes sense because there is a semantic relation between the moon orbiting
    around the earth, and the earth orbiting around the sun:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, we learned that by using the `word2vec` model we can derive valuable information
    from raw unlabeled data. This process is crucial in terms of learning the grammar
    of a language and the semantic correlations between words.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we will learn how to use these `word2vec` features as an input for the
    classification model, which helps in boosting the model's accuracy and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the word cluster using the t-SNE algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, after our analysis, we know that our `word2vec` model has learned some concepts
    from the provided corpus, but how do we visualize it? Because we have created
    a 300-dimensional space to learn the features, it's practically impossible for
    us to visualize. To make it possible, we will use a dimension reduction algorithm,
    called t-SNE, which is very well known for reducing a high dimensional space into
    more humanly understandable two or three-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '"t-Distributed Stochastic Neighbor Embedding (t-SNE) ([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    is a (prize-winning) technique for dimensionality reduction that is particularly
    well suited for the visualization of high-dimensional datasets. The technique
    can be implemented via Barnes-Hut approximations, allowing it to be applied on
    large real-world datasets. We applied it on data sets with up to 30 million examples."'
  prefs: []
  type: TYPE_NORMAL
- en: – Laurens van der Maaten
  prefs: []
  type: TYPE_NORMAL
- en: To implement this, we will use the `sklearn` package, and define `n_components=2`,
    which means we want to have 2-D space as the output. Next, we will perform the
    transformation by feeding the word vectors into the t-SNE object.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this step, we now have a set of values for each word that we can use
    as `x` and `y` coordinates, respectively, to plot it on the 2D plane. Let''s prepare
    a `DataFrame` to store all the words and their `x` and `y` coordinates in the
    same variable, as shown in the following screenshot*,* and take data from there
    to create a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is our `DataFrame` containing words and coordinates for both `x` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8c999a1-a465-405a-8e08-a79a7a592cbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Our word list with the coordinate values obtained using t-SNE
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the entire cluster looks like after plotting 425,633 tokens on
    the 2D plane. Each point is positioned after learning the features and correlations
    between the nearby words, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bc2606d-b75f-4012-86b4-ef636a7c1485.png)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot of all the unique words on a 2D plane
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the embedding space by plotting the model on TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no benefit to visualization if you cannot make use of it, in terms
    of understanding how and what the model has learned. To gain a better intuition
    of what the model has learned, we will be using TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard is a powerful tool that can be used to build various kinds of plots
    to monitor your models while in the training process, as well as building DL architectures
    and word embeddings. Let's build a TensorBoard embedding projection and make use
    of it to do various kinds of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build an embedding plot in TensorBoard, we need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect the words and the respective tensors (300-D vectors) that we learned
    in previous steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable in the graph that will hold the tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the projector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Include an appropriately named embedding layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the words in a `.tsv` formatted metadata file. These file types are
    used by TensorBoard to load and display words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Link the `.tsv` metadata file to the projector object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a function that will store all of the summary checkpoints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the code to complete the preceding seven steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the TensorBoard preparation module is executed, the binaries, metadata,
    and checkpoints get stored in the disk, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/716fd34b-b219-42ad-aa15-6a0dfaf88a18.png)'
  prefs: []
  type: TYPE_IMG
- en: The outputs created by TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the TensorBoard, execute the following command in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the browser, open `http://localhost:6006/#projector`, you should see
    TensorBoard with all the data points projected in 3D space. You can zoom in, zoom
    out, look for specific words, as well as retrain the model using t-SNE, and visualize
    the cluster formation of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b31c192-5668-4780-b9d5-ecb78d1e7979.png)'
  prefs: []
  type: TYPE_IMG
- en: The TensorBoard embedding projection
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization helps you tell your story! TensorBoard is very cool! Your
    business use case stakeholders love impressive dynamic data visualizations. They
    help with your model intuition, and with generating new hypotheses to test.
  prefs: []
  type: TYPE_NORMAL
- en: Building language models using CNN and word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned the core concepts of computational linguistics, and
    trained relations from the provided dataset, we can use this learning to implement
    a language model that can perform a task.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will build a text classification model to perform sentiment
    analysis. For classification, we will be using a combination of CNN and a pre-trained
    `word2vec` model, which we learned about in the previous section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This task is the simulation of our hypothetical business use case of taking
    text responses from restaurant patrons and classifying what they text back into
    meaningful classes for the restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: We have been inspired by Denny Britz's ([https://twitter.com/dennybritz](https://twitter.com/dennybritz))
    work on *Implementing a CNN for Text Classification in TensorFlow* ([http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/))
    in our own CNN and text classification build. We invite you to review the blog
    he created to gain a more complete understanding of the internal mechanisms that
    make CNNs useful for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: As an overview, this architecture starts with an input embedding step, then
    a 2D convolution utilizing max pooling with multiple filters, and a softmax activation
    layer producing the output.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be asking yourself, how do you use CNNs to classify text when they
    are most commonly used in image processing?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many discussions in the literature, linked at the bottom of this
    tip, which have proven that CNNs are a generic feature extraction function that
    can compute **location invariance** and **compositionality**. The location invariance
    property helps the model to capture the context of words, irrespective of their
    occurrence in the corpus. Compositionality helps to derive higher-level representations
    using lower-level features:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks for Sentence Classification ([https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A CNN Based Scene Chinese Text Recognition Algorithm with Synthetic Data Engine
    ([https://arxiv.org/abs/1604.01891](https://arxiv.org/abs/1604.01891))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-Attentional Convolutional Neural Networks for Scene Text Detection ([https://arxiv.org/pdf/1510.03283.pdf](https://arxiv.org/pdf/1510.03283.pdf))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So instead of sending pixel values for an image into the model, we feed one-hot
    encoded word vectors or the `word2vec` matrix, which represent a word or a character
    (for character-based models). Denny Britz's implementation has two filters each
    in three region sizes of two, three, and four. The convolution operation is performed
    by these filters as it processes over the sentence matrix to generate feature
    maps. Downsampling is performed by a max pooling operation over each activation
    map. Finally, all the outputs are concatenated and passed into the softmax classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we are performing sentiment analysis, there will be both a positive
    and a negative output class target. The softmax classifier will output probabilities
    for each class, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7321c34b-1557-403b-9cca-d0aa076c6a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: This diagram is taken from Denny Britz's blog post describing the functioning
    of the CNN language model
  prefs: []
  type: TYPE_NORMAL
- en: Let's look into the implementation of the model. We have modified the existing
    implementation by adding the input of the previously trained `word2vec` model
    component.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'The model resides in `text_cnn.py`. We created a class, named `TextCNN`, which
    takes a few parameters as an input for the model''s configuration, also known
    as hyperparameters. The following is a list of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence_length`: The fixed sentence length'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_classes`: The number of output classes that will be produced by the softmax
    activation (positive and negative)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size`: The count of unique words in our embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_size`: Embedding dimensionality that we created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_sizes`: The convolutional filter will cover this many words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_filters`: Each filter size will have this many filters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre_trained`: Integrates the `word2vec` representation that has been previously
    trained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following is the declaration of the `TextCNN()` class with the `init()` function
    initializing all the hyperparameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is divided into six main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Placeholders for inputs**: All the placeholders that we need to contain the
    input values for our model are defined first. In this case, inputs are the sentence
    vector and associated labels (either positive or negative). `input_x` holds the
    sentence, `input_y` holds the value of label, and we use `dropout_keep_prob` for the
    probability that we keep a neuron in the dropout layer. The following code shows
    an example of this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Embedding**: Our model''s first layer, in which we feed the word representations
    learned in the process of training the `word2vec` model, is the embedding layer.
    We will modify the baseline code that''s in the repository to use our pre-trained
    embedding model, instead of learning the embedding from scratch. This will enhance
    the model accuracy. It is also a kind of a `transfer learning`, where we transfer
    the general knowledge learned from a generic Wikipedia or social media corpus.
    The embedding matrix that is initialized with the `word2vec` model is named `W`,
    as seen as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Convolution with maxpooling:** Defining the convolution layer is done with `tf.nn.conv2d()`.
    This takes, as inputs, the previous embedding layer''s weight (`W`—filter matrix)
    and applies a nonlinear ReLU activation function. Further max polling is performed
    over each filter size using `tf.nn.max_pool()`*.* Results are concatenated, creating
    a single vector that will become the inputs for the following layer of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Dropout layer**: To regularize CNN and prevent the model from overfitting,
    a minor percentage of signals from neurons are blocked. This forces the model
    to learn more unique or individual features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Prediction**: A TensorFlow wrapper performs the *W * x+b* metric multiplications,
    where `x` is the output of the previous layer. This computation will compute the
    values for the scores and the predictions will be produced by `tf.argmax()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '****Accuracy****: We can define the `loss` function with our scores. Remember
    that the measurement of the error our network makes is called **loss**. As good
    DL engineers, we want to minimize this and make our model more accurate. For the
    problem of categorization, the cross-entropy loss ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    is the standard `loss` function used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it, we''re done with our model. Let''s use TensorBoard to visualize
    the network and improve our intuition, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e942cccf-bcff-4240-9126-3807f881ce44.png)'
  prefs: []
  type: TYPE_IMG
- en: The CNN model architecture definition
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting dataset, *Movie Review Data* from Rotten Tomatoes ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)),
    was used in this case. Half of the reviews are positive, the other half negative,
    and there are about 10,000 sentences in total. There are around 20,000 different
    words in the vocabulary. The dataset is stored in the `data` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'It contains two files: one, `rt-polarity.neg`, contains all the negative sentences, and
    another, `rt-polarity.pos`, contains only positive sentences. To perform classification,
    we need to associate them with the labels. Each positive sentence is associated
    with a one-hot encoded label, `[0, 1]`, and each negative sentence is associated
    with `[1, 0]`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ad74017-2ffb-4d21-acc7-a0f985edfc75.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample of few positive sentences and the label associated with the sentence
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing the text data is done with these four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load**: Make sure to load both the positive and negative sentence data files'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clean**: Use regex to remove punctuation and other special characters'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pad**: Make each sentence the same size by appending `<PAD>` tokens'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Index**: Map each word to an integer in an index so that each sentence can
    become a vector of integers'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have our data formatted as vectors, we can feed them into our model.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating word2vec with CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, the last time we created our `word2vec` model, we dumped that model into
    a binary file. Now it's time to use that model as part of our CNN model. We perform
    this by initializing the `W` weights in the embeddings to these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we trained on a very small corpus in our previous `word2vec` model, let''s
    choose the `word2vec` model that was pre-trained on the huge corpus. A good strategy
    is to use fastText embedding, which is trained on documents available online and
    for 294 languages ([https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)).
    We do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will download the English Embedding fastText dataset ([https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, extract the vocab and embedding vectors into a separate file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load them into the `train.py` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it—by introducing this step, we can now feed the embedding layer with
    the pre-training `word2vec` model. This incorporation of information has a sufficient
    amount of features to improve the learning process of the CNN model.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to train our model with the provided dataset and the pre-trained
    embedding model. A few hyperparameters will need fine-tuning to achieve good results.
    But once we have executed the `train.py` file with reasonably good configurations,
    we can demonstrate that the model is able to distinguish well between the positive
    and negative sentences when classifying.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following graph, the performance metric of accuracy is
    tending towards 1 and the loss factor is reducing towards 0 over each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02a5950d-45b2-4310-b7fb-0bce7b42a4ff.png)'
  prefs: []
  type: TYPE_IMG
- en: A plot of the performance metrics accuracy and loss of the CNN model during
    the training process
  prefs: []
  type: TYPE_NORMAL
- en: Voila! We just used the pre-trained embedding model to train our CNN classifier
    with an average loss of 6.9 and accuracy of 72.6%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training is completed successfully, the output of the model
    will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The checkpoints stored in `/runs/folder`. We will use these checkpoints to make
    predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary with all the loss, accuracy, histogram, and gradient value distribution
    captured during the training process. We can visualize it using the TensorBoard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the model into production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our model binaries stored in the `/runs/` folder, we just need
    to write a restful API, for which you can use Flask, and then call the `sentiment_engine()`
    defined in the `model_inference.py` code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Always make sure that you use the checkpoints of the best model and the correct
    embedding file, which is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today's project was to build a DL computational linguistics model using word2vec to
    accurately classify text in a sentiment analysis paradigm. Our hypothetical use
    case was to apply DL to enable the management of a restaurant chain to understand
    the general sentiment of text responses their customers made, in response to a
    phone text question asking about their experience after dining. Our specific task
    was to build the natural language processing model that would create business
    intelligence from the data obtained in this simple (hypothetical) application.
  prefs: []
  type: TYPE_NORMAL
- en: '**Revisit our success criteria**: How did we do? Did we succeed? What is the
    impact of success? Just as we defined success at the beginning of the project,
    these are the key questions we ask as DL data scientists as we look to wrap up
    a project.'
  prefs: []
  type: TYPE_NORMAL
- en: Our CNN model, which was built on the trained `word2vec` model created earlier
    in the chapter, reached an accuracy of 72.6%! This means that we were able to
    reasonably accurately classify the unstructured text sentences as positive or
    negative.
  prefs: []
  type: TYPE_NORMAL
- en: What are the implications of this accuracy? In our hypothetical example, this
    means that we can take a body of data that is difficult to summarize outside of
    this DL NLP model and summarize it to produce actionable insights for the restaurant
    management. With summary data points of positive or negative sentiment to the
    questions asked in a phone text, the restaurant chain can track performance over
    time, make adjustments, and possibly even reward staff for improvements.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter's project, we learned how to build `word2vec` models and analyze
    what characteristics we can learn about the provided corpus. We also learned how
    to build a language model with CNN, using the trained word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at the model performance in testing and determined whether
    we succeeded in achieving our goals. In the next chapter's project, we're going
    to leverage even more power from our computational linguistic skills to create
    a natural language pipeline that will power a chatbot for open domain question
    answering. This is exciting work—let's see what next!
  prefs: []
  type: TYPE_NORMAL
