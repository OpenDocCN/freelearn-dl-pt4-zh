<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Getting Started with PyTorch 1.3</h1>
                </header>
            
            <article>
                
<p>PyTorch 1.3 has finally arrived! Are you ready to exploit its new features and functionalities to make your research and production easier?</p>
<p class="mce-root">In this chapter, we will walk you through the breaking changes introduced in PyTorch, including switching from eager mode to graph mode. We will look at how to migrate older code to 1.x and walk you through the PyTorch ecosystem along with Cloud support.</p>
<p>Also, we will introduce how to install CUDA so that you can take advantage of GPU acceleration for faster training and evaluation with your PyTorch code. We will show you the step-by-step installation process of PyTorch on Windows 10 and Ubuntu 18.04 (with pure Python or an Anaconda environment) and how to build PyTorch from source.</p>
<p>Finally, as bonus content, we will present how to configure Microsoft VS Code for PyTorch development and some of the best extensions to make your work more enjoyable.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>What's new in PyTorch 1.3?</li>
<li>CUDA - GPU acceleration for fast training and evaluation </li>
<li>Installing Pytorch on Windows and Linux</li>
<li>References and useful reading list</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What's new in PyTorch 1.3?</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>PyTorch</strong> <span>(</span><span><a href="https://pytorch.org">https://pytorch.org</a>)</span> is an open source <span>machine learning platform for Python. It is specifically designed for deep learning applications, such as <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>), <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>), and <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>), and it includes extensive layer definitions for these applications. It has built-in tensor operations that are designed to be used in the same way as NumPy arrays, and they are also optimized to run on GPUs for fast computation. It provides an automatic computational graph scheme so that you won't need to calculate derivatives by hand.</span></p>
<p><span>After around 3 years of development and improvements, PyTorch has finally reached its newest milestone, version 1.3! What comes with it is a big package of new features and new functionalities. Don't worry about whether you'll have to re-learn the tool; even when it's a totally new version, PyTorch has always been good at keeping its core functionality consistent. In fact, its core modules haven't changed much since its alpha release (version 0.1.1): <kbd>torch.nn</kbd>, <kbd>torch.autograd</kbd>, and <kbd>torch.optim</kbd>, unlike some other platforms. (Yes! We're talking about you, TensorFlow!) Now let's take a look at some of the new features in PyTorch.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Easy switching from eager mode to graph mode</h1>
                </header>
            
            <article>
                
<p>When PyTorch first caught people's attention around 2 years ago, one of its biggest advantages over other deep learning tools was its dynamic graph support. It might be the main reason people ditch their old tools and embrace PyTorch. As you might have noticed, recently, more authors of the latest deep learning papers are using PyTorch to implement their experiments.</p>
<p><span>However, it doesn't mean that PyTorch is not fit for production environments. In version 1.0, PyTorch provides a <strong>hybrid frontend</strong> that easily transfers your code from eager mode (dynamic graph) to graph mode (static graph). You can write your code in as flexible a way as before. When you are satisfied with your code, just by changing a few lines of code in your model, it will be ready to be optimized for efficiency in graph mode. This process is accomplished by</span> the torch.jit <span>compiler. <strong>JIT</strong> (<strong>Just</strong></span>-<span><strong>In</strong></span>-<span><strong>Time</strong>) compiler is designed to serialize and optimize PyTorch code into <strong>TorchScript</strong>, which can run without a Python interpreter.</span></p>
<p><span>This means that, now, you can easily export your model to an environment where Python is not available or efficiency is extremely important, and call your model with C++ code. Two modalities are provided to convert traditional PyTorch code to TorchScript: tracing and scripting. <strong>Tracing</strong> is perfect for directly transforming your fixed model scheme with fixed inputs to graph mode.</span></p>
<p class="mce-root"/>
<p><span>However, if there is any data-dependent control flow in your model (for example, RNN), <strong>scripting</strong> is designed for this type of scenario, where all possible control flow routes are converted into TorchScript. Bear in mind that, for now (at the time of writing this book), scripting still has its limitations.</span></p>
<div class="packt_infobox"><span><strong>Dynamic graph</strong> means that the computational graph is established each time you run your model and can be changed between different runs. It's like everyone driving their own cars around the streets, when anyone can go anywhere each time they leave their home. It's flexible for research purposes. However, the additional resource overheads that building the graphs before each run requires cannot be overlooked. Therefore, it might be a little inefficient for production purposes. <strong>Static graph</strong> means that the computational graph has to be established before the first run and it cannot be changed once established. It's like everyone going to work on the bus. It's efficient, but if the passengers want to travel to different destinations, they have to talk to the bus driver, who will then talk to the public transportation authorities. Then, the bus route can be changed the next day.</span></div>
<p>Here's an example of how to change your models to graph mode.</p>
<p>Assume that we already have <kbd>model</kbd> on a given <kbd>device</kbd>:</p>
<pre>model = Net().to(device)</pre>
<p>We only need to add these lines to <kbd>trace</kbd> the <kbd>model</kbd>:</p>
<pre>trace_input = torch.rand(BATCH_SIZE, IMG_CHANNEL, IMG_HEIGHT, IMG_WIDTH).to(device)<br/>traced_model = torch.jit.trace(model, trace_input)</pre>
<p>Then, we can <kbd>save</kbd> the traced model to file:</p>
<pre>traced_model.save("model_jit.pth")</pre>
<div class="mce-root packt_tip">Note that you should avoid using <kbd>torch.save(traced_model.state_dict(), "model_jit.pth")</kbd> to save the traced model, because, at the time of writing this book, the checkpoint file created in this way cannot be properly processed by the C++ API.</div>
<p>Now, the traced model can be used in the same way as a normal <kbd>torch.nn.Module</kbd> in Python, and can also be used by other C++ code, which we will cover later. The full code for this example, where we train and export a CNN for classification on MNIST can be found in the <kbd>jit/mnist_jit.py</kbd> file located in the code repository for this chapter. <span>You can refer to the official tutorial for more information about the hybrid frontend: </span><a href="https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html">https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html</a><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The C++ frontend</h1>
                </header>
            
            <article>
                
<p>Even though the backend of PyTorch is mostly implemented by C++, its frontend API has always been focused on Python. It's partly because Python has already been very popular among data scientists and it has tons of open source packages that help you focus on solving the problems, rather than re-creating the wheel. Also, it's extremely easy to read and write. However, Python is not known for computation and memory resource efficiency. Big companies often develop their own tools in C++ for better performance. But smaller companies or individual developers find it difficult to divert their main focus to developing their own C++ tools. Luckily, PyTorch has now shipped the C++ API with version 1.0. Now, anyone can build efficient projects with it.</p>
<div class="packt_infobox">Note that, right now, the C++ API of PyTorch is still under development and may undergo some changes in the future. In fact, the changes between v1.0.1 and v1.0.0 are so huge that the official documents and tutorials for v1.0.0 would not fit v1.0.1.</div>
<p>Here's an example of how to use the C++ API provided by PyTorch.</p>
<p>Let's load the traced model we exported previously:</p>
<pre>torch::Device device = torch::kCUDA;<br/>std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load("model_jit.pth");<br/>module-&gt;to(device);</pre>
<p>Then, let's feed a dummy input image to the model:</p>
<pre>std::vector&lt;torch::jit::IValue&gt; inputs;<br/>inputs.push_back(torch::ones({BATCH_SIZE, IMG_CHANNEL, IMG_HEIGHT, IMG_WIDTH}).to(device));<br/>at::Tensor output = module-&gt;forward(inputs).toTensor();</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>The full code for the C++ example can be found under the</span> <kbd>jit</kbd> <span>directory located in the code repository for this chapter, including a</span> <kbd>CMakeLists.txt</kbd> <span>file for compiling the</span> <kbd>.cpp</kbd> <span>file. You can refer to the official documentation for more information about C++ APIs: <a href="https://pytorch.org/cppdocs">https://pytorch.org/cppdocs</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The redesigned distributed library</h1>
                </header>
            
            <article>
                
<p>Debugging multithreading programs on a CPU is painful. Designing efficient GPU programs on distributed systems can be even more so. Fortunately, PyTorch keeps delivering ease-of-use distributed solutions for this very purpose. In version 1.0, the <kbd>torch.distributed</kbd><span> </span>module is performance-driven and runs asynchronously for all backends, including Gloo, NCCL, and MPI. The new distributed library is designed to deliver near-optimal performance on both single-node and multi-node systems. It's also specially optimized for less advanced network communication scenarios by reducing bandwidth exchanges and thus improves the performance of these systems.</p>
<p>The NCCL backend is used for distributed GPU training, and the Gloo backend is used for distributed CPU training. The new distributed package also provides a helper utility, <kbd>torch.distributed.launch</kbd>, which is designed to launch multiple processes on both single-node and multi-node systems. An example of how to use it for distributed training is as follows:</p>
<ul>
<li>Single-node <kbd>distributed</kbd> training:</li>
</ul>
<pre style="padding-left: 60px"><strong>$ python -m torch.distributed.launch --nproc_per_node=NUM_GPUS YOUR_SCRIPT.py --YOUR_ARGUMENTS</strong></pre>
<ul>
<li>Multi-node <kbd>distributed</kbd> training:</li>
</ul>
<pre style="padding-left: 60px"><strong># Node 1</strong><br/><strong>$ python -m torch.distributed.launch --nproc_per_node=NUM_GPUS --nnodes=2 --node_rank=0 --master_addr=MASTER_IP --master_port=MASTER_PORT YOUR_SCRIPT.py --YOUR_ARGUMENTS</strong><br/><strong># Node 2</strong><br/><strong>$ python -m torch.distributed.launch --nproc_per_node=NUM_GPUS --nnodes=2 --node_rank=1 --master_addr=MASTER_IP --master_port=MASTER_PORT YOUR_SCRIPT.py --YOUR_ARGUMENTS</strong></pre>
<p>In the preceding, <kbd>MASTER_IP</kbd> is a string containing the IP address of the master node, for example, <kbd>192.168.1.1</kbd>.</p>
<p class="mce-root"/>
<p>Feel free to check out the official tutorial on distributed training with PyTorch 1.3:<span> <a href="https://pytorch.org/docs/master/distributed.html">https://pytorch.org/docs/master/distributed.html</a>, </span><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>, <a href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</a> and <a href="https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html">https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Better research reproducibility</h1>
                </header>
            
            <article>
                
<p>You may have heard complaints about how hard it is to reproduce the experimental results in deep learning papers. Apparently, we need to trust the reviewers, even though they have to review thousands of papers for each top conference every year. However, does it mean we cannot trust our abilities to follow the exact steps written on paper? Now PyTorch has announced torch.hub to <span>help with the research reproducible problem. Authors can now publish their trained models with Torch Hub and users can directly download and use them in their code.</span></p>
<p>Here's an example of how to publish and use pre-trained models with Torch Hub.</p>
<p>To publish your model, you need to create a <kbd>hubconf.py</kbd> file in a GitHub repository and define the entrypoint (for example, named <kbd>cnn</kbd>) like this:</p>
<pre>dependencies = ['torch']<br/><br/>def cnn(pretrained=True, *args, **kwargs):<br/>    model = Net()<br/>    checkpoint = 'models/cnn.pth'<br/>    if pretrained:<br/>        model.load_state_dict(torch.load(checkpoint))<br/>    return model</pre>
<p>In the preceding code, <kbd>dependencies</kbd> is a list of the <span>dependencies required to run your model, and <kbd>Net()</kbd> is the class that defines your model. Note that the published models have to live under certain a branch/tag, for example, the </span><span class="packt_screen">master</span> <span>branch. You can also upload your <kbd>pretrained</kbd> model files to other sites and download them in this way:</span></p>
<pre>    if pretrained:<br/>        import torch.utils.model_zoo as model_zoo<br/>        model.load_state_dict(model_zoo.load_url(checkpoint))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Say we have published our model to <a href="https://github.com/johnhany/torchhub">https://github.com/johnhany/torchhub</a>. To use a published model, you only need to call <kbd>torch.hub</kbd>:</p>
<pre>import torch.hub as hub<br/>model = hub.load("johnhany/torchhub:master", "cnn", force_reload=True, pretrained=True).to(device)</pre>
<p>The full code for the Torch Hub example can be found under the <kbd>torchhub</kbd> directory located in the code repository for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Miscellaneous</h1>
                </header>
            
            <article>
                
<p>Other than what we've mentioned previously, there are other things we can benefit from in the new version of PyTorch. By the end of this section, we will also talk about how to migrate your old PyTorch code to version 1.x.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The PyTorch ecosystem</h1>
                </header>
            
            <article>
                
<p>There are many wonderful tools and projects built on the PyTorch platform. They explore the full potential of PyTorch in many domains. For example, <strong>AllenNLP</strong> (<a href="https://allennlp.org">https://allennlp.org</a>) is an open source natural language processing library. Check out their demo site and see what state-of-the-art NLP algorithms are capable of: <a href="https://demo.allennlp.org">https://demo.allennlp.org</a>. <strong>Fastai</strong> (<a href="https://docs.fast.ai">https://docs.fast.ai</a>) provides a simplified procedure of model training with PyTorch, and also offers practical deep learning courses at <a href="https://course.fast.ai">https://course.fast.ai</a>. <strong>Translate</strong> (<a href="https://github.com/pytorch/translate">https://github.com/pytorch/translate</a>) is a PyTorch library that's dedicated to natural language translation.</p>
<p>Check out this site to find out more about the PyTorch ecosystem: <a href="https://pytorch.org/ecosystem">https://pytorch.org/ecosystem</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud support</h1>
                </header>
            
            <article>
                
<p>PyTorch is fully supported by popular cloud platforms such as Amazon AWS, Google Cloud Platform, and Microsoft Azure. If you don't currently own a CUDA-enabled GPU (which we will discuss in the next section), feel free to rent a GPU server provided by the platforms previously mentioned. Here's an official tutorial on distributed training with PyTorch on Amazon AWS: <a href="https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html">https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html</a>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Migrating your previous code to 1.x</h1>
                </header>
            
            <article>
                
<p>Despite all the breaking changes in PyTorch 1.x, most of the APIs or coding conventions have not changed too much. Therefore, if you are already comfortable with <span>PyTorch </span>0.4, your code <em>should</em> mostly work as is. API changes from v0.4 to v1.3 are listed in Breaking Changes at <a href="https://github.com/pytorch/pytorch/releases">https://github.com/pytorch/pytorch/releases</a>.</p>
<p>The most common issue you might run into when migrating older code to PyTorch 1.x would stem from indexing a 0-dimension tensor. You need to use <kbd>loss.item()</kbd> when, for example, printing the loss value, instead of <kbd>loss[0]</kbd>. The full code for this example is contained in the <kbd>ind-0-dim.py</kbd> file <span>under the </span><kbd>pytorch_test</kbd><span> directory located in the code repository for this chapter.</span></p>
<div class="packt_tip">If your code is targeted at older versions than 0.4, you should perhaps check out the migration guide for PyTorch 0.4; first: <a href="https://pytorch.org/blog/pytorch-0_4_0-migration-guide">https://pytorch.org/blog/pytorch-0_4_0-migration-guide</a>. There is no official migration guide for versions later than 0.4, however, you'll certainly find plenty of information on the internet with a simple web search.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CUDA – GPU acceleration for fast training and evaluation</h1>
                </header>
            
            <article>
                
<p>The NVIDIA CUDA Toolkit (<a href="https://developer.nvidia.com/cuda-toolkit">https://developer.nvidia.com/cuda-toolkit</a>) is a fully optimized parallel computing platform for general-purpose computing on graphics processing units (GPGPU). It allows us to perform scientific computing on <span>NVIDIA graphic cards, including linear algebra, image and video processing, deep learning, and graph analytics. It is used by a lot of commercial and open source software to enable GPU-accelerated computation across different domains. If we look back at the development of deep learning, we should realize that the latest breakthroughs in GANs would have been almost impossible without the help of CUDA and powerful GPUs. Therefore, we highly recommend you try out the experiments in this book on a CUDA-compatible GPU; otherwise, the training time of neural networks could be painfully long on CPUs.</span></p>
<p>In this section, we will walk you through the installation of CUDA on Windows 10 and Ubuntu 18.04. Before we start installing CUDA, you should make sure that your video card supports CUDA and you have installed the latest driver for your video card. To check whether your GPU is compatible with CUDA (or the exact CUDA version you want to install), you should first make sure you have an NVIDIA video card on your machine.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>On Windows, you can use third-party tools such as GPU-Z (<a href="https://www.techpowerup.com/gpuz">https://www.techpowerup.com/gpuz</a>) or GPU Caps Viewer (<a href="http://www.ozone3d.net/gpu_caps_viewer">http://www.ozone3d.net/gpu_caps_viewer</a>) to examine the specifications of your video card. <span>You can always check this web page</span><sup> </sup>to see if your video card is on the list: <a href="https://www.geforce.com/hardware/technology/cuda/supported-gpus">https://www.geforce.com/hardware/technology/cuda/supported-gpus</a>. The most straightforward and practical way, however, to check whether the latest CUDA perfectly runs on your system, is to finish the installation and evaluation steps in the following subsections without any issues.</p>
<p>At the time of writing this book, the latest version of CUDA is 10.1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing NVIDIA driver</h1>
                </header>
            
            <article>
                
<p>On Windows 10, visit <a href="https://www.nvidia.com/Download/index.aspx">https://www.nvidia.com/Download/index.aspx</a> to download the driver by choosing the product and operating system <span>based on your video card and system. Installation on Windows should be very straightforward since it has a <strong>graphical user interface</strong> (<strong>GUI</strong>). You can keep the default settings during installation.</span></p>
<p>On Ubuntu 18.04, you can always download CUDA from the <em>How to install CUDA 10.1 on Ubuntu 18.04</em> (<a href="https://gist.github.com/eddex/707f9cbadfaec9d419a5dfbcc2042611">https://gist.github.com/eddex/707f9cbadfaec9d419a5dfbcc2042611</a>). However, we recommend you install the NVIDIA driver in the following way so that your graphics driver can be updated in the same way as other software. First, open up a Terminal and add the proper repository to your package management source list by typing in the following:</p>
<pre><strong>$ sudo add-apt-repository ppa:graphics-drivers/ppa</strong><br/><strong>$ sudo apt-get update</strong></pre>
<p>Now, you can check your video card model and the recommended driver version by implementing the following:</p>
<pre><strong>$ ubuntu-drivers devices</strong></pre>
<p>The output may look like this:</p>
<pre><strong>== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==</strong><br/><strong>modalias : pci:v000010DEd00001B06sv00001458sd00003752bc03sc00i00</strong><br/><strong>vendor : NVIDIA Corporation</strong><br/><strong>model : GP102 [GeForce GTX 1080 Ti]</strong><br/><strong>driver : nvidia-driver-390 - third-party free</strong><br/><strong>driver : nvidia-driver-396 - third-party free</strong><br/><strong>driver : nvidia-driver-415 - third-party free recommended</strong><br/><strong>driver : nvidia-driver-410 - third-party free</strong><br/><strong>driver : xserver-xorg-video-nouveau - distro free builtin</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Then, install the recommended driver with the following:</p>
<pre><strong>$ sudo ubuntu-drivers autoinstall</strong></pre>
<div class="packt_tip"><span>If you already have CUDA installed and plan on installing a different version of CUDA, we recommend you uninstall both the NVIDIA driver and CUDA toolkit, reboot your system, and install the latest </span><span>driver before re-installing CUDA.</span></div>
<p>When the installation is finished, reboot your system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing CUDA</h1>
                </header>
            
            <article>
                
<p>Here's the full list of CUDA toolkits: <a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a>. Click<span class="packt_screen"> CUDA Toolkit 10.1</span> to navigate to the download page for CUDA 10.1.</p>
<p>On <strong>Windows 10</strong>, select <span class="packt_screen">Windows | x86_64 | 10 | exe(local)</span>, <span>and download the b</span>ase installer<span>. The installer file is about 2.1 GB. Again, we won't go into details regarding the installation process since it's GUI-based. Just keep the default settings during installation.</span></p>
<div class="packt_tip">Make sure you also install the official CUDA samples during installation. They are essential for us to evaluate the successful installation of CUDA later on and very useful for learning CUDA programming (if you are interested). Also, if you plan on installing Microsoft Visual Studio on Windows as well, make sure you install it before CUDA, because CUDA will then automatically detect Visual Studio and install the corresponding integration tool.</div>
<p>On Ubuntu 18.04, select <span class="packt_screen">Linux | x86_64 | Ubuntu | 18.04 | runfile(local)</span>, and download the Base Installer. The installer file is about 2.0 GB. When the download is finished, which could take a little while (say it's downloaded under the <kbd>~/Downloads</kbd> directory), open up a Terminal and type in the following:</p>
<pre><strong>$ cd ~/Downloads</strong><br/><strong>$ sudo chmod +x cuda_10.1.243_418.86.00_linux.run</strong><br/><strong>$ sudo sh cuda_10.1.243_418.86.00_linux.run</strong></pre>
<p>During the installation, accept<span> </span>all default settings, except that we don't need to install the NVIDIA driver when prompted, since we have already installed a newer version previously.</p>
<p class="mce-root"/>
<div class="packt_tip">By the end of the installation of CUDA, there might be several warning messages, such as Missing recommended library: <kbd>libGLU.so</kbd>. Simply run <kbd>apt-get install libglu1-mesa libxi-dev libxmu-dev libglu1-mesa-dev</kbd> to install those optional libraries.</div>
<p>Finally, add CUDA directories to your <kbd>~/.bashrc</kbd> file so that other software can find your CUDA library:</p>
<pre><strong>$ export PATH=$PATH:/usr/local/cuda/bin</strong><br/><strong>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64</strong></pre>
<p>Alternatively, you can open the file with <kbd>gedit ~/.bashrc</kbd> and <span>manually </span>add these two lines at the end of the file:</p>
<pre><strong>PATH=$PATH:/usr/local/cuda/bin</strong><br/><strong>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64</strong></pre>
<p>Run <kbd>sudo ldconfig</kbd> to refresh the changes we make to the <kbd>.bashrc</kbd> file. Make sure you close and re-open the Terminal before running any other bash command.</p>
<p>For other platforms, please visit <a href="https://docs.nvidia.com/cuda/archive/10.0">https://docs.nvidia.com/cuda/archive/10.0</a> and follow the instructions there to install CUDA 10.0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing cuDNN</h1>
                </header>
            
            <article>
                
<p>In order to enable the fast computation capability provided by CUDA for neural networks, we need to install cuDNN. <span>The <strong>NVIDIA CUDA Deep Neural Network library</strong> (<strong>cuDNN</strong>) is a GPU-accelerated library for deep neural networks. It's basically a low-level driver that runs on GPUs that provides multiple fully optimized forward and backward computation for common neural network operations. It has been used by many deep learning platforms, including PyTorch, so that the platform developers don't have to worry about implementing the basic neural network components and can focus on delivering better APIs for us to use.</span></p>
<p class="mce-root"/>
<p>First, we need to download cuDNN from this site: <a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a>. Previous versions are available at <a href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a>. Look for the cuDNN release that fits your CUDA version and your OS. Normally, any version of cuDNN that's bigger than <strong>7.0</strong> would be acceptable for PyTorch. You can always grab the latest version, of course. Here, we will download <strong>cuDNN v7.5.0 for CUDA 10.1</strong> from the first of the preceding links. Please note that you will need to register an NVIDIA Developer account <span>with a valid email address </span>to become a member of the NVIDIA Developer Program; then all the cuDNN release files are free to download.</p>
<p>On Windows 10, click Download cuDNN v7.5.0 (Feb 21, 2019); for CUDA 10.0, click <strong>cuDNN Library for Windows 10</strong>. This will download a <kbd>cudnn-10.0-windows10-x64-v7.5.0.56.zip</kbd> file that is about 224 MB. Unzip the downloaded file and copy the unzipped files to CUDA directory as follows:</p>
<ul>
<li><kbd>[UNZIPPED_DIR]\cuda\bin\cudnn64_7.dll -&gt; C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\cudnn64_7.dll</kbd></li>
<li><kbd>[UNZIPPED_DIR]\cuda\include\cudnn.h -&gt; C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include\cudnn.h</kbd></li>
<li><kbd>[UNZIPPED_DIR]\cuda\lib\x64\cudnn.lib -&gt; C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\lib\x64\cudnn.lib</kbd></li>
</ul>
<p>On Ubuntu 18.04, <span>click<strong> </strong></span><strong>Download cuDNN v7.5.0 (Feb 21, 2019)</strong>; for CUDA 10.0<span>, click</span> <strong>cuDNN Library for Linux</strong>. A <kbd>cudnn-10.0-linux-x64-v7.5.0.56.tgz</kbd> file will be downloaded. The file size is about 433 MB. When the download is finished, let's open up a Terminal and run the following scripts (we assume that your file has been downloaded to the <kbd>~/Downloads</kbd> directory):</p>
<p>Unzip the downloaded file:</p>
<pre><strong>$ cd ~/Downloads</strong><br/><strong>$ tar -xzvf cudnn-10.0-linux-x64-v7.5.0.56.tgz</strong></pre>
<p>Copy the files to the system directory and grant the read permissions for all users (you may need to <kbd>cd</kbd> to the extracted folder first):</p>
<pre><strong>$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include</strong><br/><strong>$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</strong><br/><strong>$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</strong></pre>
<p>On other platforms, please follow the instructions at <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html">https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html</a> to install cuDNN.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating your CUDA installation</h1>
                </header>
            
            <article>
                
<p>Let's see if CUDA works properly on your machine. Here, we assume that you have also installed the official CUDA samples.</p>
<div class="packt_infobox">Here, Microsoft Visual Studio is needed to build and test the CUDA sample on Windows. We are using Visual Studio Community 2017 in this example.</div>
<p>On Windows 10, <span>navigate to the CUDA samples directory (for example,</span> <kbd>C:\ProgramData\NVIDIA Corporation\CUDA Samples\v10.0</kbd><span>). Open the </span><kbd>1_Utilities\deviceQuery\deviceQuery_vs2017.sln</kbd> <span>solution file with Visual Studio 2017.</span></p>
<p>In <span>Visual Studio, switch the</span> <strong>Solution Configurations</strong> to <strong>Release</strong><span>. Then, click</span> <span class="packt_screen">Build | Build deviceQuery</span> <span>to build the sample code. When the build is finished, navigate to </span><kbd>C:\ProgramData\NVIDIA Corporation\CUDA Samples\v10.0\bin\win64\Release</kbd> <span>and open PowerShell under this directory. Type in the following command:</span></p>
<pre><strong>&gt; .\deviceQuery.exe</strong></pre>
<p>The output should look something like this:</p>
<pre><strong>CUDA Device Query (Runtime API) version (CUDART static linking)</strong><br/><br/><strong>Detected 1 CUDA Capable device(s)</strong><br/><br/><strong>Device 0: "GeForce GTX 1080 Ti"</strong><br/><strong>  CUDA Driver Version / Runtime Version 10.0 / 10.0</strong><br/><strong>  CUDA Capability Major/Minor version number: 6.1</strong><br/><strong>  Total amount of global memory: 11175 MBytes (11718230016 bytes)</strong><br/><strong>...</strong><br/><strong>Result = PASS</strong></pre>
<p><span>This indicates that CUDA 10.0 has been successfully installed.</span></p>
<p>On Ubuntu 18.04, navigate to the CUDA samples directory (for example, <kbd>~/NVIDIA_CUDA-10.0_Samples</kbd>). Open the Terminal and type in:</p>
<pre><strong>$ cd 1_Utilities/deviceQuery</strong><br/><strong>$ make</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This should compile the <kbd>deviceQuery</kbd> program without any issue. Then, navigate to the build directory and run the program:</p>
<pre><strong>$ cd ../../bin/x86_64/linux/release</strong><br/><strong>$ ./deviceQuery</strong></pre>
<p>The output should look similar to that from Windows 10.</p>
<p>Now we can move on to installing PyTorch 1.0!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing PyTorch on Windows and Linux</h1>
                </header>
            
            <article>
                
<p>To install and use PyTorch, we need to properly set up the Python development environment first. So, in this section, we will first talk about how to set up the Python environment, then how to install PyTorch either with official release binaries or by building from source. At the end of this section, we will introduce you to a lightweight, yet extremely powerful code editor tool, Microsoft VS Code, and show you how to configure it for PyTorch programming.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Python environment</h1>
                </header>
            
            <article>
                
<p>In the following sections, we will walk you through how to set up the Python environment and how to install or build PyTorch on Windows 10 and Ubuntu 18.04. We assume that, of course, you have successfully installed CUDA on your system (for example, CUDA 10.1).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Python</h1>
                </header>
            
            <article>
                
<p>On Windows 10, visit <a href="https://www.python.org/downloads/windows">https://www.python.org/downloads/windows</a> to download the Windows x86-64 executable installer. You may install any version you want. We'll install the latest version (at the time of writing)<span>, 3.7.5,</span> a<span>s</span> an example. Actually, 3.8.0 is the very latest version, but it's better to stay on the 3.7.x track. The downloaded <kbd>python-3.7.5-amd64.exe</kbd> file is about 25 MB. Keep the default settings during installation, except that we could change the installation path to an easier-to-find location, that is, <kbd>C:\Python37</kbd>.</p>
<p class="mce-root"/>
<div class="packt_tip">Make sure you check the box for <span class="packt_screen">Add Python 3.7 to PATH</span> during installation, otherwise, you'll have to add the environment variables manually: <kbd>C:\Python37\</kbd> and <kbd>C:\Python37\Scripts\</kbd>. The detailed process of adding <span>environment variables on Windows 10 is described later in this chapter.</span></div>
<p>On Ubuntu 18.04, Python 2.7.15 and 3.7.1 have already been shipped with the system. So, you don't have to do anything for now.</p>
<div class="packt_tip">On Ubuntu, if you plan on using the default version of Python provided by the system, think twice before you modify it (including upgrading, downgrading, or uninstalling) because it will affect many other things in your system. And always make sure you are using the right version of Python (<span>that is,</span> Python 2 vs 3). Sometimes, installing and using packages across Python 2 and Python 3 can be a little bit messy.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Anaconda Python</h1>
                </header>
            
            <article>
                
<p><span>On</span> Windows 10<span>, download the installer from <a href="https://www.anaconda.com/distribution/#windows">https://www.anaconda.com/distribution/#windows</a>. We'll download and install </span>Python 3.7 version<span> as an example. This will download an </span><kbd>Anaconda3-2018.12-Windows-x86_64.exe</kbd> <span>file that is about 614 MB in size. Open this file to install Anaconda and keep the default settings unchanged. Note that we don't have to check the box for <strong>Register Anaconda as the system Python 3.7</strong> because we will create a new Python environment and add the corresponding environment variables manually later on.</span></p>
<p><span>At the end of the installation, you will be asked whether you want to install the Microsoft VS Code. We recommend you install one for Python development.</span></p>
<p><span>On</span> Ubuntu 18.04<span>, download the installer from <a href="https://www.anaconda.com/distribution/#linux">https://www.anaconda.com/distribution/#linux</a>. Here, we download and install </span>Python version 3.7, <span>for example. An</span> <kbd>Anaconda3-2018.12-Linux-x86_64.sh</kbd> <span>file will be downloaded. The file size is around 684 MB. Run this file to install it (assume that it's located at</span> <kbd>~/Downloads</kbd><span>):</span></p>
<pre><strong>$ cd ~/Downloads</strong><br/><strong>$ chmod +x Anaconda3-2018.12-Linux-x86_64.sh</strong><br/><strong>$./Anaconda3-2018.12-Linux-x86_64.sh</strong></pre>
<p>During the installation, accept all default settings. By the end of the installation, you will be prompted as to whether to install <span>Microsoft </span>VS Code on your system. You can accept it if you haven't installed it yet.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prerequisites before we move on</h1>
                </header>
            
            <article>
                
<p>There are some important, or even necessary, Python tools and libraries we need to install before moving on to the next section, including:</p>
<ul>
<li><strong>Pip </strong><q>(</q>required<q>):</q> It is required to manage your Python packages. On Ubuntu, run <kbd>sudo apt-get install python-pip</kbd> for Python 2 or <kbd>sudo apt-get install python3-pip</kbd><span> for Python 3. On Windows, it's usually installed along with Python.</span></li>
<li><strong>NumPy</strong> <span><q>(</q></span>required<span><q>):</q> A scientific computing library for tensor representation, manipulation, and calculation, along with linear algebra, the Fourier transform, and random number capabilities. It is required to install PyTorch.</span></li>
<li><strong>SciPy</strong> <span><q>(</q></span>optional<span><q>):</q> A collection of numerical algorithms including signal processing, optimization, and statistics. We will use it mainly for its statistics capability, for example, initializing parameters based on a certain random distribution.</span></li>
<li><strong>OpenCV</strong> <span><q>(</q></span>optional<span><q>):</q> A cross-platform open source computer vision library for efficient and real-time image processing and pattern recognition. We will use it to preprocess or visualize the data, parameters, and feature maps in neural networks.</span></li>
<li><strong>Matplotlib</strong> <span><q>(</q></span>optional<span><q>):</q> A publication-quality plotting library. We will use it to illustrate loss curves or other plots.</span></li>
</ul>
<p>On Windows 10, you can visit <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs">https://www.lfd.uci.edu/~gohlke/pythonlibs</a> to download the <kbd>.whl</kbd> files for these libraries and install them with <kbd>pip install [FILENAME]</kbd> (for Python 2) or <kbd>pip3 install [FILENAME]</kbd> <span>(for Python 3).</span></p>
<p>On Ubuntu 18.04, you can install these packages with the following:</p>
<pre><strong>#For Python 2</strong><br/><strong>$ pip install numpy scipy opencv-python matplotlib</strong><br/><strong>#For Python 3</strong><br/><strong>$ pip3 install numpy scipy opencv-python matplotlib</strong></pre>
<div class="packt_tip"><span>The installation may fail due to user </span><span>permission issues. If you are an administrator user on Windows, make sure you open Command Prompt as administrator. If you have root access on Ubuntu, simply add <kbd>sudo</kbd> before the installation command. If you don't have the administrator or root access at all, install the packages with <kbd>pip3 install --user</kbd>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing PyTorch</h1>
                </header>
            
            <article>
                
<p>You can either install PyTorch using the official release binaries or by building it from source. You can install PyTorch directly on your system, or use a <span>package manager (such as Anaconda) to avoid potential conflicts with other tools. At the time of writing this book, the latest version of PyTorch is v1.3.1. Since we want to take advantage of the cutting-edge functionalities provided by PyTorch, we will install and use PyTorch 1.3 in all the remaining chapters of this book. You can, of course, choose any other version you wish, or install an even newer version than the one we use in this book. Simply change the version number to your own version when you follow the following instructions.</span></p>
<p><span>We highly recommend that you install PyTorch with Anaconda if you are using Ubuntu because it won't affect the default Python environment that's shipped with the system. If you are on Windows, you can basically delete the Python installation and re-install any other version you want, if anything goes seriously wrong.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing official binaries</h1>
                </header>
            
            <article>
                
<p>Not too long ago, installing PyTorch was a major endeavor. However, the good folks at PyTorch.org have made it very easy for you to install PyTorch on your system. Go to <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a> to get started. There, you will find a very simple point and click method to get the proper installation information.</p>
<p>You should start with the build that you want to install, and then select the operating system. Next, you should determine the way you want to install PyTorch, if it's via Conda, pip, and so on. Next, select the version of Python you are going to target, and, finally, pick which version of CUDA you are using or whether you are going to go without a GPU:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ecc98148-db55-40dd-803f-d1d83c8ffc76.png" style="width:83.33em;height:30.00em;"/></p>
<p class="mce-root"/>
<p>The last step is to select and copy the command from the box at the bottom of the grid. Paste this into your terminal or Command Prompt and run it. In a minute or two, you'll be all set.</p>
<p>You may also want to consider making the created Python environment the default Python for your system. To do that, all you need to do is add these environment variables: <kbd>C:\Users\John\Anaconda3\envs\torch</kbd> and <kbd>C:\Users\John\Anaconda3\envs\torch\Scripts</kbd>.</p>
<div class="packt_tip">How to add <span><strong>environment variables</strong> on Windows 10: (1) Right-click on the <q>Start</q> button and click <q>System</q>. (2) Click <q>System information</q> on the right in the <q>Settings</q> window, which will open up the <q>System Control Panel</q> (You may not need this step if you are on a rather old version of Windows 10). (3) Click <q>Advanced system settings</q> on the left, which will open the <q>System Properties</q> window. (4) Click <q>the Environment Variables</q> button, which will open the <q>Environment Variables</q> window. (5) Double-click the line for <q>Path</q> variable in <q>User variables</q>. Now you can add or edit the paths pointing to Anaconda or Python directories. Each time you edit environment variables, make sure that you close the <q>Environment Variables</q> window and run this script in PowerShell: <kbd>$env:Path = [System.Environment]::GetEnvironmentVariable("Path","Machine") + ";" + [System.Environment]::GetEnvironmentVariable("Path","User")</kbd>.</span></div>
<p><span>That's it! PyTorch has now been installed on your machine and you can follow the instructions in the </span><em>Evaluating your PyTorch</em> installation<span> section to see if it works properly.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building Pytorch from source</h1>
                </header>
            
            <article>
                
<p>Here, we will only talk about building PyTorch from source using Anaconda Python on Ubuntu 18.04, because the build process has a very high chance of failing on Windows. First, let's create a new Python environment called <kbd>torch-nt</kbd> for building and installing the nightly version with <kbd>conda create -n torch-nt python=3.7</kbd> and activate it with <kbd>conda activate torch-nt</kbd>.</p>
<p>Next, install the dependencies needed for building PyTorch:</p>
<pre><strong>(torch-nt)$ conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing</strong><br/><strong>(torch-nt)$ conda install magma-cuda100 -c pytorch</strong></pre>
<p class="mce-root"/>
<p>Then, download the source code of PyTorch with Git:</p>
<pre><strong>(torch-nt)$ git clone --recursive https://github.com/pytorch/pytorch</strong><br/><strong>(torch-nt)$ cd pytorch</strong><br/><strong>(torch-nt)$ export CMAKE_PREFIX_PATH="/home/john/anaconda3/envs/torch-nt"</strong><br/><strong>(torch-nt)$ python setup.py install</strong></pre>
<p>Here, <kbd>CMAKE_PREFIX_PATH</kbd> points to the root directory of your Python environment. All your environments created by Anaconda are located under the <kbd>~/anaconda3/envs</kbd> folder.</p>
<p>Wait a moment for it to finish. When it's done, run <kbd>python</kbd> in the Terminal, type in <kbd>import torch</kbd>, and press <span class="packt_screen">Enter</span>. If no error pops up, it means that PyTorch has been successfully built and installed.</p>
<div class="packt_tip">Do you remember, do not run <kbd>import torch</kbd> under the same directory you build PyTorch from, because Python will try to pick up the Torch library from the source files, instead of the installed package.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating your PyTorch installation</h1>
                </header>
            
            <article>
                
<div class="packt_infobox">From now on, we will use the Anaconda Python environment called <strong>torch</strong> we previously created as the default Python environment in this book. We will also omit the (torch) indicator in front of the scripts. Also, by default, all of the code in this book is written for Python 3 (specifically, Python 3.7). If you are looking for Python 2 implementations, you might want to look at 3to2 (<a href="https://pypi.org/project/3to2">https://pypi.org/project/3to2</a>).</div>
<p>Let's write a short snippet for matrix multiplication using PyTorch. Create a Python source code file named <kbd>pytorch_test.py</kbd> and copy the following lines into this file:</p>
<pre>import torch<br/><br/>print("PyTorch version: {}".format(torch.__version__))<br/>print("CUDA version: {}".format(torch.version.cuda))<br/><br/>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print(device)<br/><br/>a = torch.randn(1, 10).to(device)<br/>b = torch.randn(10, 1).to(device)<br/>c = a @ b<br/>print(c.item())</pre>
<p class="mce-root"/>
<p class="mce-root">Open the Terminal and run this snippet:</p>
<pre><strong>$ conda activate torch</strong><br/><strong>$ python pytorch_test.py</strong></pre>
<p>The output may look like this:</p>
<pre><strong>PyTorch version: 1.3.1</strong><br/><strong>CUDA version: 10.1.243</strong><br/><strong>cuda</strong><br/><strong>-2.18083119392395</strong></pre>
<p>The last line is totally random, so don't worry if you get a different result. The code is also available <span>under the </span><kbd>pytorch_test</kbd><span> directory located in the code repository for this chapter.</span></p>
<p>You can always use the jit or torchhub examples in previous sections to evaluate the installation of PyTorch. Also, feel free to check out the official examples at <a href="https://github.com/pytorch/examples">https://github.com/pytorch/examples</a>.</p>
<div class="packt_tip">Remember the simple GAN we implemented with NumPy in <a href="66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml">Chapter 1</a>, <em>Generative Adversarial Networks Fundamentals</em>? Now that you have your PyTorch up and ready, you can think about how you would implement it with PyTorch.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bonus: setting up VS Code for Python coding</h1>
                </header>
            
            <article>
                
<p><strong>VS Code</strong> is a lightweight, open source code editor developed by Microsoft. It has built-in syntax highlighting, autocompleting, debugging, Git management, and it has more than 10,000 extensions developed by the community. It supports Windows, macOS, and Linux, and it is the most popular development tool among software developers, according to a StackOverflow survey: <a href="https://insights.stackoverflow.com/survey/2018/#technology-most-popular-development-environments">https://insights.stackoverflow.com/survey/2018/#technology-most-popular-development-environments</a>. If you mainly work on your own machine for learning GANs with this book, we highly recommend you use VS Code for PyTorch development.</p>
<div class="packt_tip">If you often work remotely, which means that you have to write Python code locally and run that code on a remote server, you may consider using PyCharm Professional Edition (<a href="https://www.jetbrains.com/pycharm">https://www.jetbrains.com/pycharm</a>) for this purpose. It has a more mature remote development functionality than free VS Code extensions have to offer.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring VS Code for Python development</h1>
                </header>
            
            <article>
                
<p>Essentially, you only need the Python extension <em>(</em><kbd>ms-python.python</kbd><em>)</em> for Python programming in VS Code.</p>
<p>On Windows 10, <span>click </span><span class="packt_screen">File | Preferences | Settings</span><span>, click the</span> <span class="packt_screen">{}</span> <span>button </span><span>(</span>Open Settings (JSON)<span>) </span><span>on the upper right, and type in the following:</span></p>
<pre>    "python.pythonPath": "C:\\Users\\John\\Anaconda3\\envs\\torch"</pre>
<p>On Ubuntu 18.04, click <span class="packt_screen">File |Preferences | Settings</span>, click the <span class="packt_screen">{}</span> button <span>(</span>Open Settings (JSON)<span>) </span>on the upper right, and type in the following:</p>
<pre>    "python.pythonPath": "~/anaconda3/envs/torch/bin/python3"</pre>
<p>Now, VS Code will automatically recognize it as an Anaconda Python environment and you are ready to write Python code with it!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recommended VS Code extensions</h1>
                </header>
            
            <article>
                
<p>Here are some VS Code extensions that I personally find useful in Python development. I'm sure they will make your work a lot easier as well. Many thanks to their creators!</p>
<ul>
<li><strong>Bracket Pair Colorizer</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=CoenraadS.bracket-pair-colorizer">coenraads.bracket-pair-colorizer</a>): This matches each pair of brackets with different colors, which allows you to easily recognize them.</li>
<li><strong>Code Runner</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner">formulahendry.code-runner</a>): This allows you to run Python (and many other languages') code with a click of the button. However, we don't recommend you use it to run the training snippets of neural networks because the logging messages can be rather long and some messages might go missing in VS Code.</li>
<li><strong>GitLens - Git supercharged</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens">eamodio.gitlens</a>): This is a powerful tool if you rely on Git to manage your source code. For example, it shows Git history on each line you're currently looking at in the editor, shows all the local and remote changes in a tree structure, and so on.</li>
<li><strong>indent-switcher</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=ephoton.indent-switcher">ephoton.indent-switcher</a>): Everyone's programming habits are different. Some like two spaces as indentation, and some like four spaces. You can switch between two-space and four-space <span>indentation with this extension.</span></li>
<li><strong>Partial Diff</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=ryu1kn.partial-diff">ryu1kn.partial-diff</a>): This allows you to compare two code snippets across different files.</li>
<li><strong>Path Intellisense</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=christian-kohler.path-intellisense">christian-kohler.path-intellisense</a>): This extension autocompletes filenames in your code.</li>
<li><strong>Search - Open All Results</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-search-open-all-results">fabiospampinato.vscode-search-open-all-results</a>): This supports searching keywords across multiple source files.</li>
<li><strong>Settings Sync</strong> (<a href="https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync">shan.code-settings-sync</a>): This saves the installed extensions and user settings to a Gist file and recovers from that file. It can be very useful if you work on multiple machines and systems. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References and useful reading list</h1>
                </header>
            
            <article>
                
<ol>
<li>Udacity India. (2018, Mar 8). <em>Why Python is the most popular language used for Machine Learning</em>. Retrieved from <a href="https://medium.com/@UdacityINDIA/why-use-python-for-machine-learning-e4b0b4457a77">https://medium.com/@UdacityINDIA/why-use-python-for-machine-learning-e4b0b4457a77</a>.</li>
<li>S Bhutani. (2018, Oct 7). <em>PyTorch 1.0 - A brief summary of the PTDC ’18: PyTorch 1.0 Preview and Promise</em>. Retrieved from <a href="https://hackernoon.com/pytorch-1-0-468332ba5163">https://hackernoon.com/pytorch-1-0-468332ba5163</a>.</li>
<li>C Perone. (2018, Oct 2). <em>PyTorch 1.0 tracing JIT and LibTorch C++ API to integrate PyTorch into NodeJS</em>. Retrieved from <a href="http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs">http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs</a>.</li>
<li>T Wolf. (2018, Oct 15). <em>Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU and Distributed setups</em>. Retrieved from <a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255</a>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Wow! That was a lot of work and information. Take a minute, grab a cup of coffee or tea, and come back. I'll wait.</p>
<p>Let's look at all the things we've done.</p>
<p>We have made sure that we are up to date with our Python installation, installed CUDA (assuming we have an NVIDIA GPU graphics card) and installed PyTorch. If you are anything like me, you are <em>chomping at the bit</em> to get going and do some programming.</p>
<p>However, we need to get some more basics defined before we can really be productive, which is our goal. In the next chapter, we will go through some of the basics.</p>


            </article>

            
        </section>
    </body></html>