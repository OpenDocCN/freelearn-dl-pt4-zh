- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlocking the Potential of Graph Neural Networks for Real-World Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for taking the time to read *Hands-On Graph Neural Networks Using
    Python*. We hope that it has provided you with valuable insights into the world
    of graph neural networks and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this book, we would like to leave you with some final pieces
    of advice on how to effectively use GNNs. GNNs can be incredibly performant in
    the right conditions, but they suffer from the same pros and cons as other deep
    learning techniques. Knowing when and where to apply these models is a crucial
    skill to master, as over-engineered solutions can result in poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: First, GNNs are especially effective when a large amount of data is available
    for training. This is because deep learning algorithms require a lot of data to
    learn complex patterns and relationships effectively. With a large enough dataset,
    GNNs can achieve high levels of accuracy and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: For similar reasons, GNNs are most valuable when dealing with complex, high-dimensional
    data (node and edge features). They can automatically learn intricate patterns
    and relationships between features that would be difficult or impossible for humans
    to identify. Traditional machine learning algorithms, such as linear regression
    or decision trees, rely on handcrafted features that are often limited in their
    ability to capture the complexity of real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when working with GNNs, it is important to ensure that the graph representation
    adds value to the features. This is particularly applicable when the graph is
    an artificially constructed representation rather than a natural one, such as
    social networks or protein structures. The connections between nodes should not
    be arbitrary but represent meaningful relationships between the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that some examples in this book did not follow the previous
    rules. This is mostly due to the technical limitation of being able to run the
    code in Google Colab, and a general lack of high-quality datasets. However, this
    is also reflective of real-life datasets, which can be messy and difficult to
    obtain in large quantities. Most of this data also tends to be tabular, where
    excellent tree-based models such as XGBoost are difficult to beat.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, sound baseline solutions are crucial, as they can be challenging
    to outperform, even in the right conditions. A powerful strategy when working
    with GNNs is to implement multiple types of GNNs and compare their performance.
    For example, a convolutional-based GNN such as GCN ([*Chapter 6*](B19153_06.xhtml#_idTextAnchor074))
    might work well for certain types of graphs, while an attention-based GNN such
    as GAT ([*Chapter 7*](B19153_07.xhtml#_idTextAnchor082)) might be better suited
    for others. Additionally, a message-passing GNN such as MPNN ([*Chapter 12*](B19153_12.xhtml#_idTextAnchor144))
    might excel in certain contexts. Note how each approach is more expressive than
    the previous one, and each has different strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working on a more specific problem, there are several GNN approaches
    covered in this book that may be more appropriate. For example, if you’re dealing
    with small graph data that lacks node and edge features, you may want to consider
    using Node2Vec ([*Chapter 4*](B19153_04.xhtml#_idTextAnchor054)). On the contrary,
    if you’re dealing with large graphs, GraphSAGE and LightGCN can help manage the
    computational time and memory storage requirements (*Chapters 8* and *17*).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, GIN and global pooling layers may be suitable for graph classification
    tasks ([*Chapter 9*](B19153_09.xhtml#_idTextAnchor106)), while Variational Graph
    Autoencoders and SEAL can be used for link prediction ([*Chapter 10*](B19153_10.xhtml#_idTextAnchor116)).
    For generating new graphs, you can explore GraphRNN and MolGAN ([*Chapter 11*](B19153_11.xhtml#_idTextAnchor131)).
    If you’re working with heterogeneous graphs, you may want to consider one of the
    many flavors of heterogeneous GNNs (*Chapters 12* and *16*). For spatio-temporal
    graphs, Graph WaveNet, STGraph, and other temporal GNNs can be useful (*Chapters
    13* and *15*). Finally, if you need to explain the predictions made by your GNN,
    you can turn to the graph explainability techniques covered in [*Chapter 14*](B19153_14.xhtml#_idTextAnchor165).
  prefs: []
  type: TYPE_NORMAL
- en: By reading this book, you will have gained a deep understanding of GNNs and
    how they can be applied to solve real-world problems. As you continue to work
    in this field, we encourage you to put this knowledge into practice, experiment
    with new approaches, and continue to grow your expertise. The field of machine
    learning is constantly evolving, and your skills will only become more valuable
    as time goes on. We hope that you will apply what you have learned to tackle challenges
    and have a positive impact on the world. Thank you again for reading this book,
    and we wish you all the best in your future endeavors.
  prefs: []
  type: TYPE_NORMAL
