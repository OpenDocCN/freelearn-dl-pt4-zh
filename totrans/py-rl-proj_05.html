<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Virtual Worlds in Minecraft</h1>
                </header>
            
            <article>
                
<p>In the two previous  chapters, we discussed the <strong>deep Q-learning</strong> (<strong>DQN</strong>) <span>algorithm</span> for playing Atari games and the <strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>) <span>algorithm</span> for continuous control tasks. We saw the big success of these algorithms in solving complex problems when compared to traditional reinforcement learning algorithms without the use of deep neural networks to approximate the value function or the policy function. Their main disadvantage, especially for DQN, is that the training step converges too slowly, for example, training an agent to play Atari games takes about one week. For more complex games, even one week's training is insufficient.</p>
<p>This chapter will introduce a more complicated example, Minecraft, which is a popular online video game created by Swedish game developer Markus Persson and later developed by Mojang. You will learn how to launch a Minecraft environment using OpenAI Gym and play different missions. In order to build an AI player to accomplish these missions, you will learn the <strong>asynchronous advantage actor-critic</strong> (<strong>A3C</strong>) algorithm, which is a lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. A3C is a widely applied deep reinforcement learning algorithm for different kinds of tasks, training for half the time on a single multi-core CPU instead of a GPU. For Atari games such as Breakout, A3C achieves human-level performance after 3 hours' training, which is much faster than DQN, which requires 3 days' training. You will learn how to implement A3C using Python and TensorFlow. This chapter does not require as much of a mathematical background as the previous chapter—just have fun!</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introduction to the Minecraft environment</li>
<li>Data preparation for training an AI bot in <span>the Minecraft environment</span></li>
<li>The asynchronous advantage actor-critic framework</li>
<li>Implementation of the A3C framework</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the Minecraft environment</h1>
                </header>
            
            <article>
                
<p>The original OpenAI Gym does not contain the Minecraft environment. We need to install a Minecraft environment bundle, available at <a href="https://github.com/tambetm/gym-minecraft">https://github.com/tambetm/gym-minecraft</a>. This bundle is built based on Microsoft's Malmö, which is a platform for AI experimentation and research built on top of Minecraft.</p>
<p>Before installing the <kbd>gym-minecraft</kbd> <span>package, </span><span>Malmö </span>should first be downloaded from <a href="https://github.com/Microsoft/malmo">https://github.com/Microsoft/malmo</a>. We can download the latest pre-built version from <a href="https://github.com/Microsoft/malmo/releases">https://github.com/Microsoft/malmo/releases</a>. After unzipping the package, go to the <kbd>Minecraft</kbd> folder and run <kbd>launchClient.bat</kbd> on Windows, or <kbd>launchClient.sh</kbd> on Linux/MacOS, to launch a Minecraft environment. If it is successfully launched, we can now install <kbd>gym-minecraft</kbd> via the following scripts:</p>
<pre><strong>python3 -m pip install gym</strong><br/><strong>python3 -m pip install pygame</strong><br/><br/><strong>git clone https://github.com/tambetm/minecraft-py.git</strong><br/><strong>cd minecraft-py</strong><br/><strong>python setup.py install</strong><br/><br/><strong>git clone https://github.com/tambetm/gym-minecraft.git</strong><br/><strong>cd gym-minecraft</strong><br/><strong>python setup.py install</strong></pre>
<p>Then, we can run the following code to test whether <kbd>gym-minecraft</kbd> has been successfully installed or not:</p>
<pre>import logging<br/>import minecraft_py<br/>logging.basicConfig(level=logging.DEBUG)<br/><br/>proc, _ = minecraft_py.start()<br/>minecraft_py.stop(proc)</pre>
<p>The <kbd>gym-minecraft</kbd> package provides 15 different missions, including <kbd>MinecraftDefaultWorld1-v0</kbd> and <kbd>MinecraftBasic-v0</kbd>. For example, in <kbd>MinecraftBasic-v0</kbd>, the agent can move around in a small chamber with a box placed in the corner, and the goal is to reach the position of this box. The following screenshots show several missions available in <kbd>gym-minecraft</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-690 image-border" src="assets/9da0172a-3067-4be7-8bbf-d74640b8cf64.png" style="width:34.75em;height:18.33em;"/></p>
<p>The <kbd>gym-minecraft</kbd> package has the same interface as other Gym environments, such as Atari and classic control tasks. You can run the following code to test different Minecraft missions and try to get a high-level understanding of their properties, for example, goal, reward, and observation:</p>
<pre>import gym<br/>import gym_minecraft<br/>import minecraft_py<br/><br/>def start_game():<br/>    env = gym.make('MinecraftBasic-v0')<br/>    env.init(start_minecraft=True)<br/>    env.reset()<br/>     <br/>    done = False<br/>    while not done:<br/>        env.render(mode='human')<br/>        action = env.action_space.sample()<br/>        obs, reward, done, info = env.step(action)<br/>    env.close()<br/><br/>if __name__ == "__main__":<br/>    start_game()</pre>
<p>At each step, an action is randomly drawn from the action space by calling <kbd>env.action_space.sample()</kbd>, and then this action is submitted to the system by calling the <kbd>env.step(action)</kbd> function, which returns the observation and the reward corresponding to this action. You can also try other missions by replacing <kbd>MinecraftBasic-v0</kbd> with other names, for example, <kbd>MinecraftMaze1-v0</kbd> and <kbd>MinecraftObstacles-v0</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>In the Atari environment, recall that there are three modes for each Atari game, for example, Breakout, BreakoutDeterministic, and BreakoutNoFrameskip, and each mode has two versions, for example, Breakout-v0 and Breakout-v4. The main difference between the three modes is the frameskip parameter that indicates the number of frames (steps) the one action is repeated on. This is called the <strong>frame-skipping</strong> technique, which allows us to play more games without significantly increasing the runtime.</p>
<p>However, in the Minecraft environment, there is only one mode where the frameskip parameter is equal to one. Therefore, in order to apply the frame-skipping technique, we need to explicitly repeat a certain action frameskip multiple times during one timestep. Besides this, the frame images returned by the <kbd>step</kbd> function are RGB images. Similar to the Atari environment, the observed frame images are converted to grayscale and then resized to 84x84. The following code provides the wrapper for <kbd>gym-minecraft</kbd>, which contains all the data preprocessing steps:</p>
<pre>import gym<br/>import gym_minecraft<br/>import minecraft_py<br/>import numpy, time<br/>from utils import cv2_resize_image<br/> <br/>class Game:<br/> <br/>    def __init__(self, name='MinecraftBasic-v0', discrete_movement=False):<br/>         <br/>        self.env = gym.make(name)<br/>        if discrete_movement:<br/>            self.env.init(start_minecraft=True, allowDiscreteMovement=["move", "turn"])<br/>        else:<br/>            self.env.init(start_minecraft=True, allowContinuousMovement=["move", "turn"])<br/>        self.actions = list(range(self.env.action_space.n))<br/>        frame = self.env.reset()<br/>         <br/>        self.frame_skip = 4<br/>        self.total_reward = 0<br/>        self.crop_size = 84<br/>        self.buffer_size = 8<br/>        self.buffer_index = 0<br/>        self.buffer = [self.crop(self.rgb_to_gray(frame)) for _ in range(self.buffer_size)]<br/>        self.last_frame = frame<br/>     <br/>    def rgb_to_gray(self, im):<br/>        return numpy.dot(im, [0.2126, 0.7152, 0.0722])<br/>     <br/>    def reset(self):<br/>        frame = self.env.reset()<br/>        self.total_reward = 0<br/>        self.buffer_index = 0<br/>        self.buffer = [self.crop(self.rgb_to_gray(frame)) for _ in range(self.buffer_size)]<br/>        self.last_frame = frame<br/>     <br/>    def add_frame_to_buffer(self, frame):<br/>        self.buffer_index = self.buffer_index % self.buffer_size<br/>        self.buffer[self.buffer_index] = frame<br/>        self.buffer_index += 1<br/>     <br/>    def get_available_actions(self):<br/>        return list(range(len(self.actions)))<br/>     <br/>    def get_feedback_size(self):<br/>        return (self.crop_size, self.crop_size)<br/>     <br/>    def crop(self, frame):<br/>        feedback = cv2_resize_image(frame, <br/>                                    resized_shape=(self.crop_size, self.crop_size), <br/>                                    method='scale', crop_offset=0)<br/>        return feedback<br/>     <br/>    def get_current_feedback(self, num_frames=4):<br/>        assert num_frames &lt; self.buffer_size, "Frame buffer is not large enough."<br/>        index = self.buffer_index - 1<br/>        frames = [numpy.expand_dims(self.buffer[index - k], axis=0) for k in range(num_frames)]<br/>        if num_frames &gt; 1:<br/>            return numpy.concatenate(frames, axis=0)<br/>        else:<br/>            return frames[0]<br/>     <br/>    def play_action(self, action, num_frames=4):<br/>        reward = 0<br/>        termination = 0<br/>        for i in range(self.frame_skip):<br/>            a = self.actions[action]<br/>            frame, r, done, _ = self.env.step(a)<br/>            reward += r<br/>            if i == self.frame_skip - 2: <br/>                self.last_frame = frame<br/>            if done: <br/>                termination = 1<br/>        self.add_frame_to_buffer(self.crop(numpy.maximum(self.rgb_to_gray(frame), self.rgb_to_gray(self.last_frame))))<br/>         <br/>        r = numpy.clip(reward, -1, 1)<br/>        self.total_reward += reward<br/>        return r, self.get_current_feedback(num_frames), termination</pre>
<p>In the constructor, the available actions for Minecraft are restricted to <kbd>move</kbd> and <kbd>turn</kbd> (not considering other actions, such as the camera controls). Converting an RGB image into a grayscale image is quite easy. Given an RGB image with shape (height, width, channel), the <kbd>rgb_to_gray</kbd> function is used to convert an image to grayscale. For cropping and reshaping frame images, we use the <kbd>opencv-python</kbd> or <kbd>cv2</kbd> packages, which contain a Python wrapper around the original C++ OpenCV implementation, that is, the <kbd>crop</kbd> function reshapes an image into an 84x84 matrix. Unlike the Atari environment, where <kbd>crop_offset</kbd> is set to <kbd>8</kbd> to remove the scoreboard from the screen, here, we set <kbd>crop_offset</kbd> to <kbd>0</kbd> and just reshape the frame images.</p>
<p>The <kbd>play_action</kbd> function submits the input action to the Minecraft environment and returns the corresponding reward, observation, and termination signal. The default frameskip parameter is set to <kbd>4</kbd>, meaning that one action is repeated four times for each <kbd>play_action</kbd> call. The <kbd>get_current_feedback</kbd> function returns the observation that stacks the last four frame images together, since only considering the current frame image is not enough for playing Minecraft because it doesn't contain dynamic information about the game status.</p>
<p>This wrapper has the same interface as the wrappers for the Atari environment and classic control tasks. Therefore, you can try to run DQN or TRPO with the Minecraft environment without changing anything. If you have one idle GPU, it is better to run DQN first before trying the A3C algorithm that we will discuss next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous advantage actor-critic algorithm</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we discussed the DQN for playing Atari games and the use of the DPG and TRPO algorithms for continuous control tasks. Recall that DQN has the following architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/29dfe3cd-6d21-40c9-b4c1-9279fcee5630.png" style="width:29.83em;height:22.83em;"/></p>
<p>At each timestep <img class="fm-editor-equation" src="assets/41ed7ed5-deb3-4fd3-a97a-40bb6de0ee10.png" style="width:0.58em;height:1.25em;"/>, the agent observes the frame image <img class="fm-editor-equation" src="assets/28fd3757-7d72-4c43-bae2-e929925fd291.png" style="width:1.25em;height:1.25em;"/> and selects an action <img class="fm-editor-equation" src="assets/67c43cdd-89c8-4e29-a2ef-0df2ebe4ae8d.png" style="width:1.33em;height:1.17em;"/> based on the current learned policy. The emulator (the Minecraft environment) executes this action and returns the next frame image <img class="fm-editor-equation" src="assets/7a0548c3-d809-4999-8498-143237b64fd8.png" style="width:2.67em;height:1.33em;"/> and the corresponding reward <img class="fm-editor-equation" src="assets/a3a2e63a-aff0-4b10-9e59-181e45fe8f43.png" style="width:1.25em;height:1.25em;"/>. The quadruplet <img class="fm-editor-equation" src="assets/38c725ab-c097-44a5-989c-4387ba094df0.png" style="width:9.83em;height:1.83em;"/> is then stored in the experience memory and is taken as a sample for training the Q-network by minimizing the empirical loss function via stochastic gradient descent.</p>
<p>Deep reinforcement learning algorithms based on experience replay have achieved unprecedented success in playing Atari games. However, experience replay has several disadvantages:</p>
<ul>
<li>It uses more memory and computation per real interaction</li>
<li>It requires off-policy learning algorithms that can update from data generated by an older policy</li>
</ul>
<p>In order to reduce memory consumption and accelerate the training of an AI agent, Mnih et al. proposed an A3C framework for deep reinforcement learning that dramatically reduces the training time without performance loss. This work, <em>Asynchronous Methods for Deep Reinforcement Learning</em>, was published in ICML, 2016.</p>
<p>Instead of experience replay, A3C asynchronously executes multiple agents in parallel on multiple instances of the environment, such as the Atari or Minecraft environments. Since the parallel agents experience a variety of different states, this parallelism breaks the correlation between the training samples and stabilizes the training procedure, which means that the experience memory can be removed. This simple idea enables a much larger spectrum of fundamental on-policy reinforcement learning algorithms, such as Sarsa and actor-critic methods, as well as off-policy reinforcement learning algorithms, such as Q-learning, to be applied robustly and effectively using deep neural networks.</p>
<p>Another advantage is that A3C is able to run on a standard multi-core CPU without relying on GPUs or massively distributed architectures, and requires far less training time than GPU-based algorithms, such as DQN, when applied to Atari games. A3C is good for a beginner in deep reinforcement learning since you can apply it to Atari games on a standard PC with multiple cores. For example, for Breakout, it takes only two-three hours to achieve a score of 300 when executing eight agents in parallel.</p>
<p>In this chapter, we will use the same notations as before. At each timestep <img class="fm-editor-equation" src="assets/45da604e-0854-485a-a60c-23defff392c8.png" style="width:0.42em;height:0.92em;"/>, the agent observes state <img class="fm-editor-equation" src="assets/5be1b484-ebbe-4e3f-ac32-57aaee313588.png" style="width:1.00em;height:1.00em;"/>, takes action <img class="fm-editor-equation" src="assets/34265388-a7e5-448c-a166-b266407f287f.png" style="width:0.92em;height:0.83em;"/>, and then receives the corresponding reward <img class="fm-editor-equation" src="assets/a7721b22-193a-45c9-8a3c-c67ad83015cb.png" style="width:0.92em;height:0.92em;"/> generated from a function <img class="fm-editor-equation" src="assets/d527382a-d269-415b-879f-c3bc82294fc8.png" style="width:3.92em;height:1.25em;"/>. We use <img class="fm-editor-equation" src="assets/5c97dded-1a77-48a2-814e-1983905130ed.png" style="width:3.75em;height:1.33em;"/> to denote the policy of the agent, which maps states to a probability distribution over the actions. The Bellman equation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d3db8e06-3cf6-4ae8-929e-aef016a7b54b.png" style="width:28.50em;height:1.42em;"/></p>
<p>The state-action value function <img class="fm-editor-equation" src="assets/d12ee59d-1fcb-4cd7-bc9c-bd9b56094d6d.png" style="width:0.83em;height:1.08em;"/> can be approximated by a neural network parameterized by <img class="fm-editor-equation" src="assets/bed31c8b-585c-4c44-b793-05c98ef1a50c.png" style="width:1.17em;height:1.08em;"/>, and the policy <img class="fm-editor-equation" src="assets/ebacb345-5c0b-40ee-98cd-4a9e7d7685cc.png" style="width:0.67em;height:0.67em;"/> can also be represented by another neural network parameterized by <img class="fm-editor-equation" src="assets/fa647406-dbbe-4c4e-a3d7-f929057aee9a.png" style="width:1.08em;height:1.00em;"/>. Then, <img class="fm-editor-equation" src="assets/c65564c6-d8ca-4147-ba07-5c303b533323.png" style="width:0.92em;height:1.17em;"/> can be be trained by minimizing the following loss function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a34c51ab-d44d-4ee3-9814-987ae393b567.png" style="width:18.83em;height:1.67em;"/></p>
<p><img class="fm-editor-equation" src="assets/e2dc75c1-f65d-4b16-91b5-74e3bc874fc0.png" style="width:1.33em;height:1.33em;"/> is the approximated state-action value function at step <img class="fm-editor-equation" src="assets/732fb339-3138-4c91-909c-4bb069c0c7ff.png" style="width:0.42em;height:0.92em;"/>. In one-step Q-learning such as DQN, <img class="fm-editor-equation" src="assets/9ffb3f36-dc7c-46f9-8ebd-2dd889a37a22.png" style="width:3.75em;height:1.33em;"/> equals <img class="fm-editor-equation" src="assets/ea64e07b-04f6-48ca-b25f-b57528eed50f.png" style="width:7.50em;height:1.75em;"/>, so that the following is true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2936efe7-364f-497b-9fba-c6a84fd299ff.png" style="width:18.67em;height:2.08em;"/></p>
<p class="mce-root"/>
<p>One drawback of using one-step Q-learning is that obtaining a reward <img class="fm-editor-equation" src="assets/e09ee797-bb93-440b-bff7-7951332b22a0.png" style="width:3.67em;height:1.17em;"/> only directly affects the value of the state action pair <img class="fm-editor-equation" src="assets/6b4d4964-7ff3-4316-b58e-00b0846092cc.png" style="width:3.17em;height:1.25em;"/> that led to the reward. This can make the learning process slow since many updates are required to propagate a reward to the relevant preceding states and actions. One way of propagating rewards faster is by using n-step returns. In n-step Q-learning, <img class="fm-editor-equation" src="assets/caca62a4-f2dc-4df3-9cf8-d6dc2c809c77.png" style="width:1.08em;height:1.08em;"/> can be set to this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef0542d8-0f10-454d-8143-a06eb7c55615.png" style="width:46.42em;height:1.92em;"/></p>
<p>As opposed to value-based methods, a policy-based method, such as TRPO, directly optimizes the policy network <img class="fm-editor-equation" src="assets/572b6b4e-0f30-4124-9b22-4898ae972d0f.png" style="width:0.92em;height:0.92em;"/>. Besides TRPO, a much simpler method is REINFORCE, which updates the policy parameter <img class="fm-editor-equation" src="assets/603c793f-3279-4b6f-9432-da8a0c1565d7.png" style="width:1.00em;height:0.92em;"/> in the direction <img class="fm-editor-equation" src="assets/3bb579e0-2d10-4d82-b2b1-31fec55d2f2f.png" style="width:14.00em;height:1.42em;"/>, where <img class="fm-editor-equation" src="assets/d4fccc88-5310-4eb7-a341-e5edb263382d.png" style="width:13.92em;height:1.33em;"/> is the the advantage of action <img class="fm-editor-equation" src="assets/648a4f3f-f5df-4f96-a31b-6c5a0216537c.png" style="width:1.33em;height:1.17em;"/> in state <img class="fm-editor-equation" src="assets/88de09b7-bcde-459f-ba1c-d29b81f0a05d.png" style="width:1.25em;height:1.25em;"/>. This method is an actor-critic approach due to the fact that it is required to estimate the value function <img class="fm-editor-equation" src="assets/e39c8f90-7f7f-468a-8623-b840ca133089.png" style="width:2.33em;height:1.17em;"/> and the policy <img class="fm-editor-equation" src="assets/afbdb39f-4862-40dc-96d8-b9fc6a6cdf4d.png" style="width:4.00em;height:1.42em;"/>.</p>
<p>The asynchronous reinforcement learning framework can be applied in the approaches already discussed here. The main idea is that we run multiple agents in parallel with their own instances of the environment, for example, multiple players play the same game using their own games consoles. These agents are likely to be exploring different parts of the environment. The parameters <img class="fm-editor-equation" src="assets/40e8244b-9873-4b75-9fc6-2072ce8647e4.png" style="width:1.33em;height:1.25em;"/> and <img class="fm-editor-equation" src="assets/b71f66bd-8b71-4552-a87b-de87ced4eedf.png" style="width:0.83em;height:0.75em;"/> are shared among all agents. Each agent updates the policy and the value function asynchronously without considering read–write conflicts. Although it seems weird that there is no synchronization in updating the policy, this asynchronous method not only removes the communication costs of sending gradients and parameters, but also guarantees the convergence. For more details, please refer to the following paper: <em>A lock-free approach to parallelizing stochastic gradient descent</em>, Recht et al. This chapter focuses on  A3C, namely, we apply the asynchronous reinforcement learning framework in REINFORCE. The following diagram shows the A3C architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-762 image-border" src="assets/dbf41bca-f28f-4f1d-9dec-4743f801d062.png" style="width:47.25em;height:29.42em;"/></p>
<p class="mce-root"/>
<p>For A3C, the policy <img class="fm-editor-equation" src="assets/63257f79-470b-4c26-ba1c-e167c49ff93c.png" style="width:5.33em;height:1.33em;"/> and the value function <img class="fm-editor-equation" src="assets/c3d8f91a-a788-4fb6-8200-57c127489e82.png" style="width:3.92em;height:1.33em;"/> are approximated by two neural networks. A3C updates the policy parameter <img class="fm-editor-equation" src="assets/5a301c9e-5246-4be5-b347-6f53e3b147b0.png" style="width:1.25em;height:1.17em;"/> in the direction <img class="fm-editor-equation" src="assets/25d10240-2cd5-4a76-99fd-3ea072246796.png" style="width:13.17em;height:1.33em;"/>, where <img class="fm-editor-equation" src="assets/fdf6594a-5e6c-403e-b0bf-01ad6fcaed65.png" style="width:3.83em;height:1.25em;"/> is fixed, which is estimated by the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/183390c0-555b-4a40-8872-bde1af962b26.png" style="width:25.92em;height:3.67em;"/></p>
<p>A3C updates the value function parameter <img class="fm-editor-equation" src="assets/491def82-0543-4635-b454-849b899a1f61.png" style="width:1.75em;height:1.67em;"/> by minimizing the loss:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e15cf3b5-e3a0-447d-8358-9259ce6e3bef.png" style="width:24.17em;height:3.75em;"/></p>
<p><img class="fm-editor-equation" src="assets/7f7fcf68-337b-4497-93af-a621b7f57365.png" style="width:4.17em;height:1.50em;"/> is computed via the previous estimate. To encourage exploration during training, the entropy of the policy <img class="fm-editor-equation" src="assets/c8c82e94-b736-487e-b11b-03699f589607.png" style="width:0.92em;height:0.92em;"/> is also added to the policy update, acting as a regularization term. Then, the gradient for the policy update becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3363f84e-9978-4091-9b6b-eeb53f33e1c2.png" style="width:24.33em;height:1.42em;"/></p>
<p>The following pseudo code shows the A3C algorithm for each agent (thread):</p>
<pre>Initialize thread step counter <img class="fm-editor-equation" src="assets/52a674d2-1650-45a5-9d0f-45b94f0053d9.png" style="width:1.92em;height:0.75em;"/>;<br/>Initialize global shared parameters <img class="fm-editor-equation" src="assets/db7db621-0d89-4f5b-953a-58a40067f905.png" style="width:1.00em;height:0.92em;"/> and <img class="fm-editor-equation" src="assets/0e8a33e4-6418-4f76-939a-3f3c1715cc19.png" style="width:0.92em;height:0.83em;"/>;<br/>Repeat for each episode:<br/>    Reset gradients <img class="fm-editor-equation" src="assets/cae736f4-f31d-489e-baf4-4646dd4283eb.png" style="width:3.00em;height:0.92em;"/> and <img class="fm-editor-equation" src="assets/ac5d45ee-d3cc-4705-83d6-780164e1ecae.png" style="width:2.42em;height:0.67em;"/>;<br/>    Synchronize thread-specific parameters <img class="fm-editor-equation" src="assets/4af52763-1bca-4864-841e-9b9db6d4e67d.png" style="width:2.42em;height:0.83em;"/> and <img class="fm-editor-equation" src="assets/8522b35c-864a-4a80-9fea-cd1195b4bbca.png" style="width:2.75em;height:0.83em;"/>;<br/>    Set the start time step <img class="fm-editor-equation" src="assets/838a3645-130c-4575-823e-8505ab961bb6.png" style="width:3.42em;height:0.83em;"/>;<br/>    Receive an observation state <img class="fm-editor-equation" src="assets/bae55636-f51e-4ce1-975b-89236311e5e4.png" style="width:0.92em;height:0.92em;"/>;<br/>    While <img class="fm-editor-equation" src="assets/3178bb0b-37a0-470d-b6b8-7259fbfe7e58.png" style="width:0.83em;height:0.83em;"/> is not the terminal state and <img class="fm-editor-equation" src="assets/53735cbd-4247-4419-937e-a23d02557354.png" style="width:5.50em;height:0.75em;"/>:<br/>        Select an action <img class="fm-editor-equation" src="assets/0b00b10b-7a88-46b5-bc47-f2127dfe3862.png" style="width:1.08em;height:0.92em;"/> according to <img class="fm-editor-equation" src="assets/b0491fcd-ac57-45cc-a51b-ac2d82bbdac8.png" style="width:2.83em;height:0.92em;"/>;<br/>        Execute action <img class="fm-editor-equation" src="assets/13eed334-78ee-49bc-9e2c-e0e9de4bac50.png" style="width:1.08em;height:0.92em;"/> in the simulator and observe reward <img class="fm-editor-equation" src="assets/77fa3e90-233d-4c15-ac31-7745a9113a9a.png" style="width:0.75em;height:0.75em;"/> and the next state <img class="fm-editor-equation" src="assets/dd35408e-92b2-47ff-92b3-29a27080134c.png" style="width:1.83em;height:0.92em;"/>;<br/>        Set <img class="fm-editor-equation" src="assets/2effe4d0-0464-4aca-a857-b89766c916d6.png" style="width:3.50em;height:0.83em;"/>;<br/>    End While<br/>    Set <img class="fm-editor-equation" src="assets/9e3b5a2e-e7c3-498f-8e7c-8bb2d4e97569.png" style="width:2.25em;height:0.75em;"/> if <img class="fm-editor-equation" src="assets/cf83611c-50ad-454d-8b5d-c55140fbc2eb.png" style="width:0.83em;height:0.83em;"/> is the terminal state or <img class="fm-editor-equation" src="assets/6a2c8f2e-4abd-4f27-8958-fba8eacda26c.png" style="width:5.08em;height:1.08em;"/> otherwise; <br/>    For <img class="fm-editor-equation" src="assets/c122625b-14d7-4288-bfc7-a910c0d79873.png" style="width:7.50em;height:1.00em;"/> do<br/>        Update <img class="fm-editor-equation" src="assets/ac412057-bf6b-4d58-9d32-144b48cb4e1e.png" style="width:4.92em;height:0.92em;"/>;<br/>        Accumulate gradients wrt <img class="fm-editor-equation" src="assets/3de3891e-7a4e-4c4c-8748-394c40bf425d.png" style="width:0.83em;height:0.92em;"/>: <img class="fm-editor-equation" src="assets/de1bfbdb-eeeb-4d0c-9854-df0d976909de.png" style="width:27.58em;height:1.33em;"/>;<br/>        Accumulate gradients wrt <img class="fm-editor-equation" src="assets/68a6c3c8-0f2f-4bfd-945d-b94d2364f440.png" style="width:1.17em;height:1.25em;"/>: <img class="fm-editor-equation" src="assets/d8ead830-f327-4849-9160-5e34d211bd30.png" style="width:13.00em;height:1.33em;"/>;<br/>    End For<br/>    Perform asynchronous update of <img class="fm-editor-equation" src="assets/25185103-0b36-4ae0-9c82-878eb326d41f.png" style="width:0.75em;height:0.67em;"/> using <img class="fm-editor-equation" src="assets/8e72ea2b-85bd-48b5-8ecf-d23d5db27e90.png" style="width:1.50em;height:1.00em;"/> and of <img class="fm-editor-equation" src="assets/b62770ee-1410-4944-8326-2b5ffb406d4d.png" style="width:1.08em;height:1.00em;"/> using <img class="fm-editor-equation" src="assets/8ef742ba-47e3-4492-9b73-16b1f426849a.png" style="width:1.33em;height:0.83em;"/>.</pre>
<p>A3C uses ADAM or RMSProp to perform an asynchronous update of the parameters. For different environments, it is hard to tell which method leads to better performance. We can use RMSProp for the Atari and Minecraft environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of A3C</h1>
                </header>
            
            <article>
                
<p>We will now look at how to implement A3C using Python and TensorFlow. Here, the policy network and value network share the same feature representation. We implement two kinds of policies: one is based on the CNN architecture used in DQN, and the other is based on LSTM.</p>
<p>We implement the <kbd>FFPolicy</kbd> class for the policy based on CNN:</p>
<pre>class FFPolicy:<br/>     <br/>    def __init__(self, input_shape=(84, 84, 4), n_outputs=4, network_type='cnn'):<br/>         <br/>        self.width = input_shape[0]<br/>        self.height = input_shape[1]<br/>        self.channel = input_shape[2]<br/>        self.n_outputs = n_outputs<br/>        self.network_type = network_type<br/>        self.entropy_beta = 0.01<br/>         <br/>        self.x = tf.placeholder(dtype=tf.float32, <br/>                                shape=(None, self.channel, self.width, self.height))<br/>        self.build_model()</pre>
<p>The constructor requires three arguments:</p>
<ol>
<li> <kbd>input_shape</kbd></li>
<li><kbd>n_outputs</kbd></li>
<li><kbd>network_type</kbd></li>
</ol>
<p class="mce-root"/>
<p><kbd>input_shape</kbd> is the size of the input image. After data preprocessing, the input is an 84x84x4 image, so the default parameter is (84, 84, 4). <kbd>n_outputs</kbd> is the number of all the available actions. <kbd>network_type</kbd> indicates the type of the feature representation we want to use. Our implementation contains two different networks. One is the CNN architecture used in DQN. The other is a feedforward neural network used for testing.</p>
<ol>
<li>In the constructor, the <kbd>x</kbd> variable represents the input state (a batch of 84x84x4 images). After creating the input tensors, the <kbd>build_model</kbd> function is called to build the policy and value network. Here is the <kbd>build_model</kbd>:</li>
</ol>
<pre>    def build_model(self):<br/>         <br/>        self.net = {}<br/>        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))<br/>             <br/>        if self.network_type == 'cnn':<br/>            self.net['conv1'] = conv2d(self.net['input'], 16, kernel=(8, 8), stride=(4, 4), name='conv1')<br/>            self.net['conv2'] = conv2d(self.net['conv1'], 32, kernel=(4, 4), stride=(2, 2), name='conv2')<br/>            self.net['feature'] = linear(self.net['conv2'], 256, name='fc1')<br/>        else:<br/>            self.net['fc1'] = linear(self.net['input'], 50, init_b = tf.constant_initializer(0.0), name='fc1')<br/>            self.net['feature'] = linear(self.net['fc1'], 50, init_b = tf.constant_initializer(0.0), name='fc2')<br/>             <br/>        self.net['value'] = tf.reshape(linear(self.net['feature'], 1, activation=None, name='value',<br/>                                              init_b = tf.constant_initializer(0.0)), <br/>                                       shape=(-1,))<br/>         <br/>        self.net['logits'] = linear(self.net['feature'], self.n_outputs, activation=None, name='logits',<br/>                                    init_b = tf.constant_initializer(0.0))<br/>         <br/>        self.net['policy'] = tf.nn.softmax(self.net['logits'], name='policy')<br/>        self.net['log_policy'] = tf.nn.log_softmax(self.net['logits'], name='log_policy')<br/>         <br/>        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)</pre>
<p>The CNN architecture contains two convolutional layers and one hidden layer, while the feedforward architecture contains two hidden layers. As discussed previously, the policy network and the value network share the same feature representation.</p>
<ol start="2">
<li>The loss function for updating the network parameters can be constructed via the following function:</li>
</ol>
<pre>    def build_gradient_op(self, clip_grad=None):<br/> <br/>        self.action = tf.placeholder(dtype=tf.float32, shape=(None, self.n_outputs), name='action')<br/>        self.reward = tf.placeholder(dtype=tf.float32, shape=(None,), name='reward')<br/>        self.advantage = tf.placeholder(dtype=tf.float32, shape=(None,), name='advantage')<br/> <br/>        value = self.net['value']<br/>        policy = self.net['policy']<br/>        log_policy = self.net['log_policy']<br/>        entropy = -tf.reduce_sum(policy * log_policy, axis=1)<br/>        p_loss = -tf.reduce_sum(tf.reduce_sum(log_policy * self.action, axis=1) * self.advantage + self.entropy_beta * entropy)<br/>        v_loss = 0.5 * tf.reduce_sum((value - self.reward) ** 2)<br/>        total_loss = p_loss + v_loss<br/>         <br/>        self.gradients = tf.gradients(total_loss, self.vars)<br/>        if clip_grad is not None:<br/>            self.gradients, _ = tf.clip_by_global_norm(self.gradients, clip_grad)<br/>        <br/>        tf.summary.scalar("policy_loss", p_loss, collections=['policy_network'])<br/>        tf.summary.scalar("value_loss", v_loss, collections=['policy_network'])<br/>        tf.summary.scalar("entropy", tf.reduce_mean(entropy), collections=['policy_network'])<br/>        self.summary_op = tf.summary.merge_all('policy_network')<br/>         <br/>        return self.gradients</pre>
<ol start="3">
<li>This function creates three input tensors: 
<ol>
<li><kbd>action</kbd></li>
<li><kbd>reward</kbd></li>
<li><kbd>advantage</kbd></li>
</ol>
</li>
</ol>
<ol start="4">
<li>The <kbd>action</kbd><span> variable represe</span><span>nts the selected actions</span> <img class="fm-editor-equation" src="assets/70b47b9a-88ae-42c6-ad2c-83613ebe8d67.png" style="font-size: 1em;width:1.33em;height:1.17em;"/><span>. The</span> <kbd>reward</kbd> <span>variable </span><span>is the discounted cumulative reward </span><img class="fm-editor-equation" src="assets/a7d4f306-f0a7-4553-8f42-733a355c378a.png" style="font-size: 1em;width:1.17em;height:1.33em;"/><span>in the preceding A3C algorithm. The</span> <kbd>advantage</kbd><span> </span><span>variable </span><span>is the advantage function computed by</span> <img class="fm-editor-equation" src="assets/a2dc3329-3c39-43bd-b176-903ab799c8b7.png" style="font-size: 1em;width:5.58em;height:1.50em;"/><span><span>. In this implementation, the losses of the policy and the value function are combined together, since the feature representation layers are shared.</span></span></li>
<li><span>Therefore, instead of updating the</span> <kbd>policy</kbd> <span>parameter and the</span> <kbd>value</kbd> <span>parameter separately, our implementation updates these parameters simultaneously. This function also creates</span> <kbd>summary_op</kbd><span> for TensorBoard visualization.</span></li>
</ol>
<p>The implementation of the LSTM policy is quite similar to the feedforward policy. The main difference is the <kbd>build_model</kbd> function:</p>
<pre>    def build_model(self):<br/>         <br/>        self.net = {}<br/>        self.net['input'] = tf.transpose(self.x, perm=(0, 2, 3, 1))<br/>             <br/>        if self.network_type == 'cnn':<br/>            self.net['conv1'] = conv2d(self.net['input'], 16, kernel=(8, 8), stride=(4, 4), name='conv1')<br/>            self.net['conv2'] = conv2d(self.net['conv1'], 32, kernel=(4, 4), stride=(2, 2), name='conv2')<br/>            self.net['feature'] = linear(self.net['conv2'], 256, name='fc1')<br/>        else:<br/>            self.net['fc1'] = linear(self.net['input'], 50, init_b = tf.constant_initializer(0.0), name='fc1')<br/>            self.net['feature'] = linear(self.net['fc1'], 50, init_b = tf.constant_initializer(0.0), name='fc2')<br/>         <br/>        num_units = self.net['feature'].get_shape().as_list()[-1]<br/>        self.lstm = tf.contrib.rnn.BasicLSTMCell(num_units=num_units, forget_bias=0.0, state_is_tuple=True)<br/>        self.init_state = self.lstm.zero_state(batch_size=1, dtype=tf.float32)<br/>         <br/>        step_size = tf.shape(self.x)[:1]<br/>        feature = tf.expand_dims(self.net['feature'], axis=0)<br/>        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(self.lstm, feature, <br/>                                                     initial_state=self.init_state, <br/>                                                     sequence_length=step_size,<br/>                                                     time_major=False)<br/>        outputs = tf.reshape(lstm_outputs, shape=(-1, num_units))<br/>        self.final_state = lstm_state<br/>         <br/>        self.net['value'] = tf.reshape(linear(outputs, 1, activation=None, name='value',<br/>                                              init_b = tf.constant_initializer(0.0)), <br/>                                       shape=(-1,))<br/>         <br/>        self.net['logits'] = linear(outputs, self.n_outputs, activation=None, name='logits',<br/>                                    init_b = tf.constant_initializer(0.0))<br/>         <br/>        self.net['policy'] = tf.nn.softmax(self.net['logits'], name='policy')<br/>        self.net['log_policy'] = tf.nn.log_softmax(self.net['logits'], name='log_policy')<br/>         <br/>        self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)</pre>
<p>In this function, a LSTM layer follows the feature representation layers. In TensorFlow, you can easily create a LSTM layer by constructing <kbd>BasicLSTMCell</kbd> and then calling <kbd>tf.nn.dynamic_rnn</kbd> to get the layer outputs. <kbd>tf.nn.dynamic_rnn</kbd> returns the output for each time step and the final cell state.</p>
<p>We now implement the main A3C algorithm—the <kbd>A3C</kbd> class:</p>
<pre>class A3C:<br/>     <br/>    def __init__(self, system, directory, param, agent_index=0, callback=None):<br/>         <br/>        self.system = system<br/>        self.actions = system.get_available_actions()<br/>        self.directory = directory<br/>        self.callback = callback<br/>        self.feedback_size = system.get_feedback_size()<br/>        self.agent_index = agent_index<br/>         <br/>        self.set_params(param)<br/>        self.init_network()</pre>
<p>The <kbd>system</kbd> <span>parameter </span>is the emulator, either the Atari environment or Minecraft environment. <kbd>directory</kbd> indicates the folder for the saved model and logs. <kbd>param</kbd> includes all the training parameters of A3C, for example, the batch size and learning rate. <kbd>agent_index</kbd> is the label for one agent. The constructor calls <kbd>init_network</kbd> to initialize the policy network and the value network. Here is the implementation of <kbd>init_network</kbd>:</p>
<pre>    def init_network(self):<br/>         <br/>        input_shape = self.feedback_size + (self.num_frames,)<br/>        worker_device = "/job:worker/task:{}/cpu:0".format(self.agent_index)<br/>         <br/>        with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):<br/>            with tf.variable_scope("global"):<br/>                if self.use_lstm is False:<br/>                    self.shared_network = FFPolicy(input_shape, len(self.actions), self.network_type)<br/>                else:<br/>                    self.shared_network = LSTMPolicy(input_shape, len(self.actions), self.network_type)<br/>                     <br/>                self.global_step = tf.get_variable("global_step", shape=[], <br/>                                                   initializer=tf.constant_initializer(0, dtype=tf.int32),<br/>                                                   trainable=False, dtype=tf.int32)<br/>                self.best_score = tf.get_variable("best_score", shape=[], <br/>                                                   initializer=tf.constant_initializer(-1e2, dtype=tf.float32),<br/>                                                   trainable=False, dtype=tf.float32)<br/>                 <br/>        with tf.device(worker_device):<br/>            with tf.variable_scope('local'):<br/>                if self.use_lstm is False:<br/>                    self.network = FFPolicy(input_shape, len(self.actions), self.network_type)<br/>                else:<br/>                    self.network = LSTMPolicy(input_shape, len(self.actions), self.network_type)<br/>                # Sync params<br/>                self.update_local_ops = update_target_graph(self.shared_network.vars, self.network.vars)<br/>                # Learning rate<br/>                self.lr = tf.get_variable(name='lr', shape=[], <br/>                                          initializer=tf.constant_initializer(self.learning_rate),<br/>                                          trainable=False, dtype=tf.float32)<br/>                self.t_lr = tf.placeholder(dtype=tf.float32, shape=[], name='new_lr')<br/>                self.assign_lr_op = tf.assign(self.lr, self.t_lr)<br/>                # Best score<br/>                self.t_score = tf.placeholder(dtype=tf.float32, shape=[], name='new_score')<br/>                self.assign_best_score_op = tf.assign(self.best_score, self.t_score)<br/>                # Build gradient_op<br/>                self.increase_step = self.global_step.assign_add(1)<br/>                gradients = self.network.build_gradient_op(clip_grad=40.0)<br/>                # Additional summaries<br/>                tf.summary.scalar("learning_rate", self.lr, collections=['a3c'])<br/>                tf.summary.scalar("score", self.t_score, collections=['a3c'])<br/>                tf.summary.scalar("best_score", self.best_score, collections=['a3c'])<br/>                self.summary_op = tf.summary.merge_all('a3c')<br/>         <br/>        if self.shared_optimizer:<br/>            with tf.device(tf.train.replica_device_setter(1, worker_device=worker_device)):<br/>                with tf.variable_scope("global"):<br/>                    optimizer = create_optimizer(self.update_method, self.lr, self.rho, self.rmsprop_epsilon)<br/>                    self.train_op = optimizer.apply_gradients(zip(gradients, self.shared_network.vars))<br/>        else:<br/>            with tf.device(worker_device):<br/>                with tf.variable_scope('local'):<br/>                    optimizer = create_optimizer(self.update_method, self.lr, self.rho, self.rmsprop_epsilon)<br/>                    self.train_op = optimizer.apply_gradients(zip(gradients, self.shared_network.vars))</pre>
<p>The tricky part in this function is how to implement the global shared parameters. In TensorFlow, we can do this with the <kbd>tf.train.replica_device_setter</kbd> <span>function.</span> We first create a <kbd>global</kbd> device shared among all the agents. Within this device, the global shared network is created. Then, we create a local device and a local network for each agent. To synchronize the global and local parameters, <kbd>update_local_ops</kbd> is created by calling the <kbd>update_target_graph</kbd> function:</p>
<pre>def update_target_graph(from_vars, to_vars):<br/>    op_holder = []<br/>    for from_var, to_var in zip(from_vars, to_vars):<br/>        op_holder.append(to_var.assign(from_var))<br/>    return op_holder</pre>
<p>Then, the <kbd>gradients</kbd> op is constructed by calling <kbd>build_gradient_op</kbd>, which is used to compute the gradient update for each agent. With <kbd>gradients</kbd>, an optimizer is built via the <kbd>create_optimizer</kbd> function that is used for updating the global shared parameters. The <kbd>create_optimizer</kbd> function is used as follows:</p>
<pre>def create_optimizer(method, learning_rate, rho, epsilon):<br/>    if method == 'rmsprop':<br/>        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate, <br/>                                        decay=rho,<br/>                                        epsilon=epsilon)<br/>    elif method == 'adam':<br/>        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,<br/>                                     beta1=rho)<br/>    else:<br/>        raise<br/>    return opt</pre>
<p>The main function in A3C is <kbd>run</kbd>, which starts and trains the agent:</p>
<pre>    def run(self, sess, saver=None):<br/>         <br/>        num_of_trials = -1<br/>        for episode in range(self.num_episodes):<br/>             <br/>            self.system.reset()<br/>            cell = self.network.run_initial_state(sess)<br/>            state = self.system.get_current_feedback(self.num_frames)<br/>            state = numpy.asarray(state / self.input_scale, dtype=numpy.float32)<br/>            replay_memory = []<br/>             <br/>            for _ in range(self.T):<br/>                num_of_trials += 1<br/>                global_step = sess.run(self.increase_step)<br/>                if len(replay_memory) == 0:<br/>                    init_cell = cell<br/>                    sess.run(self.update_local_ops)<br/>                 <br/>                action, value, cell = self.choose_action(sess, state, cell)<br/>                r, new_state, termination = self.play(action)<br/>                new_state = numpy.asarray(new_state / self.input_scale, dtype=numpy.float32)<br/>                replay = (state, action, r, new_state, value, termination)<br/>                replay_memory.append(replay)<br/>                state = new_state<br/> <br/>                if len(replay_memory) == self.async_update_interval or termination:<br/>                    states, actions, rewards, advantages = self.n_step_q_learning(sess, replay_memory, cell)<br/>                    self.train(sess, states, actions, rewards, advantages, init_cell, num_of_trials)<br/>                    replay_memory = []<br/> <br/>                if global_step % 40000 == 0:<br/>                    self.save(sess, saver)<br/>                if self.callback:<br/>                    self.callback()<br/>                if termination:<br/>                    score = self.system.get_total_reward()<br/>                    summary_str = sess.run(self.summary_op, feed_dict={self.t_score: score})<br/>                    self.summary_writer.add_summary(summary_str, global_step)<br/>                    self.summary_writer.flush()<br/>                    break<br/> <br/>            if global_step - self.eval_counter &gt; self.eval_frequency:<br/>                self.evaluate(sess, n_episode=10, saver=saver)<br/>                self.eval_counter = global_step</pre>
<p>At each timestep, it calls <kbd>choose_action</kbd> to select an action according to the current policy, and executes this action by calling <kbd>play</kbd>. Then, the received reward, the new state, and the termination signal, as well as the current state and the selected action, are stored in the <kbd>replay_memory</kbd>, which records the trajectory that the agent visited. Given this trajectory, it then calls <kbd>n_step_q_learning</kbd> to estimate the cumulative reward and the <kbd>advantage</kbd> function:</p>
<pre>def n_step_q_learning(self, sess, replay_memory, cell):<br/>         <br/>        batch_size = len(replay_memory)<br/>        w, h = self.system.get_feedback_size()<br/>        states = numpy.zeros((batch_size, self.num_frames, w, h), dtype=numpy.float32)<br/>        rewards = numpy.zeros(batch_size, dtype=numpy.float32)<br/>        advantages = numpy.zeros(batch_size, dtype=numpy.float32)<br/>        actions = numpy.zeros((batch_size, len(self.actions)), dtype=numpy.float32)<br/>         <br/>        for i in reversed(range(batch_size)):<br/>            state, action, r, new_state, value, termination = replay_memory[i]<br/>            states[i] = state<br/>            actions[i][action] = 1<br/>            if termination != 0:<br/>                rewards[i] = r<br/>            else:<br/>                if i == batch_size - 1:<br/>                    rewards[i] = r + self.gamma * self.Q_value(sess, new_state, cell)<br/>                else:<br/>                    rewards[i] = r + self.gamma * rewards[i+1]<br/>            advantages[i] = rewards[i] - value<br/>         <br/>        return states, actions, rewards, advantages</pre>
<p>It then updates the global shared parameters by calling <kbd>train</kbd>:</p>
<pre>    def train(self, sess, states, actions, rewards, advantages, init_cell, iter_num):<br/>         <br/>        lr = self.anneal_lr(iter_num)<br/>        feed_dict = self.network.get_feed_dict(states, actions, rewards, advantages, init_cell)<br/>        sess.run(self.assign_lr_op, feed_dict={self.t_lr: lr})<br/>         <br/>        step = int((iter_num - self.async_update_interval + 1) / self.async_update_interval)<br/>        if self.summary_writer and step % 10 == 0:<br/>            summary_str, _, step = sess.run([self.network.summary_op, self.train_op, self.global_step], <br/>                                            feed_dict=feed_dict)<br/>            self.summary_writer.add_summary(summary_str, step)<br/>            self.summary_writer.flush()<br/>        else:<br/>            sess.run(self.train_op, feed_dict=feed_dict)</pre>
<p>Note that the model will be saved on the disk after 40,000 updates, and an evaluation procedure starts after <kbd>self.eval_frequency</kbd> updates.</p>
<p>To launch one agent, we can run the following codes written in the <kbd>worker.py</kbd> file:</p>
<pre>import numpy, time, random<br/>import argparse, os, sys, signal<br/>import tensorflow as tf<br/>from a3c import A3C<br/>from cluster import cluster_spec<br/>from environment import new_environment<br/> <br/>def set_random_seed(seed):<br/>    random.seed(seed)<br/>    numpy.random.seed(seed)<br/> <br/>def delete_dir(path):<br/>    if tf.gfile.Exists(path):<br/>        tf.gfile.DeleteRecursively(path)<br/>    tf.gfile.MakeDirs(path)<br/>    return path<br/> <br/>def shutdown(signal, frame):<br/>    print('Received signal {}: exiting'.format(signal))<br/>    sys.exit(128 + signal)<br/> <br/>def train(args, server):<br/>     <br/>    os.environ['OMP_NUM_THREADS'] = '1'<br/>    set_random_seed(args.task * 17)<br/>    log_dir = os.path.join(args.log_dir, '{}/train'.format(args.env))<br/>    if not tf.gfile.Exists(log_dir):<br/>        tf.gfile.MakeDirs(log_dir)<br/> <br/>    game, parameter = new_environment(args.env)<br/>    a3c = A3C(game, log_dir, parameter.get(), agent_index=args.task, callback=None)<br/> <br/>    global_vars = [v for v in tf.global_variables() if not v.name.startswith("local")] <br/>    ready_op = tf.report_uninitialized_variables(global_vars)<br/>    config = tf.ConfigProto(device_filters=["/job:ps", "/job:worker/task:{}/cpu:0".format(args.task)])<br/> <br/>    with tf.Session(target=server.target, config=config) as sess:<br/>        saver = tf.train.Saver()<br/>        path = os.path.join(log_dir, 'log_%d' % args.task)<br/>        writer = tf.summary.FileWriter(delete_dir(path), sess.graph_def)<br/>        a3c.set_summary_writer(writer)<br/>         <br/>        if args.task == 0:<br/>            sess.run(tf.global_variables_initializer())<br/>        else:<br/>            while len(sess.run(ready_op)) &gt; 0:<br/>                print("Waiting for task 0 initializing the global variables.")<br/>                time.sleep(1)<br/>        a3c.run(sess, saver)<br/> <br/>def main():<br/>     <br/>    parser = argparse.ArgumentParser(description=None)<br/>    parser.add_argument('-t', '--task', default=0, type=int, help='Task index')<br/>    parser.add_argument('-j', '--job_name', default="worker", type=str, help='worker or ps')<br/>    parser.add_argument('-w', '--num_workers', default=1, type=int, help='Number of workers')<br/>    parser.add_argument('-l', '--log_dir', default="save", type=str, help='Log directory path')<br/>    parser.add_argument('-e', '--env', default="demo", type=str, help='Environment')<br/>     <br/>    args = parser.parse_args()<br/>    spec = cluster_spec(args.num_workers, 1)<br/>    cluster = tf.train.ClusterSpec(spec)<br/> <br/>    signal.signal(signal.SIGHUP, shutdown)<br/>    signal.signal(signal.SIGINT, shutdown)<br/>    signal.signal(signal.SIGTERM, shutdown)<br/>     <br/>    if args.job_name == "worker":<br/>        server = tf.train.Server(cluster, <br/>                                 job_name="worker", <br/>                                 task_index=args.task,<br/>                                 config=tf.ConfigProto(intra_op_parallelism_threads=0, <br/>                                                       inter_op_parallelism_threads=0)) # Use default op_parallelism_threads<br/>        train(args, server)<br/>    else:<br/>        server = tf.train.Server(cluster, <br/>                                 job_name="ps", <br/>                                 task_index=args.task,<br/>                                 config=tf.ConfigProto(device_filters=["/job:ps"]))<br/>        # server.join()<br/>        while True:<br/>            time.sleep(1000)<br/> <br/>if __name__ == "__main__":<br/>    main()</pre>
<p>The main function will create a new agent and begin the training procedure if the <kbd>job_name</kbd> parameter is <kbd>worker</kbd>. Otherwise, it will start the TensorFlow parameter server for the global shared parameters. Notice that before launching multiple agents, we need to start the parameter server first. In the <kbd>train</kbd> function, an environment is created by calling <kbd>new_environment</kbd> and then an agent is built for this environment. After the agent is successfully created, the global shared parameters are initialized and the train procedure starts by calling <kbd>a3c.run(sess, saver)</kbd>.</p>
<p>Because manually launching 8 or 16 agents is quite inconvenient, this can be done <span>automatically</span><span> </span><span>by the following script:</span></p>
<pre>import argparse, os, sys, cluster<br/>from six.moves import shlex_quote<br/> <br/>parser = argparse.ArgumentParser(description="Run commands")<br/>parser.add_argument('-w', '--num_workers', default=1, type=int,<br/>                    help="Number of workers")<br/>parser.add_argument('-e', '--env', type=str, default="demo",<br/>                    help="Environment")<br/>parser.add_argument('-l', '--log_dir', type=str, default="save",<br/>                    help="Log directory path")<br/> <br/>def new_cmd(session, name, cmd, logdir, shell):<br/>    if isinstance(cmd, (list, tuple)):<br/>        cmd = " ".join(shlex_quote(str(v)) for v in cmd)<br/>    return name, "tmux send-keys -t {}:{} {} Enter".format(session, name, shlex_quote(cmd))<br/> <br/>def create_commands(session, num_workers, logdir, env, shell='bash'):<br/> <br/>    base_cmd = ['CUDA_VISIBLE_DEVICES=',<br/>                sys.executable, <br/>                'worker.py', <br/>                '--log_dir', logdir,<br/>                '--num_workers', str(num_workers),<br/>                '--env', env]<br/> <br/>    cmds_map = [new_cmd(session, "ps", base_cmd + ["--job_name", "ps"], logdir, shell)]<br/>    for i in range(num_workers):<br/>        cmd = base_cmd + ["--job_name", "worker", "--task", str(i)]<br/>        cmds_map.append(new_cmd(session, "w-%d" % i, cmd, logdir, shell))<br/>    cmds_map.append(new_cmd(session, "htop", ["htop"], logdir, shell))<br/>     <br/>    windows = [v[0] for v in cmds_map]<br/>    notes = ["Use `tmux attach -t {}` to watch process output".format(session),<br/>             "Use `tmux kill-session -t {}` to kill the job".format(session),<br/>             "Use `ssh -L PORT:SERVER_IP:SERVER_PORT username@server_ip` to remote Tensorboard"]<br/> <br/>    cmds = ["kill $(lsof -i:{}-{} -t) &gt; /dev/null 2&gt;&amp;1".format(cluster.PORT, num_workers+cluster.PORT),<br/>            "tmux kill-session -t {}".format(session),<br/>            "tmux new-session -s {} -n {} -d {}".format(session, windows[0], shell)]<br/>     <br/>    for w in windows[1:]:<br/>        cmds.append("tmux new-window -t {} -n {} {}".format(session, w, shell))<br/>    cmds.append("sleep 1")<br/> <br/>    for _, cmd in cmds_map:<br/>        cmds.append(cmd)<br/>    return cmds, notes<br/> <br/>def main():<br/>     <br/>    args = parser.parse_args()<br/>    cmds, notes = create_commands("a3c", args.num_workers, args.log_dir, args.env)<br/> <br/>    print("Executing the following commands:")<br/>    print("\n".join(cmds))<br/>     <br/>    os.environ["TMUX"] = ""<br/>    os.system("\n".join(cmds))<br/>     <br/>    print("Notes:")<br/>    print('\n'.join(notes))<br/>     <br/>if __name__ == "__main__":<br/>    main()</pre>
<p>This script creates the bash commands used to create the parameter server and a set of agents. To handle the consoles of all the agents, we use TMUX (more information is available at <a href="https://github.com/tmux/tmux/wiki">https://github.com/tmux/tmux/wiki</a>). TMUX is a terminal multiplexer that allows us to switch easily between several programs in one terminal, detach them, and reattach them to a different terminal. TMUX is quite a convenient tool for checking the training status of A3C. Note that since A3C runs on CPUs, we set <kbd>CUDA_VISIBLE_DEVICES</kbd> to empty.</p>
<p>A3C is much more sensitive to the training parameters than DQN. Random seed, initial weights, learning rate, batch size, discount factor, and even hyperparameters for RMSProp can affect the performance a lot. After testing it on different Atari games, we select the following hyperparameters listed in the <kbd>Parameter</kbd> class:</p>
<pre>class Parameter:<br/>     <br/>    def __init__(self, lr=7e-4, directory=None):<br/>        self.directory = directory<br/>        self.learning_rate = lr<br/>        self.gamma = 0.99<br/>        self.num_history_frames = 4<br/>        self.iteration_num = 100000<br/>        self.async_update_interval = 5<br/>        self.rho = 0.99<br/>        self.rmsprop_epsilon = 1e-1<br/>        self.update_method = 'rmsprop'<br/>        self.clip_delta = 0<br/>        self.max_iter_num = 10 ** 8<br/>        self.network_type = 'cnn'<br/>        self.input_scale = 255.0</pre>
<p>Here, <kbd>gamma</kbd> is the discount factor, <kbd>num_history_frames</kbd> is the parameter frameskip, <kbd>async_update_interval</kbd> is the batch size for the training update, and <kbd>rho</kbd> and <kbd>rmsprop_epsilon</kbd> are the internal hyperparameters for RMSProp. This set of hyperparameters can be used for both Atari and Minecraft.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experiments</h1>
                </header>
            
            <article>
                
<p>The full implementation of the A3C algorithm can be downloaded from our GitHub repository (<a href="https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects">https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects</a>). There are three environments in our implementation we can test. The first one is the special game, <kbd>demo</kbd>, introduced in <a href="0cd8e82e-dbf2-42f6-a525-e8689cace21b.xhtml" target="_blank">Chapter 3</a>, <em><span>Playing Atari Games</span></em>. For this game, A3C only needs to launch two agents to achieve good performance. Run the following command in the <kbd>src</kbd> folder:</p>
<pre><strong>python3 train.py -w 2 -e demo</strong></pre>
<p>The first argument, <kbd>-w</kbd>, or <kbd>--num_workers</kbd>, indicates the number of launched agents. The second argument, <kbd>-e</kbd>, or <kbd>--env</kbd>, specifies the environment, for example, <kbd>demo</kbd>. The other two environments are Atari and Minecraft. For Atari games, A3C requires at least 8 agents running in parallel. Typically, launching 16 agents can achieve better performance:</p>
<pre><strong>python3 train.py -w 8 -e Breakout</strong></pre>
<p>For Breakout, A3C takes about 2-3 hours to achieve a score of 300. If you have a decent PC with more than 8 cores, it is better to test it with 16 agents. To test Minecraft, run the following command:</p>
<pre><strong>python3 train.py -w 8 -e MinecraftBasic-v0</strong></pre>
<p>The Gym Minecraft environment provides more than 10 missions. To try other missions, just replace <kbd>MinecraftBasic-v0</kbd> with other mission names.</p>
<p>After running one of the preceding commands, type the following to monitor the training procedure:</p>
<pre><strong>tmux attach -t a3c</strong></pre>
<p>To switch between console windows, press <em>Ctrl </em>+ <em>b</em> and then press <em>0</em>-<em>9</em>. Window 0 is the parameter server. Windows 1-8 show the training stats of the 8 agents (if there are 8 launched agents). The last window runs htop. To detach TMUX, press <em>Ctrl</em> and then press <em>b</em>.</p>
<p>The <kbd>tensorboard</kbd> logs are saved in the <kbd>save/&lt;environment_name&gt;/train/log_&lt;agent_index&gt;</kbd> folder. To visualize the training procedure using TensorBoard, run the following command under this folder:</p>
<pre><strong>tensorboard --logdir=.</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter introduced the Gym Minecraft environment, available at <a href="https://github.com/tambetm/gym-minecraft">https://github.com/tambetm/gym-minecraft</a>. You have learned how to launch a Minecraft mission and how to implement an emulator for it. The most important part of this chapter was the asynchronous reinforcement learning framework. You learned what the shortcomings of DQN are, and why DQN is difficult to apply in complex tasks. Then, you learned how to apply the asynchronous reinforcement learning framework in the actor-critic method REINFORCE, which led us to the A3C algorithm. Finally, you learned how to implement A3C using Tensorflow and how to handle multiple terminals using TMUX. The tricky part in the implementation is that of the global shared parameters. This is related to creating a cluster of TensorFlow servers. For the readers who want to learn more about this, visit <a href="https://www.tensorflow.org/deploy/distributed">https://www.tensorflow.org/deploy/distributed</a>.</p>
<p>In the following chapters, you will learn more about how to apply reinforcement learning algorithms in other tasks, for example, the board game Go, and generating deep image classifiers. This will help you to get a deep understanding about reinforcement learning and help you solve real-world problems.</p>


            </article>

            
        </section>
    </body></html>