<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-17-black-box-optimizations-in-rl">
<h1 class="chapterNumber">17</h1>
<h1 class="chapterTitle" id="sigil_toc_id_420">
<span id="x1-31100017"/>Black-Box Optimizations in RL
    </h1>
<p>In this chapter, we will change our perspective on <span class="cmbx-10x-x-109">reinforcement learning</span> (<span class="cmbx-10x-x-109">RL</span>) training again and switch to the so-called <span class="cmbx-10x-x-109">black-box optimizations</span>. These methods are at least a decade old, but recently, several research <span id="dx1-311001"/>studies were conducted that showed their applicability to large-scale RL problems and their competitiveness with the value iteration and policy gradient methods. Despite their age, this family of methods is still more efficient in some situations. In particular, this chapter will cover two examples of black-box optimization methods:</p>
<ul>
<li>
<p>Evolution strategies</p>
</li>
<li>
<p>Genetic algorithms</p>
</li>
</ul>
<section class="level3 sectionHead" id="black-box-methods">
<h1 class="heading-1" id="sigil_toc_id_282"> <span id="x1-31200017.1"/>Black-box methods</h1>
<p>To begin with, let‚Äôs discuss the whole family of<span id="dx1-312001"/> black-box methods and how it differs from what we‚Äôve covered so far. Black-box optimization methods are the general approach to the optimization problem, when you treat the objective that you‚Äôre optimizing as a black box, without any assumptions about the differentiability, the value function, the smoothness of the objective, and so on. The only requirement that those <span id="dx1-312002"/>methods <span id="dx1-312003"/>expose is the ability to calculate the <span class="cmbx-10x-x-109">fitness function</span>, which should give us the measure of suitability of a particular instance of the optimized entity at hand. One of the simplest examples in this family is <span class="cmti-10x-x-109">random search</span>, which is when you randomly sample the thing you‚Äôre looking for (in the case of RL, it‚Äôs the policy, <span class="cmmi-10x-x-109">œÄ</span>(<span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">s</span>)), check the fitness of this candidate, and if the result is good enough (according to some reward criteria), then you‚Äôre done. Otherwise, you repeat the process again and again. Despite the simplicity and even naivety of this approach, especially when compared to the sophisticated methods that you‚Äôve seen so far, this is a good example to illustrate the idea of black-box methods.</p>
<p>Furthermore, with some modifications, as you will see shortly, this simple approach can be compared in terms of efficiency and the quality of the resulting policies to the <span class="cmbx-10x-x-109">deep Q-network </span>(<span class="cmbx-10x-x-109">DQN</span>) and policy gradient methods. In addition to that, black-box methods have several very appealing properties:</p>
<ul>
<li>
<p>They are at least two times faster than gradient-based methods, as we don‚Äôt need to perform the backpropagation step to obtain the gradients.</p>
</li>
<li>
<p>There are very few assumptions about the optimized objective and the policy that are treated as a black box. Traditional methods struggle with situations when the reward function is non-smooth or the policy contains steps with random choice. All of this is not an issue for black-box methods, as they don‚Äôt expect much from the black-box internals.</p>
</li>
<li>
<p>The methods can generally be parallelized very well. For example, the aforementioned random search can easily scale up to thousands of <span class="cmbx-10x-x-109">central processing units </span>(<span class="cmbx-10x-x-109">CPUs</span>) or <span class="cmbx-10x-x-109">graphics processing units</span> (<span class="cmbx-10x-x-109">GPUs</span>) working in parallel, without any dependency on each other. This is not the case for DQN or policy gradient methods, when you need to accumulate the gradients and propagate the current policy to all parallel workers, which decreases the parallelism.</p>
</li>
</ul>
<p>The downside of the preceding is usually lower sample efficiency. In particular, the na√Øve random search of the policy, parameterized with the <span id="dx1-312004"/><span class="cmbx-10x-x-109">neural</span> <span class="cmbx-10x-x-109">network </span>(<span class="cmbx-10x-x-109">NN</span>) with half a million parameters, has a very low probability of succeeding.</p>
</section>
<section class="level3 sectionHead" id="evolution-strategies">
<h1 class="heading-1" id="sigil_toc_id_283"> <span id="x1-31300017.2"/>Evolution strategies</h1>
<p>One subset <span id="dx1-313001"/>of black-box optimization<span id="dx1-313002"/> methods is called <span class="cmbx-10x-x-109">evolution strategies</span> (<span class="cmbx-10x-x-109">ES</span>), and it was inspired by the evolution process. With ES, the most successful individuals have the highest influence on the overall direction of the search. There are many different methods that fall into this class, and in this chapter, we will consider the approach taken by the OpenAI researchers Salimans et al. in their paper, <span class="cmti-10x-x-109">Evolution strategies as a scalable alternative to reinforcement</span> <span class="cmti-10x-x-109">learning </span>[<span id="x1-313003"/><a href="#">Sal+17</a>], published in March 2017.</p>
<p>The underlying idea of ES methods is that on every iteration, we perform random perturbation of our current policy parameters and evaluate the resulting policy fitness function. Then, we adjust the policy weights proportionally to the relative fitness function value.</p>
<p>The concrete<span id="dx1-313004"/> method used by Salimans et al. is called <span class="cmbx-10x-x-109">covariance matrix</span> <span class="cmbx-10x-x-109">adaptation evolution strategy </span>(<span class="cmbx-10x-x-109">CMA-ES</span>), in which the perturbation performed is the random noise sampled from the normal distribution with the zero mean and identity variance. Then, we calculate the fitness function of the policy with weights equal to the weights of the original policy plus the scaled noise. Next, according to the obtained value, we adjust the original policy weights by adding the noise multiplied by the fitness function value, which moves our policy toward weights with a higher value of the fitness function. To improve the stability, the update of the weights is performed by averaging the batch of such steps with different random noise.</p>
<p>More formally, this method could be expressed as the following sequence of steps:</p>
<ol>
<li>
<div id="x1-313006x1">
<p>Initialize the learning rate, <span class="cmmi-10x-x-109">Œ±</span>, the noise standard deviation, <span class="cmmi-10x-x-109">œÉ</span>, and the initial policy parameters, <span class="cmmi-10x-x-109">ùúÉ</span><sub><span class="cmr-8">0</span></sub>.</p>
</div>
</li>
<li>
<div id="x1-313008x2">
<p>For <span class="cmmi-10x-x-109">t </span>= 0<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span><span class="cmmi-10x-x-109">‚Ä¶</span> perform:</p>
<ol>
<li>
<div id="x1-313010x1">
<p>The sample batch of noise with the shape of the weights from the normal distribution with zero mean and variance of one: <span class="cmmi-10x-x-109">ùúñ</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">,ùúñ</span><sub><span class="cmmi-8">n</span></sub> <span class="cmsy-10x-x-109">‚àºùí©</span>(0<span class="cmmi-10x-x-109">,I</span>)</p>
</div>
</li>
<li>
<div id="x1-313012x2">
<p>Compute returns <span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">ùúÉ</span><sub><span class="cmmi-8">t</span></sub> + <span class="cmmi-10x-x-109">œÉùúñ</span><sub><span class="cmmi-8">i</span></sub>) for <span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">,</span><span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">,n</span></p>
</div>
</li>
<li>
<div id="x1-313014x3">
<p>Update weights:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="64" src="../Images/eq69.png" width="253"/>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
<p>This algorithm is the core of the method presented in <span id="dx1-313015"/>the paper, but, as usual in the RL domain, the method alone is not enough to obtain good <span id="dx1-313016"/>results. So, the paper includes several tweaks to improve the method, although the core is the same.</p>
<section class="level4 subsectionHead" id="implementing-es-on-cartpole">
<h2 class="heading-2" id="sigil_toc_id_284"> <span id="x1-31400017.2.1"/>Implementing ES on CartPole</h2>
<p>Let‚Äôs implement<span id="dx1-314001"/> and test the method <span id="dx1-314002"/>from the paper on our fruit fly environment: CartPole. You‚Äôll find the complete example in <span class="cmtt-10x-x-109">Chapter17/01</span><span class="cmtt-10x-x-109">_cartpole</span><span class="cmtt-10x-x-109">_es.py</span>.</p>
<p>In this example, we will use the single environment to check the fitness of the perturbed network weights. Our fitness function will be the undiscounted total reward for the episode.</p>
<p>We start with the imports:</p>
<div class="tcolorbox" id="tcolobox-376">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-446"><code>import gymnasium as gym 
import time 
import numpy as np 
import typing as tt 
 
import torch 
import torch.nn as nn 
 
from torch.utils.tensorboard.writer import SummaryWriter</code></pre>
</div>
</div>
<p>From the <span class="cmtt-10x-x-109">import </span>statements, you will notice how self-contained our example is. We‚Äôre not using PyTorch optimizers, as we don‚Äôt perform backpropagation at all. In fact, we could avoid using PyTorch completely and work only with NumPy, as the only thing we use PyTorch for is to perform a forward pass and calculate the network‚Äôs output.</p>
<p>Next, we define the hyperparameters:</p>
<div class="tcolorbox" id="tcolobox-377">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-447"><code>MAX_BATCH_EPISODES = 100 
MAX_BATCH_STEPS = 10000 
NOISE_STD = 0.001 
LEARNING_RATE = 0.001 
 
TNoise = tt.List[torch.Tensor]</code></pre>
</div>
</div>
<p>The number of hyperparameters is also small and includes the following values:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">MAX</span><span class="cmtt-10x-x-109">_BATCH</span><span class="cmtt-10x-x-109">_EPISODES </span>and <span class="cmtt-10x-x-109">MAX</span><span class="cmtt-10x-x-109">_BATCH</span><span class="cmtt-10x-x-109">_STEPS</span>: The limit of episodes and steps we use for training</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">NOISE</span><span class="cmtt-10x-x-109">_STD</span>: The standard deviation, <span class="cmmi-10x-x-109">œÉ</span>, of the noise used for weight perturbation</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">LEARNING</span><span class="cmtt-10x-x-109">_RATE</span>: The coefficient used to adjust the weights on the training step</p>
</li>
</ul>
<p>We alse define a type alias for list of tensors <span id="dx1-314018"/>containing weights‚Äô noises. It <span id="dx1-314019"/>will simplify the code, as we‚Äôll deal with noise a lot.</p>
<p>Now let‚Äôs check the network:</p>
<div class="tcolorbox" id="tcolobox-378">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-448"><code>class Net(nn.Module): 
    def __init__(self, obs_size: int, action_size: int): 
        super(Net, self).__init__() 
        self.net = nn.Sequential( 
            nn.Linear(obs_size, 32), 
            nn.ReLU(), 
            nn.Linear(32, action_size), 
            nn.Softmax(dim=1) 
        ) 
 
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: 
        return self.net(x)</code></pre>
</div>
</div>
<p>The model we‚Äôre using is a simple one-hidden-layer NN, which gives us the action to take from the observation. We‚Äôre using PyTorch NN machinery here only for convenience, as we need only the forward pass, but it could be replaced by the multiplication of matrices and nonlinearities application.</p>
<p>The <span class="cmtt-10x-x-109">evaluate() </span>function plays a full episode using the given policy and returns the total reward and the number of steps:</p>
<div class="tcolorbox" id="tcolobox-379">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-449"><code>def evaluate(env: gym.Env, net: Net) -&gt; tt.Tuple[float, int]: 
    obs, _ = env.reset() 
    reward = 0.0 
    steps = 0 
    while True: 
        obs_v = torch.FloatTensor(np.expand_dims(obs, 0)) 
        act_prob = net(obs_v) 
        acts = act_prob.max(dim=1)[1] 
        obs, r, done, is_tr, _ = env.step(acts.data.numpy()[0]) 
        reward += r 
        steps += 1 
        if done or is_tr: 
            break 
    return reward, steps</code></pre>
</div>
</div>
<p>The reward will be used as a fitness value, while the count of steps is needed to limit the amount of time we spend on forming the batch. The action selection is performed deterministically by calculating argmax from the network output. In principle, we could do the random sampling <span id="dx1-314046"/>from the distribution, but we‚Äôve already performed the <span id="dx1-314047"/>exploration by adding noise to the network parameters, so the deterministic action selection is fine here.</p>
<p>In the <span class="cmtt-10x-x-109">sample</span><span class="cmtt-10x-x-109">_noise() </span>function, we create random noise with zero mean and unit variance equal to the shape of our network parameters:</p>
<div class="tcolorbox" id="tcolobox-380">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-450"><code>def sample_noise(net: Net) -&gt; tt.Tuple[TNoise, TNoise]: 
    pos = [] 
    neg = [] 
    for p in net.parameters(): 
        noise = np.random.normal(size=p.data.size()) 
        noise_t = torch.FloatTensor(noise) 
        pos.append(noise_t) 
        neg.append(-noise_t) 
    return pos, neg</code></pre>
</div>
</div>
<p>The function returns two sets of noise tensors: one with positive noise and another with the same random values taken with a negative sign. These two samples are later used in a batch as independent samples. This technique is known as <span class="cmti-10x-x-109">mirrored sampling </span>and is<span id="dx1-314057"/> used to improve the stability of the convergence. In fact, without the negative noise, the convergence becomes very unstable because positive noise pushes weights in a single direction.</p>
<p>The <span class="cmtt-10x-x-109">eval</span><span class="cmtt-10x-x-109">_with</span><span class="cmtt-10x-x-109">_noise() </span>function takes the noise array created by <span class="cmtt-10x-x-109">sample</span><span class="cmtt-10x-x-109">_noise()</span> and evaluates the network with noise added:</p>
<div class="tcolorbox" id="tcolobox-381">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-451"><code>def eval_with_noise(env: gym.Env, net: nn.Module, noise: TNoise, noise_std: float, 
        get_max_action: bool = True, device: torch.device = torch.device("cpu") 
    old_params = net.state_dict() 
    for p, p_n in zip(net.parameters(), noise): 
        p.data += NOISE_STD * p_n 
    r, s = evaluate(env, net) 
    net.load_state_dict(old_params) 
    return r, s</code></pre>
</div>
</div>
<p>To achieve this, we add the noise to the network‚Äôs parameters and call the <span class="cmtt-10x-x-109">evaluate </span>function to obtain the reward and number of steps taken. After this, we need to restore the network weights to their original state, which is completed by loading the state dictionary of the network.</p>
<p>The last and the central function of the method is <span class="cmtt-10x-x-109">train</span><span class="cmtt-10x-x-109">_step()</span>, which takes the batch with noise and respective rewards and calculates the update to the network<span id="dx1-314066"/> parameters by applying<span id="dx1-314067"/> the formula:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="64" src="../Images/eq69.png" width="253"/>
</div>
<p>This can be implemented as follows:</p>
<div class="tcolorbox" id="tcolobox-382">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-452"><code>def train_step(net: Net, batch_noise: tt.List[common.TNoise], batch_reward: tt.List[float], 
               writer: SummaryWriter, step_idx: int): 
    weighted_noise = None 
    norm_reward = np.array(batch_reward) 
    norm_reward -= np.mean(norm_reward) 
    s = np.std(norm_reward) 
    if abs(s) &gt; 1e-6: 
        norm_reward /= s</code></pre>
</div>
</div>
<p>In the beginning, we normalize rewards to have zero mean and unit variance, which improves the stability of the method.</p>
<p>Then, we iterate every pair (noise, reward) in our batch and multiply the noise values with the normalized reward, summing together the respective noise for every parameter in our policy:</p>
<div class="tcolorbox" id="tcolobox-383">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-453"><code>    for noise, reward in zip(batch_noise, norm_reward): 
        if weighted_noise is None: 
            weighted_noise = [reward * p_n for p_n in noise] 
        else: 
            for w_n, p_n in zip(weighted_noise, noise): 
                w_n += reward * p_n</code></pre>
</div>
</div>
<p>As a final step, we use the accumulated scaled noise to adjust the network parameters:</p>
<div class="tcolorbox" id="tcolobox-384">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-454"><code>    m_updates = [] 
    for p, p_update in zip(net.parameters(), weighted_noise): 
        update = p_update / (len(batch_reward) * NOISE_STD) 
        p.data += LEARNING_RATE * update 
        m_updates.append(torch.norm(update)) 
    writer.add_scalar("update_l2", np.mean(m_updates), step_idx)</code></pre>
</div>
</div>
<p>Technically, what we do here is a gradient ascent, although the gradient was not obtained from backpropagation but from the random <span id="dx1-314088"/>sampling (also known as Monte Carlo sampling). This fact was also demonstrated by Salimans et al., where the authors showed that CMA-ES is very similar to the policy gradient methods, differing in just the way that we get the gradients‚Äô estimation.</p>
<p>The <span id="dx1-314089"/>preparation before the<span id="dx1-314090"/> training loop is simple; we create the environment and the network:</p>
<div class="tcolorbox" id="tcolobox-385">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-455"><code>if __name__ == "__main__": 
    writer = SummaryWriter(comment="-cartpole-es") 
    env = gym.make("CartPole-v1") 
 
    net = Net(env.observation_space.shape[0], env.action_space.n) 
    print(net)</code></pre>
</div>
</div>
<p>Every iteration of the training loop starts with batch creation, where we sample the noise and obtain rewards for both positive and negated noise:</p>
<div class="tcolorbox" id="tcolobox-386">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-456"><code>    step_idx = 0 
    while True: 
        t_start = time.time() 
        batch_noise = [] 
        batch_reward = [] 
        batch_steps = 0 
        for _ in range(MAX_BATCH_EPISODES): 
            noise, neg_noise = sample_noise(net) 
            batch_noise.append(noise) 
            batch_noise.append(neg_noise) 
            reward, steps = eval_with_noise(env, net, noise) 
            batch_reward.append(reward) 
            batch_steps += steps 
            reward, steps = eval_with_noise(env, net, neg_noise) 
            batch_reward.append(reward) 
            batch_steps += steps 
            if batch_steps &gt; MAX_BATCH_STEPS: 
                break</code></pre>
</div>
</div>
<p>When we reach the limit of episodes in the batch, or the limit of the total steps, we stop gathering the data and do a training update.</p>
<p>To perform the update of the network, we call the <span class="cmtt-10x-x-109">train</span><span class="cmtt-10x-x-109">_step() </span>function that we‚Äôve already seen:</p>
<div class="tcolorbox" id="tcolobox-387">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-457"><code>        step_idx += 1 
        m_reward = float(np.mean(batch_reward)) 
        if m_reward &gt; 199: 
            print("Solved in %d steps" % step_idx) 
            break 
 
        train_step(net, batch_noise, batch_reward, writer, step_idx)</code></pre>
</div>
</div>
<p>The goal of the <span class="cmtt-10x-x-109">train</span><span class="cmtt-10x-x-109">_step() </span>function is to scale the noise according to the total reward and then adjust the <span id="dx1-314122"/>policy weights in <span id="dx1-314123"/>the direction of the averaged noise.</p>
<p>The final steps in the training loop write metrics into TensorBoard and show the training progress on the console:</p>
<div class="tcolorbox" id="tcolobox-388">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-458"><code>        writer.add_scalar("reward_mean", m_reward, step_idx) 
        writer.add_scalar("reward_std", np.std(batch_reward), step_idx) 
        writer.add_scalar("reward_max", np.max(batch_reward), step_idx) 
        writer.add_scalar("batch_episodes", len(batch_reward), step_idx) 
        writer.add_scalar("batch_steps", batch_steps, step_idx) 
        speed = batch_steps / (time.time() - t_start) 
        writer.add_scalar("speed", speed, step_idx) 
        print("%d: reward=%.2f, speed=%.2f f/s" % ( 
            step_idx, m_reward, speed))</code></pre>
</div>
</div>
</section>
<section class="level4 subsectionHead" id="cartpole-results">
<h2 class="heading-2" id="sigil_toc_id_285"> <span id="x1-31500017.2.2"/>CartPole results</h2>
<p>Training<span id="dx1-315001"/> can be<span id="dx1-315002"/> started by just running the program without the arguments:</p>
<pre class="lstlisting" id="listing-459"><code>Chapter17$ ./01_cartpole_es.py 
Net( 
¬†¬†(net): Sequential( 
¬†¬†¬†(0): Linear(in_features=4, out_features=32, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=32, out_features=2, bias=True) 
¬†¬†¬†(3): Softmax(dim=1) 
¬†¬†) 
) 
1: reward=10.00, speed=7458.03 f/s 
2: reward=11.93, speed=8454.54 f/s 
3: reward=13.71, speed=8677.55 f/s 
4: reward=15.96, speed=8905.25 f/s 
5: reward=18.75, speed=9098.71 f/s 
6: reward=22.08, speed=9220.68 f/s 
7: reward=23.57, speed=9272.45 f/s 
...</code></pre>
<p>From my experiments, it usually takes ES about 40‚Äì60 batches to solve CartPole. The convergence dynamics for the preceding run are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-315020r1"><span class="cmti-10x-x-109">17.1</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-315021r2"><span class="cmti-10x-x-109">17.2</span></a>.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_01.png" width="600"/> <span id="x1-315020r1"/></p>
<span class="id">Figure¬†17.1: The maximum reward (left) and policy update (right) for ES on CartPole </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_02.png" width="600"/> <span id="x1-315021r2"/></p>
<span class="id">Figure¬†17.2: The mean (left) and standard deviation (right) of reward for ES on CartPole </span>
</div>
<p>The preceding graphs looks quite good‚Äîbeing <span id="dx1-315022"/>able to <span id="dx1-315023"/>solve the environment in 30 seconds is on par with the cross-entropy method from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch008.xhtml#x1-740004"><span class="cmti-10x-x-109">4</span></a>.</p>
</section>
<section class="level4 subsectionHead" id="es-on-halfcheetah">
<h2 class="heading-2" id="sigil_toc_id_286"> <span id="x1-31600017.2.3"/>ES on HalfCheetah</h2>
<p>In the next <span id="dx1-316001"/>example, we will go <span id="dx1-316002"/>beyond the simplest ES implementation and look at how this method can be parallelized efficiently using the shared seed strategy proposed by Salimans et al. To show this approach, we will use the HalfCheetah environment using the MuJoCo physics simulator. We already experimented with it in the previous chapter, so if you haven‚Äôt installed the <span class="cmtt-10x-x-109">gymnasium[mujoco] </span>package, you should do so.</p>
<p>First, let‚Äôs discuss the idea of shared seeds. The performance of the ES algorithm is mostly determined by the speed at which we can gather our training batch, which consists of sampling the noise and checking the total reward of the perturbed noise. As our training batch items are independent, we can easily parallelize this step to a large number of workers sitting on remote machines. (That‚Äôs a bit similar to the example from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch016.xhtml#x1-20300012"><span class="cmti-10x-x-109">12</span></a>, when we gathered gradients from A3C workers.) However, na√Øve implementation of this parallelization will require a large amount of data to be transferred from the worker process to the central master, which is supposed to combine the noise checked by the workers and perform the policy update. Most of this data is the noise vectors, the size of which is equal to the size of our policy parameters.</p>
<p>To avoid this overhead, a quite elegant solution was proposed by Salimans et al. As noise sampled on a worker is produced by a pseudo-random number generator, which allows us to set the random seed and reproduce the random sequence generated, the worker can transfer to the master only the seed that was used to generate the noise. Then, the master can generate the same noise vector again using the seed. Of course, the seed on every worker needs to be generated randomly to still have a random optimization process. This has the effect of dramatically decreasing the amount of data that needs to be transferred from workers to the master, improving the scalability of the method. For example, Salimans et al. reported linear speed up in optimizations involving 1,440 CPUs in the cloud. In our example, we will look at local parallelization using the same approach.</p>
</section>
<section class="level4 subsectionHead" id="implementing-es-on-halfcheetah">
<h2 class="heading-2" id="sigil_toc_id_287"> <span id="x1-31700017.2.4"/>Implementing ES on HalfCheetah</h2>
<p>The code is placed in <span class="cmtt-10x-x-109">Chapter17/02</span><span class="cmtt-10x-x-109">_cheetah</span><span class="cmtt-10x-x-109">_es.py</span>. As the code significantly overlaps with the CartPole version, we will focus here only on the differences.</p>
<p>We will begin with the worker, which is started as a separate process using the PyTorch multiprocessing wrapper. The worker‚Äôs responsibilities are simple: for every iteration, it obtains the network parameters from the master process, and then it performs the fixed number of iterations, where it samples the noise and evaluates the reward. The result with the random seed is sent to the master using the queue.</p>
<p>The following dataclass is used by the worker to <span id="dx1-317001"/>send the results of<span id="dx1-317002"/> the perturbed policy evaluation to the master process:</p>
<div class="tcolorbox" id="tcolobox-389">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-460"><code>@dataclass(frozen=True) 
class RewardsItem: 
    seed: int 
    pos_reward: float 
    neg_reward: float 
    steps: int</code></pre>
</div>
</div>
<p>It includes the random seed, the rewards obtained with the positive and negative noise, and the total number of steps we performed in both tests.</p>
<p>On every training iteration, the worker waits for the network parameters to be broadcasted from the master:</p>
<div class="tcolorbox" id="tcolobox-390">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-461"><code>def worker_func(params_queue: mp.Queue, rewards_queue: mp.Queue, 
                device: torch.device, noise_std: float): 
    env = make_env() 
    net = Net(env.observation_space.shape[0], env.action_space.shape[0]).to(device) 
    net.eval() 
 
    while True: 
        params = params_queue.get() 
        if params is None: 
            break 
        net.load_state_dict(params)</code></pre>
</div>
</div>
<p>The value of <span class="cmtt-10x-x-109">None </span>means that the master wants to stop the worker.</p>
<p>The rest is almost the same as the previous example, with the main difference being in the random seed generated and assigned before the noise generation. This allows the master to regenerate the same noise, only from the seed:</p>
<div class="tcolorbox" id="tcolobox-391">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-462"><code>        for _ in range(ITERS_PER_UPDATE): 
            seed = np.random.randint(low=0, high=65535) 
            np.random.seed(seed) 
            noise, neg_noise = common.sample_noise(net, device=device) 
            pos_reward, pos_steps = common.eval_with_noise(env, net, noise, noise_std, 
                get_max_action=False, device=device) 
            neg_reward, neg_steps = common.eval_with_noise(env, net, neg_noise, noise_std, 
                get_max_action=False, device=device) 
            rewards_queue.put(RewardsItem(seed=seed, pos_reward=pos_reward, 
                neg_reward=neg_reward, steps=pos_steps+neg_steps))</code></pre>
</div>
</div>
<p>Another difference lies in the function used by the master to perform the training step:</p>
<div class="tcolorbox" id="tcolobox-392">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-463"><code>def train_step(optimizer: optim.Optimizer, net: Net, batch_noise: tt.List[common.TNoise], 
               batch_reward: tt.List[float], writer: SummaryWriter, step_idx: int, 
               noise_std: float): 
    weighted_noise = None 
    norm_reward = compute_centered_ranks(np.array(batch_reward))</code></pre>
</div>
</div>
<p>In the CartPole example, we normalized the batch of rewards by subtracting <span id="dx1-317035"/>the mean and dividing<span id="dx1-317036"/> by the standard deviation. According to Salimans et al., better results could be obtained using ranks instead of actual rewards. As ES has no assumptions about the fitness function (which is a reward in our case), we can make any rearrangements in the reward that we want, which wasn‚Äôt possible in the case of DQN, for example.</p>
<p>Here, <span class="cmbx-10x-x-109">rank transformation </span>of the array means replacing the array with indices of the sorted array. For example, array <span class="cmtt-10x-x-109">[0.1, 10, 0.5] </span>will have the rank array <span class="cmtt-10x-x-109">[0, 2, 1]</span>. The <span class="cmtt-10x-x-109">compute</span><span class="cmtt-10x-x-109">_centered</span><span class="cmtt-10x-x-109">_ranks </span>function takes the array with the total rewards of the batch, calculates the rank for every item in the array, and then normalizes those ranks. For example, an input array of <span class="cmtt-10x-x-109">[21.0, 5.8, 7.0]</span> will have ranks <span class="cmtt-10x-x-109">[2, 0, 1]</span>, and the final centered ranks will be <span class="cmtt-10x-x-109">[0.5, -0.5,</span> <span class="cmtt-10x-x-109">0.0]</span>.</p>
<p>Another major difference in the training function is the use of PyTorch optimizers:</p>
<div class="tcolorbox" id="tcolobox-393">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-464"><code>    for noise, reward in zip(batch_noise, norm_reward): 
        if weighted_noise is None: 
            weighted_noise = [reward * p_n for p_n in noise] 
        else: 
            for w_n, p_n in zip(weighted_noise, noise): 
                w_n += reward * p_n 
    m_updates = [] 
    optimizer.zero_grad() 
    for p, p_update in zip(net.parameters(), weighted_noise): 
        update = p_update / (len(batch_reward) * noise_std) 
        p.grad = -update 
        m_updates.append(torch.norm(update)) 
    writer.add_scalar("update_l2", np.mean(m_updates), step_idx) 
    optimizer.step()</code></pre>
</div>
</div>
<p>To understand why they are used and how this is possible without doing backpropagation, some explanations are required.</p>
<p>First, Salimans et al. showed that the optimization method used by the ES algorithm is very similar to gradient ascent on the fitness function, with the difference being how the gradient is calculated. The way the <span class="cmbx-10x-x-109">stochastic</span> <span class="cmbx-10x-x-109">gradient descent </span>(<span class="cmbx-10x-x-109">SGD</span>) method is usually applied is that the gradient is obtained from the loss function by calculating the derivative of the network parameters with respect to the loss value. This imposes the limitation on the network and loss function to be differentiable, which is not always the case; for example, the rank transformation performed by the ES method is not differentiable.</p>
<p>On the other hand, optimization performed by ES works differently. We randomly <span id="dx1-317051"/>sample the neighborhood of<span id="dx1-317052"/> our current parameters by adding the noise to them and calculating the fitness function. According to the fitness function change, we adjust the parameters, which pushes our parameters in the direction of a higher fitness function. The result of this is very similar to gradient-based methods, but the requirements imposed on our fitness function are much looser: the only requirement is our ability to calculate it.</p>
<p>However, if we‚Äôre estimating some kind of gradient by randomly sampling the fitness function, we can use standard optimizers from PyTorch. Normally, optimizers adjust the parameters of the network using gradients accumulated in the parameters‚Äô <span class="cmtt-10x-x-109">grad </span>fields.</p>
<p>Those gradients are accumulated after the backpropagation step, but due to PyTorch‚Äôs flexibility, the optimizer doesn‚Äôt care about the source of the gradients. So, the only thing we need to do is copy the estimated parameters‚Äô update in the <span class="cmtt-10x-x-109">grad </span>fields and ask the optimizer to update them. Note that the update is copied with a negative sign, as optimizers normally perform gradient descent (as in a normal operation, we <span class="cmti-10x-x-109">minimize </span>the loss function), but in this case, we want to do gradient ascent. This is very similar to the actor-critic method we covered in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch016.xhtml#x1-20300012"><span class="cmti-10x-x-109">12</span></a>, when the estimated policy gradient is taken with the negative sign, as it shows the direction to <span class="cmti-10x-x-109">improve </span>the policy. The last chunk of differences in the code is taken from the training loop performed by the master process. Its responsibility is to wait for data from worker processes, perform the training update of the parameters, and broadcast the result to the workers. The communication between the master and workers is performed by two sets of queues. The first queue is per-worker and is used by the master to send the current<span id="dx1-317053"/> policy parameters <span id="dx1-317054"/>to use. The second queue is shared by the workers and is used to send the already mentioned <span class="cmtt-10x-x-109">RewardItem </span>structure with the random seed and rewards:</p>
<div class="tcolorbox" id="tcolobox-394">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-465"><code>    params_queues = [mp.Queue(maxsize=1) for _ in range(PROCESSES_COUNT)] 
    rewards_queue = mp.Queue(maxsize=ITERS_PER_UPDATE) 
    workers = [] 
 
    for params_queue in params_queues: 
        p_args = (params_queue, rewards_queue, device, args.noise_std) 
        proc = mp.Process(target=worker_func, args=p_args) 
        proc.start() 
        workers.append(proc) 
 
    print("All started!") 
    optimizer = optim.Adam(net.parameters(), lr=args.lr)</code></pre>
</div>
</div>
<p>At the beginning of the master, we create all those queues, start the worker processes, and create the optimizer.</p>
<p>Every training iteration starts with the network parameters being broadcast to the workers:</p>
<div class="tcolorbox" id="tcolobox-395">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-466"><code>    for step_idx in range(args.iters): 
        params = net.state_dict() 
        for q in params_queues: 
            q.put(params)</code></pre>
</div>
</div>
<p>Then, in the loop, the master waits for enough data to be obtained from the workers:</p>
<div class="tcolorbox" id="tcolobox-396">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-467"><code>        t_start = time.time() 
        batch_noise = [] 
        batch_reward = [] 
        results = 0 
        batch_steps = 0 
        while True: 
            while not rewards_queue.empty(): 
                reward = rewards_queue.get_nowait() 
                np.random.seed(reward.seed) 
                noise, neg_noise = common.sample_noise(net) 
                batch_noise.append(noise) 
                batch_reward.append(reward.pos_reward) 
                batch_noise.append(neg_noise) 
                batch_reward.append(reward.neg_reward) 
                results += 1 
                batch_steps += reward.steps 
 
            if results == PROCESSES_COUNT * ITERS_PER_UPDATE: 
                break 
            time.sleep(0.01)</code></pre>
</div>
</div>
<p>Every time a new result arrives, we reproduce the noise using the random seed.</p>
<p>As the last step in the training loop, we call the <span class="cmtt-10x-x-109">train</span><span class="cmtt-10x-x-109">_step() </span>function:</p>
<div class="tcolorbox" id="tcolobox-397">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-468"><code>        train_step(optimizer, net, batch_noise, batch_reward, 
                   writer, step_idx, args.noise_std)</code></pre>
</div>
</div>
<p>You‚Äôve already seen this function, which calculates <span id="dx1-317093"/>the update from the<span id="dx1-317094"/> noise and rewards, and calls the optimizer to adjust the weights.</p>
</section>
<section class="level4 subsectionHead" id="halfcheetah-results">
<h2 class="heading-2" id="sigil_toc_id_288"> <span id="x1-31800017.2.5"/>HalfCheetah results</h2>
<p>The code<span id="dx1-318001"/> supports<span id="dx1-318002"/> the optional <span class="cmtt-10x-x-109">--dev </span>flag, but from my experiments, I got a <span class="cmti-10x-x-109">slowdown </span>if GPU was enabled: without GPU, the average speed was 20-21k observations per second, but with CUDA, it was just 9k. This might look counter-intuitive, but we can explain this with the very small network and batch size of a single observation. Potentially, we might decrease the gap (or even get some speedup) with a higher batch size, but it will complicate our code.</p>
<p>During the training, we show the mean reward, the speed of training (in observations per second), and two timing values (showing how long it took to gather data and perform the training step):</p>
<pre class="lstlisting" id="listing-469"><code>$ ./02_cheetah_es.py 
Net( 
¬†¬†(mu): Sequential( 
¬†¬†¬†(0): Linear(in_features=17, out_features=64, bias=True) 
¬†¬†¬†(1): Tanh() 
¬†¬†¬†(2): Linear(in_features=64, out_features=6, bias=True) 
¬†¬†¬†(3): Tanh() 
¬†¬†) 
) 
All started! 
0: reward=-505.09, speed=17621.60 f/s, data_gather=6.792, train=0.018 
1: reward=-440.50, speed=20609.56 f/s, data_gather=5.815, train=0.007 
2: reward=-383.76, speed=20568.74 f/s, data_gather=5.827, train=0.007 
3: reward=-326.02, speed=20413.63 f/s, data_gather=5.871, train=0.007 
4: reward=-259.58, speed=20181.74 f/s, data_gather=5.939, train=0.007 
5: reward=-198.80, speed=20496.81 f/s, data_gather=5.848, train=0.007 
6: reward=-113.22, speed=20467.71 f/s, data_gather=5.856, train=0.007</code></pre>
<p>The dynamics of the training show very quick policy improvement in the beginning: in just 100 updates, which is 9 minutes of training, the agent was able to reach the score of 1,500-1,600. After 30 minutes, the peak reward was 2,833; but with more training, the <span id="dx1-318020"/>policy<span id="dx1-318021"/> was degrading.</p>
<p>The maximum, mean, and standard deviation of reward are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-318022r3"><span class="cmti-10x-x-109">17.3</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-318023r4"><span class="cmti-10x-x-109">17.4</span></a>.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_03.png" width="600"/> <span id="x1-318022r3"/></p>
<span class="id">Figure¬†17.3: The maximum reward (left) and policy update (right) for ES on HalfCheetah </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_04.png" width="600"/> <span id="x1-318023r4"/></p>
<span class="id">Figure¬†17.4: The mean (left) and standard deviation (right) of reward for ES on HalfCheetah </span>
</div>
</section>
</section>
<section class="level3 sectionHead" id="genetic-algorithms">
<h1 class="heading-1" id="sigil_toc_id_289"> <span id="x1-31900017.3"/>Genetic algorithms</h1>
<p>Another popular <span id="dx1-319001"/>class <span id="dx1-319002"/>of black-box methods is <span class="cmbx-10x-x-109">genetic algorithms </span>(<span class="cmbx-10x-x-109">GAs</span>). It is a large family of optimization methods with more than two decades of history behind it and a simple core idea of generating a population of <span class="cmmi-10x-x-109">N</span> individuals (concrete model parameters), each of which is evaluated with the fitness function. Then, some subset of top performers is used to produce the next generation of the population (this process is called <span class="cmti-10x-x-109">mutation</span>). This process is repeated until we‚Äôre satisfied with the performance of our population.</p>
<p>There are a lot of different methods in the GA family, for example, how to perform the mutation of the individuals for the next generation or how to rank the performers. Here, we will consider the simple GA method with some extensions, published in the paper by Such et al., called <span class="cmti-10x-x-109">Deep neuroevolution:</span> <span class="cmti-10x-x-109">Genetic algorithms are a competitive alternative for training deep neural networks</span> <span class="cmti-10x-x-109">for reinforcement learning </span>[<span id="x1-319003"/><a href="#">Suc+17</a>].</p>
<p>In this paper, the authors analyzed the simple GA method, which performs Gaussian noise perturbation of the parent‚Äôs weights to perform mutation. On every iteration, the top performer was copied without modification. In an algorithm form, the steps of a simple GA method can be written as follows:</p>
<ol>
<li>
<div id="x1-319005x1">
<p>Initialize the mutation power, <span class="cmmi-10x-x-109">œÉ</span>, the population size, <span class="cmmi-10x-x-109">N</span>, the number of selected individuals, <span class="cmmi-10x-x-109">T</span>, and the initial population, <span class="cmmi-10x-x-109">P</span><sup><span class="cmr-8">0</span></sup>, with <span class="cmmi-10x-x-109">N</span> randomly initialized policies and their fitness: <span class="cmmi-10x-x-109">F</span><sup><span class="cmr-8">0</span></sup> = <span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmr-8">0</span></sup>)<span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">i </span>= 1<span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">N</span><span class="cmsy-10x-x-109">}</span></p>
</div>
</li>
<li>
<div id="x1-319007x2">
<p>For generation <span class="cmmi-10x-x-109">g </span>= 1<span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">G</span>:</p>
<ol>
<li>
<div id="x1-319009x1">
<p>Sort generation <span class="cmmi-10x-x-109">P</span><sup><span class="cmmi-8">n</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> in the descending order of the fitness function value <span class="cmmi-10x-x-109">F</span><sup><span class="cmmi-8">g</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup></p>
</div>
</li>
<li>
<div id="x1-319011x2">
<p>Copy elite <span class="cmmi-10x-x-109">P</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">g</span></sup> = <span class="cmmi-10x-x-109">P</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">g</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">,F</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">g</span></sup> = <span class="cmmi-10x-x-109">F</span><sub><span class="cmr-8">1</span></sub><sup><span class="cmmi-8">g</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup></p>
</div>
</li>
<li>
<div id="x1-319013x3">
<p>For individual <span class="cmmi-10x-x-109">i </span>= 2<span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">N</span>:</p>
<ol>
<li>
<div id="x1-319015x1">
<p>Choose the <span class="cmmi-10x-x-109">k</span>: random parent from 1<span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">T</span></p>
</div>
</li>
<li>
<div id="x1-319017x2">
<p>Sample <span class="cmmi-10x-x-109">ùúñ </span><span class="cmsy-10x-x-109">‚àºùí©</span>(0<span class="cmmi-10x-x-109">,I</span>)</p>
</div>
</li>
<li>
<div id="x1-319019x3">
<p>Mutate the parent: <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmmi-8">g</span></sup> = <span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmmi-8">g</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup> + <span class="cmmi-10x-x-109">œÉùúñ</span></p>
</div>
</li>
<li>
<div id="x1-319021x4">
<p>Get its fitness: <span class="cmmi-10x-x-109">F</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmmi-8">g</span></sup> = <span class="cmmi-10x-x-109">F</span>(<span class="cmmi-10x-x-109">P</span><sub><span class="cmmi-8">i</span></sub><sup><span class="cmmi-8">g</span></sup>)</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</li>
</ol>
<p>There have been several improvements to this basic <span id="dx1-319022"/>method from the paper [2], which<span id="dx1-319023"/> we will discuss later. For now, let‚Äôs check the implementation of the core algorithm.</p>
<section class="level4 subsectionHead" id="ga-on-cartpole">
<h2 class="heading-2" id="sigil_toc_id_290"> <span id="x1-32000017.3.1"/>GA on CartPole</h2>
<p>The source <span id="dx1-320001"/>code is in <span class="cmtt-10x-x-109">Chapter17/03</span><span class="cmtt-10x-x-109">_cartpole</span><span class="cmtt-10x-x-109">_ga.py</span>, and it has a lot in common with<span id="dx1-320002"/> our ES example. The difference is in the lack of the gradient ascent code, which was replaced by the network mutation function:</p>
<div class="tcolorbox" id="tcolobox-398">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-470"><code>def mutate_parent(net: Net) -&gt; Net: 
    new_net = copy.deepcopy(net) 
    for p in new_net.parameters(): 
        noise = np.random.normal(size=p.data.size()) 
        noise_t = torch.FloatTensor(noise) 
        p.data += NOISE_STD * noise_t 
    return new_net</code></pre>
</div>
</div>
<p>The goal of the function is to create a mutated copy of the given policy by adding a random noise to all weights. The parent‚Äôs weights are kept untouched, as a random selection of the parent is performed with replacement, so this network could be used again later.</p>
<p>The count of hyperparameters is even smaller than with ES and includes the standard deviation of the noise added-on mutation, the population size, and the number of top performers used to produce the subsequent generation:</p>
<div class="tcolorbox" id="tcolobox-399">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-471"><code>NOISE_STD = 0.01 
POPULATION_SIZE = 50 
PARENTS_COUNT = 10</code></pre>
</div>
</div>
<p>Before the training loop, we create the population of randomly initialized networks and obtain their fitness:</p>
<div class="tcolorbox" id="tcolobox-400">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-472"><code>if __name__ == "__main__": 
    env = gym.make("CartPole-v1") 
    writer = SummaryWriter(comment="-cartpole-ga") 
 
    gen_idx = 0 
    nets = [ 
        Net(env.observation_space.shape[0], env.action_space.n) 
        for _ in range(POPULATION_SIZE) 
    ] 
    population = [ 
        (net, common.evaluate(env, net)) 
        for net in nets 
    ]</code></pre>
</div>
</div>
<p>At the beginning of every generation, we sort the previous generation according to its fitness and record statistics about future parents:</p>
<div class="tcolorbox" id="tcolobox-401">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-473"><code>    while True: 
        population.sort(key=lambda p: p[1], reverse=True) 
        rewards = [p[1] for p in population[:PARENTS_COUNT]] 
        reward_mean = np.mean(rewards) 
        reward_max = np.max(rewards) 
        reward_std = np.std(rewards) 
 
        writer.add_scalar("reward_mean", reward_mean, gen_idx) 
        writer.add_scalar("reward_std", reward_std, gen_idx) 
        writer.add_scalar("reward_max", reward_max, gen_idx) 
        print("%d: reward_mean=%.2f, reward_max=%.2f, reward_std=%.2f" % ( 
            gen_idx, reward_mean, reward_max, reward_std)) 
        if reward_mean &gt; 199: 
            print("Solved in %d steps" % gen_idx) 
            break</code></pre>
</div>
</div>
<p>In a separate loop over new individuals to be generated, we <span id="dx1-320041"/>randomly sample a parent, mutate <span id="dx1-320042"/>it, and evaluate its fitness score:</p>
<div class="tcolorbox" id="tcolobox-402">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-474"><code>        prev_population = population 
        population = [population[0]] 
        for _ in range(POPULATION_SIZE-1): 
            parent_idx = np.random.randint(0, PARENTS_COUNT) 
            parent = prev_population[parent_idx][0] 
            net = mutate_parent(parent) 
            fitness = common.evaluate(env, net) 
            population.append((net, fitness)) 
        gen_idx += 1</code></pre>
</div>
</div>
<p>After starting the implementation, you should see the following (concrete output and count of steps might vary due to randomness in execution):</p>
<pre class="lstlisting" id="listing-475"><code>Chapter17$ ./03_cartpole_ga.py 
0: reward_mean=29.50, reward_max=109.00, reward_std=27.86 
1: reward_mean=65.50, reward_max=111.00, reward_std=27.61 
2: reward_mean=149.10, reward_max=305.00, reward_std=57.76 
3: reward_mean=175.00, reward_max=305.00, reward_std=47.35 
4: reward_mean=200.50, reward_max=305.00, reward_std=39.98 
Solved in 4 steps</code></pre>
<p>As you can <span id="dx1-320059"/>see, the GA method is even more <span id="dx1-320060"/>efficient than the ES method.</p>
</section>
<section class="level4 subsectionHead" id="ga-tweaks">
<h2 class="heading-2" id="sigil_toc_id_291"> <span id="x1-32100017.3.2"/>GA tweaks</h2>
<p>Such et al. proposed <span id="dx1-321001"/>two tweaks to the basic GA algorithm:</p>
<ul>
<li>
<p>The first, with the name <span class="cmbx-10x-x-109">deep GA</span>, aimed <span id="dx1-321002"/>to increase the scalability of the implementation. We will implement this later in the <span class="cmti-10x-x-109">GA on</span> <span class="cmti-10x-x-109">HalfCheetah </span>section.</p>
</li>
<li>
<p>The second, called <span class="cmbx-10x-x-109">novelty search</span>, was <span id="dx1-321003"/>an attempt to replace the reward objective with a different metric of the episode. We‚Äôve left this as an exercise for you to try out.</p>
</li>
</ul>
<p>In the example used in the following <span class="cmti-10x-x-109">GA on HalfCheetah </span>section, we will implement the first improvement, whereas the second one is left as an optional exercise.</p>
<section class="level5 subsubsectionHead" id="deep-ga">
<h3 class="heading-3" id="sigil_toc_id_400"><span id="x1-322000"/>Deep GA</h3>
<p>Being a <span id="dx1-322001"/>gradient-free method, GA is potentially even more scalable than ES methods in terms of speed, with more CPUs involved in the optimization. However, the simple GA algorithm that you have seen has a similar bottleneck to ES methods: the policy parameters have to be exchanged between the workers. Such et al. (the authors) proposed a trick similar to the shared seed approach but taken to an extreme (as we‚Äôre using seeds to track thousands of mutations). They called it deep GA, and at its core, the policy parameters are represented as a list of random seeds used to create this particular policy‚Äôs weights.</p>
<p>In fact, the initial network‚Äôs weights were generated randomly on the first population, so the first seed in the list defines this initialization. On every population, mutations are also fully specified by the random seed for every mutation. So, the only thing we need to reconstruct the weights is the seeds themselves. In this approach, we need to reconstruct the weights on every worker, but usually, this overhead is much less than the overhead of transferring full weights over the network.</p>
</section>
<section class="level5 subsubsectionHead" id="novelty-search">
<h3 class="heading-3" id="sigil_toc_id_401"><span id="x1-323000"/>Novelty search</h3>
<p>Another <span id="dx1-323001"/>modification to the basic GA method is <span class="cmbx-10x-x-109">novelty search </span>(<span class="cmbx-10x-x-109">NS</span>), which was proposed by Lehman and Stanley in their paper, <span class="cmti-10x-x-109">Abandoning objectives:</span> <span class="cmti-10x-x-109">Evolution through the search for novelty alone</span>, which was published in 2011 [<span id="x1-323002"/><a href="#">LS11</a>].</p>
<p>The idea of NS is to change the objective in our optimization. We‚Äôre no longer trying to increase our total reward from the environment but, rather, reward the agent for exploring the behavior that it has never checked before (that is, <span class="cmti-10x-x-109">novel</span>). According to the authors‚Äô experiments on the maze navigation problem, with many traps for the agent, NS works much better than other reward-driven approaches.</p>
<p>To implement NS, we <span id="dx1-323003"/>define the so-called <span class="cmbx-10x-x-109">behavior characteristic</span> (<span class="cmbx-10x-x-109">BC</span>) (<span class="cmmi-10x-x-109">œÄ</span>), which describes the behavior of the policy and a distance between two BCs. Then, the k-nearest neighbors approach is used to check the novelty of the new policy and drive the GA according to this distance. In the paper by Such et al., sufficient exploration by the agent was needed. The approach of NS significantly outperformed the ES, GA, and other more traditional approaches to RL problems.</p>
</section>
</section>
<section class="level4 subsectionHead" id="ga-on-halfcheetah">
<h2 class="heading-2" id="sigil_toc_id_292"> <span id="x1-32400017.3.3"/>GA on HalfCheetah</h2>
<p>In our final <span id="dx1-324001"/>example in this chapter, we will implement the parallelized deep GA on the HalfCheetah environment. The complete code is in <span class="cmtt-10x-x-109">04</span><span class="cmtt-10x-x-109">_cheetah</span><span class="cmtt-10x-x-109">_ga.py</span>. The architecture is very close to the parallel ES version, with one master process and several workers. The goal of every worker is to evaluate the batch of networks and return the result to the master, which merges partial results into the complete population, ranks the individuals according to the obtained reward, and generates the next population to be evaluated by the workers.</p>
<p>Every individual is encoded by a list of random seeds used to initialize the initial network weights and all subsequent mutations. This representation allows very compact encoding of the network, even when the number of parameters in the policy is not very large. For example, in our network with with one hidden layer of 64 neurons, we have 1,542 float values (the input is 17 values and the action is 6 floats, which gives 17 <span class="cmsy-10x-x-109">√ó </span>64 + 64 + 64 <span class="cmsy-10x-x-109">√ó </span>6 + 6 = 1542). Every float occupies 4 bytes, which is the same size used by the random seed. So, the deep GA representation proposed by the paper will be smaller up to 1,542 generations in the optimization.</p>
<section class="level5 subsubsectionHead" id="implementation-19">
<h3 class="heading-3" id="sigil_toc_id_402"><span id="x1-325000"/>Implementation</h3>
<p>In our example, we<span id="dx1-325001"/> will perform parallelization on local CPUs so the amount of data transferred back and forth doesn‚Äôt matter much; however, if you have a couple of hundred cores to utilize, the representation might become a significant issue.</p>
<p>The set of hyperparameters is the same as in the CartPole example, with the difference of a larger population size:</p>
<div class="tcolorbox" id="tcolobox-403">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-476"><code>NOISE_STD = 0.01 
POPULATION_SIZE = 2000 
PARENTS_COUNT = 10 
WORKERS_COUNT = 6 
SEEDS_PER_WORKER = POPULATION_SIZE // WORKERS_COUNT 
MAX_SEED = 2**32 - 1</code></pre>
</div>
</div>
<p>There are two functions used to build the networks based on the seeds given. The first one performs one mutation on the already created policy network:</p>
<div class="tcolorbox" id="tcolobox-404">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-477"><code>def mutate_net(net: Net, seed: int, copy_net: bool = True) -&gt; Net: 
    new_net = copy.deepcopy(net) if copy_net else net 
    np.random.seed(seed) 
    for p in new_net.parameters(): 
        noise = np.random.normal(size=p.data.size()) 
        noise_t = torch.FloatTensor(noise) 
        p.data += NOISE_STD * noise_t 
    return new_net</code></pre>
</div>
</div>
<p>The preceding function can perform the mutation in <span id="dx1-325016"/>place or by copying the target network based on arguments (copying is needed for the first generation).</p>
<p>The second function creates the network from scratch using the list of seeds:</p>
<div class="tcolorbox" id="tcolobox-405">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-478"><code>def build_net(env: gym.Env, seeds: tt.List[int]) -&gt; Net: 
    torch.manual_seed(seeds[0]) 
    net = Net(env.observation_space.shape[0], env.action_space.shape[0]) 
    for seed in seeds[1:]: 
        net = mutate_net(net, seed, copy_net=False) 
    return net</code></pre>
</div>
</div>
<p>Here, the first seed is passed to PyTorch to influence the network initialization, and subsequent seeds are used to apply network mutations.</p>
<p>The worker function obtains the list of seeds to evaluate and outputs individual <span class="cmtt-10x-x-109">OutputItem </span>dataclass items for every result obtained:</p>
<div class="tcolorbox" id="tcolobox-406">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-479"><code>@dataclass 
class OutputItem: 
    seeds: tt.List[int] 
    reward: float 
    steps: int 
 
 
def worker_func(input_queue: mp.Queue, output_queue: mp.Queue): 
    env = gym.make("HalfCheetah-v4") 
    cache = {} 
 
    while True: 
        parents = input_queue.get() 
        if parents is None: 
            break 
        new_cache = {} 
        for net_seeds in parents: 
            if len(net_seeds) &gt; 1: 
                net = cache.get(net_seeds[:-1]) 
                if net is not None: 
                    net = mutate_net(net, net_seeds[-1]) 
                else: 
                    net = build_net(env, net_seeds) 
            else: 
                net = build_net(env, net_seeds) 
            new_cache[net_seeds] = net 
            reward, steps = common.evaluate(env, net, get_max_action=False) 
            output_queue.put(OutputItem(seeds=net_seeds, reward=reward, steps=steps)) 
        cache = new_cache</code></pre>
</div>
</div>
<p>This function<span id="dx1-325052"/> maintains the cache of networks to minimize the amount of time spent recreating the parameters from the list of seeds. This cache is cleared for every generation, as every new generation is created from the current generation winners, so there is only a tiny chance that old networks can be reused from the cache.</p>
<p>The code of the master process is also straightforward:</p>
<div class="tcolorbox" id="tcolobox-407">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-480"><code>        batch_steps = 0 
        population = [] 
        while len(population) &lt; SEEDS_PER_WORKER * WORKERS_COUNT: 
            out_item = output_queue.get() 
            population.append((out_item.seeds, out_item.reward)) 
            batch_steps += out_item.steps 
        if elite is not None: 
            population.append(elite) 
        population.sort(key=lambda p: p[1], reverse=True) 
        elite = population[0] 
        for worker_queue in input_queues: 
            seeds = [] 
            for _ in range(SEEDS_PER_WORKER): 
                parent = np.random.randint(PARENTS_COUNT) 
                next_seed = np.random.randint(MAX_SEED) 
                s = list(population[parent][0]) + [next_seed] 
                seeds.append(tuple(s)</code></pre>
</div>
</div>
<p>For every generation, we send the current population‚Äôs seeds to workers for evaluation and wait for the results. Then, we sort the results and generate the next population based on the <span id="dx1-325070"/>top performers. On the master‚Äôs side, the mutation is just a seed number generated randomly and appended to the list of seeds of the parent.</p>
</section>
<section class="level5 subsubsectionHead" id="results-23">
<h3 class="heading-3" id="sigil_toc_id_403"><span id="x1-326000"/>Results</h3>
<p>In this example, we‚Äôre using the MuJoCo HalfCheetah<span id="dx1-326001"/> environment, which doesn‚Äôt have any health checks internally, so every episode takes 2,000 steps. Because of this, every training step requires about a minute, so be patient. After 300 mutation rounds (which took about 7 hours), the best policy was able to get a reward of 6454, which is a great result. If you remember our experiments in the previous chapter, only the SAC method was able to get a higher reward of 7063 on MuJoCo HalfCheetah. Of course, HalfCheetah is not very challenging, but still ‚Äî very good.</p>
<p>The plots are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-326002r5"><span class="cmti-10x-x-109">17.5</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-326003r6"><span class="cmti-10x-x-109">17.6</span></a>.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_05.png" width="600"/> <span id="x1-326002r5"/></p>
<span class="id">Figure¬†17.5: The maximum (left) and mean rewards (right) for GA on HalfCheetah </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_17_06.png" width="500"/> <span id="x1-326003r6"/></p>
<span class="id">Figure¬†17.6: Standard deviation of reward for GA on HalfCheetah </span>
</div>
</section>
</section>
</section>
<section class="level3 sectionHead" id="summary-16">
<h1 class="heading-1" id="sigil_toc_id_293"> <span id="x1-32700017.4"/>Summary</h1>
<p>In this chapter, you saw two examples of black-box optimization methods: evolution strategies and genetic algorithms, which can provide competition for other analytical gradient methods. Their strength lies in good parallelization on a large number of resources and the smaller number of assumptions that they have on the reward function.</p>
<p>In the next chapter, we will take a look at a very important aspect of RL: advanced exploration methods.</p>
</section>
</section>
</div></body></html>