["```py\n    import sys, os \n    import tensorflow as tf \n    sys.path.append(os.path.realpath('../..')) \n    from data_utils import * \n    from logmanager import * \n    import math\n```", "```py\n    batch_size = 128 \n    num_steps = 10000 \n    learning_rate = 0.3 \n    data_showing_step = 500 \n```", "```py\n batch_size = 32 \n num_steps = 30000 \n learning_rate = 0.1 \n data_showing_step = 500 \n model_saving_step = 2000 \n log_location = '/tmp/alex_nn_log' \n\n SEED = 11215 \n\n patch_size = 5 \n depth_inc = 4 \n num_hidden_inc = 32 \n dropout_prob = 0.8 \n conv_layers = 3 \n stddev = 0.1 \n```", "```py\n    def fc_first_layer_dimen(image_size, layers): \n       output = image_size \n       for x in range(layers): \n        output = math.ceil(output/2.0) \n       return int(output) \n```", "```py\n    fc_first_layer_dimen(image_size, conv_layers) \n```", "```py\n    with tf.name_scope('Layer_1') as scope: \n        conv = tf.nn.conv2d(data, weights['conv1'], strides=[1, 1, \n         1, 1], padding='SAME', name='conv1')        \n        bias_add = tf.nn.bias_add(conv, biases['conv1'], \n         name='bias_add_1') \n        relu = tf.nn.relu(bias_add, name='relu_1') \n        max_pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], \n         strides=[1, 2, 2, 1], padding='SAME', name=scope)\n```", "```py\n    conv = tf.nn.conv2d(max_pool, weights['conv2'], strides=[1, 1, 1, \n     1], padding='SAME', name='conv2')\n```", "```py\n    conv = tf.nn.conv2d(max_pool, weights['conv3'], strides=[1, 1, 1, \n     1], padding='SAME', name='conv3')\n```", "```py\n    print \"Layer 1 CONV\", conv.get_shape() \n    print \"Layer 2 CONV\", conv.get_shape() \n    print \"Layer 3 CONV\", conv.get_shape() \n```", "```py\nLayer 1 CONV (32, 28, 28, 4) \nLayer 2 CONV (32, 14, 14, 4) \nLayer 3 CONV (32, 7, 7, 4) \nLayer 1 CONV (10000, 28, 28, 4) \nLayer 2 CONV (10000, 14, 14, 4) \nLayer 3 CONV (10000, 7, 7, 4) \nLayer 1 CONV (10000, 28, 28, 4) \nLayer 2 CONV (10000, 14, 14, 4) \nLayer 3 CONV (10000, 7, 7, 4) \n```", "```py\n print \"Layer 1 CONV\", conv.get_shape() \n print \"Layer 1 RELU\", relu.get_shape() \n print \"Layer 1 POOL\", max_pool.get_shape() \n```", "```py\nLayer 1 CONV (32, 28, 28, 4) \nLayer 1 RELU (32, 28, 28, 4) \nLayer 1 POOL (32, 14, 14, 4) \nLayer 2 CONV (32, 14, 14, 4) \nLayer 2 RELU (32, 14, 14, 4) \nLayer 2 POOL (32, 7, 7, 4) \nLayer 3 CONV (32, 7, 7, 4) \nLayer 3 RELU (32, 7, 7, 4) \nLayer 3 POOL (32, 4, 4, 4) \n... \n```", "```py\n max_pool = tf.nn.dropout(max_pool, dropout_prob, seed=SEED, \n  name='dropout')\n```", "```py\n    with tf.name_scope('FC_Layer_1') as scope: \n        matmul = tf.matmul(reshape, weights['fc1'], \n         name='fc1_matmul')       \n         bias_add = tf.nn.bias_add(matmul, biases['fc1'], \n         name='fc1_bias_add') \n        relu = tf.nn.relu(bias_add, name=scope) \n```", "```py\n    with tf.name_scope('FC_Layer_2') as scope: \n        matmul = tf.matmul(relu, weights['fc2'], \n         name='fc2_matmul')       \n        layer_fc2 = tf.nn.bias_add(matmul, biases['fc2'], \n         name=scope)\n```", "```py\n    tf_train_dataset = tf.placeholder(tf.float32, \n    shape=(batch_size, image_size, image_size,   \n    num_channels), \n    name='TRAIN_DATASET')    \n    tf_train_labels = tf.placeholder(tf.float32, \n    shape=(batch_size, num_of_classes), \n    name='TRAIN_LABEL') \n    tf_valid_dataset = tf.constant(dataset.valid_dataset,   \n    name='VALID_DATASET') \n    tf_test_dataset = tf.constant(dataset.test_dataset,  \n    name='TEST_DATASET') \n```", "```py\n    # Training computation. \n    logits = nn_model(tf_train_dataset, weights, biases,  \n    True) \n    loss = tf.reduce_mean( \n        tf.nn.softmax_cross_entropy_with_logits(logits, \n         tf_train_labels)) \n    # L2 regularization for the fully connected  \n    parameters. \n    regularizers = (tf.nn.l2_loss(weights['fc1']) + \n     tf.nn.l2_loss(biases['fc1']) + \n     tf.nn.l2_loss(weights['fc2']) + \n\n    tf.nn.l2_loss(biases['fc2'])) \n    # Add the regularization term to the loss. \n    loss += 5e-4 * regularizers \n    tf.summary.scalar(\"loss\", loss) \n```", "```py\n    optimizer = tf.train.GradientDescentOptimizer\n     (learning_rate).minimize(loss)\n```", "```py\n    train_prediction = tf.nn.softmax(nn_model(tf_train_dataset,  \n    weights, biases, TRAIN=False)) \n    valid_prediction = tf.nn.softmax(nn_model(tf_valid_dataset, \n     weights, biases))    test_prediction =  \n     tf.nn.softmax(nn_model(tf_test_dataset, \n     weights, biases))\n```", "```py\n    batch_data = dataset.train_dataset[offset:(offset + \n     batch_size), :]   \n    batch_labels = dataset.train_labels[offset: \n     (offset + \n     batch_size), :]\n```", "```py\n    valid_prediction =   \n    tf.nn.softmax(nn_model(tf_valid_dataset, \n     weights, biases))\n```", "```py\n    accuracy(valid_prediction.eval(), \n    dataset.valid_labels) \n```", "```py\n    accuracy(test_prediction.eval(), dataset.test_labels)\n```", "```py\n    dataset, image_size, num_of_classes, num_channels = \n     prepare_not_mnist_dataset() \n```", "```py\n    dataset, image_size, num_of_classes, num_channels = \n     prepare_cifar_10_dataset()\n```", "```py\n  def prepare_cifar_10_dataset(): \n    print('Started preparing CIFAR-10 dataset') \n    image_size = 32 \n    image_depth = 255 \n    cifar_dataset_url = 'https://www.cs.toronto.edu/~kriz/cifar-\n     10-python.tar.gz' \n    dataset_size = 170498071 \n    train_size = 45000 \n    valid_size = 5000 \n    test_size = 10000 \n    num_of_classes = 10 \n    num_of_channels = 3 \n    pickle_batch_size = 10000 \n```", "```py\n    writer = tf.summary.FileWriter(log_location, session.graph)\n    saver = tf.train.Saver(max_to_keep=5)\n```", "```py\n if step % model_saving_step == 0 or step == num_steps + 1: \n   path = saver.save(session, os.path.join(log_location,  \n \"model.ckpt\"), global_step=step) \n   logmanager.logger.info('Model saved in file: %s' % path) \n```", "```py\n checkpoint_path = tf.train.latest_checkpoint(log_location) \n restorer = tf.train.Saver() \n with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    restorer.restore(sess, checkpoint_path) \n```", "```py\n    restorer.restore(sess, checkpoint_path) \n```", "```py\n tf_random_dataset = tf.placeholder(tf.float32, shape=(1, \n  image_size, image_size, num_channels),  \n name='RANDOM_DATA')random_prediction =  \n tf.nn.softmax(nn_model(tf_random_dataset, \n  weights, biases))\n```", "```py\n    The prediction is: 2 \n```"]