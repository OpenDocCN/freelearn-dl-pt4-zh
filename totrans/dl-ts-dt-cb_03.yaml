- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Univariate Time Series Forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll develop deep learning models to tackle univariate time
    series forecasting problems. We’ll touch on several aspects of time series preprocessing,
    such as preparing a time series for supervised learning and dealing with conditions
    such as trend or seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover different types of models, including simple baselines such as the
    naïve or historical mean method. We’ll provide a brief background on a popular
    forecasting technique, **autoregressive integrated moving average** (**ARIMA**).
    Then, we’ll explain how to create a forecasting model using different types of
    deep learning methods. These include feedforward neural networks, **long short-term
    memory** (**LSTM**), **gated recurrent units** (**GRU**), Stacked **LSTM**, and
    **convolutional neural networks** (**CNNs**). You will also learn how to deal
    with common problems that arise in time series modeling; for example, how to deal
    with trend using first differences, and how to stabilize the variance using a
    logarithm transformation. By the end of this chapter, you will be able to solve
    a univariate time series forecasting problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will guide you through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building simple forecasting models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with ARIMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a time series for supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a feedforward neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with an LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a Stacked LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining an LSTM with multiple fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling trend – taking first differences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal dummies and a Fourier series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal differencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling non-constant variance – log transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into univariate time series forecasting problems, we need to
    ensure that we have the appropriate software and libraries installed on our system.
    Here, we’ll go over the main technical requirements for implementing the procedures
    described in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We will primarily need Python 3.9 or a later version, `pip` or Anaconda, PyTorch,
    and CUDA (optional). You can check the *Installing PyTorch* recipe from the previous
    chapter for more information on these.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NumPy (1.26.3) and pandas (2.1.4): Both these `Python` libraries provide several
    methods for data manipulation and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statsmodels` (0.14.1): This library implements several statistical methods,
    including a few useful time series analysis techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn` (1.4.0): `scikit-learn` is a popular `Python` library for statistical
    learning. It contains several methods to solve different tasks, such as classification,
    regression, and clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sktime` (0.26.0): A Python library that provides a framework for tackling
    several problems involving time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install these libraries using `pip`, Python’s package manager. For
    example, to install `scikit-learn`, you would run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Building simple forecasting models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into more complex methods, let’s get started with some simple
    forecasting models: the naive, seasonal naive, and mean models.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we focus on forecasting problems involving univariate time
    series. Let’s start by loading one of the datasets we explored in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `series` is a `pandas Series` object that contains the
    univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now forecast our time series using the three following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Python`, it could be implemented as simply as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`m`, in `Python`, this can be done as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Python` as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These three methods are useful baselines to benchmark the performance of other,
    more complex, forecasting solutions.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each of these simple models makes an assumption about the time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: The naive model assumes that the series is random and that each observation
    is independent of the previous ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seasonal naive model adds a little complexity by recognizing patterns at
    fixed intervals or “seasons”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean model assumes that the series oscillates around a constant mean, and
    future values will regress to this mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While these simple models might seem overly basic, they serve two critical
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Baselines**: Simple models such as these are often used as baselines for
    more sophisticated models. If a complex model cannot outperform these simple methods,
    it suggests that the complex model might be flawed or that the time series data
    does not contain predictable patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding data**: These models can also help us understand our data.
    If time series data can be well forecasted by a naive or mean model, it suggests
    that the data may be random or fluctuate around a constant mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing simple forecasting models such as naive, seasonal naive, and historical
    mean models can be quite straightforward, but it may be beneficial to leverage
    existing libraries that provide off-the-shelf implementations of these models.
    These libraries not only simplify the implementation but also often come with
    additional features such as built-in model validation, optimization, and other
    utilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two examples of libraries that provide these models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GluonTS: GluonTS is a `Python` library focused on probabilistic models for
    time series. Among other models, there is an implementation of the seasonal naive
    model, which can be found at the following link: [https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html](https://ts.gluon.ai/dev/api/gluonts/gluonts.model.seasonal_naive.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sktime`: This library provides a framework to develop different types of models
    with time series data. This includes a `NaiveForecaster``()` method, which implements
    several baselines. You can read more about this method at the following URL: [https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch Forecasting: This library focuses on developing state-of-the-art time
    series forecasting models with neural networks for both real-world cases and research.
    PyTorch Forecasting provides a baseline model class, which uses the last known
    target value as the prediction. This class can be found at the following link:
    [https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.baseline.Baseline.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding libraries can be a great starting point when you are working on
    a forecasting task. They not only provide implementations of simple forecasting
    models but also contain many other sophisticated models and utilities that can
    help streamline the process of developing and validating time series forecasting
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipes, we will see how these assumptions can be relaxed or extended
    to build more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate forecasting with ARIMA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ARIMA is a univariate time series forecasting method based on two components:
    an autoregression part and a moving average part. In autoregression, a **lag**
    refers to a previous point or points in the time series data that are used to
    predict future values. For instance, if we’re using a lag of one, we’d use the
    value observed in the previous time step to model a given observation. The moving
    average part uses past errors to model the future observations of the time series.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To work with the ARIMA model, you’ll need to install the `statsmodels` Python
    package if it’s not already installed. You can install it using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For this recipe, we’ll use the same dataset as in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Python, you can use the ARIMA model from the `statsmodels` library. Here’s
    a basic example of how to fit an ARIMA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ARIMA models explain a time series based on its past values. They combine aspects
    of **autoregressive** (**AR**) models, **integrated** (**I**) models, and **moving
    average** (**MA**) models:'
  prefs: []
  type: TYPE_NORMAL
- en: The AR part involves a regression where the next value of the time series is
    modeled based on the previous `p` lags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMA is defined for stationary data, so it may be necessary to preprocess the
    data before modeling. This is done by the I part, which represents the number
    of differencing operations (`d`) required to make the series stationary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MA component is another regression where the next value of the series is
    modeled based on the past `q` errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order of these operations is represented as a tuple (`p, d, q`). The best
    combination depends on the input data. In this example, we used a (`1, 1, 1`)
    order as an example.
  prefs: []
  type: TYPE_NORMAL
- en: A prediction is made for the next six observations into the future using the
    `model_fit.predict()` function. The start and end indices for the prediction are
    set to `0` and `5`, respectively. The `typ='levels'` parameter is used to return
    the predicted values directly rather than differenced values.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Determining the ARIMA model’s correct order (`p, d, q`) can be challenging.
    This often involves checking the `2` measures the correlation between a time series
    and its values in two time periods in the past. On the other hand, the PACF measures
    autocorrelation while controlling for previous lags. This means a PACF at lag
    `2` measures the correlation between a series and its values two time periods
    ago but with the linear dependence of the one time period lag removed. You can
    learn more about this at the following URL: [https://otexts.com/fpp3/acf.html](https://otexts.com/fpp3/acf.html).
    By examining the ACF and PACF plots, we can better understand the underlying patterns
    of a time series and thus make more accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the ARIMA model assumes that the time series is stationary, which might
    not always be true. Thus, it may be necessary to use transformations such as differencing
    or the log to make the time series stationary before fitting the ARIMA model.
  prefs: []
  type: TYPE_NORMAL
- en: The **seasonal ARIMA** model is commonly used for non-stationary time series
    with a seasonal component. This model adds a set of parameters to model the seasonal
    components of the time series specifically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there are automated ways to tune the parameters of ARIMA. One popular
    approach is to use the `pmdarima` library’s `auto_arima()` function. Another useful
    implementation is the one available in the `statsforecast` package. You can check
    it out at the following URL: [https://nixtlaverse.nixtla.io/statsforecast/index.html](https://nixtlaverse.nixtla.io/statsforecast/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides ARIMA, you can also explore exponential smoothing methods, which is
    another popular classical approach to forecasting. The implementation of exponential
    smoothing approaches is also available in `statsmodels` or `statsforecast`, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a time series for supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we turn our attention to machine learning approaches to forecasting.
    We start by describing the process of transforming a time series from a sequence
    of values into a format suitable for supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning involves a dataset with explanatory variables (input) and
    a target variable (output). A time series comprises a sequence of values with
    an associated timestamp. Therefore, we need to restructure the time series for
    supervised learning. A common approach to do this is using a sliding window. Each
    value of the series is based on the recent past values before it (also called
    lags).
  prefs: []
  type: TYPE_NORMAL
- en: 'To prepare for this section, you need to have your time series data available
    in a `pandas` DataFrame and have the `pandas` and NumPy libraries installed. If
    not, you can install them using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We also load the univariate time series into the Python session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following Python function takes a univariate time series and the window
    size as input and returns the input (`X`) and output (`y`) for a supervised learning
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `series_to_supervised``()` function is the heart of this script, which
    takes in four arguments: the time series data, the number of lag observations
    (`n_in`), the number of observations as output (`n_out`), and whether to drop
    rows with `NaN` values (`dropnan`):'
  prefs: []
  type: TYPE_NORMAL
- en: The function begins by checking the data type and preparing an empty list for
    columns (`cols`) and their names (`names`). It then creates the input sequence
    (`t-n, ..., t-1`) by shifting the DataFrame and appending these columns to `cols`,
    and the corresponding column names to `names`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function continues to create the forecast sequence `t, t+1 ..., t+n` similarly,
    again appending these to `cols` and `names`. Then, it aggregates all columns into
    a new DataFrame (`agg`), assigns the column `names`, and optionally drops rows
    with `NaN` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script then loads a time series dataset about solar radiation (`time_series_solar.csv`)
    into a DataFrame (`df`), extracts the `Incoming Solar` column into a NumPy array
    (`values`), and transforms this array into a supervised learning dataset with
    three lag observations using the `series_to_supervised``()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it prints the transformed data, which consists of sequences of lagged
    observations as input and the corresponding future observations as output. This
    format is ready for any supervised learning algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In supervised learning, the goal is to train a model to learn the relationship
    between input variables and a target variable. Nevertheless, this type of structure
    is not immediately available when dealing with time series data. The data is typically
    a sequence of observations (for example, temperature and stock prices) made over
    time. Thus, we must transform this time series data into a suitable format for
    supervised learning. This is what the `series_to_supervised``()` function does.
  prefs: []
  type: TYPE_NORMAL
- en: The transformation process involves creating lagged versions of the original
    time series data using a sliding window approach. This is done by shifting the
    time series data by a certain number of steps (denoted by `n_in` in the code)
    to create the input features. These lagged observations serve as the explanatory
    variables (input), with the idea that past values influence future ones in many
    real-world time series.
  prefs: []
  type: TYPE_NORMAL
- en: The target variable (output) is created by shifting the time series in the opposite
    direction by a number of steps (forecasting horizon) denoted by `n_out`. This
    means that for each input sequence, we have the corresponding future values that
    the model should predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we were to prepare a univariate time series for a simple
    forecasting task using a sliding window of size `3`. In that case, we might transform
    the series `[1, 2, 3, 4, 5, 6]` into the following supervised learning dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **input (t-3,** **t-2, t-1)** | **output (t)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 2, 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2, 3, 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3, 4, 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: Example of transforming a time series into a supervised learning
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The `series_to_supervised``()` function takes as input a sequence of observations,
    `n_in`, which specifies the number of lag observations as input, `n_out`, which
    specifies the number of observations as output, and a boolean argument `dropnan`
    to remove rows with `NaN` values. It returns a DataFrame suitable for supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The function works by iterating over the input data a specified number of times,
    each time shifting the data and appending it to a list (`cols`). The list is then
    concatenated into a DataFrame and the columns are renamed appropriately. If `dropnan=True`,
    any rows with missing values are dropped.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The window size, which represents how many past time steps we should use to
    predict future ones, depends on the specific problem and the nature of the time
    series. A too-small window might not capture important patterns, while a too-large
    one might include irrelevant information. Testing different window sizes and comparing
    model performance is a common way to select an appropriate window size.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate forecasting with a feedforward neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe walks you through the process of building a feedforward neural network
    for forecasting with univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having transformed the time series data into an appropriate format for supervised
    learning, we are now ready to employ it for training a feedforward neural network.
    We strategically decided to resample the dataset, transitioning from hourly to
    daily data. This optimization significantly accelerates our training processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps for building and evaluting a feedforward neural network
    using PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by splitting the data into training and testing and normalizing them.
    It’s important to note that the scaler should be fitted on the training set and
    used to transform both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a simple feedforward neural network with one hidden layer using
    PyTorch. `input_dim` represents the number of lags, which is often referred to
    as the lookback window. `hidden_dim` is the number of hidden units in the hidden
    layer of the neural network. Finally, `output_dim` is the forecasting horizon,
    which is set to `1` in the following example. We use a `ReLU``()` activation function,
    which we described in the *Training a feedforward neural network* recipe from
    the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the `loss` function and the `optimizer` and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we evaluate the model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This script starts by dividing the data into training and testing sets. `MinMaxScaler`
    is used to scale the features to between `-1` and `1`. It’s important to note
    that we fit the scaler only on the training set to avoid data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a simple feedforward neural network model with one hidden layer.
    The `FeedForwardNN` class extends `nn.Module`, which is the base class for all
    neural network modules in PyTorch. The class constructor defines the layers of
    the network, and the `forward` method specifies how forward propagation is done.
  prefs: []
  type: TYPE_NORMAL
- en: The model is then trained using the mean squared error loss function and the
    `Adam` optimizer. The model parameters are updated over multiple epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the model is evaluated on the testing set, and the loss of this unseen
    data measures how well the model generalizes beyond the training data.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a simple example of how a feedforward neural network can be used for
    time series forecasting. There are several ways you can improve this model:'
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with different network architectures, for example, by adding
    more layers or changing the number of neurons in the hidden layer. You can also
    try different activation functions, optimizers, and learning rates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might be beneficial to use a more sophisticated method for preparing the
    training and testing sets; for example, using a rolling-window validation strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another improvement can be using early stopping to prevent overfitting. We’ll
    learn about this technique in the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, advanced models such as **recurrent neural networks** (**RNNs**)
    and LSTM networks are specifically designed for sequence data and can give better
    results for time series forecasting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with an LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe walks you through the process of building an LSTM neural network
    for forecasting with univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 2*](B21145_02.xhtml#_idTextAnchor140), LSTM networks,
    a variant of RNNs, have gained substantial attention for their performance on
    time series and sequence data. LSTM networks are particularly suited for this
    task because they can effectively capture long-term temporal dependencies in the
    input data due to their inherent memory cells.
  prefs: []
  type: TYPE_NORMAL
- en: This section will extend our univariate time series forecasting to LSTM networks
    using PyTorch. So, we continue with the objects created in the previous recipe
    (*Univariate forecasting with a feedforward* *neural network*).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same train and test sets from the previous section. For an
    LSTM, we must reshape the input data to 3D. As we explored in the previous chapter,
    the three dimensions of the input tensor to LSTMs represent the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Samples**: One sub-sequence (for example, the past five lags) is one sample.
    A batch is a set of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time steps**: The window size; how many observations we use from the past
    at each point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Features**: The number of variables used in the model. Univariate time series
    always contain a single feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code transforms the input explanatory variables into a 3D format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding lines of code, `X_train.shape[0]` and `X_test.shape[0]` represent
    the number of samples (that is, the number of sequences), and `X_train.shape[1]`
    and `X_test.shape[1]` represent the number of time steps (the window size). The
    last dimension in the reshape operation, which is set to `1`, represents the number
    of features. We only have one feature in our univariate time series, so we set
    it to `1`. If we had a multivariate time series, this value would correspond to
    the number of variables in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The `view()` function in PyTorch is used to reshape a `tensor` object. It’s
    equivalent to the `reshape()` function in NumPy and allows us to restructure our
    data to match the input shape that our LSTM model requires. Reshaping the data
    this way ensures that the LSTM model receives the data in the expected format.
    This is crucial for its ability to model the temporal dependencies in our time
    series data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the LSTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that, for the LSTM, the `input_dim` input dimension is `1`, which is the
    number of variables in the time series. This aspect is different from the `input_dim`
    argument we passed to the feedforward neural network in the previous recipe. In
    that case, this parameter was set to `3`, which denoted the number of lags or
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now proceed to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first step, we reshape our training and testing sets to match the input
    shape that LSTM expects, i.e., `batch_size`, `sequence_length`, and `number_of_features`.
  prefs: []
  type: TYPE_NORMAL
- en: The `LSTM` class inherits from `nn.Module`, which means it is a custom neural
    network in PyTorch. The `LSTM` model has an `LSTM` layer with a specified number
    of hidden dimensions and layers, followed by a fully connected (linear) layer
    that outputs the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward``()` function defines the forward pass of the `LSTM` model. We
    first initialize the hidden states (`h0`) and cell states (`c0`) of `LSTM` with
    zeros. Then, we pass the input data and initial states into the `LSTM` layer,
    which returns the `LSTM` outputs and the final hidden and cell states. Note that
    we only use the final time-step output of the `LSTM` to pass into the fully connected
    layer to produce the output.
  prefs: []
  type: TYPE_NORMAL
- en: We then instantiate the model, define the `loss``()` function as the `Adam`
    optimizer for training the network.
  prefs: []
  type: TYPE_NORMAL
- en: During training, we first set the model into training mode, reset the gradients,
    perform the forward pass, calculate the loss, perform back-propagation via `loss.backward`,
    and then perform a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we evaluate the model on the test data and print the test loss. Note
    that we did not do any hyperparameter tuning, which is a very important step when
    training neural networks. We’ll learn about this process in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTM models are especially effective for time series forecasting due to their
    ability to capture long-term dependencies. However, their performance can significantly
    depend on the choice of hyperparameters. Hence, it may be useful to perform hyperparameter
    tuning to find the optimal configuration. Some important hyperparameters to consider
    are the number of hidden dimensions, the number of LSTM layers, and the learning
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to remember that LSTMs, like all deep learning models, may
    be prone to overfitting if the model complexity is too high. Techniques such as
    dropout, early stopping, or regularization (`L1`, `L2`) can be used to prevent
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, advanced variants of LSTMs such as bidirectional LSTMs, or other
    types of RNNs such as GRUs can also be used to improve performance possibly.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, while LSTMs are powerful, they may not always be the best choice due
    to their computational and memory requirements, especially for very large datasets
    or complex models. In these cases, simpler models or other types of neural networks
    may be more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate forecasting with a GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe walks you through the process of building a GRU neural network for
    forecasting with univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen how LSTMs can be used for univariate time series forecasting,
    let’s now shift our attention to another type of RNN architecture known as GRU.
    GRUs, like LSTMs, are designed to capture long-term dependencies in sequence data
    effectively but do so with a slightly different and less complex internal structure.
    This often makes them faster to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will use the same training and testing sets as in the
    previous sections. Again, the input data should be reshaped into a `3D` tensor
    with dimensions representing observations, time steps, and features respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start constructing a GRU network with the help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by constructing a GRU network in PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Like before, we define our `loss` function and `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we evaluate our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the LSTM, the GRU also requires 3D input data. We begin by reshaping
    our input data accordingly. Next, we define our GRU model. This model contains
    a GRU layer and a linear layer. The initial hidden state for the GRU is defined
    and initialized with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: We then define our `loss``()` function and optimizer and train our model. The
    model’s output from the last time step is used for predictions. Finally, we evaluate
    our model on the test set and print the test loss.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways to improve this model:'
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with different GRU architectures or varying the number of GRU
    layers may yield better results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a different loss function or optimizer could also potentially improve
    model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing early stopping or other regularization techniques can help prevent
    overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying more sophisticated data preparation techniques, such as sequence padding
    or truncation, can better equip the model to handle sequences of varying lengths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More advanced models, such as the sequence-to-sequence model or the transformer,
    may provide better results for more complex time series forecasting tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a Stacked LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe walks you through the process of building an LSTM neural network
    with multiple layers for forecasting with univariate time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For complex time series prediction problems, one LSTM layer may not be sufficient.
    In this case, we can use a stacked LSTM, which is essentially multiple layers
    of LSTM stacked one on top of the other. This can provide a higher level of input
    abstraction and may lead to improved prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use the same reshaped train and test sets from the previous
    recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We also use the LSTM neural network defined in the *Univariate forecasting
    with an* *LSTM* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use these elements to train a stacked LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To construct a stacked LSTM in PyTorch, we need to call the `LSTM` class with
    the input `num_layers=2`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the training process is quite similar to what we did in the preceding
    recipes. We define our loss function and `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The setup for the stacked LSTM model is similar to the single-layer LSTM model.
    The major difference lies in the LSTM layer, where we specify that we want more
    than one LSTM layer. This is accomplished by setting `num_layers` to `2` or more.
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass for the stacked LSTM is identical to that of the single-layer
    LSTM. We initialize the hidden state `h0` and cell state `c0` with zeros, pass
    the input and the initial states into the LSTM layers, and then use the output
    from the final time step for our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The test set loss is again closely aligned with previous results. Several factors
    could contribute to this observation. It could be a result of limited data or
    the fact that the expressiveness of the data may not benefit from the complexity
    of our model. Additionally, we have not conducted any hyperparameter optimization,
    which could potentially enhance the model’s performance. In subsequent sections,
    we will delve deeper into these aspects, exploring potential solutions and strategies
    for further improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Combining an LSTM with multiple fully connected layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it may be valuable to combine different types of neural networks
    in a single model. In this recipe, you’ll learn how to combine an LSTM module
    with a fully connected layer that is the basis of feedforward neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll use a hybrid model that combines an LSTM layer with multiple
    fully connected (also known as dense) layers. This allows us to further abstract
    features from the sequence, and then learn complex mappings to the output space.
  prefs: []
  type: TYPE_NORMAL
- en: We continue using the reshaped train and test sets from the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To construct this hybrid model in PyTorch, we add two fully connected layers
    after the LSTM layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We define our loss function and `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We train and evaluate our model similarly to the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The setup for the hybrid LSTM model involves an `LSTM` layer followed by two
    fully connected layers. After passing through the `LSTM` layer, the output of
    the final time step is processed by the fully connected layers. Using the `ReLU``()`
    activation function between these layers introduces non-linearities, allowing
    our model to capture more complex relationships in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output from an `LSTM` layer is a tensor of shape (`batch_size,
    seq_length, hidden_dim)`. This is because `LSTM`, by default, outputs the hidden
    states for each time step in the sequence for each item in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: In this specific model, we’re interested only in the last time step’s hidden
    state to feed into the fully connected layers. We achieve this with `out[:, -1,
    :]`, effectively selecting the last time step’s hidden state for each sequence
    in the batch. The result is a tensor of shape `(``batch_size, hidden_dim)`.
  prefs: []
  type: TYPE_NORMAL
- en: The reshaped output is then passed through the first fully connected (linear)
    layer with the `self.fc1(out[:, -1, :])` function call. This layer has 50 neurons,
    so the output shape changes to `(``batch_size, 50)`.
  prefs: []
  type: TYPE_NORMAL
- en: After applying the `ReLU``()` activation function, this output is then passed
    to the second fully connected layer `self.fc2(out)`, which has a size equal to
    `output_dim`, reducing the tensor to the shape `(batch_size, output_dim)`. This
    is the final output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the hidden dimension (`hidden_dim`) is a hyperparameter of the
    LSTM and can be chosen freely. The number of neurons in the first fully connected
    layer (`50`, in this case) is also a hyperparameter and can be modified to suit
    the specific task better.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with hybrid models, consider the following tips:'
  prefs: []
  type: TYPE_NORMAL
- en: Vary the number of fully connected layers and their sizes to explore different
    model complexities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different activation functions in the fully connected layers may lead to varied
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the complexity of the model increases, so does the computational cost. Be
    sure to balance complexity and computational efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Univariate forecasting with a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we turn our attention to convolutional neural networks that have also shown
    promising results with time series data. Let’s learn how these methods can be
    used for univariate time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs are commonly used in problems involving images, but they can also be applied
    to time series forecasting tasks. By treating time series data as a “sequence
    image,” CNNs can extract local features and dependencies from the data. To implement
    this, we’ll need to prepare our time series data similarly to how we did for LSTM
    models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s define a simple CNN model in PyTorch. For this example, we will use a
    single convolutional layer followed by a fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We train and evaluate our model similarly to the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CNN model is based on convolutional layers. These are designed to extract
    local features directly from the input data. These features are then passed to
    one or more fully connected layers that model the future values of the time series.
    The training stage of this type of neural network is similar to others, such as
    the LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through our neural network architecture. It has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: An input layer, which accepts time series data of shape `(batch_size, sequence_length,
    number_of_features)`. For univariate time series forecasting, `number_of_features`
    is `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A convolutional layer with `64` filters and a kernel size of `3`, defined in
    PyTorch as `self.conv1 = nn.Conv1d(in_channels=1,` `out_channels=64, kernel_size=3)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected (or linear) layer that maps the output of the convolutional
    layer to our prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how these layers transform the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(batch_size,` `sequence_length, 1)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Conv1d`: A `1D` convolution is performed over the time series data. The kernel
    slides over the sequence, computing the dot product of the weights and the input.
    After this convolution operation, the shape of our data is `(batch_size, out_channels,
    sequence_length-kernel_size+1)`, or in this case, `(batch_size,` `64, sequence_length-3+1)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(batch_size, remaining_dims)`. `remaining_dims` is calculated by multiplying
    the remaining dimensions of the tensor (`64` and `sequence_length-2` in our case).
    The resulting shape would be `(batch_size, 64 * (sequence_length-2))`. We can
    achieve this by using the `view``()` function in PyTorch as follows: `x =` `x.view(x.size(0),
    -1)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, `x` is ready to be fed into the fully connected layer, `self.fc = nn.Linear(64
    * (sequence_length-2), output_dim)`, where `output_dim` is the dimensionality
    of the output space, `1` for univariate time series prediction. The output of
    this layer is of shape `(batch_size, output_dim)`, or `(batch_size, 1)`, and these
    are our final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This way, we can see how the tensor shapes are handled and transformed as they
    pass through each network layer. Understanding this process is crucial for troubleshooting
    and designing your own architectures.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs can be extended in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple convolutional layers can be stacked to form a deeper network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers can be added after convolutional layers to reduce dimensionality
    and computational cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout or other regularization techniques can be applied to prevent overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model could be extended to a ConvLSTM, which combines the strengths of CNNs
    and LSTMs for handling spatial and temporal dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling trend – taking first differences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), we learned about different
    time series patterns such as trend or seasonality. This recipe describes the process
    of dealing with trend in time series before training a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), trend is the
    long-term change in the time series. When the average value of the time series
    changes, this means that the data is not stationary. Non-stationary time series
    are more difficult to model, so it’s important to transform the data into a stationary
    series.
  prefs: []
  type: TYPE_NORMAL
- en: Trend is usually removed from the time series by taking the first differences
    until the data becomes stationary.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s start by splitting the time series into training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We leave the last 20% of observations for testing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two ways we can compute the difference between consecutive observations
    using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with the standard approach using the `diff()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `periods` argument details the number of steps used to compute the differences.
    In this case, `periods=1` means that we compute the difference between consecutive
    observations, also known as first differences. As an example, setting the number
    of periods to `7` would compute the difference between each observation and the
    observation captured 7 time steps before it. In the case of a daily time series,
    this can be an effective way of removing seasonality. But more on that later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another way to difference a time series is using the `shift()` method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We created a second time series that is shifted by the desired number of periods
    (in this case, `1`). Then, we subtract this series from the original one to get
    a differenced series.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differencing stabilizes the level of the series. Still, we can normalize the
    data into a common value range:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we transform the time series for supervised learning using the `series_to_supervised``()`
    function as in the previous recipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model training phase will remain the same as in the previous recipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'But our job is not done yet. The preceding neural network is trained on differenced
    data. So, the predictions are also differenced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We next need to revert the data transformation processes to get the forecasts
    in the time series' original scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we denormalize the time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we revert the differencing operation by adding back the shifted time
    series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we skip the first three values as they were used during
    the transformation process by the `series_to_supervised()` function.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Differencing works by stabilizing the level of the time series, thus making
    it stationary. Instead of modeling the actual values of the series, the neural
    network models the series of changes; how the time series changes from one time
    step to another. The raw forecasts that come out of the neural network represent
    the predicted changes. We need to revert the differencing process to get the forecasts
    at their original scale.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also deal with trend by including the time information in the input
    data. An explanatory variable that denotes the step at which each observation
    is collected. For example, the first observation has a value of `1`, and the second
    one has a value of `2`. This approach is effective if the trend is deterministic
    and we do not expect it to change. Differencing provides a more general way of
    dealing with trends.
  prefs: []
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal dummies and Fourier series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll describe how to deal with seasonality in time series using
    seasonal dummy variables and a Fourier series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Seasonality represents repeatable patterns that recur over a given period, such
    as every year. Seasonality is an important piece of time series, and it is important
    to capture it. The consensus in the literature is that neural networks cannot
    capture seasonal effects optimally. The best way to model seasonality is by feature
    engineering or data transformation. One way to handle seasonality is to add extra
    information that captures the periodicity of patterns. This can be done with seasonal
    dummies or a Fourier series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by preparing the data using the `series_to_supervised``()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we’ll skip the trend removal part for simplicity and focus on
    modeling seasonality. So, the `train_df` and `test_df` objects contain the lagged
    values of the training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both seasonal dummy variables and a Fourier series can be added to the input
    data as additional explanatory variables. Let’s start by exploring seasonal dummies.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal dummies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Seasonal dummies are binary variables that describe the period of each observation.
    For example, whether a given value is collected on a Monday.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build seasonal dummies, we first get the period information of each point.
    This can be done with the `DateTimeFeatures` class from `sktime` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The main argument for `DateTimeFeatures` is `ts_freq`, which we set to `D`.
    This means that we’re telling this method that our data is in a daily granularity.
    Then, we use the training set to fit a `DateTimeFeatures` object by passing it
    the observations of the first lags of this data (`train_df.iloc[:, -1]`). This
    results in a `pandas` DataFrame that contains the information detailed in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 3.2: Information about the period of each observation](img/B21145_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.2: Information about the period of each observation'
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we’ll continue this recipe by using the information about the
    day of the week and month of the year. We get these columns with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we convert this data into binary variables using a one-hot encoding approach
    from `sklearn` (`OneHotEncoder`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to a set of seasonal dummy variables that are shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 3.3: Information about the period of each observation as binary variables](img/B21145_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.3: Information about the period of each observation as binary variables'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this process using the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note that we fit `DateTimeFeatures` and `OneHotEncoder` on the training data
    (using the `fit_transform()` method). With the test set, we can use the `transform()`
    method from the respective object.
  prefs: []
  type: TYPE_NORMAL
- en: A Fourier series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Fourier series is made up of deterministic sine and cosine waves. The oscillations
    of these waves enable seasonality to be modeled as a repeating pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute Fourier-based features using `sktime` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `FourierFeatures` transformer to extract Fourier features. There
    are two main parameters to this operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sp_list`: The periodicity of the data. In this example, we set this parameter
    to `365.25`, which captures yearly variations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fourier_terms_list`: The number of Fourier waves for each sine and cosine
    function. We set this parameter to `2`, which means we compute `2` sine series
    plus `2` cosine series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After extracting seasonal dummies and the Fourier series, we add the extra
    variables to the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The `np.hstack``()` function is used to merge multiple arrays horizontally (column-wise).
    In this case, we merge the seasonal dummies and the Fourier series with the lagged
    features computed using the `series_to_supervised``()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we feed this data to a neural network as we did in previous recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: When utilizing seasonal dummies or a Fourier series, there is no need to perform
    any additional transformations after the inference step. In the previous code,
    we reversed the normalization process to obtain the forecasts in their original
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Seasonal dummies and the Fourier series are variables that capture the recurrence
    of seasonal patterns. These work as explanatory variables that are added to the
    input data. The cyclical nature of the Fourier series is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Fourier deterministic series that capture seasonality](img/B21145_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Fourier deterministic series that capture seasonality'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this process is independent of the neural network used for training.
    In this recipe, we resorted to a TCN but we could have picked any learning algorithm
    for multiple regression.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative to the Fourier series or seasonal dummies is repeating basis
    functions. Instead of using trigonometric series, seasonality is modeled using
    radial basis functions. These are implemented in the `sklego` `Python` package.
    You can check out the documentation at the following link: [https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction](https://scikit-lego.netlify.app/api/preprocessing.html#sklego.preprocessing.RepeatingBasisFunction).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, a time series can exhibit seasonality at multiple periods. For example,
    the example daily time series can show repeating patterns not only every month
    but also every year. In this recipe, we computed seasonal dummies that provide
    information about different periods, namely the month and day of the week. But
    you can also do this with a Fourier series by passing multiple periods. Here’s
    how you could capture weekly and yearly seasonality with a Fourier series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code would compute `four` Fourier series for each period (`two`
    sine and `two` cosine waves for each).
  prefs: []
  type: TYPE_NORMAL
- en: Another important recurrent phenomenon in time series is holidays, some of which
    move year after year (for example, Easter). A common way to model these events
    is by using binary dummy variables.
  prefs: []
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we show how differencing can be used to model seasonal patterns
    in time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve learned to use first differences to remove the trend from time series.
    Differencing can also work for seasonality. But, instead of taking the difference
    between consecutive observations, for each point, you subtract the value of the
    previous observation from the same season. For example, suppose you’re modeling
    monthly data. You perform seasonal differencing by subtracting the value of February
    of the previous year from the value of February of the current year.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is similar to what we did with first differences to remove the
    trend. Let’s start by loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we’ll use seasonal differencing to remove yearly seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We resort to the `shift``()` method to apply the differencing operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After differencing the series, we transformed it for supervised learning using
    `series_to_supervised`. Then, we can train a neural network with the differenced
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we need to revert the differencing operation to get the forecasts
    in the original scale of the time series. We do that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, we add back the shifted test series to the denormalized predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Seasonal differencing removes periodic variations, thus stabilizing the level
    of the series and making it stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal differencing is particularly effective when the seasonal patterns change
    in magnitude and periodicity. In such cases, it’s usually a better approach than
    seasonal dummies or a Fourier series.
  prefs: []
  type: TYPE_NORMAL
- en: Handling seasonality – seasonal decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe describes yet another approach to modeling seasonality, this time
    using a time series decomposition approach.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned about time series decomposition methods in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019).
    Decomposition methods aim at extracting the individual parts that make up a time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this approach to deal with seasonality. The idea is to separate the
    seasonal component from the rest (trend plus residuals). We can use a deep neural
    network to model the seasonally adjusted series. Then, we use a simple model to
    forecast the seasonal component.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we’ll start with the daily solar radiation time series. This time, we
    won’t split training and testing to show how the forecasts are obtained in practice.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by decomposing the time series using STL. We learned about this method
    in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The seasonally adjusted series and the seasonal component are shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Seasonal part and the remaining seasonally adjusted series](img/B21145_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Seasonal part and the remaining seasonally adjusted series'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we use an LSTM to model the seasonally adjusted series. We’ll use a process
    similar to what we did before in previous recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code trains the LSTM on the seasonally adjusted series. Now,
    we use it to forecast the next 14 days of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we see in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We get the latest `three` lags from the time series and structure it as the
    input data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the model to predict the next value of the series
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we denormalize the forecast using the `scaler` object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to forecast the seasonal component. This is usually done with
    a seasonal naive method. In this recipe, we’ll use the implementation available
    in the `sktime` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `NaiveForecaster` object fits with the seasonal component. The idea of this
    method is to predict future observations using the previous known value from the
    same season.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we get the final forecast by adding the two predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This addition reverts the decomposition process carried out before, and we get
    the forecast in the original series scale.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modeling seasonality with a decomposition approach involves removing the seasonal
    part and modeling the seasonally adjusted time series with a neural network. Another
    simpler model is used to forecast the future of the seasonal part.
  prefs: []
  type: TYPE_NORMAL
- en: This process is different than when using seasonal dummies, a Fourier series,
    or seasonal differencing. Seasonal dummies or a Fourier series work as extra input
    variables for the neural network to model. In the case of decomposition or differencing,
    the time series is transformed before modeling. This means that we need to revert
    these transformations after making the predictions with the neural network. With
    decomposition, this means adding the forecasts of the seasonal part. Differencing
    is also reverted by adding back the previous values from the same season.
  prefs: []
  type: TYPE_NORMAL
- en: Handling non-constant variance – log transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve learned how to deal with changes in the level of the time series that
    occur due to either trend or seasonal patterns. In this recipe, we’ll deal with
    changes in the variance of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve learned in [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019) that some time
    series are heteroscedastic, which means that the variance changes over time. Non-constant
    variance is problematic as it makes the learning process more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by splitting the solar radiation time series into training and
    testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Again, we leave the last 20% of observations for testing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll show how to stabilize the variance of a time series using the logarithm
    transformation and a Box-Cox power transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B21145_01.xhtml#_idTextAnchor019), we defined the `LogTransformation`
    class that applies the logarithm to a time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'You can apply the transformation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The `train_log` and `test_log` objects are the transformed datasets with a stabilized
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: Box-Cox transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The logarithm is often an effective approach to stabilize the variance, and
    is a particular instance of the Box-Cox method. You can apply this method using
    the `boxcox``()` function from `scipy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The Box-Cox method relies on a `lambda` parameter (`bc_lambda`), which we estimate
    using the training set. Then, we use it to transform the test set as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: After transforming the data using either the logarithm or the Box-Cox transformation,
    we train a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process is identical to what we did in the previous recipes. We’ll
    continue the recipe using the transformed series with the logarithm (but the process
    would be the same for the Box-Cox case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we run the model on the test set. The predictions need to be
    reverted to the original scale of the time series. This is done with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'After denormalizing the predictions, we also use the `inverse_transform``()`
    method to revert the log transformation. With the Box-Cox transformation, this
    process could be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we pass the transformed predictions and the `bc_lambda`
    transformation parameter to get the forecasts in the original scale.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process carried out in this recipe attempts to mitigate the problem of non-constant
    variance. Both the logarithm transformation and the Box-Cox method can be used
    to stabilize the variance. These methods also bring the data closer to a `Normal`
    distribution. This type of transformation benefits the training of neural networks
    as it helps avoid saturation areas in the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: The transformation methods work directly on the input data, so they are agnostic
    to the learning algorithm. The models work with transformed data, which means
    that the forecasts need to be transformed back to the original scale of the time
    series.
  prefs: []
  type: TYPE_NORMAL
