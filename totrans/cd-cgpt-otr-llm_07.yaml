- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security Considerations and Measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we shall study the security threats and risks we can open ourselves
    up to by using AI-generated code, specifically code from **Large Language Models**
    ( **LLMs** ), as well as how to guard against these and operate in as safe a way
    as necessary. We need to learn how weaknesses are exploited, even the subtle ones.
    This can help you to plan, be vigilant, deal with threats, and avoid them. We’ll
    get into systems for constant monitoring, effective planning, and collaboration
    with trusted parties.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are extremely useful for many tasks, including generating code for software;
    they can debug, document, comment, and test functions, and even architect entire
    applications. However, they do present a new space for security challenges, one
    that is shifting all the time.
  prefs: []
  type: TYPE_NORMAL
- en: If a single line of AI-generated code could compromise an entire system or a
    prompt could accidentally lead to the exposure of sensitive data, then we have
    to work hard to initially avoid threats and constantly monitor threats too.
  prefs: []
  type: TYPE_NORMAL
- en: Some threats can be highly sophisticated and exploit the very nature of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to get you thinking like a security expert, as well
    as a developer, and anticipate weaknesses before they are exploited.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll not only have the opportunity to understand
    the risks but also feel empowered to harness the full potential of LLMs in your
    development projects while maintaining ironclad security.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the security risks of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing security measures for LLM-powered coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for secure LLM-powered coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the future more secure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll need an internet browser for all the links, patience
    and a good memory for all the security requirements, and your thinking cap for
    what you want the future of AI security to look like, that is all.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the security risks of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we look at the security considerations in AI-assisted programming.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have revolutionized many aspects of software development, from code generation
    to documentation. However, their integration into the development process brings
    new security challenges that developers must understand and address. This section
    explores the security risks associated with LLMs, both in their general use and
    specifically in code generation, providing practical advice for technical professionals
    working in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy and confidentiality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This subsection highlights several threats and weaknesses to be aware of when
    using LLMs in general. The next subsection is about code from LLMs specifically.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are trained on vast amounts of data, and when used, they process user
    inputs that may contain sensitive information. This raises several privacy and
    confidentiality concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Data Leakage** : LLMs might inadvertently reproduce sensitive information
    from their training data. For example, if an LLM was trained on a dataset that
    included private code repositories, it might generate code snippets that are too
    similar to proprietary code [Carlini2021]. This would be something you need to
    check for and correct if you were developing LLMs. However, your data could still
    be copied if you put it in an insecure place. Of course, if you share data or
    code, be sure it’s what you are allowed to share and you won’t be hurt if it is
    copied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input Data Exposure** : When developers use LLMs for tasks such as code completion
    or debugging, they might unknowingly input sensitive information. If the LLM service
    stores these inputs, it could lead to data breaches. Never let any passwords or
    API keys get copied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Inference** : In some cases, it might be possible to infer sensitive
    information about the training data or recent inputs by analyzing the model’s
    outputs. So, if you’re in a company or research group developing LLMs, make sure
    you don’t let too much information get into LLMs in public use, certainly not
    private/sensitive data, including private code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To mitigate these risks, developers should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid inputting sensitive data into public LLM services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LLMs that offer strong privacy guarantees, such as those that don’t store
    user inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be cautious about the information revealed in LLM outputs, especially when sharing
    them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model poisoning and adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model poisoning is an attack on an ML model where an attacker tries to manipulate
    the ML model’s training data, feeding it bad data to learn the wrong thing entirely
    or introduce damaging bias.
  prefs: []
  type: TYPE_NORMAL
- en: Having inherently biased training data may unintentionally cause what amounts
    to model poisoning, a biased model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ways LLMs can be vulnerable to attacks that aim to manipulate
    their behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data poisoning** : If an attacker can influence the training data, they might
    be able to introduce bias or backdoors into the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model extraction** : Through careful querying, an attacker might be able
    to reconstruct parts of the model or its training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial inputs** : Carefully crafted inputs can sometimes cause LLMs
    to produce unexpected or harmful outputs [ *NIST2023* ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developers should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement input validation and sanitization before passing data to LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor LLM outputs for unexpected behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always test any code from LLMs for robustness to attacks by exposing it to the
    sorts of actions that attackers may use but do this in a safe environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt injection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompt injection is a technique where an attacker crafts input that manipulates
    the LLM into performing unintended actions or revealing sensitive information
    [ *Kang2023* ]. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To prevent prompt injection, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement strict input validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use role-based prompts to constrain the LLM’s behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using a separate LLM instance for user inputs to isolate potential
    attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attackers might attempt to manipulate LLM outputs to produce malicious content
    or misleading information. This is particularly dangerous in code generation scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mitigation strategies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Always review and validate LLM-generated content before use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement output filtering to catch known malicious patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use multiple LLMs or cross-reference outputs for critical tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know things to be aware of when using LLMs, we’ll move onto code,
    as this book is primarily for you, the software developer, software engineer,
    or coder.
  prefs: []
  type: TYPE_NORMAL
- en: Security risks in LLM-generated code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This subsection is specifically about code from LLMs: threats and weaknesses.'
  prefs: []
  type: TYPE_NORMAL
- en: Code vulnerabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs might generate code that contains security vulnerabilities, either due
    to limitations in their training data or because they don’t fully understand the
    security implications of certain coding practices [ *Pearce2022* ].
  prefs: []
  type: TYPE_NORMAL
- en: 'Common issues include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SQL injection vulnerabilities** : SQL injection is a very common web hacking
    method where malicious code is put into SQL statements and can destroy a database
    [ *W3Schools* ]!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-site scripting (XSS) vulnerabilities** : Cross-site scripting returns
    malicious JavaScript to site users and takes over the interaction with the application.
    This enables the attacker to pretend to be a user, thus being able to access their
    data. If the user is a privileged user the attacker may be able to take full control
    of the application [ *PortSwigger* ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insecure cryptographic implementations** : If you’re storing passwords or
    other sensitive data in plain text, using MD5, SHA1, or using short or weak encryption
    keys, you’re using insecure methods [ *Aakashyap* ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developers should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Always review and test LLM-generated code thoroughly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use static analysis tools to catch common vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never use generated code in production without proper security auditing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insecure coding practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs might suggest or generate code that follows outdated or insecure coding
    practices. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem with the code is that it uses **hashlib.md5** for password hashing.
    Here’s why this is insecure: **MD5 is not secure** . MD5 is a cryptographic hash
    function, but it’s no longer considered secure for password hashing. It’s vulnerable
    to collision attacks, where an attacker can find another input that generates
    the same hash as your password. This could allow them to crack your password.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several better options for password hashing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**bcrypt** : This is a popular and secure password hashing algorithm. It uses
    a technique called **key stretching** , which makes it computationally expensive
    to brute-force passwords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scrypt** : Similar to bcrypt, scrypt is another secure key derivation function
    that’s resistant to brute-force attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Argon2** : A newer and highly secure password hashing function that’s becoming
    increasingly popular.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code doesn't use a **salt:** The code doesn’t use a salt. A salt is a random
    value added to the password before hashing. This prevents attackers from pre-computing
    rainbow tables to quickly crack passwords. Each user should have a unique salt
    for their password.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an improved version of the code, using **bcrypt** , in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Always test the code from this book and use with caution.
  prefs: []
  type: TYPE_NORMAL
- en: This code generates a random salt and uses it to hash the password. The resulting
    hash can then be stored securely in your database [ *Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example of an **insecure** coding practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What’s wrong with this code?
  prefs: []
  type: TYPE_NORMAL
- en: '**Unvalidated user input** : This code directly processes the user input from
    the web form using **request.args.get("data")** . This is insecure because attackers
    can inject malicious code into the user input field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insecure logic based on user input** : The code checks if the user input
    is exactly **"admin"** and grants admin privileges if so. An attacker could easily
    manipulate the input to bypass this check and gain unauthorized access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s some safer and improved code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The improvements from the earlier code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input validation and sanitization** : The improved code introduces a **sanitize_input**
    function. This function should remove potentially malicious characters from the
    user input before it’s processed. Techniques such as escaping special characters
    or using whitelists can be used for sanitization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization checks** : The code now checks for proper user authentication
    using **is_authenticated_admin** before granting admin privileges. This ensures
    that only authorized users can access admin functionalities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure coding practices** : This example highlights the importance of secure
    coding practices such as input validation and proper authorization checks to prevent
    vulnerabilities such as injection attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember to always stay updated on secure coding practices and compare them
    with LLM outputs. Use code linters and security scanners to catch insecure patterns.
    You should also provide the LLM with explicit instructions about required security
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: Intellectual property concerns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using LLMs for code generation, there’s a risk of inadvertently incorporating
    copyrighted code or violating open source licenses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practices include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Carefully review generated code for similarities to known code bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use plagiarism detection tools on generated code. Here are some code plagiarism
    checkers: [https://www.duplichecker.com/](https://www.duplichecker.com/) and [https://www.check-plagiarism.com/](https://www.check-plagiarism.com/)
    . These are both free with no sign-up needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain clear documentation of which parts of your code base were assisted
    by LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered the legal concerns and best practices in much greater depth in
    [*Chapter 6*](B21009_06.xhtml#_idTextAnchor137) .
  prefs: []
  type: TYPE_NORMAL
- en: LLMs offer powerful capabilities for software development, but they also introduce
    new security risks that must be carefully managed. By understanding these risks
    and implementing best practices, developers can harness the benefits of LLMs while
    maintaining the security and integrity of their software systems.
  prefs: []
  type: TYPE_NORMAL
- en: As the field of AI and LLMs continues to evolve rapidly, it’s crucial to stay
    informed about new developments, both in terms of capabilities and potential vulnerabilities.
    Regular training, updating of practices, and ongoing vigilance are essential for
    maintaining security in an LLM-assisted development environment [ *OWASP2023*
    ].
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of LLMs/chatbots to generate code could introduce risks just because
    of the huge volume of code that developers can now produce thanks to LLMs. This
    high volume of code needs careful testing for flaws and vulnerabilities as well
    as securing. If the code isn’t high enough quality, it will lead to code needing
    to be corrected, with downtime, and will open up companies and people to threats.
    That is, according to Ian Barker of Beta News [BetaNews: “AI-generated code could
    increase developer workload and add to risk”, Ian Barker, [https://betanews.com/2024/06/14/ai-generated-code-could-increase-developer-workload-and-add-to-risk](https://betanews.com/2024/06/14/ai-generated-code-could-increase-developer-workload-and-add-to-risk)
    ]. I dare say it’d cause embarrassment and lost earnings too.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are aware of the threats and weaknesses of using LLMs for code as
    well as using them in general, let’s learn about how to put in effective practices
    to mitigate problems and secure the code as well as the developers and users,
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing security measures for LLM-powered coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we integrate LLMs into our development workflows, it’s crucial to implement
    robust security measures. These measures will help ensure that our LLM-assisted
    code is ready for real-world deployment. Let’s explore key areas of focus and
    practical steps to enhance security in LLM-powered coding environments.
  prefs: []
  type: TYPE_NORMAL
- en: Here are seven measures that should be taken to get more secure code.
  prefs: []
  type: TYPE_NORMAL
- en: Input sanitization and validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using LLMs for code generation or completion, it’s important to sanitize
    and validate all inputs, both those provided to the LLM and those generated by
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Validation is where the data is checked to make sure it’s correct/accurate before
    processing or using it. Sanitization is where the data is cleaned, where parts
    that could be dangerous are removed or changed enough that they’re not dangerous
    [ *NinjaOne, Informatica* ].
  prefs: []
  type: TYPE_NORMAL
- en: Before passing any input to an LLM, validate it against a predetermined set
    of rules. This helps prevent injection attacks and ensures that only expected
    input types are processed.
  prefs: []
  type: TYPE_NORMAL
- en: When generating database queries or similar sensitive operations, use parameterized
    queries to separate data from commands, reducing the risk of SQL injection or
    similar attacks. Parameterized queries are queries that don’t use the user queries
    or values directly but instead use parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters are values passed into a function to control their behavior. So,
    instead of using the user name directly, such as **"Derrick"** , the parameterized
    version would have the parameter, **username_parameter = "Derrick"** . This way,
    any harmful values cannot perform harmful actions on the code but are kept in
    little wrappers [ *Dbvis* ].
  prefs: []
  type: TYPE_NORMAL
- en: The parameterization is the same method across all programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re using code from LLMs, treat it as untrusted. Make sure you correctly
    use sanitization to remove or escape any potentially harmful elements before putting
    them into your code base [OWASP_validation].
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more, see [https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html](https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Secure integration patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating LLMs into your development pipeline means you have to carefully
    put security in at every step.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you can do in the pipeline is to run the LLM-powered tools in
    their own isolated environments to limit damage from malicious outputs. Containerization
    tools such as Docker, Kubernetes, LXD by Canonical, Azure Container Instances,
    and Google Cloud Run can be used to create secure, isolated environments for using
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t give LLMs access to all of your data and software. Make sure your LLM-powered
    tools and the environments they run in have only the absolute minimum permissions
    they need to function. This will limit the impact of any security breaches.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re using LLMs through APIs, use strong and secure authentication and
    authorization mechanisms. That means you should use API keys, OAuth, or other
    secure methods to control access to LLM resources. Another method is org management,
    where only those who need to have access to data and systems get those permissions
    and powers. Everybody in the organization, including contractors and other temporary
    staff, should have the minimum required access to do their work. This is called
    access control. Nobody should have the power to change everything or let an LLM
    have power over everything. That would risk everything.
  prefs: []
  type: TYPE_NORMAL
- en: The resources each person accesses should be logged. That brings us to the next
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Sources such as [ *MSTechCommun* ] can help here.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing comprehensive monitoring and logging systems is really important
    for identifying and responding to security issues in LLM-powered coding environments!
    In fact, it’s crucial for any organization’s software, data, and coding environments.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations should set up and carry out detailed logging of all LLM interactions
    – including inputs, outputs, and metadata – ensuring logs are stored securely
    and tamper-proof to allow forensic analysis if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Your organization should use automated static and dynamic code analysis tools,
    as they can scan LLM-generated code for any vulnerabilities or compliance issues.
    Static analysis tools analyze code without running it. If it’s not run, it cannot
    execute malicious functions.
  prefs: []
  type: TYPE_NORMAL
- en: Also, employing machine learning-based anomaly detection systems helps identify
    unusual patterns in LLM usage or outputs that might suggest a security threat.
    This is done by training with normal and expected LLM output and looking for outliers
    or anomalies. Anomaly detection methods include autoencoders, isolation forests,
    **long short-term memories** ( **LSTMs** ), and one-class **support vector** **machines**
    ( **SVMs** ).
  prefs: []
  type: TYPE_NORMAL
- en: This work is usually done by **system administrators** ( **sysadmins** ), DevOps
    engineers, security analysts, data engineers, site reliability engineers, and
    specialized logging and monitoring teams.
  prefs: []
  type: TYPE_NORMAL
- en: Follow the standards and regulations in the countries and jurisdictions your
    organization operates. ISO 27001 or IEC 27001 is an international standard for
    **information security management systems** ( **ISMSs** ) ( [https://www.iso.org/standard/27001](https://www.iso.org/standard/27001)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: NIST Cybersecurity Framework was developed by the **National Institute of Standards
    and Technology** ( **NIST** ) and it provides a set of cybersecurity standards,
    including recommendations for data logging and monitoring ( [https://www.nist.gov/cyberframework](https://www.nist.gov/cyberframework)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: '**GDPR** is the **General Data Protection Regulation** , a European Union law
    that sets strict rules for the processing of personal data, including data logging
    and retention ( [https://gdpr-info.eu/](https://gdpr-info.eu/) ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources: TechMagic, Wiki_ISO, Wiki_prog_analysis, Liu_2024, Gemini, Llama 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Version control and traceability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maintaining a clear record of code changes and their origins is imperative when
    working with LLM-generated code.
  prefs: []
  type: TYPE_NORMAL
- en: When you work with LLMs, you must adapt your version control system to specifically
    tag or annotate code segments generated by LLMs. This practice, called **LLM-aware
    version control** , helps you track the origin of the code and can be crucial
    for audits or when addressing potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical practice is code signing. You use digital certificates to sign
    software and code, ensuring its integrity and authenticity throughout your development
    and deployment pipeline. If the certificate is invalidated, it indicates that
    someone has tampered with the code.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it’s beneficial to implement automated tools that can continuously
    monitor and verify the integrity of your code base. These tools can alert you
    to any unauthorized changes, providing an extra layer of security.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you should maintain detailed audit trails of LLM usage in your development
    process. This includes keeping records of which models you used, when you used
    them, and for what purposes. These records help in compliance and auditing and
    in understanding the performance and behavior of different models over time.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also advisable to regularly review and update your security practices to
    keep up with evolving threats and advancements in technology. Staying informed
    about the latest security trends and best practices can help you better protect
    your data and systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Useful sources: [ *EncryptionConsulting, Gemini, GitHubLFS,* *Copilot, Nexthink*
    ].'
  prefs: []
  type: TYPE_NORMAL
- en: Encryption and data protection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with LLMs, it’s essential to protect sensitive data both while
    it’s being transferred and when it’s stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'First off, make sure to use strong end-to-end encryption for all data transfers
    between your systems and LLM services, especially if you’re using cloud-based
    LLMs. In 2024, strong encryption typically means using at least 128-bit encryption,
    usually 256-bit: AES-256 or XChaCha20.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, any data stored by LLM systems, including caches of generated code or
    user inputs, should be encrypted and safeguarded against unauthorized access.
    This ensures that even if someone manages to get their hands on the data, they
    won’t be able to read it.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it’s a good idea to follow the principle of data minimization when interacting
    with LLMs. This means only providing the minimum amount of context or data necessary
    for the LLM to do its job. By doing this, you reduce the risk of exposing sensitive
    information. Sources to help [ *Eyer, StineDang,* *ICOMini, Esecurityplanet* ].
  prefs: []
  type: TYPE_NORMAL
- en: Regular security assessments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maintaining a strong and reliable security position for your LLM-powered coding
    environment is paramount in today’s increasingly complex threat landscape. By
    implementing a comprehensive security strategy, you can reduce risks, protect
    sensitive data, and ensure the integrity of your operations.
  prefs: []
  type: TYPE_NORMAL
- en: Regularly conduct penetration tests (pen tests) focused on where you’ve inserted
    LLM-gen code to identify potential vulnerabilities. Employ a variety of attack
    techniques to simulate real-world threats and uncover hidden weaknesses. If you’ve
    not got time to learn how to do security well, consider asking external security
    experts for an objective assessment or recruit a cybersecurity expert .
  prefs: []
  type: TYPE_NORMAL
- en: Perform thorough security audits of your entire LLM-powered development pipeline,
    including third-party LLM services, to identify and address potential risks. Prioritize
    vulnerabilities based on their severity and potential impact, and implement ongoing
    security monitoring to detect and respond to emerging threats. You can’t humanly
    catch all threats, so automated systems must be continually operating.
  prefs: []
  type: TYPE_NORMAL
- en: Conduct regular security vulnerability scans of your code base, paying particular
    attention to components that interact with or are generated by LLMs. Prioritize
    addressing critical vulnerabilities first to minimize risk and integrate vulnerability
    scanning into your development life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Implement robust API security measures to protect your LLM integration points
    from unauthorized access and data breaches. Ensure that sensitive data is handled
    securely when interacting with LLMs. Validate user input to prevent injection
    attacks and other weaknesses. API security measures include OAuth, storing and
    using API keys carefully, token-based authentication, checking that data types
    are as expected, input sanitization, and rate limiting the requests to prevent
    DoS attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Use advanced security monitoring tools to detect anomalies and potential threats
    in real time. Keep your LLM and associated software up to date with the latest
    security patches and updates. This must be done frequently. Conduct regular security
    reviews to assess the effectiveness of your security measures and identify areas
    for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: By following these guidelines, you can significantly enhance the security of
    your LLM-powered coding environment and protect your organization from potential
    threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'These sources can help: [https://www.linkedin.com/pulse/safeguard-your-ai-llm-penetration-testing-checklist-based-smith-nneac/](https://www.linkedin.com/pulse/safeguard-your-ai-llm-penetration-testing-checklist-based-smith-nneac/)
    , [https://infinum.com/blog/code-audit/](https://infinum.com/blog/code-audit/)
    , [https://docs.github.com/en/code-security/code-scanning](https://docs.github.com/en/code-security/code-scanning)
    , [https://www.nist.gov/cyberframework](https://www.nist.gov/cyberframework) .'
  prefs: []
  type: TYPE_NORMAL
- en: Incident response planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite our best efforts, security incidents can still occur. When they do,
    having a robust incident response plan is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: For LLM-powered environments, develop and maintain specific procedures to address
    security issues arising from LLM use, such as data leaks or malicious code injection.
  prefs: []
  type: TYPE_NORMAL
- en: Implement mechanisms to quickly roll back changes to use earlier versions or
    disable LLM-powered features in the event of a detected security issue. This all
    helps to contain the damage and prevent further exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: Establish clear communication protocols/procedures for reporting and addressing
    LLM-related security incidents, including notifying affected parties if need be.
    Quick and clear communication ensures a swift and coordinated response to minimize
    the impact of incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Under the GDPR, organizations have 72 hours from the time they become aware
    of a data breach to report it to the relevant supervisory authority. This requirement
    applies to breaches that are likely to result in a high risk to individuals’ rights
    and freedoms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sources you may find useful on these topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Developing incident response procedures: [https://www.ncsc.gov.uk/collection/incident-management/cyber-incident-response-processes](https://www.ncsc.gov.uk/collection/incident-management/cyber-incident-response-processes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rollback: [https://www.linkedin.com/advice/0/what-tools-techniques-you-use-automate-streamline](https://www.linkedin.com/advice/0/what-tools-techniques-you-use-automate-streamline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incident response: [https://www.sans.org/white-papers/1516/](https://www.sans.org/white-papers/1516/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM security in general: [https://www.tigera.io/learn/guides/llm-security/](https://www.tigera.io/learn/guides/llm-security/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you put in place these security measures, you can greatly enhance the safety
    and reliability of your LLM-powered coding environments. Remember, security is
    an ongoing process, and these measures should be regularly reviewed and updated
    to address emerging threats and changes in LLM technology.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, LLMs such as Claude and Gemini can help too.
  prefs: []
  type: TYPE_NORMAL
- en: Bonus – training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Get your employees comprehensive security training, including best practices
    for handling sensitive data and recognizing potential threats. Of course, educate
    employees about phishing scams and other social engineering tactics to prevent
    unauthorized access, and develop an incident response plan to address security
    breaches effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Phishing, vishing (voice calls), smishing (SMSing or texting), impersonation,
    tailgating, baiting (offering rewards), blackmail, and other social engineering
    methods are used by scammers to circumvent the automated security systems and
    exploit human kindness, desires, and mistakes to get access to otherwise secure
    and sensitive information and systems. These can be a lot more effective than
    hacking the software and hardware. Everybody needs to be trained on how to handle
    these threats.
  prefs: []
  type: TYPE_NORMAL
- en: That is a lot to get through but implementing some of these will make you and
    your systems more secure. Every added measure will put you in a better position,
    now and in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Who can help here?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might look at all the requirements and wonder how to get through all of
    this!
  prefs: []
  type: TYPE_NORMAL
- en: Rather than trying to manage it all by yourself, you might decide to ask the
    specialists in this field for help. There are cyber security companies for just
    these purposes.
  prefs: []
  type: TYPE_NORMAL
- en: You or your organization might need to know more about cyber security, data
    protection, information security, penetration testing (pen testing), cyber testing,
    vulnerability scanning, GDPR and DSAR, auditing, risk management, gap analysis,
    and/or business continuity.
  prefs: []
  type: TYPE_NORMAL
- en: Gap analysis is when the experts look at the areas where an organization doesn’t
    meet the standards, that is, is not compliant.
  prefs: []
  type: TYPE_NORMAL
- en: Business continuity is making sure all the essential processes and procedures
    of a company are able to keep running during and after a disaster or other disruption.
    The ISO standard for business continuity is ISO 22301.
  prefs: []
  type: TYPE_NORMAL
- en: Other ISO standards relevant in this chapter are ISO 42001 for AI technologies,
    ISO 27001 for ISMSs, and ISO 9001 for QMS, quality management systems.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll most likely need a **data protection** **officer** ( **DPO** ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a link to a cyber security experts’ site, where they consult and also
    train on all of the above: [https://www.urmconsulting.com/](https://www.urmconsulting.com/)
    . Tell them that ABT NEWS LTD sent you.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at how to implement ways to secure our code and stay
    as safe as we can, let’s learn about the best practices in AI-generated code security
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for secure LLM-powered coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a lot of measures that need to be implemented to keep our systems
    secure when using LLM-generated code or AI-generated code or any code. This section
    is a summary of some best practices for code security, especially with LLM-generated
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Treat LLMs as untrusted input** : Always validate and sanitize both the inputs
    to and outputs from LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implement least privilege** : When integrating LLMs into your development
    pipeline, ensure they have access only to the minimum necessary resources and
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control** : Grant access to only authorized users, limiting the effects
    of LLMs to what your organization can control and monitor carefully. This is important
    from the point of view of the developers and the end users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API security** : When LLMs communicate with other systems, ensure secure
    communication channels to prevent unauthorized access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encryption** : Encrypt end-to-end when communicating with LLMs (or transferring
    data anywhere).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data minimization** : Only give LLMs the minimum data they need to help you
    deliver the solutions you need; don’t give sensitive data to public LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use version control** : Keep track of all code changes, including those suggested
    by LLMs, to maintain accountability and enable rollbacks if issues are discovered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous security testing** : Regularly perform security assessments on
    your code base, including parts generated or influenced by LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education and training** : Ensure your development team understands the risks
    and best practices associated with LLM usage in programming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish clear policies** : Develop and enforce policies on how LLMs should
    be used in your development process, including what types of tasks they can be
    used for and what data can be input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor and audit** : Implement logging and monitoring for LLM usage to detect
    potential misuse or security issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stay up to date** : This is a continuous process. The world’s technology
    and the actions of criminals don’t stop advancing and adapting. So, you have to
    keep on your toes, rather make software systems that keep on their toes, of course.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research resilience** : Invest in research on adversarial training methods
    and robust model architectures to enhance LLM resilience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sources: [Gemini, Codecademy, Claude]'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned and summarized the best practices for securing LLM-generated
    code, let’s learn about how to keep this security, robustness, and wisdom going
    for the long term in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making the future more secure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading the previous sections, you’ll be aware of many of the risks and
    threats, and once your organization has implemented many or all of the security
    measures around AI-generated code, I wouldn’t blame you if you wanted to provide
    cybersecurity solutions to others or move your career in that direction. Every
    problem is a business opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: Even if that isn’t your plan, you might want to think about the long-term future
    of AI-generated code security. Here, we think about how to remain secure as technologies,
    regulations, and times change.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an overview of potential future threats and how organizations can prepare.
  prefs: []
  type: TYPE_NORMAL
- en: Emerging threats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s something called a zero-day threat. This is an unknown threat, so no
    patch exists. New, unforeseen vulnerabilities might emerge in LLMs or their generated
    code, potentially getting past traditional security solutions. Implementing continuous
    monitoring with advanced threat detection tools and conducting regular security
    audits can help identify and address zero-day vulnerabilities promptly.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs could be used to create more convincing phishing attempts or deepfakes.
    To manage these threats, include emerging threats in your staff education programs
    and install advanced detection methods for AI-generated content. So, AI, including
    LLMs, can be used to harm and to help us.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting focus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating security considerations into the design and development of LLMs
    from the outset can significantly enhance their resilience. Invest in training
    and education for developers on secure AI development practices and work towards
    standardizing security best practices for LLM development.
  prefs: []
  type: TYPE_NORMAL
- en: Combining human expertise with AI-powered tools can create a more comprehensive
    security approach. Train your workforce on AI and security, and encourage a culture
    of collaboration between AI specialists and security professionals.
  prefs: []
  type: TYPE_NORMAL
- en: Stay informed about ever-changing government and industry regulations for LLM
    development and use. Consider participating in discussions and contributing to
    the development of future LLM security standards.
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to develop AI systems that can explain their decision-making processes
    to enhance security auditing and incident response. Organizations can invest in
    research and the implementation of explainable AI techniques in their LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: You can also look into decentralized AI training methods such as federated learning
    to improve data privacy and security. Research and pilot federated learning approaches
    for your AI development processes.
  prefs: []
  type: TYPE_NORMAL
- en: You could even attempt to prepare for potential threats from quantum computing,
    which is developing nicely now, by beginning to explore and implement quantum-resistant
    encryption methods for AI systems and data protection. Quantum computers aren’t
    good at everything, only specific tasks, such as cracking code made with our current
    widely used encryption methods. While these are very proactive approaches and
    measures, by considering these potential, even likely future challenges and actively
    preparing for them, organizations can help ensure that AI remains a good investment
    and generally beneficial for your organization, while mitigating unforeseen security
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should now be aware of AI-generated code and
    LLM security risks, vulnerabilities, and threats. You have the chance to learn
    more about some methods unethical hackers use to attack organizations such as
    yours, and specifically how LLMs can introduce risks.
  prefs: []
  type: TYPE_NORMAL
- en: There is more in here about data privacy, intellectual property concerns, how
    to put security measures into place, how to keep monitoring for threats, what
    audits are needed, how to always be ready, and how to plan and collaborate for
    more security in LLM-generated code. You’ve got links to follow and cyber security
    experts to consult. This chapter ended by prompting you to think and know how
    to start preparing for future risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B21009_08.xhtml#_idTextAnchor203) , we will be examining the
    limitations of coding with LLMs: inherent limitations, challenges with integrating
    LLMs into coding workflows, and future research directions to address limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Aakashyap* : “Insecure Cryptographic Storage”, Aakashyap, [https://medium.com/@aakashyap_42928/insecure-cryptographic-storage-fe5d40d10765](mailto:https://medium.com/@aakashyap_42928/insecure-cryptographic-storage-fe5d40d10765)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Carlini2021* : “Extracting Training Data from Large Language Models”, N. Carlini,
    et al., USENIX Security Symposium, [https://arxiv.org/abs/2012.07805](https://arxiv.org/abs/2012.07805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Claude* : “Claude 3.5 Sonnet”, Anthropic, [https://claude.ai/](https://claude.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Codecademy* : “LLM Data Security Best Practices”, Codecademy Team, [https://www.codecademy.com/article/llm-data-security-best-practices](https://www.codecademy.com/article/llm-data-security-best-practices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Copilot* : Microsoft Copilot, Microsoft, [https://copilot.microsoft.com/?dpwa=1](https://copilot.microsoft.com/?dpwa=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dbvis* : “Parameterized Queries in SQL – A Guide”, Lukas Vileikis, [https://www.dbvis.com/thetable/parameterized-queries-in-sql-a-guide](https://www.dbvis.com/thetable/parameterized-queries-in-sql-a-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*EncryptionConsulting* : “What is Code Signing? How does Code Signing work?”,
    [https://www.encryptionconsulting.com/education-center/what-is-code-signing/](https://www.encryptionconsulting.com/education-center/what-is-code-signing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Eyer* : “10 Endpoint Encryption Best Practices 2024”, _EYER, [https://eyer.ai/blog/10-endpoint-encryption-best-practices-2024/](https://eyer.ai/blog/10-endpoint-encryption-best-practices-2024/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Esecurityplanet* : “Strong Encryption Explained: 6 Encryption Best Practices”,
    Chad Kime, [https://www.esecurityplanet.com/networks/strong-encryption/](https://www.esecurityplanet.com/networks/strong-encryption/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gemini* : Gemini 1.5, Google, [https://gemini.google.com](https://gemini.google.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GitHubLFS* : “Configuring Git Large File Storage”, GitHub, [https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage](https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ICOMini* : Information Commissioners Office, “Principle (c): Data minimisation”,
    ico, [https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/the-principles/data-minimisation/](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/the-principles/data-minimisation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Informatica: “What is Data Validation?”, Informatica Inc., [https://www.informatica.com/gb/services-and-training/glossary-of-terms/data-validation-definition.html](https://www.informatica.com/gb/services-and-training/glossary-of-terms/data-validation-definition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang2023: “Prompt Injection Attacks and Defenses in Large Language Models”,
    D. Kang, et al., [https://arxiv.org/abs/2306.05499](https://arxiv.org/abs/2306.05499)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu_2024: “Using the Logger Class in Python for Effective Logging”, Luca Liu,
    [https://luca1iu.medium.com/using-the-logger-class-in-python-for-effective-logging-23b75a6c3a45](https://luca1iu.medium.com/using-the-logger-class-in-python-for-effective-logging-23b75a6c3a45)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Llama 3: Llama 3, Meta and Ollama, [https://ollama.com/library/llama3](https://ollama.com/library/llama3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MSTechCommun: “Integrating AI: Best Practices and Resources to Get Started
    with Azure Cognitive Services” Aysegul Yonet, [https://techcommunity.microsoft.com/t5/apps-on-azure-blog/integrating-ai-best-practices-and-resources-to-get-started-with/ba-p/2271522](https://techcommunity.microsoft.com/t5/apps-on-azure-blog/integrating-ai-best-practices-and-resources-to-get-started-with/ba-p/2271522)
    ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nexthink: “Audit trail codes”, nexthink, [https://docs.nexthink.com/platform/latest/audit-trail](https://docs.nexthink.com/platform/latest/audit-trail)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NinjaOne: “What Is Input Sanitization?”, Makenzie Buenning, [https://www.ninjaone.com/it-hub/endpoint-security/what-is-input-sanitization](https://www.ninjaone.com/it-hub/endpoint-security/what-is-input-sanitization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NIST2023: “Adversarial Machine Learning: A Taxonomy and Terminology of Attacks
    and Mitigations”, Apostol Vassilev, Alina Oprea, Alie Fordyce, Hyrum Anderson,
    [https://site.unibo.it/hypermodelex/en/publications/15-2024-01-nist-adversarial.pdf/@@download/file/15-2024-01-NIST-ADVERSARIAL.pdf](mailto:https://site.unibo.it/hypermodelex/en/publications/15-2024-01-nist-adversarial.pdf/@@download/file/15-2024-01-NIST-ADVERSARIAL.pdf)
    , [https://doi.org/10.6028/NIST.AI.100-2e2023](https://doi.org/10.6028/NIST.AI.100-2e2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OWASP2023: “OWASP Top 10 for Large Language Model Applications”, OWASP Foundation,
    [https://owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OWASP_validation: Input Validation Cheat Sheet”, Cheat Sheets Series Team,
    [https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html](https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pearce2022: “Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s
    Code Contributions”, H. Pearce, et al., IEEE Symposium on Security and Privacy
    ( SP), [https://arxiv.org/abs/2108.09293](https://arxiv.org/abs/2108.09293)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PortSwigger: “Cross-site scripting”, Port Swigger, [https://portswigger.net/web-security/cross-site-scripting](https://portswigger.net/web-security/cross-site-scripting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StineDang: “Journal of AHIMA (American Health Information Management Association)
    Encryption Basics”, Kevin Stine, Quynh Dang, [https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=908084](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=908084)
    ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TechMagic: “AI Anomaly Detection: Best Tools And Use Cases”, Victoria Shutenko,
    [https://www.techmagic.co/blog/ai-anomaly-detection/](https://www.techmagic.co/blog/ai-anomaly-detection/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'W3Schools: “SQL Injection”, w3Schools, [https://www.w3schools.com/sql/sql_injection.asp](https://www.w3schools.com/sql/sql_injection.asp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weidinger2021: “Ethical and social risks of harm from Language Models”, L.
    Weidinger, et al., [https://arxiv.org/abs/2112.04359](https://arxiv.org/abs/2112.04359)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wiki_ISO: “ISO/IEC 27001”, various, [https://en.wikipedia.org/wiki/ISO/IEC_27001](https://en.wikipedia.org/wiki/ISO/IEC_27001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wiki_prog_analysis: “Static program analysis”, Wikipedia, [https://en.wikipedia.org/wiki/Static_program_analysis](https://en.wikipedia.org/wiki/Static_program_analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Explainability, Shareability, and the Future of LLM-Powered Coding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section highlights the importance of making LLM-generated code clear, collaborative,
    and adaptable. We will cover various techniques for refining code to improve performance
    and maintainability. We will also learn how to enhance code clarity, ensuring
    that it’s understandable to collaborators and future users. Lastly, we will learn
    the importance of a shared learning environment and check out strategies for effective
    teamwork and knowledge exchange within LLM-assisted development.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section covers the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21009_08.xhtml#_idTextAnchor203) *,* *Limitations of Coding
    with LLMs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21009_09.xhtml#_idTextAnchor225) *,* *Cultivating Collaboration
    in LLM-Enhanced Coding*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21009_10.xhtml#_idTextAnchor249) *,* *Expanding the LLM Toolkit
    for Coders: Beyond LLMs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
