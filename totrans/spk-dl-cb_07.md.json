["```py\nspark = SparkSession.builder \\\n        .master(\"local\") \\\n        .appName(\"Natural Language Processing\") \\\n        .config(\"spark.executor.memory\", \"6gb\") \\\n        .getOrCreate()\ndf = spark.read.format('com.databricks.spark.csv')\\\n     .options(header='true', inferschema='true')\\\n     .load('TherapyBotSession.csv')  \n```", "```py\ndf = df.select('id', 'label', 'chat')\ndf.show()\n```", "```py\nimport pyspark.sql.functions as F\n```", "```py\ndf.groupBy(\"label\") \\\n   .count() \\\n   .orderBy(\"count\", ascending = False) \\\n   .show()\n```", "```py\nimport pyspark.sql.functions as F\ndf = df.withColumn('word_count', F.size(F.split(F.col('response_text'),' ')))\n```", "```py\ndf.groupBy('label')\\\n  .agg(F.avg('word_count').alias('avg_word_count'))\\\n  .orderBy('avg_word_count', ascending = False) \\\n  .show()\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\ndf_plot = df.select('id', 'word_count').toPandas()\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf_plot.set_index('id', inplace=True)\ndf_plot.plot(kind='bar', figsize=(16, 6))\nplt.ylabel('Word Count')\nplt.title('Word Count distribution')\nplt.show()\n```", "```py\nfrom pyspark.sql.types import FloatType \n```", "```py\nfrom textblob import TextBlob\n```", "```py\nfrom textblob import TextBlob\ndef sentiment_score(chat):\n    return TextBlob(chat).sentiment.polarity\n```", "```py\nfrom pyspark.sql.types import FloatType\nsentiment_score_udf = F.udf(lambda x: sentiment_score(x), FloatType())\n```", "```py\ndf = df.select('id', 'label', 'chat','word_count',\n                   sentiment_score_udf('chat').alias('sentiment_score'))\n```", "```py\ndf.groupBy('label')\\\n     .agg(F.avg('sentiment_score').alias('avg_sentiment_score'))\\\n     .orderBy('avg_sentiment_score', ascending = False) \\\n     .show()\n```", "```py\nfrom pyspark.ml.feature import StopWordsRemover \nfrom pyspark.ml import Pipeline\n```", "```py\ndf = df.withColumn('words',F.split(F.col('chat'),' '))\n```", "```py\nstop_words = ['i','me','my','myself','we','our','ours','ourselves',\n'you','your','yours','yourself','yourselves','he','him',\n'his','himself','she','her','hers','herself','it','its',\n'itself','they','them','their','theirs','themselves',\n'what','which','who','whom','this','that','these','those',\n'am','is','are','was','were','be','been','being','have',\n'has','had','having','do','does','did','doing','a','an',\n'the','and','but','if','or','because','as','until','while',\n'of','at','by','for','with','about','against','between',\n'into','through','during','before','after','above','below',\n'to','from','up','down','in','out','on','off','over','under',\n'again','further','then','once','here','there','when','where',\n'why','how','all','any','both','each','few','more','most',\n'other','some','such','no','nor','not','only','own','same',\n'so','than','too','very','can','will','just','don','should','now']\n```", "```py\nfrom pyspark.ml.feature import StopWordsRemover \n\nstopwordsRemovalFeature = StopWordsRemover(inputCol=\"words\", \n                   outputCol=\"words without stop\").setStopWords(stop_words)\n```", "```py\nfrom pyspark.ml import Pipeline\n\nstopWordRemovalPipeline = Pipeline(stages=[stopwordsRemovalFeature])\npipelineFitRemoveStopWords = stopWordRemovalPipeline.fit(df)\n```", "```py\ndf = pipelineFitRemoveStopWords.transform(df)\n```", "```py\nlabel = F.udf(lambda x: 1.0 if x == 'escalate' else 0.0, FloatType())\ndf = df.withColumn('label', label('label'))\n```", "```py\nimport pyspark.ml.feature as feat\nTF_ = feat.HashingTF(inputCol=\"words without stop\", \n                     outputCol=\"rawFeatures\", numFeatures=100000)\nIDF_ = feat.IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n```", "```py\npipelineTFIDF = Pipeline(stages=[TF_, IDF_])\n```", "```py\npipelineFit = pipelineTFIDF.fit(df)\ndf = pipelineFit.transform(df)\n```", "```py\n(trainingDF, testDF) = df.randomSplit([0.75, 0.25], seed = 1234)\n```", "```py\nfrom pyspark.ml.classification import LogisticRegression\nlogreg = LogisticRegression(regParam=0.25)\n```", "```py\nlogregModel = logreg.fit(trainingDF)\npredictionDF = logregModel.transform(testDF)\n```", "```py\npredictionDF.crosstab('label', 'prediction').show()\n```", "```py\nfrom sklearn import metrics\n\nactual = predictionDF.select('label').toPandas()\npredicted = predictionDF.select('prediction').toPandas()\nprint('accuracy score: {}%'.format(round(metrics.accuracy_score(actual,         predicted),3)*100))\n```", "```py\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nscores = predictionDF.select('label', 'rawPrediction')\nevaluator = BinaryClassificationEvaluator()\nprint('The ROC score is {}%'.format(round(evaluator.evaluate(scores),3)*100))\n```", "```py\npredictionDF.describe('label').show()\n```"]