<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Word Representations in FastText</h1>
                
            
            <article>
                
<p class="calibre2">Now that you have taken a look at creating models in the command line, you might be wondering how fastText creates those word representations. In this chapter, you will get to know what happens behind the scenes and the algorithms that power fastText.</p>
<p class="calibre2">We will cover the following topics in this chapter:</p>
<ul class="calibre10">
<li class="calibre11">Word-to-vector representations</li>
<li class="calibre11">Types of word representations</li>
<li class="calibre11">Getting vector representations from text</li>
<li class="calibre11">Model architecture in fastText</li>
<li class="calibre11">The unsupervised model</li>
<li class="calibre11">fastText skipgram implementation</li>
<li class="calibre11"><strong class="calibre1">CBOW</strong> (<span><strong class="calibre1">Continuous bag of words</strong>)</span></li>
<li class="calibre11">Comparison between skipgram and CBOW</li>
<li class="calibre11">Loss functions and optimizations</li>
<li class="calibre11">Softmax</li>
<li class="calibre11">Context definitions</li>
</ul>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Word-to-vector representations</h1>
                
            
            <article>
                
<p class="calibre2">Almost all machine learning and deep learning algorithms manipulate vectors and matrices. The reason they work is because of their base mathematics, which is heavily rooted in linear algebra. So, in short, for both supervised and unsupervised learning, you will need to create matrices of numbers. In other domains, this is not an issue as information is generally captured as numbers. For example, in retail, the sales information for how many units were sold or how much revenue the store is making in the current month is all numbers. Even in a more abstract field such as computer vision, the image is always stored as pixel intensity of the three basic colors: red, green, and blue. 0 for a particular color means no intensity and 255 means the highest possible intensity for the screen. Similarly, in the case of sound, it is stored as power spectral density coefficients. In the case of sound, the analog signal that is picked up by the microphone is then converted to a discrete time and discrete amplitude. The amplitude is essentially the number of bits that can be passed in a given amount of time and hence that is essentially a number.<br class="calibre6"/></p>
<p class="calibre2">The challenge that comes in raw text in computer systems is that they are stored and analyzed as strings, which do not work well with these matrix systems. So you need a method to convert text into matrices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Types of word representations</h1>
                
            
            <article>
                
<p class="calibre2">Depending on the target languages, there are various concepts that should be taken care of for an optimal word representation of the given corpus:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Distributed word representation</strong>: In <span>a distributed representation, the sense of the word should not be concentrated on only one dimension but be distributed across all dimensions. If it is not distributed, then the resulting vectors may be too big, which can be a limiting factor when performing the necessary vector transformations both in terms of memory and the time needed to perform the transformations. A distributed representation is compact and can represent an exponential number of clusters in the number of dimensions.</span></li>
<li class="calibre11"><strong class="calibre1">Distributional word representation</strong>: <span>You can argue that there is some kind of similarity between "cat" and "dog" and another kind of similarity between "cat" and "tiger". Word representations that focus on capturing those kinds of implicit relationships are called distributional. To get such distributional properties, the following very common paradigm is used:<br class="title-page-name"/></span></li>
</ul>
<div class="packt_quote">You shall know the meaning of the word by the company it keeps<br class="title-page-name"/>
                                                                                                     - John Rupert Firth (1962)</div>
<p class="calibre27">So <span class="calibre5">if</span><span class="calibre5"> we take an example with two sentences, "Mary has a cat" and "Mary has a dog", the context around "dog" and "cat" is the same and hence the word representation should be able to get the "pet" context by reading the two sentences.</span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Zipf's law and Heap's law</strong>: We will have some more discussion on Zipf's law when we go to the n-grams but we will state Heap's law here:</li>
</ul>
<div class="packt_quote"><span>The number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation).</span></div>
<p class="calibre27">Taken together, Heap's law and Zipf's law are essentially saying the same thing, which is that you will always have new words. Hence, you should not be throwing away rare words from the document and will need a word representation that is more welcoming of new words. You cannot model language, close the vocabulary, and say you are done.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting vector representations from text
</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will take a look at what it means to have a vector representation for the target group of text. We will start with one of the simplest forms of word vectors and how to implement it. Then we will explore the rationale behind some other types of word vectors and, finally, we will take an in-depth look at the algorithms that are used in fastText to create the word vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">One-hot encoding</h1>
                
            
            <article>
                
<p class="calibre2">In the simplest approach, raw text can be taken as a collection of tokens, where are assumption is that each "word" contributes in some way to the meaning of the sentence as a whole. All words are meant to signify something specific and hence are categories by themselves. The presence of a word would mean the presence of the category for which the word stands for, and absence of the word would mean that the category is not there. Hence, the traditional method was to represent categorical variables as binary variables. First the dictionary of words is created and then each word is assigned a unique position. Then the vectors are created by putting 1 in the respective index and 0 for all other variables. This system of creating vectors is called one-hot encoding:</p>
<div class="mce-root"><img src="../images/00018.jpeg" class="calibre19"/></div>
<p class="calibre2">Implementation of the one-hot model is shown here:</p>
<pre class="calibre17"># define input string<br class="title-page-name"/>data = 'the quick brown fox jumped over the lazy dog'<br class="title-page-name"/>consecutive_words = data.split()<br class="title-page-name"/>print(data)<br class="title-page-name"/><br class="title-page-name"/># construct the dictionary<br class="title-page-name"/>all_words = list(set(consecutive_words))<br class="title-page-name"/><br class="title-page-name"/># define a mapping of word to integers<br class="title-page-name"/>word_to_int = dict((w, i) for i, w in enumerate(all_words))<br class="title-page-name"/>int_to_word = dict((i, w) for i, w in enumerate(all_words))<br class="title-page-name"/># integer encode input data<br class="title-page-name"/>integer_encoded = [word_to_int[w] for w in consecutive_words]<br class="title-page-name"/><br class="title-page-name"/># one hot encode<br class="title-page-name"/>onehot_encoded = list()<br class="title-page-name"/>for value in integer_encoded:<br class="title-page-name"/>    letter = [0 for _ in range(len(all_words))]<br class="title-page-name"/>    letter[value] = 1<br class="title-page-name"/>    onehot_encoded.append(letter)<br class="title-page-name"/>_ = [print(x) for x in onehot_encoded]<br class="title-page-name"/><br class="title-page-name"/>def argmax(vector):<br class="title-page-name"/>    # since vector is actually a list and its one hot encoding hence the<br class="title-page-name"/>    # maximum value is always 1<br class="title-page-name"/>    return vector.index(1)<br class="title-page-name"/><br class="title-page-name"/># invert encoding<br class="title-page-name"/>inverted = int_to_word[argmax(onehot_encoded[0])]<br class="title-page-name"/>print(inverted)</pre>
<p class="calibre2">Although one-hot encoding is simple to understand and implement, there are a number of disadvantages associated with it:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Not a distributed representation</strong>: The number of dimensions for each vector grows with the size of the vocabulary. The matrix that is formed is highly sparse—which means that most of the individual values are 0s. Hence, the matrix manipulations become computationally too expensive even for a corpus of normal size.</li>
<li class="calibre11"><strong class="calibre1">Out-of-vocabulary words</strong>: It is not able to handle new words at test time.</li>
<li class="calibre11"><strong class="calibre1">Not a distributional representation</strong>: In one-hot encoding, all the vectors are equidistant from each other.</li>
</ul>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Bag of words</h1>
                
            
            <article>
                
<p class="calibre2">The bag of words model is concerned about whether known words occur in the document and only the frequency of the tokens in the document will be taken into account. So to create the document matrix using the bag of words approach, the following algorithm is used:</p>
<ol class="calibre13">
<li value="1" class="calibre11">Find the number of separate words that are used in all the documents. Words are identified using spaces and punctuation as the separators.</li>
<li value="2" class="calibre11">Using the tokens, a feature space is created. For each document, each feature value is the count of the number of times the feature is present in the document. Hence, each row in the resultant matrix will correspond to each document. Count the number of tokens in each document. This is because each document will generate its own vector.</li>
<li value="3" class="calibre11">Normalize the vectors.</li>
</ol>
<p class="calibre2">For example, let's say that there are two documents comprising the whole corpus:</p>
<pre class="calibre17">Document1: "John likes to watch movies. Mary likes too."<br class="title-page-name"/>Document2: "John also likes to watch football games."</pre>
<p class="calibre2">So for all the sentences, our vocabulary is as follows:</p>
<pre class="calibre17">['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']</pre>
<p class="calibre2">To get the bag of words, we count the number of times each word occurs in the sentence. So the following are the vectors formed for each document:</p>
<pre class="calibre17">Document1: {'likes': 2, 'John': 1, 'to': 1, 'watch': 1, 'movies': 1, 'Mary': 1, 'too': 1}<br class="title-page-name"/>Document2: {'John': 1, 'also': 1, 'likes': 1, 'to': 1, 'watch': 1, 'football': 1, 'games': 1}</pre>
<p class="calibre2">The main disadvantage of the bag of words approach is that the context of the word is lost. You can think of examples such as "Toy Dog" and "Dog Toy", which do not mean the same thing but will share the same vector. A simple implementation of bag of words is shown here:</p>
<pre class="calibre17">import collections, re<br class="title-page-name"/>texts = ['John likes to watch movies. Mary likes too.', 'John also likes to watch football games.']<br class="title-page-name"/>bagsofwords = [collections.Counter(re.findall(r'\w+', txt)) for txt in texts]</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<pre class="calibre17"><br class="title-page-name"/>print(bagsofwords[0])<br class="title-page-name"/>print(bagsofwords[1])<br class="title-page-name"/>sumbags = sum(bagsofwords, collections.Counter())<br class="title-page-name"/>print(sumbags)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">TF-IDF</h1>
                
            
            <article>
                
<p class="calibre2">Just counting the number of tokens in a document may not give sufficient information about the whole corpus. The idea is that rarer words give more information about what the document is about. In TF-IDF, the term frequency is normalized by the document frequency. The intuition is that TF-IDF makes rare words more prominent and scales down the effect of common words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">N-grams</h1>
                
            
            <article>
                
<p class="calibre2">N-gram-based approaches are based on Zipf's law which states the following:</p>
<div class="packt_quote">The nth most common word in a human language text occurs with a frequency inversely proportional to n.</div>
<p class="calibre2">In all languages, there are some words that are used more commonly than the others. The difference between more common words and less common words is not drastic but continuous. Another good corollary of this law is that if a class of documents corresponding to a specific frequency gets cut off, that will not massively affect the n-gram frequency profile. Hence, if we are comparing documents of the same category, they should have similar frequency profile.</p>
<p class="calibre2">N-grams frequency means the frequency of overlapping sequence of words. Here is a quote:</p>
<div class="packt_quote">"Even now They talked in Their tombs."<br class="title-page-name"/>
                                                                  - H.P. Lovecraft</div>
<p class="calibre2">From this sentence you can obtain the following n-grams. "_" is to show the start and end of the sentence:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">1</strong>-<strong class="calibre1">grams (unigrams)</strong>: Even, now, They, talked, in, Their, tombs (number of features: <span>7</span>).</li>
<li class="calibre11"><strong class="calibre1">2</strong>-<strong class="calibre1">grams (bigrams)</strong>: (_, Even), (Even, now), (now, They), (They, talked), (talked, in), (in, Their), (Their, tombs), (tombs, _) (number of features: 8).</li>
<li class="calibre11"><strong class="calibre1">3</strong>-<strong class="calibre1">grams (trigrams)</strong>: (_, _, Even), (_, Even, now), (Even, now, They), (now, They, talked), (They, talked, in), (talked, in, Their), (in, Their, tombs), (Their, tombs, _), (tombs, _, _) (features: 9).</li>
<li class="calibre11"><strong class="calibre1">4</strong>-<strong class="calibre1">grams (trigrams)</strong>: (_, _, _, Even), (_, _, Even, now), (_, Even, now, They), (Even, now, They, talked), (now, They, talked, in), (They, talked, in, Their), (talked, in, Their, tombs), (in, Their, tombs, _), (Their, tombs, _, _), (tombs, _, _, _) (features: 10).</li>
</ul>
<p class="calibre2">And so on.</p>
<p class="calibre2">When dealing with only unigrams, the probability of the whole sentence can be written as follows:</p>
<pre class="calibre17">P("Even now They talked in Their tombs.") = P("Even") * P("now") * P("They") * P("talked") * P("in") * P("Their") * P("tombs")</pre>
<p class="calibre2">Similarly, in the case of bigrams, <span class="calibre5">the probability of the whole sentence can be written as follows:</span></p>
<pre class="calibre17">P("Even now They talked in Their tombs.") = P("Even" | start of sentence) * ("now" | "Even") * ("They" | "now") * ("talked" | "They") * ("in" | "talked") * ("Their" | "in") * ("tombs" | "Their") * (end of sentence | "tombs")</pre>
<p class="calibre2">As per maximum likelihood estimation, the conditional probability of something like P("now" | "Even") can be given as the ratio of count of the observed occurrence of "Even now" together by the count of the observed occurrence of "Even". This probability model can now be used to predict new sentences.</p>
<p class="calibre2">Let's build a model in case of bigrams. This file has been taken from the servers of University of Maryland Institute for Advanced Computer Studies, <a href="http://www.umiacs.umd.edu/" class="calibre9">http://www.umiacs.umd.edu/</a> or you can use your own corpus. Keep it in the <kbd class="calibre12">data</kbd> folder.</p>
<p class="calibre2">Now, the following command will remove the new lines, then squash all the consecutive spaces, then get all the bigrams and sort them as per frequency:</p>
<pre class="calibre17"><strong class="calibre1">$ cat data/persuasion.txt | tr '\n' ' ' | tr -s ' ' | tr -sc 'A-Za-z' '\012' |   sed -e '1!{$!p' -e '}' | paste -d' ' - - | sort | uniq -c | sort -nr &gt; data/persuasion_bigrams.txt</strong></pre>
<p class="calibre2">Now we can build a sentence generator using this n-grams file:</p>
<pre class="calibre17">def get_next_word(ngram_file, word1=None, sentence_length=0):<br class="title-page-name"/>    with open(ngram_file) as f:<br class="title-page-name"/>        for line in f:<br class="title-page-name"/>            _, w1, w2 = line.split()<br class="title-page-name"/>            if word1 is None or word1 == w1:<br class="title-page-name"/>                sentence_length -= 1<br class="title-page-name"/>                word1 = w2<br class="title-page-name"/>                return w1, word1, sentence_length<br class="title-page-name"/><br class="title-page-name"/>def build_sentence(ngram_file, sentence_length):<br class="title-page-name"/>    first_word = None<br class="title-page-name"/>    sentence = ''<br class="title-page-name"/>    while sentence_length &gt; 0:<br class="title-page-name"/>        w1, first_word, sentence_length = get_next_word(ngram_file, first_word, sentence_length)<br class="title-page-name"/>        sentence = sentence + ' ' + w1<br class="title-page-name"/>    final_sentence = sentence + ' ' + first_word + '.'<br class="title-page-name"/>    return final_sentence<br class="title-page-name"/><br class="title-page-name"/>print(build_sentence('data/persuasion_bigrams.txt', 10))</pre>
<p class="calibre2">Using this n-gram sentence builder, we get the following sentence:</p>
<pre class="calibre17"><strong class="calibre1">$ python build_sentence_ngrams.py</strong><br class="title-page-name"/><strong class="calibre1"> of the same time to be a very well as she.</strong></pre>
<p class="calibre2">If this fascinates you, then try to build with trigrams or more.</p>
<p class="calibre2">The major drawback of n-grams is that they are extremely sparse and are not able to distinguish when encountering new words in the test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Model architecture in fastText</h1>
                
            
            <article>
                
<p class="calibre2">FastText models are a little bit different depending on whether they are unsupervised models or supervised models. In this chapter, we will mostly look at the unsupervised model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The unsupervised model</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">In fastText, you can have the option to use two model architectures for computing a distributed representation of words. They are skipgram and CBOW. </span>The model architectures used in fastText are both distributed architectures. So the aim is to learn a high-dimensional dense representation for each vocabulary term. The representation should be distributional as well as it tries to learn from context.</p>
<p class="calibre2">In both the architectures, you train a two-layer, shallow neural network to construct the context of words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Skipgram</h1>
                
            
            <article>
                
<p class="calibre2">In skipgram, a context window of <em class="calibre16">k</em> is considered. All the other positions are skipped and only the relationship between the panel and the word is explored. This is done by feeding a one-hot encoding of the word to a two-layer shallow neural network. Since the input is one-hot encoded, the hidden layer consists of only one row of input hidden weight matrix. The task for the neural network is to predict the <em class="calibre16">i</em>th context given the word:</p>
<div class="cdpaligncenter"><img src="../images/00019.jpeg" class="calibre28"/></div>
<p class="calibre2">The scores for each word are computed using the following equation:</p>
<p class="calibre2"><img class="fm-editor-equation1" src="../images/00020.jpeg"/></p>
<p class="calibre2">Here, <em class="calibre16">h</em> is a vector in the hidden layer and <em class="calibre16">W</em> is the hidden output weight matrix. After computing <em class="calibre16">u</em>, <em class="calibre16">c</em> multinomial weight distributions are computed, where <em class="calibre16"><span class="calibre5"><span class="calibre5">c</span></span></em> is the window size. The distributions are computed using the following equation:</p>
<p class="calibre2"><img class="fm-editor-equation2" src="../images/00021.jpeg"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Here, <em class="calibre16">w<sub class="calibre29">c,j</sub></em> is the <em class="calibre16">j</em>th word on the <em class="calibre16">c</em>th panel of the output layer; <em class="calibre16">w<sub class="calibre29">O,c</sub></em> is the actual <em class="calibre16">c</em>th word in the output context words; <em class="calibre16">w<sub class="calibre29">I</sub></em> is the only input word; and <em class="calibre16">u<sub class="calibre29">c,j</sub></em> is the net input of the <em class="calibre16">j</em>th unit on the <em class="calibre16">c</em>th panel of the output layer. So you can see this is, in effect, trying to predict the context words given the input word. The probability is then converted into a softmax. If you try to visualize the above architecture, this should translate to something like the following:</p>
<div class="cdpaligncenter"><img src="../images/00022.jpeg" class="calibre30"/><br class="title-page-name"/></div>
<p class="calibre2"> </p>
<p class="calibre2">In addition, more distant words are given less weight-age by randomly sampling them. When you give the window size parameter, only the maximum window size is configured. In effect, the actual window size is randomly chosen between 1 and the maximum window size for each training sample. Thus the words that are the farthest are chosen with the probability of 1/<em class="calibre16">c</em>, whereas the nearest words are always chosen.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Subword information skipgram</h1>
                
            
            <article>
                
<p class="calibre2">The skipgram model was taken in its original form from the word2vec implementation. The skipgram model is effective because it emphasizes the specific word and the word that comes along with it. But along with the specific word, the character of the n-grams may also have a lot of information. This is especially true of languages which are morphologically rich. In fastText, the authors took the skipgram implementation in word2vec, which is simply taking the vector representation of the whole word, and said that the vector representation of the word is actually the sum of the vector representations of the n-grams. Hence, in fastText, the scoring function (<em class="calibre16">u</em>) that you saw earlier is actually changed to the following:</p>
<p class="calibre2"><img class="fm-editor-equation3" src="../images/00023.jpeg"/></p>
<p class="calibre2"><em class="calibre16">SCORE</em> = [<em class="calibre16">3-6 char level n-grams</em>] + [<em class="calibre16">word</em>]</p>
<p class="calibre2">In the library, n-grams for n greater or equal to 3 and less or equal to 6 is taken along with the word. So for something like <kbd class="calibre12">Schadenfreude</kbd> the collection of n-grams taken are as follows:</p>
<pre class="calibre17">"shadenfreude" = {"sha", "had", "ade", ..., "shad", "frue", ..., "freude", ..., "shadenfreude"}</pre>
<p class="calibre2">The major advantage of this method is that in the case of out-of-vector words, the [word] vector is not present and hence the score function transforms to the following:</p>
<p class="calibre2"><em class="calibre16">SCORE</em> = [<em class="calibre16">3-6 char level n-grams</em>]</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementing skipgram </h1>
                
            
            <article>
                
<p class="calibre2">Now let's try to understand the skipgram method through some Python code. The Keras library has a very good and easy to understand skipgram function that you can see and understand how the skipgram should be implemented. For code in this section, you can take a look at the <kbd class="calibre12">fasttext skipgram cbow.ipynb</kbd> notebook which is inspired by the Keras implementations.<br class="calibre6"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">As discussed in skipgram, the task for the model is to predict the <em class="calibre16">i</em>th context given the word. How this is achieved in practice is by taking pairs of words from the document and then saying output is <kbd class="calibre12">1</kbd> in case the second word is the context word.</p>
<p class="calibre2">Now, given a sequence or an individual document (which, in this case, is probably a particular sentence), first create two lists: <kbd class="calibre12">couples</kbd> and <kbd class="calibre12">labels</kbd>. Now, for each target word, get the context window and in the context window for each combination of target word and context word, capture the combination in the <kbd class="calibre12">couples</kbd> list, and capture the label in the <kbd class="calibre12">labels</kbd> list:</p>
<pre class="calibre17">couples = []<br class="title-page-name"/>labels = []<br class="title-page-name"/>for i, wi in enumerate(sequence):<br class="title-page-name"/>    if not wi:<br class="title-page-name"/>        continue<br class="title-page-name"/><br class="title-page-name"/>    window_start = max(0, i - window_size)<br class="title-page-name"/>    window_end = min(len(sequence), i + window_size + 1)<br class="title-page-name"/>    for j in range(window_start, window_end):<br class="title-page-name"/>        if j != i:<br class="title-page-name"/>            wj = sequence[j]<br class="title-page-name"/>            if not wj:<br class="title-page-name"/>                continue<br class="title-page-name"/>            couples.append([wi, wj])<br class="title-page-name"/>            if categorical:<br class="title-page-name"/>                labels.append([0, 1])<br class="title-page-name"/>            else:<br class="title-page-name"/>                labels.append(1)</pre>
<p class="calibre2">Since we have captured only positive values right now, we will need to capture some negative cases as well to train the model effectively. In the negative sampling case, for the number of negative samples, randomly generate some out-of-context word indexes with the target words that we have:</p>
<pre class="calibre17">num_negative_samples = int(len(labels) * negative_samples)<br class="title-page-name"/>words = [c[0] for c in couples]<br class="title-page-name"/>random.shuffle(words)<br class="title-page-name"/><br class="title-page-name"/>couples += [[words[i % len(words)],<br class="title-page-name"/>            random.randint(1, vocabulary_size - 1)] # basically get some out of context word indices<br class="title-page-name"/>            for i in range(num_negative_samples)]</pre>
<pre class="calibre17"><br class="title-page-name"/>if categorical:<br class="title-page-name"/>    labels += [[1, 0]] * num_negative_samples # opposite of what you would define for positive samples<br class="title-page-name"/>else:<br class="title-page-name"/>    labels += [0] * num_negative_samples</pre>
<p class="calibre2">You can encapsulate the previous logic in a function, as has been done in the Keras function <kbd class="calibre12">skipgrams</kbd>  and then return the combinations (denoted by the <kbd class="calibre12">couples</kbd> list) and the labels. This will be then passed on to the neural network, which will train on these combinations and corresponding labels:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
    <span>loss</span> <span>=</span> <span>0.</span>
    <span>for</span> <span>i</span><span>,</span> <span>doc</span> <span>in</span> <span>enumerate</span><span>(</span><span>tokenizer</span><span>.</span><span>texts_to_sequences</span><span>(</span><span>corpus</span><span>)):</span>
        <span>data</span><span>,</span> <span>labels</span> <span>=</span> <span>skipgrams</span><span>(</span><span>sequence</span><span>=</span><span>doc</span><span>,</span> <span>vocabulary_size</span><span>=</span><span>V</span><span>,</span> <span>window_size</span><span>=</span><span>5</span><span>,</span> <span>negative_samples</span><span>=</span><span>5.</span><span>)</span>
        <span>x</span> <span>=</span> <span>[</span><span>np</span><span>.</span><span>array</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>x</span> <span>in</span> <span>zip</span><span>(</span><span>*</span><span>data</span><span>)]</span>
        <span>y</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>labels</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>int32</span><span>)</span>
        <span>if</span> <span>x</span><span>:</span>
            <span>loss</span> <span>+=</span> <span>model</span><span>.</span><span>train_on_batch</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>

    <span>print</span><span>(</span><span>loss</span><span>)</span></pre></div>
</div>
<p class="calibre2">The skipgram model <span class="calibre5">is</span><span class="calibre5"> </span><span class="calibre5">essentially a hidden layer sandwiched between an input layer and the output layer. We can create a simple Keras model to capture that:</span></p>
<pre class="calibre17">embedding_dim = 100<br class="title-page-name"/><br class="title-page-name"/># inputs<br class="title-page-name"/>w_inputs = Input(shape=(1, ), dtype='int32')<br class="title-page-name"/>w = Embedding(V, embedding_dim)(w_inputs)<br class="title-page-name"/><br class="title-page-name"/># context<br class="title-page-name"/>c_inputs = Input(shape=(1, ), dtype='int32')<br class="title-page-name"/>c = Embedding(V, embedding_dim)(c_inputs)<br class="title-page-name"/>o = Dot(axes=2)([w, c])<br class="title-page-name"/>o = Reshape((1,), input_shape=(1, 1))(o)<br class="title-page-name"/>o = Activation('sigmoid')(o)<br class="title-page-name"/><br class="title-page-name"/>ft_model = Model(inputs=[w_inputs, c_inputs], outputs=o)<br class="title-page-name"/># ft_model.summary()<br class="title-page-name"/>ft_model.compile(loss='binary_crossentropy', optimizer='adam')<br class="title-page-name"/><br class="title-page-name"/>Image(model_to_dot(ft_model, show_shapes=True).create(prog='dot', format='png'))</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">This creates the following model:</p>
<div class="cdpaligncenter"><img src="../images/00024.gif" class="calibre31"/></div>
<p class="calibre2">Finally, once the model is trained, we get the vectors from the trained weights of the embedding dimension:</p>
<pre class="calibre17">with open('vectors.txt' ,'w') as f:<br class="title-page-name"/>    f.write('{} {}\n'.format(V-1, embedding_dim))<br class="title-page-name"/>    vectors = ft_model.get_weights()[0]<br class="title-page-name"/>    for word, i in tokenizer.word_index.items():<br class="title-page-name"/>        f.write('{} {}\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))</pre>
<p class="calibre2">Now we can save the vectors in our file and load them up when necessary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">CBOW</h1>
                
            
            <article>
                
<p class="calibre2">CBOW is the opposite of skipgram, where the specific word is taken as the target given the context. The number of words that are used as context words depends on the context word. So in this case, taking the previous example "<span class="calibre5">Even now They talked in Their tombs</span>", we can take the whole context <kbd class="calibre12">[<span>"</span><span>Even" "now" "They" "in" "Their" "tombs.</span><span>"</span>]</kbd> and generate the word "talked" from it.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">So the algorithm is to take the one-hot vectors of all the words since now we are taking all the context words as the input. Considering that the window size is k, there will be 2 million one-hot vectors. Then take the embedding words vectors for all the words. Average out the word vectors to get the cumulative context. The output of the hidden layer is thus generated using the following equation:</p>
<p class="calibre2"><img class="fm-editor-equation4" src="../images/00025.jpeg"/></p>
<p class="calibre2">It is worth noting that the hidden layer is one of the main differences between skipgram and CBOW in terms of being mirror images of one another.</p>
<p class="calibre2"><span class="calibre5">Generate the score with the same score function as we saw when defining skipgram. The equation is almost the same except that since we are predicting all the words in the output based on the context, hence u and ν for the different columns (denoted by j) needs to be computed:</span></p>
<p class="calibre2"><img class="fm-editor-equation5" src="../images/00026.jpeg"/></p>
<p class="calibre2"><span class="calibre5">Turn the score into probabilities using the softmax. Now we need to train this model so that the probabilities match the true probabilities of the word, which is the one-hot encoding of the actual word:</span></p>
<p class="calibre2"><img class="fm-editor-equation6" src="../images/00027.jpeg"/></p>
<p class="calibre2"><span class="calibre5">The CBOW architecture looks something like the following:</span></p>
<div class="cdpaligncenter"><img src="../images/00028.jpeg" class="calibre32"/> </div>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">CBOW implementation</h1>
                
            
            <article>
                
<p class="calibre2">CBOW implementation is easier to code than skipgram as the <kbd class="calibre12">cbow</kbd> method is pretty much straightforward. For each target word, you take the context and try to predict the target word, keeping the context as the input.</p>
<p class="calibre2">Hence, for the implementation perspective, writing code for CBOW is simpler. For each word in the sequence, the same labels lists will be created but this list will be the actual target word under focus. The other list is the context list which will have the context words depending on the window. Now, once the input and output are fixed, we can then yield them so that the model can train on it:</p>
<pre class="calibre17">def generate_data_for_cbow(corpus, window_size, V):<br class="title-page-name"/>    maxlen = window_size*2<br class="title-page-name"/>    corpus = tokenizer.texts_to_sequences(corpus)<br class="title-page-name"/>    for words in corpus:<br class="title-page-name"/>        L = len(words)<br class="title-page-name"/>        for index, word in enumerate(words):<br class="title-page-name"/>            contexts = []<br class="title-page-name"/>            labels = [] <br class="title-page-name"/>            s = index - window_size<br class="title-page-name"/>            e = index + window_size + 1<br class="title-page-name"/>            <br class="title-page-name"/>            contexts.append([words[i] for i in range(s, e) if 0 &lt;= i &lt; L and i != index])<br class="title-page-name"/>            labels.append(word)<br class="title-page-name"/>            x = sequence.pad_sequences(contexts, maxlen=maxlen)<br class="title-page-name"/>            y = np_utils.to_categorical(labels, V)<br class="title-page-name"/>            yield (x, y)</pre>
<p class="calibre2">The output of the preceding model will be in terms of NumPy vectors which can be passed to a keras model for batch training, similar to what you saw in the <em class="calibre16">Implementing skipgram</em> section. Here, <kbd class="calibre12">cbow</kbd> is the Keras model:</p>
<pre class="calibre17">for ite in range(5):<br class="title-page-name"/>    loss = 0.<br class="title-page-name"/>    for x, y in generate_data_for_cbow(corpus, window_size, V):<br class="title-page-name"/>        loss += cbow.train_on_batch(x, y)<br class="title-page-name"/><br class="title-page-name"/>    print(ite, loss)</pre>
<p class="calibre2">For the CBOW model, you will need to define a input layer first. The embedding layer can be the average of the embedding layer, which is then passed on to an output layer using the <kbd class="calibre12">softmax</kbd> function. Hence we have the following:</p>
<pre class="calibre17">cbow = Sequential()<br class="title-page-name"/>cbow.add(Embedding(input_dim=V, output_dim=embedding_dim, input_length=window_size*2))<br class="title-page-name"/>cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))<br class="title-page-name"/>cbow.add(Dense(V, activation='softmax'))</pre>
<p class="calibre2">You should see the following architecture being built:</p>
<div class="cdpaligncenter"><img src="../images/00029.jpeg" class="calibre33"/></div>
<p class="calibre2">Run the code in the <kbd class="calibre12">fasttext skipgram cbow.ipynb</kbd> notebook. You will be able to compare the vectors created using skipgram and CBOW.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Comparison between skipgram and CBOW</h1>
                
            
            <article>
                
<p class="calibre2">Now, you might be wondering which architecture should be used more <span class="calibre5">during the actual training of data</span><span class="calibre5">. The following are some guidelines for differentiating between CBOW and skipgram when choosing to train the data:</span></p>
<ul class="calibre10">
<li class="calibre11">Skipgram works well with a small amount of training data. It works well even on rare words and phrases.</li>
<li class="calibre11">CBOW is faster to train than skipgram. It also has higher accuracy on frequent words and phrases.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Loss functions and optimization</h1>
                
            
            <article>
                
<p class="calibre2">Choosing a loss function and an optimizing algorithm along with it is one of the fundamental strategies of machine learning.  Loss functions are a way of associating a cost with the difference between the present model and the actual data distribution. The idea is that for specific loss function, optimizing algorithm pair, it would be possible to optimize the parameters of the model to make them mimic the real data as closely as possible.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Language models that use the neural probabilistic networks are generally trained using the maximum likelihood principle. The task is to maximize the probability of the next word <em class="calibre16">w<sub class="calibre29">t</sub></em>, which is taken as the target, given the previous words h which is the "history". We can model that in terms of the softmax function that we will discuss next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Softmax</h1>
                
            
            <article>
                
<p class="calibre2">The most popular methods for learning parameters of a model is using gradient descent. Gradient descent is basically an optimization algorithm that is meant for minimizing a function, based on which way the negative gradient points toward. In machine learning, the input function that gradient descent acts on is a loss function that is decided for the model. The idea is that if we move towards minimizing the loss function, the actual model will "learn" the ideal parameters and will ideally generalize to out-of-sample or new data to a large extent as well. In practice, it has been seen this is generally the case and stochastic gradient, which is a variant of gradient descent, has a fast training time as well.</p>
<p class="calibre2">For the gradient descent to be effective, we need such an optimizing function that is convex and we want the logarithm of the model's output to be well behaved for gradient-based optimization of the likelihood, going with the <strong class="calibre4">maximum likelihood estimation</strong> (<strong class="calibre4">MLE</strong>) principle. Now, consider the fact that taking the logarithm of a series of products transforms it to a series of additions, and because the likelihood for the whole training dataset is actually the product of the individual likelihoods of each sample, it is easier to maximize the log-likelihood as this would mean that you are optimizing the sum of the log-likelihood of each sample indexed by k:</p>
<p class="calibre2"><img class="fm-editor-equation7" src="../images/00030.jpeg"/></p>
<p class="calibre2">Now we need to chose a suitable function for determining the probabilities, given by P in this case. There are some good functions out there that can be used and one popular function is the sigmoid function. The sigmoid function looks like an S:</p>
<div class="cdpaligncenter"><img src="../images/00031.jpeg" class="calibre34"/></div>
<p class="calibre2">The sigmoid function is best used for binary classification tasks and is used in logistic regression.</p>
<p class="calibre2">Since we need to obtain the posterior distribution of words, our problem statement is more of a multinomial distribution instead of a binary one. Hence, we can chose the softmax distribution, which is a generalization of the sigmoid for the multi-class problem.</p>
<p class="calibre2">The softmax function calculates the probabilities distribution of the event over n different events. The softmax takes a class of values and converts them to probabilities with sum 1. So you can say that it is effectively squashing a k-dimensional vector of arbitrary real values to k-dimensional vector of real values within the range 0 to 1. The function is given by the following equation:</p>
<p class="calibre2"><img class="fm-editor-equation8" src="../images/00032.jpeg"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">You can use the following code to see what the softmax function looks like:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>def softmax(arr):<br class="title-page-name"/>    return np.exp(arr)/float(sum(np.exp(arr)))<br class="title-page-name"/>def line_graph(x, y, x_title, y_title):<br class="title-page-name"/>    plt.plot(x, y)<br class="title-page-name"/>    plt.xlabel(x_title)<br class="title-page-name"/>    plt.ylabel(y_title)<br class="title-page-name"/>    plt.show()<br class="title-page-name"/>graph_x = range(10)<br class="title-page-name"/>graph_y = softmax(graph_x)<br class="title-page-name"/>print('Graph X readings: {}'.format(graph_x))<br class="title-page-name"/>print('Graph Y readings: {}'.format(graph_y))<br class="title-page-name"/>line_graph(graph_x, graph_y, 'Inputs', 'softmax scores')</pre>
<p class="calibre2">If you run the previous code in a Jupyter notebook, you should see a graph similar to the following. You can also see this in the <kbd class="calibre12">softmax function.ipynb</kbd> notebook under <kbd class="calibre12">chapter 3</kbd> folder:</p>
<div class="cdpaligncenter"><img src="../images/00033.jpeg" class="calibre35"/></div>
<p class="calibre2">Note that as the values are higher, the probabilities are <span class="calibre5">also </span><span class="calibre5">higher. This is an interesting property of softmax, that is, the reaction to low stimuli is a rather uniform distribution and the reaction to high stimuli is probabilities that are closer to 0 and 1. If you are wondering why that is the case, this is because of the impact of the exponential function, which focuses on the extreme values.</span></p>
<p class="calibre2">So, for a given input word, this function will calculate the probabilities of each word over all possible words. If you train using the softmax function, the probability associated with the actual word should be the highest:</p>
<p class="calibre2"><img class="fm-editor-equation9" src="../images/00034.jpeg"/></p>
<p class="calibre2">Here, the score function can be considered to be the compatibility of the word w<sub class="calibre29">t</sub> with the context h.</p>
<p class="calibre2">Since we are training this model using the negative log likelihood on the training set:</p>
<p class="calibre2"> <img class="fm-editor-equation10" src="../images/00035.jpeg"/></p>
<p class="calibre2">Now we need to learn our softmax model using gradient descent and hence we need to compute the gradient with respect to the input words:</p>
<p class="calibre2"><img class="fm-editor-equation11" src="../images/00036.jpeg"/></p>
<p class="calibre2">The update of the parameter models will be in the opposite direction to the gradient:</p>
<p class="calibre2"><img class="fm-editor-equation12" src="../images/00037.jpeg"/></p>
<p class="calibre2">In our context, let the vocabulary be <em class="calibre16">V</em> and the hidden layer size be <em class="calibre16">N</em>. The units on the adjacent layer are fully connected. The input is a one-hot encoded vector, which means for a given word input word context, only one out of the <em class="calibre16">V</em> units, {<em class="calibre16">x<sub class="calibre29">1</sub>, x<sub class="calibre29">2</sub>, ..., x<sub class="calibre29">V</sub>}</em>, will be 1, and all other units are 0.</p>
<p class="calibre2">The weights between the input layer and output layer can be represented by a <em class="calibre16">V</em> x <em class="calibre16">N</em> matrix <em class="calibre16">W</em>. Each row of <em class="calibre16">W</em> is the <em class="calibre16">N</em>-dimensional vector representation <em class="calibre16">v<sub class="calibre29">w</sub></em> of the associated word of the input layer. Formally, row <em class="calibre16">i</em> of <em class="calibre16">W</em> is <em class="calibre16">v<sub class="calibre29">w</sub><sup class="calibre23">T</sup></em>. Given a context, assuming <em class="calibre16">x<sub class="calibre29">k</sub></em>=<em class="calibre16">1</em> for a specific context word and 0 otherwise, we have the following:</p>
<p class="calibre2"><img class="fm-editor-equation13" src="../images/00038.jpeg"/></p>
<p class="calibre2">This is essentially the <em class="calibre16">k<sup class="calibre23">th</sup></em> row of <em class="calibre16">W</em>. Lets call this <em class="calibre16">ν<sub class="calibre29">wI<sup class="calibre36">T</sup></sub></em>. From the hidden layer to the output matrix, there is a different weight <em class="calibre16">W<sup class="calibre23">'</sup> = {w<sub class="calibre29">ij</sub><sup class="calibre23">'</sup>}</em>, which is a <em class="calibre16">N</em> x <em class="calibre16">V</em> matrix. Using these weights, you can compute a score <em class="calibre16">u<sub class="calibre29">j</sub></em> for each word in the vocabulary:</p>
<p class="calibre2"><img class="fm-editor-equation14" src="../images/00039.jpeg"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Here, <em class="calibre16">ν<sub class="calibre29">w</sub><sup class="calibre23">'</sup><sub class="calibre29">j</sub></em> is the <em class="calibre16">jth</em> column of the matrix <em class="calibre16">W<sup class="calibre23">'</sup></em>. Now, using the softmax equation, we can obtain the following:</p>
<p class="calibre2"><img class="fm-editor-equation15" src="../images/00040.jpeg"/></p>
<p class="calibre2"><em class="calibre16"><span class="calibre5">ν</span><sub class="calibre29">w</sub></em><span class="calibre5"> and <em class="calibre16">ν<sub class="calibre29">w</sub><sup class="calibre23">'</sup></em> are two representations of the word <em class="calibre16">w</em>.  <em class="calibre16">ν<sub class="calibre29">w</sub></em> comes from rows of <em class="calibre16">W</em>, which is the input to hidden weight matrix, and <em class="calibre16">ν<sub class="calibre29">w</sub><sup class="calibre23">'</sup></em> comes from columns of <em class="calibre16">W<sup class="calibre23">'</sup></em>, which is the hidden output matrix. In subsequent analysis, we will call <em class="calibre16">ν<sub class="calibre29">w</sub></em> the "input vector" and <em class="calibre16">ν<sub class="calibre29">w</sub><sup class="calibre23">'</sup></em> the "output vector" of the word <em class="calibre16">w</em>.</span> Considering <em class="calibre16">u<sub class="calibre29">j</sub></em> as the score as described, we can transform the loss function to equation (6) to the one as follows:</p>
<p class="calibre2"><img class="fm-editor-equation16" src="../images/00041.jpeg"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hierarchical softmax</h1>
                
            
            <article>
                
<p class="calibre2">Finding the softmax is highly computationally intensive. For each training instance, we have to iterate through every word in the vocabulary and compute the softmax. Thus, it is impractical to scale up to large vocabularies and large training corpora. To solve this problem, there are two approaches that are used in fastText: the hierarchical softmax and the negative sampling approach. We will discuss hierarchical softmax in this section and will discuss negative sampling in the next section. In both the approaches, the trick is to recognize that we don't need to update all the output vectors per training instance.</p>
<p class="calibre2">In hierarchical softmax, a binary tree is computed to represent all the words in the vocabulary. The <em class="calibre16">V</em> words must be leaf units of the tree. It can be proved that there are <em class="calibre16">V-1</em> inner units. For each unit, there exists a unique path from the root of the tree to the unit, and this path is used to estimate the probability of the word represented in the leaf unit:</p>
<div class="cdpaligncenter"><img src="../images/00042.gif" class="calibre37"/></div>
<p class="calibre2">Each of the words can be reached by a path from the root through the inner nodes, which represent probability mass along the way. Those values are produced by the usage of simple sigmoid function as long as the path we are calculating is simply the product of those probability mass functions defined with the following:</p>
<p class="calibre2"><img class="fm-editor-equation17" src="../images/00043.jpeg"/></p>
<p class="calibre2">What is <em class="calibre16">x</em> in our specific case? It is calculated with the dot product of input and output vector representations of the word we are working with:</p>
<p class="calibre2"><img class="fm-editor-equation18" src="../images/00044.jpeg"/></p>
<p class="calibre2">Here, <em class="calibre16">n(w, j)</em> is the <em class="calibre16">j</em>th node on the path from the root to <em class="calibre16">w</em>.</p>
<p class="calibre2">In the hierarchical softmax model, there is no output representation of words. Instead, each of the <em class="calibre16">V - 1</em> inner units has an output vector <em class="calibre16">ν<sub class="calibre29">n(w,j)</sub><sup class="calibre23">'</sup></em>. And the probability of a word being the output word is defined as follows:</p>
<p class="calibre2"><img class="fm-editor-equation19" src="../images/00045.jpeg"/></p>
<p class="calibre2">Here, <em class="calibre16">ch(n)</em> is the left child of unit <em class="calibre16">n</em>; <em class="calibre16">ν<sub class="calibre29">n(w,j)</sub><sup class="calibre23">'</sup></em> is the vector representation ("output vector" ) of the inner unit <em class="calibre16">n(w,j)</em>; <em class="calibre16">h</em> is the output value of the hidden layer (in the skipgram model <em class="calibre16">h = ν<sub class="calibre29">ω</sub></em> and in CBOW, <img class="fm-editor-equation20" src="../images/00046.jpeg"/>); <img class="fm-editor-equation21" src="../images/00047.jpeg"/> is a special function defined as follows:</p>
<p class="calibre2"><img class="fm-editor-equation22" src="../images/00048.jpeg"/></p>
<p class="calibre2">To calculate the probability of any output word, we need the probabilities of each intermediate node in the path from the root to the output word.</p>
<p class="calibre2">We define the probability of going right at an intermediate node as follows:</p>
<p class="calibre2"><img class="fm-editor-equation23" src="../images/00049.jpeg"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Since we are computing a binary tree, the probability of going left will be as follows:</p>
<p class="calibre2"><img class="fm-editor-equation24" src="../images/00050.jpeg"/></p>
<p class="calibre2"><span class="calibre5">Theoretically one can use many different types of trees for </span><span class="calibre5">hierarchical</span><span class="calibre5"> softmax</span>. You can randomly generate the tree. Or you can use existing linguistic resources such as WordNet. Morin and Benzio used this and showed that there was a 258x improvement over the randomly generated tree. But the nodes in the trees built this way generally have more than one edge. Another strategy is to learn the hierarchy using a recursive partitioning strategy or clustering strategy. The clustering algorithm can be a greedy approach, as shown in <em class="calibre16">Self-organized Hierarchical Softmax</em> by Yikang Shen, Shawn Tan, Christopher Pal, and Aaron Courville. We have another option in the form of Huffman codes, which are traditionally used in data compression circles. Since we are quite interested in clustering the documents by Nikhil Pawar, 2012, it is seen that when Huffman encoding is used to encode strings to integers, the clustering on the integer instances is much more effective. In word2vec and fastText, the Huffman tree is used. An interesting property of Huffman trees is that w<span class="calibre5">hile an inner unit of a binary tree may not always have both children, a binary Huffman tree's inner units always do:</span></p>
<div class="cdpaligncenter"><img src="../images/00051.jpeg" class="calibre38"/></div>
<div class="mce-root">Tree generated using http://huffman.ooz.ie/?text=abcab</div>
<p class="calibre2">When building a Huffman tree, codes are assigned to the tokens such that the length of the code depends on the relative frequency or weight of the token. For example, in the previous example, the code for word E is 0000, which is one of the highest, and hence you can think that this word has occurred the most number of times in the corpus.</p>
<p class="calibre2">The code to build the tree can be found at <a href="https://github.com/facebookresearch/fastText/blob/d72255386b8cd00981f4a83ae346754697a8f4b4/src/model.cc#L279" class="calibre9">https://github.com/facebookresearch/fastText/blob/d72255386b8cd00981f4a83ae346754697a8f4b4/src/model.cc#L279<span>.</span></a></p>
<p class="calibre2">You can find the python implementation as part of the <kbd class="calibre12">Vocab</kbd> class in the method <kbd class="calibre12">encode_huffman</kbd>. For a simpler implementation, you can find the python implementation in the <kbd class="calibre12">huffman coding.ipynb</kbd> notebook in <kbd class="calibre12">chapter3</kbd> folder of the repository.</p>
<p class="calibre2">For the update equations, the computational complexity per training instance reduces from <em class="calibre16">O(V)</em> to <em class="calibre16">O(log(V))</em>, which is a huge improvement in terms of speed. We still roughly have the same number of parameters (<em class="calibre16">V-1</em> vectors for the inner units as compared to originally <em class="calibre16">V</em> output vectors for words).</p>
<div class="packt_infobox">Google Allo uses hierarchical softmax layer to make their phrase recommendation faster.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Negative sampling</h1>
                
            
            <article>
                
<p class="calibre2">An alternative to the hierarchical softmax is <strong class="calibre4">noise contrast estimation</strong> (<strong class="calibre4">NCE</strong>), which was introduced by Gutmann and Hyvarinen and applied to language modeling by Mnih and Teh. NCE posits that a good model should be able to differentiate data from noise by means of logistic regression.</p>
<p class="calibre2">While NCE can be shown to approximate the log probability of the softmax, the skipgram model is only concerned with the learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retrain their quality. We define negative sampling by the following objective:</p>
<p class="calibre2"><img class="fm-editor-equation25" src="../images/00052.jpeg"/></p>
<p class="calibre2">This is used to replace the <em class="calibre16">log(P(W<sub class="calibre29">O</sub> | W<sub class="calibre29">I</sub>))</em> term in the skipgram objective. Thus, the task is to distinguish the target word <em class="calibre16">w<sub class="calibre29">O</sub></em> from draws from the noise distribution <em class="calibre16">P<sub class="calibre29">n</sub>(w)</em> using logistic regression, where there are <em class="calibre16">k</em> negative samples for each data sample. In fastText, five negatives are sampled by default. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Subsampling of frequent words</h1>
                
            
            <article>
                
<p class="calibre2">In a very large corpus, the most frequent words can easily occur hundreds of millions of times, for example, words such as "in", "the" and "a". Such words generally provide less information value than the rare words. You can easily see that while the fastText model benefits from observing co-occurrences of "France" and "Paris", it benefits much less from observing co-occurrences of "France" and "the", as nearly every word co-occurs frequently within a sentence with "the". The idea can also be applied in the opposite direction. The vector representations of frequent words do not change significantly after training on several million additional examples.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">To counter the imbalance between the rare and frequent words, we use a simple subsampling approach: each word <em class="calibre16">w<sub class="calibre29">i</sub></em> in the training set is discarded with the probability computed by the following formula:</p>
<p class="calibre2"><img class="fm-editor-equation26" src="../images/00053.jpeg"/></p>
<p class="calibre2">Here, the function <em class="calibre16">f</em> is the frequency of the ith word <em class="calibre16">w</em> and <em class="calibre16">t</em> is a chosen threshold and hence a hyperparameter. In fastText, the default value of t is chosen to be 0.0001. The code for this can be found at <a href="https://github.com/facebookresearch/fastText/blob/53dd4c5cefec39f4cc9f988f9f39ab55eec6a02f/src/dictionary.cc#L281" class="calibre9">https://github.com/facebookresearch/fastText/blob/53dd4c5cefec39f4cc9f988f9f39ab55eec6a02f/src/dictionary.cc#L281<span>.</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Context definitions</h1>
                
            
            <article>
                
<p class="calibre2">Generally speaking, for a sentence of n words <em class="calibre16">w<sub class="calibre29">1</sub></em>, <em class="calibre16"><span class="calibre5">w<sub class="calibre29">2</sub></span></em><sub class="calibre29">, ...,</sub> <span class="calibre5"><em class="calibre16">w<sub class="calibre29">n</sub></em> contexts of a word <em class="calibre16">w<sub class="calibre29">i</sub></em> comes from window of size <em class="calibre16">k</em> around the word:</span></p>
<p class="calibre2"><img class="fm-editor-equation27" src="../images/00054.jpeg"/></p>
<p class="calibre2">Here, <em class="calibre16">k</em> is a parameter. However there are two subtleties:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Dynamic window size</strong>: The window size that is being used is dynamic—the parameter <em class="calibre21">k</em> denotes the maximal window size. For each word in the corpus, a window size <em class="calibre21">k<sup class="calibre23">'</sup></em> is sampled uniformly from <em class="calibre21">1</em>,...,<em class="calibre21">k</em>.</li>
<li class="calibre11"><strong class="calibre1">Effect of subsampling and rare word pruning: </strong>Similar to word2vec, fastText has two additional parameters for discarding some of the input words: words appearing less frequently than <kbd class="calibre12">minCount</kbd> are not considered as either words or contexts, and in addition, frequent words (as defined by the <em class="calibre21">-t</em> parameter) are down sampled. Importantly, these words are removed from the text before generating the contexts. This has the effect of increasing the effective window size for certain words. Subsampling of frequent words should improve the quality of the resultant embeddings.</li>
</ul>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, you have taken a look at unsupervised learning in fastText, as well as the algorithms and methods that enable it.</p>
<p class="calibre2">The next chapter will be about how fastText has approached supervised learning and you will also learn about how model quantization works in fastText.</p>


            </article>

            
        </section>
    </body></html>