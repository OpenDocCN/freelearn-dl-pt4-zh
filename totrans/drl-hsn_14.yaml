- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web Navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now take a look at some other practical applications of reinforcement
    learning (RL): web navigation and browser automation. This is a really useful
    example of how RL methods could be applied to a practical problem, including the
    complications you might face and how they could be addressed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss web navigation in general and the practical application of browser automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore how web navigation can be solved with an RL approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a deep look at one very interesting, but commonly overlooked and a bit
    abandoned, RL benchmark that was implemented by OpenAI, called Mini World of Bits
    (MiniWoB).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of web navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the web was invented, it started as several text-only web pages interconnected
    by hyperlinks. If you’re curious, here is the home of the first web page, [http://info.cern.ch](http://info.cern.ch),
    with text and links. The only thing you can do is read the text and click on links
    to navigate between pages.
  prefs: []
  type: TYPE_NORMAL
- en: Several years later, in 1995, the Internet Engineering Task Force (IETF) published
    the HTML 2.0 specification, which had a lot of extensions to the original version
    invented by Tim Berners-Lee. Among these extensions were forms and form elements
    that allowed web page authors to add activity to their websites. Users could enter
    and change text, toggle checkboxes, select drop-down lists, and push buttons.
    The set of controls was similar to a minimalistic set of graphical user interface
    (GUI) application controls. The difference was that this happened inside the browser’s
    window, and both the data and user interface (UI) controls that users interacted
    with were defined by the server’s page, but not by the local installed application.
  prefs: []
  type: TYPE_NORMAL
- en: Fast forward 29 years, and now we have JavaScript, HTML5 canvas, and Microsoft
    Office applications working inside our browsers. The boundary between the desktop
    and the web is so thin and blurry that you may not even know whether the app you’re
    using is an HTML page or a native app. However, it is still the browser that understands
    HTML and communicates with the outside world using HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, web navigation is defined as the process of a user interacting
    with a website or websites. The user can click on links, type text, or carry out
    any other actions to reach their goal, such as sending an email, finding out the
    exact dates of the French Revolution, or checking recent Facebook notifications.
    All this will be done using web navigation, so that leaves a question: can our
    program learn how to do the same?'
  prefs: []
  type: TYPE_NORMAL
- en: Browser automation and RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a long time, automating website interaction focused on the very practical
    tasks of website testing and web scraping. Website testing is especially critical
    when you have a complicated website that you (or other people) have developed
    and you want to ensure that it does what it is supposed to do. For example, if
    you have a login page that has been redesigned and is ready to be deployed on
    a live website, then you will want to be sure that this new design does sane things
    in case a wrong password is entered, the user clicks on I forgot my password,
    and so on. A complex website could potentially include hundreds or thousands of
    use cases that should be tested on every release, so all such functions should
    be automated.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping solves the problem of extracting data from websites at scale. For
    example, if you want to build a system that aggregates all prices for all the
    pizza places in your town, you will potentially need to deal with hundreds of
    different websites, which could be problematic to build and maintain. Web scraping
    tools try to solve the problem of interacting with websites, providing various
    functionality from simple HTTP requests and subsequent HTML parsing to full emulation
    of the user moving the mouse, clicking buttons, user’s reaction delays, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard approach to browser automation normally allows you to control
    the real browser, such as Chrome or Firefox, with your program, which can observe
    the web page data, like the Document Object Model (DOM) tree and an object’s location
    on the screen, and issue the actions, like moving the mouse, pressing some keys,
    pushing the Back button, or just executing some JavaScript code. The connection
    to the RL problem setup is obvious: our agent interacts with the web page and
    browser by issuing actions and observing the state. The reward is not that clear
    and should be task-specific, like successfully filling a form in or reaching the
    page with the desired information. Practical applications of a system that could
    learn browser tasks are related to the previous use cases, and include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In web testing for very large websites, it’s extremely tedious to define the
    testing process using low-level browser actions like “move the mouse five pixels
    to the left, then press the left button.” What you want to do is give the system
    some demonstrations and let it generalize and repeat the shown actions in all
    similar situations, or at least make it robust enough for UI redesign, button
    text change, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many cases when you don’t know the problem in advance, for example,
    when you want the system to explore the weak points of the website, like security
    vulnerabilities. In that case, the RL agent could try a lot of weird actions very
    quickly, much faster than humans could. Of course, the action space for security
    testing is enormous, so random clicking won’t be as effective as experienced human
    testers. In that case, the RL-based system could, potentially, combine the prior
    knowledge and experience of humans but still keep the ability to explore and learn
    from this exploration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another potential domain that could benefit from RL browser automation is scraping
    and web data extraction in general. For example, you might want to extract some
    data from hundreds of thousands of different websites, like hotel websites, car
    rental agents, or other businesses around the world. Very often, before you get
    to the desired data, a form with parameters needs to be filled out, which becomes
    a very nontrivial task given the different websites’ design, layout, and natural
    language flexibility. With such a task, an RL agent can save tons of time and
    effort by extracting the data reliably and at scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in browser automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Potential practical applications of browser automation with RL are attractive
    but have one very serious drawback: they’re too large to be used for research
    and the comparison of methods. In fact, the implementation of a full-sized web
    scraping system could take months of effort from a team, and most of the issues
    would not be directly related to RL, like data gathering, browser engine communication,
    input and output representation, and lots of other questions that real production
    system development consists of.'
  prefs: []
  type: TYPE_NORMAL
- en: By solving all these issues, we can easily miss the forest by looking at the
    trees. That’s why researchers love benchmark datasets, like MNIST, ImageNet, and
    the Atari suite. However, not every problem makes a good benchmark. On the one
    hand, it should be simple enough to allow quick experimentation and comparison
    between methods. On the other hand, the benchmark has to be challenging and leave
    room for improvement. For example, Atari benchmarks consist of a wide variety
    of games, from very simple ones that can be solved in half an hour (like Pong),
    to quite complex games that were properly solved only recently (like Montezuma’s
    Revenge, which requires the complex planning of actions). To the best of my knowledge,
    there is only one such benchmark for the browser automation domain, which makes
    it even worse that this benchmark was undeservedly forgotten by the RL community.
    As an attempt to fix this issue, we will take a look at the benchmark in this
    chapter. Let’s talk about its history first.
  prefs: []
  type: TYPE_NORMAL
- en: The MiniWoB benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In December 2016, OpenAI published a dataset called MiniWoB that contains 80
    browser-based tasks. These tasks are observed at the pixel level (strictly speaking,
    besides pixels, a text description of tasks is given to the agent) and are supposed
    to be communicated with the mouse and keyboard actions using the Virtual Network
    Computing (VNC) client. VNC is a standard remote desktop protocol by which a VNC
    server allows clients to connect to and work with a server’s GUI applications
    using the mouse and keyboard via the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 80 tasks vary a lot in terms of complexity and the actions required from
    the agent. Some tasks are very simple, even for RL, like “click on the dialog’s
    close button,” or “push the single button,” but some require multiple steps, for
    example, “open collapsed groups and click on the link with some text,” or “select
    a specific date using the date picker tool” (and this date is randomly generated
    every episode). Some of the tasks are simple for humans but require character
    recognition, for example, “mark checkboxes with this text” (and the text is generated
    randomly). Some screenshots of MiniWoB problems are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: MiniWoB environments'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, despite the brilliant idea and the challenging nature of MiniWoB,
    it was almost abandoned by OpenAI right after the initial release. Several years
    later, a group of Stanford researchers released an updated version called MiniWoB++,
    which had more games and a reworked architecture.
  prefs: []
  type: TYPE_NORMAL
- en: MiniWoB++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of using VNC protocol and a real web browser, MiniWoB++ uses the Selenium
    ([https://www.selenium.dev](https://www.selenium.dev)) library for web browser
    automation, which has significantly increased the performance and stability of
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, MiniWob++ is being maintained by the Farama Foundation ( [https://miniwob.farama.org/](https://miniwob.farama.org/)),
    which is really great news for the RL community. In this chapter, we’ll use their
    latest version, but before jumping into the RL part of the agent, we need to understand
    how MiniWoB++ works.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The original MiniWoB used VNC and OpenAI Universe, which created lots of complications
    during installation and usage. The previous edition of this book provided a custom
    Docker image with detailed installation instructions. Now, it is much simpler:
    you don’t need to deal with Docker and VNC anymore. The Selenium library (which
    is a de facto standard in browser automation) hides all the complications of communicating
    with the browser, which is started in the background in headless mode. Selenium
    supports various browsers, but the MiniWoB++ developers recommend using Chrome
    or Chromium, as other browsers might render environments differently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the MiniWoB++ package (which can be installed with pip install miniwob==1.0),
    you will need chromedriver to be set up on your machine. ChromeDriver is a small
    binary that communicates with the browser and runs it in the “testing mode.” The
    version of ChromeDriver has to match the installed version of Chrome (to check,
    go to Chrome → About Google Chrome), so please download the chromedriver archive
    for your platform and Chrome version from this website: [https://googlechromelabs.github.io/chrome-for-testing/](https://googlechromelabs.github.io/chrome-for-testing/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful: besides the ChromeDriver archive, they also provide archives for
    the full version of Chrome, most likely you don’t need it. For example, chromedriver
    for Chrome v123 on Mac M2 hardware will have this URL: [https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip](https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip).
    In the archive, a single chromedriver binary is present, which should be put somewhere
    in the PATH of your shell (on Mac and Linux machines, you can use the which chromedriver
    console command, which has to write the full path to the binary. If nothing is
    shown, you need to modify the PATH). To test your installation, you can use a
    simple program, Chapter14/adhoc/01_wob_create.py. If everything is working, a
    browser window with a task will appear for 2 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: Actions and observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast with Atari games and the other Gym environments that we have worked
    with so far, MiniWoB exposes a much more generic action space. Atari games used
    six or seven discrete actions corresponding to the controller’s buttons and joystick
    directions. CartPole’s action space is even smaller, with just two actions. However,
    the browser gives our agent much more flexibility in terms of what it can do.
    First, the full keyboard, with control keys and the up/down state of every key,
    is exposed. So, your agent can decide to press 10 buttons simultaneously and it
    will be totally fine from a MiniWoB point of view. The second part of the action
    space is the mouse: you can move the mouse to any coordinates and control the
    state of its buttons. This significantly increases the dimensionality of the action
    space that the agent needs to learn how to handle. In addition, the mouse allows
    double-clicking and mouse-wheel up/down scrolling events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of observation space, MiniWoB is also much richer than the environments
    we’ve dealt with so far. The full observation is represented as a dict with the
    following data:'
  prefs: []
  type: TYPE_NORMAL
- en: Text with a description of the task, like Click button ONE or You are playing
    as X in TicTacToe, win the game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen’s pixel as RGB values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of all DOM elements from the underlying web page with attributes (dimensions,
    colors, font, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides that, you can access the underlying browser to get even more information
    (to get some information that is not directly provided, like CSS attributes or
    raw HTML data).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this set of tasks has lots of flexibility for experimentation:
    you can focus on the visual side of the task, working at the pixel level; you
    can use DOM information (the environment allows you to click on specific elements);
    or use NLP components — to understand the task description and plan the actions.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To gain some practical experience with MiniWoB, let’s take a look at the program
    you used to validate your installation, which you will find at Chapter14/adhoc/01_wob_create.py.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to register the MiniWoB environment in Gymnasium, which is done
    with the register_envs() function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In fact, this register_envs() function does nothing, as all the environments
    are registered when the module is imported. But modern IDEs are smart enough to
    start complaining about unused modules, so this method creates the impression
    for the IDE that the module is being used in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we create an environment using the standard gym.make() method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we’re using the click-test-2 problem, which asks you to click
    on one of two buttons randomly placed on the webpage. The Farama website contains
    a very convenient list of environments that you can play with yourself. The click-test-2
    problem is available here: [https://miniwob.farama.org/environments/click-test-2/](https://miniwob.farama.org/environments/click-test-2/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On environment creation, we passed the render_mode argument. If it equals ’human’,
    then the browser window will be shown in the background. In Figure [14.2](#x1-255021r2),
    you can see the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: click-test-2 environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the program, it will show us the environment object and information
    about the observation (which is quite a large dict, so, I output just a list of
    its keys). The following is the part that is shown by the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have utterance (which is a task to be performed), DOM elements,
    screenshot with exactly the same dimensions as the Atari platform (I don’t think
    this is just a coincidence!), and a list of fields, which are task-specific important
    elements in the DOM tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go back to our code. The following snippet finds the element in
    the dom_elements list that we have to click on to perform the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is iterating over the dom_elements observation’s field, filtering
    elements that have the text ONE. The element that is found has quite a rich set
    of attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the final piece of the code, where we take the reference
    of the element (which is an integer identifier) and create the CLICK_ELEMENT action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As we have already mentioned, MiniWoB provides a rich set of actions to be executed.
    This particular one emulates a mouse click on a specific DOM element.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of this action, we should get a reward, which in fact does happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you disable rendering with RENDER_ENV = False, everything that happens in
    the console and the browser won’t be shown. This mode will also lead to a higher
    reward, as the reward decreases with time. Full headless mode on my machine obtains
    a reward of 0.9918 in 0.09 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The simple clicking approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with web navigation, let’s implement a simple A3C agent that
    decides where it should click given the image observation. This approach can solve
    only a small subset of the full MiniWoB suite, and we will discuss the restrictions
    of this approach later. For now, it will allow us to get a better understanding
    of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: As with the previous chapter, I won’t discuss the complete source code here.
    Instead, we will focus on the most important functions and I will provide a brief
    overview of the rest. The complete source code is available in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Grid actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we talked about MiniWoB architecture and organization, we mentioned that
    the richness and flexibility of the action space creates a lot of challenges for
    the RL agent. The active area inside the browser is just 210 × 160 pixels, but
    even with such a small area, our agent could be asked to move the mouse, perform
    clicks, drag objects, and so on. Just the mouse alone could be problematic to
    master, as, in the extreme case, there could be an almost infinite number of different
    actions that the agent could perform, like pressing the mouse button at some point
    and dragging the mouse to a different location. In our example, we will simplify
    our problem a lot by just considering clicks at some fixed grid points inside
    the active webpage area. The sketch of our action space is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: A grid action space'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the original version of MiniWob, the wrapper for such actions was already
    present in OpenAI Universe. But since it is not available for MiniWoB++, I implemented
    it myself in the lib/wob.py module. Let’s quickly check the code, starting with
    the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the constructor, we create the observation space (which is a tensor of 3
    × 210 × 160) and the action space, which will be 256 discrete actions for a bin
    size of 10\. As an option, we can ask the wrapper to preserve the text of the
    task to be performed. This functionality will be used in subsequent examples in
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we provide a class method to create the environment with a specific configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides just creating the environment and wrapping it, we’re asking for a custom
    ActionSpaceConfig, which will take into account our grid’s dimensions. With this
    customization, we will need to pass the (x,y) coordinates of the grid cell to
    perform the click action. Then, we define a helper method, which converts the
    full observation dict into the format we need. The reset() method is just calling
    this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the final piece of the wrapper, the step() method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To perform the action, we need to convert the index of the grid cell index (in
    the 0…255 range) into the (x,y) coordinates of the cell. Then, as an action for
    the underlying MiniWoB environment, we pass a dict with action_type=0 (which is
    an index in ActionSpaceConfig we used in the environment creation) and a NumPy
    array with those cell coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the wrapper, there is a small program in the GitHub repository
    in the adhoc/03_clicker.py file, which uses a brute force approach on the click-dialog-v1
    task. The goal is to close the randomly placed dialog using the corner button
    with the cross. In this example (we’re not showing the code here), we sequentially
    click through all the 256 grid cells to illustrate the wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: The RL part of our implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the transformation of observation and actions, the RL part is quite straightforward.
    We will use the A3C method to train the agent, which should decide from the 160
    × 210 observation which grid cell to click on. Besides the policy, which is a
    probability distribution over 256 grid cells, our agent estimates the value of
    the state, which will be used as a baseline in policy gradient estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several modules in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'lib/common.py: Methods shared among examples in this chapter, including the
    already familiar RewardTracker and unpack_batch functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'lib/model.py: Includes a definition of the model, which we’ll take a look at
    in the next section'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'lib/wob.py: Includes MiniWoB-specific code, like environment wrappers and other
    utility functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'wob_click_train.py: The script used to train the clicker model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'wob_click_play.py: The script that loads the model weights and uses them against
    the single environment, recording observations and counting statistics about the
    reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is nothing new in the code in these modules, so it is not shown here.
    You can find it in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: The model and training code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model is very straightforward and uses the same patterns that you have
    seen in other A3C examples. I haven’t spent much time optimizing and fine-tuning
    the architecture and hyperparameters, so it’s likely that the final result could
    be improved significantly (you can try doing this yourself based on what you’ve
    learned in this book so far). The following is the model definition with two convolution
    layers, a single-layered policy, and value heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You’ll find the training script in wob_click_train.py, and it is exactly the
    same as in Chapter [12](ch016.xhtml#x1-20300012). We’re using AsyncVectorEnv with
    8 parallel environments, which starts 8 Chrome instances in the background. If
    your machine’s memory allows, you can increase this count and check the effect
    on the training.
  prefs: []
  type: TYPE_NORMAL
- en: Training results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, the training uses the click-dialog-v1 problem, and it took about
    8 minutes of training to reach an average reward of 0.9\. Figure [14.4](#x1-260002r4)
    shows the plots with average reward and number of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Training reward (left) and count of steps in episodes (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The episode_steps chart on the right shows the mean count of actions that the
    agent should carry out before the end of the episode. Ideally, for this problem,
    the count should be 1, as the only action that the agent needs to take is to click
    on the dialog’s close button. However, in fact, the agent sees seven to nine frames
    before the episode ends. This happens for two reasons: the cross on the dialog
    close button may appear after some delay, and the browser inside the container
    adds a time gap before the agent clicks and the reward is obtained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the learned policy, you can use the wob_click_play.py tool, which
    loads the model and uses it in one environment. It can play several episodes to
    test the average model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If this begins with the --render command-line option, the browser window will
    be shown during the agent’s actions.
  prefs: []
  type: TYPE_NORMAL
- en: Simple clicking limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, the demonstrated approach can only be used to solve relatively
    simple problems, like click-dialog. If you try to use it for more complicated
    tasks, convergence is unlikely. There are several reasons for this.
  prefs: []
  type: TYPE_NORMAL
- en: First, our agent is stateless, which means that it makes the decisions about
    actions only from observations, without taking into account its previous actions.
    You may remember that in Chapter [1](ch005.xhtml#x1-190001), we discussed the
    Markov property of the Markov decision process (MDP) and that this Markov property
    allowed us to discard all previous history, keeping only the current observation.
    Even in relatively simple problems from MiniWoB, this Markov property could be
    violated. For example, there is a problem called click-button-sequence (the screenshot
    is shown in Figure [14.5](#x1-261002r5), and documentation for this environment
    is available at [https://miniwob.farama.org/environments/click-button-sequence/](https://miniwob.farama.org/environments/click-button-sequence/)),
    which requires our agent to first click on button ONE and then on button TWO.
    Even if our agent is lucky enough to randomly click on the buttons in the required
    order, it won’t be able to distinguish from the single image which button needs
    to be clicked on next.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: An example of an environment that the stateless agent could struggle
    to solve'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the simplicity of this problem, we cannot use our RL methods to solve
    it, because MDP formalism is not applicable anymore. Such problems are called
    partially observable MDPs, or POMDPs (we briefly discussed these in Chapter [6](#)),
    and the usual approach for them is to allow the agent to keep some kind of state.
    The challenge here is to find the balance between keeping only minimal relevant
    information and overwhelming the agent with non-relevant information by adding
    everything into the observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another issue that we can face with our example is that the data required to
    solve the problem might not be available in the image or could be in an inconvenient
    form. For example, two problems, click-tab ( [https://miniwob.farama.org/environments/click-tab/](https://miniwob.farama.org/environments/click-tab/))
    and click-checkboxes ([https://miniwob.farama.org/environments/click-checkboxes/](https://miniwob.farama.org/environments/click-checkboxes/)),
    are shown in Figure [14.6](#x1-261005r6):'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: An example of environments where the text description is important'
  prefs: []
  type: TYPE_NORMAL
- en: In the first one, you need to click on one of three tabs, but every time, the
    tab that needs to be clicked is randomly chosen. Which tab needs to be clicked
    is shown in a description (provided with an in-text field of observation and shown
    at the top of the environment’s page), but our agent sees only pixels, which makes
    it complicated to connect the tiny number at the top with the outcome of the random
    click result. The situation is even worse with the click-checkboxes problem, when
    several checkboxes with randomly generated text need to be clicked. One of the
    possible ways to prevent overfitting to the problem is to use some kind of optical
    character recognition (OCR) network to convert the image in the observation into
    text form. Another approach (which will be shown in the next section) is to mix
    the text description into the agent’s observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another issue could be related to the dimensionality of the action space
    that the agent needs to explore. Even for single-click problems, the number of
    actions could be very large, so it can take a long time for the agent to discover
    how to behave. One of the possible solutions here is incorporating demonstrations
    into the training. For example, in Figure [14.7](#x1-261007r7), there is a problem
    called count-sides ([https://miniwob.farama.org/environments/count-sides/](https://miniwob.farama.org/environments/count-sides/)).
    The goal there is to click on the button that corresponds to the number of sides
    of the shape shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Examples of the count-sides environment'
  prefs: []
  type: TYPE_NORMAL
- en: This issue was addressed by adding human demonstrations into the training. In
    my experiments, training from scratch gave zero progress after a day of training.
    However, after adding a couple of dozen examples of correct clicks, it successfully
    solved the problem in 15 minutes of training. Of course, we could spend time fine-tuning
    the hyperparameters further, but still, the effect of the demonstrations is quite
    impressive. Later in this chapter, we will take a look at how we can record and
    inject human demonstrations to improve convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Adding text description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a first step to improve our clicker agent, we’ll add the text description
    of the problem into the model. I have already mentioned that some problems contain
    vital information that is provided in a text description, like the index of tabs
    that need to be clicked or the list of entries that the agent needs to check.
    The same information is shown at the top of the image observation, but pixels
    are not always the best representation of simple text.
  prefs: []
  type: TYPE_NORMAL
- en: To take this text into account, we need to extend our model’s input from an
    image only to an image and text data. We worked with text in the previous chapter,
    so a recurrent neural network (RNN) is quite an obvious choice (maybe not the
    best for such a toy problem, but it is flexible and scalable).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will just focus on the most important points of the implementation.
    You will find the whole code in the Chapter16/wob_click_mm_train.py module. In
    comparison to our clicker model, a text extension doesn’t add too much.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we should ask MiniWoBClickWrapper to keep the text obtained from the
    observation. The complete source code of this class was shown earlier in this
    chapter, in the Grid actions section. To keep the text, we should pass keep_text=True
    to the wrapper constructor, which makes this class return a tuple with a NumPy
    array and text string, instead of just a NumPy array with the image. Then, we
    need to prepare our model to be able to process such tuples instead of a batch
    of NumPy arrays. This needs to be done in two places: in our agent (when we use
    the model to choose the action) and in the training code. To adapt the observation
    in a model-friendly way, we can use a special functionality of the PTAN library,
    called preprocessor. The core idea is very simple: preprocessor is a callable
    function that needs to convert the list of observations to a form that is ready
    to be passed to the model. By default, preprocessor converts the list of NumPy
    arrays into a PyTorch tensor and, optionally, copies it into GPU memory. However,
    sometimes, more sophisticated transformations are required, like in our case,
    when we need to pack the images into the tensor, but text strings require special
    handling. In that case, you can redefine the default preprocessor and pass it
    into the ptan.Agent class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, the preprocessor functionality could be moved into the model itself,
    thanks to PyTorch’s flexibility, but the default preprocessor simplifies our lives
    in cases when observations are just NumPy arrays. The following is the preprocessor
    class source code taken from the lib/model.py module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the constructor in the preceding code, we create a mapping from the token
    to the identifier (which will be dynamically extended) and create the tokenizer
    from the nltk package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have the __call__() method, which transforms the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal of our preprocessor is to convert a batch of (image, text) tuples
    into two objects: the first has to be a tensor with the image data of shape (batch_size,
    3, 210, 160), and the second has to contain the batch of tokens from text descriptions
    in the form of a packed sequence. The packed sequence is a PyTorch data structure
    suitable for efficient processing with an RNN. We discussed this in Chapter [13](ch017.xhtml#x1-21900013).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the batch could have two different forms: it could be a tuple with
    an image batch and a text batch, or it could be a list of tuples with individual
    (image, text tokens) samples. This happens because of the difference in VectorEnv
    handling of gym.Tuple observation space. But those details are not very relevant
    here; we just handle the difference by checking the type of the batch variable
    and performing the necessary processing.'
  prefs: []
  type: TYPE_NORMAL
- en: As the first step of our transformation, we tokenize text strings and convert
    every token into the list of integer IDs. Then, we sort our batch by decreasing
    the token length, which is a requirement of the underlying cuDNN library for efficient
    RNN processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we convert the images into a tensor and the sequences into a padded sequence,
    which is a matrix of batch size × the length of the longest sequence. We saw this
    in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following tokens_to_idx() function converts the list of tokens into a list
    of IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The tricky thing is that we don’t know in advance the size of the dictionary
    from the text descriptions. One approach would be to work on the character level
    and feed individual characters into the RNN, but it would result in sequences
    that are too long to process. The alternative solution is to hard-code a reasonable
    dictionary size, say 100 tokens, and dynamically assign token IDs to tokens that
    we have never seen before. In this implementation, the latter approach is used,
    but it might not be applicable to MiniWoB problems that contain randomly generated
    strings in the text description. As potential solutions for this issue, we can
    either use character-level tokenization or use a pre-defined dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at our model class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The difference is in a new embedding layer, which converts integer token IDs
    into dense token vectors and a long short-term memory (LSTM) RNN. The outputs
    from the convolution and RNN layers are concatenated and fed into the policy and
    value heads, so the dimensionality of their input is the image and text features
    combined.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function performs the concatenation of the image and RNN features into
    a single tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the forward() function, we expect two objects prepared by the preprocessor:
    a tensor with input images and packed sequences of the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Images are processed with convolutions and text data is fed through the RNN;
    then, the results are concatenated, and the policy and value results are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: That’s most of the new code. The training Python script, wob_click_mm_train.py,
    is mostly a copy of wob_click_train.py, with just the small modifications in the
    wrapper creation, a different model, and preprocessor.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I ran several experiments in the click-button environment ( [https://miniwob.farama.org/environments/click-button/](https://miniwob.farama.org/environments/click-button/))
    that have the goal of making a selection between several random buttons. In Figure [14.8](#x1-264002r8),
    several situations in this environment are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file185.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Tasks in the click-button environment'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure [14.9](#x1-264003r9), after 3 hours of training, the model
    was able to learn how to click (the average count of steps in episodes was reduced
    to 5-7) and get to an average reward of 0.2\. But subsequent training had no visible
    effect. It might be an indication that the hyperparameters have to be tuned, or
    of the ambiguity of the environment. In this case, I noticed that this environment
    sometimes shows several buttons with the same title, but only one of them gives
    a positive reward. An example of this is shown in the first section of Figure [14.8](#x1-264002r8),
    where two identical Submit buttons are present.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Training reward (left) and count of steps in episodes (right)
    on click-button'
  prefs: []
  type: TYPE_NORMAL
- en: Another environment in which the text description is important is click-tab,
    which demands the agent to click on a specific tab, chosen randomly. Screenshots
    are shown in Figure [14.10](#x1-264005r10).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Tasks in the click-tab environment'
  prefs: []
  type: TYPE_NORMAL
- en: In this environment, the training was not successful, which is a bit strange,
    as the task looks easier than click-button (the position of the place to click
    is fixed). Most likely, hyperparameter tuning is required. This is another interesting
    challenge that you can try to address through experimentation, using the knowledge
    you’ve gained so far.
  prefs: []
  type: TYPE_NORMAL
- en: Human demonstrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to improve the training process, let’s try to incorporate human demonstrations.
    The idea behind demonstrations is simple: to help our agent to discover the best
    way to solve the task, we show it some examples of actions that we think are required
    for the problem. Those examples might not be the best solution or not 100% accurate,
    but they should be good enough to show the agent promising directions to explore.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this is a very natural thing to do, as all human learning is based
    on some prior examples given by a teacher in class, parents, or other people.
    Those examples could be in a written form (for example, recipe books) or given
    as demonstrations that you need to repeat several times to get right (for example,
    dance classes). Such forms of training are much more effective than random searches.
    Just imagine how complicated and lengthy it would be to learn how to clean your
    teeth by trial and error alone. Of course, there is a danger from learning how
    to follow demonstrations, which could be wrong or not the most efficient way to
    solve the problem; but overall, it’s much more effective than a random search.
  prefs: []
  type: TYPE_NORMAL
- en: 'All our previous examples followed this workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: They used zero prior knowledge and started with random weight initializations,
    which caused random actions to be performed at the beginning of the training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After some iterations, the agent discovered that some actions in some states
    give more promising results (via the Q-value or policy with the higher advantage)
    and started to prefer those actions over the others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, this process led to a more or less optimal policy, which gave the agent
    a high reward at the end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This worked well when our action space dimensionality was low and the environment’s
    behavior wasn’t very complex, but just doubling the action count caused at least
    twice the observations needed. In the case of our clicker agent, we have 256 different
    actions corresponding to 10 × 10 grids in the active area, which is 128 times
    more actions than we had in the CartPole environment. It is not surprising that
    the training process is lengthy and may fail to converge at all.
  prefs: []
  type: TYPE_NORMAL
- en: This issue of dimensionality can be addressed in various ways, like smarter
    exploration methods, training with better sampling efficiency (one-shot training),
    incorporating prior knowledge (transfer learning), and other means. There is a
    lot of research activity focused on making RL better and faster, and we can be
    sure that many breakthroughs are ahead. In this section, we will try the more
    traditional approach of incorporating the demonstration recorded by humans into
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might remember our discussion about on-policy and off-policy methods (which
    were discussed in Chapter [4](ch008.xhtml#x1-740004) and Chapter [8](ch012.xhtml#x1-1240008)).
    This is very relevant to our human demonstrations because, strictly speaking,
    we can’t use off-policy data (human observation-action pairs) with an on-policy
    method (A3C in our case). That is due to the nature of on-policy methods: they
    estimate the policy gradients using the samples gathered from the current policy.
    If we just push human-recorded samples into the training process, the estimated
    gradient will be relevant for a human policy, but not our current policy given
    by the neural network (NN). To solve this issue, we need to cheat a bit and look
    at our problem from the supervised learning angle. To be concrete, we will use
    the log-likelihood objective to push our NN toward taking actions based on demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: With this, we’re not replacing RL with supervised learning. Rather, we’re reusing
    supervised learning techniques to help our RL methods. Fundamentally, this isn’t
    the first time we’ve done something similar; for instance, the training of the
    value function in Q-learning is purely supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can go into the implementation details, we need to address a very
    important question: how do we obtain the demonstrations in the most convenient
    form?'
  prefs: []
  type: TYPE_NORMAL
- en: Recording the demonstrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before MiniWoB++ and the transition to Selenium, recording a demonstration was
    technically challenging. In particular, the VNC protocol has to be captured and
    decoded to be able to extract screenshots of the browser and the actions executed
    by the user. In the previous edition of the book, I provided my own version of
    the VNC protocol parser to record the demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, those challenges are mostly gone now. There is no VNC anymore and the
    browser has been started in the local process (before, it was inside the Docker
    container), so we can communicate with it almost directly.
  prefs: []
  type: TYPE_NORMAL
- en: Farama MiniWoB++ is shipped with a Python script that can capture the demonstrations
    in a JSON file. This script can be started with the python -m miniwob.scripts.record
    command and is documented at [https://miniwob.farama.org/content/demonstrations/](https://miniwob.farama.org/content/demonstrations/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, it has a limitation: in observations, it captures only the DOM
    structure of the webpage and has no pixel-level information. As examples in this
    chapter make heavy use of pixels, demonstrations recorded by this script are useless.
    To overcome this, I implemented my own version of a tool to record demonstrations
    that include pixels from the browser. It is called Chapter14/record_demo.py and
    can be started as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This command starts the environment with render_mode=’human’, which shows the
    browser window and allows you to communicate with the page. In the background,
    it records the observations (with screenshots) and, when the episode is done,
    it joins screenshots to your actions and stores everything in a JSON file in the
    directory given by the -o command-line option. Using the -g command-line option
    allows you to change the environment, and the -d parameter sets the delay in seconds
    between the episodes. If the -d option is not given, you need to press Enter in
    the console to start a new episode. The following screenshot shows the process
    of recording a demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Recording a human demonstration for tic-tac-toe'
  prefs: []
  type: TYPE_NORMAL
- en: In the Chapter14/demos directory, I stored the demonstrations used for experiments,
    but you, of course, can record your own demonstrations using the provided script.
  prefs: []
  type: TYPE_NORMAL
- en: Training with demonstrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know how to record the demonstration data, we have only one question
    unanswered: how does our training process need to be modified to incorporate human
    demonstrations? The simplest solution, which nevertheless works surprisingly well,
    is to use the log-likelihood objective that we used in training the cross-entropy
    method in Chapter [4](ch008.xhtml#x1-740004). To do this, we need to look at our
    A3C model as a classification problem producing the classification of input observations
    in its policy head. In its simplest form, the value head will be left untouched,
    but, in fact, it won’t be hard to train it: we know the rewards obtained during
    the demonstrations, so what is needed is to calculate the discounted reward from
    every observation until the end of the episode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check how it was implemented, let’s look at the relevant code pieces in
    Chapter16/wob_click_train.py. First, we can pass the directory with the demonstration
    data by passing the demo <DIR> option in the command line. This will enable the
    branch shown in the following code block, where we load the demonstration samples
    from the specified directory. The demos.load_demo_dir() function automatically
    loads demonstrations from JSON files in the given directory and converts them
    into ExperienceFirstLast instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The second piece of code relevant to demonstration training is inside the training
    loop and is executed before any normal batch. The training from demonstrations
    is performed with some probability (by default, it is 0.5) and specified by the
    DEMO_PROB hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic is simple: with probability DEMO_PROB, we sample BATCH_SIZE samples
    from our demonstration data and perform a round of training of our network on
    the data in the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual training, which is very simple and straightforward, is performed
    by the model.train_demo() function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We split our batch on the observation and the actions list, preprocess the observations
    to convert them into a PyTorch tensor, and place them on the GPU. We then ask
    our A3C network to return the policy and calculate the cross-entropy loss between
    the result and the desired actions. From an optimization point of view, we’re
    pushing our network toward the actions taken in the demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To check the effect of demonstrations, I performed two sets of training on
    the count-sides problem with the same hyperparameters: one was done without demonstrations,
    and another used 25 demonstration episodes, which are available in the demos/count-sides
    directory.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference was dramatic. Training performed from scratch reached the best
    mean reward of -0.4 after 12 hours of training and 4 million frames without any
    significant improvement in the training dynamics. On the other hand, training
    with demonstrations was able to get to the average reward of 0.5 just after 30,000
    training frames, which took 8 minutes. Figure [14.12](#x1-268002r12) shows the
    reward and the count of steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Training reward (left) and count of steps in episodes (right)
    on count-sides with demonstrations'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more challenging problem I experimented with is the Tic Tac Toe game, available
    as the tic-tac-toe environment. Figure [14.13](#x1-268003r13) shows the process
    of one of the demo games I recorded (available in the demos/tic-tac-toe directory).
    The dot shows where the click was performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file192.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Demonstration TicTacToe game'
  prefs: []
  type: TYPE_NORMAL
- en: After two hours of training, the best average reward reached was 0.05, which
    means that the agent can win some games, but some are lost or end up in a draw.
    In Figure [14.14](#x1-268005r14), plots with reward dynamics and the count of
    episode steps are shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/B22150_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Training reward (left) and count of steps in episodes (right)
    on tic-tac-toe with demonstrations'
  prefs: []
  type: TYPE_NORMAL
- en: Things to try
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we only started playing with MiniWoB++ by looking at some
    of the easiest environments from the full set of over 100 problems, so there is
    plenty of uncharted territory ahead. If you want to practice, there are several
    items you can experiment with:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the robustness of demonstrations to noisy clicks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action space for the clicking approach could be improved by predicting the
    x and y coordinates of the place to click.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DOM data could be used instead of (or in addition to) screen pixels. Then, the
    prediction will be the element of the tree to be clicked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try other problems. There is a wide variety of them, requiring keyboard events
    to be generated, the sequence of actions planned, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very recently, the LaVague project was published ([https://github.com/lavague-ai/LaVague](https://github.com/lavague-ai/LaVague)),
    which uses LLMs for web automation. Their approach is to ask an LLM to generate
    Selenium Python code to perform specific tasks. It will be very interesting to
    check it against MiniWoB++ problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw the practical application of RL methods for browser
    automation and used the MiniWoB++ benchmark. I believe that browser automation
    (and communicating with software humans are using in general) is an important
    milestone in future AI development.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes Part 3 of the book. The next part will be devoted to
    more complicated and recent methods related to continuous action spaces, non-gradient
    methods, and other more advanced methods of RL.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at continuous control problems, which
    are an important subfield of RL, both theoretically and practically.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a Review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an Amazon review; it will only take
    a minute, but it makes a big difference for readers like you. Scan the QR code
    below to receive a free ebook of your choice. [https://packt.link/NzOWQ](https://packt.link/NzOWQ)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: Part 4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
