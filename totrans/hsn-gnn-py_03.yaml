- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating Node Representations with DeepWalk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DeepWalk** is one of the first major successful applications of **machine
    learning** (**ML**) techniques to graph data. It introduces important concepts
    such as embeddings that are at the core of GNNs. Unlike traditional neural networks,
    the goal of this architecture is to produce **representations** that are then
    fed to other models, which perform downstream tasks (for example, node classification).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the DeepWalk architecture and its two
    major components: `gensim` library on a **natural language processing** (**NLP**)
    example to understand how it is supposed to be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will focus on the DeepWalk algorithm and see how performance can be
    improved using **hierarchical softmax** (**H-Softmax**). This powerful optimization
    of the softmax function can be found in many fields: it is incredibly useful when
    you have a lot of possible classes in your classification task. We will also implement
    random walks on a graph before wrapping things up with an end-to-end supervised
    classification exercise on Zachary’s Karate Club.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will master Word2Vec in the context of NLP and
    beyond. You will be able to create node embeddings using the topological information
    of the graphs and solve classification tasks on graph data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepWalk and random walks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing DeepWalk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter03).
    Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step to comprehending the DeepWalk algorithm is to understand its
    major component: Word2Vec.'
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec has been one of the most influential deep-learning techniques in NLP.
    Published in 2013 by Tomas Mikolov et al. (Google) in two different papers, it
    proposed a new technique to translate words into vectors (also known as **embeddings**)
    using large datasets of text. These representations can then be used in downstream
    tasks, such as sentiment classification. It is also one of the rare examples of
    patented and popular ML architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of how Word2Vec can transform words into vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_001.jpg)![](img/Formula_B19153_03_002.jpg)![](img/Formula_B19153_03_003.jpg)![](img/Formula_B19153_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see in this example that, in terms of the Euclidian distance, the word
    vectors for *king* and *queen* are closer than the ones for *king* and *woman*
    (4.37 versus 8.47). In general, other metrics, such as the popular **cosine similarity**,
    are used to measure the likeness of these words. Cosine similarity focuses on
    the angle between vectors and does not consider their magnitude (length), which
    is more helpful in comparing them. Here is how it is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the most surprising results of Word2Vec is its ability to solve analogies.
    A popular example is how it can answer the question “*man is to woman, what king
    is to ___?*” It can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is not true with any analogy, but this property can bring interesting applications
    to perform arithmetic operations with embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: CBOW versus skip-gram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A model must be trained on a pretext task to produce these vectors. The task
    itself does not need to be meaningful: its only goal is to produce high-quality
    embeddings. In practice, this task is always related to predicting words given
    a certain context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors proposed two architectures with similar tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The continuous bag-of-words (CBOW) model**: This is trained to predict a
    word using its surrounding context (words coming before and after the target word).
    The order of context words does not matter since their embeddings are summed in
    the model. The authors claim to obtain better results using four words before
    and after the one that is predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The continuous skip-gram model**: Here, we feed a single word to the model
    and try to predict the words around it. Increasing the range of context words
    leads to better embeddings but also increases the training time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, here are the inputs and outputs of both models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – CBOW and skip-gram architectures](img/B19153_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – CBOW and skip-gram architectures
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the CBOW model is considered faster to train, but the skip-gram
    model is more accurate thanks to its ability to learn infrequent words. This topic
    is still debated in the NLP community: a different implementation could fix issues
    related to CBOW in some contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating skip-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For now, we will focus on the skip-gram model since it is the architecture
    used by DeepWalk. Skip-grams are implemented as pairs of words with the following
    structure: ![](img/Formula_B19153_03_007.png), where ![](img/Formula_B19153_03_008.png)
    is the input and ![](img/Formula_B19153_03_009.png) is the word to predict. The
    number of skip grams for the same target word depends on a parameter called **context
    size**, as shown in *Figure 3**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Text to skip-grams](img/B19153_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Text to skip-grams
  prefs: []
  type: TYPE_NORMAL
- en: The same idea can be applied to a corpus of text instead of a single sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we store all the context words for the same target word in a list
    to save memory. Let’s see how it’s done with an example on an entire paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we create skip-grams for an entire paragraph stored
    in the `text` variable. We set the `CONTEXT_SIZE` variable to `2`, which means
    we will look at the two words before and after our target word:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to set the `CONTEXT_SIZE` variable to `2` and bring in the text
    we want to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create the skip-grams thanks to a simple `for` loop to consider every
    word in `text`. A list comprehension generates the context words, stored in the
    `skipgrams` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, use the `print()` function to see the skip-grams we generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These two target words, with their corresponding context, work to show what
    the inputs to Word2Vec look like.
  prefs: []
  type: TYPE_NORMAL
- en: The skip-gram model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of Word2Vec is to produce high-quality word embeddings. To learn these
    embeddings, the training task of the skip-gram model consists of predicting the
    correct context words given a target word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we have a sequence of ![](img/Formula_B19153_03_010.png) words
    ![](img/Formula_B19153_03_011.png). The probability of seeing the word ![](img/Formula_B19153_03_012.png)
    given the word ![](img/Formula_B19153_03_0121.png) is written ![](img/Formula_B19153_03_014.png).
    Our goal is to maximize the sum of every probability of seeing a context word
    given a target word in an entire text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/Formula_B19153_03_016.png) is the size of the context vector.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use a log probability in the previous equation? Transforming probabilities
    into log probabilities is a common technique in ML (and computer science in general)
    for two main reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Products become additions (and divisions become subtractions). Multiplications
    are more computationally expensive than additions, so it’s faster to compute the
    log probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The way computers store very small numbers (such as 3.14e-128) is not perfectly
    accurate, unlike the log of the same numbers (-127.5 in this case). These small
    errors can add up and bias the final results when events are extremely unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: On the whole, this simple transformation allows us to gain speed and accuracy
    without changing our initial objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic skip-gram model uses the softmax function to calculate the probability
    of a context word embedding ![](img/Formula_B19153_03_018.png) given a target
    word embedding ![](img/Formula_B19153_03_019.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/Formula_B19153_03_021.png) is the vocabulary of size ![](img/Formula_B19153_03_022.png).
    This vocabulary corresponds to the list of unique words the model tries to predict.
    We can obtain this list using the `set` data structure to remove duplicate words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the size of our vocabulary, there is one more parameter we
    need to define: ![](img/Formula_B19153_03_023.png), the dimensionality of the
    word vectors. Typically, this value is set between 100 and 1,000\. In this example,
    we will set it to 10 because of the limited size of our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The skip-gram model is composed of only two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A **projection layer** with a weight matrix ![](img/Formula_B19153_03_024.png),
    which takes a one-hot encoded-word vector as an input and returns the corresponding
    ![](img/Formula_B19153_03_025.png)-dim word embedding. It acts as a simple lookup
    table that stores embeddings of a predefined dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **fully connected layer** with a weight matrix ![](img/Formula_B19153_03_026.png),
    which takes a word embedding as input and outputs ![](img/Formula_B19153_03_027.png)-dim
    logits. A softmax function is applied to these predictions to transform logits
    into probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no activation function: Word2Vec is a linear classifier that models
    a linear relationship between words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call ![](img/Formula_B19153_03_028.png) the one-hot encoded-word vector
    the *input*. The corresponding word embedding can be calculated as a simple projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the skip-gram model, we can rewrite the previous probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The skip-gram model outputs a ![](img/Formula_B19153_03_031.png)-dim vector,
    which is the conditional probability of every word in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_03_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: During training, these probabilities are compared to the correct one-hot encoded-target
    word vectors. The difference between these values (calculated by a loss function
    such as the cross-entropy loss) is backpropagated through the network to update
    the weights and obtain better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire Word2Vec architecture is summarized in the following diagram, with
    both matrices and the final softmax layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The Word2Vec architecture](img/B19153_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The Word2Vec architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this model using the `gensim` library, which is also used
    in the official implementation of DeepWalk. We can then build the vocabulary and
    train our model based on the previous text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by installing `gensim` and importing the `Word2Vec` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize a skip-gram model with a `Word2Vec` object and an `sg=1` parameter
    (skip-gram = 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It’s a good idea to check the shape of our first weight matrix. It should correspond
    to the vocabulary size and the word embeddings’ dimensionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we train the model for `10` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can print a word embedding to see what the result of this training
    looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While this approach works well with small vocabularies, the computational cost
    of applying a full softmax function to millions of words (the vocabulary size
    ) is too costly in most cases. This has been a limiting factor in developing accurate
    language models for a long time. Fortunately for us, other approaches have been
    designed to solve this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec (and DeepWalk) implements one of these techniques, called H-Softmax.
    Instead of a flat softmax that directly calculates the probability of every word,
    this technique uses a binary tree structure where leaves are words. Even more
    interestingly, a Huffman tree can be used, where infrequent words are stored at
    deeper levels than common words. In most cases, this dramatically speeds up the
    word prediction by a factor of at least 50.
  prefs: []
  type: TYPE_NORMAL
- en: H-Softmax can be activated in `gensim` using `hs=1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the most difficult part of the DeepWalk architecture. But before we
    can implement it, we need one more component: how to create our training data.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepWalk and random walks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proposed in 2014 by Perozzi et al., DeepWalk quickly became extremely popular
    among graph researchers. Inspired by recent advances in NLP, it consistently outperformed
    other methods on several datasets. While more performant architectures have been
    proposed since then, DeepWalk is a simple and reliable baseline that can be quickly
    implemented to solve a lot of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of DeepWalk is to produce high-quality feature representations of
    nodes in an unsupervised way. This architecture is heavily inspired by Word2Vec
    in NLP. However, instead of words, our dataset is composed of nodes. This is why
    we use random walks to generate meaningful sequences of nodes that act like sentences.
    The following diagram illustrates the connection between sentences and graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Sentences can be represented as graphs](img/B19153_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Sentences can be represented as graphs
  prefs: []
  type: TYPE_NORMAL
- en: Random walks are sequences of nodes produced by randomly choosing a neighboring
    node at every step. Thus, nodes can appear several times in the same sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Why are random walks important? Even if nodes are randomly selected, the fact
    that they often appear together in a sequence means that they are close to each
    other. Under the **network homophily** hypothesis, nodes that are close to each
    other are similar. This is particularly the case in social networks, where people
    are connected to friends and family.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea is at the core of the DeepWalk algorithm: when nodes are close to
    each other, we want to obtain high similarity scores. On the contrary, we want
    low scores when they are farther apart.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a random walk function using a `networkx` graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries and initialize the random number generator
    for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We generate a random graph thanks to the `erdos_renyi_graph` function with
    a fixed number of nodes (`10`) and a predefined probability of creating an edge
    between two nodes (`0.3`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We plot this random graph to see what it looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Random graph](img/B19153_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Random graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement random walks with a simple function. This function takes two
    parameters: the starting node (`start`) and the length of the walk (`length`).
    At every step, we randomly select a neighboring node (using `np.random.choice`)
    until the walk is complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we print the result of this function with the starting node as `0` and
    a length of `10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that certain nodes, such as `0` and `9`, are often found together.
    Considering that it is a homophilic graph, it means that they are similar. It
    is precisely the type of relationship we’re trying to capture with DeepWalk.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have implemented Word2Vec and random walks separately, let’s combine
    them to create DeepWalk.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DeepWalk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a good understanding of every component in this architecture,
    let’s use it to solve an ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use is Zachary’s Karate Club. It simply represents the relationships
    within a karate club studied by Wayne W. Zachary in the 1970s. It is a kind of
    social network where every node is a member, and members who interact outside
    the club are connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the club is divided into two groups: we would like to assign
    the right group to every member (node classification) just by looking at their
    connections:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the dataset using `nx.karate_club_graph()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to convert string class labels into numerical values (Mr. Hi
    = `0`, `Officer` = `1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot this graph using our new labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.6 – Zachary’s Karate Club](img/B19153_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Zachary’s Karate Club
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to generate our dataset, the random walks. We want to be as
    exhaustive as possible, which is why we will create `80` random walks of a length
    of `10` for every node in the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print a walk to verify that it is correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the first walk that was generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final step consists of implementing Word2Vec. Here, we use the skip-gram
    model previously seen with H-Softmax. You can play with the other parameters to
    improve the quality of the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model is then simply trained on the random walks we generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that our model is trained, let’s see its different applications. The first
    one allows us to find the most similar nodes to a given one (in terms of cosine
    similarity):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output for `Nodes that are the most similar to`
    `node 0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important application is calculating the similarity score between two
    nodes. It can be performed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This code directly gives us the cosine similarity between two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the resulting embeddings using **t-distributed stochastic neighbor
    embedding** (**t-SNE**) to visualize these high-dimensional vectors in 2D:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `TSNE` class from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create two arrays: one to store the word embeddings and the other one to
    store the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we train the t-SNE model with two dimensions (`n_components=2`) on the
    embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s plot the 2D vectors produced by the trained t-SNE model with
    the corresponding labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.7 – A t-SNE plot of the nodes](img/B19153_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – A t-SNE plot of the nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'This plot is quite encouraging since we can see a clear line that separates
    the two classes. It should be possible for a simple ML algorithm to classify these
    nodes with enough examples (training data). Let’s implement a classifier and train
    it on our node embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import a Random Forest model from `sklearn`, which is a popular choice when
    it comes to classification. The accuracy score is the metric we’ll use to evaluate
    this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to split the embeddings into two groups: training and test data. A
    simple way of doing it is to create masks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we train the Random Forest classifier on the training data with the appropriate
    labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we evaluate the trained model on the test data based on its accuracy
    score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the final result of our classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our model obtains an accuracy score of 95.45%, which is pretty good considering
    the unfavorable train/test split we gave it. There is still room for improvement,
    but this example showed two useful applications of DeepWalk:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Discovering similarities between nodes* using embeddings and cosine similarity
    (unsupervised learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using these embeddings as a dataset* for a supervised task such as node classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we are going to see in the following chapters, the ability to learn node
    representations offers a lot of flexibility to design deeper and more complex
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about DeepWalk architecture and its major components.
    Then, we transformed graph data into sequences using random walks to apply the
    powerful Word2Vec algorithm. The resulting embeddings can be used to find similarities
    between nodes or as input to other algorithms. In particular, we solved a node
    classification problem using a supervised approach.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B19153_04.xhtml#_idTextAnchor054), *Improving Embeddings with
    Biased Random Walks in Node2Vec*, we will introduce a second algorithm based on
    Word2Vec. The difference with DeepWalk is that the random walks can be biased
    towards more or less exploration, which directly impacts the embeddings that are
    produced. We will implement this algorithm on a new example and compare its representations
    with those obtained using DeepWalk.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] B. Perozzi, R. Al-Rfou, and S. Skiena, *DeepWalk*, Aug. 2014\. DOI: 10.1145/2623330.2623732\.
    Available at [https://arxiv.org/abs/1403.6652](B19153_03.xhtml#_idTextAnchor052).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Fundamentals'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this second part of the book, we will delve into the process of constructing
    node representations using graph learning. We will start by exploring traditional
    graph learning techniques, drawing on the advancements made in natural language
    processing. Our aim is to understand how these techniques can be applied to graphs
    and how they can be used to build node representations.
  prefs: []
  type: TYPE_NORMAL
- en: We will then move on to incorporating node features into our models and explore
    how they can be used to build even more accurate representations. Finally, we
    will introduce two of the most fundamental GNN architectures, the **Graph Convolutional
    Network** (**GCN**) and the **Graph Attention Network** (**GAT**). These two architectures
    are the building blocks of many state-of-the-art graph learning methods and will
    provide a solid foundation for the next part.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this part, you will have a deeper understanding of how traditional
    graph learning techniques, such as random walks, can be used to create node representations
    and develop graph applications. Additionally, you will learn how to build even
    more powerful representations using GNNs. You will be introduced to two key GNN
    architectures and learn how they can be used to tackle various graph-based tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B19153_03.xhtml#_idTextAnchor041)*, Creating Node Representations
    with DeepWalk*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19153_04.xhtml#_idTextAnchor054)*, Improving Embeddings with
    Biased Random Walks in Node2Vec*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19153_05.xhtml#_idTextAnchor064)*, Including Node Features with
    Vanilla Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19153_06.xhtml#_idTextAnchor074)*, Introducing Graph Convolutional
    Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19153_07.xhtml#_idTextAnchor082)*, Graph Attention Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
