- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Anomalies Using Heterogeneous GNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, anomaly detection is a popular task that aims to identify
    patterns or observations in data that deviate from the expected behavior. This
    is a fundamental problem that arises in many real-world applications, such as
    detecting fraud in financial transactions, identifying defective products in a
    manufacturing process, and detecting cyber attacks in a computer network.
  prefs: []
  type: TYPE_NORMAL
- en: GNNs can be trained to learn the normal behavior of a network and then identify
    nodes or patterns that deviate from that behavior. Indeed, their ability to understand
    complex relationships makes them particularly appropriate to detect weak signals.
    Additionally, GNNs can be scaled to large datasets, making them an efficient tool
    for processing large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build a GNN application for anomaly detection in computer
    networks. First, we will introduce the `CIDDS-001` dataset, which contains attacks
    and benign traffic in a computer network. Next, we will process the dataset, preparing
    it for input into GNNs. We will then move on to implementing a heterogenous GNN
    to handle different types of nodes and edges. Finally, we will train the network
    using the processed dataset and evaluate the results to see how well it detects
    anomalies in the network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to implement a GNN for intrusion
    detection. In addition, you will know how to build relevant features to detect
    attacks and process them to feed them to a GNN. Finally, you will learn how to
    implement and evaluate a heterogenous GNN to detect rare attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the CIDDS-001 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the CIDDS-001 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a heterogeneous GNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter16](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter16).
  prefs: []
  type: TYPE_NORMAL
- en: The installation steps required to run the code on your local machine can be
    found in the *Preface* of this book. This chapter requires a large amount of GPU.
    You can lower it by decreasing the size of the training set in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the CIDDS-001 dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will explore the dataset and get more insights about feature importance
    and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: The `CIDDS-001` dataset [1] is designed to train and evaluate anomaly-based
    network intrusion detection systems. It provides realistic traffic that includes
    up-to-date attacks to assess these systems. It was created by collecting and labeling
    8,451,520 traffic flows in a virtual environment using OpenStack. Precisely, each
    row corresponds to a NetFlow connection, describing **Internet Protocol** (**IP**)
    traffic statistics, such as the number of bytes exchanged.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure provides an overview of the simulated network environment
    in `CIDDS-001`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – Overview of the virtual network simulated by CIDDS-001](img/B19153_16_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 – Overview of the virtual network simulated by CIDDS-001
  prefs: []
  type: TYPE_NORMAL
- en: 'We see four different subnets (developer, office, management, and server) with
    their respective IP address ranges. All these subnets are linked to a single server
    connected to the internet through a firewall. An external server is also present
    and provides two services: a file synchronization service and a web server. Finally,
    attackers are represented outside of the local network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connections in `CIDDS-001` were collected from the local and external servers.
    The goal of this dataset is to correctly classify these connections into five
    categories: benign (no attack), brute-force, denial of service, ping scan, and
    port scan.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s download the `CIDDS-001` dataset and explore its input features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We download `CIDDS-001`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the dataset using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the data corresponding to the first five connections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are a few interesting features we can use for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: The date first seen is a timestamp we can process to extract information about
    the day of the week and the time of day. In general, network traffic is seasonal,
    and connections that occur at night or on unusual days are suspicious.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP addresses (such as `192.168.100.5`) are notoriously difficult to process
    because they are not numerical values and follow a complex set of rules. We could
    bin them into a few categories since we know how our local network is set up.
    Another popular and more generalizable solution is to convert them into a binary
    representation (“192” becomes “11000000”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duration, the number of packets, and the number of bytes are features that usually
    display heavy-tailed distributions. Therefore, they will require special processing
    if that is the case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s check this last point and look closely at the distribution of attacks
    in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by removing features we will not consider in this project: ports,
    the number of flows, type of service, class, attack ID, and attack description:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We rename the benign class and convert the “date first seen” feature into the
    timestamp data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We count the labels and make a pie chart with the three most represented classes
    (the two others are under 0.1%):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtain the following plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Proportion of each class in the CIDDS-001 dataset](img/B19153_16_002..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 – Proportion of each class in the CIDDS-001 dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, benign traffic represents the immense majority of this dataset.
    On the contrary, brute-force attacks and ping scans have very few samples in comparison.
    This imbalanced learning setting could negatively impact the model’s performance
    when dealing with rare classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can display the duration distribution, number of packets, and number
    of bytes. This allows us to see whether they really need a specific rescaling
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It outputs the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3 – Distributions of duration, the number of packets, and number
    of bytes](img/B19153_16_003..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – Distributions of duration, the number of packets, and number of
    bytes
  prefs: []
  type: TYPE_NORMAL
- en: We can see that most values are close to zero, but there is also a long tail
    of rare values stretching along the *x* axes. We will use a power transform to
    make these features more Gaussian-like, which should help the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the main characteristics of the `CIDDS-001` dataset,
    we can move on to the preprocessing stage.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the CIDDS-001 dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we identified some issues with the dataset we need to address
    to improve the accuracy of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CIDDS-001` dataset includes diverse types of data: we have numerical values
    such as duration, categorical features such as protocols (TCP, UDP, ICMP, and
    IGMP), and others such as timestamps or IP addresses. In the following exercise,
    we will choose how to represent these data types based on the information from
    the previous section and expert knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can one-hot-encode the day of the week by retrieving this information
    from the timestamp. We will rename the resulting columns to make them more readable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another important type of information we can get using timestamps is the time
    of day. We also normalize it between `0` and `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have not talked about TCP flags yet. Each flag indicates a particular state
    during a TCP connection. For example, `F` or `FIN` signifies that the TCP peer
    has finished sending data. We can extract each flag, and one-hot-encode them as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now process the IP addresses. In this example, we will use binary encoding.
    Instead of taking 32 bits to encode the complete IPv4 address, we will only keep
    the last 16 bits, which are the most significant here. Indeed, the 16 first bits
    either correspond to `192.168` if the host belongs to the internal network or
    another value if it’s external:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We repeat this process for destination IP addresses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There is an issue with the ‘`Bytes`’ feature: millions are represented as `m`
    instead of a numerical value. We can fix it by multiplying the numerical part
    of these non-numerical values by one million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last features we need to encode are the easiest ones: categorical features
    such as protocols and attack types. We use the `get_dummies()` function from `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a train/validation/test split with 80/10/10 ratios:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we need to address the scaling of three features: duration, the number
    of packets, and the number of bytes. We use `PowerTransformer()` from `scikit-learn`
    to modify their distributions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the new distributions to see how they compare:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtain the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – Rescaled distributions of duration, the number of packets,
    and the number of bytes](img/B19153_16_004..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – Rescaled distributions of duration, the number of packets, and
    the number of bytes
  prefs: []
  type: TYPE_NORMAL
- en: These new distributions are not Gaussian, but the values are more spread out,
    which should help the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the dataset we processed is purely tabular. We still need to convert
    it into a graph dataset before we can feed it to a GNN. In our case, there is
    no obvious way of converting our traffic flows into nodes. Ideally, flows between
    the same computers should be connected. This can be achieved using a heterogeneous
    graph with two types of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hosts**, which correspond to computers and use IP addresses as features.
    If we had more information, we could add other computer-related features, such
    as logs or CPU utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flows**, which correspond to connections between two hosts. They consider
    all the other features from the dataset. They also have the label we want to predict
    (a benign or malicious flow).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this example, flows are unidirectional, which is why we also define two
    types of edges: host-to-flow (source), and flow-to-host (destination). A single
    graph would require too much memory, so we will divide it into subgraphs and place
    them into data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the batch size and the features we want to consider for host and
    flow nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the function that will create our data loaders. It takes two parameters:
    the tabular DataFrame we created, and the subgraph size (`1024` nodes in this
    example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize a list called `data` to store our subgraphs, and we count the
    number of subgraphs we will create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each subgraph, we retrieve the corresponding samples in the DataFrame,
    the list of source IP addresses, and the list of destination IP addresses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a dictionary that maps the IP addresses to a node index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This dictionary will help us to create the edge index from the host to the flow
    and vice versa. We use a function called `get_connections()`, which we will create
    after this one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use all the data we have collected so far to create a heterogeneous graph
    for each subgraph and append it to the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we return the data loader with the appropriate batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There is one function we still need to implement – `get_connections()` – which
    calculates two edge indices from the list of source and destination IP addresses
    and their corresponding map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get indexes from the IP addresses (both source and destination) and stack
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The connections are unique, so we can easily index them with the appropriate
    range of numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we return the two following edge indexes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have everything we need, we can call the first function to create
    the training, validation, and test data loaders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have three data loaders corresponding to our training, validation, and
    test sets. The next step consists of implementing the GNN model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing a heterogeneous GNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement a heterogeneous GNN using a `GraphSAGE` operator.
    This architecture will allow us to consider both node types (hosts and flows)
    to build better embeddings. This is done by duplicating and sharing messages across
    different layers, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – Architecture of the heterogeneous GNN](img/Image98417.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 – Architecture of the heterogeneous GNN
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement three layers of `SAGEConv` with `LeakyRELU` for each node
    type. Finally, a linear layer will output a five-dimensional vector, where each
    dimension corresponds to a class. Furthermore, we will train this model in a supervised
    way using the cross-entropy loss and the `Adam` optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the relevant neural network layers from PyTorch Geometric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the heterogeneous GNN with three parameters: the number of hidden
    dimensions, the number of output dimensions, and the number of layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a heterogenous version of the `GraphSAGE` operator for each layer
    and edge type. Here, we could apply a different GNN layer to each edge type, such
    as `GCNConv` or `GATConv`. The `HeteroConv()` wrapper manages the messages between
    layers, as shown in *Figure 16**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a linear layer that will output the final classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the `forward()` method, which computes embeddings for host and flow
    nodes (stored in the `x_dict` dictionary). The flow embeddings are then used to
    predict a class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We instantiate the heterogeneous GNN with 64 hidden dimensions, 5 outputs (our
    5 classes), and 3 layers. If available, we place it on a GPU and create an `Adam`
    optimizer with a learning rate of `0.001`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the `test()` function and create arrays to store predictions and
    `true` labels. We also want to count the number of subgraphs and the total loss,
    so we create the corresponding variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the model’s prediction for each batch and compute the cross-entropy
    loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We append the predicted class to the list of predictions and do the same with
    the `true` labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We count the number of subgraphs and the total loss as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the batch loop is over, we compute the `F1` score (macro) using the
    prediction and `true` label lists. The macro-averaged `F1` score is a good metric
    in this imbalanced learning setting because it treats all classes equally regardless
    of the number of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We return the final loss, the macro-averaged `F1` score, the list of predictions,
    and the list of `true` labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the training loop to train the model for `101` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train the heterogenous GNN on each batch using the cross-entropy loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Every 10 epochs, we evaluate the model on the validation set and display relevant
    metrics (the training loss, validation loss, and validation macro-averaged `F1`
    score):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtain the following output during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the model on the test set. We also print `scikit-learn`’s
    classification report, which includes the macro-averaged F1 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We obtained a macro-averaged `F1` score of `0.9831`. This excellent result shows
    that our model has learned to predict each class reliably.
  prefs: []
  type: TYPE_NORMAL
- en: The approach we adopted would be even more relevant if we could access more
    host-related features, but it shows how you can expand it to fit your needs. The
    other main advantage of GNNs is their ability to process large amounts of data.
    This approach makes even more sense when dealing with millions of flows. To finish
    this project, let’s plot the model’s errors to see how we could improve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a dataframe to store the predictions (`y_pred`) and the true labels
    (`y_true`). We use this new dataframe to plot the proportion of misclassified
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.6 – Proportion of each misclassified class](img/B19153_16_006..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.6 – Proportion of each misclassified class
  prefs: []
  type: TYPE_NORMAL
- en: If we compare this pie chart to the original proportions in the dataset, we
    see that the model performs better for the majority classes. This is not surprising
    since minority classes are harder to learn (fewer samples), and not detecting
    them is less penalizing (with 700,000 benign flows versus 336 ping scans). Port
    and ping scan detection could be improved with techniques such as oversampling
    and introducing class weights during training.
  prefs: []
  type: TYPE_NORMAL
- en: We can gather even more information by looking at the confusion matrix (the
    code can be found on GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.7 – Confusion matrix for multi-class flow classification](img/B19153_16_007..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.7 – Confusion matrix for multi-class flow classification
  prefs: []
  type: TYPE_NORMAL
- en: This confusion matrix displays interesting results, such as a bias toward the
    benign class or errors between ping and port scans. These errors can be attributed
    to the similarity between these attacks. Engineering additional features could
    help the model distinguish these classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the use of GNNs for detecting anomalies in a new
    dataset, the `CIDDS-001` dataset. First, we preprocessed the dataset and converted
    it into a graph representation, allowing us to capture the complex relationships
    between the different components of the network. We then implemented a heterogeneous
    GNN with `GraphSAGE` operators. It captured the heterogeneity of the graph and
    allowed us to classify the flows as benign or malicious.
  prefs: []
  type: TYPE_NORMAL
- en: The application of GNNs in network security has shown promising results and
    opened up new avenues for research. As technology continues to advance and the
    amount of network data increases, GNNs will become an increasingly important tool
    for detecting and preventing security breaches.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 17*](B19153_17.xhtml#_idTextAnchor195), *Recommending Books Using
    LightGCN*, we will explore the most popular application of GNNs with recommender
    systems. We will implement a lightweight GNN on a large dataset and produce book
    recommendations for given users.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] M. Ring, S. Wunderlich, D. Grüdl, D. Landes, and A. Hotho, *Flow-based
    benchmark data sets for intrusion detection*, in *Proceedings of the 16th European
    Conference on Cyber Warfare and Security* (ECCWS), ACPI, 2017, pp. 361–369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
