- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Graph Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Graph Convolutional Network** (**GCN**) architecture is the blueprint
    of what a GNN looks like. Introduced by Kipf and Welling in 2017 [1], it is based
    on the idea of creating an efficient variant of **Convolutional Neural Networks**
    (**CNNs**) applied to graphs. More accurately, it is an approximation of a graph
    convolution operation in graph signal processing. Thanks to its versatility and
    ease of use, the GCN has become the most popular GNN in scientific literature.
    More generally, it is the architecture of choice to create a solid baseline when
    dealing with graph data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll talk about the limitations of our previous vanilla GNN
    layer. This will help us to understand the motivation behind GCNs. We’ll detail
    how the GCN layer works and why it performs better than our solution. We’ll test
    this statement by implementing a GCN on the `Cora` and `Facebook Page-Page` datasets
    using PyTorch Geometric. This should improve our results even further.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last section is dedicated to a new task: **node regression**. This is not
    a very common task when it comes to GNNs, but it is particularly useful when you’re
    working with tabular data. If you have the opportunity to transform your tabular
    dataset into a graph, this will enable you to perform regression in addition to
    classification.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to implement a GCN in PyTorch Geometric
    for classification or regression tasks. Thanks to linear algebra, you’ll understand
    why this model performs better than our vanilla GNN. Finally, you’ll know how
    to plot node degrees and the density distribution of a target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing the graph convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing graph convolutional and graph linear layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting web traffic with node regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* section of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the graph convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let’s talk about a problem we did not anticipate in the previous chapter.
    Unlike tabular or image data, nodes do not always have the same number of neighbors.
    For instance, in *Figure 6**.1*, node ![](img/Formula_B19153_06_001.png) has 3
    neighbors while node ![](img/Formula_B19153_06_002.png) only has 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Simple graph where nodes have different numbers of neighbors](img/B19153_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Simple graph where nodes have different numbers of neighbors
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we look at our GNN layer, we don’t take into account this difference
    in the number of neighbors. Our layer consists of a simple sum without any normalization
    coefficient. Here is how we calculated the embedding of a node, ![](img/Formula_B19153_06_003.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Imagine that node ![](img/Formula_B19153_06_005.png) has 1,000 neighbors and
    node ![](img/Formula_B19153_06_006.png) only has 1: the embedding ![](img/Formula_B19153_06_007.png)
    will have much larger values than ![](img/Formula_B19153_06_008.png). This is
    an issue because we want to compare these embeddings. How are we supposed to make
    meaningful comparisons when their values are so vastly different?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a simple solution: dividing the embedding by the number
    of neighbors. Let’s write ![](img/Formula_B19153_06_009.png), the degree of node
    ![](img/Formula_B19153_06_010.png). Here is the new formula for the GNN layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But how do we translate it into a matrix multiplication? As a reminder, this
    was what we obtained for our vanilla GNN layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B19153_06_013.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing that is missing from this formula is a matrix to give us the
    normalization coefficient, ![](img/Formula_B19153_06_020.png). This is something
    that can be obtained thanks to the degree matrix ![](img/Formula_B19153_06_015.png),
    which counts the number of neighbors for each node. Here is the degree matrix
    for the graph shown in *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the same matrix in NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By definition, ![](img/Formula_B19153_06_017.png) gives us the degree of each
    node, ![](img/Formula_B19153_06_018.png). Therefore, the inverse of this matrix
    ![](img/Formula_B19153_06_019.png) directly gives us the normalization coefficients,
    ![](img/Formula_B19153_06_0201.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The inverse of a matrix can directly be calculated using the `numpy.linalg.inv()`function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is exactly what we were looking for. To be even more accurate, we added
    self-loops to the graph, represented by ![](img/Formula_B19153_06_0131.png). Likewise,
    we should add self-loops to the degree matrix, ![](img/Formula_B19153_06_023.png).
    The final matrix we are actually interested in is ![](img/Formula_B19153_06_024.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'NumPy has a specific function, `numpy.identity(n)`, to quickly create an identity
    matrix ![](img/Formula_B19153_06_026.png) of ![](img/Formula_B19153_06_027.png)
    dimensions. In this example, we have four dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our matrix of normalization coefficients, where should we
    put it in the formula? There are two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_028.png) will normalize every row of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_029.png)will normalize every column of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can verify this experimentally by calculating ![](img/Formula_B19153_06_030.png)
    and ![](img/Formula_B19153_06_031.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_032.jpg)![](img/Formula_B19153_06_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Indeed, in the first case, the sum of every row is equal to 1\. In the second
    case, the sum of every column is equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix multiplications can be performed using the `numpy.matmul()` function.
    Even more conveniently, Python has had its own matrix multiplication operator,
    `@`, since version 3.5\. Let’s define the adjacency matrix ![](img/Formula_B19153_06_034.png)
    and use this operator to compute our matrix multiplications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We obtain the same results as with manual matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: So, which option should we use? Naturally, the first option looks more appealing
    because it nicely normalizes neighboring node features.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Kipf and Welling [1] noticed that features from nodes with a lot of
    neighbors spread very easily, unlike features from more isolated nodes. In the
    original GCN paper, the authors proposed a hybrid normalization to counterbalance
    this effect. In practice, they assign higher weights to nodes with few neighbors
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In terms of individual embeddings, this operation can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Those are the original formulas to implement a graph convolutional layer. As
    with our vanilla GNN layer, we can stack these layers to create a GCN. Let’s implement
    a GCN and verify that it performs better than our previous approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing graph convolutional and graph linear layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, our vanilla GNN outperformed the Node2Vec model, but
    how does it compare to a GCN? In this section, we will compare their performance
    on the Cora and Facebook Page-Page datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the vanilla GNN, the main feature of the GCN is that it considers
    node degrees to weigh its features. Before the real implementation, let’s analyze
    the node degrees in both datasets. This information is relevant since it is directly
    linked to the performance of the GCN.
  prefs: []
  type: TYPE_NORMAL
- en: 'From what we know about this architecture, we expect it to perform better when
    node degrees vary greatly. If every node has the same number of neighbors, these
    architectures are equivalent: (![](img/Formula_B19153_06_037.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `Planetoid` class from PyTorch Geometric. To visualize the node
    degrees, we also import `matplotlib` and two additional classes: `degree` to get
    the number of neighbors of each node and `Counter` to count the number of nodes
    for each degree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Cora dataset is imported and its graph is stored in `data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compute the number of neighbors of each node in the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To produce a more natural visualization, we count the number of nodes for each
    degree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot this result using a bar plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That gives us the following plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset](img/B19153_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Number of nodes with specific node degrees in the Cora dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'This distribution looks exponential with a heavy tail: it ranges from 1 neighbor
    (485 nodes) to 168 neighbors (1 node)! This is exactly the kind of dataset where
    we want a normalization process to consider this disbalance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same process is repeated with the Facebook Page-Page dataset with the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page
    dataset](img/B19153_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Number of nodes with specific node degrees in the Facebook Page-Page
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: This distribution of node degrees looks even more skewed, with a number of neighbors
    that ranges from 1 to 709\. For the same reason, the Facebook Page-Page dataset
    is also a good case in which to apply a GCN.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could build our own graph layer but, conveniently enough, PyTorch Geometric
    already has a predefined GCN layer. Let’s implement it on the Cora dataset first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import PyTorch and the GCN layer from PyTorch Geometric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a function to calculate the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a GCN class with a `__init_()` function that takes three parameters
    as input: the number of input dimensions, `dim_in`, the number of hidden dimensions,
    `dim_h`, and the number of output dimensions, `dim_out`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `forward` method is identical, and has two GCN layers. A log `softmax`
    function is applied to the result for classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `fit()` method is the same, with the exact same parameters for the `Adam`
    optimizer (a learning rate of 0.1 and L2 regularization of 0.0005):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We implement the same `test()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s instantiate and train our model for `100` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s evaluate it on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we repeat this experiment 100 times, we obtain an average accuracy score
    of 80.17% (± 0.61%), which is significantly higher than the 74.98% (± 1.50%) obtained
    by our vanilla GNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact same model is then applied to the Facebook Page-Page dataset, where
    it obtains an average accuracy of 91.54% (± 0.28%). Once again, it is significantly
    higher than the result obtained by the vanilla GNN, with only 84.85% (± 1.68%).
    The following table summarizes the accuracy scores with standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MLP** | **GNN** | **GCN** |'
  prefs: []
  type: TYPE_TB
- en: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) | 80.17%(±0.61%) |'
  prefs: []
  type: TYPE_TB
- en: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) | 91.54%(±0.28%) |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.4 – Summary of accuracy scores with standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: We can attribute these high scores to the wide range of node degrees in these
    two datasets. By normalizing features and considering the number of neighbors
    of the central node and its own neighbors, the GCN gains a lot of flexibility
    and can work well with various types of graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, node classification is not the only task that GNNs can perform.
    In the next section, we’ll see a new type of application that is rarely covered
    in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting web traffic with node regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, **regression** refers to the prediction of continuous values.
    It is often contrasted with **classification**, where the goal is to find the
    correct categories (which are not continuous). In graph data, their counterparts
    are node classification and node regression. In this section, we will try to predict
    a continuous value instead of a categorical variable for each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we will use is the Wikipedia Network (GNU General Public License
    v3.0), introduced by Rozemberckzi et al. in 2019 [2]. It is composed of three
    page-page networks: chameleons (2,277 nodes and 31,421 edges), crocodiles (11,631
    nodes and 170,918 edges), and squirrels (5,201 nodes and 198,493 edges). In these
    datasets, nodes represent articles and edges are mutual links between them. Node
    features reflect the presence of particular words in the articles. Finally, the
    goal is to predict the log average monthly traffic of December 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will apply a GCN to predict this traffic on the chameleon
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the Wikipedia Network and download the chameleon dataset. We apply
    the `transform` function, `RandomNodeSplit()`, to randomly create an evaluation
    mask and a test mask:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We print information about this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a problem with our dataset: the output says that we have five classes.
    However, we want to perform node regression, not classification. So what happened?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In fact, these five classes are bins of the continuous values we want to predict.
    Unfortunately, these labels are not the ones we want: we have to change them manually.
    First, let’s download the `wikipedia.zip` file from the following page: [https://snap.stanford.edu/data/wikipedia-article-networks.xhtml](https://snap.stanford.edu/data/wikipedia-article-networks.xhtml).
    After unzipping the file, we import `pandas` and use it to load the targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply a log function to the target values using `np.log10()` because the
    goal is to predict the log average monthly traffic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We redefine `data.y` as a tensor of the continuous values from the previous
    step. Note that these values are not normalized in this example, which is a good
    practice that is usually implemented. We will not perform it here for ease of
    exposition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once again, it is a good idea to visualize the node degrees as we did for the
    two previous datasets. We use the exact same code to produce the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia
    Network](img/B19153_6_0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Number of nodes with specific node degrees in the Wikipedia Network
  prefs: []
  type: TYPE_NORMAL
- en: 'This distribution has a shorter tail than the previous ones but keeps a similar
    shape: most nodes have one or a few neighbors, but some of them act as “hubs”
    and can connect more than 80 nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of node regression, the distribution of node degrees is not the
    only type of distribution we should check: the distribution of our target values
    is also essential. Indeed, non-normal distribution (such as node degrees) tends
    to be harder to predict. We can use the Seaborn library to plot the target values
    and compare them to a normal distribution provided by `scipy.stats.norm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Density plot of target values from the Wikipedia Network](img/B19153_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Density plot of target values from the Wikipedia Network
  prefs: []
  type: TYPE_NORMAL
- en: This distribution is not exactly normal, but it is not exponential like the
    node degrees either. We can expect our model to perform well to predict these
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement it step by step with PyTorch Geometric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the GCN class and the `__init__()` function. This time, we have three
    `GCNConv` layers with a decreasing number of neurons. The idea behind this encoder
    architecture is to force the model to select the most relevant features to predict
    the target values. We also added a linear layer to output a prediction that is
    not limited to a number between 0 or -1 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `forward()` method includes the new `GCNConv` and `nn.Linear` layers. There
    is no need for a log `softmax` function here since we’re not predicting a class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The main change in the `fit()` method is the `F.mse_loss()` function, which
    replaces the cross-entropy loss used in classification tasks. The **Mean Squared
    Error** (**MSE**) will be our main metric. It corresponds to the average of the
    squares of the errors and can be defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, this is how it is implemented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The MSE is also included in the `test()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We instantiate the model with `128` hidden dimensions and only `1` output dimension
    (the target value). It is trained on `200` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We test it to obtain the MSE on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This MSE loss is not the most interpretable metric by itself. We can get more
    meaningful results using the two following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RMSE, which measures the average magnitude of the error:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_06_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Mean Absolute Error** (**MAE**), which gives the mean absolute difference
    between the predicted and real values:![](img/Formula_B19153_06_040.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement them in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly import the MSE and the MAE from the scikit-learn library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We convert the PyTorch tensors for the predictions into the NumPy arrays given
    by the model using `.detach().numpy()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compute the MSE and the MAE with their dedicated function. The RMSE is calculated
    as the square root of MSE using `np.sqrt()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These metrics are useful for comparing different models, but it can be difficult
    to interpret the MSE and the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best tool to visualize the results of our model is a scatter plot, where
    the horizontal axis represents our predictions and the vertical axis represents
    the real values. Seaborn has a dedicated function (`regplot()`) for this type
    of visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values
    (y-axis)](img/B19153_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Ground truth test values (x-axis) versus. predicted test values
    (y-axis)
  prefs: []
  type: TYPE_NORMAL
- en: We don’t have a baseline to work with in this example, but this is a decent
    prediction with few outliers. It would work in a lot of applications, despite
    a minimalist dataset. If we wanted to improve these results, we could tune the
    hyperparameters and do more error analysis to understand where the outliers come
    from.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we improved our vanilla GNN layer to correctly normalize features.
    This enhancement introduced the GCN layer and smart normalization. We compared
    this new architecture to Node2Vec and our vanilla GNN on the Cora and Facebook
    Page-Page datasets. Thanks to this normalization process, the GCN obtained the
    highest accuracy scores by a large margin in both cases. Finally, we applied it
    to node regression with the Wikipedia Network and learned how to handle this new
    task.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B19153_07.xhtml#_idTextAnchor082), *Graph Attention Networks*,
    we will go a step further by discriminating neighboring nodes based on their importance.
    We will see how to automatically weigh node features through a process called
    self-attention. This will improve our performance, as we will see by comparing
    it to the GCN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] T. N. Kipf and M. Welling, *Semi-Supervised Classification with Graph Convolutional
    Networks*. arXiv, 2016\. DOI: 10.48550/ARXIV.1609.02907\. Available: [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Rozemberczki, C. Allen, and R. Sarkar, *Multi-scale Attributed Node
    Embedding*. arXiv, 2019\. DOI: 10.48550/ARXIV.1909.13021\. Available: [https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
