- en: Finding Meaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we mostly used TensorFlow for image processing, and to a lesser extent,
    for text-sequence processing. In this chapter, we will revisit the written word
    to find meaning in text. This is part of an area that is commonly termed **Natural
    Language Processing** (**NLP**). Some of the activities in this area include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis**—This extracts a general sentiment category from text
    without extracting the subject or action of the sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity extraction**—This extracts the subject, for example, person, place,
    and event, from a piece of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keyword extraction**—This extracts key terms from a piece of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word-relation extraction**—This extracts not only entities but also the associated
    action and parts of speech of each'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is just scratching the surface of NLP—there are other techniques, as well
    as a range of sophistication across each technique. Initially, this seems somewhat
    academic, but consider what just these four techniques can enable. Some examples
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading news and understanding the subject of the news (individual, company,
    location, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking the preceding news and understanding the sentiment (happy, sad, angry,
    and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing product reviews and understanding the user's sentiment toward the product
    (pleased, disappointed, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a bot to respond to user chat-box commands given in natural language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much like the previous machine learning efforts we've explored, a decent bit
    of effort goes into setup. In this case, we'll spend some time writing scripts
    to actually grab text from sources of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Additional setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additional setup is required to include libraries required for text processing.
    Take a look at the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First is **Bazel**. On Ubuntu, you will need to follow the official tutorial
    on this link to install Bazel. [https://docs.bazel.build/versions/master/install-ubuntu.html](https://docs.bazel.build/versions/master/install-ubuntu.html).
    On macOS, you can use HomeBrew to `install bazel` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will install `swig`, which will allow us to wrap C/C++ functions to
    allow calls in Python. On Ubuntu, you can install it using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On Mac OS, we will also install it using `brew`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll install the protocol buffer support, which will allow us to store
    and retrieve serialized data in a more efficient manner than with XML. We specifically
    need version `3.3.0` to install it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our text classification will be represented as trees, so we''ll need a library
    to display trees on the command line. We will install it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll need a scientific computing library. If you did image classification
    chapters, you are already familiar with this. But if not, install **NumPy** as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With all this, we'll now install **SyntaxNet**, which does the heavy lifting
    for our NLP. SyntaxNet is an open source framework for TensorFlow ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    that provides base functionality. Google trained a SyntaxNet model with English
    and named it **Parsey McParseface**, which will be included in our installation.
    We'll be able to either train our own, better or more specific, models in English
    or train in other languages altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Training data will pose a challenge, as always, so we'll start with just using
    the pre-trained English model, Parsey McParseface.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s grab the package and configure it, as shown in the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s test the system as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will take a while. Have patience. If you followed all the instructions
    closely, all the tests will pass. There may be some errors that appeared on our
    computer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find that `bazel` can''t download a package, you can try to use the
    following command and run the test command again:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you encounter some failed tests, we suggest that you add the following line
    into your `.bazelrc` in `home` directory in order to receive more error information
    to debug:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you encounter the error `Tensor already registered`, you need to follow
    the solution on the Github issue: [https://github.com/tensorflow/models/issues/2355](https://github.com/tensorflow/models/issues/2355).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s perform a more run-of-the-mill test. Let''s provide an English
    sentence and see how it is parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are feeding in a sentence via the echo statement and piping it into the
    `syntaxnet` demo script that accepts standard input from the console. Note that
    to make the example more interesting, I will use an uncommon name, for example,
    `Faaris`. Running this command will produce a great deal of debugging information,
    shown as follows. I cut out stack traces with ellipses (`...`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The final section, starting with `Input:`, is the most interesting part, and
    the output we will consume when we use this foundation programmatically. Notice
    how the sentence is broken down into parts of speech and entity-action-object
    pairs? Some of the word designations we see are—`nsubj`, `xcomp`, `aux`, `dobj`,
    `det`, and `punct`. Some of these designations are obvious, while others are not.
    If you are into deep dive, we suggest perusing the Stanford dependency hierarchy
    at [https://nlp-ml.io/jg/software/pac/standep.html](https://nlp-ml.io/jg/software/pac/standep.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another sentence before we proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, here, we will find the model performs pretty well in dissecting the phrase.
    Try some of your own.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's actually train a model. Training SyntaxNet is fairly trivial as
    it is a compiled system. So far, we've piped in data via standard input (STDIO),
    but we can also pipe in a corpus of text. Remember the protocol buffer library
    we installed? We will use it now to edit the source file—`syntaxnet/models/parsey_mcparseface/context.pbtxt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we will change the source to other training sources, or our own,
    as shown in the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is how we will train the set; however, it will be pretty challenging to
    do something better than the natively trained model, Parsey McParseface. So let's
    train on an interesting dataset using a new model—a **Convolutional neural network**
    (**CNN**) to process text.
  prefs: []
  type: TYPE_NORMAL
- en: I'm a little biased in favor of my alma mater, so we'll use movie review data
    that Cornell University's department of computer science compiled. The dataset
    is available at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/).'
  prefs: []
  type: TYPE_NORMAL
- en: We'll first download and process the movie reviews dataset, then train on it,
    and finally evaluate based on it.
  prefs: []
  type: TYPE_NORMAL
- en: All our code is available at— [https://github.com/dennybritz/cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)
  prefs: []
  type: TYPE_NORMAL
- en: The code was inspired by Yoon Kim's paper on the subject, CNNs for sentence
    classification, implemented and maintained by Google's Denny Britz. Now, we will
    walk through the code to see how Danny Britz implemented the network
  prefs: []
  type: TYPE_NORMAL
- en: 'We start on figure 1 with the usual helpers. The only new entrant here is the
    data helper that downloads and prepares this particular dataset, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddac5b61-96db-4f10-9eaa-72db394f6813.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We start defining parameters. The training parameters will be very familiar
    by now—these define the batch size that gets processed on each sweep and how many
    epochs or full runs we''ll undertake. We will also define how often we evaluate
    progress (100 steps here) and how often we save checkpoints for the model (to
    allow evaluation and recontinuation). ). Next, we have the code to load and prepare
    the dataset in figure 2, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e737d91-9797-4669-b073-43c45a94b20d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will take a look at the training part of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03caa6cb-ff5f-420b-b0c4-b414d66c788a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 shows us instantiating our CNN—a Natural Language CNN—with some of
    the parameters we defined earlier. We also set up the code to enable the TensorBoard
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4 shows more items we''re capturing for TensorBoard—loss, accuracy for
    the training, and evaluation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32d4bb99-086c-46c6-9ae2-074920af7b60.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, in figure 5, we will define the training and evaluation methods, which
    are very similar to those we used for image processing. We will receive a set
    of training data and labels and house them in a dictionary. Then, we will run
    our TensorFlow session on the dictionary of data, capturing the performance metrics
    returned.
  prefs: []
  type: TYPE_NORMAL
- en: We will set up the methods at the top and then loop through the training data
    in batches, applying the training and evaluation methods to each batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At select intervals, we will also save checkpoints for optional evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0482e7b5-a255-4640-8aaf-252d7682cdfd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can run this and end up with a trained model, after a good hour of training
    on a CPU-only machine. The trained model will be stored as a checkpoint file,
    which can then be fed into the evaluation program shown in figure 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09400684-0fbf-4b9a-8faf-00671388e1a6.png)'
  prefs: []
  type: TYPE_IMG
- en: The evaluation program is just an example of usage, but let's go through it.
    We will start with the typical imports and parameter settings. Here, we will also
    take the checkpoint directory as an input and we will load some test data; however,
    you should use your own data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s examine the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47ccb93f-c1e1-4522-9e0a-65caecb20d66.png)'
  prefs: []
  type: TYPE_IMG
- en: We will start with the checkpoint file by just loading it up and recreating
    a TensorFlow session from it. This allows us to evaluate against the model we
    just trained, and reuse it over and over.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will run the test data in batches. In regular use, we will not use
    a loop or batches, but we have a sizeable set of test data, so we'll do it as
    a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will simply run the session against each set of test data and keep the returned
    predictions (negative versus positive.) The following is some sample positive
    review data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we have negative data. They are all in the `data` folder as `rt-polarity.pos`
    and `rt-polarity.neg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the network architecture we used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ecb8737-7f33-43b1-ab6f-aa7dc5bc8f68.png)'
  prefs: []
  type: TYPE_IMG
- en: It is very similar to the architecture we used for images. In fact, the entire
    effort looks very similar, and it is. The beauty of many of these techniques is
    its generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the output of training first, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s look at the evaluation step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That is pretty good accuracy on the dataset we have. The next step will be to
    apply the trained model to regular use. Some interesting experiments may be to
    obtain movie review data from another source, perhaps IMDB or Amazon. As the data
    will not necessarily be tagged, we can use % positive as a metric of general agreement
    across sites.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use the model in the field. Consider you were a product manufacturer.
    You could track, in real time, all reviews from myriad sources and filter for
    just highly negative reviews. Then, your field-representatives could try and address
    such issues. The possibilities are endless, so we propose an interesting project
    you could undertake, combining the two items we've learned.
  prefs: []
  type: TYPE_NORMAL
- en: Write a twitter stream reader that takes each tweet and extracts the subject
    of the tweet. For a specific set of subjects, say companies, evaluate whether
    the tweet is positive or negative. Create running metrics on percent positive
    and negative, which evaluates the subject on different time scales.
  prefs: []
  type: TYPE_NORMAL
- en: Skills learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have learned the following skills in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up more advanced TensorFlow libraries, including those requiring Bazel-driven
    compilation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying RNNs and CNNs to text instead of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating text against saved models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using prebuilt libraries to extract sentence structure details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying text into buckets based on positive and negative sentiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Excellent! We just took our knowledge of neural networks and applied it to text
    to understand language. This is quite a feat because full automation leads to
    vast scale. Even if particular evaluations are not correct, statistically, we'll
    have a powerful tool in our hands, again, built using the same building blocks.
  prefs: []
  type: TYPE_NORMAL
