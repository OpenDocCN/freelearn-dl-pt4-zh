<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Word Representation Using word2vec</h1>
                </header>
            
            <article>
                
<p>Our <em>Python Deep Learning Projects</em> team is doing good work, and our (hypothetical) business use case has expanded! In the last project, we were asked to accurately classify handwritten digits to generate a phone number so that an <em>available table notification</em> text could be sent out to patrons of a restaurant chain. What we learned after the project was that the text that the restaurant sent out had a message that was friendly and well received. The restaurant was actually getting texts back!</p>
<p class="mce-root"><span>The notification text was: <em>We're excited that you're here and your table is ready! See the greeter, and we'll seat you now.</em></span></p>
<p>Response texts were varied and usually short, but the responses were noticed by the greeter and the restaurant management, who started thinking that maybe they could use this simple system to get feedback on the dining experience. This feedback would provide useful business intelligence on how the food tasted, how the service was delivered, and the overall quality of the experience.</p>
<div class="packt_tip"><strong>Define success</strong>: The goal of this project is to build a computational linguistic model, using word2vec, that can take a text response (as identified in our hypothetical use case for this chapter) and output a sentiment classification that is <span>meaningful</span><span>.</span></div>
<p class="p1"><span class="s1">In this chapter, we introduce the foundational knowledge of <strong>deep learning</strong> (<strong>DL</strong>) for computational linguistics.</span></p>
<p class="p1"><span class="s1">We present the role of the dense vector representation of words in various computational linguistic tasks and how to construct them from an unlabeled monolingual corpus.</span></p>
<p class="p1"><span class="s1">We'll then present the role of language models in various computational linguistic tasks, such as text classification, and how to construct them from an unlabeled monolingual corpus using <strong>convolutional neural networks</strong> (<strong>CNNs</strong>). <span>We'll also explore CNN architecture for language modeling.</span></span></p>
<div class="packt_tip">While working with machine learning/DL, the structure of data is very important. <span>Unfortunately, raw data is often very unclean and </span>unstructured, especially in the practice of <strong>natural language processing</strong> (<strong>NLP</strong>). When working with textual<strong> </strong>data, we cannot feed strings as input in <span>most DL algorithms;</span><span> hence, </span><strong>word embedding</strong><span> methods come to the rescue. Word embedding is used to transform the textual data into dense vector (tensors) form, which we can feed to the learning algorithm.</span></div>
<p><span>T</span>here are several ways in which we can perform word embeddings, such as one-hot encoding, GloVe, word2vec, and many more, and each of them have their own pros and cons. Our current favorite is word2vec because it has been proven to be the most efficient approach when it comes to learning high quality features.</p>
<div class="p1 packt_infobox">If you have ever worked <span><span>on a use case where the input data is in text form, then you know that it's a really messy affair because you have to teach a computer about the irregularities of human language.</span> Language<span> has lots of ambiguities, and you have to teach sort of like hierarchical and the sparse nature of grammar. So these are the kinds of problems that word vectors solve by removing the ambiguities and making all different kinds of concepts similar.</span></span></div>
<p>In this chapter, we will learn how to build word2vec models and analyze what characteristics we can learn about the provided corpus. Also, we will learn how to build a language model that utilizes a CNN with trained word vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning word vectors</h1>
                </header>
            
            <article>
                
<p>To implement a fully functional word embedding model, we will perform the following steps:</p>
<ol>
<li>Loading all the dependencies</li>
<li>Preparing the text corpus</li>
</ol>
<ol start="3">
<li>Defining the model</li>
<li>Training the model</li>
<li>Analyzing the model </li>
<li>Plotting the word cluster using the <strong>t-Distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>) algorithm</li>
<li>Plotting the model on TensorBoard</li>
</ol>
<p>Let's make some world-class word embedding models!</p>
<div class="packt_infobox">The code for this section is available at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading all the dependencies</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using the <kbd>gensim</kbd> module (<a href="https://github.com/RaRe-Technologies/gensim">https://github.com/RaRe-Technologies/gensim</a>) to train our <kbd>word2vec</kbd> model. Gensim provides large-scale multi-core processing support to many popular algorithms, including <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>)<span>,</span><span> </span><strong>Hierarchical Dirichlet</strong> <strong>Process</strong> (<strong>HDP</strong>),<span> and </span>word2vec. <span>There are other approaches that we could take, such as the use of TensorFlow (<a href="https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py">https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py</a>) to define our own computation graph and build the model—this is something that we will look into later on.</span></p>
<div class="packt_infobox">Know the code! Python dependencies are quite manageable. You can learn more at <a href="https://packaging.python.org/tutorials/managing-dependencies/">https://packaging.python.org/tutorials/managing-dependencies/</a>. <br/>
<br/>
This tutorial walks you through the use of<span> </span><span class="std std-ref">Pipenv</span><span> </span>to manage dependencies for an application. It will show you how to install and use the necessary tools and make strong recommendations on best practices. Keep in mind that Python is used for a great many different purposes, and precisely how you want to manage your dependencies may change based on how you decide to publish your software. The guidance presented here is most directly applicable to the development and deployment of network services (including web applications), but is also very well suited to managing development and testing environments for any kind of project.</div>
<p>We will be using the <kbd>seaborn</kbd> package to plot the word clusters, <kbd>sklearn</kbd> to implement the t-SNE algorithm, and <kbd>tensorflow</kbd> for building TensorBoard plots:</p>
<pre>import multiprocessing<br/>import os , json , requests<br/>import re<br/>import nltk<br/>import gensim.models.word2vec as w2v<br/>import sklearn.manifold<br/>import pandas as pd<br/>import seaborn as sns<br/>import tensorflow as tf<br/>from tensorflow.contrib.tensorboard.plugins import projector</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the text corpus</h1>
                </header>
            
            <article>
                
<p>We will use the previously trained <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) tokenizer (<a href="http://www.nltk.org/index.html">http://www.nltk.org/index.html</a>) and stop words for the English language to clean our corpus and extract relevant unique words from the corpus. We will also create a small module to clean the provided collection, with a list of unprocessed sentences, to output the list of words:</p>
<pre>"""**Download NLTK tokenizer models (only the first time)**"""<br/><br/>nltk.download("punkt")<br/>nltk.download("stopwords")<br/><br/>def sentence_to_wordlist(raw):<br/>    clean = re.sub("[^a-zA-Z]"," ", raw)<br/>    words = clean.split()<br/>    return map(lambda x:x.lower(),words)<br/><br/></pre>
<p>Since we haven't yet captured the data from the text responses in our hypothetical business use case, let's collect a good quality dataset that's available on the web. Demonstrating our understanding and skills with this corpus will prepare us for the hypothetical business use case data. You can also use your own dataset, but it's important to have a huge amount of words so that the <kbd>word2vec</kbd> model can generalize well. So, we will load our data from the Project Gutenberg website, available at<span> </span><a href="http://Gutenberg.org">Gutenberg.org</a>.</p>
<p>Then we tokenize the raw corpus into the list of unique clean words, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1378 image-border" src="assets/e713b0f9-7853-4d45-bffa-b1c79dae435e.png" style="width:162.50em;height:56.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">This process depicts the data transformation, from raw data, to the list of words that will be fed into the word2vec model</div>
<p>Here we will download the text data from the URL and process them as shown in the preceding figure:</p>
<pre># Article 0on earth from Gutenberg website<br/>filepath = 'http://www.gutenberg.org/files/33224/33224-0.txt<br/><br/>corpus_raw = requests.get(filepath).text<br/><br/>tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')<br/><br/>raw_sentences = tokenizer.tokenize(corpus_raw)<br/><br/>#sentence where each word is tokenized<br/>sentences = []<br/>for raw_sentence in raw_sentences:<br/>    if len(raw_sentence) &gt; 0:<br/>        sentences.append(sentence_to_wordlist(raw_sentence))<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining our word2vec model</h1>
                </header>
            
            <article>
                
<p>Now let's use <span><kbd>gensim</kbd></span> in our definition of the <kbd>word2vec</kbd> model. To begin, let's define a few hyperparameters for our model, such as the dimension, which means how many low-level features we want to learn. Each dimension will learn a unique concept of gender, objects, age, and so on.</p>
<div class="packt_tip"><strong>Computational linguistics model tip #1</strong>: Increasing the number of dimensions leads to better generalization... but it also adds more computational complexity. The right number is an empirical question for you to determine as an applied AI deep learning engineer!<br/>
<br/>
<strong>Computational linguistics model tip #2</strong>: Pay attention to <kbd>context_size</kbd><em>. </em>This is important because it sets the upper limit for the distance between the current and target word prediction within a sentence. This helps the model in learning the deeper relationships between a word and the other nearby words.</div>
<p>Using the <kbd>gensim</kbd> instance, we will define our model, including all the hyperparameters:</p>
<pre>num_features = 300<br/><br/># Minimum word count threshold.<br/>min_word_count = 3<br/><br/># Number of threads to run in parallel.<br/><br/>#more workers, faster we train<br/>num_workers = multiprocessing.cpu_count()<br/><br/># Context window length.<br/>context_size = 7<br/><br/># Downsample setting for frequent words. 0 - 1e-5 is good for this<br/>downsampling = 1e-3<br/><br/>seed = 1<br/><br/>model2vec = w2v.Word2Vec(<br/>            sg=1,<br/>            seed=seed,<br/>            workers=num_workers,<br/>            size=num_features,<br/>            min_count=min_word_count,<br/>            window=context_size,<br/>            sample=downsampling<br/>        )<br/>        <br/>model2vec.build_vocab(sentences)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Once we have configured the <kbd>gensim word2vec</kbd> object, we need to give the model some training. Be prepared, as this might take some time depending on the amount of data and the computation power you have. In this process, we have to define the number of epochs<em> </em>we need to run, which can vary depending on your data size. You can play around with these values and evaluate your <kbd>word2vec</kbd> model's performance.</p>
<p>Also, we will save the trained model so that we can use it later on while building our language models:</p>
<pre>"""**Start training, this might take a minute or two...**"""<br/><br/>model2vec.train(sentences ,total_examples=model2vec.corpus_count , epochs=100)<br/><br/>"""**Save to file, can be useful later**"""<br/><br/>if not os.path.exists(os.path.join("trained",'sample')):<br/>    os.makedirs(os.path.join("trained",'sample'))<br/><br/>model2vec.save(os.path.join("trained",'sample', ".w2v"))</pre>
<p>Once the training process is complete, you can see a binary file stored in <kbd>/trained/sample.w2v</kbd>. You can share the <kbd><span>sample.w2v</span></kbd> file with others and they can use this word vectors in their NLP usecases and load it later into any other NLP task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the model</h1>
                </header>
            
            <article>
                
<p>Now that we have trained our <kbd>word2vec</kbd> model, let's explore what our model was able to learn. We will use <kbd>most_similar()</kbd> to explore the relations between various words. In the following example, you see that the model was able to learn that the word <kbd>earth</kbd> is related to <kbd>crust</kbd>, <kbd>globe</kbd>, and other words. It is interesting to see that we only provided the raw data and the model was able to learn all of these relations and concepts automatically! The following is the example:</p>
<pre>model2vec.most_similar("earth")<br/><br/>[(u'crust', 0.6946468353271484),  <br/> (u'globe', 0.6748907566070557),  <br/> (u'inequalities', 0.6181437969207764),  <br/> (u'planet', 0.6092090606689453),  <br/> (u'orbit', 0.6079996824264526),  <br/> (u'laboring', 0.6058655977249146),  <br/> (u'sun', 0.5901342630386353),  <br/> (u'reduce', 0.5893668532371521),  <br/> (u'moon', 0.5724939107894897),  <br/> (u'eccentricity', 0.5709577798843384)]</pre>
<p>Let's try to find words related to <kbd>human</kbd> and see what the model has learned:</p>
<pre>model2vec.most_similar("human")<br/><br/> [(u'art', 0.6744576692581177),  <br/> (u'race', 0.6348963975906372),  <br/> (u'industry', 0.6203593611717224),  <br/> (u'man', 0.6148483753204346),  <br/> (u'population', 0.6090731620788574),  <br/> (u'mummies', 0.5895125865936279),  <br/> (u'gods', 0.5859177112579346),  <br/> (u'domesticated', 0.5857442021369934),  <br/> (u'lives', 0.5848811864852905),  <br/> (u'figures', 0.5809590816497803)]</pre>
<div class="packt_tip"><span><strong>Critical thinking tip</strong>: It's interesting to observe that <kbd>art</kbd>, <kbd>race</kbd>, and <kbd>industry</kbd> are the most similar outputs. Remember that these similarities are based on the corpus of text that we used for training, and they should be thought of in that context. Generalization, and its unwanted sidekick, bias, can come into play when similarities from outdated or dissimilar training corpora are used to train a model that is applied to a new set of language data or cultural norms.</span></div>
<p>Even when we try to derive an analogy by using two positive vectors, <kbd>earth</kbd> and <kbd>moon</kbd>, and a negative vector, <kbd>orbit</kbd>, the model predicts the word <kbd>sun</kbd>, which makes sense because there is a semantic relation between the moon orbiting around the earth, and the earth orbiting around the sun:</p>
<pre>model2vec.most_similar_cosmul(positive=['earth','moon'], negative=['orbit'])<br/><br/><strong><span>(u'sun', 0.8161555624008179)</span></strong></pre>
<p>So, we learned that by using the <kbd>word2vec</kbd> model we can derive valuable information from raw unlabeled data. This process is crucial in terms of learning the grammar of a language and the semantic correlations between words.</p>
<div class="packt_tip packt_infobox">Later, we will learn how to use these <kbd>word2vec</kbd> features as an input for the classification model, which helps in boosting the model's accuracy and performance.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting the word cluster using the t-SNE algorithm</h1>
                </header>
            
            <article>
                
<p>So, after our analysis, we know that our <kbd>word2vec</kbd> model has learned some concepts from the provided corpus, but how do we visualize it? Because we have created a 300-dimensional space to learn the features, it's practically impossible for us to visualize. To make it possible, we will use a dimension reduction algorithm, called t-SNE, which is very well known for reducing a high dimensional space into more humanly understandable two or three-dimensional space.</p>
<div class="packt_quote CDPAlignLeft CDPAlign"><span>"t-Distributed Stochastic Neighbor Embedding (t-SNE) (</span><a href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a><span>) is a (</span>prize-winning<span><span>) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The technique can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. We applied it on data sets with up to 30 million examples."<br/>
                                                                                                   – Laurens van der Maaten</span></span></div>
<p>To implement this, we will use the <kbd>sklearn</kbd> package, and define <kbd>n_components=2</kbd>, which means we want to have 2-D space as the output. Next, we will perform the transformation by feeding the word vectors into the t-SNE object.</p>
<p>After this step, we now have a set of values for each word that we can use as <kbd>x</kbd> and <kbd>y</kbd> coordinates, respectively, to plot it on the 2D plane. Let's prepare a <kbd>DataFrame</kbd> to store all the words and their <kbd>x</kbd> and <kbd>y</kbd> coordinates in the same variable, as shown in the following screenshot<em>,</em> and take data from there to create a scatter plot:</p>
<pre>tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)<br/><br/>all_word_vectors_matrix = model2vec.wv.vectors<br/><br/>all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)<br/><br/>points = pd.DataFrame(<br/>    [<br/>        (word, coords[0], coords[1])<br/>        for word, coords in [<br/>            (word, all_word_vectors_matrix_2d[model2vec.wv.vocab[word].index])<br/>            for word in model2vec.wv.vocab<br/>        ]<br/>    ],<br/>    columns=["word", "x", "y"]<br/>)<br/><br/>sns.set_context("poster")  <br/>ax = points.plot.scatter("x", "y", s=10, figsize=(20, 12))<br/>fig = ax.get_figure()<br/><br/></pre>
<p>This is our <kbd>DataFrame</kbd> containing words and coordinates for both <kbd>x</kbd> and <kbd>y</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f8c999a1-a465-405a-8e08-a79a7a592cbd.png" style="width:18.33em;height:24.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Our word list with the coordinate values obtained using t-SNE</div>
<p>This is what the entire cluster looks like after plotting <span>425,633 tokens on the 2D plane. Each point is positioned after learning the features and correlations between the nearby words, as shown:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1bc2606d-b75f-4012-86b4-ef636a7c1485.png" style="width:54.17em;height:32.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A scatter plot of all the unique words on a 2D plane</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the embedding space by plotting the model on TensorBoard</h1>
                </header>
            
            <article>
                
<p>There is no benefit to visualization if you cannot make use of it, in terms of understanding how and what the model has learned. To gain a better intuition of what the model has learned, we will be using TensorBoard. </p>
<p>TensorBoard is a powerful tool that can be used to build various kinds of plots to monitor your models while in the training process, as well as building DL architectures and word embeddings. Let's build a TensorBoard embedding projection and make use of it to do various kinds of analysis.</p>
<p>To build an embedding plot in TensorBoard, we need to perform the following steps:</p>
<ol>
<li>Collect the words and the respective tensors (300-D vectors) that we learned in previous steps.</li>
<li>Create a variable in the graph that will hold the tensors.</li>
<li>Initialize the projector.</li>
<li>Include an appropriately named embedding layer.</li>
<li>Store all the words in a <kbd>.tsv</kbd> formatted metadata file. These file types are used by TensorBoard to load and display words.</li>
<li> Link the <kbd>.tsv</kbd> metadata file to the projector object.</li>
<li>Define a function that will store all of the summary checkpoints. </li>
</ol>
<p>The following is the code to complete the preceding seven steps:</p>
<pre>vocab_list = points.word.values.tolist()<br/>embeddings = all_word_vectors_matrix<br/><br/><br/>embedding_var = tf.Variable(all_word_vectors_matrix, dtype='float32', name='embedding')<br/>projector_config = projector.ProjectorConfig()<br/><br/><br/>embedding = projector_config.embeddings.add()<br/>embedding.tensor_name = embedding_var.name<br/><br/>LOG_DIR='./'<br/>metadata_file = os.path.join("sample.tsv")<br/><br/>with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:<br/>    metadata.writelines("%s\n" % w.encode('utf-8') for w in vocab_list)<br/><br/>embedding.metadata_path = os.path.join(os.getcwd(), metadata_file)<br/><br/># Use the same LOG_DIR where you stored your checkpoint.<br/>summary_writer = tf.summary.FileWriter(LOG_DIR)<br/><br/># The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will<br/># read this file during startup.<br/>projector.visualize_embeddings(summary_writer, projector_config)<br/><br/>saver = tf.train.Saver([embedding_var])<br/><br/>with tf.Session() as sess:<br/>    # Initialize the model<br/>    sess.run(tf.global_variables_initializer())<br/><br/>    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))</pre>
<p>Once the TensorBoard preparation module is executed, the binaries, metadata, and checkpoints get stored in the disk, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/716fd34b-b219-42ad-aa15-6a0dfaf88a18.png" style="width:44.75em;height:4.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The outputs created by TensorBoard</div>
<p>To visualize the TensorBoard, execute the following command in the Terminal:</p>
<pre><strong>tensorboard --logdir=/path/of/the/checkpoint/</strong></pre>
<p>Now, in the browser, open <kbd>http://localhost:6006/#projector</kbd>, you should see TensorBoard with all the data points projected in 3D space. You can zoom in, zoom out, look for specific words, as well as retrain the model using t-SNE, and visualize the cluster formation of the dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1380 image-border" src="assets/8b31c192-5668-4780-b9d5-ecb78d1e7979.png" style="width:65.42em;height:33.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The TensorBoard embedding projection</div>
<div class="packt_tip">Data visualization helps you tell your story! TensorBoard is very cool! Your business use case stakeholders love impressive dynamic data visualizations. They help with your model intuition, and with generating new hypotheses to test.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building language models using CNN and word2vec</h1>
                </header>
            
            <article>
                
<p>Now that we have learned the core concepts of computational linguistics, and trained relations from the provided dataset, we can use this learning to implement a language model that can perform a task. </p>
<p>In this section, we will build a text classification model to perform sentiment analysis. For classification, we will be using a combination of CNN and a pre-trained <kbd>word2vec</kbd> model, which we learned about in the previous section of this chapter. </p>
<div class="packt_infobox">This task is the simulation of our hypothetical business use case of taking text responses from restaurant patrons and classifying what they text back into meaningful classes for the restaurant.</div>
<p>We have been inspired by Denny Britz's (<a href="https://twitter.com/dennybritz">https://twitter.com/dennybritz</a>) work on <span><em>Implementing a CNN for Text Classification in TensorFlow</em> (<a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a>) in our own CNN and text classification build. We invite you to review the blog he created to gain a more complete understanding of the internal mechanisms that make CNNs useful for text classification.</span></p>
<p class="mce-root"/>
<p><span>As an overview, this architecture starts with an input embedding step, then a 2D convolution utilizing max pooling with multiple filters, and a softmax activation layer producing the output.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the CNN model</h1>
                </header>
            
            <article>
                
<p class="mce-root">You might be asking yourself, how do you use CNNs to classify text when they are most commonly used in image processing?</p>
<div class="mce-root packt_tip">There are many <span>discussions</span> in the literature, linked at the bottom of this tip, which have proven that CNNs are a generic feature extraction function that can compute <strong>location invariance</strong> and <strong>compositionality</strong>. The location invariance property helps the model to capture the context of words, irrespective of their occurrence in the corpus. Compositionality helps to derive higher-level representations using lower-level features:<br/>
<br/>
<ul>
<li><span><span>Convolutional Neural Networks for Sentence Classification (<a href="https://arxiv.org/abs/1408.5882" target="_blank">https://arxiv.org/abs/1408.5882</a>)</span></span></li>
</ul>
<ul>
<li><span><span>A CNN Based Scene Chinese Text Recognition Algorithm with Synthetic Data Engine (<a href="https://arxiv.org/abs/1604.01891" target="_blank">https://arxiv.org/abs/1604.01891</a>)</span></span></li>
</ul>
<ul>
<li><span>Text-Attentional Convolutional Neural Networks for Scene Text Detection (<a href="https://arxiv.org/pdf/1510.03283.pdf" target="_blank">https://arxiv.org/pdf/1510.03283.pdf</a>)</span></li>
</ul>
</div>
<p class="mce-root">So instead of sending pixel values for an image into the model, we feed one-hot encoded word vectors or the <kbd>word2vec</kbd> matrix, which represent a word or a character (for character-based models). <span>Denny Britz's </span>implementation has two filters each in three region sizes of two, three, and four. The convolution operation is performed by these filters as it processes over the sentence matrix to generate feature maps. Downsampling is performed by a max pooling operation over each activation map. Finally, all the outputs are concatenated and passed into the softmax classifier.</p>
<p class="mce-root">Because we are performing sentiment analysis, there will be both a positive and a negative output class target. The softmax classifier will output probabilities for each class, as shown:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><br/>
<img class="aligncenter size-full wp-image-1124 image-border" src="assets/7321c34b-1557-403b-9cca-d0aa076c6a3b.png" style="width:39.17em;height:42.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">This diagram is taken from Denny Britz's blog post describing the functioning of the CNN language model</div>
<p>Let's look into the implementation of the model. We have modified the existing implementation by adding the input of the previously trained <kbd>word2vec</kbd> model component.</p>
<div class="packt_infobox">The code for this project can be found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis</a>.<a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis" target="_blank"/></div>
<p>The model resides in <kbd>text_cnn.py</kbd>. We created a class, named <kbd>TextCNN</kbd>, which takes a few parameters as an input for the model's configuration, also known as hyperparameters. The following is a list of hyperparameters:</p>
<ul>
<li style="font-weight: 400"><kbd>sequence_length</kbd>: The fixed sentence length</li>
<li style="font-weight: 400"><kbd>num_classes</kbd>: The number of output classes that will be produced by the softmax activation (positive and negative)</li>
<li style="font-weight: 400"><kbd>vocab_size</kbd>: The count of unique words in our embeddings</li>
<li style="font-weight: 400"><kbd>embedding_size</kbd>: Embedding dimensionality that we created</li>
<li style="font-weight: 400"><kbd>filter_sizes</kbd>: The convolutional filter will cover this many words</li>
<li style="font-weight: 400"><kbd>num_filters</kbd>: Each filter size will have this many filters</li>
<li><kbd>pre_trained</kbd>: Integrates the <kbd>word2vec</kbd> representation that has been previously trained</li>
</ul>
<p class="mce-root">Following is the declaration of the <kbd>TextCNN()</kbd> class with the <kbd>init()</kbd> function initializing all the hyperparameter values:</p>
<pre>import tensorflow as tf<br/>import numpy as np<br/><br/><br/>class TextCNN(object):<br/>    """<br/>    A CNN for text classification.<br/>    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.<br/>    """<br/><br/>    def __init__(self,<br/>                 sequence_length,<br/>                 num_classes,<br/>                 vocab_size,<br/>                 embedding_size,<br/>                 filter_sizes,<br/>                 num_filters,<br/>                 l2_reg_lambda=0.0,<br/>                 pre_trained=False):</pre>
<p class="mce-root"/>
<p>The code is divided into six main parts:</p>
<ol>
<li style="font-weight: 400"><strong>Placeholders for inputs</strong>: All the placeholders that we need to contain the input values for our model are defined first. In this case, inputs are the sentence vector and associated labels (either positive or negative). <kbd>input_x</kbd> holds the sentence, <kbd>input_y</kbd> holds the value of label, and we use <kbd>dropout_keep_prob</kbd> for <span>the probability that we keep a neuron in the dropout layer. The following code shows an example of this:</span></li>
</ol>
<pre style="padding-left: 60px"># Placeholders for input, output and dropout<br/>self.input_x = tf.placeholder(<br/>    tf.int32, [<br/>        None,<br/>        sequence_length,<br/>    ], name="input_x")<br/>self.input_y = tf.placeholder(<br/>    tf.float32, [None, num_classes], name="input_y")<br/>self.dropout_keep_prob = tf.placeholder(<br/>    tf.float32, name="dropout_keep_prob")<br/># Keeping track of l2 regularization loss (optional)<br/>l2_loss = tf.constant(0.0)</pre>
<ol start="2">
<li><strong>Embedding</strong>: Our model's first layer, in which we feed the word representations learned in the process of training the <kbd>word2vec</kbd> model, is the embedding layer. We will modify the baseline code that's in the repository to use our pre-trained embedding model, instead of learning the embedding from scratch. This will enhance the model accuracy. It is also a kind of a <kbd>transfer learning</kbd>, where we transfer the general knowledge learned from a generic Wikipedia or social media corpus. The embedding matrix that is initialized with the <kbd>word2vec</kbd> model is named <kbd>W</kbd>, as seen as follows:</li>
</ol>
<pre style="padding-left: 60px"># Embedding layer<br/>with tf.device('/cpu:0'), tf.name_scope("embedding"):<br/>    if pre_trained:<br/>        W_ = tf.Variable(<br/>            tf.constant(0.0, shape=[vocab_size, embedding_size]),<br/>            trainable=False,<br/>            name='W')<br/>        self.embedding_placeholder = tf.placeholder(<br/>            tf.float32, [vocab_size, embedding_size],<br/>            name='pre_trained')<br/>        W = tf.assign(W_, self.embedding_placeholder)<br/>    else:<br/>        W = tf.Variable(<br/>            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),<br/>            name="W")<br/>    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)<br/>    self.embedded_chars_expanded = tf.expand_dims(<br/>        self.embedded_chars, -1)</pre>
<ol start="3">
<li style="font-weight: 400"><strong>Convolution with maxpooling:</strong> Defining the convolution layer is done with <kbd>tf.nn.conv2d()</kbd>. This takes, as inputs, the previous embedding layer's weight (<kbd>W</kbd>—filter matrix) and applies a nonlinear ReLU activation function. Further max polling is performed over each filter size using <kbd><span>tf.nn.max_pool(</span><span>)</span></kbd><em><span>.</span></em> R<span>esults are concatenated, creating a single vector that will become the inputs for the following layer of the model:</span></li>
</ol>
<pre style="padding-left: 60px"># Create a convolution + maxpool layer for each filter size<br/>pooled_outputs = []<br/>for i, filter_size in enumerate(filter_sizes):<br/>    with tf.name_scope("conv-maxpool-%s" % filter_size):<br/>        # Convolution Layer<br/>        filter_shape = [filter_size, embedding_size, 1, num_filters]<br/>        W = tf.Variable(<br/>            tf.truncated_normal(filter_shape, stddev=0.1), name="W")<br/>        b = tf.Variable(<br/>            tf.constant(0.1, shape=[num_filters]), name="b")<br/>        conv = tf.nn.conv2d(<br/>            self.embedded_chars_expanded,<br/>            W,<br/>            strides=[1, 1, 1, 1],<br/>            padding="VALID",<br/>            name="conv")<br/>        # Apply nonlinearity<br/>        h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")<br/>        # Maxpooling over the outputs<br/>        pooled = tf.nn.max_pool(<br/>            h,<br/>            ksize=[1, sequence_length - filter_size + 1, 1, 1],<br/>            strides=[1, 1, 1, 1],<br/>            padding='VALID',<br/>            name="pool")<br/>        pooled_outputs.append(pooled)<br/><br/># Combine all the pooled features<br/>num_filters_total = num_filters * len(filter_sizes)<br/>self.h_pool = tf.concat(pooled_outputs, 3)<br/>self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])</pre>
<p class="mce-root"/>
<ol start="4">
<li><strong>Dropout layer</strong>: To regularize CNN and prevent the model from overfitting, a minor percentage of signals from neurons are blocked. This forces the model to learn more unique or individual features:</li>
</ol>
<pre style="padding-left: 60px"># Add dropout<br/>with tf.name_scope("dropout"):<br/>    self.h_drop = tf.nn.dropout(self.h_pool_flat,<br/>                                self.dropout_keep_prob)</pre>
<ol start="5">
<li><strong>Prediction</strong>: A TensorFlow wrapper performs the <em>W * x+b</em> metric multiplications, where <kbd>x</kbd> is the output of the previous layer. This computation will compute the values for the scores and the predictions will be produced by <kbd>tf.argmax()</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Final (unnormalized) scores and predictions<br/>with tf.name_scope("output"):<br/>    W = tf.get_variable(<br/>        "W",<br/>        shape=[num_filters_total, num_classes],<br/>        initializer=tf.contrib.layers.xavier_initializer())<br/>    b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name="b")<br/>    l2_loss += tf.nn.l2_loss(W)<br/>    l2_loss += tf.nn.l2_loss(b)<br/>    self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name="scores")<br/>    self.predictions = tf.argmax(self.scores, 1, name="predictions")</pre>
<ol start="6">
<li><strong><strong>Accuracy</strong></strong>: We can define the <kbd>loss</kbd> function with our scores. Remember that the measurement of the error our network makes is called <strong>loss</strong>. As good DL engineers, we want to minimize this and make our model more accurate. For the problem of categorization, the<span> </span>cross-entropy loss (<a href="http://cs231n.github.io/linear-classify/#softmax">http://cs231n.github.io/linear-classify/#softmax</a>) is the standard <kbd>loss</kbd> function used:</li>
</ol>
<pre style="padding-left: 60px"># CalculateMean cross-entropy loss<br/>with tf.name_scope("loss"):<br/>    losses = tf.nn.softmax_cross_entropy_with_logits(<br/>        labels=self.input_y, logits=self.scores)<br/>    self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss<br/><br/># Accuracy<br/>with tf.name_scope("accuracy"):<br/>    correct_predictions = tf.equal(self.predictions,<br/>                                   tf.argmax(self.input_y, 1))<br/>    self.accuracy = tf.reduce_mean(<br/>        tf.cast(correct_predictions, "float"), name="accuracy")</pre>
<p><span>That's it, we're done with our model. Let's use TensorBoard to </span><span>visualize the network and improve our intuition, as shown</span><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e942cccf-bcff-4240-9126-3807f881ce44.png" style="width:50.50em;height:27.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The CNN model architecture definition</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding data format</h1>
                </header>
            
            <article>
                
<p>An interesting dataset,<span> </span><em>Movie Review Data</em> from Rotten Tomatoes (<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">http://www.cs.cornell.edu/people/pabo/movie-review-data/</a>), was used in this case<span>.</span> Half of the reviews are positive, the other half negative, and there are about 10,000 sentences in total. There are around 20,000 different words in the vocabulary. The dataset is stored in the <kbd>data</kbd> folder.</p>
<p class="mce-root"/>
<p>It contains two files: one, <span><kbd>rt-polarity.neg</kbd>,</span> contains all the negative sentences, and another, <span><kbd>rt-polarity.pos</kbd>,</span> contains only positive sentences. To perform classification, we need to associate them with the labels. Each positive sentence is associated with a one-hot encoded label, <kbd>[0, 1]</kbd>, and each negative sentence is associated with <kbd>[1, 0]</kbd>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4ad74017-2ffb-4d21-acc7-a0f985edfc75.png" style="width:44.33em;height:32.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A sample of few positive sentences and the label associated with the sentence</div>
<p>Pre-processing the text data is done with these four steps:</p>
<ol>
<li><strong>Load</strong>: Make sure to load both the positive and negative sentence data files</li>
<li><strong>Clean</strong>: Use regex to remove punctuation and other special characters</li>
<li><strong>Pad</strong>: Make each sentence the same size by appending <kbd>&lt;PAD&gt;</kbd><span> </span>tokens</li>
<li><strong>Index</strong>: Map each word to an integer in an index so that each sentence can become a vector of integers</li>
</ol>
<p>Now that we have our data formatted as vectors, we can feed them into our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating word2vec with CNN</h1>
                </header>
            
            <article>
                
<p>So, the last time we created our <kbd>word2vec</kbd> model, we dumped that model into a binary file. Now it's time to use that model as part of our CNN model. We perform this by initializing the <kbd>W</kbd> <span>weights </span>in the embeddings to these values.</p>
<p>Since we trained on a very small corpus in our previous <kbd>word2vec</kbd> model, let's choose the <kbd>word2vec</kbd> model that was pre-trained on the huge corpus. A good strategy is to use fastText embedding, which is trained on documents available online and for <span>294</span> languages (<a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</a>). We do this as follows:</p>
<ol>
<li>We will download the English Embedding fastText dataset (<a href="https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip">https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip</a>)</li>
<li>Next, extract the vocab and embedding vectors into a separate file</li>
<li>Load them into the <kbd>train.py</kbd> file</li>
</ol>
<p>That's it—by introducing this step, we can now feed the embedding layer with the pre-training <kbd>word2vec</kbd> model. This incorporation of information has a sufficient amount of features to improve the learning process of the CNN model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing the model </h1>
                </header>
            
            <article>
                
<p>Now it's time to train our model with the provided dataset and the pre-trained embedding model. A few hyperparameters will need fine-tuning to achieve good results. But once we have executed the <kbd>train.py</kbd> file with reasonably good configurations, we can demonstrate that the model is able to distinguish well between the positive and negative sentences when classifying.</p>
<p>As we can see in the following graph, the performance metric of accuracy is tending towards 1 and the loss factor is reducing towards 0 over each iteration:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/02a5950d-45b2-4310-b7fb-0bce7b42a4ff.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A plot of the performance metrics accuracy and loss of the CNN model during the training process</div>
<p class="mce-root"/>
<p><span>Voila! We just used the pre-trained embedding model to train our CNN classifier with an average loss of 6.9 and accuracy of 72.6%.</span></p>
<p>Once the model training is completed successfully, the output of the model will have the following:</p>
<ul>
<li>The checkpoints stored in <kbd>/runs/folder</kbd>. We will use these checkpoints to make predictions.</li>
<li>A summary with all the loss, accuracy, histogram, and gradient value distribution captured during the training process. We can visualize it using the TensorBoard.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploy the model into production</h1>
                </header>
            
            <article>
                
<p>Now that we have our model binaries stored in the <kbd>/runs/</kbd> folder, we just need to write a restful API, for which you can use Flask, and then call the <kbd>sentiment_engine()</kbd> defined in the <kbd>model_inference.py</kbd> code.</p>
<p>Always make sure that you use the checkpoints of the best model and the correct embedding file, which is defined as the following:</p>
<pre>checkpoint_dir = "./runs/1508847544/"<br/>embedding = np.load('fasttext_embedding.npy')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>Today's project was to build a</span><span class="s1"> DL computational linguistics model using</span> word2vec<span class="s1"> to accurately classify text in a sentiment analysis paradigm.</span> <span>Our hypothetical use case was to apply DL to enable the management of a restaurant chain to understand the general sentiment of text responses their customers made, in response to a phone text question asking about their experience after dining. Our specific task was to build the natural language processing model that would create business intelligence from the data obtained in this simple (hypothetical) application.</span></p>
<div class="packt_tip"><strong>Revisit our success criteria</strong>: How did we do? Did we succeed? What is the impact of success? Just as we defined success at the beginning of the project, these are the key questions we ask as DL data scientists as we look to wrap up a project.</div>
<p>Our CNN model, which was built on the trained <kbd>word2vec</kbd> model created earlier in the chapter, reached an accuracy of 72.6%! This means that we were able to reasonably accurately classify the unstructured text sentences as positive or negative.</p>
<p class="mce-root">What are the implications of this accuracy? In our hypothetical example, this means that we can take a body of data that is difficult to summarize outside of this DL NLP model and summarize it to produce actionable insights for the restaurant management. With summary data points of positive or negative sentiment to the questions asked in a phone text, the restaurant chain can track performance over time, make adjustments, and possibly even reward staff for improvements.</p>
<p>In this <span>chapter's project, we learned how to build <kbd>word2vec</kbd> models and analyze what characteristics we can learn about the provided corpus. We also learned how to build a language model with CNN, using the trained word embeddings.</span></p>
<p>Finally, we looked at the model performance in testing and determined whether we succeeded in achieving our goals. In the next chapter's project, we're going to leverage even more power from our computational linguistic skills<span> to create a natural language pipeline that will power a chatbot for open domain question answering.</span><span class="Apple-converted-space"> This is exciting work—let's see what next!</span></p>


            </article>

            
        </section>
    </body></html>