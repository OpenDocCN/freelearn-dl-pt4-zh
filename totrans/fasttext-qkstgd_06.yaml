- en: Sentence Classification in FastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fastText supervised learning:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical softmax architecture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'N-grams features and the hashing trick:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Fowler**-**Noll**-**Vo** (**FNV**) hash
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings and their use in sentence classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fastText model quantization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compression:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vector quantization:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the codebook for high-dimensional spaces
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Product quantization
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional steps
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentence classification deals with understanding text found in natural languages
    and determining the classes that it may belong to. In the text classification
    set of problems, you will have a set of documents *d* that belongs to the corpus
    *X* (which contains all the documents). You will also have a set of finite classes
    *C* = *{c[1] , c[2], ..., c[n]}*. Classes are also called categories or labels.
    To train a model, you would need a classifier, which is generally a well-tested
    algorithm (not necessary but in this case we will be talking about a well-tested
    algorithm that is used in fastText) and you will need a corpus with documents
    and associated labeling identifying the classes that each document belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text classification has many practical uses, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating spam classifiers in email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page ranking and indexing in search engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment detection in reviews that will give an idea whether customers are
    happy with the product or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification is generally a way to augment manual classification. Labeling
    a document is largely subjective and depends on the corpus. A sample document
    "I like traveling to Hawaii" may be regarded as falling under the class "Travel"
    by a librarian but may be regarded as "Irrelevant" by a doctor. So the idea is
    that a set of documents will be labeled by a domain expert, the labeled data will
    be used to train a text classifier, and then the text classifier can be used to
    predict new incoming text, saving the time and resources of the domain expert
    (maybe the domain expert can periodically check and audit the performance of the
    classifier against incoming text). Also, the proposed idea is for general people
    and does not apply to crowd-sourced labeling as done by stack overflow when it
    asks for users to label; most business problems do not have the luxury of such *auto*
    labeling and hence you will have to spend some time manually labeling the documents.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the performance of a classification model, we divide the training
    corpus into test and train sets. Only the train set is used for model training.
    Once done, we classify the test set and compare the predictions with the actual
    ones and measure the performance. The portion of correctly classified documents
    to the portion of actual documents is called accuracy. There are two more parameters
    that we can look at that will give a measure for the model performance. One is
    the *recall*, which means the percentage of all the correct labels that we recalled
    as opposed to the labels that actually existed. We can also look at the *precision*
    of the model, which means that we look at all the predicted labels and say which
    portions of them are the actual labels in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: fastText supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fastText classifier is built on top of a linear classifier, specifically a
    BoW classifier. In this section, you will get to know the architecture of the
    fastText classifier and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can consider that each piece of text and each label is actually a vector
    in space and the coordinates of that vector are what we are actually trying to
    tweak and train so that the vector for a text and associated label are really
    close in space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Vector representation of the text
  prefs: []
  type: TYPE_NORMAL
- en: So, in this example, which is an example shown in 2D space, you have texts that
    are saying things such as "Nigerian Tommy Thompson is also a relative newcomer
    to the wrestling scene" and "James scored 20 of his 46 points in the opening quarter"
    are closer to the "sports" label and not the "travel" label.
  prefs: []
  type: TYPE_NORMAL
- en: The way we can do this is we can take the vector representing the text and the
    vector representing the label and input it into the scoring function. We then
    take the score and then we normalize across sum of all the scores between the
    vector representing the text and the vector representations for every other possible
    label. And that provides us with the type of probability that the given text above
    will have the label. The scores are the individual values and we convert them
    to probabilities using the softmax function which was also discussed in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The fastText uses similar vector space models for text classification, where
    the words are reduced to low-dimensional vectors called embeddings. The aim is
    to train and arrive at a vector space such that the sentence vectors and the label
    vectors are really close to each other. To apply vector space models to sentences
    or documents, one must first select an appropriate function, which is a mathematical
    process for combining multiple words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Composition functions fall into two classes: unordered or syntactic. Unordered
    functions treat input texts as **bag of words** (**BoW**) embeddings, while syntactic
    representations take word order and sentence structure into account. The fastText
    is mostly an unordered approach since it takes the BoW approach but has a little
    bit of syntactic representations using the n-grams, as we will see later.'
  prefs: []
  type: TYPE_NORMAL
- en: What you can do next is take the representations and then train a linear classifier
    on top of them. Good linear classifiers that can be used are logistic regression
    and **support vector machines** (**SVM**). Linear classifiers, though, have a
    little caveat. They do not share parameters between features and classes. As a
    result of this, there is a possibility that this limits the generalization capabilities
    to those types of classes which do not have many examples to train on. The solution
    to this problem is to use multilayered neural networks or to factorize the linear
    classifier into low rank matrices and then run a neural network on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic representations are more sparse than BoW approach and hence require
    more training time. This makes them computationally very expensive in case of
    huge datasets or when you have limited computational resources. For example, if
    you build a recursive neural network for training on syntactic word representations
    that again computes costly tensor products, and furthermore there will be non-linearity
    in every node of a syntactic parse tree, then your training time may stretch to
    days, which is prohibitive for fast feedback cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'So you can use an averaging network, which is an unordered model that can be
    explained in three simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the vector average of the embeddings associated with an input sequence
    of tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the average through one or more feed-forward layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform linear classification on the final layer's representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model can be improved by applying a novel dropout-inspired regularizer.
    In this case, for each training instance, some of the token embeddings will be
    randomly dropped before computing the average. In fastText, this is done by subsampling
    frequent words, which was also discussed in the previous chapter and is used in
    the classifier as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A general form of the architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we want to map an input sequence of tokens to k labels. We first
    apply a composition function g to the sequence of word embeddings ν[ω] for ω ∈
    X. The output of this composition function is a vector z that serves as input
    to the logistic regression function. The architecture of the classification is
    similar to the cbow model that was discussed in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a weighted average of the word embeddings is taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Feeding *z* to a softmax layer induces estimated probabilities for each output
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the softmax function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, `W_s` is a *k* x *d* matrix for the dataset with *k* output labels and
    *b* is a bias term. fastText uses a generic bias term in the form of an end of
    sentence character `<s>` that gets added to all input examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model can then be trained to minimize cross-entropy error, which for a
    single training instance with ground truth label *y* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In fastText, this gets translated to computing the probability distribution
    over the predefined classes. For the set of *N* documents, this leads to minimizing
    the negative log likelihood over the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x[n]* is the normalized bag of features of the nth document, *y[n]* is
    the label, and A and B are the weight matrices. A is just a lookup table over
    the words in fastText.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can use stochastic gradient descent to keep tweaking those coordinates
    until we maximize the probability of the correct label for every piece of text.
    The fastText trains this model asynchronously on multiple CPUs using stochastic
    gradient descent and a linearly decaying learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture that is used in fastText is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.gif)'
  prefs: []
  type: TYPE_IMG
- en: Model architecture of fastText for a sentence with N gram word features *x[1]*,
    *x[2]*, ..., *x[N]*. The features are embedded and averaged to form the hidden
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s summarize the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: It starts with word representations, which are averaged into text representations,
    which are fed into a linear classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classifier is essentially a linear model with a rank constraint and fast
    loss approximation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text representation is a hidden state that can be shared among features
    and classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A softmax layer is used to obtain a probability distribution over predefined
    classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High computational complexity *O(kh)*, where *k* is the number of classes and
    *h* is the dimension of the text representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical softmax architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding the softmax is computationally quite expensive and prohibitive for a
    large corpus as this means that we not only have to find the score for the text
    with the label but the scores for the text with all the labels. So for *n* text
    and *m* labels, this scales with worst case performance of *O(n²)*, which you
    know is not good.
  prefs: []
  type: TYPE_NORMAL
- en: Also, softmax and other similar methods for finding the probabilities do not
    take into account the semantically meaningful organization of classes. A classifier
    should know that classifying a dog as a submarine should have a higher penalty
    than a dog as a wolf. An intuition that we may have is that the target labels
    are probably not flat but rather a tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we have *k* classes that we want each input to be classified into. So
    let''s consider these classes are the leaves of a tree. This tree is organized
    in such as way that the hierarchy is semantically meaningful. Consider further
    that our classifier maps an input to an output probability distribution over the
    leaves. Hopefully, this leaf will be the correct class of the corresponding input.
    The probability of any node in the tree is the probability of the path from the
    root to that node. If the node is at depth *l + 1* with parents *n[1]*, *n[2]*,
    ..., *n[l]* then the probability of the node is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A leaf falls under the node if the node is on the path from the root to the
    leaf. We then define the amount of the "win" or the "winnings" to be the weighted
    sum of the probabilities of the nodes along its path from the root to the leaf
    corresponding to the correct classes. During the optimization or the training
    process, we want to maximize this "winnings" for our model, and conversely minimize
    the "loss". Loss in this case is considered the negative of the win.
  prefs: []
  type: TYPE_NORMAL
- en: So in fastText, what is used is the hierarchical classifier, which is similar
    to the hierarchical softmax that you saw in the earlier chapter. In this method,
    it represents the labels in a binary tree and so every node in the binary tree
    is represented as a probability and so a label is represented by the probability
    along the path to that given label. In this case, the correct label is generated
    using the **breadth first search** (**BFS**) algorithm. BFS is quite fast for
    searching and hence you bring down the complexity to *log[2]n*. Now we just need
    to compute the probabilities of the path to the correct label. So when we have
    a lot of labels, this really increases the speed of computation for all the labels
    and hence the model training. And as you have seen in the previous chapter, the
    hierarchical probability representation asymptotes to the softmax probabilities
    and hence this approximations actually give the same kind of model performance
    and are vastly faster to train.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen in the previous chapter, in this case the output of the hierarchical
    classifier is the label. Similar to training word embeddings, in this case a Huffman
    tree is formed. Since we have already discussed the internals of the Huffman tree
    in the previous chapter, in this case we will tinker at little bit with the code
    and try to see the exact tree that is formed and find the probabilities associated
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things simple, we will take a very small dataset with very small number
    of labels. In this example, the following set of sentences along with the labels
    are taken and saved in a file named `labeledtextfile.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Since fastText is written in C++, for performance reasons it does not manipulate
    and work with the direct label strings. To get the Huffman codes of the label,
    you can change the `hierarchicalSoftmax` function on line `81` of `model.cc` to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, I am listing for multiple labels. But you can choose the label
    that you want. You will get output similar to this. You will need to get the last
    occurrence of the target value to get the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So the similar vector, corresponding to target 2, is 101.
  prefs: []
  type: TYPE_NORMAL
- en: The n-gram features and the hashing trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen, the BoW of the vocabulary is taken to arrive at the word representation
    to be used later in the classification process. But the BoW is unordered and does
    not have any syntactic information. Hence, the bag of n-grams are used as additional
    features to capture some of the syntactic information.
  prefs: []
  type: TYPE_NORMAL
- en: As we have already discussed, large-scale NLP problems almost always involve
    using a large corpus. This corpus will always have *unbounded* number of unique
    words, as we have seen from the Zipf's law. Words are generally defined as a string
    of characters separated with a delimiter, such as a space in English. Hence, taking
    word n-grams is simply not scalable to large corpora, which is essential to come
    to accurate classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Because of these two factors, the matrices that are formed naively are always
    sparse and high-dimensional. You can try to reduce the dimensions of the matrices
    using techniques such as PCA but that would still involve doing matrix manipulations
    that require such a high amount of memory that it would make the whole computation
    infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you can do something so that you are able to circumvent the creation
    of the dictionary? A similar problem is tackled with what is known as the kernel
    trick. The kernel trick enables us to use linear classifiers on non-linear data.
    In this method, the input data is transformed to a high-dimensional feature space.
    Interestingly, you just need to specify the kernel for this step, no need to transform
    all the data to the feature space, and it will work. In other words, when you
    compute the distance and apply the kernel, you get a number. The number is the
    same as what you would have got if you expanded your initial points into the higher-order
    space that your kernel points to and computed their inner product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a lot easier to get the inner product in a higher-dimensional space than
    the actual points in the higher dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of text classification are complementary. The original input
    space is generally linearly separable (because generally humans decide on the
    features based on tagging) but the training set is prohibitively large in size
    and very high-dimensional. For this common scenario, a complementary variation
    to the kernel trick is used. This method is called the hashing trick. Here, the
    high-dimensional vectors in *ℜ^d* are mapped into a lower-dimensional feature
    space *ℜ^m* such that *m << d*. We will train the classifier in *ℜ^m* space.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea in the hashing trick is the hash functions. So in text classification
    and similar NLP tasks, we take a non-cryptographic hash such as murmur or FNV
    (more on this later in the chapter) and map the work into a finite integer (usually
    32-bit or 64-bit integers which are modulo of a prime number).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the characteristics that define a hash function:'
  prefs: []
  type: TYPE_NORMAL
- en: The most important one—if you feed the same input to a hash function, it will
    always give the same output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the hash function determines the range of the possible outputs.
    The range is generally fixed. For example, modern websites use SHA256 hashes that
    are truncated to 128 bits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash functions are meant to be one way. Given a hash, the input should not be
    computable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Due to the fact that they have a fixed range, a pleasant side effect of using
    hash functions is that there are fixed memory requirements. Another advantage
    is that we get benefits on the **out-of-vocabulary** (**OOV**) front as well.
    This part is not that obvious so let me explain it. The first step on that note
    is that we don''t have to deal with the vocabulary at all. Instead, when starting
    with our BoW representations, we will start with a big column vector (2 million in
    our case) with a lot of elements for each of our training samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we will choose a hash function *f* that takes in strings as inputs and outputs
    values. In other words, we are making sure that our hash function will never address
    an index outside our feature's dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: There is the advantage in terms of OOV words as well, as compared to maintaining
    a large vocabulary in a *naive* BoW approach. Because the vector representation
    is created using a hash function, any string, even OOV words, will have a vector
    in the hash space. New words will worsen the accuracy of our classifier, true,
    but it will still work. No need to throw away the new words when predicting.
  prefs: []
  type: TYPE_NORMAL
- en: One reason why this should work comes from the same Zipf's law that we are time
    and again referring to. Hash collisions that may happen (if any), would probably
    be between infrequent words or between frequent word and an infrequent word. This
    is because frequent words by definition will occur earlier and hence tend to occupy
    the spaces first. Thus the collided feature used for classification will be either
    unlikely to be selected for feature selection or represent the word that led the
    classifier to select it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established the benefits of the hashing trick, we need to focus
    on the hashing function. There are many hash functions that are used for implementing
    the hashing trick, for example, Vowpal Wabbit and scikit-learn murmurhash v3\.
    A good list of possible non-cryptographic hashes can be found at the following
    Wiki link:[ https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions](https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions).
    FastText uses the FNV-1a hash, which will be discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: The FNV hash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'fastText uses the FNV-1a hash, which is the derivative of the FNV hash. To
    implement this algorithm, start with an initial hash value of `FNV_offset_basis`.
    For each byte in the input, take the XOR of the hash and the byte. Now multiply
    the result with the FNV prime. In terms of pseudo code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Source: Wikipedia, [https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function](https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fastText, this is implemented in the `dictionary.cc` hash function ([https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc#L143)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the offset basis considered is `2166136261` and corresponding
    prime number is `16777619`.
  prefs: []
  type: TYPE_NORMAL
- en: FNV is not cryptographic yet it has a high dispersion quality and a variable
    size hash result that can be any power of 2 from 32 to 1024 bits. The formula
    to generate it is among the simplest hash functions ever invented that actually
    achieve good dispersion. It also has good collision and bias resistance. One of
    the best properties of the FNV is that although it is not considered cryptographic,
    subverting bit sizes above 64 bits is mostly unsolvable. As the computational
    overhead needs to be kept to a minimum, fastText uses 32 bit, which has an FNV
    offset bias of 2166136261.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the Python implementation in the file `fnv1a.py` under the `chapter4`
    folder in the repo.
  prefs: []
  type: TYPE_NORMAL
- en: In fastText, word- and character-level n-grams are hashed into a fixed number
    of buckets. This prevents the memory requirements when training the model. You
    can change the number of buckets using the `-buckets` parameter. By default, the
    number of buckets is fixed at 2M (2 million).
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings and their use in sentence classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen in the previous chapter, word embeddings are the numerical
    representation of words in the shape of a vector in *ℜ^d*. They are unsupervised
    learned word representation vectors where there should be a correlation on semantic
    similarity. We also discussed what distributional representations and distributed
    representations are in [Chapter 7](part0160.html#4OIQ00-05950c18a75943d0a581d9ddc51f2755),
    *Deploying Models to Web and Mobile*.
  prefs: []
  type: TYPE_NORMAL
- en: When performing sentence classification, there is the hypothesis that one can
    take an existing, near-state-of-the-art, supervised NLP system and improve it
    using word embeddings. You can either create your own unsupervised word embeddings
    using the fastText cbow/skipgram approach as shown in [Chapter 2](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755), *Creating
    Models Using FastText Command Line*, or you can download them from the `fasttext.cc`
    website.
  prefs: []
  type: TYPE_NORMAL
- en: A question that may arise is whether certain word representations are better
    for certain tasks. Present research on some specific areas shows that word representations
    that work well in some tasks do not work well in others. An example that is generally
    given is that word representations that work in named entity recognition tasks
    do not work well when the problem domain is search query classification and vice
    versa.
  prefs: []
  type: TYPE_NORMAL
- en: fastText model quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the efforts of the Facebook AI Research team, there is a way to get vastly
    smaller models (in terms of the size that they take up in the hard drive), as
    you have seen in the *M**odel quantization* section in [Chapter 2](part0036.html#12AK80-05950c18a75943d0a581d9ddc51f2755),* Creating
    Models Using FastText Command Line*. Models which take up hundreds of MBs can
    be quantized to only a couple of MBs. For example, if you see the DBpedia model
    released by Facebook, which can be accessed at the web page[ https://fasttext.cc/docs/en/supervised-models.html](https://fasttext.cc/docs/en/supervised-models.html),
    notice that the regular model (this is the BIN file) is of 427 MB while the smaller
    model (the FTZ file) is only 1.7 MB.
  prefs: []
  type: TYPE_NORMAL
- en: This reduction in size is achieved by throwing out some of the information that
    is encoded in the BIN files (or the bigger model). The problem that needs to be
    solved here is how to keep information that is important and how to identify information
    that is not that important so that the overall accuracy and performance of the
    model is not compromised by a significant margin. In this section, we will take
    a look at the considerations for that.
  prefs: []
  type: TYPE_NORMAL
- en: Compression techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we are interested in understanding how to go about compressing the big
    fastText model files, let's take a look at some of the compression techniques
    that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different compression techniques can be categorized as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lossless compression**: As the name suggests, lossless compression techniques
    are those techniques that will reproduce the same information structure when you
    compress, then uncompress. There is no loss in information. This type of compression
    is mostly done using statistical modeling. You have already encountered an algorithm
    that is used in this type of compression, namely Huffman coding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lossy compression**: Lossy compression is about discarding as much data as
    possible without discarding the benefits or usefulness of the data as much as
    possible. This is a good technique when we are not interested in recreating the
    original data, but more on what the original data represents. As you can correctly
    infer, you will generally be able to get a higher level of compression using lossy
    compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastText employs a type of lossy compression known as product quantization.
    In the following sections, we will try to understand what quantization is, then
    how the idea of vector quantization comes from that and how it implements compression.
    Then we will take a look at how product quantization is a better variant of vector
    quantization for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of quantization has been taken from the world of **digital signal
    processing** (**DSP**). In DSP, when converting an analog signal (for example,
    sound waves) into a digital signal, the signal is broken down to a series of individual
    samples based on a bit depth that determines the number of levels that the quantized
    signal will have. In the case of 8-bit quantization, you will have 2⁸=256 possible
    combinations for the amplitude signal, and similarly, in 16-bit quantization,
    you will have 2^(16)=65536 possible combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, a sine wave is quantized to 3-bit levels and hence
    it will support 8 values for the continuous sine wave. You can see the code to
    get the following image in the `product quantization.ipynb` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Vector quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vector quantization is a technique to apply the ideas of quantization to vector
    spaces. Let''s say that you have a vector space *ℜ[k]* and a training set consisting
    of *N* samples of *k*-dimensional vectors on the target vector space. Vector quantization
    is the process where you map your vectors into a finite set of vectors Y, which
    are part of the target vector space. Each vector *y[i]* is called a **code vector**
    or a **codeword** and the set of all *codewords* is called a **codebook**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The amount of compression that is achieved is dependent on the size of the codebook.
    If we have a codebook of size *k* and the input vector is of dimension *L*, we
    need to specify ![](img/00067.jpeg) bits to specify which of the codewords are
    selected from the codebook. Hence, the rate for an L-dimensional vector quantizer
    with a code book of size k is ![](img/00068.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand how that happens in more detail. If you have a vector of
    *L* dimension where *L=8*, it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now there are 8 numbers and hence you need 3 bits to put it in memory in case
    you choose to encode in binary. Converting the array into binary you get the following
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So you need 24 bits to save the previous array in memory if you want to save
    each number as 3 bits. What if you want to reduce the amount of memory consumption
    of each number by 1 bit? If you are able to achieve that, you will be able to
    use only 16 bits to save the previous array in memory and thus save 8 bits, achieving
    compression. To do that, you can consider the numbers 0, 2, 4, 6 as your codebook
    which will maps to the vectors 00, 01, 10, 11:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'During the transformation, all numbers between 0 and 2 are mapped to 00, everything
    from 2 to 4 is mapped to 01, and so on. Hence your original vector gets changed
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The amount of space that this vector occupies in memory is only 8 bits.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can also target to encode our representation to 1 bit. In that
    case, only 2 bits of overall memory would be used for the array. But we lose more
    information about the original distribution. Hence, generalizing this understanding,
    decreasing the size of the codebook increases the compression ratio, but distortion
    of the original data increases.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the codebook for high-dimensional spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The principal goal while designing for vector quantizers is to find a codebook,
    specifying a decoder, and a rule for specifying the encoder, such that the overall
    performance of the vector space is optimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important class of quantizers is the Voronoi or nearest-neighbor quantizers.
    Given a set of L codevectors ![](img/00073.jpeg)  of size N, along with a distance
    measure ![](img/00074.jpeg), the R^k space is partitioned into L disjoint regions,
    known as Voronoi regions, with each codevector associated with each region. A
    particular Voronoi region Ω[j] associated with the codevector *v[j]* contains
    all the points in *R^k* space nearer to *v[j]* than any other codevector and is
    the nearest-neighbor locus region of *v[j]*. The following is an example Voronoi
    diagram for a given space with the associated codevectors denoted by points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The code for getting the above diagram can also be found in the Jupyter notebook
    for the repo: [https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb](https://github.com/PacktPublishing/Learn-fastText/blob/master/chapter4/product%20quantization.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, essentially, you can see that there are two steps to the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constructing the Voronoi space**: A preprocessing phase of constructing the
    Voronoi space, representing it in the form of a graph data structure and associating
    with each facet (for instance, vertex, edge, and region), the set of closest codevectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Finding the codevector**: Given the Voronoi subdivisions, determine the facet
    of the subdivision which contains the query vector, and the associate code vector
    is the desired nearest neighbor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Product quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Till now, you might have understood that in vector quantization, you cluster
    the search space into a number of bins based on the distance to the cluster centroid.
    If a query vector is quantized to that bin, all other vectors in the bin are good
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, if a query lies at the edge, one has to consider all the neighboring
    bins as well. This might not seem a big deal until you realize that number of
    neighbors adjacent to each Voronoi cell increases exponentially with respect to
    the dimension *N* of the space. Note that when creating fastText vectors, we routinely
    deal in high dimensions such as 100, 300, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default vectors in fastText are 100-dimensional vectors. A quantizer working
    on 50-bit codes, which means we only have a half (0.5) bit per component, contains ![](img/00076.jpeg)
    centroids (around 150 TB). Product quantization is an efficient solution to address
    this issue. The input vector *x[i]* is divided into m distinct subvectors j ![](img/00077.jpeg)
    of dimension *D** = *D/m*, where *D* is a multiple of *m*. The subvectors are
    quantized separately using m distinct quantizers. A given vector ν is therefore
    mapped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: _![](img/00078.jpeg)
  prefs: []
  type: TYPE_NORMAL
- en: Here, *q[j]* is a low-complexity quantizer associated with the *j*th subvector.
    With the subquantizer *q[j]*, we associate the index *Ι[j]*, the codebook *C[j]*,
    and the corresponding reproduction values *c[j,i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A reproduction value of the **product quantizer** (**PQ**) is identified by
    an element of the product index set ![](img/00079.jpeg). The codebook is therefore
    defined as the Cartesian product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A centroid of this set is the concatenation of the centroids of the m sub-quantizers.
    From now on, we assume that all sub-quantizers have the same finite number *k**
    of reproduction values. In that case, the total number of centroids is given by
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In fastText, the two parameters involved in product quantization, namely the
    number of sub-quantizers m and the number of bits b per quantization index, are
    typically set to ![](img/00082.jpeg), and *b*=8.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a PQ can generate an exponentially large codebook at very low memory/time
    cost. The essence of a PQ is to decompose the high-dimensional vector space into
    the Cartesian product of the subspaces and then quantize these subspaces separately.
    The optimal space decomposition is important for good product quantization implementation,
    and as per current knowledge, this is generally done by minimizing quantization
    distortions with respect to the space decomposition and quantization codebooks.
    There are two known ways to solve this optimization problem. One is using iterative
    quantization. A simple example of iteratively finding the codevectors is shown
    in the notebook, which can be considered a specific subquantizer. If you are interested,
    you can take a look at *Optimised Product Quantization* by Kaiming He et al.
  prefs: []
  type: TYPE_NORMAL
- en: The other method, and one which fastText adopts, is having a Gaussian assumption
    for the input vectors and finding the k-means through expectation maximization.
    Take a look at the algorithm in this k-means function: [src/productquantizer.cc#L115.](https://github.com/facebookresearch/fastText/blob/d647be03243d2b83d0b4659a9dbfb01e1d1e1bf7/src/productquantizer.cc#L115)
  prefs: []
  type: TYPE_NORMAL
- en: After quantizing on the input matrix, retraining is done on the output matrix,
    keeping the input matrix the same. This is done so that the network readjusts
    to the quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Additional steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are the additional steps that can be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection and pruning**: Pruning is done on those features that do
    not have a big influence on the decision of the classifier. During the classification
    step, only a limited number of *K* words and n-grams are selected. So for each
    document, first verification is done if it is covered by a retrained feature and,
    if not, we add the feature with the highest norm to the set of retrained features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hashing**: Both the words and n-grams are also hashed to further save on
    the memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you decide to implement these techniques discussed in your own model compression
    methods, there are various ideas here that you can tweak and see if you get better
    performance in your particular domain:'
  prefs: []
  type: TYPE_NORMAL
- en: You can explore whether some other distance metrics makes sense for finding
    the k-means.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can change the pruning strategy based on neighborhood, entropy, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this chapter, you have completed a deep dive into the theory behind how
    the fastText model is designed and implemented, the benefits, and the things that
    you need to consider while implementing it in your ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The next part of the book is about implementation and deployment and we start
    with how to use fastText in a Python environment in the next chapter.
  prefs: []
  type: TYPE_NORMAL
