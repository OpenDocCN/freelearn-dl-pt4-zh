<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">AI Game Playing</h1>
            </header>

            <article>
                
<p>In previous chapters, we looked at supervised learning techniques such as regression and classification, and unsupervised learning techniques such as GANs, autoencoders and generative models. In the case of supervised learning, we train the network with the expected input and output and expect it to predict the output given a new input. In the case of unsupervised learning, we show the network some input and expect it to learn the structure of the data so that it can apply this knowledge to a new input.</p>
<p>In this chapter, we will learn about reinforcement learning, or more specifically deep reinforcement learning, that is, the application of deep neural networks to reinforcement learning. Reinforcement learning has its roots in behavioral psychology. An agent is trained by rewarding it for correct behavior and punishing it for incorrect behavior. In the context of deep reinforcement learning, a network is shown some input and is given a positive or negative reward based on whether it produces the correct output from that input. Thus, in reinforcement learning, we have sparse and time-delayed labels. Over many iterations, the network learns to produce the correct output.</p>
<p>The pioneer in the deep reinforcement learning space was a small British company called DeepMind, which in 2013 published a paper (for more information refer to: <span><em>Playing Atari with Deep Reinforcement Learning</em>, by V. Mnih, arXiv:1312.5602, 2013.</span>) describing how a <strong>convolutional neural network</strong> (<strong>CNN</strong>) could be taught to play Atari 2600 video games by showing it screen pixels and giving it a reward when the score increases. The same architecture was used to learn seven different Atari 2600 games, in six of which the model outperformed all previous approaches, and it outperformed a human expert in three.</p>
<p>Unlike the learning strategies we learned about previously, where each network learns about a single discipline, reinforcement learning seems to be a general learning algorithm that can be applied to a variety of environments; it may even be the first step to general artificial intelligence. DeepMind has since been acquired by Google, and the group has been on the forefront of AI research. A subsequent paper (<span>for more information refer to: <em>Human-Level Control through Deep Reinforcement Learning</em>, by V. Mnih, Nature 518.7540, 2015: 529-533.</span>) was featured in the prestigious Nature journal in 2015, where they applied the same model to 49 different games.</p>
<p>In this chapter, we will explore the theoretical framework that underlies deep reinforcement learning. We'll then apply this framework to build a network using Keras that learns to play a game of catch. We'll briefly look at some ideas that can make this network better as well as some promising new areas of research in this space.</p>
<p>To sum up, we will learn the following core concepts around reinforcement learning in this chapter:</p>
<ul>
<li>Q-learning</li>
<li>Exploration versus exploitation</li>
<li>Experience replay</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reinforcement learning</h1>
            </header>

            <article>
                
<p>Our objective is to build a neural network to play the game of catch. Each game starts with a ball being dropped from a random position from the top of the screen. The objective is to move a paddle at the bottom of the screen using the left and right arrow keys to catch the ball by the time it reaches the bottom. As games go, this is quite simple. At any point in time, the state of this game is given by the <em>(x, y)</em> coordinates of the ball and paddle. Most arcade games tend to have many more moving parts, so a general solution is to provide the entire current game screen image as the state. The following screenshot shows four consecutive screenshots of our catch game:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/catch-frames-1.png"/></div>
<p>Astute readers might note that our problem could be modeled as a classification problem, where the input to the network are the game screen images and the output is one of three actions--move left, stay, or move right. However, this would require us to provide the network with training examples, possibly from recordings of games played by experts. An alternative and simpler approach might be to build a network and have it play the game repeatedly, giving it feedback based on whether it succeeds in catching the ball or not. This approach is also more intuitive and is closer to the way humans and animals learn.</p>
<p>The most common way to represent such a problem is through a <strong>markov decision process</strong> (<strong>MDP</strong>). Our game is the environment within which the agent is trying to learn. The state of the environment at time step <em>t</em> is given by <em>s<sub>t</sub></em> (and contains the location of the ball and paddle). The agent can perform certain actions (such as moving the paddle left or right). These actions can sometimes result in a reward <em>r<sub>t</sub></em>, which can be positive or negative (such as an increase or decrease in the score). Actions change the environment and can lead to a new state <em>s<sub>t+1</sub></em>, where the agent can perform another action <em>a<sub>t+1</sub></em>, and so on. The set of states, actions and rewards, together with the rules for transitioning from one state to the other, make up a markov decision process. A single game is one episode of this process, and is represented by a finite sequence of states, actions, and rewards:</p>
<div class="CDPAlignCenter CDPAlign"><img height="14" src="assets/episode.png" width="367"/></div>
<p>Since, this is a markov decision process, the probability of state <em>s<sub>t+1</sub></em> depends only on current state <em>s<sub>t</sub></em> and action <em>a<sub>t</sub></em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Maximizing future rewards</h1>
            </header>

            <article>
                
<p>As an agent, our objective is to maximize the total reward from each game. The total reward can be represented as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/total_reward.png"/></div>
<p>In order to maximize the total reward, the agent should try to maximize the total reward from any time point <em>t</em> in the game. The total reward at time step <em>t</em> is given by <em>R<sub>t</sub></em> and is represented as:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/reward_at_t.png"/></div>
<p>However, it is harder to predict the value of the rewards the further we go into the future. In order to take this into consideration, our agent should try to maximize the total discounted future reward at time <em>t</em> instead. This is done by discounting the reward at each future time step by a factor γ over the previous time step. If γ is <em>0</em>, then our network does not consider future rewards at all, and if γ is <em>1</em>, then our network is completely deterministic. A good value for γ is around <em>0.9</em>. Factoring the equation allows us to express the total discounted future reward at a given time step recursively as the sum of the current reward and the total discounted future reward at the next time step:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/reward_recursive.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Q-learning</h1>
            </header>

            <article>
                
<p>Deep reinforcement learning utilizes a model-free reinforcement learning technique called <strong>Q-learning</strong>. Q-learning can be used to find an optimal action for any given state in a finite markov decision process. Q-learning tries to maximize the value of the Q-function which represents the maximum discounted future reward when we perform action <em>a</em> in state <em>s</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/qfunc.png"/></div>
<p>Once we know the Q-function, the optimal action <em>a</em> at a state <em>s</em> is the one with the highest Q-value. We can then define a policy <em>Ï€(s)</em> that gives us the optimal action at any state:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/qpolicy.png"/></div>
<p>We can define the Q-function for a transition point (<em>s<sub>t</sub></em>, <em>a<sub>t</sub></em>, <em>r<sub>t</sub></em>, <em>s<sub>t+1</sub></em>) in terms of the Q-function at the next point (<em>s<sub>t+1</sub></em>, <em>a<sub>t+1</sub></em>, <em>r<sub>t+1</sub></em>, <em>s<sub>t+2</sub></em>) similar to how we did with the total discounted future reward. This equation is known as the <strong>Bellman equation</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bellman.png"/></div>
<p>The Q-function can be approximated using the Bellman equation. You can think of the Q-function as a lookup table (called a <strong>Q-table</strong>) where the states (denoted by <em>s</em>) are rows and actions (denoted by <em>a</em>) are columns, and the elements (denoted by <em>Q(s, a)</em>) are the rewards that you get if you are in the state given by the row and take the action given by the column. The best action to take at any state is the one with the highest reward. We start by randomly initializing the Q-table, then carry out random actions and observe the rewards to update the Q-table iteratively according to the following algorithm:</p>
<pre>
initialize Q-table Q<br/>observe initial state s<br/>repeat<br/>   select and carry out action a<br/>   observe reward r and move to new state s'<br/>   Q(s, a) = Q(s, a) + α(r + γ max_a' Q(s', a') - Q(s, a))<br/>   s = s'<br/>until game over
</pre>
<p>You will realize that the algorithm is basically doing stochastic gradient descent on the Bellman equation, backpropagating the reward through the state space (or episode) and averaging over many trials (or epochs). Here α is the learning rate that determines how much of the difference between the previous Q-value and the discounted new maximum Q-value should be incorporated.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The deep Q-network as a Q-function</h1>
            </header>

            <article>
                
<p>We know that our Q-function is going to be a neural network, the natural question is: what kind? For our simple example game, each state is represented by four consecutive black and white screen images of size <em>(80, 80)</em>, so the total number of possible states (and the number of rows of our Q-table) is <em>2<sup>80x80x4</sup></em>. Fortunately, many of these states represent impossible or highly improbable pixel combinations. Since convolutional neural networks have local connectivity (that is, each neuron is connected to only a local region of its input), it avoids these impossible or improbable pixel combinations. In addition, neural networks are generally very good at coming up with good features for structured data such as images. Hence a CNN can be used to model a Q-function very effectively.</p>
<p>The DeepMind paper (for more information refer to: <span><em>Playing Atari with Deep Reinforcement Learning</em>, by V. Mnih, arXiv:1312.5602, 2013.</span>), also uses three layers of convolutions followed by two fully connected layers. Unlike traditional CNNs used for image classification or recognition, there are no pooling layers. This is because pooling layers makes the network less sensitive to the location of specific objects in the image. In case of games this information is likely to be required to compute the reward, and thus cannot be discarded.</p>
<p>The following diagram, shows the structure of the deep Q-network that is used for our example. It follows the same structure as the original DeepMind paper except for the input and output layer shapes. The shape for each of our inputs is <em>(80, 80, 4)</em>: four black and white consecutive screenshots of the game console, each <em>80</em> x <em>80</em> pixels in size. Our output shape is (<em>3</em>), corresponding to the Q-value for each of three possible actions (move left, stay, move right):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/deep-q-cnn.png"/></div>
<p>Since our output are the three Q-values, this is a regression task, and we can optimize this by minimizing the difference of the squared error between the current value of <em>Q(s, a)</em> and its computed value in terms of the sum of the reward and the discounted Q-value <em>Q(s', a')</em> one step into the future. The current value is already known at the beginning of the iteration and the future value is computed based on the reward returned by the environment:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/loss_function.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Balancing exploration with exploitation</h1>
            </header>

            <article>
                
<p>Deep reinforcement learning is an example of online learning, where the training and prediction steps are interspersed. Unlike batch learning techniques where the best predictor is generated by learning on the entire training data, a predictor trained with online learning is continuously improving as it trains on new data.</p>
<p>Thus in the initial epochs of training, a deep Q-network gives random predictions which can give rise to poor Q-learning performance. To alleviate this, we can use a simple exploration method such as &amp;epsi;-greedy. In case of &amp;epsi;-greedy exploration, the agent chooses the action suggested by the network with probability <em>1-&amp;epsi;</em> or an action uniformly at random otherwise. That is why this strategy is called exploration/exploitation.</p>
<p>As the number of epochs increases and the Q-function converges, it begins to return more consistent Q-values. The value of &amp;epsi; can be attenuated to account for this, so as the network begins to return more consistent predictions, the agent chooses to exploit the values returned by the network over choosing random actions. In case of DeepMind, the value of &amp;epsi; decreases over time from <em>1</em> to <em>0.1</em>, and in our example it decreases from <em>0.1</em> to <em>0.001</em>.</p>
<p>Thus, &amp;epsi;-greedy exploration ensures that in the beginning the system balances the unreliable predictions made from the Q-network with completely random moves to explore the state space, and then settles down to less aggressive exploration (and more aggressive exploitation) as the predictions made by the Q-network improve.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Experience replay, or the value of experience</h1>
            </header>

            <article>
                
<p>Based on the equations that represent the Q-value for a state action pair <em>(s<sub>t</sub>, a<sub>t</sub>)</em> in terms of the current reward <em>r<sub>t</sub></em> and the discounted maximum Q-value for the next time step <em>(s<sub>t+1</sub>, a<sub>t+1</sub>)</em>, our strategy would logically be to train the network to predict the best next state <em>s'</em> given the current state <em>(s, a, r)</em>. It turns out that this tends to drive the network into a local minimum. The reason for this is that consecutive training samples tend to be very similar.</p>
<p>To counter this, during game play, we collect all the previous moves <em>(s, a, r, s')</em> into a large fixed size queue called the <strong>replay memory</strong>. The replay memory represents the experience of the network. When training the network, we generate random batches from the replay memory instead of the most recent (batch of) transactions. Since the batches are composed of random experience tuples <em>(s, a, r, s')</em> that are out of order, the network trains better and avoids getting stuck in local minima.</p>
<p>Experiences could be collected from human gameplay as well instead of (or in addition to) from previous moves during game play by the network. Yet another approach is to collect experiences by running the network in <em>observation</em> mode for a while in the beginning, when it generates completely random actions (<img height="11" src="assets/B06258_08_13-1.png" width="9"/><em> = 1</em>) and extracts the reward and next state from the game and collects them into its experience replay queue.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Example - Keras deep Q-network for catch</h1>
            </header>

            <article>
                
<p>The objective of our game is to catch a ball released from a random location from the top of the screen with a paddle at the bottom of the screen by moving the paddle horizontally using the left and right arrow keys. The player wins if the paddle can catch the ball and loses if the balls falls off the screen before the paddle gets to it. The game has the advantage of being very simple to understand and build, and is modeled after the game of catch described by Eder Santana in his blog post (for more information refer to: <span><em>Keras Plays Catch, a Single File Reinforcement Learning Example</em></span>, by Eder Santana, <span>2017.</span>) on deep reinforcement learning. We built the original game using Pygame (<a href="https://www.pygame.org/news">https://www.pygame.org/news</a>), a free and open source library for building games. This game allows the player to move the paddle using the left and right arrow keys. The game is available as <kbd>game.py</kbd> in the code bundle for this chapter in case you want to get a feel for it.</p>
<div class="packt_infobox"><strong>Installing Pygame</strong>:<br/>
Pygame runs on top of Python, and is available for Linux (various flavors), macOS, Windows, as well as some phone operating systems such as Android and Nokia. The full list of distributions can be found at: <a href="http://www.pygame.org/download.shtml" target="_blank">http://www.pygame.org/download.shtml</a>. Pre-built versions are available for 32-bit and 64-bit versions of Linux and Windows and 64-bit version of macOS. On these platforms, you can install Pygame with <kbd>pip install pygame</kbd> command.<br/>
If a pre-built version does not exist for your platform, you can also build it from source using instructions available at: <a href="http://www.pygame.org/wiki/GettingStarted" target="_blank">http://www.pygame.org/wiki/GettingStarted</a>.<br/>
Anaconda users can find pre-built Pygame versions on the conda-forge:<br/>
<kbd>conda install binstar</kbd><br/>
<kbd>conda install anaconda-client</kbd><br/>
<kbd>conda install -c https://conda.binstar.org/tlatorre pygame # Linux</kbd><br/>
<kbd>conda install -c https://conda.binstar.org/quasiben pygame # Mac</kbd></div>
<p>In order to train our neural network, we need to make some changes to the original game so the network can play instead of the human player. We want to wrap the game to allow the network to communicate with it via an API instead of the keyboard left and right arrow keys. Let us look at the code for this wrapped game.</p>
<p>As usual, we start with the imports:</p>
<pre>
from __future__ import division, print_function<br/>import collections<br/>import numpy as np<br/>import pygame<br/>import random<br/>import os
</pre>
<p>We define our class. Our constructor can optionally set the wrapped version of the game to run in <em>headless</em> mode, that is, without needing to display a Pygame screen. This is useful where you have to run on a GPU box in the cloud and only have access to a text based terminal. You can comment this line out if you are running the wrapped game locally where you have access to a graphics terminal. Next we call the <kbd>pygame.init()</kbd> method to initialize all Pygame components. Finally, we set a bunch of class level constants:</p>
<pre>
class MyWrappedGame(object):<br/><br/>    def __init__(self):<br/>        # run pygame in headless mode<br/>        os.environ["SDL_VIDEODRIVER"] = "dummy"<br/><br/>        pygame.init()<br/><br/>        # set constants<br/>        self.COLOR_WHITE = (255, 255, 255)<br/>        self.COLOR_BLACK = (0, 0, 0)<br/>        self.GAME_WIDTH = 400<br/>        self.GAME_HEIGHT = 400<br/>        self.BALL_WIDTH = 20<br/>        self.BALL_HEIGHT = 20<br/>        self.PADDLE_WIDTH = 50<br/>        self.PADDLE_HEIGHT = 10<br/>        self.GAME_FLOOR = 350<br/>        self.GAME_CEILING = 10<br/>        self.BALL_VELOCITY = 10<br/>        self.PADDLE_VELOCITY = 20<br/>        self.FONT_SIZE = 30<br/>        self.MAX_TRIES_PER_GAME = 1<br/>        self.CUSTOM_EVENT = pygame.USEREVENT + 1<br/>        self.font = pygame.font.SysFont("Comic Sans MS", self.FONT_SIZE)
</pre>
<p>The <kbd>reset()</kbd> method defines the operations that need to be called at the start of each game, such as clearing out the state queue, setting the ball, and paddle to their starting positions, initializing the scores, and so on:</p>
<pre>
    def reset(self):<br/>        self.frames = collections.deque(maxlen=4)<br/>        self.game_over = False<br/>        # initialize positions<br/>        self.paddle_x = self.GAME_WIDTH // 2<br/>        self.game_score = 0<br/>        self.reward = 0<br/>        self.ball_x = random.randint(0, self.GAME_WIDTH)<br/>        self.ball_y = self.GAME_CEILING<br/>        self.num_tries = 0<br/><br/>        # set up display, clock, etc<br/>        self.screen = pygame.display.set_mode((self.GAME_WIDTH, self.GAME_HEIGHT))<br/>        self.clock = pygame.time.Clock()
</pre>
<p>In the original game, there is a Pygame event queue into which the left and right arrow key events raised by the player as he moves the paddle, as well as internal events raised by Pygame components are written to. The central part of the game code is basically a loop (called the <strong>event loop</strong>), that reads the event queue and reacts to it.</p>
<p>In the wrapped version, we have moved the event loop to the caller. The <kbd>step()</kbd> method describes what happens in a single pass in the loop. The method takes an integer <kbd>0</kbd>, <kbd>1</kbd>, or <kbd>2</kbd> representing an action (respectively move left, stay, and move right), and then it sets variables that control the position of the ball and paddle at this time step. The <kbd>PADDLE_VELOCITY</kbd> variable represents a <em>speed</em> that moves the paddle that many pixels to the left or right when the move left and move right actions are sent. If the ball has dropped past the paddle, it checks whether there is a collision. If there is, the paddle <em>catches</em> the ball and the player (the neural network) wins, otherwise the player loses. The method then redraws the screen and appends it to the fixed length <kbd>deque</kbd> that contains the last four frames of the game screen. Finally, it returns the state (given by the last four frames), the reward for the current action and a flag that tells the caller if the game is over:</p>
<pre>
    def step(self, action):<br/>        pygame.event.pump()<br/><br/>        if action == 0: # move paddle left<br/>            self.paddle_x -= self.PADDLE_VELOCITY<br/>            if self.paddle_x &lt; 0:<br/>                # bounce off the wall, go right<br/>                self.paddle_x = self.PADDLE_VELOCITY<br/>        elif action == 2: # move paddle right<br/>            self.paddle_x += self.PADDLE_VELOCITY<br/>            if self.paddle_x &gt; self.GAME_WIDTH - self.PADDLE_WIDTH:<br/>                # bounce off the wall, go left<br/>                self.paddle_x = self.GAME_WIDTH - self.PADDLE_WIDTH - self.PADDLE_VELOCITY<br/>        else: # don't move paddle<br/>            pass<br/><br/>        self.screen.fill(self.COLOR_BLACK)<br/>        score_text = self.font.render("Score: {:d}/{:d}, Ball: {:d}"<br/>            .format(self.game_score, self.MAX_TRIES_PER_GAME,<br/>                    self.num_tries), True, self.COLOR_WHITE)<br/>        self.screen.blit(score_text, <br/>            ((self.GAME_WIDTH - score_text.get_width()) // 2,<br/>            (self.GAME_FLOOR + self.FONT_SIZE // 2)))<br/><br/>        # update ball position<br/>        self.ball_y += self.BALL_VELOCITY<br/>        ball = pygame.draw.rect(self.screen, self.COLOR_WHITE,<br/>            pygame.Rect(self.ball_x, self.ball_y, self.BALL_WIDTH, <br/>            self.BALL_HEIGHT))<br/>        # update paddle position<br/>        paddle = pygame.draw.rect(self.screen, self.COLOR_WHITE,<br/>            pygame.Rect(self.paddle_x, self.GAME_FLOOR, <br/>                        self.PADDLE_WIDTH, self.PADDLE_HEIGHT))<br/><br/>        # check for collision and update reward<br/>        self.reward = 0<br/>        if self.ball_y &gt;= self.GAME_FLOOR - self.BALL_WIDTH // 2:<br/>            if ball.colliderect(paddle):<br/>                self.reward = 1<br/>            else:<br/>                self.reward = -1<br/><br/>        self.game_score += self.reward<br/>        self.ball_x = random.randint(0, self.GAME_WIDTH)<br/>        self.ball_y = self.GAME_CEILING<br/>        self.num_tries += 1<br/><br/>        pygame.display.flip()<br/><br/>        # save last 4 frames<br/>        self.frames.append(pygame.surfarray.array2d(self.screen))<br/><br/>        if self.num_tries &gt;= self.MAX_TRIES_PER_GAME:<br/>            self.game_over = True<br/><br/>        self.clock.tick(30)<br/>        return np.array(list(self.frames)), self.reward, self.game_over
</pre>
<p>We will look at the code to train our network to play the game.</p>
<p>As usual, first we import the libraries and objects that we need. In addition to third-party components from Keras and SciPy, we also import the <kbd>wrapped_game</kbd> class we described previously:</p>
<pre>
from __future__ import division, print_function<br/>from keras.models import Sequential<br/>from keras.layers.core import Activation, Dense, Flatten<br/>from keras.layers.convolutional import Conv2D<br/>from keras.optimizers import Adam<br/>from scipy.misc import imresize<br/>import collections<br/>import numpy as np<br/>import os<br/><br/>import wrapped_game
</pre>
<p>We define two convenience functions. The first converts the set of four input images to a form suitable for use by the network. The input comes in a set of four 800 x 800 images, so the shape of the input is <em>(4, 800, 800)</em>. However, the network expects its input as a four-dimensional tensor of shape <em>(batch size, 80, 80, 4)</em>. At the very beginning of the game, we don't have four frames, so we fake it by stacking the first frame four times. The shape of the output tensor returned from this function is <em>(80, 80, 4)</em>.</p>
<p>The <kbd>get_next_batch()</kbd> function samples <kbd>batch_size</kbd> state tuples from the experience replay queue, and gets the reward and predicted next state from the neural network. It then calculates the value of the Q-function at the next time step and returns it:</p>
<pre>
def preprocess_images(images):<br/>    if images.shape[0] &lt; 4:<br/>        # single image<br/>        x_t = images[0]<br/>        x_t = imresize(x_t, (80, 80))<br/>        x_t = x_t.astype("float")<br/>        x_t /= 255.0<br/>        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)<br/>    else:<br/>        # 4 images<br/>        xt_list = []<br/>        for i in range(images.shape[0]):<br/>            x_t = imresize(images[i], (80, 80))<br/>            x_t = x_t.astype("float")<br/>            x_t /= 255.0<br/>            xt_list.append(x_t)<br/>        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), <br/>                       axis=2)<br/>    s_t = np.expand_dims(s_t, axis=0)<br/>    return s_t<br/><br/><br/>def get_next_batch(experience, model, num_actions, gamma, batch_size):<br/>    batch_indices = np.random.randint(low=0, high=len(experience), <br/>        size=batch_size)<br/>    batch = [experience[i] for i in batch_indices]<br/>    X = np.zeros((batch_size, 80, 80, 4))<br/>    Y = np.zeros((batch_size, num_actions))<br/>    for i in range(len(batch)):<br/>        s_t, a_t, r_t, s_tp1, game_over = batch[i]<br/>        X[i] = s_t<br/>        Y[i] = model.predict(s_t)[0]<br/>        Q_sa = np.max(model.predict(s_tp1)[0])<br/>        if game_over:<br/>            Y[i, a_t] = r_t<br/>        else:<br/>            Y[i, a_t] = r_t + gamma * Q_sa<br/>    return X, Y
</pre>
<p>We define our network. This is the network that models the Q-function for our game. Our network is very similar to the one proposed in the DeepMind paper. The only difference is the size of the input and the output. Our input shape is <em>(80, 80, 4)</em> while theirs was <em>(84, 84, 4)</em> and our output is <em>(3)</em> corresponding to the three actions for which the value of the Q-function needs to be computed, whereas their was <em>(18)</em>, corresponding to the actions possible from Atari.</p>
<p>There are three convolutional layers and two fully connected (dense) layers. All layers, except the last have the ReLU activation unit. Since we are predicting values of Q-functions, it is a regression network and the last layer has no activation unit:</p>
<pre>
# build the model<br/>model = Sequential()<br/>model.add(Conv2D(32, kernel_size=8, strides=4, <br/>                 kernel_initializer="normal", <br/>                 padding="same",<br/>                 input_shape=(80, 80, 4)))<br/>model.add(Activation("relu"))<br/>model.add(Conv2D(64, kernel_size=4, strides=2, <br/>                 kernel_initializer="normal", <br/>                 padding="same"))<br/>model.add(Activation("relu"))<br/>model.add(Conv2D(64, kernel_size=3, strides=1,<br/>                 kernel_initializer="normal",<br/>                 padding="same"))<br/>model.add(Activation("relu"))<br/>model.add(Flatten())<br/>model.add(Dense(512, kernel_initializer="normal"))<br/>model.add(Activation("relu"))<br/>model.add(Dense(3, kernel_initializer="normal"))
</pre>
<p>As we have described previously, our loss function is the squared difference between the current value of <em>Q(s, a)</em> and its computed value in terms of the sum of the reward and the discounted Q-value <em>Q(s', a')</em> one step into the future, so the mean squared error (MSE) loss function works very well. For the optimizer, we choose Adam, a good general-purpose optimizer, instantiated with a low learning rate:</p>
<pre>
model.compile(optimizer=Adam(lr=1e-6), loss="mse")
</pre>
<p>We define some constants for our training. The <kbd>NUM_ACTIONS</kbd> constant defines the number of output actions that the network can send to the game. In our case, these actions are <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd>, corresponding to move left, stay, and move right. The <kbd>GAMMA</kbd> value is the discount factor <img height="11" src="assets/image_01_042.jpg" width="9"/> for future rewards. The <kbd>INITIAL_EPSILON</kbd> and <kbd>FINAL_EPSILON</kbd> refer to starting and ending values for the <img height="11" src="assets/B06258_08_13-1.png" width="9"/> parameter in <img height="11" src="assets/B06258_08_13-1.png" width="9"/>-greedy exploration. The <kbd>MEMORY_SIZE</kbd> is the size of the experience replay queue. The <kbd>NUM_EPOCHS_OBSERVE</kbd> refer to the number of epochs where the network is allowed to explore the game by sending it completely random actions and seeing the rewards. The <kbd>NUM_EPOCHS_TRAIN</kbd> variable refers to the number of epochs the network will undergo online training. Each epoch corresponds to a single game or episode. The total number of games played for a training run is the sum of the <kbd>NUM_EPOCHS_OBSERVE</kbd> and <kbd>NUM_EPOCHS_TRAIN</kbd> values. The <kbd>BATCH_SIZE</kbd> is the size of the mini-batch that we will use for training:</p>
<pre>
# initialize parameters<br/>DATA_DIR = "../data"<br/>NUM_ACTIONS = 3 # number of valid actions (left, stay, right)<br/>GAMMA = 0.99 # decay rate of past observations<br/>INITIAL_EPSILON = 0.1 # starting value of epsilon<br/>FINAL_EPSILON = 0.0001 # final value of epsilon<br/>MEMORY_SIZE = 50000 # number of previous transitions to remember<br/>NUM_EPOCHS_OBSERVE = 100<br/>NUM_EPOCHS_TRAIN = 2000<br/><br/>BATCH_SIZE = 32<br/>NUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN
</pre>
<p>We instantiate the game and the experience replay queue. We also open up a log file and initialize some variables in preparation for training:</p>
<pre>
game = wrapped_game.MyWrappedGame()<br/>experience = collections.deque(maxlen=MEMORY_SIZE)<br/><br/>fout = open(os.path.join(DATA_DIR, "rl-network-results.tsv"), "wb")<br/>num_games, num_wins = 0, 0<br/>epsilon = INITIAL_EPSILON
</pre>
<p>Next up, we set up the loop that controls the number of epochs of training. As noted previously, each epoch corresponds to a single game, so we reset the game state at this point. A game corresponds to a single episode of a ball falling from the ceiling and either getting caught by the paddle or being missed. The loss is the squared difference between the predicted and actual Q-value for the game.</p>
<p>We start the game off by sending it a dummy action (in our case, a <em>stay</em>) and get back the initial state tuple for the game:</p>
<pre>
for e in range(NUM_EPOCHS):<br/>    game.reset() <br/>    loss = 0.0<br/><br/>    # get first state<br/>    a_0 = 1 # (0 = left, 1 = stay, 2 = right)<br/>    x_t, r_0, game_over = game.step(a_0) <br/>    s_t = preprocess_images(x_t)
</pre>
<p>The next block is the main loop of the game. This is the event loop in the original game that we moved to the calling code. We save the current state because we will need that for our experience replay queue, then decide what action signal to send the wrapped game. If we are in observation mode, we will just generate a random number corresponding to one of our actions, otherwise we will use <img height="11" src="assets/B06258_08_13-1.png" width="9"/>-greedy exploration to either select a random action or use our neural network (which we are also training) to predict the action we should send:</p>
<pre>
    while not game_over:<br/>        s_tm1 = s_t<br/><br/>        # next action<br/>        if e &lt;= NUM_EPOCHS_OBSERVE:<br/>            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]<br/>        else:<br/>            if np.random.rand() &lt;= epsilon:<br/>                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]<br/>            else:<br/>                q = model.predict(s_t)[0]<br/>                a_t = np.argmax(q)
</pre>
<p>Once we know our action, we send it to the game by calling <kbd>game.step()</kbd>, which returns the new state, the reward and a Boolean flag indicating the game is over. If the reward is positive (indicating that the ball was caught), we increment the number of wins, and we store this <em>(state, action, reward, new state, game over)</em> tuple in our experience replay queue:</p>
<pre>
        # apply action, get reward<br/>        x_t, r_t, game_over = game.step(a_t)<br/>        s_t = preprocess_images(x_t)<br/>        # if reward, increment num_wins<br/>        if r_t == 1:<br/>            num_wins += 1<br/>        # store experience<br/>        experience.append((s_tm1, a_t, r_t, s_t, game_over))
</pre>
<p>We then draw a random mini-batch from our experience replay queue and train our network. For each session of training, we compute the loss. The sum of the losses for all the trainings in each epoch is the loss for the entire epoch:</p>
<pre>
        if e &gt; NUM_EPOCHS_OBSERVE:<br/>            # finished observing, now start training<br/>            # get next batch<br/>            X, Y = get_next_batch(experience, model, NUM_ACTIONS, GAMMA, BATCH_SIZE)<br/>            loss += model.train_on_batch(X, Y)
</pre>
<p>When the network is relatively untrained, its predictions are not very good, so it makes sense to explore the state space more in an effort to reduce the chances of getting stuck in a local minima. However, as the network gets more and more trained, we reduce the value of <img height="11" src="assets/B06258_08_13-1.png" width="9"/> gradually so the model gets to predict more and more of the actions the network sends to the game:</p>
<pre>
    # reduce epsilon gradually<br/>    if epsilon &gt; FINAL_EPSILON:<br/>        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS
</pre>
<p>We write out a per epoch log both on console and into a log file for later analysis. After 100 epochs of training, we save the current state of the model so that we can recover in case we decide to stop training for any reason. We also save our final model so that we can use it to play our game later:</p>
<pre>
    print("Epoch {:04d}/{:d} | Loss {:.5f} | Win Count {:d}"<br/>        .format(e + 1, NUM_EPOCHS, loss, num_wins))<br/>    fout.write("{:04d}t{:.5f}t{:d}n".format(e + 1, loss, num_wins))<br/><br/>    if e % 100 == 0:<br/>        model.save(os.path.join(DATA_DIR, "rl-network.h5"), overwrite=True)<br/><br/>fout.close()<br/>model.save(os.path.join(DATA_DIR, "rl-network.h5"), overwrite=True)
</pre>
<p>We trained the game by making it observe 100 games, followed by playing 1,000, 2,000, and 5,000 games respectively. The last few lines of the log file for the 5,000 game run are shown next. As you can see, towards the end of the training, the network gets quite skilled at playing the game:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="379" src="assets/ss-8-1.png" width="301"/></div>
<p>The plot of loss and win count over epoch, shown in the following graph, also tells a similar story. While it does look like the loss could converge further with more training, it has gone down from <em>0.6</em> to around <em>0.1</em> in <em>5000</em> epochs of training. Similarly, the plot of the number of wins curve upward, showing that the network is learning faster as the number of epochs increases:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/loss_function.png"/></div>
<p>Finally, we evaluate the skill of our trained model by making it play a fixed number of games (100 in our case) and seeing how many it can win. Here is the code to do this. As previously, we start with our imports:</p>
<pre>
from __future__ import division, print_function<br/>from keras.models import load_model<br/>from keras.optimizers import Adam<br/>from scipy.misc import imresize<br/>import numpy as np<br/>import os<br/>import wrapped_game
</pre>
<p>We load up the model we had saved at the end of training and compile it. We also instantiate our <kbd>wrapped_game</kbd>:</p>
<pre>
DATA_DIR = "../data"<br/>model = load_model(os.path.join(DATA_DIR, "rl-network.h5"))<br/>model.compile(optimizer=Adam(lr=1e-6), loss="mse")<br/><br/>game = wrapped_game.MyWrappedGame()
</pre>
<p>We then loop over 100 games. We instantiate each game by calling its <kbd>reset()</kbd> method, and start it off. Then, for each game, until it is over, we call on the model to predict the action with the best Q-function. We report a running total of how many games it won.</p>
<p>We ran the test with each of our models. The first one that was trained for 1,000 games won 42 of 100 games, the one trained for 2,000 games won 74 of 100 games, and the one trained for 5,000 games won 87 of 100 games. This clearly shows that the network is improving with training:</p>
<pre>
num_games, num_wins = 0, 0<br/>for e in range(100):<br/>    game.reset()<br/><br/>    # get first state<br/>    a_0 = 1 # (0 = left, 1 = stay, 2 = right)<br/>    x_t, r_0, game_over = game.step(a_0) <br/>    s_t = preprocess_images(x_t)<br/><br/>    while not game_over:<br/>        s_tm1 = s_t<br/>        # next action<br/>        q = model.predict(s_t)[0]<br/>        a_t = np.argmax(q)<br/>        # apply action, get reward<br/>        x_t, r_t, game_over = game.step(a_t)<br/>        s_t = preprocess_images(x_t)<br/>        # if reward, increment num_wins<br/>        if r_t == 1:<br/>            num_wins += 1<br/><br/>    num_games += 1<br/>    print("Game: {:03d}, Wins: {:03d}".format(num_games, num_wins), end="r")<br/>print("")
</pre>
<p>If you run the evaluation code with the call to run it in headless mode commented out, you can watch the network playing the game and it's quite amazing to watch. Given that the Q-value predictions start off as random values and that it's mainly the sparse reward mechanism that provides the guidance to the network during training, it is almost unreasonable that the network learns to play the game this effectively. But as with other areas of deep learning, the network does in fact learn to play quite well.</p>
<p>The example presented previously is fairly simple, but it illustrates the process by which deep reinforcement learning models work, and hopefully has helped create a mental model using which you can approach more complex implementations. One implementation you might find interesting is Ben Lau's implementation of FlappyBird (for more information refer to:<span> <em>Using Keras and Deep Q-Network to Play FlappyBird</em>, by Ben Lau, 2016. and GitHub page: <a href="https://github.com/yanpanlau/Keras-FlappyBird" target="_blank">https://github.com/yanpanlau/Keras-FlappyBird</a></span>) using Keras. The Keras-RL project (<a href="https://github.com/matthiasplappert/keras-rl" target="_blank">https://github.com/matthiasplappert/keras-rl</a>), a Keras library for deep reinforcement learning, also has some very good examples.</p>
<p>Since the original proposal from DeepMind, there have been other improvements suggested, such as double Q-learning (for more information refer to: <span><em>Deep Reinforcement Learning with Double Q-Learning</em>, by H. Van Hasselt, A. Guez, and D. Silver, AAAI. 2016</span>), prioritized experience replay (<span>for more information refer to: <em>Prioritized Experience Replay</em>, by T. Schaul, arXiv:1511.05952, 2015</span>), and dueling network architectures (<span>for more information refer to: <em>Dueling Network Architectures for Deep Reinforcement Learning</em>, by Z. Wang, arXiv:1511.06581, 2015</span>). Double Q-learning uses two networks - the primary network chooses the action and the target network chooses the target Q-value for the action. This reduces possible overestimation of Q-values by the single network, and allows the network to train quicker and better. Prioritized experience replay increases the probability of sampling experience tuples with a higher expected learning progress. Dueling network architectures decompose the Q-function into state and action components and combine them back separately.</p>
<p>All of the code discussed in this section, including the base game that can be played by a human player, is available in the code bundle accompanying this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The road ahead</h1>
            </header>

            <article>
                
<p>In January 2016, DeepMind announced the release of AlphaGo (<span>for more information refer to: </span><span><em>Mastering the Game of Go with Deep Neural Networks and Tree Search</em>, by D. Silver, Nature 529.7587, pp. 484-489, 2016</span>), a neural network to play the game of Go. Go is regarded as a very challenging game for AIs to play, mainly because at any point in the game, there are an average of approximately <em>10<sup>170</sup></em> possible (<span>for more information refer to: <a href="http://ai-depot.com/LogicGames/Go-Complexity.html" target="_blank">http://ai-depot.com/LogicGames/Go-Complexity.html</a></span>) moves (compared with approximately <em>10<sup>50</sup></em> for chess). Hence determining the best move using brute force methods is computationally infeasible. At the time of publication, AlphaGo had already won 5-0 in a 5-game competition against the current European Go champion, Fan Hui. This was the first time that any computer program had defeated a human player at Go. Subsequently, in March 2016, AlphaGo won 4-1 against Lee Sedol, the world's second professional Go player.</p>
<p>There were several notable new ideas that went into AlphaGo. First, it was trained using a combination of supervised learning from human expert games and reinforcement learning by playing one copy of AlphaGo against another. You have seen applications of both these ideas in previous chapters.</p>
<p>Second, AlphaGo was composed of a value network and a policy network. During each move, AlphaGo uses Monte Carlo simulation, a process used to predict the probability of different outcomes in the future in the presence of random variables, to imagine many alternative games starting from the current position. The value network is used to reduce the depth of the tree search to estimate win/loss probability without having to compute all the way to the end of the game, sort of like an intuition about how good the move is. The policy network is used to reduce the breadth of the search by guiding the search towards actions that promise the maximum immediate reward (or Q-value). For a more detailed description, please refer to the blog post: <span><em>AlphaGo: Mastering the ancient game of Go with Machine Learning</em>, Google Research Blog, 2016.</span></p>
<p>While AlphaGo was a major improvement over the original DeepMind network, it was still playing a game where all the players can see all the game pieces, that is, they are still games of perfect information. In January, 2017, researchers at Carnegie Mellon University announced Libratus (for more information refer to: <span><em>AI Takes on Top Poker Players</em>, by T. Revel, New Scientist 223.3109, pp. 8, 2017</span>), an AI that plays Poker. Simultaneously, another group comprised of researchers from the University of Alberta, Charles University of Prague, and Czech Technical University (also from Prague), have proposed the DeepStack architecture (<span>for more information refer to: <em>DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker</em>, by M. Moravaa­k, arXiv:1701.01724, 2017</span>) to do the same thing. Poker is a game of imperfect information, since a player cannot see the opponent's cards. So, in addition to learning how to play the game, the Poker playing AI also needs to develop an intuition about the opponent's game play.</p>
<p>Rather than use a built-in strategy for its intuition, Libratus has an algorithm that computes this strategy by trying to achieve a balance between risk and reward, also known as the Nash equilibrium. From January 11, 2017 to January 31, 2017, Libratus was pitted against four top human Poker players (<span>for more information refer to: <em>Upping the Ante: Top Poker Pros Face Off vs. Artificial Intelligence</em>, Carnegie Mellon University, January 2017</span>), and beat them resoundingly.</p>
<p>DeepStack's intuition is trained using reinforcement learning, using examples generated from random Poker situations. It has played 33 professional Poker players from 17 countries and has a win rating that makes it an <em>order of magnitude</em> better than a good player rating (<span>for more information refer to: <em>The Uncanny Intuition of Deep Learning to Predict Human Behavior</em>, by C. E. Perez, Medium corporation, Intuition Machine, February 13, 2017</span>).</p>
<p>As you can see, these are very exciting times indeed. Advances that started with deep learning networks able to play arcade games have led to networks that can effectively read your mind, or at least anticipate (sometimes non-rational) human behavior and win at games of bluffing. The possibilities with deep learning seem to be just limitless.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we have learned the concepts behind reinforcement learning, and how it can be used to build deep learning networks with Keras that learn how to play arcade games based on reward feedback. From there, we moved on to briefly discuss advances in this field, such as networks that have been taught to play harder games such as Go and Poker at a superhuman level. While game playing might seem like a frivolous application, these ideas are the first step towards general artificial intelligence, where a network learns from experience rather than large amounts of training data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>