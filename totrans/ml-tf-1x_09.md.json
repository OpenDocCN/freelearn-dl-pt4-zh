["```py\n- data\n--VGG16.npz\n- samples_data\n- production\n- utils\n--__init__.py\n--debug_print.py\n- README.md\n```", "```py\n    import tensorflow as tf \n    import numpy as np \n\n    def inference(images): \n    with tf.name_scope(\"preprocess\"): \n        mean = tf.constant([123.68, 116.779, 103.939],  \n    dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean') \n        input_images = images - mean \n    conv1_1 = _conv2d(input_images, 3, 3, 64, 1, 1,   \n    name=\"conv1_1\") \n    conv1_2 = _conv2d(conv1_1, 3, 3, 64, 1, 1, name=\"conv1_2\") \n    pool1 = _max_pool(conv1_2, 2, 2, 2, 2, name=\"pool1\") \n\n    conv2_1 = _conv2d(pool1, 3, 3, 128, 1, 1, name=\"conv2_1\") \n    conv2_2 = _conv2d(conv2_1, 3, 3, 128, 1, 1, name=\"conv2_2\") \n    pool2 = _max_pool(conv2_2, 2, 2, 2, 2, name=\"pool2\") \n\n    conv3_1 = _conv2d(pool2, 3, 3, 256, 1, 1, name=\"conv3_1\") \n    conv3_2 = _conv2d(conv3_1, 3, 3, 256, 1, 1, name=\"conv3_2\") \n    conv3_3 = _conv2d(conv3_2, 3, 3, 256, 1, 1, name=\"conv3_3\") \n    pool3 = _max_pool(conv3_3, 2, 2, 2, 2, name=\"pool3\") \n\n    conv4_1 = _conv2d(pool3, 3, 3, 512, 1, 1, name=\"conv4_1\") \n    conv4_2 = _conv2d(conv4_1, 3, 3, 512, 1, 1, name=\"conv4_2\") \n    conv4_3 = _conv2d(conv4_2, 3, 3, 512, 1, 1, name=\"conv4_3\") \n    pool4 = _max_pool(conv4_3, 2, 2, 2, 2, name=\"pool4\") \n\n    conv5_1 = _conv2d(pool4, 3, 3, 512, 1, 1, name=\"conv5_1\") \n    conv5_2 = _conv2d(conv5_1, 3, 3, 512, 1, 1, name=\"conv5_2\") \n    conv5_3 = _conv2d(conv5_2, 3, 3, 512, 1, 1, name=\"conv5_3\") \n    pool5 = _max_pool(conv5_3, 2, 2, 2, 2, name=\"pool5\") \n\n    fc6 = _fully_connected(pool5, 4096, name=\"fc6\") \n    fc7 = _fully_connected(fc6, 4096, name=\"fc7\") \n    fc8 = _fully_connected(fc7, 1000, name='fc8', relu=False) \n    outputs = _softmax(fc8, name=\"output\") \n    return outputs \n```", "```py\n def _conv2d(input_data, k_h, k_w, c_o, s_h, s_w, name, relu=True,  \n padding=\"SAME\"): \n    c_i = input_data.get_shape()[-1].value \n    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1],  \n padding=padding) \n    with tf.variable_scope(name) as scope: \n        weights = tf.get_variable(name=\"kernel\", shape=[k_h, k_w,  \n c_i, c_o], \n\n initializer=tf.truncated_normal_initializer(stddev=1e-1,  \n dtype=tf.float32)) \n        conv = convolve(input_data, weights) \n        biases = tf.get_variable(name=\"bias\", shape=[c_o],  \n dtype=tf.float32, \n\n initializer=tf.constant_initializer(value=0.0)) \n        output = tf.nn.bias_add(conv, biases) \n        if relu: \n            output = tf.nn.relu(output, name=scope.name) \n        return output \n def _max_pool(input_data, k_h, k_w, s_h, s_w, name,  \n padding=\"SAME\"): \n    return tf.nn.max_pool(input_data, ksize=[1, k_h, k_w, 1], \n                          strides=[1, s_h, s_w, 1], padding=padding,  \n name=name) \n```", "```py\n def _fully_connected(input_data, num_output, name, relu=True): \n    with tf.variable_scope(name) as scope: \n        input_shape = input_data.get_shape() \n        if input_shape.ndims == 4: \n            dim = 1 \n            for d in input_shape[1:].as_list(): \n                dim *= d \n            feed_in = tf.reshape(input_data, [-1, dim]) \n        else: \n            feed_in, dim = (input_data, input_shape[-1].value) \n        weights = tf.get_variable(name=\"kernel\", shape=[dim,  \n num_output], \n\n initializer=tf.truncated_normal_initializer(stddev=1e-1,  \n dtype=tf.float32)) \n        biases = tf.get_variable(name=\"bias\", shape=[num_output], \n dtype=tf.float32, \n\n initializer=tf.constant_initializer(value=0.0)) \n        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b \n        output = op(feed_in, weights, biases, name=scope.name) \n        return output \n def _softmax(input_data, name): \n    return tf.nn.softmax(input_data, name=name) \n```", "```py\n   def load_caffe_weights(path, sess, ignore_missing=False): \n    print(\"Load caffe weights from \", path) \n    data_dict = np.load(path).item() \n    for op_name in data_dict: \n        with tf.variable_scope(op_name, reuse=True): \n            for param_name, data in   \n    data_dict[op_name].iteritems(): \n                try: \n                    var = tf.get_variable(param_name) \n                    sess.run(var.assign(data)) \n                except ValueError as e: \n                    if not ignore_missing: \n                        print(e) \n                        raise e \n```", "```py\n    if __name__ == \"__main__\": \n    path = \"data/VGG16.npz\" \n    data_dict = np.load(path).item() \n    for op_name in data_dict: \n        print(op_name) \n        for param_name, data in     ].iteritems(): \n            print(\"\\t\" + param_name + \"\\t\" + str(data.shape)) \n```", "```py\nconv1_1\n    weights (3, 3, 3, 64)\n    biases  (64,)\nconv1_2\n    weights (3, 3, 64, 64)\n    biases  (64,)\n```", "```py\n    import os \n    from utils import debug_print \n    from scipy.misc import imread, imresize \n\n    if __name__ == \"__main__\": \n    SAMPLES_FOLDER = \"samples_data\" \n    with open('%s/imagenet-classes.txt' % SAMPLES_FOLDER, 'rb') as   \n    infile: \n     class_labels = map(str.strip, infile.readlines()) \n\n    inputs = tf.placeholder(tf.float32, [None, 224, 224, 3],   \n    name=\"inputs\") \n    outputs = inference(inputs) \n\n    debug_print.print_variables(tf.global_variables()) \n    debug_print.print_variables([inputs, outputs]) \n\n    with tf.Session() as sess: \n     load_caffe_weights(\"data/VGG16.npz\", sess,   \n    ignore_missing=False) \n\n        files = os.listdir(SAMPLES_FOLDER) \n        for file_name in files: \n            if not file_name.endswith(\".jpg\"): \n                continue \n            print(\"=== Predict %s ==== \" % file_name) \n            img = imread(os.path.join(SAMPLES_FOLDER, file_name),  \n            mode=\"RGB\") \n            img = imresize(img, (224, 224)) \n\n            prob = sess.run(outputs, feed_dict={inputs: [img]})[0] \n            preds = (np.argsort(prob)[::-1])[0:3] \n\n            for p in preds: \n                print class_labels[p], prob[p]\n```", "```py\n      We get the model graph with outputs = inference(inputs). \n```", "```py\n    == Predict car.jpg ==== \n    racer, race car, racing car 0.666172\n    sports car, sport car 0.315847\n    car wheel 0.0117961\n    === Predict cat.jpg ==== \n    Persian cat 0.762223\n    tabby, tabby cat 0.0647032\n    lynx, catamount 0.0371023\n    === Predict dog.jpg ==== \n    Border collie 0.562288\n    collie 0.239735\n    Appenzeller 0.0186233\n```", "```py\n- data\n-- VGG16.npz\n-- datasets\n---- annotations\n------ trainval.txt\n---- images\n------ *.jpg\n```", "```py\n    import os \n    import tensorflow as tf \n    from tqdm import tqdm \n    from scipy.misc import imread, imsave \n\n    FLAGS = tf.app.flags.FLAGS \n\n    tf.app.flags.DEFINE_string( \n    'dataset_dir', 'data/datasets', \n    'The location of Oxford IIIT Pet Dataset which contains    \n     annotations and images folders' \n    ) \n\n    tf.app.flags.DEFINE_string( \n    'target_dir', 'data/train_data', \n    'The location where all the images will be stored' \n    ) \n\n    def ensure_folder_exists(folder_path): \n    if not os.path.exists(folder_path): \n        os.mkdir(folder_path) \n    return folder_path \n\n    def read_image(image_path): \n    try: \n        image = imread(image_path) \n        return image \n    except IOError: \n        print(image_path, \"not readable\") \n    return None \n```", "```py\n def convert_data(split_name, save_label=False): \n    if split_name not in [\"trainval\", \"test\"]: \n    raise ValueError(\"split_name is not recognized!\") \n    target_split_path =  \n    ensure_folder_exists(os.path.join(FLAGS.target_dir, split_name)) \n    output_file = open(os.path.join(FLAGS.target_dir, split_name +  \n    \".txt\"), \"w\") \n\n    image_folder = os.path.join(FLAGS.dataset_dir, \"images\") \n    anno_folder = os.path.join(FLAGS.dataset_dir, \"annotations\") \n\n    list_data = [line.strip() for line in open(anno_folder + \"/\" +  \n    split_name + \".txt\")] \n\n    class_name_idx_map = dict() \n    for data in tqdm(list_data, desc=split_name): \n      file_name,class_index,species,breed_id = data.split(\" \") \n      file_label = int(class_index) - 1 \n\n      class_name = \"_\".join(file_name.split(\"_\")[0:-1]) \n      class_name_idx_map[class_name] = file_label \n\n      image_path = os.path.join(image_folder, file_name + \".jpg\") \n      image = read_image(image_path) \n      if image is not None: \n      target_class_dir =  \n       ensure_folder_exists(os.path.join(target_split_path,    \n       class_name)) \n      target_image_path = os.path.join(target_class_dir,  \n       file_name + \".jpg\") \n            imsave(target_image_path, image) \n            output_file.write(\"%s %s\\n\" % (file_label,  \n            target_image_path)) \n\n    if save_label: \n        label_file = open(os.path.join(FLAGS.target_dir,  \n        \"labels.txt\"), \"w\") \n        for class_name in sorted(class_name_idx_map,  \n        key=class_name_idx_map.get): \n        label_file.write(\"%s\\n\" % class_name) \n\n def main(_): \n    if not FLAGS.dataset_dir: \n    raise ValueError(\"You must supply the dataset directory with  \n    --dataset_dir\") \n\n    ensure_folder_exists(FLAGS.target_dir) \n    convert_data(\"trainval\", save_label=True) \n    convert_data(\"test\") \n\n if __name__ == \"__main__\": \n    tf.app.run() \n```", "```py\npython scripts/convert_oxford_data.py --dataset_dir data/datasets/ --target_dir data/train_data.\n```", "```py\n- train_data\n-- trainval.txt\n-- test.txt\n-- labels.txt\n-- trainval\n---- Abyssinian\n---- ...\n-- test\n---- Abyssinian\n---- ...\n```", "```py\n    import tensorflow as tf \n    import os \n\n    def load_files(filenames): \n    filename_queue = tf.train.string_input_producer(filenames) \n    line_reader = tf.TextLineReader() \n    key, line = line_reader.read(filename_queue) \n    label, image_path = tf.decode_csv(records=line, \n\n    record_defaults=[tf.constant([], dtype=tf.int32),   \n    tf.constant([], dtype=tf.string)], \n                                      field_delim=' ') \n    file_contents = tf.read_file(image_path) \n    image = tf.image.decode_jpeg(file_contents, channels=3) \n\n    return image, label \n```", "```py\n def input_pipeline(dataset_dir, batch_size, num_threads=8,   \n    is_training=True, shuffle=True): \n    if is_training: \n        file_names = [os.path.join(dataset_dir, \"trainval.txt\")] \n    else: \n        file_names = [os.path.join(dataset_dir, \"test.txt\")] \n    image, label = load_files(file_names) \n\n    image = preprocessing(image, is_training) \n\n    min_after_dequeue = 1000 \n    capacity = min_after_dequeue + 3 * batch_size \n    if shuffle: \n     image_batch, label_batch = tf.train.shuffle_batch( \n     [image, label], batch_size, capacity,  \n     min_after_dequeue, num_threads \n      ) \n    else: \n        image_batch, label_batch = tf.train.batch( \n            [image, label], batch_size, num_threads, capacity \n            ) \n    return image_batch, label_batch\n```", "```py\n def preprocessing(image, is_training=True, image_size=224,  \n resize_side_min=256, resize_side_max=312): \n    image = tf.cast(image, tf.float32) \n\n    if is_training: \n        resize_side = tf.random_uniform([], minval=resize_side_min,  \n        maxval=resize_side_max+1, dtype=tf.int32) \n        resized_image = _aspect_preserving_resize(image,  \n        resize_side) \n\n        distorted_image = tf.random_crop(resized_image, [image_size,  \n        image_size, 3]) \n\n        distorted_image =  \n        tf.image.random_flip_left_right(distorted_image) \n\n        distorted_image =  \n        tf.image.random_brightness(distorted_image, max_delta=50) \n\n        distorted_image = tf.image.random_contrast(distorted_image,  \n        lower=0.2, upper=2.0) \n\n        return distorted_image \n    else: \n        resized_image = _aspect_preserving_resize(image, image_size) \n        return tf.image.resize_image_with_crop_or_pad(resized_image,  \n        image_size, image_size)\n```", "```py\n    def _smallest_size_at_least(height, width, smallest_side): \n      smallest_side = tf.convert_to_tensor(smallest_side,   \n      dtype=tf.int32) \n\n      height = tf.to_float(height) \n      width = tf.to_float(width) \n      smallest_side = tf.to_float(smallest_side) \n\n      scale = tf.cond(tf.greater(height, width), \n                    lambda: smallest_side / width, \n                    lambda: smallest_side / height) \n      new_height = tf.to_int32(height * scale) \n      new_width = tf.to_int32(width * scale) \n      return new_height, new_width \n\n    def _aspect_preserving_resize(image, smallest_side): \n      smallest_side = tf.convert_to_tensor(smallest_side,   \n      dtype=tf.int32) \n      shape = tf.shape(image) \n      height = shape[0] \n      width = shape[1] \n      new_height, new_width = _smallest_size_at_least(height, width,   \n      smallest_side) \n      image = tf.expand_dims(image, 0) \n      resized_image = tf.image.resize_bilinear(image, [new_height,   \n      new_width], align_corners=False) \n      resized_image = tf.squeeze(resized_image) \n      resized_image.set_shape([None, None, 3]) \n      return resized_image \n```", "```py\n    def inference(images, is_training=False): \n    # \n    # All the code before fc7 are not modified. \n    # \n    fc7 = _fully_connected(fc6, 4096, name=\"fc7\") \n    if is_training: \n        fc7 = tf.nn.dropout(fc7, keep_prob=0.5) \n    fc8 = _fully_connected(fc7, 37, name='fc8-pets', relu=False) \n    return fc8\n```", "```py\n def compute_loss(logits, labels): \n   labels = tf.squeeze(tf.cast(labels, tf.int32)) \n\n   cross_entropy =   \n   tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,    \n   labels=labels) \n   cross_entropy_mean = tf.reduce_mean(cross_entropy) \n   tf.add_to_collection('losses', cross_entropy_mean) \n\n   return tf.add_n(tf.get_collection('losses'),    \n   name='total_loss') \n\n def compute_accuracy(logits, labels): \n   labels = tf.squeeze(tf.cast(labels, tf.int32)) \n   batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32) \n   predicted_correctly = tf.equal(batch_predictions, labels) \n   accuracy = tf.reduce_mean(tf.cast(predicted_correctly,   \n   tf.float32)) \n   return accuracy\n```", "```py\n def get_learning_rate(global_step, initial_value, decay_steps,          \n   decay_rate): \n   learning_rate = tf.train.exponential_decay(initial_value,   \n   global_step, decay_steps, decay_rate, staircase=True) \n   return learning_rate \n\n def train(total_loss, learning_rate, global_step, train_vars): \n\n   optimizer = tf.train.AdamOptimizer(learning_rate) \n\n   train_variables = train_vars.split(\",\") \n\n   grads = optimizer.compute_gradients( \n       total_loss, \n       [v for v in tf.trainable_variables() if v.name in   \n       train_variables] \n       ) \n   train_op = optimizer.apply_gradients(grads,   \n   global_step=global_step) \n   return train_op \n```", "```py\n import tensorflow as tf \n import os \n from datetime import datetime \n from tqdm import tqdm \n\n import nets, models, datasets \n\n # Dataset \n dataset_dir = \"data/train_data\" \n batch_size = 64 \n image_size = 224 \n\n # Learning rate \n initial_learning_rate = 0.001 \n decay_steps = 250 \n decay_rate = 0.9 \n\n # Validation \n output_steps = 10  # Number of steps to print output \n eval_steps = 20  # Number of steps to perform evaluations \n\n # Training \n max_steps = 3000  # Number of steps to perform training \n save_steps = 200  # Number of steps to perform saving checkpoints \n num_tests = 5  # Number of times to test for test accuracy \n max_checkpoints_to_keep = 3 \n save_dir = \"data/checkpoints\" \n train_vars = 'models/fc8-pets/weights:0,models/fc8-pets/biases:0' \n\n # Export \n export_dir = \"/tmp/export/\" \n export_name = \"pet-model\" \n export_version = 2 \n```", "```py\n images, labels = datasets.input_pipeline(dataset_dir, batch_size,   \n is_training=True) \n test_images, test_labels = datasets.input_pipeline(dataset_dir,  \n batch_size, is_training=False) \n\n with tf.variable_scope(\"models\") as scope: \n    logits = nets.inference(images, is_training=True) \n    scope.reuse_variables() \n    test_logits = nets.inference(test_images, is_training=False) \n\n total_loss = models.compute_loss(logits, labels) \n train_accuracy = models.compute_accuracy(logits, labels) \n test_accuracy = models.compute_accuracy(test_logits, test_labels) \n\n global_step = tf.Variable(0, trainable=False) \n learning_rate = models.get_learning_rate(global_step,  \n initial_learning_rate, decay_steps, decay_rate) \n train_op = models.train(total_loss, learning_rate, global_step,  \n train_vars) \n\n saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) \n checkpoints_dir = os.path.join(save_dir,  \n datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")) \n if not os.path.exists(save_dir): \n    os.mkdir(save_dir) \n if not os.path.exists(checkpoints_dir): \n    os.mkdir(checkpoints_dir) \n```", "```py\n with tf.Session() as sess: \n    sess.run(tf.global_variables_initializer()) \n    coords = tf.train.Coordinator() \n    threads = tf.train.start_queue_runners(sess=sess, coord=coords) \n\n    with tf.variable_scope(\"models\"): \n       nets.load_caffe_weights(\"data/VGG16.npz\", sess,  \n       ignore_missing=True) \n\n    last_saved_test_accuracy = 0 \n    for i in tqdm(range(max_steps), desc=\"training\"): \n                  _, loss_value, lr_value = sess.run([train_op,    \n                  total_loss,  learning_rate]) \n\n      if (i + 1) % output_steps == 0: \n          print(\"Steps {}: Loss = {:.5f} Learning Rate =  \n          {}\".format(i + 1, loss_value, lr_value)) \n\n      if (i + 1) % eval_steps == 0: \n          test_acc, train_acc, loss_value =  \n          sess.run([test_accuracy, train_accuracy, total_loss]) \n          print(\"Test accuracy {} Train accuracy {} : Loss =  \n          {:.5f}\".format(test_acc, train_acc, loss_value)) \n\n      if (i + 1) % save_steps == 0 or i == max_steps - 1: \n          test_acc = 0 \n          for i in range(num_tests): \n              test_acc += sess.run(test_accuracy) \n          test_acc /= num_tests \n      if test_acc > last_saved_test_accuracy: \n            print(\"Save steps: Test Accuracy {} is higher than  \n            {}\".format(test_acc, last_saved_test_accuracy)) \n             last_saved_test_accuracy = test_acc \n             saved_file = saver.save(sess, \n\n     os.path.join(checkpoints_dir, 'model.ckpt'), \n                  global_step=global_step) \n          print(\"Save steps: Save to file %s \" % saved_file) \n      else: \n          print(\"Save steps: Test Accuracy {} is not higher  \n                than {}\".format(test_acc, last_saved_test_accuracy)) \n\n    models.export_model(checkpoints_dir, export_dir, export_name,  \n    export_version) \n\n    coords.request_stop() \n    coords.join(threads) \n```", "```py\n    models.export_model(checkpoints_dir, export_dir, export_name,  \n    export_version) \n```", "```py\npython scripts/train.py\n```", "```py\n('Load caffe weights from ', 'data/VGG16.npz')\ntraining:   0%|▏                | 9/3000 [00:05<24:59,  1.99it/s]\nSteps 10: Loss = 31.10747 Learning Rate = 0.0010000000475\ntraining:   1%|▎                | 19/3000 [00:09<19:19,  2.57it/s]\nSteps 20: Loss = 34.43741 Learning Rate = 0.0010000000475\nTest accuracy 0.296875 Train accuracy 0.0 : Loss = 31.28600\ntraining:   1%|▍                | 29/3000 [00:14<20:01,  2.47it/s]\nSteps 30: Loss = 15.81103 Learning Rate = 0.0010000000475\ntraining:   1%|▌                | 39/3000 [00:18<19:42,  2.50it/s]\nSteps 40: Loss = 14.07709 Learning Rate = 0.0010000000475\nTest accuracy 0.53125 Train accuracy 0.03125 : Loss = 20.65380  \n```", "```py\n def export_model(checkpoint_dir, export_dir, export_name,  \n export_version): \n    graph = tf.Graph() \n    with graph.as_default(): \n        image = tf.placeholder(tf.float32, shape=[None, None, 3]) \n        processed_image = datasets.preprocessing(image,  \n        is_training=False) \n        with tf.variable_scope(\"models\"): \n         logits = nets.inference(images=processed_image,  \n          is_training=False) \n\n        model_checkpoint_path =  \n        get_model_path_from_ckpt(checkpoint_dir) \n        saver = tf.train.Saver() \n\n        config = tf.ConfigProto() \n        config.gpu_options.allow_growth = True \n        config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n\n        with tf.Session(graph=graph) as sess: \n            saver.restore(sess, model_checkpoint_path) \n            export_path = os.path.join(export_dir, export_name,  \n            str(export_version)) \n            export_saved_model(sess, export_path, image, logits) \n            print(\"Exported model at\", export_path)\n```", "```py\n def export_saved_model(sess, export_path, input_tensor,  \n output_tensor): \n    from tensorflow.python.saved_model import builder as  \n saved_model_builder \n    from tensorflow.python.saved_model import signature_constants \n    from tensorflow.python.saved_model import signature_def_utils \n    from tensorflow.python.saved_model import tag_constants \n    from tensorflow.python.saved_model import utils \n    builder = saved_model_builder.SavedModelBuilder(export_path) \n\n    prediction_signature = signature_def_utils.build_signature_def( \n        inputs={'images': utils.build_tensor_info(input_tensor)}, \n        outputs={ \n            'scores': utils.build_tensor_info(output_tensor) \n        }, \n        method_name=signature_constants.PREDICT_METHOD_NAME) \n\n    legacy_init_op = tf.group( \n        tf.tables_initializer(), name='legacy_init_op') \n    builder.add_meta_graph_and_variables( \n        sess, [tag_constants.SERVING], \n        signature_def_map={ \n          'predict_images': \n           prediction_signature, \n        }, \n        legacy_init_op=legacy_init_op) \n\n    builder.save() \n```", "```py\npython scripts/train.py\n```", "```py\nSteps 3000: Loss = 0.59160 Learning Rate = 0.000313810509397\nTest accuracy 0.659375 Train accuracy 0.853125: Loss = 0.25782\nSave steps: Test Accuracy 0.859375 is not higher than 0.921875\ntraining: 100%|██████████████████| 3000/3000 [23:40<00:00,  1.27it/s]\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\n    ('Exported model at', '/home/ubuntu/models/pet-model/1')\n```", "```py\n- /home/ubuntu/models/\n-- pet_model\n---- 1\n------ saved_model.pb\n------ variables\n```", "```py\nbazel build   \n//tensorflow_serving/model_servers:tensorflow_model_server\n```", "```py\n- /home/ubuntu/productions/\n-- 1\n---- saved_model.pb\n---- variables\n```", "```py\nsudo apt-get install tmux\n```", "```py\ntmux new -s serving\n```", "```py\n    bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=pet-model --model_base_path=/home/ubuntu/productions\n```", "```py\n    2017-05-29 13:44:32.203153: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:274] Loading SavedModel: success. Took 537318 microseconds.\n    2017-05-29 13:44:32.203243: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: pet-model version: 1}\n    2017-05-29 13:44:32.205543: I tensorflow_serving/model_servers/main.cc:298] Running ModelServer at 0.0.0.0:9000 ...  \n```", "```py\n    import tensorflow as tf \n    import numpy as np \n    from tensorflow_serving.apis import prediction_service_pb2,     \n    predict_pb2 \n    from grpc.beta import implementations \n    from scipy.misc import imread \n    from datetime import datetime \n\n    class Output: \n    def __init__(self, score, label): \n        self.score = score \n        self.label = label \n\n    def __repr__(self): \n        return \"Label: %s Score: %.2f\" % (self.label, self.score) \n\n    def softmax(x): \n    return np.exp(x) / np.sum(np.exp(x), axis=0) \n\n    def process_image(path, label_data, top_k=3): \n    start_time = datetime.now() \n    img = imread(path) \n\n    host, port = \"0.0.0.0:9000\".split(\":\") \n    channel = implementations.insecure_channel(host, int(port)) \n    stub =  \n    prediction_service_pb2.beta_create_PredictionService_stub(channel) \n\n    request = predict_pb2.PredictRequest() \n    request.model_spec.name = \"pet-model\" \n    request.model_spec.signature_name = \"predict_images\" \n\n    request.inputs[\"images\"].CopyFrom( \n        tf.contrib.util.make_tensor_proto( \n            img.astype(dtype=float), \n            shape=img.shape, dtype=tf.float32 \n        ) \n    ) \n\n    result = stub.Predict(request, 20.) \n    scores =    \n    tf.contrib.util.make_ndarray(result.outputs[\"scores\"])[0] \n    probs = softmax(scores) \n    index = sorted(range(len(probs)), key=lambda x: probs[x],  \n    reverse=True) \n\n    outputs = [] \n    for i in range(top_k): \n        outputs.append(Output(score=float(probs[index[i]]),  \n        label=label_data[index[i]])) \n\n    print(outputs) \n    print(\"total time\", (datetime.now() -   \n    start_time).total_seconds()) \n    return outputs \n\n    if __name__ == \"__main__\": \n    label_data = [line.strip() for line in   \n    open(\"production/labels.txt\", 'r')] \n    process_image(\"samples_data/dog.jpg\", label_data) \n    process_image(\"samples_data/cat.jpg\", label_data) \n```", "```py\npython production/client.py\n```", "```py\n    [Label: saint_bernard Score: 0.78, Label: american_bulldog Score: 0.21, Label: staffordshire_bull_terrier Score: 0.00]\n    ('total time', 14.943942)\n    [Label: Maine_Coon Score: 1.00, Label: Ragdoll Score: 0.00, Label: Bengal Score: 0.00]\n    ('total time', 14.918235)\n```", "```py\n    [Label: saint_bernard Score: 0.78, Label: american_bulldog Score: 0.21, Label: staffordshire_bull_terrier Score: 0.00]\n    ('total time', 0.493618)\n    [Label: Maine_Coon Score: 1.00, Label: Ragdoll Score: 0.00, Label: Bengal Score: 0.00]\n    ('total time', 0.023753)\n```", "```py\ntmux new -s \"flask\"\npython production/server.py\n```", "```py\n    import tensorflow as tf \n    import os \n    import json \n    import random \n    import requests \n    import shutil \n    from scipy.misc import imread, imsave \n    from datetime import datetime \n    from tqdm import tqdm \n\n    import nets, models, datasets \n\n    def ensure_folder_exists(folder_path): \n    if not os.path.exists(folder_path): \n        os.mkdir(folder_path) \n    return folder_path \n\n    def download_user_data(url, user_dir, train_ratio=0.8): \n    response = requests.get(\"%s/user-labels\" % url) \n    data = json.loads(response.text) \n\n    if not os.path.exists(user_dir): \n        os.mkdir(user_dir) \n    user_dir = ensure_folder_exists(user_dir) \n    train_folder = ensure_folder_exists(os.path.join(user_dir,   \n    \"trainval\")) \n    test_folder = ensure_folder_exists(os.path.join(user_dir,   \n    \"test\")) \n\n    train_file = open(os.path.join(user_dir, 'trainval.txt'), 'w') \n    test_file = open(os.path.join(user_dir, 'test.txt'), 'w') \n\n    for image in data: \n        is_train = random.random() < train_ratio \n        image_url = image[\"url\"] \n        file_name = image_url.split(\"/\")[-1] \n        label = image[\"label\"] \n        name = image[\"name\"] \n\n        if is_train: \n          target_folder =  \n          ensure_folder_exists(os.path.join(train_folder, name)) \n        else: \n          target_folder =   \n          ensure_folder_exists(os.path.join(test_folder, name)) \n\n        target_file = os.path.join(target_folder, file_name) +   \n        \".jpg\" \n\n        if not os.path.exists(target_file): \n            response = requests.get(\"%s%s\" % (url, image_url)) \n            temp_file_path = \"/tmp/%s\" % file_name \n            with open(temp_file_path, 'wb') as f: \n                for chunk in response: \n                    f.write(chunk) \n\n            image = imread(temp_file_path) \n            imsave(target_file, image) \n            os.remove(temp_file_path) \n            print(\"Save file: %s\" % target_file) \n\n        label_path = \"%s %s\\n\" % (label, target_file) \n        if is_train: \n            train_file.write(label_path) \n        else: \n            test_file.write(label_path) \n```", "```py\n   [ \n    { \n     \"id\": 1,  \n     \"label\": 0,  \n     \"name\": \"Abyssinian\",  \n     \"url\": \"/uploads/2017-05-23_14-56-45_Abyssinian-cat.jpeg\" \n    },  \n    { \n     \"id\": 2,  \n      \"label\": 32,  \n      \"name\": \"Siamese\",  \n     \"url\": \"/uploads/2017-05-23_14-57-33_fat-Siamese-cat.jpeg\" \n    } \n   ] \n```", "```py\n    def get_latest_model(url): \n    response = requests.get(\"%s/model\" % url) \n    data = json.loads(response.text) \n    print(data) \n    return data[\"ckpt_name\"], int(data[\"version\"]) \n```", "```py\n    { \n     \"ckpt_name\": \"2017-05-26_02-12-49\",  \n     \"id\": 10,  \n     \"link\": \"http://1.53.110.161:8181/pet-model/8.zip\",  \n     \"name\": \"pet-model\",  \n     \"version\": 8 \n    } \n```", "```py\n    # Server info \n    URL = \"http://localhost:5000\" \n    dest_api = URL + \"/model\" \n\n    # Server Endpoints \n    source_api = \"http://1.53.110.161:8181\" \n\n    # Dataset \n    dataset_dir = \"data/train_data\" \n    user_dir = \"data/user_data\" \n    batch_size = 64 \n    image_size = 224 \n\n    # Learning rate \n    initial_learning_rate = 0.0001 \n    decay_steps = 250 \n    decay_rate = 0.9 \n\n    # Validation \n    output_steps = 10  # Number of steps to print output \n    eval_steps = 20  # Number of steps to perform evaluations \n\n    # Training \n    max_steps = 3000  # Number of steps to perform training \n    save_steps = 200  # Number of steps to perform saving    \n    checkpoints \n    num_tests = 5  # Number of times to test for test accuracy \n    max_checkpoints_to_keep = 1 \n    save_dir = \"data/checkpoints\" \n    train_vars = 'models/fc8-pets/weights:0,models/fc8- \n    pets/biases:0' \n\n    # Get the latest model \n    last_checkpoint_name, last_version = get_latest_model(URL) \n    last_checkpoint_dir = os.path.join(save_dir,   \n    last_checkpoint_name) \n\n    # Export \n    export_dir = \"/home/ubuntu/models/\" \n    export_name = \"pet-model\" \n    export_version = last_version + 1 \n```", "```py\n    # Download user-labels data \n    download_user_data(URL, user_dir) \n\n    images, labels = datasets.input_pipeline(dataset_dir,     \n    batch_size, is_training=True, user_dir=user_dir) \n    test_images, test_labels =    \n    datasets.input_pipeline(dataset_dir, batch_size,    \n    is_training=False, user_dir=user_dir) \n\n     with tf.variable_scope(\"models\") as scope: \n     logits = nets.inference(images, is_training=True) \n     scope.reuse_variables() \n     test_logits = nets.inference(test_images, is_training=False) \n\n    total_loss = models.compute_loss(logits, labels) \n    train_accuracy = models.compute_accuracy(logits, labels) \n    test_accuracy = models.compute_accuracy(test_logits,  \n    test_labels) \n\n    global_step = tf.Variable(0, trainable=False) \n    learning_rate = models.get_learning_rate(global_step,      \n    initial_learning_rate, decay_steps, decay_rate) \n    train_op = models.train(total_loss, learning_rate,  \n    global_step, train_vars) \n\n    saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) \n    checkpoint_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") \n    checkpoints_dir = os.path.join(save_dir, checkpoint_name) \n    if not os.path.exists(save_dir): \n      os.mkdir(save_dir) \n    if not os.path.exists(checkpoints_dir): \n      os.mkdir(checkpoints_dir) \n\n    with tf.Session() as sess: \n      sess.run(tf.global_variables_initializer()) \n      coords = tf.train.Coordinator() \n      threads = tf.train.start_queue_runners(sess=sess,   \n      coord=coords) \n\n    saver.restore(sess,  \n    models.get_model_path_from_ckpt(last_checkpoint_dir)) \n    sess.run(global_step.assign(0)) \n\n    last_saved_test_accuracy = 0 \n    for i in range(num_tests): \n        last_saved_test_accuracy += sess.run(test_accuracy) \n    last_saved_test_accuracy /= num_tests \n    should_export = False \n    print(\"Last model test accuracy    \n    {}\".format(last_saved_test_accuracy)) \n    for i in tqdm(range(max_steps), desc=\"training\"): \n        _, loss_value, lr_value = sess.run([train_op, total_loss,   \n        learning_rate]) \n\n     if (i + 1) % output_steps == 0: \n       print(\"Steps {}: Loss = {:.5f} Learning Rate =   \n       {}\".format(i + 1, loss_value, lr_value)) \n\n        if (i + 1) % eval_steps == 0: \n          test_acc, train_acc, loss_value =  \n          sess.run([test_accuracy, train_accuracy, total_loss]) \n            print(\"Test accuracy {} Train accuracy {} : Loss =  \n            {:.5f}\".format(test_acc, train_acc, loss_value)) \n\n        if (i + 1) % save_steps == 0 or i == max_steps - 1: \n          test_acc = 0 \n          for i in range(num_tests): \n            test_acc += sess.run(test_accuracy) \n            test_acc /= num_tests \n\n        if test_acc > last_saved_test_accuracy: \n          print(\"Save steps: Test Accuracy {} is higher than  \n          {}\".format(test_acc, last_saved_test_accuracy)) \n          last_saved_test_accuracy = test_acc \n          saved_file = saver.save(sess, \n\n        os.path.join(checkpoints_dir, 'model.ckpt'), \n                                        global_step=global_step) \n                should_export = True \n                print(\"Save steps: Save to file %s \" % saved_file) \n            else: \n                print(\"Save steps: Test Accuracy {} is not higher  \n       than {}\".format(test_acc, last_saved_test_accuracy)) \n\n    if should_export: \n        print(\"Export model with accuracy \",  \n        last_saved_test_accuracy) \n        models.export_model(checkpoints_dir, export_dir,   \n        export_name, export_version) \n        archive_and_send_file(source_api, dest_api,  \n        checkpoint_name, export_dir, export_name, export_version) \n      coords.request_stop() \n      coords.join(threads)\n```", "```py\n    def make_archive(dir_path): \n    return shutil.make_archive(dir_path, 'zip', dir_path) \n\n    def archive_and_send_file(source_api, dest_api, ckpt_name,    \n    export_dir, export_name, export_version): \n    model_dir = os.path.join(export_dir, export_name,    \n    str(export_version)) \n    file_path = make_archive(model_dir) \n    print(\"Zip model: \", file_path) \n\n    data = { \n        \"link\": \"{}/{}/{}\".format(source_api, export_name,  \n     str(export_version) + \".zip\"), \n        \"ckpt_name\": ckpt_name, \n        \"version\": export_version, \n        \"name\": export_name, \n    } \n     r = requests.post(dest_api, data=data) \n    print(\"send_file\", r.text) \n```", "```py\nsudo apt-get install apache2\n```", "```py\n    Listen 8181 \n```", "```py\n    <VirtualHost *:8181> \n      DocumentRoot \"/home/ubuntu/models\" \n      <Directory /> \n        Require all granted \n      </Directory> \n    </VirtualHost> \n```", "```py\nsudo service apache2 restart\n```", "```py\npython scripts/finetune.py\n```", "```py\n    Save steps: Test Accuracy 0.84 is higher than 0.916875\n    Save steps: Save to file data/checkpoints/2017-05-29_18-46-43/model.ckpt-2000\n    ('Export model with accuracy ', 0.916875000000004)\n    2017-05-29 18:47:31.642729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\n    ('Exported model at', '/home/ubuntu/models/pet-model/2')\n    ('Zip model: ', '/home/ubuntu/models/pet-model/2.zip')\n    ('send_file', u'{\\n  \"ckpt_name\": \"2017-05-29_18-46-43\", \\n  \"id\": 2, \\n  \"link\": \"http://1.53.110.161:8181/pet-model/2.zip\", \\n  \"name\": \"pet-model\", \\n  \"version\": 2\\n}\\n')\n```", "```py\n('Start downloading', u'http://1.53.110.161:8181/pet-model/2.zip')\n('Downloaded file at', u'/tmp/2.zip')\n('Extracted at', u'/home/ubuntu/productions/2')\n127.0.0.1 - - [29/May/2017 18:49:05] \"POST /model HTTP/1.1\" 200 -\n```", "```py\n    2017-05-29 18:49:06.234808: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: pet-model version: 2}\n    2017-05-29 18:49:06.234840: I tensorflow_serving/core/loader_harness.cc:137] Quiescing servable version {name: pet-model version: 1}\n    2017-05-29 18:49:06.234848: I tensorflow_serving/core/loader_harness.cc:144] Done quiescing servable version {name: pet-model version: 1}\n    2017-05-29 18:49:06.234853: I tensorflow_serving/core/loader_harness.cc:119] Unloading servable version {name: pet-model version: 1}\n    2017-05-29 18:49:06.240118: I ./tensorflow_serving/core/simple_loader.h:226] Calling MallocExtension_ReleaseToSystem() with 645327546\n    2017-05-29 18:49:06.240155: I tensorflow_serving/core/loader_harness.cc:127] Done unloading servable version {name: pet-model version: 1}\n```", "```py\ncrontab -e\n```", "```py\n0 3 * * * python /home/ubuntu/project/scripts/finetune.py\n```"]