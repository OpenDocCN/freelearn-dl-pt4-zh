["```py\nimport requests \n\nAPI_KEY = '<your_api_key>' \n\nstart_date = '2010-01-01' \nend_date = '2015-01-01' \norder = 'asc' \ncolumn_index = 4 \n\nstock_exchange = 'WIKI' \nindex = 'GOOG' \n\ndata_specs = 'start_date={}&end_date={}&order={}&column_index={}&api_key={}' \n   .format(start_date, end_date, order, column_index, API_KEY) \nbase_url = \"https://www.quandl.com/api/v3/datasets/{}/{}/data.json?\" + data_specs \nstock_data = requests.get(base_url.format(stock_exchange, index)).json()\n```", "```py\ncodes = [\"WSE/OPONEO_PL\", \"WSE/VINDEXUS\", \"WSE/WAWEL\", \"WSE/WIELTON\", \"WIKI/SNPS\"] \nclosings = pd.DataFrame() \nfor code in codes: \n    code_splits = code.split(\"/\") \n    stock_exchange = code_splits[0] \n    index = code_splits[1] \n    stock_data = requests.get(base_url.format(stock_exchange,  \n    index)).json() \n    dataset_data = stock_data['dataset_data'] \n    data = np.array(dataset_data['data']) \n    closings[index] = pd.Series(data[:, 1].astype(float)) \n    closings[index + \"_scaled\"] = closings[index] / \n     max(closings[index]) \n    closings[index + \"_log_return\"] = np.log(closings[index] / closings[index].shift()) \nclosings = closings.fillna(method='ffill')  # Fill the gaps in data \n```", "```py\ndef show_plot(key=\"\", show=True): \n    fig = plt.figure() \n    fig.set_figwidth(20) \n    fig.set_figheight(15) \n    for code in codes: \n        index = code.split(\"/\")[1] \n        if key and len(key) > 0: \n            label = \"{}_{}\".format(index, key) \n        else: \n            label = index \n        _ = plt.plot(closings[label], label=label) \n\n    _ = plt.legend(loc='upper right') \n    if show: \n        plt.show() \n\nshow = True \nshow_plot(\"\", show=show) \nshow_plot(\"scaled\", show=show) \nshow_plot(\"log_return\", show=show) \n```", "```py\nfeature_columns = ['SNPS_log_return_positive', 'SNPS_log_return_negative'] \nfor i in range(len(codes)): \n    index = codes[i].split(\"/\")[1] \n    feature_columns.extend([ \n        '{}_log_return_1'.format(index), \n        '{}_log_return_2'.format(index), \n        '{}_log_return_3'.format(index) \n    ]) \nfeatures_and_labels = pd.DataFrame(columns=feature_columns) \nclosings['SNPS_log_return_positive'] = 0 \nclosings.ix[closings['SNPS_log_return'] >= 0, 'SNPS_log_return_positive'] = 1 \nclosings['SNPS_log_return_negative'] = 0 \nclosings.ix[closings['SNPS_log_return'] < 0, 'SNPS_log_return_negative'] = 1 \nfor i in range(7, len(closings)): \n    feed_dict = {'SNPS_log_return_positive': closings['SNPS_log_return_positive'].ix[i], \n        'SNPS_log_return_negative': closings['SNPS_log_return_negative'].ix[i]} \n    for j in range(len(codes)): \n        index = codes[j].split(\"/\")[1] \n        k = 1 if j == len(codes) - 1 else 0 \n        feed_dict.update({'{}_log_return_1'.format(index): closings['{}_log_return'.format(index)].ix[i - k], \n                '{}_log_return_2'.format(index): closings['{}_log_return'.format(index)].ix[i - 1 - k], \n                '{}_log_return_3'.format(index): closings['{}_log_return'.format(index)].ix[i - 2 - k]}) \n    features_and_labels = features_and_labels.append(feed_dict, ignore_index=True) \n```", "```py\nfeatures = features_and_labels[features_and_labels.columns[2:]] \nlabels = features_and_labels[features_and_labels.columns[:2]] \ntrain_size = int(len(features_and_labels) * train_test_split) \ntest_size = len(features_and_labels) - train_size \ntrain_features = features[:train_size] \ntrain_labels = labels[:train_size] \ntest_features = features[train_size:] \ntest_labels = labels[train_size:]\n```", "```py\nsess = tf.Session() \nnum_predictors = len(train_features.columns) \nnum_classes = len(train_labels.columns) \nfeature_data = tf.placeholder(\"float\", [None, num_predictors]) \nactual_classes = tf.placeholder(\"float\", [None, 2]) \nweights1 = tf.Variable(tf.truncated_normal([len(codes) * 3, 50], stddev=0.0001)) \nbiases1 = tf.Variable(tf.ones([50])) \nweights2 = tf.Variable(tf.truncated_normal([50, 25], stddev=0.0001)) \nbiases2 = tf.Variable(tf.ones([25])) \nweights3 = tf.Variable(tf.truncated_normal([25, 2], stddev=0.0001)) \nbiases3 = tf.Variable(tf.ones([2])) \nhidden_layer_1 = tf.nn.relu(tf.matmul(feature_data, weights1) + biases1) \nhidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2) \nmodel = tf.nn.softmax(tf.matmul(hidden_layer_2, weights3) + biases3) \ncost = -tf.reduce_sum(actual_classes * tf.log(model)) \ntrain_op1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost) \ninit = tf.initialize_all_variables() \nsess.run(init) \ncorrect_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1)) \naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n```", "```py\nfor i in range(1, 30001): \n    sess.run(train_op1, feed_dict={feature_data: train_features.values, \n            actual_classes: train_labels.values.reshape(len(train_labels.values), 2)}) \n    if i % 5000 == 0: \n        print(i, sess.run(accuracy, feed_dict={feature_data: train_features.values, \n                actual_classes: train_labels.values.reshape(len(train_labels.values), 2)})) \n```", "```py\nfeed_dict = { \n    feature_data: test_features.values, \n    actual_classes: test_labels.values.reshape(len(test_labels.values), 2) \n} \ntf_confusion_metrics(model, actual_classes, sess, feed_dict) \n```"]