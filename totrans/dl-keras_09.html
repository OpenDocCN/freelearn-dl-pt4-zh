<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Conclusion</h1>
            </header>

            <article>
                
<p>Congratulations on making it to the end of the book! Let us take a moment and see how far we have come since we started.</p>
<p>If you are like most readers, you started with some knowledge of Python and some background in machine learning, but you were interested in learning more about deep learning and wanted to be able to apply these deep learning skills using Python.</p>
<p>You learned how to install Keras on your machine and started using it to build simple deep learning models. You then learned about the original deep learning model, the multi-layer perceptron, also called the <strong>fully connected network</strong> (<strong>FCN</strong>). You learned how to build this network using Keras.</p>
<p>You also learned about the many tunable parameters that you need to tweak to get good results from your network. With Keras, a lot of the hard work has been done for you since it comes with sensible defaults, but there are occasions where this knowledge will be helpful to you.</p>
<p>Continuing on from there, you were introduced to <strong>convolutional neural network</strong> (<strong>CNN</strong>), originally built to exploit feature locality of images, although you can also use them for other types of data such as text, audio or video. Once again, you saw how to build a CNN using Keras. You also saw the functionality that Keras provides to build CNNs easily and intuitively. You saw how to use pre-trained image networks to make predictions about your own images, via the process of transfer learning and fine-tuning.</p>
<p>From there, you learned about <strong>generative adversarial network</strong> (<strong>GAN</strong>), which are a pair of networks (usually CNN) that attempt to work against each other and, in the process, make each other stronger. GANs are a cutting-edge technology in the deep learning space; a lot of recent work is going on around GANs.</p>
<p>From there, we turned our attention to text and we learned about <strong>word embeddings</strong>, which have become the most common technology used for the vector representation of text in the last couple of years. We looked at various popular word embedding algorithms and saw how to use pre-trained word embeddings to represent collections of words, as well as support for word embeddings in Keras and gensim.</p>
<p>We then looked at <strong>recurrent neural network</strong> (<strong>RNN</strong>), a class of neural network optimized for handing sequence data such as text or time series. We learned about the shortcomings of the basic RNN model and how these are alleviated in the more powerful variants such as the <strong>long short term model</strong> (<strong>LSTM</strong>) and <strong>gated recurrent unit</strong> (<strong>GRU</strong>). We looked at a few examples where these components are used. We also looked briefly at Stateful RNN models and where they might be used.</p>
<p>Next up, we looked at a few additional models that don't quite fit the molds of the models we have spoken so far. Among them are <strong>autoencoders</strong>, a model for unsupervised learning—<strong>regression networks</strong> that predict a continuous value rather than a discrete label. We introduced the <strong>Keras functional API</strong>, which allows us to build complex networks with multiple inputs and outputs and share components among multiple pipelines. We looked at ways to customize Keras to add functionality that doesn't currently exist.</p>
<p>Finally, we looked at training deep learning networks using <strong>reinforcement learning</strong> in the context of playing arcade games, which many consider a first step toward a general artificial intelligence. We provided a Keras example of training a simple game. We then briefly described advances in this field in the context of networks playing even harder games such as Go and Poker at a superhuman level.</p>
<p>We believe you are now equipped with the skills to solve new machine learning problems using deep learning and Keras. This is an important and valuable skill in your journey to becoming a deep learning expert.</p>
<p>We would like to thank you for letting us help you on your journey to deep learning mastery.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Keras 2.0 — what is new</h1>
            </header>

            <article>
                
<p>According to Francois Chollet, Keras was released two years ago, in March, 2015. It then proceeded to grow from one user to one hundred thousand. The following image, taken from the Keras blog, shows the growth of number of Keras users over time.</p>
<div class="CDPAlignCenter CDPAlign"><q><img class="image-border" height="176" src="assets/keras_users_2015_2016.png" width="319"/></q></div>
<p>One important update with Keras 2.0 is that the API will now be a part of TensorFlow, starting with TensorFlow 1.2. Indeed, Keras is becoming more and more the <em>lingua franca</em> for deep learning, a <em>spec</em> used in an increasing number of deep learning contexts. For instance, Skymind is implementing Keras spec in Scala for ScalNet, and Keras.js is doing the same for JavaScript for running of deep learning directly in the browser. Efforts are also underway to provide a Keras API for MXNET and CNTK deep learning toolkits.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras 2.0</h1>
            </header>

            <article>
                
<p>Installing Keras 2.0 is very simple via the <kbd>pip install keras --upgrade</kbd> followed by <kbd>pip install tensorflow --upgrade</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">API changes</h1>
            </header>

            <article>
                
<p>The Keras 2.0 changes implied the need to rethink some APIs. For full details, please refer to the release notes (<a href="https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes" target="_blank">https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes</a>). This module <kbd>legacy.py</kbd> summarizes the most impactful changes and prevents warnings when using Keras 1.x calls:</p>
<pre>
""<br/>Utility functions to avoid warnings while testing both Keras 1 and 2.<br/>"""<br/>import keras<br/>keras_2 = int(keras.__version__.split(".")[0]) &gt; 1 # Keras &gt; 1<br/><br/>def fit_generator(model, generator, epochs, steps_per_epoch):<br/>    if keras_2:<br/>        model.fit_generator(generator, epochs=epochs, steps_per_epoch=steps_per_epoch)<br/>    else:<br/>        model.fit_generator(generator, nb_epoch=epochs, samples_per_epoch=steps_per_epoch)<br/><br/>def fit(model, x, y, nb_epoch=10, *args, **kwargs):<br/>    if keras_2:<br/>        return model.fit(x, y, *args, epochs=nb_epoch, **kwargs)<br/>    else:<br/>        return model.fit(x, y, *args, nb_epoch=nb_epoch, **kwargs)<br/><br/>def l1l2(l1=0, l2=0):<br/>    if keras_2:<br/>        return keras.regularizers.L1L2(l1, l2)<br/>    else:<br/>        return keras.regularizers.l1l2(l1, l2)<br/><br/>def Dense(units, W_regularizer=None, W_initializer='glorot_uniform', **kwargs):<br/>    if keras_2:<br/>        return keras.layers.Dense(units, kernel_regularizer=W_regularizer, kernel_initializer=W_initializer, **kwargs)<br/>    else:<br/>        return keras.layers.Dense(units, W_regularizer=W_regularizer, <br/>                                  init=W_initializer, **kwargs)<br/><br/>def BatchNormalization(mode=0, **kwargs):<br/>    if keras_2:<br/>        return keras.layers.BatchNormalization(**kwargs)<br/>    else:<br/>        return keras.layers.BatchNormalization(mode=mode, **kwargs)<br/><br/>def Convolution2D(units, w, h, W_regularizer=None, W_initializer='glorot_uniform', border_mode='same', **kwargs):<br/>    if keras_2:<br/>        return keras.layers.Conv2D(units, (w, h), padding=border_mode,<br/>                                   kernel_regularizer=W_regularizer,<br/>                                   kernel_initializer=W_initializer,<br/>                                   **kwargs)<br/>    else:<br/>        return keras.layers.Conv2D(units, w, h, border_mode=border_mode, W_regularizer=W_regularizer, init=W_initializer, **kwargs)<br/><br/>def AveragePooling2D(pool_size, border_mode='valid', **kwargs):<br/>    if keras_2:<br/>        return keras.layers.AveragePooling2D(pool_size=pool_size, <br/>                                             padding=border_mode, **kwargs)<br/>    else:<br/>        return keras.layers.AveragePooling2D(pool_size=pool_size, <br/>                                             border_mode=border_mode, **kwargs)
</pre>
<p>There are also a number of breaking changes. In particular:</p>
<ul>
<li>The maxout dense, time distributed dense, and highway legacy layers have been removed</li>
<li>The batch normalization layer no longer supports the mode argument, because Keras internals have changed</li>
<li>Custom layers have to be updated</li>
<li>Any undocumented Keras functionality could have broken</li>
</ul>
<p>In addition, the Keras code base has been instrumented to detect the use of the Keras 1.x API calls and show deprecation warnings that show how to change the call to conform to the Keras 2 API. If you have some volume of Keras 1.x code already and are hesitant to try Keras 2 because of the fear of non-breaking changes, these deprecation warnings from the Keras 2 code base can be very helpful in making the transition.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>