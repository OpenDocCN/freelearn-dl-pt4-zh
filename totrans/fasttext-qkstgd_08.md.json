["```py\ndef normalize(s):\n    \"\"\"\n    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n    \"\"\"\n    s = s.lower()\n    # Replace ips\n    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n    # Isolate punctuation\n    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n    # Remove some special characters\n    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n    # Replace numbers and symbols with language\n    s = s.replace('&', ' and ')\n    s = s.replace('@', ' at ')\n    s = s.replace('0', ' zero ')\n    s = s.replace('1', ' one ')\n    s = s.replace('2', ' two ')\n    s = s.replace('3', ' three ')\n    s = s.replace('4', ' four ')\n    s = s.replace('5', ' five ')\n    s = s.replace('6', ' six ')\n    s = s.replace('7', ' seven ')\n    s = s.replace('8', ' eight ')\n    s = s.replace('9', ' nine ')\n    return s\n```", "```py\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain['Text'] = train['Text'].fillna('_empty_')\ntest['Text'] = test['Text'].fillna('_empty_')\n```", "```py\nsg_model = fastText.train_unsupervised(input='data.txt', model='skipgram')\n```", "```py\n./fasttext skipgram -input data.train -output model\n```", "```py\ncbow_model = fastText.train_unsupervised(input='data.txt', model='cbow')\n```", "```py\n./fasttext cbow -input data.train -output model\n```", "```py\nsg_model.save_model(\"sg_model.bin\")\n```", "```py\nsg_model = fastText.train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss=\"ns\", bucket=2000000, thread=12, lrUpdateRate=100, t=1e-4, label=\"__label__\", verbose=2, pretrainedVectors=\"\")\n```", "```py\n>>> model.get_word_vector('targetword')\narray([ 0.09973086,  ...  0.14613365], dtype=float32)\n```", "```py\n>>> nn(sg_model, ['dog', 'pizza', 'hungry'], k=5)\nwords similar to dog:\ndogs\npup\ntreats\npuppy\ndogie\n#########################################\nwords similar to pizza:\npizza;\npizza\"\npizzas\n\"pizza\nbread\n#########################################\nwords similar to hungry:\nhungry\";\nhungrygirl\n>hungry\nhungry-girl\nhungries\n#########################################\n```", "```py\ndef similarity(v1, v2):\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n    return np.dot(v1, v2) / n1 / n2\n\nv1 = sg_model.get_word_vector('drink')\nv2 = sg_model.get_word_vector('drinks')\nprint(similarity(v1, v2))\n```", "```py\n>>> dataset, corr, oov = compute_similarity('data/rw/rw.txt', sg_model)\n>>> print(\"{0:20s}: {1:2.0f}  (OOV: {2:2.0f}%)\".format(dataset, corr, 0))\nrw.txt              : 32  (OOV:  0%)\n```", "```py\n>>> su_model = fastText.train_supervised(input=train_file, epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1)\n```", "```py\n$ ./fasttext supervised -input train_file -output su_model\n```", "```py\n>>> su_model = fastText.train_supervised(input=train_file, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss=\"softmax\", bucket=2000000, thread=12,  lrUpdateRate=100, t=1e-4, label=\"__label__\", verbose=2, pretrainedVectors=\"\")\n```", "```py\n>>> su_model.save_model(\"su_model.bin\")\n```", "```py\n>>> su_model.get_word_vector('restaurant')\narray([ 0.9739366 , ..., -0.17078586], dtype=float32)\n>>> su_model.get_sentence_vector('I love this restaurant')\narray([ 0.31301185,  ... , -0.21543942], dtype=float32)\n>>> su_model.predict('I love this restaurant')\n(('__label__5',), array([1.00001001]))\n```", "```py\n>>> su_model.predict(\"I love this restaurant\", k=3)\n```", "```py\n(('__label__5', '__label__2', '__label__3'),\n array([1.00001001e+00, 1.00000034e-05, 1.00000034e-05]))\n```", "```py\n$ echo \"I love this restaurant\" | ./fasttext predict-prob su_model.bin - 3\n__label__5 1.00001 __label__2 1e-05 __label__3 1e-05\n```", "```py\n>>> n, p, r = su_model.test(test_file, 5)\n>>> print(\"N\\t\" + str(n))\n>>> print(\"P@{}\\t{:.3f}\".format(5, p))\n>>> print(\"R@{}\\t{:.3f}\".format(5, r))\nN 113691\nP@5 0.200\nR@5 1.000\n```", "```py\nIn [1]: from gensim.models.fasttext import FastText\n   ...: from gensim.corpora import Dictionary\n   ...: import pandas as pd\n   ...: import re\n   ...: from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n   ...: import numpy as np\n   ...: \n\nIn [2]: df_fake = pd.read_csv('fake.csv')\n   ...: df_fake[['title', 'text', 'language']].head()\n   ...: df_fake = df_fake.loc[(pd.notnull(df_fake.text)) & (df_fake.language == 'english')]\n   ...: \n\nIn [3]: # remove stopwords and punctuations\n   ...: def preprocess(row):\n   ...: return strip_punctuation(remove_stopwords(row.lower()))\n   ...: \n   ...: df_fake['text'] = df_fake['text'].apply(preprocess)\n   ...: \n\nIn [4]: # Convert data to required input format by LDA\n   ...: texts = []\n   ...: for line in df_fake.text:\n   ...: lowered = line.lower()\n   ...: words = re.findall(r'\\w+', lowered)\n   ...: texts.append(words)\n   ...: \n```", "```py\n>>> from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n\n>>> # Set FastText home to the path to the FastText executable\n>>> ft_home = '/usr/local/bin/fasttext'\n\n>>> # train the model\n>>> model_wrapper = FT_wrapper.train(ft_home, lee_train_file)\n\n>>> print(model_wrapper)\n```", "```py\nIn [1]: \n   ...: import gensim\n   ...: import os\n   ...: from gensim.models.word2vec import LineSentence\n   ...: from gensim.models.fasttext import FastText\n\nIn [2]: # Set file names for train and test data\n   ...: data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n   ...: lee_train_file = data_dir + 'lee_background.cor'\n   ...: lee_data = LineSentence(lee_train_file)\n\nIn [3]: model = FastText(size=100)\n\nIn [4]: model.build_vocab(lee_data)\n\nIn [5]: # train the model\n   ...: model.train(lee_data, total_examples=model.corpus_count, epochs=model.epochs)\n   ...: print(model)\nFastText(vocab=1762, size=100, alpha=0.025)\n```", "```py\nIn [6]: # saving a model trained via Gensim's fastText implementation\n   ...: model.save('saved_model_gensim')\n   ...: loaded_model = FastText.load('saved_model_gensim')\n   ...: print(loaded_model)\n   ...: \nFastText(vocab=1762, size=100, alpha=0.025)\n\nIn [7]: import os; print(os.path.exists('saved_model_gensim'))\nTrue\n```", "```py\nIn [1]: from gensim.models.fasttext import FastText\nIn [2]: modelpath = \"wiki.simple.bin\"\nIn [3]: model = FastText.load_fasttext_format(modelpath)\nIn [4]: print(model)\nFastText(vocab=111051, size=300, alpha=0.025)\n```", "```py\nIn [1]: import gensim\n   ...: import os\n   ...: from gensim.models.word2vec import LineSentence\n   ...: from gensim.models.fasttext import FastText\n   ...: \n\nIn [2]: # Set file names for train and test data\n   ...: data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n   ...: lee_train_file = data_dir + 'lee_background.cor'\n   ...: lee_data = LineSentence(lee_train_file)\n   ...: \n```", "```py\n\nIn [3]: model = FastText(size=100)\n\nIn [4]: # build the vocabulary\n   ...: model.build_vocab(lee_data)\n\nIn [5]: # train the model\n   ...: model.train(lee_data, total_examples=model.corpus_count, epochs=model.epochs)\n\nIn [6]: print('night' in model.wv.vocab)\nTrue\n\nIn [7]: print('nights' in model.wv.vocab) # this is not present\nFalse\n\nIn [8]: print(model.wv['night'])\n[-0.02308581 ... 0.15816787]\n\nIn [9]: print(model.wv['nights'])\n[-0.02073629 ... 0.1486301 ]\n\nIn [10]: # Raises a KeyError since none of the character ngrams of the word `axe` are present in the training data\n    ...: model.wv['axe']\n---------------------------------------------------------------------------\nKeyError Traceback (most recent call last)\n<ipython-input-10-902d47f807a0> in <module>()\n      1 # Raises a KeyError since none of the character ngrams of the word `axe` are present in the training data\n----> 2 model.wv['axe']\n\n...\n\nKeyError: 'all ngrams for word axe absent from model'\n```", "```py\nIn []: model.wv.similarity('night', 'nights')\nOut[]: 0.9999931241743173\n```", "```py\nIn []: model.wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\nOut[]: 0.6041413398970296\n\nIn []: model.wv.n_similarity('Obama speaks to the media in Illinois'.lower().split(), 'The president greets the press in Chicago'.lower().spl\n    ...: it())\nOut[]: 0.7653119647179297\n```", "```py\nIn []: model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())\nOut[]: 'cereal'\n```", "```py\nIn []: model.wv.most_similar('food')\nOut[]: \n[('foods', 0.6859725713729858),\n ('foodstuffs', 0.679445743560791),\n ('seafood', 0.6695178151130676),\n ('eat', 0.5922832489013672),\n ('meals', 0.5820232629776001),\n ('meat', 0.5773770213127136),\n ('eaten', 0.5611693263053894),\n ('nutritious', 0.5602636337280273),\n ('snacks', 0.5574883818626404),\n ('cooked', 0.5470614433288574)]\n=\n```", "```py\nIn []: model.wv.evaluate_word_pairs('wordsim353.tsv')\nOut[]: \n((0.6645467362164186, 2.4591009701535706e-46),\n SpearmanrResult(correlation=0.7179229895090848, pvalue=3.58449522917263e-57),\n 0.0)\n```", "```py\nIn []: model.wv.most_similar(positive=['story', 'dove'], negative=['stories'])  # Vector('story') - Vector('stories') + Vector('dove')\nOut[]: \n[('doves', 0.5111404657363892),\n ('dovepaw', 0.5014846324920654),\n ('turtledove', 0.4434218406677246),\n ('dovecote', 0.4430897831916809),\n ('warbler', 0.43106675148010254),\n ('warble', 0.40401384234428406),\n ('asshole', 0.4017521142959595),\n ('dovre', 0.39799436926841736),\n ('nothofagus', 0.389825701713562),\n ('moriarty', 0.388924241065979)]\n```", "```py\nmodel.accuracy(\"question-words.txt\")\n```", "```py\nIn []: sentence_obama = 'Obama speaks to the media in Illinois'\n   ...: sentence_president = 'The president greets the press in Chicago'\n   ...: sentence_obama = sentence_obama.lower().split()\n   ...: sentence_president = sentence_president.lower().split()\n   ...: \n\nIn []: # Remove stopwords.\n   ...: stop_words = stopwords.words('english')\n   ...: sentence_obama = [w for w in sentence_obama if w not in stop_words]\n   ...: sentence_president = [w for w in sentence_president if w not in stop_words]\n```", "```py\n\nIn []: distance = model.wv.wmdistance(sentence_obama, sentence_president)\n   ...: print('distance = %.4f' % distance)\ndistance = 4.9691\n```", "```py\nfrom gensim.similarities import WmdSimilarity\nnum_best = 10\ninstance = WmdSimilarity(wmd_corpus, model, num_best=10)\n```", "```py\nfrom gensim.test.utils import common_texts as sentences\nfrom gensim.models.callbacks import CallbackAny2Vec\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import get_tmpfile\n\nclass EpochSaver(CallbackAny2Vec):\n    \"Callback to save model after every epoch\"\n    def __init__(self, path_prefix):\n        self.path_prefix = path_prefix\n        self.epoch = 0\n    def on_epoch_end(self, model):\n        output_path = '{}_epoch{}.model'.format(self.path_prefix, self.epoch)\n        print(\"Save model to {}\".format(output_path))\n        model.save(output_path)\n        self.epoch += 1\n```", "```py\n# to save the similarity scores\nsimilarity = []\n\nclass EpochLogger(CallbackAny2Vec):\n    \"Callback to log information about training\"\n    def __init__(self):\n        self.epoch = 0\n    def on_epoch_begin(self, model):\n        print(\"Epoch #{} start\".format(self.epoch))\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        self.epoch += 1\n    def on_batch_begin(self, model):\n        similarity.append(model.wv.similarity('woman', 'man'))\n```", "```py\nimport gensim\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.fasttext import FastText\n\n# Set file names for train and test data\nlee_train_file = './gensim/gensim/test/test_data/lee_background.cor'\nlee_data = LineSentence(lee_train_file)\n\nmodel_gensim = FastText(size=100)\n\n# build the vocabulary\nmodel_gensim.build_vocab(lee_data)\n\n# instantiate the callbacks\nepoch_saver = EpochSaver(get_tmpfile(\"temporary_model\"))\nepoch_logger = EpochLogger()\n\n# train the model\nmodel_gensim.train(lee_data, \n                   total_examples=model_gensim.corpus_count,\n                   epochs=model_gensim.epochs,\n                   callbacks=[epoch_saver, epoch_logger])\n\nprint(model_gensim)\n```", "```py\n$ pip install visdom\n$ python -m visdom.server\n```", "```py\nimport visdom\nvis = visdom.Visdom()\n\ntrace = dict(x=list(range(len(similarity))), y=similarity, mode=\"markers+lines\", type='custom',\n             marker={'color': 'red', 'symbol': 104, 'size': \"10\"},\n             text=[\"one\", \"two\", \"three\"], name='1st Trace')\nlayout = dict(title=\"First Plot\", xaxis={'title': 'x1'}, yaxis={'title': 'x2'})\n\nvis._send({'data': [trace], 'layout': layout, 'win': 'mywin'})\n```", "```py\ntransmat = translation_matrix.TranslationMatrix(source_word_vec, target_word_vec, word_pair)\ntransmat.train(word_pair)\nprint (\"the shape of translation matrix is: \", transmat.translation_matrix.shape)\n```", "```py\n# The pair is in the form of (English, Italian), we can see whether the translated word is correct\nwords = [(\"one\", \"uno\"), (\"two\", \"due\"), (\"three\", \"tre\"), (\"four\", \"quattro\"), (\"five\", \"cinque\")]\nsource_word, target_word = zip(*words)\ntranslated_word = transmat.translate(source_word, 5)\nfor k, v in translated_word.iteritems():\n    print (\"word \", k, \" and translated word\", v)\n```", "```py\n('word ', 'one', ' and translated word', [u'solo', u'due', u'tre', u'cinque', u'quattro'])\n('word ', 'two', ' and translated word', [u'due', u'tre', u'quattro', u'cinque', u'otto'])\n('word ', 'three', ' and translated word', [u'tre', u'quattro', u'due', u'cinque', u'sette'])\n('word ', 'four', ' and translated word', [u'tre', u'quattro', u'cinque', u'due', u'sette'])\n('word ', 'five', ' and translated word', [u'cinque', u'tre', u'quattro', u'otto', u'dieci'])\n```"]