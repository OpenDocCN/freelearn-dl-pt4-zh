<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Up and Running with Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">What will <strong>artificial intelligence</strong> (<strong>AI</strong>) look like in the future? As  applications of AI algorithms and software become more prominent, it is a question that should interest many. Researchers and practitioners of AI face further relevant questions; how will we realize what we envision and solve known problems? What kinds of innovations and algorithms are yet to be developed? Several subfields in machine learning display<span> great promise toward answering many of our questions. In this book, we shine the spotlight on reinforcement learning, one such, area and perhaps one of the most exciting topics in machine learning.</span></p>
<p>Reinforcement learning is motivated by the objective to learn from the environment by interacting with it. Imagine an infant and how it goes about in its environment. By moving around and acting upon its surroundings, the infant learns about physical phenomena, causal relationships, and various attributes and properties of the objects he or she interacts with. The infant's learning is often motivated by a desire to accomplish some objective, such as playing with surrounding objects or satiating some spark of curiosity. In reinforcement learning, we pursue a similar endeavor; we take a computational approach toward learning about the environment. In other words, our goal is to design algorithms that learn through their interactions with the environment in order to accomplish a task.</p>
<p>What use do such algorithms provide? By having a generalized learning algorithm, we can offer effective solutions to several real-world problems. A prominent example is the use of reinforcement learning algorithms to drive cars autonomously. While not fully realized, such use cases would provide great benefits to society, for reinforcement learning algorithms have empirically proven their ability to surpass human-level performance in several tasks. One watershed moment occurred in 2016 when DeepMind's AlphaGo program defeated 18-time Go world champion Lee Sedol four games to one. AlphaGo was essentially able to learn and surpass three millennia of Go wisdom cultivated by humans in a matter of months. Recently, reinforcement learning algorithms have been shown to be effective in playing more complex, real-time multi-agent games such as Dota. The same algorithms that power these game-playing algorithms have also succeeded in controlling robotic arms to pick up objects and navigating drones through mazes. These examples suggest not only what these algorithms are capable of, but also what they can potentially accomplish down the road.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to this book</h1>
                </header>
            
            <article>
                
<p>This book offers a practical guide for those eager to learn about reinforcement learning. We will take a hands-on approach toward learning about reinforcement learning by going through numerous examples of algorithms and their applications. Each chapter focuses on a particular use case and introduces reinforcement learning algorithms that are used to solve the given problem. Some of these use cases rely on state-of-the-art algorithms; hence through this book, we will learn about and implement some of the best-performing algorithms and techniques in the industry.</p>
<p>The projects increase in difficulty/complexity as you go through the book. The following table describes what you will learn from each chapter:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Chapter name</strong></td>
<td><strong>The use case/problem</strong></td>
<td><strong>Concepts/algorithms/technologies discussed and used</strong></td>
</tr>
<tr>
<td><em>Balancing Cart Pole</em></td>
<td>Control horizontal movement of a cart to balance a vertical bar</td>
<td>OpenAI Gym framework, Q-Learning</td>
</tr>
<tr>
<td><em>Playing Atari Games</em></td>
<td>Play various Atari games at human-level proficiency</td>
<td>Deep Q-Networks</td>
</tr>
<tr>
<td><em>Simulating Control Tasks</em></td>
<td>Control agents in a continuous action space as opposed to a discrete one</td>
<td><strong>Deterministic policy gradients</strong> (<strong>DPG</strong>), <strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>), multi-tasking</td>
</tr>
<tr>
<td><em>Building Virtual Worlds in Minecraft</em></td>
<td>Navigate a character in the virtual world of Minecraft</td>
<td>Asynchronous Advantage Actor-Critic (<strong>A3C</strong>)</td>
</tr>
<tr>
<td><em>Learning to Play Go</em></td>
<td>Go, one of the oldest and most complex board games in the world</td>
<td>Monte Carlo tree search, policy and value networks</td>
</tr>
<tr>
<td><em>Creating a Chatbot</em></td>
<td>Generating natural language in a conversational setting</td>
<td>Policy gradient methods, <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>)</td>
</tr>
<tr>
<td><em>Auto Generating a Deep Learning Image Classifier</em></td>
<td>Create an agent that generates neural networks to solve a given task</td>
<td>
<p class="mce-root">Recurrent neural networks, policy gradient methods (REINFORCE)</p>
</td>
</tr>
<tr>
<td><em>Predicting Future Stock Prices</em></td>
<td>Predict stock prices and make buy and sell decisions</td>
<td>Actor-Critic methods, time-series analysis, experience replay</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Expectations</h1>
                </header>
            
            <article>
                
<p>This book is best suited for the reader who:</p>
<ul>
<li>Has intermediate proficiency in Python</li>
<li>Possesses a basic understanding of machine learning and deep learning, especially for the following topics:
<ul>
<li>Neural networks</li>
<li>Backpropagation</li>
<li>Convolution</li>
<li>Techniques for better generalization and reduced overfitting</li>
</ul>
</li>
<li>Enjoys a hands-on, practical approach toward learning</li>
</ul>
<p>Since this book serves as a practical introduction to the field, we try to keep theoretical content to a minimum. However, it is advisable for the reader to have basic knowledge of some of the fundamental mathematical and statistical concepts on which the field of machine learning depends. These include the following:</p>
<ul>
<li>Calculus (single and multivariate)</li>
<li>Linear algebra</li>
<li>Probability theory</li>
<li>Graph theory</li>
</ul>
<p>Having some experience with these subjects would greatly assist the reader in understanding the concepts and algorithms we will cover throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hardware and software requirements</h1>
                </header>
            
            <article>
                
<p>The ensuing chapters will require you to implement various reinforcement learning algorithms. Hence a proper development environment is necessary for a smooth learning journey. In particular, you should have the following:</p>
<ul>
<li>A computer running either macOS or the Linux operating system (for those on Windows, try setting up a Virtual Machine with a Linux image)</li>
<li>A stable internet connection</li>
<li>A GPU (preferably)</li>
</ul>
<p>We will exclusively use the Python programming language to implement our reinforcement learning and deep learning algorithms. Moreover, we will be using Python 3.6. A list of libraries we will be using can be found on the official GitHub repository, located at (<a href="https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects">https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects</a>). You will also find the implementations of every algorithm we will cover in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing packages</h1>
                </header>
            
            <article>
                
<p>Assuming you have a working Python installation, you can install all the required packages using the <kbd>requirements.txt</kbd> file found in our repository. We also recommend you create a <kbd>virtualenv</kbd> to isolate your development environment from your main OS system. The following steps will help you construct an environment and install the packages:</p>
<pre># Install virtualenv using pip<br/>$ pip install virtualenv<br/><br/># Create a virtualenv<br/>$ virtualenv rl_projects<br/><br/># Activate virtualenv<br/>$ source rl_projects/bin/activate<br/><br/># cd into the directory with our requirements.txt<br/>(rl_projects) $ cd /path/to/requirements.txt<br/><br/># pip install the required packages<br/>(rl_projects) $ pip install -r requirements.txt</pre>
<p>And now you are all set and ready to start! The next few sections of this chapter will introduce the field of reinforcement learning and will also provide a refresher on deep learning.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is reinforcement learning?</h1>
                </header>
            
            <article>
                
<p>Our journey begins with understanding what reinforcement learning is about. Those who are familiar with machine learning may be aware of several learning paradigms, namely <strong>supervised learning</strong> and <strong>unsupervised learning</strong>. In supervised learning, a machine learning model has a supervisor that gives the ground truth for every data point. The model learns by minimizing the distance between its own prediction and the ground truth. The dataset is thus required to have an annotation for each data point, for example, each image of a dog and a cat would have its respective label. In unsupervised learning, the model does not have access to the ground truths of the data and thus has to learn about the distribution and patterns of the data without them.</p>
<p>In reinforcement learning, the agent refers to the model/algorithm that learns to complete a particular task. The agent learns primarily by receiving <strong>reward signals</strong>, which is a scalar indication of how well the agent is performing a task.</p>
<p>Suppose we have an agent that is tasked with controlling a robot's walking movement; the agent would receive positive rewards for successfully walking toward a destination and negative rewards for falling/failing to make progress.</p>
<p>Moreover, unlike in supervised learning, these reward signals are not given to the model immediately; rather, they are returned as a consequence of a sequence of <strong>actions</strong> that the agent makes. Actions are simply the things an agent can do within its <strong>environment</strong>. The environment refers to the world in which the agent resides and is primarily responsible for returning reward signals to the agent. An agent's actions are usually conditioned on what the agent perceives from the environment. What the agent perceives is referred to as the <strong>observation</strong> or the <strong>state</strong> of the environment. What further distinguishes reinforcement learning from other paradigms is that the actions of the agent can alter the environment and its subsequent responses.</p>
<p>For example, suppose an agent is tasked with playing Space Invaders, the popular Atari 2600 arcade game. The environment is the game itself, along with the logic upon which it runs. During the game, the agent queries the environment to make an observation. The observation is simply an array of the (210, 160, 3) shape, which is the screen of the game that displays the agent's ship, the enemies, the score, and any projectiles. Based on this observation, the agent makes some actions, which can include moving left or right, shooting a laser, or doing nothing. The environment receives the agent's action as input and makes any necessary updates to the state.</p>
<p>For instance, if a laser touches an enemy ship, it is removed from the game. If the agent decides to simply move to the left, the game updates the agent's coordinates accordingly. This process repeats until a <strong>terminal state</strong><em>,</em> a state that represents the end of the sequence, is reached. In Space Invaders, the terminal state corresponds to when the agent's ship is destroyed, and the game subsequently returns the score that it keeps track of, a value that is calculated based on the number of enemy ships the agent successfully destroys.</p>
<div class="packt_infobox">Note that some environments do not have terminal states, such as the stock market. These environments keep running for as long as they exist.</div>
<p>Let's recap the terms we have learned about so far:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 15.2265%"><strong>Term</strong></td>
<td style="width: 44.7735%"><strong>Description</strong></td>
<td style="width: 36%"><strong>Examples</strong></td>
</tr>
<tr>
<td style="width: 15.2265%">Agent</td>
<td style="width: 44.7735%">A model/algorithm that is tasked with learning to accomplish a task.</td>
<td style="width: 36%">Self-driving cars, walking robots, video game players</td>
</tr>
<tr>
<td style="width: 15.2265%">Environment</td>
<td style="width: 44.7735%">The world in which the agent acts. It is responsible for controlling what the agent perceives and providing feedback on how well the agent is performing a particular task.</td>
<td style="width: 36%">The road on which a car drives, a video game, the stock market</td>
</tr>
<tr>
<td style="width: 15.2265%">Action</td>
<td style="width: 44.7735%">A decision the agent makes in an environment, usually dependent on what the agent perceives.</td>
<td style="width: 36%">Steering a car, buying or selling a particular stock, shooting a laser from the spaceship the agent is controlling</td>
</tr>
<tr>
<td style="width: 15.2265%">Reward signal</td>
<td style="width: 44.7735%">A scalar indication of how well the agent is performing a particular task.</td>
<td style="width: 36%">Space Invaders score, return on investment for some stock, distance covered by a robot learning to walk</td>
</tr>
<tr>
<td style="width: 15.2265%">Observation/state</td>
<td style="width: 44.7735%">A description of the environment as can be perceived by the agent.</td>
<td style="width: 36%">Video from a dashboard camera, the screen of the game, stock market statistics</td>
</tr>
<tr>
<td style="width: 15.2265%">Terminal state</td>
<td style="width: 44.7735%">A state at which no further actions can be made by the agent.</td>
<td style="width: 36%">Reaching the end of a maze, the ship in Space Invaders getting destroyed</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Put formally, at a given timestep, <kbd>t</kbd>, the following happens for an agent,<span> </span><kbd>P</kbd><span>, </span>and environment, <kbd>E</kbd>:</p>
<pre>- P queries E for some observation <img src="assets/de495866-56f1-4afd-b742-4275dc6a0bdc.png" style="width:1.00em;height:1.00em;"/><br/>- P decides to take action <img src="assets/55b392b2-0e4b-4e30-a422-d9278a7f2037.png" style="width:1.08em;height:0.92em;"/> based on observation <img src="assets/1b06b648-f5fc-4900-975a-468912d17740.png" style="width:0.83em;height:0.83em;"/><br/>- E receives <img src="assets/0f644930-15c8-4726-b767-fc6983fe6a51.png" style="width:0.92em;height:0.83em;"/> and returns reward <img src="assets/e1acf1ef-46fa-4b05-b849-97d1bceffa3d.png" style="width:1.00em;height:1.00em;"/> based on the action<br/>- P receives <img src="assets/c9fbe338-9808-47dc-a09e-c75f8368e75a.png" style="width:0.83em;height:0.83em;"/><br/>- E updates <img src="assets/238cbf92-17bd-4817-9d9f-afa83885ef03.png" style="width:0.92em;height:0.92em;"/> to <img src="assets/7a254a41-0dbf-4ad8-867d-a1e9251b7df8.png" style="width:2.17em;height:1.08em;"/> based on <img class="fm-editor-equation" src="assets/dd919f2d-4aeb-40d2-929a-218508c82135.png" style="width:1.08em;height:0.92em;"/> and other factors</pre>
<p>How does the environment compute<img class="fm-editor-equation" src="assets/c1cbd545-2753-478c-866f-ed057127bf3d.png" style="width:1.08em;height:1.08em;"/>and <img class="fm-editor-equation" src="assets/8f52597c-a3e6-43ed-baaa-1e849131b1fe.png" style="width:2.17em;height:1.08em;"/>? The environment usually has its own algorithm that computes these values based on numerous input/factors, including what action the agent takes.</p>
<p>Sometimes, the environment is composed of multiple agents that try to maximize their own rewards. The way gravity acts upon a ball that we drop from a height is a good representation of how the environment works; just like how our surroundings obey the laws of physics, the environment has some internal mechanism for computing rewards and the next state. This internal mechanism is usually hidden to the agent, and thus our job is to build agents that can learn to do a good job at their respective tasks, despite this uncertainty.</p>
<p>In the following sections, we will discuss in more detail the main protagonist of every reinforcement learning problem—the agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The agent</h1>
                </header>
            
            <article>
                
<p>The goal of a reinforcement learning agent is to learn to perform a task well in an environment. Mathematically, this means to maximize the cumulative reward, <em>R</em>, which can be expressed in the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/34073884-68e7-4113-9acc-2d51f9f4216f.png" style="width:12.33em;height:1.33em;"/></p>
<p>We are simply calculating a weighted sum of the reward received at each timestep.<img class="fm-editor-equation" src="assets/349f3223-b4de-429a-a18a-8ec1a2f14483.png" style="width:0.50em;height:0.75em;"/>is called the <strong>discount factor</strong>, which is a scalar value between 0 and 1. The idea is that the later a reward comes, the less valuable it becomes. This reflects our perspectives on rewards as well; that we'd rather receive $100 now rather than a year later shows how the same reward signal can be valued differently based on its proximity to the present.</p>
<p>Because the mechanics of the environment are not fully observable or known to the agent, it must gain information by performing an action and observing how the environment reacts to it. This is much like how humans learn to perform certain tasks as well.</p>
<p class="mce-root"/>
<p>Suppose we are learning to play chess. While we don't have all the possible moves committed to memory or know exactly how an opponent will play, we are able to improve our proficiency over time. In particular, we are able to become proficient in the following:</p>
<ul>
<li>Learning how to react to a move made by the opponent</li>
<li>Assessing how good of a position we are in to win the game</li>
<li>Predicting what the opponent will do next and using that prediction to decide on a move</li>
<li>Understanding how others would play in a similar situation</li>
</ul>
<p>In fact, reinforcement learning agents can learn to do similar things. In particular, an agent can be composed of multiple functions and models to assist its decision-making. There are three main components that an agent can have: the policy, the value function, and the model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy</h1>
                </header>
            
            <article>
                
<p>A policy is an algorithm or a set of rules that describe how an agent makes its decisions. An example policy can be the strategy an investor uses to trade stocks, where the investor buys a stock when its price goes down and sells the stock when the price goes up.</p>
<p>More formally, a policy is a function, usually denoted as <img class="fm-editor-equation" src="assets/82dc222a-f23e-4460-ac00-c4ea31ac3cd5.png" style="width:0.83em;height:0.83em;"/>, that maps a state, <img class="fm-editor-equation" src="assets/170b9e93-b340-45e9-988b-09c9e07e7fce.png" style="width:1.08em;height:1.08em;"/>, to an action, <img class="fm-editor-equation" src="assets/db64ce78-6f6b-4bd8-913f-f1e4ec162373.png" style="width:0.92em;height:0.83em;"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/67683436-16d2-412c-ade2-3dcd805dbb1f.png" style="width:5.58em;height:1.50em;"/></p>
<p>This means that an agent decides its action given its current state. This function can represent anything, as long as it can receive a state as input and output an action, be it a table, graph, or machine learning classifier.</p>
<p>For example, suppose we have an agent that is supposed to navigate a maze. We shall further assume that the agent knows what the maze looks like; the following is how the agent's policy can be represented:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-609 image-border" src="assets/b35ccef7-dfd6-4d7e-8f9d-38584fd4bf87.png" style="width:26.83em;height:23.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: A maze where each arrow indicates where an agent would go next</div>
<p>Each white square in this maze represents a state the agent can be in. Each blue arrow refers to the action an agent would take in the corresponding square. This essentially represents the agent's policy for this maze. Moreover, this can also be regarded as a deterministic policy, for the mapping from the state to the action is deterministic. This is in contrast to a stochastic policy, where a policy would output a probability distribution over the possible actions given some state:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55a6880b-56ca-4c80-b1c8-22b60a84e942.png" style="width:10.50em;height:1.33em;"/></p>
<p>Here,<img class="fm-editor-equation" src="assets/fbac4100-413c-4698-b8dc-bee15f1dc709.png" style="width:3.83em;height:1.17em;"/>is a normalized probability vector over all the possible actions, as shown in the following example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-610 image-border" src="assets/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png" style="width:29.33em;height:11.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: A policy mapping the game state (the screen) to actions (probabilities)</div>
<p>The agent playing the game of Breakout has a policy that takes the screen of the game as input and returns a probability for each possible action.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value function</h1>
                </header>
            
            <article>
                
<p>The second component an agent can have is called the <strong>value function</strong>. As mentioned previously, it is useful to assess your position, good or bad, in a given state. In a game of chess, a player would like to know the likelihood that they are going to win in a board state. An agent navigating a maze would like to know how close it is to the destination. The value function serves this purpose; it predicts the expected future reward an agent would receive in a given state. In other words, it measures whether a given state is desirable for the agent. More formally, the value function takes a state and a policy as input and returns a scalar value representing the expected cumulative reward:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/37ff7369-c631-4dcd-ac2a-3a8e563ae28c.png" style="width:24.75em;height:1.58em;"/></p>
<p>Take our maze example, and suppose the agent receives a reward of -1 for every step it takes. The agent's goal is to finish the maze in the smallest number of steps possible. The value of each state can be represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-611 image-border" src="assets/a0d2e053-a325-409f-b5b6-43627cd96e45.png" style="width:28.83em;height:25.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: A maze where each square indicates the value of being in the state</div>
<p>Each square basically represents the number of steps it takes to get to the end of the maze. As you can see, the smallest number of steps required to reach the goal is 15.</p>
<p>How can the value function help an agent perform a task well, other than informing us of how desirable a given state is? As we will see in the following sections, value functions play an integral role in predicting how well a sequence of actions will do even before the agent performs them. This is similar to chess players imagining how well a sequence of future actions will do in improving his or her  chances of winning. To do this, the agent also needs to have an understanding of how the environment works. This is where the third component of an agent, the <strong>model</strong>, becomes relevant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we discussed how the environment is not fully known to the agent. In other words, the agent usually does not have an idea of how the internal algorithm of the environment looks. The agent thus needs to interact with it to gain information and learn how to maximize its expected cumulative reward. However, it is possible for the agent to have an internal replica, or a model, of the environment. The agent can use the model to predict how the environment would react to some action in a given state. A model of the stock market, for example, is tasked with predicting what the prices will look like in the future. If the model is accurate, the agent can then use its value function to assess how desirable future states look. More formally, a model can be denoted as a function, <img class="fm-editor-equation" src="assets/9cbc6ea0-aae1-46ea-b5f8-e0d8ac439aa0.png" style="width:1.33em;height:1.08em;"/>, that predicts the probability of the next state given the current state and an action:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5605d9cb-dc64-4b04-8515-c60b34e88486.png" style="width:14.67em;height:1.50em;"/></p>
<p>In other scenarios, the model of the environment can be used to enumerate possible future states. This is commonly used in turn-based games, such as chess and tic-tac-toe, where the rules and scope of possible actions are clearly defined. Trees are often used to illustrate the possible sequence of actions and states in turn-based games:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-612 image-border" src="assets/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png" style="width:31.17em;height:24.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: A model using its value function to assess possible moves</div>
<p>In the preceding example of the tic-tac-toe game,<img class="fm-editor-equation" src="assets/357a67fa-d45f-4755-90ce-03d347133091.png" style="width:4.75em;height:1.42em;"/>denotes the possible states that taking the<img class="fm-editor-equation" src="assets/e8560c16-362b-4411-b69f-50eddf987dba.png" style="width:1.17em;height:1.00em;"/>action (represented as the shaded circle) could yield in a given state, <img class="fm-editor-equation" src="assets/02d85b2c-a516-4199-8255-d805511504ea.png" style="width:1.08em;height:1.08em;"/>. Moreover, we can calculate the value of each state using the agent's value function. The middle and bottom states would yield a high value since the agent would be one step away from victory, whereas  the top state would yield a medium value since the agent needs to prevent the opponent from winning.</p>
<p>Let's review the terms we have covered so far:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 16.031%"><strong>Term</strong></td>
<td style="width: 33.969%"><strong>Description</strong></td>
<td style="width: 45%"><strong>What does it output?</strong></td>
</tr>
<tr>
<td style="width: 16.031%">Policy</td>
<td style="width: 33.969%">The algorithm or function that outputs decisions the agent makes</td>
<td style="width: 45%">A scalar/single decision (deterministic policy) or a vector of probabilities over possible actions (stochastic policy)</td>
</tr>
<tr>
<td style="width: 16.031%">Value Function</td>
<td style="width: 33.969%">The function that describes how good or bad a given state is</td>
<td style="width: 45%">A scalar value representing the expected cumulative reward</td>
</tr>
<tr>
<td style="width: 16.031%">Model</td>
<td style="width: 33.969%">An agent's representation of the environment, which predicts how the environment will react to the agent's actions</td>
<td style="width: 45%">
<p class="mce-root">The probability of the next state given an action and current state, or an enumeration of possible states given the rules of the environment</p>
</td>
</tr>
</tbody>
</table>
<p>In the following sections, we will use these concepts to learn about one of the most fundamental frameworks in reinforcement learning: the Markov decision process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov decision process (MDP)</h1>
                </header>
            
            <article>
                
<p>A Markov decision process is a framework used to represent the environment of a reinforcement learning problem. It is a graphical model with directed edges (meaning that one node of the graph points to another node). Each node represents a possible state in the environment, and each edge pointing out of a state represents an action that can be taken in the given state. For example, consider the following MDP:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-613 image-border" src="assets/3689b1c6-d71e-44ff-b5bf-e7ddf2ac33aa.png" style="width:29.00em;height:24.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: A sample Markov Decision Process</div>
<p>The preceding MDP represents what a typical day of a programmer could look like. Each circle represents a particular state the programmer can be in, where the blue state (<span class="packt_screen">Wake Up</span>) is the initial state (or the state the agent is in at <em>t</em>=0), and the orange state (<span class="packt_screen">Publish Code</span>) denotes the terminal state. Each arrow represents the transitions that the programmer can make between states. Each state has a reward that is associated with it, and the higher the reward, the more desirable the state is.</p>
<p>We can tabulate the rewards as an adjacency matrix as well:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>State\action</strong></td>
<td><strong>Wake Up</strong></td>
<td><strong>Netflix</strong></td>
<td><strong>Code and debug</strong></td>
<td><strong>Nap</strong></td>
<td><strong>Deploy</strong></td>
<td><strong>Sleep</strong></td>
</tr>
<tr>
<td>Wake Up</td>
<td>N/A</td>
<td>-2</td>
<td>-3</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Netflix</td>
<td>N/A</td>
<td>-2</td>
<td>N/A</td>
<td>N/A</td>
<td>
<p class="mce-root">N/A</p>
</td>
<td>N/A</td>
</tr>
<tr>
<td>Code and debug</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>1</td>
<td>10</td>
<td>3</td>
</tr>
<tr>
<td>Nap</td>
<td>0</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Deploy</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>3</td>
</tr>
<tr>
<td>Sleep</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The left column represents the possible states and the top row represents the possible actions. <span><span>N/A</span></span> means that the action is not performable from the given state. This system basically represents the decisions that a programmer can make throughout their day.</p>
<p>When the programmer wakes up, they can either decide to work (code and debug the code) or watch Netflix. Notice that the reward for watching Netflix is higher than that of coding and debugging. For the programmer in question, watching Netflix seems like a more rewarding activity, while coding and debugging is perhaps a chore (which, I hope, is not the case for the reader!). However, both actions yield negative rewards, even though our objective is to maximize our cumulative reward. If the programmer chooses to watch Netflix, they will be stuck in an endless loop of binge-watching, which continuously lowers the reward. Rather, more rewarding states will become available to the programmer if they decide to code diligently. Let's look at the possible trajectories, which are the sequence of actions, the programmer can take:</p>
<ul>
<li>Wake Up | Netflix | Netflix | ...</li>
<li>Wake Up | Code and debug | Nap | Wake Up | Code and debug | Nap | ...</li>
<li>Wake Up | Code and debug | Sleep</li>
<li>Wake Up | Code and debug | Deploy | Sleep</li>
</ul>
<p>Both the first and second trajectories represent infinite loops. Let's calculate the cumulative reward for each, where we set <img class="fm-editor-equation" src="assets/2ce1a228-1215-4a7d-8ede-d76546086518.png" style="width:3.58em;height:1.17em;"/>:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/0e4c2b34-4bcc-483b-af84-4487c6bcbada.png" style="width:25.83em;height:1.33em;"/></li>
<li><img class="fm-editor-equation" src="assets/f13ee029-465e-44d3-a489-b3f0f3efe3b2.png" style="width:36.58em;height:1.33em;"/></li>
<li><img class="fm-editor-equation" src="assets/df4ac549-14a7-409e-9a1a-903e43cc4000.png" style="width:19.33em;height:1.33em;"/></li>
<li><img class="fm-editor-equation" src="assets/dd65787b-7681-4863-af35-b177d76c4ee2.png" style="width:24.42em;height:1.25em;"/></li>
</ul>
<p>It is easy to see that both the first and second trajectories, despite not reaching a terminal state, will never return positive rewards. The fourth trajectory yields the highest reward (successfully deploying code is a highly rewarding accomplishment!).</p>
<p>What we have calculated are the value functions for four policies that a programmer can take to go through their day. Recall that the value function is the expected cumulative reward starting from a given state and following a policy. We have observed four possible policies and have evaluated how each leads to a different cumulative reward; this exercise is also called <strong>policy evaluation</strong>. Moreover, the equations we have applied to calculate the expected rewards are also known as <strong>Bellman expectation equations</strong>. The Bellman equations are a set of equations used to evaluate and improve policies and value functions to help a reinforcement learning agent learn better. Though a thorough introduction to Bellman equations is outside the scope of this book, they are foundational to building a theoretical understanding of reinforcement learning. We encourage the reader to look into this further.</p>
<div class="packt_infobox">While we will not cover Bellman equations in depth, we highly recommend the reader to do so in order to build a solid understanding of reinforcement learning. For more information, refer to <em>Reinforcement Learning: An Introduction</em>, by Richard S. Sutton and Andrew Barto (reference at the end of this chapter).</div>
<p>Now that you have learned about some the key terms and concepts of reinforcement learning, you may be wondering how we teach a reinforcement learning agent to maximize its reward, or in other words, find that the fourth trajectory is the best. In this book, you will be working on solving this question for numerous tasks and problems, all using deep learning. While we encourage you to be familiar with the basics of deep learning, the following sections will serve as a light refresher to the field.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning</h1>
                </header>
            
            <article>
                
<p class="p1">Deep learning has become one of the most popular and recognizable fields of machine learning and computer science. Thanks to an increase in both available data and computational resources, deep learning algorithms have successfully surpassed previous state-of-the-art results in countless tasks. For several domains, including image recognition and playing Go, deep learning has even exceeded the capabilities of mankind.</p>
<p class="p1">It is thus not surprising that many reinforcement learning algorithms have started to utilize deep learning to bolster performance. Many of the reinforcement learning algorithms from the beginning of this chapter rely on deep learning. This book, too, will revolve around deep learning algorithms used to tackle reinforcement learning problems.</p>
<p>The following sections will serve as a refresher on some of the most fundamental concepts of deep learning, including neural networks, backpropagation, and convolution. However, if are unfamiliar with these topics, we highly encourage you to seek other sources for a more in-depth introduction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks</h1>
                </header>
            
            <article>
                
<p>A neural network is a type of computational architecture that is composed of layers of perceptrons. A perceptron, first conceived  in the 1950s by Frank Rosenblatt, models the biological neuron and computes a linear combination of a vector of input. It also outputs a transformation of the linear combination using a non-linear activation, such as the sigmoid function. Suppose a perceptron receives an input vector of <img class="fm-editor-equation" src="assets/b39f4391-4708-416b-b9ec-d84b48dd6199.png" style="width:3.92em;height:1.17em;"/>. The output, <em>a</em>, of the perceptron, would be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e0b5df84-5d65-4f22-8c83-ffac52e8f37b.png" style="width:10.92em;height:3.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/83824b91-f9f3-465e-9729-60b612a40ac7.png" style="width:6.17em;height:2.08em;"/></p>
<p>Where<img class="fm-editor-equation" src="assets/f5edf276-9cdb-4ffb-9a46-b75cf44e960a.png" style="width:8.92em;height:1.42em;"/>are the weights of the perceptron, <em>b</em> is a constant, called the <strong>bias</strong>, and<img class="fm-editor-equation" src="assets/4a6b153a-6786-4012-8b2a-af618dba2365.png" style="width:0.92em;height:0.92em;"/>is the sigmoid activation function that outputs a value between 0 and 1.</p>
<p>Perceptrons have been widely used as a computational model to make decisions. Suppose the task was to predict the likelihood of sunny weather the next day. Each<img class="fm-editor-equation" src="assets/0d974f85-0cd9-4f6a-9af3-185498a44dd6.png" style="width:1.08em;height:0.92em;"/>would represent a variable, such as the temperature of the current day, humidity, or the weather of the previous day. Then,<img class="fm-editor-equation" src="assets/5a2faf8a-f99b-4355-8570-70d24f7dfb7b.png" style="width:2.08em;height:1.33em;"/>would compute a value that reflects how likely it is that there will be sunny weather tomorrow. If the model has a good set of values for <img class="fm-editor-equation" src="assets/d46a16f7-4915-42c1-8f20-1d63cdc4612b.png" style="width:1.08em;height:1.00em;"/>, it is able to make accurate decisions.</p>
<p>In a typical neural network, there are multiple layers of neurons, where each neuron in a given layer is connected to all neurons in the prior and subsequent layers. Hence these layers are also referred to as <strong>fully-connected layers</strong>. The weights of a given layer, <em>l</em>, can be represented as a matrix, <em>W<sup>l</sup></em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9b87c57b-a48f-45a5-be1e-1e8975ee902e.png" style="width:16.00em;height:6.67em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/888affc6-ff04-4339-b490-62aff2826f91.png" style="width:9.58em;height:1.50em;"/></p>
<p>Where each <em>w<sub>ij</sub></em> denotes the weight between the <em>i</em> neuron of the previous layer and the <em>j</em> neuron of this layer. <em>B<sup>l</sup></em> denotes a vector of biases, one for each neuron in the <em>l</em> layer. Hence, the activation, <em>a<sup>l</sup></em>, of a given layer, <em>l</em>, can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16fbd6ed-bd48-47ec-8f89-fcf3e21390d8.png" style="width:13.75em;height:1.58em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/89cfdef4-6377-40c7-a9c2-c7b3689f9ec8.png" style="width:5.00em;height:1.50em;"/></p>
<p class="CDPAlignLeft CDPAlign">Where <em>a<sup>0</sup>(x)</em> is just the input. Such neural networks with multiple layers of neurons are called <strong>multilayer perceptrons</strong> (<strong>MLP</strong>). There are three components in an MLP: the input layer, the hidden layers, and the output layer. The data flows from the input layer, transformed through a series of linear and non-linear functions in the hidden layers, and is outputted from the output layer as a decision or a prediction. Hence this architecture is also referred to as a feed-forward network. The following diagram shows what a fully-connected network would look like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-614 image-border" src="assets/84aad163-22ce-4216-b5bb-afd517c12ce7.png" style="width:34.25em;height:27.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6: A sketch of a multilayer perceptron</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p class="p1">As mentioned previously, a neural network's performance depends on how good the values of <em>W</em> are (for simplicity, we will refer to both the weights and biases as <em>W</em>). When the whole network grows in size, it becomes untenable to manually determine the optimal weights for each neuron in every layer. Therefore, we rely on backpropagation, an algorithm that iteratively and automatically updates the weights of every neuron.</p>
<p class="p1">To update the weights, we first need the ground truth, or the target value that the neural network tries to output. To understand what this ground truth could look like, we formulate a sample problem. The <kbd>MNIST</kbd> dataset is a large repository of 28x28 images of handwritten digits. It contains 70,000 images in total and serves as a popular benchmark for machine learning models. Given ten different classes of digits (from zero to nine), we would like to identify which digit class a given images belongs to. We can represent the ground truth of each image as a vector of length 10, where the index of the class (starting from 0) is marked as 1 and the rest are 0s. For example, an image, <em>x</em>, with a class label of five would have the ground truth of <img class="fm-editor-equation" src="assets/82513266-77d8-4584-9f41-3c233a88f953.png" style="width:17.33em;height:1.58em;"/>, where <em>y</em> is the target function we approximate.</p>
<p class="p1">What should the neural network look like? If we take each pixel in the image to be an input, we would have 28x28 neurons in the input layer (every image would be flattened to become a 784-dimensional vector). Moreover, because there are 10 digit classes, we have 10 neurons in the output layer, each neuron producing a sigmoid activation for a given class. There can be an arbitrary number of neurons in the hidden layers.</p>
<p class="p1">Let <em>f</em> represent the sequence of transformations that the neural network computes, parameterized by the weights, <em>W</em>. <em>f</em> is essentially an approximation of the target function, <em>y</em>, and maps the 784-dimensional input vector to a 10 dimensional output prediction. We classify the image according to the index of the largest sigmoid output.</p>
<p class="p1">Now that we have formulated the ground truth, we can measure the distance between it and the network's prediction. This error is what allows the network to update its weights. We define the error function <em>E(W)</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11792f9a-716f-4a06-b64a-82ac338d94d3.png" style="width:14.67em;height:2.83em;"/></p>
<p>The goal of backpropagation is to minimize <em>E</em> by finding the right set of <em>W</em>. This minimization is an optimization problem whereby we use gradient descent to iteratively compute the gradients of <em>E</em> with respect to <em>W</em> and propagate them through the network starting from the output layer.</p>
<p>Unfortunately, an in-depth explanation of backpropagation is outside the scope of this introductory chapter. If you are unfamiliar with this concept, we highly encourage you to study it first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p class="p1">Using backpropagation, we are now able to train large networks automatically. This has led to the development of increasingly complex neural network architectures. One example is the <strong>convolutional neural network</strong> (<strong>CNN</strong>). There are mainly three types of layers in a CNN: the convolutional layer, the pooling layer, and the fully-connected layer. The fully-connected layer is identical to the standard neural network discussed previously. In the convolutional layer, weights are part of convolutional kernels. Convolution on a two-dimensional array of image pixels is defined as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c81ae657-337c-4b21-be08-f4a2a7dc81f5.png" style="width:17.83em;height:2.42em;"/></p>
<p class="p1">Where <em>f(u, v)</em> is the pixel intensity of the input at coordinate <em>(u, v)</em>, and <em>g(x-u, y-v)</em> is the weight of the convolutional kernel at that location.</p>
<p class="p1">A convolutional layer comprises a stack of convolutional kernels; hence the weights of a convolutional layer can be visualized as a three-dimensional box as opposed to the two-dimensional array that we defined for fully-connected layers. The output of a single convolutional kernel applied to an input is also a two-dimensional mapping, which we call a filter. B<span>ecause there are multiple kernels, the output of a convolutional layer is again a three-dimensional box, which can be referred to as a volume.</span></p>
<p class="p1">Finally, the pooling layer reduces the size of the input by taking <em>m*m</em> local patches of pixels and outputting a scalar. The max-pooling layer takes <em>m*m</em> patches and outputs the greatest value among the patch of pixels.</p>
<p class="p1">Given an input volume of the (32, 32, 3) shape—corresponding to height, width, and depth (channels)—a max-pooling layer with a pooling size of 2x2 will output a volume of the (16, 16, 3) shape. The input to the CNN are usually images, which can also be viewed as volumes where the depth corresponds to RGB channels.</p>
<p class="p1">The following is a depiction of a typical convolutional neural network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-748 image-border" src="assets/dc56335f-cb3f-48b6-bfac-238e51e32725.png" style="width:162.50em;height:49.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: An example convolutional neural network</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advantages of neural networks</h1>
                </header>
            
            <article>
                
<p class="p1">The main advantage of a CNN over a standard neural network is that the former is able to learn visual and spatial features of the input, while for the latter such information is lost due to flattening input data into a vector. CNNs have made significant strides in the field of computer vision, starting with increased classification accuracies of <kbd>MNIST</kbd> data and object recognition, semantic segmentation, and other domains. CNNs have many applications in real life, from facial detection in social media to autonomous vehicles. Recent approaches have also applied CNNs to natural language processing and text classification tasks to produce state-of-the-art results.</p>
<p>Now that we have covered the basics of machine learning, we will go through our first implementation exercise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a convolutional neural network in TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this section, we will implement a simple convolutional neural network in TensorFlow to solve an image classification task. As the rest of this book will be heavily reliant on TensorFlow and CNNs, we highly recommend that  you become sufficiently familiar with implementing deep learning algorithms using this framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow, developed by Google in 2015, is one of the most popular deep learning frameworks in the world. It is used widely for research and commercial projects and boasts a rich set of APIs and functionalities to help researchers and practitioners develop deep learning models. TensorFlow programs can run on GPUs as well as CPUs, and thus abstract the GPU programming to make development more convenient.</p>
<p>Throughout this book, we will be using TensorFlow exclusively, so make sure you are familiar with the basics as you progress through the chapters.</p>
<div class="packt_infobox">Visit <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a> for a complete set of documentation and other tutorials.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Fashion-MNIST dataset</h1>
                </header>
            
            <article>
                
<p>Those who have experience with deep learning have most likely heard about the <kbd>MNIST</kbd> dataset. It is one of the most widely-used image datasets, serving as a benchmark for tasks such as image classification and image generation, and is used by many computer vision models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-615 image-border" src="assets/8248ffb8-5363-4ec7-84fc-21d807ecd434.png" style="width:23.92em;height:23.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8: The MNIST dataset (reference at end of chapter)</div>
<p class="mce-root"/>
<p>There are several problems with <kbd>MNIST</kbd>, however. First of all, the dataset is too easy, since a simple convolutional neural network is able to achieve 99% test accuracy. In spite of this, the dataset is used far too often in research and benchmarks. The <kbd>F-MNIST</kbd> dataset, produced by the online fashion retailer Zalando, is a more complex, much-needed upgrade to <kbd>MNIST</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-616 image-border" src="assets/1323649e-eba8-48da-8242-490ff261a28b.png" style="width:40.33em;height:40.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9: The Fashion-MNIST dataset (taken from <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>, reference at the end of this chapter)</div>
<p>Instead of digits, the <kbd>F-MNIST</kbd> dataset includes photos of ten different clothing types (ranging from t-shirts to shoes) compressed in to 28x28 monochrome thumbnails. Hence, <kbd>F-MNIST</kbd> serves as a convenient drop-in replacement to <kbd>MNIST</kbd> and is increasingly gaining popularity in the community. Hence we will train our CNN on <kbd>F-MNIST</kbd> as well. The preceding table maps each label index to its class:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px"><strong>Index</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px"><strong>Class</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">0</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">T-shirt/top</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">1</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Trousers</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">2</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Pullover</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">3</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Dress</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">4</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Coat</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">5</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Sandal</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">6</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Shirt</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">7</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Sneaker</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">8</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Bag</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 220px">9</td>
<td class="CDPAlignCenter CDPAlign" style="width: 433px">Ankle boot</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the following subsections, we will design a convolutional neural network that will learn to classify data from this dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the network</h1>
                </header>
            
            <article>
                
<p>Multiple deep learning frameworks have already implemented APIs for loading the <kbd>F-MNIST</kbd> dataset, including TensorFlow. For our implementation, we will be using Keras, another popular deep learning framework that is integrated with TensorFlow. The Keras datasets module provides a highly convenient interface for loading the datasets as <kbd>numpy</kbd> arrays.</p>
<p>Finally, we can start coding! For this exercise, we only need one Python module, which we will call <kbd>cnn.py</kbd>. Open up your favorite text editor or IDE, and let's get started.</p>
<p>Our first step is to declare the modules that we are going to use:</p>
<pre><span>import </span>logging<br/><span>import </span>os<br/><span>import </span>sys<br/><br/>logger = logging.getLogger(__name__)<br/><br/><span>import </span>tensorflow <span>as </span>tf<br/><span>import </span>numpy <span>as </span>np<br/><span>from </span>keras.datasets <span>import </span>fashion_mnist<br/><span>from </span>keras.utils <span>import </span>np_utils</pre>
<p>The following describes what each module is for and how we will use it:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><strong>Module(s)</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 74.7725%"><strong>Purpose</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><kbd>logging</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 74.7725%">For printing statistics as we run the code</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><kbd>os</kbd>, <kbd>sys</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 74.7725%">
<p class="mce-root">For interacting with the operating system, including writing files</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><kbd>tensorflow</kbd></td>
<td class="CDPAlignCenter" style="width: 74.7725%">The main TensorFlow library</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><kbd>numpy</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 74.7725%">An optimized library for vector calculations and simple data processing</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 23.2275%"><kbd>keras</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 74.7725%">For downloading the F-MNIST dataset</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We will implement our CNN as a class called <kbd>SimpleCNN</kbd>. The <kbd>__init__</kbd> constructor takes a number of parameters:</p>
<pre><span>class </span><span>SimpleCNN</span>(<span>object</span>):<br/><br/>    <span>def </span>__init__(<span>self</span>, <span>learning_rate</span>, <span>num_epochs</span>, <span>beta</span>, <span>batch_size</span>):<br/>        <span>self</span>.learning_rate = <span>learning_rate<br/></span><span>        </span><span>self</span>.num_epochs = <span>num_epochs<br/></span><span>        </span><span>self</span>.beta = <span>beta<br/></span><span>        </span><span>self</span>.batch_size = <span>batch_size<br/></span><span>        </span><span>self</span>.save_dir = <span>"saves"<br/></span><span>        </span><span>self</span>.logs_dir = <span>"logs"<br/></span><span>        </span>os.makedirs(<span>self</span>.save_dir, <span>exist_ok</span>=<span>True</span>)<br/>        os.makedirs(<span>self</span>.logs_dir, <span>exist_ok</span>=<span>True</span>)<br/>        <span>self</span>.save_path = os.path.join(<span>self</span>.save_dir, <span>"simple_cnn"</span>)<br/>        <span>self</span>.logs_path = os.path.join(<span>self</span>.logs_dir, <span>"simple_cnn"</span>)</pre>
<p>The parameters our <kbd>SimpleCNN</kbd> is initialized with are described here:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignLeft CDPAlign"><strong>Parameter</strong></td>
<td class="CDPAlignLeft CDPAlign"><strong>Purpose</strong></td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>learning_rate</kbd></td>
<td class="CDPAlignLeft CDPAlign">The learning rate for the optimization algorithm</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>num_epochs</kbd></td>
<td class="CDPAlignLeft CDPAlign"><span>The number of epochs it takes to train the network</span></td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>beta</kbd></td>
<td class="CDPAlignLeft CDPAlign">A float value (between 0 and 1) that controls the strength of the L2-penalty</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>batch_size</kbd></td>
<td class="CDPAlignLeft CDPAlign">
<p class="mce-root"><span>The number of images to train on in a single step</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Moreover, <kbd>save_dir</kbd> and <kbd>save_path</kbd> refer to the locations where we will store our network's parameters. <kbd>logs_dir</kbd> and <kbd>logs_path</kbd> refer to the locations where the statistics of the training run will be stored (we will show how we can retrieve these logs later).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Methods for building the network</h1>
                </header>
            
            <article>
                
<p>Now, in this section, we will see two methods that can be used to build the function, which are:</p>
<ul>
<li>build method</li>
<li>fit method</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">build method</h1>
                </header>
            
            <article>
                
<p>The first method we will define for our <kbd>SimpleCNN</kbd> class is the <kbd>build</kbd> method, which is responsible for building the architecture of our CNN. Our <kbd>build</kbd> method takes two pieces of input: the input tensor and the number of classes it should expect:</p>
<pre><span>def </span><span>build</span>(<span>self</span>, <span>input_tensor</span>, <span>num_classes</span>):<br/>    <span>"""<br/></span><span>    Builds a convolutional neural network according to the input shape and the number of classes.<br/></span><span>    Architecture is fixed.<br/></span><span><br/></span><span>    Args:<br/></span><span>        input_tensor: Tensor of the input<br/></span><span>        num_classes: (int) number of classes<br/></span><span><br/></span><span>    Returns:<br/></span><span>        The output logits before softmax<br/></span><span>    """</span></pre>
<p>We will first initialize <kbd>tf.placeholder</kbd>, called <kbd>is_training</kbd>. TensorFlow placeholders are like variables that don't have values. We only pass them values when we actually train the network and call the relevant operations:</p>
<pre><span>with </span>tf.name_scope(<span>"input_placeholders"</span>):<br/>    <span>self</span>.is_training = tf.placeholder_with_default(<span>True</span>, <span>shape</span>=(), <span>name</span>=<span>"is_training"</span>)</pre>
<p>The <kbd>tf.name_scope(...)</kbd> block allows us to name our operations and tensors properly. While this is not absolutely necessary, it helps us organize our code better and will help us to visualize the network. Here, we define a <kbd>tf.placeholder_with_default</kbd> called <kbd>is_training</kbd>, which has a default value of <kbd>True</kbd>. This placeholder will be used for our dropout operations (since dropout has different modes during training and inference).</p>
<p class="mce-root"/>
<div class="packt_tip">Naming your operations and tensors is considered a good practice. It helps you organize your code.</div>
<p>Our next step is to define the convolutional layers of our CNN. We make use of three different kinds of layers to create multiple layers of convolutions: <kbd>tf.layers.conv2d</kbd>, <kbd>tf.max_pooling2d</kbd>, and <kbd>tf.layers.dropout</kbd>:</p>
<pre><span>with </span>tf.name_scope(<span>"convolutional_layers"</span>):<br/>    conv_1 = tf.layers.conv2d(<br/>        <span>input_tensor</span>,<br/>        <span>filters</span>=<span>16</span>,<br/>        <span>kernel_size</span>=(<span>5</span>, <span>5</span>),<br/>        <span>strides</span>=(<span>1</span>, <span>1</span>),<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>activation</span>=tf.nn.relu,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"conv_1"</span>)<br/>    conv_2 = tf.layers.conv2d(<br/>        conv_1,<br/>        <span>filters</span>=<span>32</span>,<br/>        <span>kernel_size</span>=(<span>3</span>, <span>3</span>),<br/>        <span>strides</span>=(<span>1</span>, <span>1</span>),<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>activation</span>=tf.nn.relu,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"conv_2"</span>)<br/>    pool_3 = tf.layers.max_pooling2d(<br/>        conv_2,<br/>        <span>pool_size</span>=(<span>2</span>, <span>2</span>),<br/>        <span>strides</span>=<span>1</span>,<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>name</span>=<span>"pool_3"<br/></span><span>    </span>)<br/>    drop_4 = tf.layers.dropout(pool_3, <span>training</span>=<span>self</span>.is_training, <span>name</span>=<span>"drop_4"</span>)<br/><br/>    conv_5 = tf.layers.conv2d(<br/>        drop_4,<br/>        <span>filters</span>=<span>64</span>,<br/>        <span>kernel_size</span>=(<span>3</span>, <span>3</span>),<br/>        <span>strides</span>=(<span>1</span>, <span>1</span>),<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>activation</span>=tf.nn.relu,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"conv_5"</span>)<br/>    conv_6 = tf.layers.conv2d(<br/>        conv_5,<br/>        <span>filters</span>=<span>128</span>,<br/>        <span>kernel_size</span>=(<span>3</span>, <span>3</span>),<br/>        <span>strides</span>=(<span>1</span>, <span>1</span>),<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>activation</span>=tf.nn.relu,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"conv_6"</span>)<br/>    pool_7 = tf.layers.max_pooling2d(<br/>        conv_6,<br/>        <span>pool_size</span>=(<span>2</span>, <span>2</span>),<br/>        <span>strides</span>=<span>1</span>,<br/>        <span>padding</span>=<span>"SAME"</span>,<br/>        <span>name</span>=<span>"pool_7"<br/></span><span>    </span>)<br/>    drop_8 = tf.layers.dropout(pool_7, <span>training</span>=<span>self</span>.is_training, <span>name</span>=<span>"drop_8"</span>)</pre>
<p>Here are some explanations of the parameters:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><strong>Parameter</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><strong>Type</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.9342%"><strong>Description</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>filters</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>int</kbd></td>
<td style="width: 48.9342%">Number of filters output by the convolution.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>kernel_size</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%">Tuple of <kbd>int</kbd></td>
<td style="width: 48.9342%">The shape of the kernel.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>pool_size</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%">Tuple of <kbd>int</kbd></td>
<td style="width: 48.9342%">The shape of the max-pooling window.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>strides</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>int</kbd></td>
<td style="width: 48.9342%">The number of pixels to slide across per convolution/max-pooling operation.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>padding</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>str</kbd></td>
<td style="width: 48.9342%">Whether to add padding (SAME) or not (VALID). If padding is added, the output shape of the convolution remains the same as the input shape.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>activation</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>func</kbd></td>
<td style="width: 48.9342%">A TensorFlow activation function.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd><span><span>kernel_regularizer</span></span></kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>op</kbd></td>
<td style="width: 48.9342%"><span>Which regularization to use for the convolutional kernel. The d</span>efault<span> value is </span><kbd>None</kbd>.</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 22%"><kbd>training</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.0658%"><kbd>op</kbd></td>
<td style="width: 48.9342%">A tensor/placeholder that tells the dropout operation whether the forward pass is for training or for inference.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding table, we have specified the convolutional architecture to have the following sequence of layers:</p>
<p class="CDPAlignCenter CDPAlign">CONV | CONV | POOL | DROPOUT | CONV | CONV | POOL | DROPOUT</p>
<p>However, you are encouraged to explore different configurations and architectures. For example, you could add batch-normalization layers to improve the stability of training.</p>
<p>Finally, we add the fully-connected layers that lead to the output of the network:</p>
<pre><span>with </span>tf.name_scope(<span>"fully_connected_layers"</span>):<br/>    flattened = tf.layers.flatten(drop_8, <span>name</span>=<span>"flatten"</span>)<br/>    fc_9 = tf.layers.dense(<br/>        flattened,<br/>        <span>units</span>=<span>1024</span>,<br/>        <span>activation</span>=tf.nn.relu,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"fc_9"<br/></span><span>    </span>)<br/>    drop_10 = tf.layers.dropout(fc_9, <span>training</span>=<span>self</span>.is_training, <span>name</span>=<span>"drop_10"</span>)<br/>    logits = tf.layers.dense(<br/>        drop_10,<br/>        <span>units</span>=<span>num_classes</span>,<br/>        <span>kernel_regularizer</span>=tf.contrib.layers.l2_regularizer(<span>scale</span>=<span>self</span>.beta),<br/>        <span>name</span>=<span>"logits"<br/></span><span>    </span>)<br/><br/><span>return </span>logits</pre>
<p><kbd>tf.layers.flatten</kbd> turns the output of the convolutional layers (which is 3-D) into a single vector (1-D) so that we can pass them through the <kbd>tf.layers.dense</kbd> layers. After going through two fully-connected layers, we return the final output, which we define as <kbd>logits</kbd>.</p>
<p>Notice that in the final <kbd>tf.layers.dense</kbd> layer, we do not specify an <kbd>activation</kbd>. We will see why when we move on to specifying the training operations of the network.</p>
<p>Next, we implement several helper functions. <kbd>_create_tf_dataset</kbd> takes two instances of <kbd>numpy.ndarray</kbd> and turns them into TensorFlow tensors, which can be directly fed into a network. <kbd>_log_loss_and_acc</kbd> simply logs training statistics, such as loss and accuracy:</p>
<pre><span>def </span><span>_create_tf_dataset</span>(<span>self</span>, <span>x</span>, <span>y</span>):<br/>    dataset = tf.data.Dataset.zip((<br/>            tf.data.Dataset.from_tensor_slices(<span>x</span>),<br/>            tf.data.Dataset.from_tensor_slices(<span>y</span>)<br/>        )).shuffle(<span>50</span>).repeat().batch(<span>self</span>.batch_size)<br/>    <span>return </span>dataset<br/><br/><span>def </span><span>_log_loss_and_acc</span>(<span>self</span>, <span>epoch</span>, <span>loss</span>, <span>acc</span>, <span>suffix</span>):<br/>    summary = tf.Summary(<span>value</span>=[<br/>        tf.Summary.Value(<span>tag</span>=<span>"loss_{}"</span>.format(<span>suffix</span>), <span>simple_value</span>=<span>float</span>(<span>loss</span>)),<br/>        tf.Summary.Value(<span>tag</span>=<span>"acc_{}"</span>.format(<span>suffix</span>), <span>simple_value</span>=<span>float</span>(<span>acc</span>))<br/>    ])<br/>    <span>self</span>.summary_writer.add_summary(summary, <span>epoch</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">fit method</h1>
                </header>
            
            <article>
                
<p>The last method we will implement for our <kbd>SimpleCNN</kbd> is the <kbd>fit</kbd> method. This function triggers training for our CNN. Our <kbd>fit</kbd> method takes four input:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><strong>Argument</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21.6273%"><strong>Description</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><kbd>X_train</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21.6273%">Training data</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><kbd>y_train</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21.6273%">Training labels</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><kbd>X_test</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21.6273%">Test data</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><kbd>y_test</kbd></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21.6273%">Test labels</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>The first step of </span><kbd>fit</kbd><span> is to initialize</span> <kbd>tf.Graph</kbd><span> and</span> <kbd>tf.Session</kbd><span>. Both of these objects are essential to any TensorFlow program.</span> <kbd>tf.Graph</kbd><span> represents the graph in which all the operations for our CNN are defined. You can think of it as a sandbox where we define a</span><span>ll the layers and functions.</span> <kbd>tf.Session</kbd><span> is the class that actually executes the operations d</span><span>efined in</span> <kbd>tf.Graph</kbd><span>:</span></p>
<pre><span>def </span><span>fit</span>(<span>self</span>, <span>X_train</span>, <span>y_train</span>, <span>X_valid</span>, <span>y_valid</span>):<br/>    <span>"""<br/></span><span>    Trains a CNN on given data<br/></span><span><br/></span><span>    Args:<br/></span><span>        numpy.ndarrays representing data and labels respectively<br/></span><span>    """<br/></span><span>    </span>graph = tf.Graph()<br/>    <span>with </span>graph.as_default():<br/>        sess = tf.Session()</pre>
<p>We then create datasets using TensorFlow's Dataset API and the <kbd>_create_tf_dataset</kbd> method we defined earlier:</p>
<pre>train_dataset = <span>self</span>._create_tf_dataset(<span>X_train</span>, <span>y_train</span>)<br/>valid_dataset = <span>self</span>._create_tf_dataset(<span>X_valid</span>, <span>y_valid</span>)<br/><br/><span># Creating a generic iterator<br/></span>iterator = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)<br/>next_tensor_batch = iterator.get_next()<br/><br/><span># Separate training and validation set init ops<br/></span>train_init_ops = iterator.make_initializer(train_dataset)<br/>valid_init_ops = iterator.make_initializer(valid_dataset)<br/><br/>input_tensor, labels = next_tensor_batch</pre>
<p><kbd>tf.data.Iterator</kbd> builds an iterator object that outputs a batch of images every time we call <kbd>iterator.get_next()</kbd>. We initialize a dataset each for the training and testing data. The result of <kbd>iterator.get_next()</kbd> is a tuple of input images and corresponding labels.</p>
<p>The former is <kbd>input_tensor</kbd>, which we feed into the <kbd>build</kbd> method. The latter is used for calculating the loss function and backpropagation:</p>
<pre>num_classes = <span>y_train</span>.shape[<span>1</span>]<br/><br/><span># Building the network<br/></span>logits = <span>self</span>.build(<span>input_tensor</span>=input_tensor, <span>num_classes</span>=num_classes)<br/>logger.info(<span>'Built network'</span>)<br/><br/>prediction = tf.nn.softmax(logits, <span>name</span>=<span>"predictions"</span>)<br/>loss_ops = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(<br/>    <span>labels</span>=labels, <span>logits</span>=logits), <span>name</span>=<span>"loss"</span>)</pre>
<p><kbd>logits</kbd> (the non-activated outputs of the network) are fed into two other operations: <kbd>prediction</kbd>, which is just the softmax over <kbd>logits</kbd> to obtain normalized probabilities over the classes, and <kbd>loss_ops</kbd>, which calculates the mean categorical cross-entropy between the predictions and the labels.</p>
<p>We then define the backpropagation algorithm used to train the network and the operations used for calculating accuracy:</p>
<pre>optimizer = tf.train.AdamOptimizer(<span>learning_rate</span>=<span>self</span>.learning_rate)<br/>train_ops = optimizer.minimize(loss_ops)<br/><br/>correct = tf.equal(tf.argmax(prediction, <span>1</span>), tf.argmax(labels, <span>1</span>), <span>name</span>=<span>"correct"</span>)<br/>accuracy_ops = tf.reduce_mean(tf.cast(correct, tf.float32), <span>name</span>=<span>"accuracy"</span>)</pre>
<p>We are now done building the network along with its optimization algorithms. We use <kbd>tf.global_variables_initializer()</kbd> to initialize the weights and operations of our network. We also initialize the <kbd>tf.train.Saver</kbd> and <kbd>tf.summary.FileWriter</kbd> objects. The<span><span> <kbd>tf.train.Saver</kbd> object</span></span> saves the weights and architecture of the network, whereas the latter keeps track of various training statistics:</p>
<pre>initializer = tf.global_variables_initializer()<br/><br/>logger.info(<span>'Initializing all variables'</span>)<br/>sess.run(initializer)<br/>logger.info(<span>'Initialized all variables'</span>)<br/><br/>sess.run(train_init_ops)<br/>logger.info(<span>'Initialized dataset iterator'</span>)<br/><span>self</span>.saver = tf.train.Saver()<br/><span>self</span>.summary_writer = tf.summary.FileWriter(<span>self</span>.logs_path)</pre>
<p>Finally, once we have set up everything we need, we can implement the actual training loop. For every epoch, we keep track of the training cross-entropy loss and accuracy of the network. At the end of every epoch, we save the updated weights to disk. We also calculate the validation loss and accuracy every 10 epochs. This is done by calling <kbd>sess.run(...)</kbd>, where the arguments to this function are the operations that the <kbd>sess</kbd> object should execute:</p>
<pre>logger.info(<span>"Training CNN for {} epochs"</span>.format(<span>self</span>.num_epochs))<br/><span>for </span>epoch_idx <span>in </span><span>range</span>(<span>1</span>, <span>self</span>.num_epochs+<span>1</span>):<br/>    loss, _, accuracy = sess.run([<br/>        loss_ops, train_ops, accuracy_ops<br/>    ])<br/>    <span>self</span>._log_loss_and_acc(epoch_idx, loss, accuracy, <span>"train"</span>)<br/><br/>    <span>if </span>epoch_idx % <span>10 </span>== <span>0</span>:<br/>        sess.run(valid_init_ops)<br/>        valid_loss, valid_accuracy = sess.run([<br/>            loss_ops, accuracy_ops<br/>        ], <span>feed_dict</span>={<span>self</span>.is_training: <span>False</span>})<br/>        logger.info(<span>"=====================&gt; Epoch {}"</span>.format(epoch_idx))<br/>        logger.info(<span>"</span><span>\t</span><span>Training accuracy: {:.3f}"</span>.format(accuracy))<br/>        logger.info(<span>"</span><span>\t</span><span>Training loss: {:.6f}"</span>.format(loss))<br/>        logger.info(<span>"</span><span>\t</span><span>Validation accuracy: {:.3f}"</span>.format(valid_accuracy))<br/>        logger.info(<span>"</span><span>\t</span><span>Validation loss: {:.6f}"</span>.format(valid_loss))<br/>        <span>self</span>._log_loss_and_acc(epoch_idx, valid_loss, valid_accuracy, <span>"valid"</span>)<br/><br/>    <span># Creating a checkpoint at every epoch<br/></span><span>    </span><span>self</span>.saver.save(sess, <span>self</span>.save_path)</pre>
<p>And that completes our <kbd>fit</kbd> function. Our final step is to create the script for instantiating the datasets, the neural network, and then running training, which we will write at the bottom of <kbd>cnn.py</kbd>.</p>
<p>We will first configure our logger and load the dataset using the Keras <kbd>fashion_mnist</kbd> module, which loads the training and testing data:</p>
<pre><span>if </span>__name__ == <span>"__main__"</span>:<br/>    logging.basicConfig(<span>stream</span>=sys.stdout,<br/>                        <span>level</span>=logging.DEBUG,<br/>                        <span>format</span>=<span>'%(asctime)s %(name)-12s %(levelname)-8s %(message)s'</span>)<br/>    logger = logging.getLogger(__name__)<br/><br/>    logger.info(<span>"Loading Fashion MNIST data"</span>)<br/>    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()</pre>
<p>We then apply some simple preprocessing to the data. The Keras API returns <kbd>numpy</kbd> arrays of the <kbd>(Number of images, 28, 28)</kbd> shape.</p>
<p>However, what we actually want is <kbd>(Number of images, 28, 28, 1)</kbd>, where the third axis is the channel axis. This is required because our convolutional layers expect input that have three axes. Moreover, the pixel values themselves are in the range of <kbd>[0, 255]</kbd>. We will divide them by 255 to get a range of <kbd>[0, 1]</kbd>. This is a common technique that helps stabilize training.</p>
<p>Furthermore, we turn the labels, which are simply an array of label indices, into one-hot encodings:</p>
<pre>logger.info(<span>'Shape of training data:'</span>)<br/>logger.info(<span>'Train: {}'</span>.format(X_train.shape))<br/>logger.info(<span>'Test: {}'</span>.format(X_test.shape))<br/><br/>logger.info(<span>'Adding channel axis to the data'</span>)<br/>X_train = X_train[:,:,:,np.newaxis]<br/>X_test = X_test[:,:,:,np.newaxis]<br/><br/>logger.info(<span>"Simple transformation by dividing pixels by 255"</span>)<br/>X_train = X_train / <span>255.<br/></span>X_test = X_test / <span>255.<br/></span><span><br/></span>X_train = X_train.astype(np.float32)<br/>X_test = X_test.astype(np.float32)<br/>y_train = y_train.astype(np.float32)<br/>y_test = y_test.astype(np.float32)<br/>num_classes = <span>len</span>(np.unique(y_train))<br/><br/>logger.info(<span>"Turning ys into one-hot encodings"</span>)<br/>y_train = np_utils.to_categorical(y_train, <span>num_classes</span>=num_classes)<br/>y_test = np_utils.to_categorical(y_test, <span>num_classes</span>=num_classes)</pre>
<p>We then define the input to the constructor of our <kbd>SimpleCNN</kbd>. Feel free to tweak the numbers to see how they affect the performance of the model:</p>
<pre>cnn_params = {<br/>    <span>"learning_rate"</span>: <span>3e-4</span>,<br/>    <span>"num_epochs"</span>: <span>100</span>,<br/>    <span>"beta"</span>: <span>1e-3</span>,<br/>    <span>"batch_size"</span>: <span>32<br/></span>}</pre>
<p>And finally, we instantiate <kbd>SimpleCNN</kbd> and call its <kbd>fit</kbd> method:</p>
<pre>logger.info(<span>'Initializing CNN'</span>)<br/>simple_cnn = SimpleCNN(**cnn_params)<br/>logger.info(<span>'Training CNN'</span>)<br/>simple_cnn.fit(<span>X_train</span>=X_train,<br/>               <span>X_valid</span>=X_test,<br/>               <span>y_train</span>=y_train,<br/>               <span>y_valid</span>=y_test)</pre>
<p>To run the entire script, all you need to do is run the module:</p>
<pre><strong>$ python cnn.py</strong></pre>
<p>And that's it! You have successfully implemented a convolutional neural network in TensorFlow to train on the <kbd>F-MNIST</kbd> dataset. To track the progress of the training, you can simply look at the output in your terminal/editor. You should see an output that resembles the following:</p>
<pre>$ python cnn.py<br/><strong>Using TensorFlow backend.</strong><br/><strong>2018-07-29 21:21:55,423 __main__ INFO Loading Fashion MNIST data</strong><br/><strong>2018-07-29 21:21:55,686 __main__ INFO Shape of training data:</strong><br/><strong>2018-07-29 21:21:55,687 __main__ INFO Train: (60000, 28, 28)</strong><br/><strong>2018-07-29 21:21:55,687 __main__ INFO Test: (10000, 28, 28)</strong><br/><strong>2018-07-29 21:21:55,687 __main__ INFO Adding channel axis to the data</strong><br/><strong>2018-07-29 21:21:55,687 __main__ INFO Simple transformation by dividing pixels by 255</strong><br/><strong>2018-07-29 21:21:55,914 __main__ INFO Turning ys into one-hot encodings</strong><br/><strong>2018-07-29 21:21:55,914 __main__ INFO Initializing CNN</strong><br/><strong>2018-07-29 21:21:55,914 __main__ INFO Training CNN</strong><br/><strong>2018-07-29 21:21:58,365 __main__ INFO Built network</strong><br/><strong>2018-07-29 21:21:58,562 __main__ INFO Initializing all variables</strong><br/><strong>2018-07-29 21:21:59,284 __main__ INFO Initialized all variables</strong><br/><strong>2018-07-29 21:21:59,639 __main__ INFO Initialized dataset iterator</strong><br/><strong>2018-07-29 21:22:00,880 __main__ INFO Training CNN for 100 epochs</strong><br/><strong>2018-07-29 21:24:23,781 __main__ INFO =====================&gt; Epoch 10</strong><br/><strong>2018-07-29 21:24:23,781 __main__ INFO Training accuracy: 0.406</strong><br/><strong>2018-07-29 21:24:23,781 __main__ INFO Training loss: 1.972021</strong><br/><strong>2018-07-29 21:24:23,781 __main__ INFO Validation accuracy: 0.500</strong><br/><strong>2018-07-29 21:24:23,782 __main__ INFO Validation loss: 2.108872</strong><br/><strong>2018-07-29 21:27:09,541 __main__ INFO =====================&gt; Epoch 20</strong><br/><strong>2018-07-29 21:27:09,541 __main__ INFO Training accuracy: 0.469</strong><br/><strong>2018-07-29 21:27:09,541 __main__ INFO Training loss: 1.573592</strong><br/><strong>2018-07-29 21:27:09,542 __main__ INFO Validation accuracy: 0.500</strong><br/><strong>2018-07-29 21:27:09,542 __main__ INFO Validation loss: 1.482948</strong><br/><strong>2018-07-29 21:29:57,750 __main__ INFO =====================&gt; Epoch 30</strong><br/><strong>2018-07-29 21:29:57,750 __main__ INFO Training accuracy: 0.531</strong><br/><strong>2018-07-29 21:29:57,750 __main__ INFO Training loss: 1.119335</strong><br/><strong>2018-07-29 21:29:57,750 __main__ INFO Validation accuracy: 0.625</strong><br/><strong>2018-07-29 21:29:57,750 __main__ INFO Validation loss: 0.905031</strong><br/><strong>2018-07-29 21:32:45,921 __main__ INFO =====================&gt; Epoch 40</strong><br/><strong>2018-07-29 21:32:45,922 __main__ INFO Training accuracy: 0.656</strong><br/><strong>2018-07-29 21:32:45,922 __main__ INFO Training loss: 0.896715</strong><br/><strong>2018-07-29 21:32:45,922 __main__ INFO Validation accuracy: 0.719</strong><br/><strong>2018-07-29 21:32:45,922 __main__ INFO Validation loss: 0.847015</strong></pre>
<p>Another thing to check out is TensorBoard, a visualization tool developed by the developers of TensorFlow, to graph the model's accuracy and loss. The <kbd>tf.summary.FileWriter</kbd> object we have used serves this purpose. You can run TensorBoard with the following command:</p>
<pre><strong>$ tensorboard --logdir=logs/</strong></pre>
<p><kbd>logs</kbd> is where our <kbd>SimpleCNN</kbd> model writes the statistics to. TensorBoard is a great tool for visualizing the structure of our <kbd>tf.Graph</kbd>, as well as seeing how statistics such as accuracy and loss change over time. By default, the TensorBoard logs can be accessed by pointing your browser to <kbd>localhost:6006</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-617 image-border" src="assets/92d80158-fa80-4b15-ae61-602e9bd0af1f.png" style="width:144.75em;height:75.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10: TensorBoard and its visualization of our CNN</div>
<p>Congratulations! We have successfully implemented a convolutional neural network using TensorFlow. However, the CNN we implemented is rather rudimentary, and only achieves mediocre accuracy—the challenge to the reader is to tweak the architecture to improve its performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took our first step in the world of reinforcement learning. We covered some of the fundamental concepts and terminology of the field, including the agent, the policy, the value function, and the reward. We  also covered basic topics in deep learning and implemented a simple convolutional neural network using TensorFlow.</p>
<p>The field of reinforcement learning is vast and ever-expanding; it would be impossible to cover all of it in a single book. We do, however, hope to equip you with the practical skills and the necessary experience to navigate this field.</p>
<p>The following chapters will consist of individual projects—we will use a combination of reinforcement learning and deep learning algorithms to tackle several tasks and problems. We will build agents that will learn to play Go, explore the world of Minecraft, and play Atari video games. We hope you are ready to embark on this exciting learning journey!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Sutton, Richard S., and Andrew G. Barto. </span><em>Reinforcement learning: An introduction</em><span>. MIT press, 1998.</span></p>
<p><span>Y. LeCun</span><span>, L. Bottou, Y. Bengio, and P. Haffner</span><em><span>. Gradient-based learning applied to document recognition. </span>Proceedings of the IEEE<span>, 86(11):2278-2324, November 1998. </span></em></p>
<p><span>Xiao, Han, Kashif Rasul, and Roland Vollgraf. <em>Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</em>. </span>arXiv preprint arXiv:1708.07747<span> (2017).</span></p>


            </article>

            
        </section>
    </body></html>